1999.mtsummit-1.45,1998.amta-tutorials.5,0,0.219056,"s even more impractical when fully multilingual information systems were considered, because document translation has to be conducted on each language pair in such a system. We therefore adopt the query translation approach. To translate user queries from source languages to target languages, we need multilingual/bilingual transfer dictionaries or corpora (parallel or non-parallel) [ 1 8 ]. This task includes the challenges of disambiguating senses of the translated queries and distributing the weighting for each translation candidate in a vector space model or a probabilistic retrieval model [7]. Our system MuST currently uses all the possible translations for each content word and performs no weight adjustment. Research on these specific issues will be the primary focus in the second phase of the MuST project. We currently concentrate on system 1 Logos Corporation, 111 Howard Boulevard, Suite 214, Mount Arlington. NJ 07856, USA. MT Summit VII Sept. 1999 design and integration, which we describe in this paper. With respect to the retrieved documents, the issue is whether they can be presented in the original language. Probably not: most users do not master many foreign languages. Oar"
1999.mtsummit-1.45,E99-1011,0,0.111326,"not necessary. Here the retrieved image automatically explains itself. However, this will not always be the case: a caption for an image has a very good reason for existing. A rough translation probably suits the case better. Still, the question remains: of what quality? High quality is not always possible, and translation speed is also a concern. Therefore, shallow translation of browsing quality seems a more practical alternative. To help overcome the problem of speed, one can consider producing only a translated summary of the foreign original text. Tombros & Sanderson [26] and Mani et al. [17] have separately reported that user biased summaries can improve monolingual retrieval performance. We believe translated summaries can also help users in a similar way. But what is the cost of developing a robust and portable multilingual text summarizer? Is this possible? MuST is a prototype multilingual information retrieval, summarization, and translation system, in which we have tried to identify reasonable solutions for the questions mentioned above. Although the main focus of this paper is on system design and implementation, we believe that understanding of these issues helps explain m"
1999.mtsummit-1.45,oard-1998-comparative,0,0.110267,"val. - 308- What needs to be done to link MT and IR to create multilingual information retrieval (MLIR)? The problem of language encoding and display used to be an issue, but is now less daunting with the advent of Unicode and web browsers such as Microsoft Internet Explorer and Netscape Navigator. This allows us to focus on the two difficult problems of query translation and result translation. One way to tackle the multilingual information retrieval problem is to translate all the target language text into source language text and then perform monolingual search on the translated text. Oard [20] reports that machine translation (MT) based document translation outperforms MT based query translation. However, translation of 251.840 documents from German to English takes about 10 machine-months on a mix of SPARC 20, SPARC 5. and Ultra SPARC 1 using the Logos translation engine1. Without better machines and high speed/quality MT. we can rule out the practical application of this approach for the web. The document translation approach is even more impractical when fully multilingual information systems were considered, because document translation has to be conducted on each language pair"
1999.mtsummit-1.45,C00-1072,1,\N,Missing
2020.coling-main.272,P05-1018,0,0.0937102,"en automatically constructed from parallel structured data and descriptions in websites. Some of the collected instances inevitably contain noise where input structured data are semantically inequivalent with associated texts (Perez-Beltrachini and Gardent, 2017; Nie et al., 2019). Fig. 1 depicts an example. The slot-value pair Rating:low in the input meaning representation (MR) is contradicted with the text span highly recommended in descriptions, while the text span restaurant in descriptions should refer to a slot-value pair EatType:restaurant, which is not in the paired MR. Previous work (Barzilay and Lapata, 2005; Liang et al., 2009; Perez-Beltrachini and Lapata, 2018) aligns text spans in descriptions by only focusing on building semantic correspondences with the paired MR. However, in the scenario of semantically inequivalent MR-text pairs, due to the noise of input, there is not sufficient information to induce alignments for text spans such as restaurant by merely looking at its current input. To tackle the challenge of inducing correspondences (i.e., alignments) from semantically inequivalent data text pairs, we propose a local-to-global Alignment (L2GA) framework. This framework is composed of a"
2020.coling-main.272,D09-1014,0,0.0315496,"elated to previous work on extracting information from user queries with the backend data structure. Most of these approaches contain two steps. Initially, a separate model is applied to match the unstructured texts with relevant input records and then an extraction model is learned based on collected annotations. (Agichtein and Ganti, 2004) and (Canisius and Sporleder, 2007) train a language model on data records to identify related text spans in book description. Several approaches train a CRF based extractor to detect the related text spans (Michelson and Knoblock, 2008; Li et al., 2009). (Bellare and McCallum, 2009) apply a generalized expectation criteria to learn alignments between database and the texts, and train the information extractor to induce semantic annotations for text spans. Compared to these work, our approach is an unified neural based alignment model which avoids the error propagation of each step. 6 Conclusion In this paper, we study the problem of learning alignments in semantically inequivalent data-text pairs. We propose a local-to-global framework which not only induces semantic correspondences for words that are related to its paired input but also infers potential labels for text"
2020.coling-main.272,W10-4217,0,0.0409578,"ext span highly. While L2GA can dynamically integrate the alignment results provided by the local model, therefore produce the correct alignment Rating for the text span highly rated correctly. 5 Related Work Previous work exploiting loosely aligned data and text corpora have mostly focused on discovering verbalisation spans for data units. These line of work usually follows a two stage paradigm, where data 3057 units are first aligned with sentences from related corpora using heuristics and then subsequently extra content is discarded in order to retain only text spans verbalising the data. (Belz and Kow, 2010) use a measure of association between data units and words to obtain verbalisation spans. (Walter et al., 2013) extract patterns from paths in dependency trees. One exception is (Perez-Beltrachini and Lapata, 2018), the induced alignments are used to guide the generation. Our work takes a step further to also induce alignments for text spans not supported by the noisy paired input with possible semantics. Our work is also related to previous work on extracting information from user queries with the backend data structure. Most of these approaches contain two steps. Initially, a separate model"
2020.coling-main.272,D07-1087,0,0.0275143,"s (Perez-Beltrachini and Lapata, 2018), the induced alignments are used to guide the generation. Our work takes a step further to also induce alignments for text spans not supported by the noisy paired input with possible semantics. Our work is also related to previous work on extracting information from user queries with the backend data structure. Most of these approaches contain two steps. Initially, a separate model is applied to match the unstructured texts with relevant input records and then an extraction model is learned based on collected annotations. (Agichtein and Ganti, 2004) and (Canisius and Sporleder, 2007) train a language model on data records to identify related text spans in book description. Several approaches train a CRF based extractor to detect the related text spans (Michelson and Knoblock, 2008; Li et al., 2009). (Bellare and McCallum, 2009) apply a generalized expectation criteria to learn alignments between database and the texts, and train the information extractor to induce semantic annotations for text spans. Compared to these work, our approach is an unified neural based alignment model which avoids the error propagation of each step. 6 Conclusion In this paper, we study the prob"
2020.coling-main.272,N19-1423,0,0.0502921,"Missing"
2020.coling-main.272,W19-8652,0,0.0874565,"nvestigate different ways of incorporating local model with the sequence labeling framework. A straight forward way is to train a LSTM-CRF model with the alignments produced by the local models, which referred as Local CRF in Table 2. L2GA outperforms Local CRF by dynamically integrating the alignment results provided by the local model and avoiding label noise for training, therefore achieves better result. 4.3.2 How can Alignment Help Generation? In this section, we provide an extrinsic evaluation by testing whether alignments can be beneficial to neural generation. Inspired by recent work (Dusek et al., 2019) on removing semantically inequivalent 4 127 out of 630 in E2E and 71 out of 100 in Computer testsets are recognized as semantically inequivalent data-text pairs. 3056 Meaning Representation (MR) Name Rating EatType Price The Cricketers high £20-25 All Bar One restaurant Near Food Area FamilyFriendly English riverside no Food Name Near The Cricketers is a kid friendly restaurant that serves English food near All Bar One in the MIL: Area Rating Price riverside area . It has a price range of 20-25 pounds and is a highly rated restaurant . Name Modified LSTMCRF: FamilyFriendly EatType Food Near T"
2020.coling-main.272,W17-5224,0,0.0234306,"Missing"
2020.coling-main.272,D17-1206,0,0.0765698,"Missing"
2020.coling-main.272,N18-1014,0,0.0146768,"lignments (e.g., EatType). For slots with string values (e.g., Name), the slot-value pairs (e.g., Name:Golden Palace) are recovered by recognizing text spans (e.g., Golden Palace) as slot values. For slots with categorical values (e.g., Price), we retrieve a most frequent slot-value pair (e.g., Price:high) in the training corpus with the same text span (e.g., expensive). In this way, a refined training corpus is produced. We use the new training corpus to train a sequence-to-sequence (S2S) generation model. To evaluate the correctness of generation, a well-crafted rule-based aligner built by (Juraska et al., 2018) is adopted to approximately reflect the semantic correctness. The error rate is calculated by matching the slot values in output texts containing missing or conflict slots in the realization given its input MR. The generation results are shown in Table 3. Vanilla S2S model trained on semantically inequivalent data-text pairs performs poorly in semantic correctness. After training on the corpus refined by our proposed L2GA method, S2S model can reduce the inconsistent errors in a large margin. Note that (Dusek et al., 2019) clean noise in training corpus by rich heuristic rules, L2GA refines t"
2020.coling-main.272,N16-1030,0,0.0289447,"t is either not directly labeled using string matching or not represented in its paired MR; 3) it is marked as a non-entity1 . Therefore, we change the sequence paths in CRF layer to allow inducing alignments for words with unknown types. Moreover, local model provides alignment clues for text spans. Some of them are ignored by string heuristics but are semantically relevant to one slot in paired MRs (e.g., affordable is relevant to Price in Fig. 2). These alignments are treated as a soft memory to guide the CRF layer. Modified LSTM-CRF: In conventional LSTM-CRF based sequence labeling model (Lample et al., 2016), given the text description X = {xt }Tt=1 and the pseudo labels Y = {yt }Tt=1 . We first obtain contextual representations U for words in description X using the Eq. 3, and context vector ut for word xt is decoded by a linear layer Wc into the label space to compute the score Pt,yt for label yt . Pt = Wc ut (6) On top of the model, a CRF (Lafferty et al., 2001) layer is applied to capture the dependencies among predicted labels. We define the score of the predicted sequence (y1 , ..., yT ) as: s(X, Y ) = T X Φyt ,yt +1 + t=0 1 T X Pt,yt (7) t=1 Note that all stopwords without exact string mat"
2020.coling-main.272,P09-1011,0,0.0495261,"d from parallel structured data and descriptions in websites. Some of the collected instances inevitably contain noise where input structured data are semantically inequivalent with associated texts (Perez-Beltrachini and Gardent, 2017; Nie et al., 2019). Fig. 1 depicts an example. The slot-value pair Rating:low in the input meaning representation (MR) is contradicted with the text span highly recommended in descriptions, while the text span restaurant in descriptions should refer to a slot-value pair EatType:restaurant, which is not in the paired MR. Previous work (Barzilay and Lapata, 2005; Liang et al., 2009; Perez-Beltrachini and Lapata, 2018) aligns text spans in descriptions by only focusing on building semantic correspondences with the paired MR. However, in the scenario of semantically inequivalent MR-text pairs, due to the noise of input, there is not sufficient information to induce alignments for text spans such as restaurant by merely looking at its current input. To tackle the challenge of inducing correspondences (i.e., alignments) from semantically inequivalent data text pairs, we propose a local-to-global Alignment (L2GA) framework. This framework is composed of a local and a global"
2020.coling-main.272,P19-1256,1,0.810772,"respondences indicate which subset of input data is verbalized in the description texts (what to say) and how data are described in natural language (how to say) (Angeli et al., 2010; Perez-Beltrachini and Lapata, 2018). Recent neural methods for data-to-text generation require using large-scale training corpus. These datasets are often automatically constructed from parallel structured data and descriptions in websites. Some of the collected instances inevitably contain noise where input structured data are semantically inequivalent with associated texts (Perez-Beltrachini and Gardent, 2017; Nie et al., 2019). Fig. 1 depicts an example. The slot-value pair Rating:low in the input meaning representation (MR) is contradicted with the text span highly recommended in descriptions, while the text span restaurant in descriptions should refer to a slot-value pair EatType:restaurant, which is not in the paired MR. Previous work (Barzilay and Lapata, 2005; Liang et al., 2009; Perez-Beltrachini and Lapata, 2018) aligns text spans in descriptions by only focusing on building semantic correspondences with the paired MR. However, in the scenario of semantically inequivalent MR-text pairs, due to the noise of i"
2020.coling-main.272,D17-1238,0,0.0297108,"Missing"
2020.coling-main.272,W17-5525,0,0.0539135,"Missing"
2020.coling-main.272,D14-1162,0,0.090663,"Missing"
2020.coling-main.272,W17-3537,0,0.0200419,"in data-to-text generation, these correspondences indicate which subset of input data is verbalized in the description texts (what to say) and how data are described in natural language (how to say) (Angeli et al., 2010; Perez-Beltrachini and Lapata, 2018). Recent neural methods for data-to-text generation require using large-scale training corpus. These datasets are often automatically constructed from parallel structured data and descriptions in websites. Some of the collected instances inevitably contain noise where input structured data are semantically inequivalent with associated texts (Perez-Beltrachini and Gardent, 2017; Nie et al., 2019). Fig. 1 depicts an example. The slot-value pair Rating:low in the input meaning representation (MR) is contradicted with the text span highly recommended in descriptions, while the text span restaurant in descriptions should refer to a slot-value pair EatType:restaurant, which is not in the paired MR. Previous work (Barzilay and Lapata, 2005; Liang et al., 2009; Perez-Beltrachini and Lapata, 2018) aligns text spans in descriptions by only focusing on building semantic correspondences with the paired MR. However, in the scenario of semantically inequivalent MR-text pairs, du"
2020.coling-main.272,N18-1137,0,0.371845,"ondences in the same framework. Experimental results show that our proposed method can be generalized to both restaurant and computer domains and improve the alignment accuracy. 1 Introduction Discovering semantic correspondences between structured input (e.g., a set of slot-value pairs) and associated descriptions is a central step for many downstream NLP tasks. For example, in data-to-text generation, these correspondences indicate which subset of input data is verbalized in the description texts (what to say) and how data are described in natural language (how to say) (Angeli et al., 2010; Perez-Beltrachini and Lapata, 2018). Recent neural methods for data-to-text generation require using large-scale training corpus. These datasets are often automatically constructed from parallel structured data and descriptions in websites. Some of the collected instances inevitably contain noise where input structured data are semantically inequivalent with associated texts (Perez-Beltrachini and Gardent, 2017; Nie et al., 2019). Fig. 1 depicts an example. The slot-value pair Rating:low in the input meaning representation (MR) is contradicted with the text span highly recommended in descriptions, while the text span restaurant"
2020.coling-main.272,D18-1230,0,0.015348,"lobal alignment model is based on a CRF module which is capable of leveraging dependencies among alignments. Compared with the conventional sequence labeling, our scenario differs in two aspects. First, we employ a weak supervision method to obtain the training labels for sequence labeling. Second, the alignment supervision provided by the local model is leveraged for sequence labeling. To provide labels for training a CRF module, we first generate pseudo labels for words in texts by exact string matching, where conflicted matches are resolved by maximizing the total number of matched tokens (Shang et al., 2018). Based on the result of dictionary matching, each word falls into one of three categories: 1) it belongs to an entity mention with one slot in its paired MR; 2) it belongs to an (unknown) entity where its slot is either not directly labeled using string matching or not represented in its paired MR; 3) it is marked as a non-entity1 . Therefore, we change the sequence paths in CRF layer to allow inducing alignments for words with unknown types. Moreover, local model provides alignment clues for text spans. Some of them are ignored by string heuristics but are semantically relevant to one slot i"
2020.coling-main.272,I17-2032,1,0.850531,"provided in a data-text pair. Pseudo alignment labels are constructed using string matching heuristic between words and slots (e.g., Golden Palace is aligned with slot Name in Fig. 1), which left large portion of unmatched text spans (e.g., low price and restaurant cannot be directly matched in Fig. 1). In order to infer the alignments of unmatched words, we modify the calculation of sequence probability in the memory-guided CRF module by accumulating the probabilities over all possible sequential labels. We conduct experiments on datasets in restaurant (Novikova et al., 2017a) and computer (Wang et al., 2017) domains to evaluate our proposed method. Experimental results show that our proposed method can improve the alignment accuracy and the effectiveness of introducing global alignment model for detecting noise presented in the original training corpus. In summary, our contributions are: 1) we propose to learn semantic correspondences for loosely corresponded data text pairs with both local and global supervisions; 2) we propose a local-to-global framework which not only induces semantic correspondences for words that are related to its paired input but also infers potential labels for text spans"
2020.coling-main.272,P18-1135,0,0.0202319,"ption X, then the alignment objectives to acquire the alignments for words in text X. MR Encoder: A slot-value pair r in MR can be treated as a short sequence w1 , . . . , wn by concatenating words in its slot and value. The word sequence is first represented as a sequence of word embedding vectors (v1 , . . . , vn ) using a pre-trained word embedding matrix Ew , and then passed through a bidirectional LSTM layer to yield the contextualized representations H = (h1 , . . . , hn ). To produce a summary context vector over variable-length sequences, we adopt the same self-attention structure in (Zhong et al., 2018) to obtain the vector of slot-value pair c. X c= βi hi ; β = softmax(Ws H) (1) i where Ws is a trainable parameter Ws and β is the learned importance. We also embed each slot si into a slot vector as zi = Ez (si ) where Ez is a trainable slot embedding matrix. 3052 (2) Sentence Encoder: For description X = (x1 , . . . , xT ), each word xt is first embeded into vector et by concatenating the word embedding and character-level representation generated with a group of convolutional neural network (CNNs). Then we feed the word vectors e1 , ..., eT to a bidirectional LSTM to obtain contextualized v"
2021.acl-demo.39,2020.emnlp-main.550,0,0.0262943,"tion family, which is fast during the generation process. Besides, we retrieve entities and relevant schema items (relations and types) in parallel by leveraging the recent advance of entity linking (Orr et al., 2021) and dense retrieval (Wu 5 http://github.com/nju-websoft/SPARQA http://github.com/lanyunshi/Multi-hopComplexKBQA 7 http://github.com/OceanskySun/GraftNet 8 https://github.com/scottyih/STAGG 9 https://github.com/guoday/Dialog-to-Action 10 https://github.com/dongpobeyond/Seq2Act 11 This module is implemented in our codebase. The detailed analysis is in the Appendix. 6 et al., 2020; Karpukhin et al., 2020). Our system can interact with users timely, demonstrating the efficiency of the proposed ReTraCk framework. • Effectiveness ReTraCk is designed to enhance the controllability of transduction-based methods in both syntax level and semantic level. It first employs a grammar based decoder (Yin and Neubig, 2018) to guarantee the syntax correctness. Then it leverages a checker to alleviate the semantic inconsistency issues. Inspired by previous work, four checking mechanisms are proposed and implemented in the checker: instancelevel checking (Liang et al., 2017), ontologylevel checking (Chen et al"
2021.acl-demo.39,2020.acl-main.91,0,0.272512,"utational Linguistics sequence of tree-constructing actions under grammar specification. However, T RANX does not include the essential retriever components used in grounding, and thus does not support KBQA by now. On the other hand, recent neural semantic parsing methods mostly emphasize performance on benchmark datasets while neglecting the efficiency (speed) dimension. This limits the understanding of how designed approaches fit into real applications. For example, the popular query graph generation methods generate and rank a set of query graphs (Yih et al., 2015; Maheshwari et al., 2019; Lan and Jiang, 2020). Since all query graph candidates keep in line with the knowledge base (KB) structure, these methods take full advantage of the KB. However, they suffer from poor efficiency due to the large number of candidates and heavily querying on KB. To verify that, we performed a preliminary study on available SOTA models5,6,7,8,9,10 . According to our study, these models either have difficulties in supporting interactive online services, or limit the candidate space for specific datasets, which makes them difficult to apply in practice. To this end, we present ReTraCk, a practical framework for large"
2021.acl-demo.39,P17-1003,0,0.331979,"ng that aims to satisfy users’ information needs based on factual information stored in knowledge bases. Over the years, it has attracted a great deal of research attention from academia and industry. Early KBQA systems are generally rule-based. They rely on predefined rules or templates to parse questions into logical forms (Cabrio et al., 2012; Abujabal et al., 2017), suffering from coverage and scalability problems. Recently, researchers usually focus more on neural semantic parsing approaches. These data-driven parsing methods (Yih et al., 2015; Jia and Liang, 2016; Dong and Lapata, 2016; Liang et al., 2017; Gu et al., 2021) significantly improve the state-of-the-art (SOTA) performance on KBQA tasks. ∗ The first three authors contributed equally. This work was conducted during Shuang and Qian’s internship at Microsoft Research Asia. 1 https://dki-lab.github.io/GrailQA/ 2 https://aka.ms/ReTraCk Although various neural semantic parsing methods have been proposed for KBQA, there are few works investigating how to leverage the advantages of SOTA models to build a comprehensive system, and how to fit the system with practical application purpose (e.g., balancing effectiveness and efficiency). To inve"
2021.acl-demo.39,D18-1298,0,0.0448133,"Missing"
2021.acl-demo.39,D14-1162,0,0.0863688,"Missing"
2021.acl-demo.39,2020.acl-main.412,0,0.0135466,"Ve + T RANSDUCTION (Gu et al., 2021) 17.6 18.4 50.5 51.6 BERT + T RANSDUCTION (Gu et al., 2021) 33.3 36.8 51.8 53.9 QGG (Lan and Jiang, 2020) GloVe + R ANKING (Gu et al., 2021) BERT + R ANKING (Gu et al., 2021) 58.1 41.6 Ours – Checker 65.3 44.2 84.4 73.2 87.5 74.5 Zero-shot EM F1 EM F1 − 40.0 45.5 33.0 47.8 53.9 − 28.9 48.6 36.6 33.8 55.7 16.4 31.0 18.5 36.0 3.0 25.7 3.1 29.3 61.5 43.4 70.9 48.3 44.6 26.2 52.5 28.4 Table 2: EM and F1 results on the hidden test set of GrailQA. Method ∗ Ours – Checker Ours – Checker type items and top-500 relation items. F1 Hits@1 IR-based methods EmbedKGQA∗♥ (Saxena et al., 2020) − EmbedKGQA∗ (Saxena et al., 2020) − PullNet∗ (Sun et al., 2019) − GRAFT-Net∗ (Sun et al., 2018) 62.8 Query-Graph Generation methods GrailQA R ANKING∗♥ (Gu et al., 2021) 67.0 STAGG♥ (Yih et al., 2015) 69.0 Topic Units♥ (Lan et al., 2019) 67.9 TextRay♥ (Bhutani et al., 2019) 60.3 QGG♥ (Lan and Jiang, 2020) 74.0 UHop (Chen et al., 2019) 68.5 Transduction-based methods NSM♥ (Liang et al., 2017) 69.0 74.7 62.0 71.0 56.9 3.3 72.5 66.6 68.1 67.8 We compare our model with previous state-of-theart models on GrailQA (Lan and Jiang, 2020; Gu et al., 2021) and WebQSP (Liang et al., 2017; Sun et al., 201"
2021.acl-demo.39,D19-1242,0,0.0122718,"NSDUCTION (Gu et al., 2021) 33.3 36.8 51.8 53.9 QGG (Lan and Jiang, 2020) GloVe + R ANKING (Gu et al., 2021) BERT + R ANKING (Gu et al., 2021) 58.1 41.6 Ours – Checker 65.3 44.2 84.4 73.2 87.5 74.5 Zero-shot EM F1 EM F1 − 40.0 45.5 33.0 47.8 53.9 − 28.9 48.6 36.6 33.8 55.7 16.4 31.0 18.5 36.0 3.0 25.7 3.1 29.3 61.5 43.4 70.9 48.3 44.6 26.2 52.5 28.4 Table 2: EM and F1 results on the hidden test set of GrailQA. Method ∗ Ours – Checker Ours – Checker type items and top-500 relation items. F1 Hits@1 IR-based methods EmbedKGQA∗♥ (Saxena et al., 2020) − EmbedKGQA∗ (Saxena et al., 2020) − PullNet∗ (Sun et al., 2019) − GRAFT-Net∗ (Sun et al., 2018) 62.8 Query-Graph Generation methods GrailQA R ANKING∗♥ (Gu et al., 2021) 67.0 STAGG♥ (Yih et al., 2015) 69.0 Topic Units♥ (Lan et al., 2019) 67.9 TextRay♥ (Bhutani et al., 2019) 60.3 QGG♥ (Lan and Jiang, 2020) 74.0 UHop (Chen et al., 2019) 68.5 Transduction-based methods NSM♥ (Liang et al., 2017) 69.0 74.7 62.0 71.0 56.9 3.3 72.5 66.6 68.1 67.8 We compare our model with previous state-of-theart models on GrailQA (Lan and Jiang, 2020; Gu et al., 2021) and WebQSP (Liang et al., 2017; Sun et al., 2019; Saxena et al., 2020; Lan and Jiang, 2020). Notably, both T RAN"
2021.acl-demo.39,D18-1455,0,0.0450885,"Missing"
2021.acl-demo.39,J84-3009,0,0.644812,"Missing"
2021.acl-demo.39,2020.emnlp-main.519,0,0.0192611,"disambiguation, we implement a prior baseline which selects the most popular entity based on the prior score. Besides, we also implement an alternative model by leveraging B OOTLEG (Orr et al., 2021) enriched with the prior features12 . Due to space limitations, the model details and its comparison with the entity linker used in Gu et al. (2021) are put in the Appendix. Schema Retriever As schema items are not always mentioned explicitly in the question and their vocabularies are much fewer than entities13 , we leverage the dense retriever framework (Mazar´e et al., 2018; Humeau et al., 2020; Wu et al., 2020) 12 In our demo system, we choose the prior baseline method since it is more memory efficient than the B OOTLEG (Orr et al., 2021) method. 13 In the latest version of Freebase, there are more than 120 million entities, 16k types and 20k relations. to obtain the related types and relations. To be specific, we train a bi-encoder architecture (Wu et al., 2020) such that related schema items are close to the question embedding. This architecture allows for fast real-time inference, as it is able to cache the encoded candidates. We use two independent BERT-base encoders (Devlin et al., 2019) to rep"
2021.acl-demo.39,P15-1128,0,0.58475,"ering (KBQA) is an important task in natural language processing that aims to satisfy users’ information needs based on factual information stored in knowledge bases. Over the years, it has attracted a great deal of research attention from academia and industry. Early KBQA systems are generally rule-based. They rely on predefined rules or templates to parse questions into logical forms (Cabrio et al., 2012; Abujabal et al., 2017), suffering from coverage and scalability problems. Recently, researchers usually focus more on neural semantic parsing approaches. These data-driven parsing methods (Yih et al., 2015; Jia and Liang, 2016; Dong and Lapata, 2016; Liang et al., 2017; Gu et al., 2021) significantly improve the state-of-the-art (SOTA) performance on KBQA tasks. ∗ The first three authors contributed equally. This work was conducted during Shuang and Qian’s internship at Microsoft Research Asia. 1 https://dki-lab.github.io/GrailQA/ 2 https://aka.ms/ReTraCk Although various neural semantic parsing methods have been proposed for KBQA, there are few works investigating how to leverage the advantages of SOTA models to build a comprehensive system, and how to fit the system with practical application"
2021.acl-demo.39,P16-2033,0,0.0216962,"Missing"
2021.acl-demo.39,D18-2002,0,0.0932527,"ages of SOTA models to build a comprehensive system, and how to fit the system with practical application purpose (e.g., balancing effectiveness and efficiency). To investigate, we identify two key issues hindering the development of KBQA systems. On the one hand, there is a lack of a generic and extensible framework for KBQA. For example, the popular S EMPRE3 toolkit (Berant et al., 2013) provides infrastructures to develop statistical semantic parsers for KBQA with rich features, but its performance and scalability are inferior to recent neural semantic parsing methods. The T RANX toolkit4 (Yin and Neubig, 2018) employs a transition-based neural semantic parser to model the logical form generation procedure as a 3 4 https://github.com/percyliang/sempre https://github.com/pcyin/tranX 325 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 325–336, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics sequence of tree-constructing actions under grammar specification. However, T RANX does not include the essential re"
2021.acl-short.99,D19-1418,0,0.0282943,"6.9 3.8 49.1 44.2 72.1 69.6 56.2 51.7 89.1 77.4 Table 3: Results of five runs for training BERT on MNLI with model selection via target domain dev set SST-2 -5.1 / 67.2 -2.0 / 77.5 -7.2 / 64.6 +1.6 / 54.8 - / 10.0 +0.4 / 82.7 +0.3 / 90.0 +0.5 / 90.6 +2.7 / 84.9 - / 25.0 -13.5 / 75.9 +0.6 / 94.5 -9.5 / 82.3 -6.4 / 84.4 - / 50.0 training. We experiment with three schemes on the MNLI data to see whether they could lead to better generalization of zero-shot classification: (1) Data augmentation with syntactic transformations (Min et al., 2020)8 , denoted as DA, (2) Instance reweighting following Clark et al. (2019) that reweights each example with one minus the probability a bias-only model assigns the correct label, denoted as RW, and (3) The bias product method (Clark et al., 2019) that ensembles a bias-only model via a product of experts, denoted as BP, which is essentially the same as its concurrent work via fitting the residual of the biased models (He et al., 2019). There exist additional solutions with richer details such as multi-task learning (Tu et al., 2020) where proper auxiliary tasks could be identified to improve robustness. We plan to explore more in this line in our more extensive futur"
2021.acl-short.99,P19-1554,0,0.022524,"ining to detect pairwise coherence. In this way, NSP could serve as a non-trivial, strong alternative baseline for zero-shot text classification scenarios where the target labels are semantically more concrete (e.g., topics) or more frequently appeared (e.g., words expressing sentiment). In such scenarios, fine-tuning on limited NLI data could weaken the semantic coherence acquired from the raw BERT pre-trained on generic-domain corpora, especially now that fine-tuned models have utilized many spurious lexical cooccurrence features as shown in many similar sentence pair classification models (Feng et al., 2019; Niven and Kao, 2019), possibly due to the inherent lexical bias from the current NLI datasets collected from crowd workers. 6 Readers who are curious about more details on this problem can refer to our qualitative analysis in the Appendix which could hopefully help establish 6 Some readers might guess that other NLI datasets collected via a more careful process (Jiang and de Marneffe, 2019; Eisenschlos et al., 2021) might partially mitigate the bias appearing from crowdsourced annotation, but this does not mean that such better intended datasets can be free from statistically biased lexical"
2021.acl-short.99,N19-1423,0,0.0174185,"al., 2016; Raffel et al., 2020, inter alia). Text classification is then reduced to textual entailment by setting ∗ Work during internship at Microsoft Research Asia. the input sentence as the premise and simultaneously casting the candidate label into a hypothesis sentence using pre-defined templates or lexical definitions from WordNet. Once we have any pretrained NLI models at hand, zero-shot text classification under any specified label space is enabled for free without the need to collect annotated data. With contextualized representation based on pretrained language models such as BERT (Devlin et al., 2019), NLI performance has been drastically improved. Promising empirical results have been shown on various text classification benchmarks that vary across topic classification, emotion classification, and situation classification, outperforming earlier standard approaches (Chang et al., 2008) or simple scoring schemes derived from distributional similarity (Mikolov et al., 2013). However, such generality is conceptually contradictory with the specificity of text classification in many practical scenarios. In this opinion piece, we conduct extended analysis on the recent attempts (Yin et al., 2019"
2021.acl-short.99,2021.naacl-main.71,0,0.0273999,"urious about more details on this problem can refer to our qualitative analysis in the Appendix which could hopefully help establish 6 Some readers might guess that other NLI datasets collected via a more careful process (Jiang and de Marneffe, 2019; Eisenschlos et al., 2021) might partially mitigate the bias appearing from crowdsourced annotation, but this does not mean that such better intended datasets can be free from statistically biased lexical distributions with coincidental cooccurrences that could be utilized by our strong data-fitting models during fine-tuning (Geirhos et al., 2020; Du et al., 2021). Our additional results described in the Appendix do not seem to be promising on this direction towards better NLI data. a slightly better sense on the behavioral difference introduced by NLI fine-tuning. On the other hand, fine-tuning on NLI data might seem to be marginally helpful for more abstract cases such as emotion and situation typing, but the performance metrics are in fact pathetically disappointing across all systems. 2.2.2 How stable are these NLI models? Apart from the obvious difference caused by different training data, there underlies a more serious concern: the discrepancy be"
2021.acl-short.99,2021.naacl-main.32,0,0.061666,"Missing"
2021.acl-short.99,2021.acl-long.295,0,0.0187162,"use the English news data from (Zhang et al., 2015) that consists of four types of articles: World, Sports, Business, Sci/Tech. SST-2 : The Stanford Sentiment Treebank dataset3 processed by Socher et al. (2013) for sentiment polarity classification with binary labels (positive and negative). 1 Another reason for not studying on this setting is that the split of development set and test set in (Yin et al., 2019) contain the same label space, which is flawed to be used for any claim on the performance of “unseen labels”. 2 https://github.com/snipsco/snips-nlu 3 For SST-2 we follow Zhang et al. (2021) and Gao et al. (2021) to use the development set from GLUE for testing. • ESA: Representing the text and label in the Wikipedia concept vector space. Using the implementation5 from Chang et al. (2008). Moreover, due to the obvious variance in performance for models trained on different NLI datasets, we are also tempted to check how much the performance might degrade when given no NLI data at all for fine-tuning. This corresponds to naively using a raw BERT model which has been pre-trained for next sentence prediction (NSP). For consistency, we use the same premises and hypotheses as the deleg"
2021.acl-short.99,2020.blackboxnlp-1.16,0,0.0274167,"alization performance as measured by the early-stopping dev set performance. Results are listed in Table 2, where the absolute differences between the worst and the best are large, especially on classifying topic or intent. We observe even worse trends on other smaller NLI datasets (see Appendix). These results are consistent with recent studies within the scope of NLI reporting that BERT instances which achieve similar performance metrics on standard NLI datasets could have huge variance in out-ofdistribution generalization or linguistic stress testing (McCoy et al., 2020; Zhou et al., 2020; Geiger et al., 2020), while providing another instance of the underspecification problem in modern machine learning (D’Amour et al., 2020). As a verification, we also try to tune the models for different development sets that better characterize the generalization behavior for zero-shot 788 Dataset MNLI dev set Yahoo Emotion Situation AGNews SST-2 Snips Average Std Min Max Model 90.5 39.0 18.1 16.2 63.7 68.6 74.1 0.3 10.5 2.0 1.5 11.0 2.0 3.9 90.0 26.9 15.7 14.5 50.0 66.1 68.4 90.8 50.2 20.5 18.7 77.7 70.9 77.6 NSP(Reverse) RTE FEVER MNLI Random classification. We reorganize the splitted development set and the t"
2021.acl-short.99,D18-1352,0,0.0257113,", and currently more robust NLI methods might not help. Our observations reveal implicit but massive difficulties in building a usable zero-shot text classifier based on text entailment models. Given the difficulty of NLI data collection that aims at out-ofdomain generalization or transfer learning (Bowman et al., 2020), we question the feasibility of this setup in the current progress of language technology. Before significant progress in language understanding and reasoning, it seems more promising to consider alternative schemes built on explicit external knowledge (Zellers and Choi, 2017; Rios and Kavuluru, 2018; Zhang et al., 2019) or more crafted usage of pre-trained models that hopefully have captured more comprehensive semantic coverage and better compositionality from large corpora or grounded texts (Meng et al., 2020; Brown et al., 2020; Radford et al., 2021). This study also implies the huge difficulty for benchmarking zero-shot text classification without any further restriction on the task setting. The three datasets used by Yin et al. (2019) were originally intended for diverse coverage but are not sufficient to draw consistent conclusions as we have shown. We suggest future studies on zero"
2021.acl-short.99,2021.acl-long.170,0,0.0320111,"e three datasets used by Yin et al. (2019) were originally intended for diverse coverage but are not sufficient to draw consistent conclusions as we have shown. We suggest future studies on zero-shot text classification either conduct experiments over even more diverse classification scenarios to verify any claimed generality, or directly focus on more specific task settings and verify claims within a smaller but clearer scope such as zero-shot intent classification or zero-shot situation typing for more reliable results with less instability, and perhaps based on more carefully curated data (Rogers, 2021). 790 References Laura-Ana-Maria Bostan and Roman Klinger. 2018. An analysis of annotated corpora for emotion classification in text. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2104–2119, Santa Fe, New Mexico, USA. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal. Samuel R. Bowman, Jennimaria Palomaki, Livio Baldini Soares, and"
2021.acl-short.99,N18-1179,0,0.025859,"l., 2019) that ensembles a bias-only model via a product of experts, denoted as BP, which is essentially the same as its concurrent work via fitting the residual of the biased models (He et al., 2019). There exist additional solutions with richer details such as multi-task learning (Tu et al., 2020) where proper auxiliary tasks could be identified to improve robustness. We plan to explore more in this line in our more extensive future study. 2.2.3 Is more robust NLI helpful? Previous studies have raised concerns on that the current NLI models heavily rely on spurious lexical overlap patterns (Sanchez et al., 2018; Naik et al., 2018; McCoy et al., 2019, inter alia). For analytical purposes, we randomly permute the tokens of each input instance to see how much the predictions might change. Results shown in Table 4 suggest that shuffling the input tokens does not affect the model performance by much, which is consistent with similar recent findings (Gupta et al., 2021; Sinha et al., 2021). This reveals a concern that all these models might just predict with shallow lexical patterns that may not be robust for more semantically abstractive input instances. There have been a few recent attempts trying to re"
2021.acl-short.99,2021.acl-long.569,0,0.0415985,"to explore more in this line in our more extensive future study. 2.2.3 Is more robust NLI helpful? Previous studies have raised concerns on that the current NLI models heavily rely on spurious lexical overlap patterns (Sanchez et al., 2018; Naik et al., 2018; McCoy et al., 2019, inter alia). For analytical purposes, we randomly permute the tokens of each input instance to see how much the predictions might change. Results shown in Table 4 suggest that shuffling the input tokens does not affect the model performance by much, which is consistent with similar recent findings (Gupta et al., 2021; Sinha et al., 2021). This reveals a concern that all these models might just predict with shallow lexical patterns that may not be robust for more semantically abstractive input instances. There have been a few recent attempts trying to remove the shallow overlap bias for NLI model 7 AGNews Table 4: Results of shuffling perturbation. In each cell: the change of accuracy after input shuffling, followed by the percentage of examples where the predictions do not change. All these results are reported as the average score of five different random shuffles. Table 2: Results of five runs of BERT fine-tuned on MNLI and"
2021.acl-short.99,D17-1099,0,0.0121989,"d by text classification, and currently more robust NLI methods might not help. Our observations reveal implicit but massive difficulties in building a usable zero-shot text classifier based on text entailment models. Given the difficulty of NLI data collection that aims at out-ofdomain generalization or transfer learning (Bowman et al., 2020), we question the feasibility of this setup in the current progress of language technology. Before significant progress in language understanding and reasoning, it seems more promising to consider alternative schemes built on explicit external knowledge (Zellers and Choi, 2017; Rios and Kavuluru, 2018; Zhang et al., 2019) or more crafted usage of pre-trained models that hopefully have captured more comprehensive semantic coverage and better compositionality from large corpora or grounded texts (Meng et al., 2020; Brown et al., 2020; Radford et al., 2021). This study also implies the huge difficulty for benchmarking zero-shot text classification without any further restriction on the task setting. The three datasets used by Yin et al. (2019) were originally intended for diverse coverage but are not sufficient to draw consistent conclusions as we have shown. We sugge"
C00-1072,J90-1003,0,0.0318813,"l systems in a series of annual comparisons. This data set contains essential text fragments (phrases, clauses, and sentences) which must be included in summaries to answer some TREC topics. These fragments are each judged by a human judge. As described in Section 3, SUMMARIST employs several independent modules to assign a score to each sentence, and then combines the scores to decide which sentences to extract from the input text. One can gauge the eÆcacy 6 The mutual information is de ned according to chapter 2 of (Cover and Thomas, 1991) and is not the pairwise mutual information used in (Church and Hanks, 1990). TREC Topic Description hnumi Number: 151 htitlei Topic: Coping with overcrowded prisons hdesci Description: The document will provide information on jail and prison overcrowding and how inmates are forced to cope with those conditions; or it will reveal plans to relieve the overcrowded condition. narr Narrative: A relevant document will describe scenes of overcrowding that have become all too common in jails and prisons around the country. The document will identify how inmates are forced to cope with those overcrowded conditions, and/or what the Correctional System is doing, or planning to"
C00-1072,J93-1003,0,0.022173,"elated terms. Each ti is an term highly correlated to topic with association weight wi . The number of related terms n can be set empirically according to a cuto associated weight. We describe how to acquire related terms and their associated weights in the next section. 4.1 Signature Term Extraction and Weight Estimation On the assumption that semantically related terms tend to co-occur, one can construct topic signatures from preclassi ed text using the 2 test, mutual information, or other standard statistic tests and information-theoretic measures. Instead of 2 , we use likelihood ratio (Dunning, 1993) , since  is more appropriate for sparse data than 2 test and the quantity 2log is asymptotically 2 distributed5 . Therefore, we can determine the con dence level for a speci c 2log value by looking up 2 distribution table and use the value to select an appropriate cuto associated weight. We have documents preclassi ed into a set R of ~ of nonrelevant texts for a relevant texts and a set R given topic. Assuming the following two hypotheses: Hypothesis 1 (H1 ): P (Rjti ) = p = P (Rjt~i ), i.e. the relevancy of a document is independent of ti . Hypothesis 2 (H2 ): P (Rjti ) = p1 6= p2 = P"
C00-1072,J97-1003,0,0.609167,"ant (central) topics of the texts. SUMMARIST uses positional importance, topic signature, and term frequency. Importance based on discourse structure will be added later. This is the most developed stage in SUMMARIST. Topic Interpretation: To fuse concepts such as waiter, menu, and food into one generalized concept restaurant, we need more than the simple word aggregation used in traditional information retrieval. We have investigated concept 3 We would like to use only the relevant parts of documents to generate topic signatures in the future. Text segmentation algorithms such as TextTiling (Hearst, 1997) can be used to nd subtopic segments in text. ABCNEWS.com : Delay in Handing Flight 990 Probe to FBI NTSB Chairman James Hall says Egyptian officials want to review results of the investigation into the crash of EgyptAir Flight 990 before the case is turned over to the FBI. Nov. 16 - U.S. investigators appear to be leaning more than ever toward the possibility that one of the co-pilots of EgyptAir Flight 990 may have deliberately crashed the plane last month, killing all 217 people on board. However, U.S. officials say the National Transportation Safety Board will delay transferring the invest"
C00-1072,W97-0704,1,\N,Missing
C02-1026,A00-1041,0,0.141359,"Missing"
C02-1026,W01-1203,0,0.0496782,"learning how well a system places a correct answer within the five responses regardless of its rank. We called this percent of correctness in the top 5 (PCT5). PCT5 is a precision related metric and indicates the upper bound that a system can achieve if it always places the correct answer as its first response. 3 Webclopedia: An Automated Question Answering System Webclopedia’s architecture follows the principle outlined in Section 1. We briefly describe each stage in the following. Please refer to (Hovy et al. 2002) for more detail. (1) Question Analysis: We used an in-house parser, CONTEX (Hermjakob 2001), to parse and analyze questions and relied on BBN’s IdentiFinder (Bikel et al., 1999) to provide basic named entity extraction capability. (2) Document Retrieval/Sentence Ranking: The IR engine MG (Witten et al. 1994) was used to return at least 500 documents using Boolean queries generated from the query formation stage. However, fewer than 500 documents may be returned when very specific queries are given. To decrease the amount of text to be processed, the documents were broken into sentences. Each sentence was scored using a formula that rewards word and phrase overlap with the question a"
C02-1026,C02-1042,1,0.83554,"Missing"
C02-1026,A00-1023,0,0.23155,"Missing"
C02-1042,A00-1041,0,0.0611023,"ord density within a fixed n-word window, to pinpoint answers. Robust though this may be, the window method is not accurate enough. In response, factoid question answering systems have evolved into two types: • Use-Knowledge: extract query words from the input question, perform IR against the source corpus, possibly segment resulting documents, identify a set of segments containing likely answers, apply a set of heuristics that each consults a different source of knowledge to score each candidate, rank them, and select the best (Harabagiu et al., 2001; Hovy et al., 2001; Srihari and Li, 2000; Abney et al., 2000). • Use-the-Web: extract query words from the question, perform IR against the web, extract likely answer-bearing sentences, canonicalize the results, and select the most frequent answer(s). Then, for justification, locate examples of the answers in the source corpus (Brill et al., 2001; Buchholz, 2001). Of course, these techniques can be combined: the popularity ratings from Use-the-Web can also be applied as a filtering criterion (Clarke et al., 2001), or the knowledge resource heuristics can filter the web results. However, simply going to the web without using further knowledge (Brill et a"
C02-1042,P97-1062,1,0.880343,"Missing"
C02-1042,W01-1203,1,0.907763,"Missing"
C02-1042,C02-1026,1,0.781267,"ry to explain what Washington is: Ex: “Later in the day, the president returned to Washington, the capital of the United States.” While WordNet’s definition Wordnet: “Washington—the capital of the United States” directly provides the answer to the matcher, it also allows the IR module to focus its search on passages containing “Washington”, “capital”, and “United States”, and the matcher to pick a good motivating passage in the source corpus. Clearly, this capability can be extended to include (definitional and other) information provided by other sources, including encyclopedias and the web (Lin 2002). 3.8 Type 8: Semantic Relation Matching So far, we have considered individual words and groups of words. But often this is insufficient to accurately score an answer. As also noted in (Buchholz, 2001), pinpointing can be improved significantly by matching semantic relations among constituents: Q: Who killed Lee Harvey Oswald? Qtargets: PROPER-PERSON & PROPER-NAME, PROPER-ORGANIZATION S1: “Belli’s clients have included Jack Ruby, who killed John F. Kennedy assassin Lee Harvey Oswald, and Jim and Tammy Bakker.” S2: “On Nov. 22, 1963, the building gained national notoriety when Lee Harvey Oswald"
C02-1042,P02-1006,1,0.686785,"bject something that in the ontology is subordinate to TEMP-QUANTIFIABLE-ABSTRACT with, as well, the word “how” paired with “warm”, “cold”, “hot”, etc., or the phrase “how many degrees” and a TEMPERATUREUNIT (as defined in the ontology). 3.3 Type 3: Surface Pattern Matching Often qtarget answers are expressed using rather stereotypical words or phrases. For example, the year of birth of a person is typically expressed using one of these phrases: <name> was born in <birthyear> <name> (<birthyear>–<deathyear>) We have developed a method to learn such patterns automatically from text on the web (Ravichandran and Hovy, 2002). We have added into the QA Typology the patterns for appropriate qtargets (qtargets with closed-list answers, such as PLANETS, require no patterns). Where some QA systems use such patterns exclusively (Soubbotin and Soubbotin, 2001) or partially (Wang et al., 2001; Lee et al., 2001), we employ them as an additional source of evidence for the answer. Preliminary results on for a range of qtargets, using the TREC-10 questions and the TREC corpus, are: Question type (qtarget) BIRTHYEAR INVENTORS DISCOVERERS DEFINITIONS WHY-FAMOUS LOCATIONS Number of questions 8 6 4 102 3 16 MRR on TREC docs 0.47"
C02-1042,A00-1023,0,0.103532,"technique, question word density within a fixed n-word window, to pinpoint answers. Robust though this may be, the window method is not accurate enough. In response, factoid question answering systems have evolved into two types: • Use-Knowledge: extract query words from the input question, perform IR against the source corpus, possibly segment resulting documents, identify a set of segments containing likely answers, apply a set of heuristics that each consults a different source of knowledge to score each candidate, rank them, and select the best (Harabagiu et al., 2001; Hovy et al., 2001; Srihari and Li, 2000; Abney et al., 2000). • Use-the-Web: extract query words from the question, perform IR against the web, extract likely answer-bearing sentences, canonicalize the results, and select the most frequent answer(s). Then, for justification, locate examples of the answers in the source corpus (Brill et al., 2001; Buchholz, 2001). Of course, these techniques can be combined: the popularity ratings from Use-the-Web can also be applied as a filtering criterion (Clarke et al., 2001), or the knowledge resource heuristics can filter the web results. However, simply going to the web without using further"
C04-1072,P04-1077,1,0.632252,"tical machine translation framework – A metric that ranks a good translation high in an nbest list could be easily integrated in a minimal error rate statistical machine translation training framework (Och 2003). The overall system performance in terms of 1 Oracles refer to the reference translations used in the evaluation procedure. generating more human like translations should also be improved. Before we demonstrate how to use ORANGE to evaluate automatic metrics, we briefly introduce three new metrics in the next section. 3 Three New Metrics ROUGE-L and ROUGE-S are described in details in Lin and Och (2004). Since these two metrics are relatively new, we provide short summaries of them in Section 3.1 and Section 3.3 respectively. ROUGE-W, an extension of ROUGE-L, is new and is explained in details in Section 3.2. 3.1 ROUGE-L: sequence Longest Common SubGiven two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length (Cormen et al. 1989). To apply LCS in machine translation evaluation, we view a translation as a sequence of words. The intuition is that the longer the LCS of two translations is, the more similar the two translations are. We p"
C04-1072,niessen-etal-2000-evaluation,1,0.635456,"Missing"
C04-1072,P03-1021,1,0.11247,"heir reference translations. Therefore, reference translations should be ranked higher than machine translations on average if a good automatic evaluation metric is used. Based on these assumptions, we propose a new automatic evaluation method for evaluation of automatic machine translation metrics as follows: Given a source sentence, its machine translations, and its reference translations, we compute the average rank of the reference translations within the combined machine and reference translation list. For example, a statistical machine translation system such as ISI’s AlTemp SMT system (Och 2003) can generate a list of n-best alternative translations given a source sentence. We compute the automatic scores for the n-best translations and their reference translations. We then rank these translations, calculate the average rank of the references in the n-best list, and compute the ratio of the average reference rank to the length of the n-best list. We call this ratio “ORANGE” (Oracle1 Ranking for Gisting Evaluation) and the smaller the ratio is, the better the automatic metric is. There are several advantages of the proposed ORANGE evaluation method: • No extra human involvement – ORAN"
C04-1072,2001.mtsummit-papers.3,0,0.0128539,"Missing"
C04-1072,2001.mtsummit-papers.68,0,0.0489947,"Missing"
C04-1072,C92-2067,0,0.0155207,"Missing"
C04-1072,2003.mtsummit-papers.32,0,0.408803,"Missing"
C04-1072,P02-1040,0,\N,Missing
C08-1063,P00-1071,0,0.0185064,"Missing"
C08-1063,H01-1069,1,\N,Missing
C08-1063,P02-1058,1,\N,Missing
C08-1063,C04-1200,0,\N,Missing
C08-1063,X98-1026,1,\N,Missing
C12-1189,W06-2911,0,0.165659,"learn the shared information from the “name"" group instead of individual instance. As shown in Figure 1, the data set M for entity linking usually has a certain number of names (e.g.“Hoffman"",“Chad Johnson"", etc.), each with some labeled instances. Then, we treat each “name” and its associated instances in M as a prediction problem of structural learning. Besides, the queried name (e.g. “AZ"" in Figure 1) with auto-labeled instances Aq is our target prediction problem, which is the problem we are aiming to solve. According to the applications of structural learning in other tasks, such as WSD (Ando, 2006), structural learning assumes that there exists a predictive structure shared by multiple related problems. In order to learn the predictive structure Θ shared by M and Aq , we need to (a) select relevant prediction problems (i.e. relevant names) from M. That is, they should share a certain predictive structure with the target problem; (b) select useful features from the feature set shown in Table 2. The relevant prediction problems may only has shared structure with target problem over certain features. In this paper, we use a set of experiments including feature split and data set M 3097 par"
C12-1189,P05-1001,0,0.150011,"e.g. feature “acronym"" is true), certain surface features effective for linking to “Communist Party of China"" may be also effective for disambiguating “NY"", and vice versa However, with the gap in other aspects between the distributions of Aq and M shown in Section 1, directly adding M to our training set will produce a lot of noise with respect to the queried name. Thus, instead of using all the distribution knowledge in M, we propose to only incorporate the shared knowledge with Aq from M into u estimation based on structural learning. The Structural Learning Algorithm. Structural learning (Ando and Zhang, 2005b) is a multitask learning algorithm that takes advantage of the low-dimensional predictive structure shared by multiple related problems. Let us assume that we have K prediction problems indexed by l ∈ {1, .., K}, each with n(l) instances (Xli , Yli ). Each Xli is a feature vector of dimension p. Let Θ be an orthonormal h×p (h is a parameter) matrix, that captures the predictive structure shared by all the 3096 K problems. Then, we decompose the weight vector ul for problem l into two parts: one part that models the distribution knowledge specific to each problem l and one part that models th"
C12-1189,S07-1012,0,0.0166509,"do not refer the company. Amigo et al. (2010) concluded that it was not viable to train separate system for each of the companies, as the system must immediately react to any imaginable company name. Thus, in this benchmark, the set of company names in the training and test corpora are different. However, the lazy learning approach proposed in this paper demonstrates that it is feasible to train separate system for each company, and the system can immediately react to any company name without manually labeling new corpora. More generally, resolving ambiguous names in Web People Search (WePS) (Artiles et al., 2007) and Cross-document Coreference (Bagga and Baldwin, 1998) disambiguates names by clustering the articles according to the entity mentioned. This differs significantly from entity linking, which has a given entity list (i.e. the KB) to which we disambiguate the mentions. 3 Candidate Generation Because the knowledge base usually contains millions of entries, it is time-consuming to apply the disambiguation algorithm to the entire knowledge base. Thus, the following pre-processing process is conducted to filter out irrelevant KB entries and select only a set of candidates that are potentially the"
C12-1189,P98-1012,0,0.143468,"d that it was not viable to train separate system for each of the companies, as the system must immediately react to any imaginable company name. Thus, in this benchmark, the set of company names in the training and test corpora are different. However, the lazy learning approach proposed in this paper demonstrates that it is feasible to train separate system for each company, and the system can immediately react to any company name without manually labeling new corpora. More generally, resolving ambiguous names in Web People Search (WePS) (Artiles et al., 2007) and Cross-document Coreference (Bagga and Baldwin, 1998) disambiguates names by clustering the articles according to the entity mentioned. This differs significantly from entity linking, which has a given entity list (i.e. the KB) to which we disambiguate the mentions. 3 Candidate Generation Because the knowledge base usually contains millions of entries, it is time-consuming to apply the disambiguation algorithm to the entire knowledge base. Thus, the following pre-processing process is conducted to filter out irrelevant KB entries and select only a set of candidates that are potentially the correct match to the given query (a query consists of a"
C12-1189,E06-1002,0,0.0256635,"we have discussed, most of the previous entity linking work (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Gottipati and Jiang, 2011; Han and Sun, 2011) fall into the traditional entity linking framework shown in Figure 1. Besides, the collaborative approach (Chen and Ji, 2011) tried to search similar queries as their query collaboration group by clustering texts. This differs from our method where we use the selective knowledge from unlabeled data. In some other work, entity linking is also called named entity disambiguation using Wikipedia (Bunescu and Pasca, 2006; Cucerzan, 2007) or Wikification (Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011). These two similar tasks link expressions in text to their referent Wikipedia pages. However, since Bunescu and Pasca (2006) used Wikipedia hyperlinks to train the SVM kernel, Cucerzan (2007) used Wikipedia collection and news stories as the development data, and all of the three Wikification work (Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011) generalized their ranker on the data generated from Wikipedia without considering the knowledge of the queried name, th"
C12-1189,D11-1071,0,0.0121384,"retrieve the possible KB entries 3091 for a given mention. Section 4 presents our lazy learning for entity linking with query-specific information. Section 5 discusses a special case - NIL mentions . The experiments are shown in Section 6. Section 7 concludes the paper. 2 Related Work As we have discussed, most of the previous entity linking work (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Gottipati and Jiang, 2011; Han and Sun, 2011) fall into the traditional entity linking framework shown in Figure 1. Besides, the collaborative approach (Chen and Ji, 2011) tried to search similar queries as their query collaboration group by clustering texts. This differs from our method where we use the selective knowledge from unlabeled data. In some other work, entity linking is also called named entity disambiguation using Wikipedia (Bunescu and Pasca, 2006; Cucerzan, 2007) or Wikification (Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011). These two similar tasks link expressions in text to their referent Wikipedia pages. However, since Bunescu and Pasca (2006) used Wikipedia hyperlinks to train the SVM kernel, Cucerzan (2007) used W"
C12-1189,D07-1074,0,0.0346276,"of the previous entity linking work (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Gottipati and Jiang, 2011; Han and Sun, 2011) fall into the traditional entity linking framework shown in Figure 1. Besides, the collaborative approach (Chen and Ji, 2011) tried to search similar queries as their query collaboration group by clustering texts. This differs from our method where we use the selective knowledge from unlabeled data. In some other work, entity linking is also called named entity disambiguation using Wikipedia (Bunescu and Pasca, 2006; Cucerzan, 2007) or Wikification (Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011). These two similar tasks link expressions in text to their referent Wikipedia pages. However, since Bunescu and Pasca (2006) used Wikipedia hyperlinks to train the SVM kernel, Cucerzan (2007) used Wikipedia collection and news stories as the development data, and all of the three Wikification work (Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011) generalized their ranker on the data generated from Wikipedia without considering the knowledge of the queried name, they also fall into"
C12-1189,C10-1032,0,0.0754017,"s task are name variation and name ambiguity. Name variation refers to the case that more than one name variation such as alias, misspelling and acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible t"
C12-1189,D11-1074,0,0.0677527,"acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www.wikipedia.org/ http://www.opencyc.org/ http://www.ontotext.com/kim 4 http://nlp.cs.qc.cuny.edu/kbp/2011/ 2 3"
C12-1189,P11-1095,0,0.0684377,"entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www.wikipedia.org/ http://www.opencyc.org/ http://www.ontotext.com/kim 4 http://nlp.cs.qc.cuny.edu/kbp/2011/ 2 3 3090 data set for e"
C12-1189,P11-1115,0,0.046208,"ta sets of queried and other names by mining their shared predictive structure. Keywords: Entity Linking, Lazy Learning, Query-Specific Information. Proceedings of COLING 2012: Technical Papers, pages 3089–3104, COLING 2012, Mumbai, December 2012. 3089 1 Introduction Recently, more and more knowledge bases (KB) which contain rich knowledge about the world’s entities such as Wikipedia 1 , OpenCyc 2 and KIM 3 (Popov et al., 2004) have become available. These knowledge bases have been shown to form a valuable component for many natural language processing tasks such as knowledge base population (Ji and Grishman, 2011), text classification (Wang and Domeniconi, 2008), and cross-document coreference (Finin et al., 2009). However, to be able to utilize or enrich these KB resources, the applications usually require linking the mentions of entities in text to their corresponding entries in the knowledge bases, which is called entity linking task and has been proposed and studied in Text Analysis Conference (TAC) since 2009 (McNamee and Dang, 2009). Given a mention of an entity in text and a KB, entity linking is to link the mention to its corresponding entry in KB. The major challenges of this task are name var"
C12-1189,P11-3004,0,0.045958,"t more than one name variation such as alias, misspelling and acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www.wikipedia.org/ http://www.opency"
C12-1189,P11-1138,0,0.0581951,"ne name variation such as alias, misspelling and acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www.wikipedia.org/ http://www.opencyc.org/ http://www.ontot"
C12-1189,C10-1145,1,0.940304,"fers to the case that more than one name variation such as alias, misspelling and acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www.wikipedia.org/ http"
C12-1189,N10-1072,0,0.167552,"y. Name variation refers to the case that more than one name variation such as alias, misspelling and acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www"
C12-1189,C98-1012,0,\N,Missing
C18-1018,P16-1014,0,0.0337782,"r, the model is prone to generate numbers that do not exist in the problem or in wrong positions. Our model addresses these issues by incorporating copy and align. 7.2 Sequence-to-Sequence Models Recent applications of seq2seq model in many areas have shown promising results. Copy mechanism is proved effective in dealing with rare or unknown words. The main idea is to decide when and what to copy from the source words in the decoding phase. Jia and Liang (2016) use an attention-based copy mechanism to copy arguments from natural language query to logical form for the task of semantic parsing. Gulcehre et al. (2016) design a pointer network to select tokens in the input. Different from previous work, we consider copying only numbers from the problem description. Some recent work have been proposed to improve the alignment for neural machine translation (Liu et al., 2016; Mi et al., 2016). They introduced a supervised attention mechanism to utilize the word alignment information between sentence pairs in the training data. They first obtain soft word alignments from conventional alignment models. Then they try minimize the distance between the soft word alignments and the word alignments based on model at"
C18-1018,P17-1097,0,0.0135629,"e training data. They first obtain soft word alignments from conventional alignment models. Then they try minimize the distance between the soft word alignments and the word alignments based on model attention in the training procedure. In our math problems, the alignment information is explicit and can be directly obtained. 7.3 Reinforcement Learning for Sequence Generation The classic REINFORCE algorithm (Williams, 1992) has been applied to solve a wide variety of tasks: machine translation (Norouzi et al., 2016), image captioning (Rennie et al., 2017), semantic parsing (Liang et al., 2017; Guu et al., 2017) and summarization (Paulus et al., 2018). Reinforcement learning is applied usually when the evaluation metric is non-differentiable, or there are multiple candidates that yields to the ground truth despite of token orders in target sequence. It trains an agent with a given environment to directly optimize the task evaluation metric (e.g., BLEU or ROUGE). We apply reinforcement learning in math problem solving for two reasons: (1) we evaluate our task with solution accuracy which is not directly optimized in maximum likelihood estimation; (2) there are multiple equations that yield to the corr"
C18-1018,D14-1058,0,0.118688,"uence generation. 220 7.1 Math Word Problem Solving The approaches to solve math word problems can be divided into three categories: rule-based approach, feature-based approach, and neural-based approach. Rule-based approaches (Bobrow, 1964a; Bobrow, 1964b; Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015) accept only well formed input sentences and map them into predefined structures by rules. These methods require strict constraints on both input text and math problem types. Feature-based approaches design features and learn rankers to rank equation candidates to the math problems. Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Kushman et al. (2014), Zhou et al. (2015), Upadhyay et al. (2016) use features such as dependency path between two numbers. Koncel-Kedziorski et al. (2015), Roy and Roth (2015) extract quantity information as features. Wang et al. (2018) extract features for quantity pairs and uses a reinforcement framework to construct an equation tree with the constraint of one unknown variable. Roy and Roth (2015), Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016), Roy and Roth (2018) design features for a few ma"
C18-1018,P16-1084,1,0.861266,"corresponds to at least 6 problems (T6 setting). It only contains 28 templates in total. • Number Word Problem (NumWord) is created by Shi et al. (2015). It contains 2,871 number word problems (i.e., verbally expressed number problems) with 1,183 templates. The T6 subset contains 348 problems. One example problem is “The sum of two numbers is 10. Their difference is 4. What are the two numbers?”. We use its linear subset, which contains 986 problems. The vocabulary of this dataset is the smallest among the three, but contains more templates than Alg514. • Dolphin18K (Dophin18K) is created by Huang et al. (2016). It contains 18,711 math word problems from Yahoo! Answers with 5,738 templates. It has much more problem types than the previous two datasets. This dataset is the most challenging of the three. We use its subset with equation annotation, which contains 10,644 problems. The T6 subset contains 6,827 problems. 3 Modeling Math word problem solving can be formulated as a sequence prediction problem. We set x as the sequence of words in a math word problem description and we want to generate y, the sequence of tokens in an equation system. In this section, we describe (1) the baseline seq2seq mode"
C18-1018,D17-1084,1,0.900909,"imation (MLE) suffers from the issue of “train-test discrepancy”. It means that MLE uses a surrogate objective of maximizing equation likelihood during training, while the evaluation metric of the task is solution accuracy, which is non-differentiable. Therefore we use policy gradient to directly optimize the solution accuracy, which is more capable for this task. Furthermore, we observe that the neural model and the traditional feature-based model are complementary. To take the advantage of both approaches, we add the result of our neural model as a simple feature to the feature-based model (Huang et al., 2017) to create a combined model. We test our model on three publicly available datasets. The experimental results show that the copy and alignment mechanism is effective. Reinforcement learning leads to better performance than MLE. When combining our neural model with the feature-based model, we achieve the state-of-the-art results on all publicly available datasets. The contributions of this paper are as follows: 1) We incorporate copy and alignment mechanism that augment the standard seq2seq model to address two types of errors: generating spurious numbers and generating numbers in wrong positio"
C18-1018,P16-1002,0,0.159589,"g numbers in the problem description. 3.2 Copy Numbers In the basic model, the output token yj is chosen via a softmax over all words in the output vocabulary. However, this model has the problem of generating spurious number tokens. For example, Problem 1 in Figure 1 only contains three numbers, replaced with tokens {n1 , n2 , n3 }. Since the output vocabulary contains other tokens, the model may generate a token “n4 ”, which cannot be recovered and results in generating wrong equations. To address this problem, we incorporate an attention-based copy mechanism into the basic model similar to Jia and Liang (2016). In our case, we only copy numbers from the source problem. At each decoding step j, the model has to decide whether to generate a token from target vocabulary or copy a number from the problem description. The generation probability pgen is modeled by: pgen = σ(Wc cj + Ws0 sj + Wy yj−1 + bgen ) (7) where Wc , Ws0 , Wy , and bgen are parameters of the model and σ is the sigmoid function. Then the output candidates are extended to the concatenation of the target vocabulary and the numbers in the math problem. We can obtain the output probability distribution: P i:w =w aji P (wj = w) = pgen ∗ P"
C18-1018,Q15-1042,0,0.281941,"-based approaches (Bobrow, 1964a; Bobrow, 1964b; Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015) accept only well formed input sentences and map them into predefined structures by rules. These methods require strict constraints on both input text and math problem types. Feature-based approaches design features and learn rankers to rank equation candidates to the math problems. Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Kushman et al. (2014), Zhou et al. (2015), Upadhyay et al. (2016) use features such as dependency path between two numbers. Koncel-Kedziorski et al. (2015), Roy and Roth (2015) extract quantity information as features. Wang et al. (2018) extract features for quantity pairs and uses a reinforcement framework to construct an equation tree with the constraint of one unknown variable. Roy and Roth (2015), Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016), Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Huang et al. (2017), Roy and Roth (2017) focus on the use of fine-grained expression and number units. These approaches requires manual feature design and it is difficult to g"
C18-1018,P14-1026,0,0.618357,"erating the answer. This task requires the machine to have the ability of natural language understanding and reasoning. In the past years, most of the proposed methods heavily rely on predefined rules or feature engineering. On one hand, the rule-based approaches (Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015) predefine a structured representation and maps the problem description into the structure by rules. These approaches usually accept only well-formed input and are difficult to scale to other problem types. On the other hand, the feature-based statistical learning approaches (Kushman et al., 2014; Roy and Roth, 2018) generate equation candidates and find the most probable equation by using predefined features. These approaches have two major drawbacks: (1) Their ability of equation generation is weak. An equation is generated by either replacing the numbers of existing equations in the training data, or enumerating possible combinations of math operators, numbers and variables, which leads to intractably huge search space. (2) They need manually designed features that are specific to math word problems. Recent attempts (Ling et al., 2017; Wang et al., 2017) use sequence-to-sequence (s"
C18-1018,P17-1003,0,0.0292356,"sentence pairs in the training data. They first obtain soft word alignments from conventional alignment models. Then they try minimize the distance between the soft word alignments and the word alignments based on model attention in the training procedure. In our math problems, the alignment information is explicit and can be directly obtained. 7.3 Reinforcement Learning for Sequence Generation The classic REINFORCE algorithm (Williams, 1992) has been applied to solve a wide variety of tasks: machine translation (Norouzi et al., 2016), image captioning (Rennie et al., 2017), semantic parsing (Liang et al., 2017; Guu et al., 2017) and summarization (Paulus et al., 2018). Reinforcement learning is applied usually when the evaluation metric is non-differentiable, or there are multiple candidates that yields to the ground truth despite of token orders in target sequence. It trains an agent with a given environment to directly optimize the task evaluation metric (e.g., BLEU or ROUGE). We apply reinforcement learning in math problem solving for two reasons: (1) we evaluate our task with solution accuracy which is not directly optimized in maximum likelihood estimation; (2) there are multiple equations tha"
C18-1018,P17-1015,0,0.433933,"ature-based statistical learning approaches (Kushman et al., 2014; Roy and Roth, 2018) generate equation candidates and find the most probable equation by using predefined features. These approaches have two major drawbacks: (1) Their ability of equation generation is weak. An equation is generated by either replacing the numbers of existing equations in the training data, or enumerating possible combinations of math operators, numbers and variables, which leads to intractably huge search space. (2) They need manually designed features that are specific to math word problems. Recent attempts (Ling et al., 2017; Wang et al., 2017) use sequence-to-sequence (seq2seq) model for math word problem solving and they have shown promising results. Wang et al. (2017) apply a standard seq2seq model to generate equations. They have shown that seq2seq models have the power to generate equations of which the problem types do not exist in the training data. ∗ Work was done at Microsoft Research. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 213 Proceedings of the 27th International Conference on Computational Ling"
C18-1018,C16-1291,0,0.0259826,"ng results. Copy mechanism is proved effective in dealing with rare or unknown words. The main idea is to decide when and what to copy from the source words in the decoding phase. Jia and Liang (2016) use an attention-based copy mechanism to copy arguments from natural language query to logical form for the task of semantic parsing. Gulcehre et al. (2016) design a pointer network to select tokens in the input. Different from previous work, we consider copying only numbers from the problem description. Some recent work have been proposed to improve the alignment for neural machine translation (Liu et al., 2016; Mi et al., 2016). They introduced a supervised attention mechanism to utilize the word alignment information between sentence pairs in the training data. They first obtain soft word alignments from conventional alignment models. Then they try minimize the distance between the soft word alignments and the word alignments based on model attention in the training procedure. In our math problems, the alignment information is explicit and can be directly obtained. 7.3 Reinforcement Learning for Sequence Generation The classic REINFORCE algorithm (Williams, 1992) has been applied to solve a wide v"
C18-1018,D16-1249,0,0.161315,"n wrong positions. Problem 2 in Figure 1 is an example. The output equation structure is correct by the basic model. However, the numbers are aligned wrongly. When decoding the last token, the model should pay more attention to the source token “1170”. Instead, the model wrongly aligned to the source token “30”. One characteristic of math word problems is that they have explicit alignment information between numbers in the problems and numbers in the equations. To improve the alignment in math problems, we use a supervised align mechanism to guide the training of our seq2seq model. Similar to Mi et al. (2016), the basic idea is minimize the cost of distance between the “true” alignment and the model predicted attention. We use the cross entropy loss function in the following: XX δ(ai , a ˆi ) = − a ˆim,n × log aim,n (9) m n Please note that the disagreement only exists in numbers. The actual distribution a ˆim,n will be 1 if the mth token in the source is a number and equals to the nth tokens in the target sequence, otherwise it is 0. 216 4 Reinforcement Learning As previously mentioned, MLE optimizes the surrogate objective of maximizing equation likelihood, while the evaluation metric of the tas"
C18-1018,P16-1202,0,0.597867,"to rank equation candidates to the math problems. Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Kushman et al. (2014), Zhou et al. (2015), Upadhyay et al. (2016) use features such as dependency path between two numbers. Koncel-Kedziorski et al. (2015), Roy and Roth (2015) extract quantity information as features. Wang et al. (2018) extract features for quantity pairs and uses a reinforcement framework to construct an equation tree with the constraint of one unknown variable. Roy and Roth (2015), Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016), Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Huang et al. (2017), Roy and Roth (2017) focus on the use of fine-grained expression and number units. These approaches requires manual feature design and it is difficult to generalizing the features to other problem types. Recently, researchers try to build end-to-end neural models to solve math word problems. Ling et al. (2017) focus on multiple-choice problems. It takes a problem description as input and outputs the rationale and the final choice. Wang et al. (2017) apply a standard seq2seq model to"
C18-1018,D15-1202,0,0.209701,"Bobrow, 1964b; Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015) accept only well formed input sentences and map them into predefined structures by rules. These methods require strict constraints on both input text and math problem types. Feature-based approaches design features and learn rankers to rank equation candidates to the math problems. Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Kushman et al. (2014), Zhou et al. (2015), Upadhyay et al. (2016) use features such as dependency path between two numbers. Koncel-Kedziorski et al. (2015), Roy and Roth (2015) extract quantity information as features. Wang et al. (2018) extract features for quantity pairs and uses a reinforcement framework to construct an equation tree with the constraint of one unknown variable. Roy and Roth (2015), Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016), Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Huang et al. (2017), Roy and Roth (2017) focus on the use of fine-grained expression and number units. These approaches requires manual feature design and it is difficult to generalizing the featu"
C18-1018,Q18-1012,0,0.590733,"is task requires the machine to have the ability of natural language understanding and reasoning. In the past years, most of the proposed methods heavily rely on predefined rules or feature engineering. On one hand, the rule-based approaches (Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015) predefine a structured representation and maps the problem description into the structure by rules. These approaches usually accept only well-formed input and are difficult to scale to other problem types. On the other hand, the feature-based statistical learning approaches (Kushman et al., 2014; Roy and Roth, 2018) generate equation candidates and find the most probable equation by using predefined features. These approaches have two major drawbacks: (1) Their ability of equation generation is weak. An equation is generated by either replacing the numbers of existing equations in the training data, or enumerating possible combinations of math operators, numbers and variables, which leads to intractably huge search space. (2) They need manually designed features that are specific to math word problems. Recent attempts (Ling et al., 2017; Wang et al., 2017) use sequence-to-sequence (seq2seq) model for mat"
C18-1018,D16-1117,0,0.0135369,"Feature-based approaches design features and learn rankers to rank equation candidates to the math problems. Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Kushman et al. (2014), Zhou et al. (2015), Upadhyay et al. (2016) use features such as dependency path between two numbers. Koncel-Kedziorski et al. (2015), Roy and Roth (2015) extract quantity information as features. Wang et al. (2018) extract features for quantity pairs and uses a reinforcement framework to construct an equation tree with the constraint of one unknown variable. Roy and Roth (2015), Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016), Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Huang et al. (2017), Roy and Roth (2017) focus on the use of fine-grained expression and number units. These approaches requires manual feature design and it is difficult to generalizing the features to other problem types. Recently, researchers try to build end-to-end neural models to solve math word problems. Ling et al. (2017) focus on multiple-choice problems. It takes a problem description as input and outputs the rationale and the f"
C18-1018,D15-1135,1,0.834232,"(3) Our neural model is complementary to the feature-based model and their combination significantly outperforms the state-of-the-art results. 1 Introduction The task of math word problem solving aims to automatically solve a math problem by reading the text description of the problem and generating the answer. This task requires the machine to have the ability of natural language understanding and reasoning. In the past years, most of the proposed methods heavily rely on predefined rules or feature engineering. On one hand, the rule-based approaches (Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015) predefine a structured representation and maps the problem description into the structure by rules. These approaches usually accept only well-formed input and are difficult to scale to other problem types. On the other hand, the feature-based statistical learning approaches (Kushman et al., 2014; Roy and Roth, 2018) generate equation candidates and find the most probable equation by using predefined features. These approaches have two major drawbacks: (1) Their ability of equation generation is weak. An equation is generated by either replacing the numbers of existing equations in the trainin"
C18-1018,D16-1029,0,0.261131,"ies: rule-based approach, feature-based approach, and neural-based approach. Rule-based approaches (Bobrow, 1964a; Bobrow, 1964b; Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015) accept only well formed input sentences and map them into predefined structures by rules. These methods require strict constraints on both input text and math problem types. Feature-based approaches design features and learn rankers to rank equation candidates to the math problems. Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Kushman et al. (2014), Zhou et al. (2015), Upadhyay et al. (2016) use features such as dependency path between two numbers. Koncel-Kedziorski et al. (2015), Roy and Roth (2015) extract quantity information as features. Wang et al. (2018) extract features for quantity pairs and uses a reinforcement framework to construct an equation tree with the constraint of one unknown variable. Roy and Roth (2015), Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016), Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Huang et al. (2017), Roy and Roth (2017) focus on the use of fine-grained expression"
C18-1018,D15-1096,0,0.336375,"d into three categories: rule-based approach, feature-based approach, and neural-based approach. Rule-based approaches (Bobrow, 1964a; Bobrow, 1964b; Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015) accept only well formed input sentences and map them into predefined structures by rules. These methods require strict constraints on both input text and math problem types. Feature-based approaches design features and learn rankers to rank equation candidates to the math problems. Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Kushman et al. (2014), Zhou et al. (2015), Upadhyay et al. (2016) use features such as dependency path between two numbers. Koncel-Kedziorski et al. (2015), Roy and Roth (2015) extract quantity information as features. Wang et al. (2018) extract features for quantity pairs and uses a reinforcement framework to construct an equation tree with the constraint of one unknown variable. Roy and Roth (2015), Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016), Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Huang et al. (2017), Roy and Roth (2017) focus on the use of"
D07-1035,W06-1650,0,0.871179,"l to have a mechanism capable of assessing the quality of reviews and detecting low-quality/noisy reviews. Some shopping sites already provide a function of assessing the quality of reviews. For example, Ming Zhou Microsoft Research Asia Beijing, China mingzhou@microsoft.com Amazon1 allows users to vote for the helpfulness of each review and then ranks the reviews based on the accumulated votes. However, according to our survey in Section 3, users’ votes at Amazon have three kinds of biases as follows: (1) imbalance vote bias, (2) winner circle bias, and (3) early bird bias. Existing studies (Kim et al, 2006; Zhang and Varadarajan, 2006) used these users’ votes for training ranking models to assess the quality of reviews, which therefore are subject to these biases. In this paper, we demonstrate the aforementioned biases and define a standard specification to measure the quality of product reviews. We then manually annotate a set of ground-truth with real world product review data conforming to the specification. To automatically detect low-quality product reviews, we propose a classification-based approach learned from the annotated ground-truth. The proposed approach explores three aspects of p"
D07-1035,P98-2127,0,0.02747,"Missing"
D07-1035,P04-1035,0,0.0233245,"developing a specification on the quality of reviews and building a ground-truth according to the specification. 2.2 Mining Opinions from Reviews One area of research on opinion mining from product reviews is to judge whether a review expresses a positive or a negative opinion. For example, Turney (2006) presented a simple unsupervised learning algorithm in judging reviews as “thumbs up” (recommended) or “thumbs down” (not recommended). Pang et al (2002) considered the same problem and presented a set of supervised machine learning approaches to it. For other work see also Dave et al. (2003), Pang and Lee (2004, 2005). Another area of research on opinion mining is to extract and summarize users’ opinions from product reviews (Hu and Liu, 2004; Liu et al., 2005; Popescu and Etzioni, 2005). Typically, a sentence or a text segment in the reviews is treated as the basic unit. The polarity of users’ sentiments on a product feature in each unit is extracted. Then the aggregation of the polarities of individual senti335 Quality of Product Reviews In this section, we will first show three biases of users’ votes observed on Amazon, and then present our specification on the quality of product reviews. 3.1 Ama"
D07-1035,P05-1015,0,0.0925453,"Missing"
D07-1035,W02-1011,0,0.0260208,"oduct reviews, a set of specifications for judging the quality of reviews is first defined. A classificationbased approach is proposed to detect the low-quality reviews. We apply the proposed approach to enhance opinion summarization in a two-stage framework. Experimental results show that the proposed approach effectively (1) discriminates lowquality reviews from high-quality ones and (2) enhances the task of opinion summarization by detecting and filtering lowquality reviews. 1 Introduction In the past few years, there has been an increasing interest in mining opinions from product reviews (Pang, et al, 2002; Liu, et al, 2004; Popescu and Etzioni, 2005). However, due to the lack of editorial and quality control, reviews on products vary greatly in quality. Thus, it is crucial to have a mechanism capable of assessing the quality of reviews and detecting low-quality/noisy reviews. Some shopping sites already provide a function of assessing the quality of reviews. For example, Ming Zhou Microsoft Research Asia Beijing, China mingzhou@microsoft.com Amazon1 allows users to vote for the helpfulness of each review and then ranks the reviews based on the accumulated votes. However, according to our surve"
D07-1035,H05-1043,0,0.613796,"s for judging the quality of reviews is first defined. A classificationbased approach is proposed to detect the low-quality reviews. We apply the proposed approach to enhance opinion summarization in a two-stage framework. Experimental results show that the proposed approach effectively (1) discriminates lowquality reviews from high-quality ones and (2) enhances the task of opinion summarization by detecting and filtering lowquality reviews. 1 Introduction In the past few years, there has been an increasing interest in mining opinions from product reviews (Pang, et al, 2002; Liu, et al, 2004; Popescu and Etzioni, 2005). However, due to the lack of editorial and quality control, reviews on products vary greatly in quality. Thus, it is crucial to have a mechanism capable of assessing the quality of reviews and detecting low-quality/noisy reviews. Some shopping sites already provide a function of assessing the quality of reviews. For example, Ming Zhou Microsoft Research Asia Beijing, China mingzhou@microsoft.com Amazon1 allows users to vote for the helpfulness of each review and then ranks the reviews based on the accumulated votes. However, according to our survey in Section 3, users’ votes at Amazon have th"
D07-1035,H05-2017,0,\N,Missing
D07-1035,P02-1053,0,\N,Missing
D07-1035,C98-2122,0,\N,Missing
D08-1018,W98-1115,0,0.626384,"] VP VP VP VP NP? VB get 1 VP [VP CC] Introduction NP DT NN the bag CC VB VB and go get (a) final parse Binarization, which transforms an n-ary grammar into an equivalent binary grammar, is essential for achieving an O(n3 ) time complexity in the contextfree grammar parsing. O(n3 ) tabular parsing algorithms, such as the CKY algorithm (Kasami, 1965; Younger, 1967), the GHR parser (Graham et al., 1980), the Earley algorithm (Earley, 1970) and the chart parsing algorithm (Kay, 1980; Klein and Manning, 2001) all convert their grammars into binary branching forms, either explicitly or implicitly (Charniak et al., 1998). In fact, the number of all possible binarizations of a production with n + 1 symbols on its right ∗ This work was done when Xinying Song and Shilin Ding were visiting students at Microsoft Research Asia. NP DT NN the bag CC VB VB and go get (b) with left NP DT NN the bag CC VB and go (c) with right Figure 1: Parsing with left and right binarization. If a left binarized grammar is used, see Figure 1(b), an extra constituent [N P CC] spanning “the bag and” will be produced. Because rule [N P CC] → N P CC is in the left binarized grammar and there is an N P over “the bag” and a CC over the righ"
D08-1018,N06-1022,0,0.323712,"always selects the left most pair of symbols and combines them to form an intermediate nonterminal. This procedure is repeated until all productions are binary. In this paper, we assume that all binarizations follow the fashion above, except that the choice of pair of symbols for combination can be arbitrary. Next we show three other known binarizations. Right binarization is almost the same with left binarization, except that it always selects the right most pair, instead of left, to combine. Head binarization always binarizes from the head outward (Klein and Manning, 2003b). Please refer to Charniak et al. (2006) for more details. Compact binarization (Schmid, 2004) tries to minimize the size of the binarized grammar. It leads to a compact grammar. We therefore call it compact binarization. It is done via a greedy approach: it always selects the pair that occurs most on the right hand sides of rules to combine. 3 The optimal binarization The optimal binarization should help CKY parsing to achieve its best efficiency. We formalize the idea as follows: Definition 2. The optimal binarization is π ∗ , for a given n-ary grammar G and a test corpus C: π ∗ = arg min T (π(G), C) π (1) where T (π(G), C) is the"
D08-1018,P97-1003,0,0.502433,"his would affect the accuracy. We do not address this interaction on in this paper, but leave it to the future work. In Section 7.3 we will use the iterative CKY for testing. In addition, we believe there exist some speed-up techniques which are incompatible with our binarization. One such example may be the top-down left-corner filtering (Graham et al., 1980; Moore, 2000), which seems to be only applicable to the process of left binarization. A detailed investigation on this problem will be left to the future work. The last issue is how our binarization performs on a lexicalized parser, like Collins (1997). Our intuition is that we cannot apply our binarization to Collins (1997). The key fact in lexicalized parsers is that we cannot explicitly write down all rules and compute their probabilities precisely, due to the great number of rules and the severe data sparsity problem. Therefore in Collins (1997) grammar rules are already factorized into a set of probabilities. In order to capture the dependency relationship between lexcial heads Collins (1997) breaks down the rules from head outwards, which prevents us from factorizing them in other ways. Therefore our binarization cannot apply to the l"
D08-1018,P81-1022,0,0.785732,"hermore we show that it is feasible to combine existing parsing speed-up techniques with our binarization to achieve even better performance. VP VP [CC VP] NP? VP VP [NP CC] VP VP VP VP NP? VB get 1 VP [VP CC] Introduction NP DT NN the bag CC VB VB and go get (a) final parse Binarization, which transforms an n-ary grammar into an equivalent binary grammar, is essential for achieving an O(n3 ) time complexity in the contextfree grammar parsing. O(n3 ) tabular parsing algorithms, such as the CKY algorithm (Kasami, 1965; Younger, 1967), the GHR parser (Graham et al., 1980), the Earley algorithm (Earley, 1970) and the chart parsing algorithm (Kay, 1980; Klein and Manning, 2001) all convert their grammars into binary branching forms, either explicitly or implicitly (Charniak et al., 1998). In fact, the number of all possible binarizations of a production with n + 1 symbols on its right ∗ This work was done when Xinying Song and Shilin Ding were visiting students at Microsoft Research Asia. NP DT NN the bag CC VB VB and go get (b) with left NP DT NN the bag CC VB and go (c) with right Figure 1: Parsing with left and right binarization. If a left binarized grammar is used, see Figure 1(b), an extra co"
D08-1018,W97-0302,0,0.211043,"1: for X → Y Z, Y in left span and Z in right span 2: Add X to parent span 3.2 Model assumption We have shown that both binarization and the forstatement implementation in the inner most loop of CKY will affect the parsing speed. About the for-statement implementations, no previous study has addressed which one is superior. The actual choice may affect our study on binarization. If using M1, since it enumerates all rules in the grammar, the optimal binarization will be the one with minimal number of rules, i.e. minimal binarized grammar size. However, M1 is usually not preferred in practice (Goodman, 1997). For other methods, it is hard to tell which binarization is optimal theoretically. In this paper, for simplicity reasons we do not consider the effect of for-statement implementations on the optimal binarization. On the other hand, it is well known that reducing the number of constituents produced in parsing can greatly improve CKY parsing efficiency. That is how most thresholding systems (Goodman, 1997; Tsuruoka and Tsujii, 2004; Charniak et al., 2006) speed up CKY parsing. Apparently, the number of 1 Note that we should skip Y (Z) if it never appears as the first (second) symbol on the rig"
D08-1018,W07-0405,0,0.0749719,"ze the grammar and prevent the binarized grammar becoming too specific (Charniak et al., 2006). It is equipped with head binarization to help improve parsing accuracy, following the traditional linguistic insight that phrases are organized around the head (Collins, 1997; Klein and Manning, 2003b). In contrast, we focus our attention on parsing efficiency not accuracy in this paper. Binarization also attracts attention in the syntaxbased models for machine translation, where translation can be modeled as a parsing problem and binarization is essential for efficient parsing (Zhang et al., 2006; Huang, 2007). Wang et al. (2007) employs binarization to decompose syntax trees to acquire more re-usable translation rules in order to improve translation accuracy. Their binarization is restricted to be a mixture of left and right binarization. This constraint may decrease the power of binarization when applied to speeding up parsing in our problem. 9 Conclusions and future work We have studied the impact of grammar binarization on parsing efficiency and presented a novel binarization which utilizes rich information learnt from training corpus. Experiments not only showed that our learnt binarization ou"
D08-1018,W01-1812,0,0.0288213,"rsing speed-up techniques with our binarization to achieve even better performance. VP VP [CC VP] NP? VP VP [NP CC] VP VP VP VP NP? VB get 1 VP [VP CC] Introduction NP DT NN the bag CC VB VB and go get (a) final parse Binarization, which transforms an n-ary grammar into an equivalent binary grammar, is essential for achieving an O(n3 ) time complexity in the contextfree grammar parsing. O(n3 ) tabular parsing algorithms, such as the CKY algorithm (Kasami, 1965; Younger, 1967), the GHR parser (Graham et al., 1980), the Earley algorithm (Earley, 1970) and the chart parsing algorithm (Kay, 1980; Klein and Manning, 2001) all convert their grammars into binary branching forms, either explicitly or implicitly (Charniak et al., 1998). In fact, the number of all possible binarizations of a production with n + 1 symbols on its right ∗ This work was done when Xinying Song and Shilin Ding were visiting students at Microsoft Research Asia. NP DT NN the bag CC VB VB and go get (b) with left NP DT NN the bag CC VB and go (c) with right Figure 1: Parsing with left and right binarization. If a left binarized grammar is used, see Figure 1(b), an extra constituent [N P CC] spanning “the bag and” will be produced. Because r"
D08-1018,N03-1016,0,0.530375,"ermediate) nonterminals. Left binarization always selects the left most pair of symbols and combines them to form an intermediate nonterminal. This procedure is repeated until all productions are binary. In this paper, we assume that all binarizations follow the fashion above, except that the choice of pair of symbols for combination can be arbitrary. Next we show three other known binarizations. Right binarization is almost the same with left binarization, except that it always selects the right most pair, instead of left, to combine. Head binarization always binarizes from the head outward (Klein and Manning, 2003b). Please refer to Charniak et al. (2006) for more details. Compact binarization (Schmid, 2004) tries to minimize the size of the binarized grammar. It leads to a compact grammar. We therefore call it compact binarization. It is done via a greedy approach: it always selects the pair that occurs most on the right hand sides of rules to combine. 3 The optimal binarization The optimal binarization should help CKY parsing to achieve its best efficiency. We formalize the idea as follows: Definition 2. The optimal binarization is π ∗ , for a given n-ary grammar G and a test corpus C: π ∗ = arg min"
D08-1018,P03-1054,0,0.20546,"ermediate) nonterminals. Left binarization always selects the left most pair of symbols and combines them to form an intermediate nonterminal. This procedure is repeated until all productions are binary. In this paper, we assume that all binarizations follow the fashion above, except that the choice of pair of symbols for combination can be arbitrary. Next we show three other known binarizations. Right binarization is almost the same with left binarization, except that it always selects the right most pair, instead of left, to combine. Head binarization always binarizes from the head outward (Klein and Manning, 2003b). Please refer to Charniak et al. (2006) for more details. Compact binarization (Schmid, 2004) tries to minimize the size of the binarized grammar. It leads to a compact grammar. We therefore call it compact binarization. It is done via a greedy approach: it always selects the pair that occurs most on the right hand sides of rules to combine. 3 The optimal binarization The optimal binarization should help CKY parsing to achieve its best efficiency. We formalize the idea as follows: Definition 2. The optimal binarization is π ∗ , for a given n-ary grammar G and a test corpus C: π ∗ = arg min"
D08-1018,H94-1020,0,0.042704,"Missing"
D08-1018,2000.iwpt-1.18,0,0.0420782,", ctr(w)) = −∞. 4 Since f is used for ranking, the magnitude is not important. 172 bols. With the same complete constituents, one binarization might derive incomplete constitutes that could be pruned while another binarization may not. This would affect the accuracy. We do not address this interaction on in this paper, but leave it to the future work. In Section 7.3 we will use the iterative CKY for testing. In addition, we believe there exist some speed-up techniques which are incompatible with our binarization. One such example may be the top-down left-corner filtering (Graham et al., 1980; Moore, 2000), which seems to be only applicable to the process of left binarization. A detailed investigation on this problem will be left to the future work. The last issue is how our binarization performs on a lexicalized parser, like Collins (1997). Our intuition is that we cannot apply our binarization to Collins (1997). The key fact in lexicalized parsers is that we cannot explicitly write down all rules and compute their probabilities precisely, due to the great number of rules and the severe data sparsity problem. Therefore in Collins (1997) grammar rules are already factorized into a set of probab"
D08-1018,P06-1055,0,0.088687,"parsers is that we cannot explicitly write down all rules and compute their probabilities precisely, due to the great number of rules and the severe data sparsity problem. Therefore in Collins (1997) grammar rules are already factorized into a set of probabilities. In order to capture the dependency relationship between lexcial heads Collins (1997) breaks down the rules from head outwards, which prevents us from factorizing them in other ways. Therefore our binarization cannot apply to the lexicalized parser. However, there are state-of-the-art unlexicalized parsers (Klein and Manning, 2003b; Petrov et al., 2006), to which we believe our binarization can be applied. 7 Experiments We conducted two experiments on Penn Treebank II corpus (Marcus et al., 1994). The first is to compare the effects of different binarizations on parsing and the second is to test the feasibility to combine our work with iterative CKY parsing (Tsuruoka and Tsujii, 2004) to achieve even better efficiency. 7.1 Experimental setup Following conventions, we learnt the grammar from Wall Street Journal (WSJ) section 2 to 21 and modified it by discarding all functional tags and empty nodes. The parser obtained this way is a pure unlex"
D08-1018,C04-1024,0,0.165238,"to form an intermediate nonterminal. This procedure is repeated until all productions are binary. In this paper, we assume that all binarizations follow the fashion above, except that the choice of pair of symbols for combination can be arbitrary. Next we show three other known binarizations. Right binarization is almost the same with left binarization, except that it always selects the right most pair, instead of left, to combine. Head binarization always binarizes from the head outward (Klein and Manning, 2003b). Please refer to Charniak et al. (2006) for more details. Compact binarization (Schmid, 2004) tries to minimize the size of the binarized grammar. It leads to a compact grammar. We therefore call it compact binarization. It is done via a greedy approach: it always selects the pair that occurs most on the right hand sides of rules to combine. 3 The optimal binarization The optimal binarization should help CKY parsing to achieve its best efficiency. We formalize the idea as follows: Definition 2. The optimal binarization is π ∗ , for a given n-ary grammar G and a test corpus C: π ∗ = arg min T (π(G), C) π (1) where T (π(G), C) is the running time for CKY to parse corpus C, using the bin"
D08-1018,D07-1078,0,0.0312495,"and prevent the binarized grammar becoming too specific (Charniak et al., 2006). It is equipped with head binarization to help improve parsing accuracy, following the traditional linguistic insight that phrases are organized around the head (Collins, 1997; Klein and Manning, 2003b). In contrast, we focus our attention on parsing efficiency not accuracy in this paper. Binarization also attracts attention in the syntaxbased models for machine translation, where translation can be modeled as a parsing problem and binarization is essential for efficient parsing (Zhang et al., 2006; Huang, 2007). Wang et al. (2007) employs binarization to decompose syntax trees to acquire more re-usable translation rules in order to improve translation accuracy. Their binarization is restricted to be a mixture of left and right binarization. This constraint may decrease the power of binarization when applied to speeding up parsing in our problem. 9 Conclusions and future work We have studied the impact of grammar binarization on parsing efficiency and presented a novel binarization which utilizes rich information learnt from training corpus. Experiments not only showed that our learnt binarization outperforms other exis"
D08-1018,N06-1033,0,0.0767166,"are used to generalize the grammar and prevent the binarized grammar becoming too specific (Charniak et al., 2006). It is equipped with head binarization to help improve parsing accuracy, following the traditional linguistic insight that phrases are organized around the head (Collins, 1997; Klein and Manning, 2003b). In contrast, we focus our attention on parsing efficiency not accuracy in this paper. Binarization also attracts attention in the syntaxbased models for machine translation, where translation can be modeled as a parsing problem and binarization is essential for efficient parsing (Zhang et al., 2006; Huang, 2007). Wang et al. (2007) employs binarization to decompose syntax trees to acquire more re-usable translation rules in order to improve translation accuracy. Their binarization is restricted to be a mixture of left and right binarization. This constraint may decrease the power of binarization when applied to speeding up parsing in our problem. 9 Conclusions and future work We have studied the impact of grammar binarization on parsing efficiency and presented a novel binarization which utilizes rich information learnt from training corpus. Experiments not only showed that our learnt b"
D09-1054,N04-4027,0,0.0194412,"work. Cong et al. (2008) proposed a supervised approach for question detection and an unsupervised approach for answer detection without considering contexts. Ding et al. (2008) used CRFs to detect contexts and answers of questions from forum threads. Some researches on summarizing discussion threads and emails are related to our work, too. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into sub-topics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. The work (Shrestha and McKeown, 2004) used RIPPER as a classifier to detect interrogative questions and their answers then used the resulting question and answer pairs as summaries. We also note the existing work on extracting knowledge from discussion threads. Huang et al. (2007) used SVMs to extract input-reply pairs from forums for chatbot knowledge. Feng et al. (2006) implemented a discussion-bot which used cosine similarity to match students’ query with reply posts from an annotated corpus of archived"
D09-1054,P08-1081,1,0.096151,"g the structural Support Vector Machine method. Our customization has several attractive properties: (1) it gives a comprehensive graphical representation of thread discussion. (2) It designs special inference algorithms instead of generalpurpose ones. (3) It can be readily extended to different task preferences by varying loss functions. Experimental results on a real data set show that our methods are both promising and flexible. 1 Introduction Recently, extracting questions, contexts and answers from post discussions of online forums incurs increasing academic attention (Cong et al., 2008; Ding et al., 2008). The extracted knowledge can be used either to enrich the knowledge base of community question answering (QA) services such as Yahoo! Answers or to augment the knowledge base of chatbot (Huang et al., 2007). Figure 1 gives an example of a forum thread with questions, contexts and answers annotated. This thread contains three posts and ten sentences, among which three questions are discussed. The three questions are proposed in three sentences, S3, S5 and S6. The context sentences S1 and S2 provide contextual information for question sentence S3. Similarly, the context sentence S4 provides con"
D09-1054,C04-1128,0,0.485542,"detection without considering contexts. Ding et al. (2008) used CRFs to detect contexts and answers of questions from forum threads. Some researches on summarizing discussion threads and emails are related to our work, too. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into sub-topics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. The work (Shrestha and McKeown, 2004) used RIPPER as a classifier to detect interrogative questions and their answers then used the resulting question and answer pairs as summaries. We also note the existing work on extracting knowledge from discussion threads. Huang et al. (2007) used SVMs to extract input-reply pairs from forums for chatbot knowledge. Feng et al. (2006) implemented a discussion-bot which used cosine similarity to match students’ query with reply posts from an annotated corpus of archived threaded discussions. Moreover, extensive researches have been done within the area of question answering (Burger et We have"
D09-1054,W06-1643,0,0.0153613,"igure 2: Structured models 2.2 Graphical Representation notated with its allowed labels and the labels C, A, Q and P stand for context, answer, question and plain sentence labels, respectively. Note that the complete skip-chain model completely links each two context and answer candidates and the label group model combines the labels of one sentence into one label group. Recently, Ding et al. (2008) use skip-chain and 2D Conditional Random Fields (CRFs) (Lafferty et al., 2001) to perform the relational learning for context and answer extraction. The skip-chain CRFs (Sutton and McCallum, 2004; Galley, 2006) model the long distance dependency between context and answer sentences and the 2D CRFs (Zhu et al., 2005) model the dependency between contiguous questions. The graphical representation of those two models are shown in Figures 2(a) and 2(c), respectively. Those two CRFs are both extensions of the linear chain CRFs for the sake of powerful relational learning. However, directly using the skip-chain and 2D CRFs without any customization has obvious disadvantages: (a) the skip-chain model does not model the dependency between answer sentence and multiple context sentences; and (b) the 2D model"
D09-1054,P06-1114,0,0.0407426,"Missing"
D09-1054,C04-1079,0,0.0192126,"most related with our work. Cong et al. (2008) proposed a supervised approach for question detection and an unsupervised approach for answer detection without considering contexts. Ding et al. (2008) used CRFs to detect contexts and answers of questions from forum threads. Some researches on summarizing discussion threads and emails are related to our work, too. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into sub-topics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. The work (Shrestha and McKeown, 2004) used RIPPER as a classifier to detect interrogative questions and their answers then used the resulting question and answer pairs as summaries. We also note the existing work on extracting knowledge from discussion threads. Huang et al. (2007) used SVMs to extract input-reply pairs from forums for chatbot knowledge. Feng et al. (2006) implemented a discussion-bot which used cosine similarity to match students’ query with reply posts from an annotat"
D09-1054,P05-1037,0,0.0155275,"al., 2006). They mainly focused on using sophisticated linguistic analysis to construct answer from a large document collection. Related work 8 Conclusion and Future Work Previous work on extracting questions, answers and contexts is most related with our work. Cong et al. (2008) proposed a supervised approach for question detection and an unsupervised approach for answer detection without considering contexts. Ding et al. (2008) used CRFs to detect contexts and answers of questions from forum threads. Some researches on summarizing discussion threads and emails are related to our work, too. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into sub-topics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. The work (Shrestha and McKeown, 2004) used RIPPER as a classifier to detect interrogative questions and their answers then used the resulting question and answer pairs as summaries. We also note the existing work on extracting knowledge from discussion th"
D09-1130,W04-3240,0,0.740207,"1 http://tripadvisor.com/ in this paper) and large unlabeled data sets can be collected from the Web. Thus, we focus on the problem of how to accurately recognize speech acts in emails and forums by making maximum use of data from existing resources. Recently, there are increasing interests in speech act recognition of online text-based conversations. Analysis of speech acts for online chat and instant messages and have been studied in computer-mediated communication (CMC) and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). In natural language processing, Cohen et al. (2004) and Feng et al. (2006) used speech acts to capture the intentional focus of emails and discussion boards. However, they assume that enough labeled data are available for developing speech act recognition models. A main contribution of this paper is that we address the problem of learning speech act recognition in a semi-supervised way. To our knowledge, this is the first use of semi-supervised speech act recognition in emails and online forums. To do this, we make use of labeled data from spoken conversations (Jurafsky et al., 1997; Dhillon et al., 2004). A second contribution is that our mod"
D09-1130,P04-1085,0,0.142462,"der Dialog Act (MRDA). SWBD is an annotation scheme and collection of labeled dialog act2 data for telephone conversations (Jurafsky et al., 1997). The main purpose of SWBD is to acquire stochastic discourse grammars for training better language models for automatic speech recognition. More recently, an MRDA corpus has been adapted from SWBD but its tag set for labeling meetings has been modified to better reflect the types of interaction in multi-party face-to-face meetings (Dhillon et al., 2004). These two corpora have been extensively studied, e.g., (Stolcke et al., 2000; Ang et al., 2005; Galley et al., 2004). We also use these for our experiments. 2 A dialog act is the meaning of an utterance at the level of illocutionary force (Austin, 1962), and broadly covers the speech act and adjacency pair (Stolcke et al., 2000). In this paper, we use only the term ‘speech act’ for clarity. This paper focuses on the problem of semisupervised speech act recognition. The goal of semi-supervised learning techniques is to use auxiliary data to improve a model’s capability to recognize speech acts. The approach in Tur et al. (2005) presented semi-supervised learning to employ auxiliary unlabeled data in call cla"
D09-1130,W04-3239,0,0.390245,"n “What site should we use to book a Beijing-Chonqing flight?” can be predicted by two discriminative features, “(<s>, WRB) → QW” and “(?, </s>) → QW” where <s> and </s> are sentence start and end symbols, and WRB is a part-of-speech tag that denotes a Wh-adverb. In addition, useful features could be of various lengths, i.e. not fixed length n-grams, and nonadjacent. One key idea of this paper is a novel use of subtree features to model these for speech act recognition. 4.1 Exploiting Subtree Features To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). We briefly introduce this algorithm here. In Section 3.1, we defined x = {xj } as the forest that is a set of trees. More precisely, xj is a labeled ordered tree where each node has its own label and is ordered leftto-right. Several types of labeled ordered trees Figure 3: Representations of tree: (a) bag-ofwords, (b) n-gram, (c) word pair, and (d) dependency tree. A node denotes a word and a directed edge indicates a parent-and-child relationship. are possible (Figure 3). Note that S-expression can be used instead for computation, for example (a(b(c(d)))) for the n-gram (Figure 3(b)). Moreo"
D09-1130,N06-1027,0,0.714336,"m/ in this paper) and large unlabeled data sets can be collected from the Web. Thus, we focus on the problem of how to accurately recognize speech acts in emails and forums by making maximum use of data from existing resources. Recently, there are increasing interests in speech act recognition of online text-based conversations. Analysis of speech acts for online chat and instant messages and have been studied in computer-mediated communication (CMC) and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). In natural language processing, Cohen et al. (2004) and Feng et al. (2006) used speech acts to capture the intentional focus of emails and discussion boards. However, they assume that enough labeled data are available for developing speech act recognition models. A main contribution of this paper is that we address the problem of learning speech act recognition in a semi-supervised way. To our knowledge, this is the first use of semi-supervised speech act recognition in emails and online forums. To do this, we make use of labeled data from spoken conversations (Jurafsky et al., 1997; Dhillon et al., 2004). A second contribution is that our model learns subtree featu"
D09-1130,N03-1030,0,0.073973,"Missing"
D09-1130,J00-3003,0,0.446913,"Missing"
D12-1094,P08-1004,0,0.0613743,"Missing"
D12-1094,P11-1062,0,0.0127454,"Missing"
D12-1094,P04-1056,0,0.0133143,"Missing"
D12-1094,I05-2045,0,0.0163377,"Missing"
D12-1094,D11-1142,0,0.258339,"Missing"
D12-1094,P04-1053,1,0.931988,"Missing"
D12-1094,C92-2082,0,0.229379,"complexity. 4.1 Knowledge Sources Entity similarity graph We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML tag patterns include tables, dropdown boxes, etc. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and anot"
D12-1094,P08-1119,0,0.342981,"ings an open-ended set of relation types. To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types. The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance. However, most algorithms (Hasegawa et al., 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Recently, Kok and Domingos (2008) proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types. However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering. Empirically we found that it fails to group many relevant relation instances. These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice. In contrast, there exist sever"
D12-1094,N04-1041,0,0.0169342,"these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel & Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include NP {,} (such as) {NP,}* {and|or} NP, NP (is|are|was|were|being) (a|an|the) NP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity:"
D12-1094,I05-1011,0,0.0506813,"Missing"
D12-1094,D09-1025,0,0.0418617,"Missing"
D12-1094,I05-5011,0,0.0637491,"Missing"
D12-1094,C10-1112,1,0.817003,"been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy c"
D12-1094,D08-1061,0,0.0252795,"ge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel & Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include NP {,} (such as) {NP,}* {and|or} NP, NP (is|are|was|were|being) (a|an|the) NP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity: To generate the pairwise similarity graph"
D12-1094,P10-2068,0,0.0211762,"Missing"
D12-1094,N09-1033,0,0.0312473,"Missing"
D12-1094,P09-1050,0,0.0297532,"Missing"
D12-1094,P10-1013,0,0.166586,"Missing"
D12-1094,P03-1016,0,0.079683,"Missing"
D12-1094,D11-1135,0,0.228249,"Missing"
D12-1094,N07-1016,0,0.124788,"Missing"
D12-1094,P11-1116,1,0.779565,"ity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel & Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include NP {,} (such as) {NP,}* {and|or} NP, NP (is|are|was|were|being) (a|an|the) NP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity: To generate the pairwise similarity graph for relation phrases"
D12-1094,P09-1052,1,0.836204,"tity similarity graph We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML tag patterns include tables, dropdown boxes, etc. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hype"
D12-1094,N06-1039,0,\N,Missing
D12-1094,D09-1098,0,\N,Missing
D13-1009,P08-1019,1,0.269213,"arisons between questions and users. Our experimental results show that our model significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflow1 and Yahoo! Answers2 have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). ∗ This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Informat"
D13-1009,C12-1190,0,0.0137067,"el significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflow1 and Yahoo! Answers2 have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). ∗ This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. 1 http://stackoverflow.com 2 http:"
D13-1009,P11-1066,0,\N,Missing
D13-1159,P08-1019,1,0.824173,"e learning and retrieval. The paper proceeds as follows. We present related work in Section 2. We detail our framework to construct CETs and show empirical results in Section 3. Section 4 and Section 5 evaluate the effectiveness of CET on knowledge organization from user and system aspects respectively. We conclude the paper in Section 6. 2 Related Work UGC organization in social media. Most UGC in social media is unstructured, or organized in a predefined category hierarchy. These categories give shallow semantics of UGC, and boosts the performance of information retrieval (Cao et al., 2008; Duan et al., 2008; Cao et al., 2012) and recommendation (Guo et al., 2008; Li et al., 2011). With new content kept adding into a hierarchy, we need to maintain category hierarchy (Yuan et al., 2012) to make content within the same category 1523 more topically cohesive. Apart from category hierarchy, UGC can also be organized by topic models and tags. Topic models, such as latent Dirichlet allocation (LDA) (Blei et al., 2003), are widely applied in document clustering and classification, However, it is not trivial to control the granularity of topics (Chen et al., 2011). What’s more, it is generally difficult t"
D13-1159,P05-1045,0,0.00407909,"utilized to identify subtopics. In addition, relationships among subtopics which under the same parent are not investigated. The metro maps proposed in (Shahaf et al., 2013) are also related to our work. Different from (Shahaf et al., 2013), we employ a large-scale entity repository to extract more meaningful and interpretable key terms (entities), which make each subtopic much easier to understand. Entity extraction. In our framework, we leverage an entity repository to extract named entities from UGC. A common approach is to utilize a Named Entity Recognition (NER) system like Stanford NER (Finkel et al., 2005), which recognizes the names of things (e.g., person and product names) from texts. For cross-domain NER, (R¨ud et al., 2011) employed search engines. For short-text NER, (Liu et al., 2012) proposed a graphical model. However, most of above systems are restricted from 3 http://stackoverflow.com/ producing labels for a few entity classes. To address the problem, (Ling and Weld, 2012) defined a fine-grained set of 112 tags based on Freebase for entity extraction. However, this approach still faces the “low-recall” problem in the real world. Our approach, which leverages a large-scale entity repo"
D13-1159,P12-1055,0,0.0182847,"o our work. Different from (Shahaf et al., 2013), we employ a large-scale entity repository to extract more meaningful and interpretable key terms (entities), which make each subtopic much easier to understand. Entity extraction. In our framework, we leverage an entity repository to extract named entities from UGC. A common approach is to utilize a Named Entity Recognition (NER) system like Stanford NER (Finkel et al., 2005), which recognizes the names of things (e.g., person and product names) from texts. For cross-domain NER, (R¨ud et al., 2011) employed search engines. For short-text NER, (Liu et al., 2012) proposed a graphical model. However, most of above systems are restricted from 3 http://stackoverflow.com/ producing labels for a few entity classes. To address the problem, (Ling and Weld, 2012) defined a fine-grained set of 112 tags based on Freebase for entity extraction. However, this approach still faces the “low-recall” problem in the real world. Our approach, which leverages a large-scale entity repository, addresses this issue. Entity-based document classification and retrieval. Entity repository has been employed in other research areas, like document classification and retrieval. (S"
D13-1159,D09-1025,0,0.0226791,"the approach in (Shi et al., 2010), which estimates the similarity of two terms according to their firstorder and second-order co-occurrences. For example, “such as NP, NP” is a good pattern for detecting similar entities using first-order co-occurrences. In addition, if two entities usually co-occur with a third entity (second-order co-occurrence), these two entities are likely to be similar. To construct similarity functions, pattern-based approaches (Ohshima et al., 2006; Zhang et al., 2009) utilize first-order co-occurrences while distributional similarity approaches (Pasca et al., 2006; Pennacchiotti and Pantel, 2009) employ second-order co-occurrences. In the following, we briefly introduce the patternbased approach (PB) and the distributional similarity approach (DS) in (Shi et al., 2010). PB. Some well-designed patterns are leveraged to extract similar entities from a huge repository of webpages. The set of terms extracted by applying a pattern one time is called a raw semantic class (RASC). Given two entities ta and tb , PB calculates their similarity based on the number of RASCs containing both of them (Zhang et al., 2009): Sim(ta , tb ) = log(1 + rab X Pabi )) · p idf (ta ) · idf (tb ), (2) i=1 N whe"
D13-1159,P11-1097,0,0.021511,"Missing"
D13-1159,C10-1112,0,0.0629132,"cluster index and corresponding similarity in the candidate list. • AC-AVG: If the mean similarity between entity ei and any entity ej in one of the clusters is larger than threshold θavg , we put the cluster index and corresponding similarity in the candidate list. 3. If the candidate list is not empty, put ei in the cluster with highest similarly. 4. If the candidate list is empty, a new cluster with ei as the element will be created. 5. Stop when all entities are clustered. Similarity Function. In our entity repository, the similarity between two entities is computed using the approach in (Shi et al., 2010), which estimates the similarity of two terms according to their firstorder and second-order co-occurrences. For example, “such as NP, NP” is a good pattern for detecting similar entities using first-order co-occurrences. In addition, if two entities usually co-occur with a third entity (second-order co-occurrence), these two entities are likely to be similar. To construct similarity functions, pattern-based approaches (Ohshima et al., 2006; Zhang et al., 2009) utilize first-order co-occurrences while distributional similarity approaches (Pasca et al., 2006; Pennacchiotti and Pantel, 2009) emp"
D13-1159,D12-1116,0,0.0187407,"of 112 tags based on Freebase for entity extraction. However, this approach still faces the “low-recall” problem in the real world. Our approach, which leverages a large-scale entity repository, addresses this issue. Entity-based document classification and retrieval. Entity repository has been employed in other research areas, like document classification and retrieval. (Schonhofen, 2006) utilized Wikipedia to classify documents. (Yerva et al., 2012a) proposed an entity-based classification for tweets. In addition, entity-based retrieval models were proposed and applied in both QA archives (Singh, 2012a) and tweets (Yerva et al., 2012b). Besides, (Singh, 2012b) proposed an entitybased translation language model and demonstrated that it outperformed classical translation language model in question retrieval. However, to the best of our knowledge, no previous study leverages entities to organize UGC in social media. 3 CET Construction In this section, we formulate the framework to construct CET and show the empirical results. Firstly, we provide the definitions of the entity repository and CET. Definition 3.1 (Entity Repository) Let ER = {R, g} be an entity repository, where R is a set of nam"
D13-1159,P09-1052,0,0.0265635,"ies are clustered. Similarity Function. In our entity repository, the similarity between two entities is computed using the approach in (Shi et al., 2010), which estimates the similarity of two terms according to their firstorder and second-order co-occurrences. For example, “such as NP, NP” is a good pattern for detecting similar entities using first-order co-occurrences. In addition, if two entities usually co-occur with a third entity (second-order co-occurrence), these two entities are likely to be similar. To construct similarity functions, pattern-based approaches (Ohshima et al., 2006; Zhang et al., 2009) utilize first-order co-occurrences while distributional similarity approaches (Pasca et al., 2006; Pennacchiotti and Pantel, 2009) employ second-order co-occurrences. In the following, we briefly introduce the patternbased approach (PB) and the distributional similarity approach (DS) in (Shi et al., 2010). PB. Some well-designed patterns are leveraged to extract similar entities from a huge repository of webpages. The set of terms extracted by applying a pattern one time is called a raw semantic class (RASC). Given two entities ta and tb , PB calculates their similarity based on the number of"
D14-1087,C92-2082,0,0.213197,"ightly different from query template construction. First, some useful features such as query click-through is not available in category template construction. Second, categories should be valid natural language phrases, while queries need not. For example, “city Germany” is a query but not a valid category name. We discuss in more details in the related work section. Our major contributions are as follows. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”"
D14-1087,P08-1119,0,0.0413024,"Missing"
D14-1087,P10-1136,0,0.0325053,"m to group existing query templates by search intents of users. Compared to the open-domain unsupervised methods for query template construction, our approach improves on two aspects. First, we propose to incorporate multiple types of semantic knowledge (e.g., term peer similarity and term clusters) to improve template generation. Second, we propose a nonlinear template scoring function which is demonstrated to be more effective. Query tagging/labeling: Some research work in recent years focuses on segmenting web search queries and assigning semantic tags to key segments. Li et al. (2009) and Li (2010) employed CRF (Conditional Random Field) or semi-CRF models for query tagging. A crowdsourcingassisted method was proposed by Han et al. (2013) for query structure interpretation. These supervised or semi-supervised approaches require much manual annotation effort. Unsupervised methods were proposed by Sarkas et al. (2010) and Reisinger and Pasca (2011). As been discussed in the introduction section, query tagging is only one of the two stages of template generation. The tagging results are for one query only, without aggregating the global information of all queries to generate the final temp"
D14-1087,N09-1003,0,0.0596501,"Missing"
D14-1087,P11-1098,0,0.034426,"d ours is that we automatically assign semantic types to the pattern variables (or called arguments) while they do not. 800 Template mining for IE: Some research work in information extraction (IE) involves patterns. Yangarber (2003) and Stevenson and Greenwood (2005) proposed to learn patterns which were in the form of [subject, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Li et al. (2013) proposed an clustering algorithm to group existing query templates by search intents of users. Compared to the open-domain unsupervised methods for query template construction, our approach improves on two aspects. First, we propose to incorporate multiple types of semantic knowledge (e.g., term peer similarity and term clusters) to improve template generation. Second, we propose a nonlinear template scoring function which is demonstrated to be"
D14-1087,N04-1041,0,0.295994,"nt from query template construction. First, some useful features such as query click-through is not available in category template construction. Second, categories should be valid natural language phrases, while queries need not. For example, “city Germany” is a query but not a valid category name. We discuss in more details in the related work section. Our major contributions are as follows. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010)"
D14-1087,P11-1120,0,0.0148478,"d, we propose a nonlinear template scoring function which is demonstrated to be more effective. Query tagging/labeling: Some research work in recent years focuses on segmenting web search queries and assigning semantic tags to key segments. Li et al. (2009) and Li (2010) employed CRF (Conditional Random Field) or semi-CRF models for query tagging. A crowdsourcingassisted method was proposed by Han et al. (2013) for query structure interpretation. These supervised or semi-supervised approaches require much manual annotation effort. Unsupervised methods were proposed by Sarkas et al. (2010) and Reisinger and Pasca (2011). As been discussed in the introduction section, query tagging is only one of the two stages of template generation. The tagging results are for one query only, without aggregating the global information of all queries to generate the final templates. The goal of this paper is to construct a list of category templates from a collection of open-domain category names. Input: The input is a collection of category names, which can either be manually compiled (like Wikipedia categories) or be automatically extracted. The categories used in our experiments were automatically mined from the web, by f"
D14-1087,C10-1112,1,0.8823,"Missing"
D14-1087,P05-1047,0,0.0246128,"and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to find lexical patterns in category names to define axioms (in medical domain). One example pattern mentioned in their papers is “[X] binding”. They need manual intervention to determine what X means. The main difference between the above work and ours is that we automatically assign semantic types to the pattern variables (or called arguments) while they do not. 800 Template mining for IE: Some research work in information extraction (IE) involves patterns. Yangarber (2003) and Stevenson and Greenwood (2005) proposed to learn patterns which were in the form of [subject, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Li et al. (2013) proposed an clustering algorithm to group existing query templates by search intents of u"
D14-1087,W12-1511,0,0.0145865,"tions are as follows. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to find lexical patterns in category names to define axioms (in medical domain). One example pattern mentioned in their papers is “[X] binding”. They need manual intervention to determine what X means. The main difference between the above work and ours is that we automatically assign semantic types to the patte"
D14-1087,P03-1044,0,0.0515478,"nior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to find lexical patterns in category names to define axioms (in medical domain). One example pattern mentioned in their papers is “[X] binding”. They need manual intervention to determine what X means. The main difference between the above work and ours is that we automatically assign semantic types to the pattern variables (or called arguments) while they do not. 800 Template mining for IE: Some research work in information extraction (IE) involves patterns. Yangarber (2003) and Stevenson and Greenwood (2005) proposed to learn patterns which were in the form of [subject, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Li et al. (2013) proposed an clustering algorithm to group existing que"
D14-1087,P11-1116,1,0.786456,"atures such as query click-through is not available in category template construction. Second, categories should be valid natural language phrases, while queries need not. For example, “city Germany” is a query but not a valid category name. We discuss in more details in the related work section. Our major contributions are as follows. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to f"
D14-1087,D09-1098,0,\N,Missing
D14-1213,P12-2012,1,0.920034,"k three judges, fluent in English and graduate students/researchers, to annotate each tweet with the level of self-disclosure. Judges first read and discussed the definitions and examples of self-disclosure level shown in (Barak and Gluck-Ofri, 2007), then they worked separately on a Web-based platform. As a result of annotation, there are 122 G level converstaions, 147 M level and 32 H level con3 https://dev.twitter.com/docs/api/ streaming 4 https://github.com/shuyo/ldig 1990 • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a SD level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories5 . Used in previous work (Houghton and Joinson, 2012). • Bag of Words + Bigrams + Trigrams (BOW+): A bag of words, bigram and trigram features. We exclude features that appear only once or twice. Figure 4: Screenshot of annotation web-based platform. Annotators read a Twitter conversation and annotate self-disclosure level to each tweet. • Se"
D14-1213,N10-1020,0,0.0282212,"g(c, t, l , k ) = P 0 −(ct) l 0 V Γ( v=1 βvl + nk0 v + mctk0 (·) ) ! V 0 −(ct) 0 l0 (−ct) l Y Γ(βvl + n 0 αk0 + nck0 + mctk0 v ) kv . PK 0 −(ct) 0 l 0 l Γ(βvl + nk0 v ) k=1 αk + nck v=1 0 4 Users 101,686 Classifying M vs H levels 0 Γ( Data Collection and Annotation To test our self-disclosure topic model, we use a large dataset of conversations consisting of Tweets over three years such that we can analyze the relationship between self-disclosure behavior and conversation frequency and length over time. We chose to crawl Twitter because it offers a practical and large source of conversations (Ritter et al., 2010). Others have also analyzed Twitter conversations for natural language and social media Dyads 61,451 Conv’s 1,956,993 Tweets 17,178,638 Table 5: Dataset of Twitter conversations. We chose conversations consisting of five or more tweets each. We chose dyads with twenty or more conversations. research (boyd et al., 2010; Danescu-NiculescuMizil et al., 2011), but we collect conversations from the same set of dyads over several months for a unique longitudinal dataset. We also make sure that each conversation is at least five tweets, and that each dyad has at least twenty conversations. 4.1 Collec"
D14-1213,J04-3002,0,0.0115544,"effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and self-disclosure for high accuracy in classifying the three levels of self-disclosure. Subjectivity which is aspect of expressing opinions (Pang and Lee, 2008; Wiebe et al., 2004) is related with self-disclosure, but they are different dimensions of linguistic behavior. Because there indeed are many high self-disclosure tweets that are subjective, but there are also counter examples in annotated dataset. The tweet “England manager is Roy Hodgson.” is low self-disclosure and low subjectivity, “I have barely any hair left.” is high self-disclosure but low subjectivity, and “Senator stop lying!” is low self-disclosure but high subjectivity. cial psychology research with more robust results from a large-scale dataset, and show the effectiveness of computationally analyzing"
D14-1213,D10-1006,0,0.0130624,"rts, and the first part classifies G vs. M/H levels with first-person pronouns (I, my, me). In the graphical model, y is the latent variable that represents this classification, and ω is the distribution over y. x is the observation of the firstperson pronoun in the tweets, and λ are the parameters learned from the maximum entropy classifier. With the annotated Twitter conversation dataset (described in Section 4.2), we experimented with several classifiers (Decision tree, Naive Bayes) and chose the maximum entropy classifier because it performed the best, similar to other joint topic models (Zhao et al., 2010; Mukherjee et al., 2013). 1989 3.3 The second part of the classification, the M and the H level, is driven by informative priors with seed words and seed trigrams. In the graphical model, r is the latent variable that represents this classification, and π is the distribution over r. γ is a non-informative prior for π, and β l is an informative prior for each SD level by seed words. For example, we assign a high value for the seed word ‘acne’ for β H , and a low value for ‘My name is’. This approach is the same as joint models of topic and sentiment (Jo and Oh, 2011; Kim et al., 2013). 3.4 Inf"
D14-1213,P13-1165,0,0.0240036,"part classifies G vs. M/H levels with first-person pronouns (I, my, me). In the graphical model, y is the latent variable that represents this classification, and ω is the distribution over y. x is the observation of the firstperson pronoun in the tweets, and λ are the parameters learned from the maximum entropy classifier. With the annotated Twitter conversation dataset (described in Section 4.2), we experimented with several classifiers (Decision tree, Naive Bayes) and chose the maximum entropy classifier because it performed the best, similar to other joint topic models (Zhao et al., 2010; Mukherjee et al., 2013). 1989 3.3 The second part of the classification, the M and the H level, is driven by informative priors with seed words and seed trigrams. In the graphical model, r is the latent variable that represents this classification, and π is the distribution over r. γ is a non-informative prior for π, and β l is an informative prior for each SD level by seed words. For example, we assign a high value for the seed word ‘acne’ for β H , and a low value for ‘My name is’. This approach is the same as joint models of topic and sentiment (Jo and Oh, 2011; Kim et al., 2013). 3.4 Inference For posterior infe"
D14-1213,N13-1039,0,0.0191267,"nd contains one or more questions or comments before or after the self-disclosure tweet. We compare SDTM with the following methods for classifying conversations for SD level: • ASUM (Jo and Oh, 2011): A joint model of sentiments and topics. We map each SD level to one sentiment and use the same seed words/trigrams from S ECRET as in SDTM below. Used in previous work (Bak et al., 2012). • First-person pronouns (FirstP): Occurrence of first-person pronouns which are described in section 3.2. To identify first-person pronouns, we tagged parts of speech in each tweet with the Twitter POS tagger (Owoputi et al., 2013). • First-person pronouns + Seed words/trigrams (FP+SE1): First-person pronouns and seed words/trigrams from S ECRET. • Two stage classifier with First-person pronouns + Seed words/trigrams (FP+SE2): A 5 personal pronouns, 3rd person singular words, family words, human words, sexual words, etc 1991 Method LDA MedLDA LIWC BOW+ SEED ASUM SDTM− FirstP FP+SE1 FP+SE2 SDTM Acc 49.2 43.3 49.2 54.1 54.4 56.6 60.4 63.2 61.0 60.4 64.5 G F1 0.00 0.41 0.34 0.50 0.52 0.32 0.57 0.63 0.61 0.64 0.61 M F1 0.65 0.52 0.61 0.59 0.60 0.70 0.70 0.69 0.67 0.69 0.71 H F1 0.05 0.09 0.18 0.15 0.14 0.38 0.14 0.10 0.16 0"
D15-1104,E06-1002,0,0.0254374,"It increases the problem complexity, is usually inefficient, and 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linking usually takes mentions detected by NER as its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambigua"
D15-1104,D07-1074,0,0.0356332,"complexity, is usually inefficient, and 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linking usually takes mentions detected by NER as its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambiguation Challenge” w"
D15-1104,doddington-etal-2004-automatic,0,0.0900805,"while in their model the confidence of outputs is lost in the linking phrase. Furthermore, in our model NER can naturally benefit from entity linking’s decision since both decisions are made together, while in their model, it is not clear how the linking decision can help the NER decision in return. 2 Related Work The NER problem has been widely addressed by symbolic, statistical, as well as hybrid approaches. It has been encouraged by several editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC Joint optimization is costly. It increases the problem complexity, is usually inefficient, and 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millio"
D15-1104,N09-1037,0,0.0127338,"Missing"
D15-1104,P05-1045,0,0.286104,"rn. 2 Related Work The NER problem has been widely addressed by symbolic, statistical, as well as hybrid approaches. It has been encouraged by several editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC Joint optimization is costly. It increases the problem complexity, is usually inefficient, and 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikiped"
D15-1104,P11-1138,0,0.0212791,"Missing"
D15-1104,P11-1095,0,0.0147547,"Missing"
D15-1104,N03-1028,0,0.0179935,"the knowledge base, linking system should return a special identifier “NIL”. 882 The gradient of L(T , w) is derived as, where L is the max segmentation length in SemiCRF, and Yje∗ is all valid assignments for yje P|Es |e = 1. The ψuner which satisfies k=0j yj,k j ,vj ,yj ,yj−1 and ψuel.cr are precomputed ahead as below, e j ,vj ,yj ,y ∂L X (G(xt , at ) = ∂w t X w (2) − G(xt , a0 )P (a0 |xt , w)) − 2 σ 0 j ψuner = ew j ,vj ,yj ,yj−1 a As shown in Figure 1, our model’s factor graph is a tree, which means the calculation of the gradient is tractable. Inspired by the forward backward algorithm (Sha and Pereira, 2003) and Semi-CRF (Sarawagi and Cohen, 2004), we leverage dynamic programming techniques to compute the normalization factor Zw and marginal probability P (a0j |xt , w) when w is given.(Sutton and McCallum, 2006) The parameter estimation algorithm is abstracted in Algorithm 1. e ψuel.cr j ,vj ,yj ,yj Zw (x) and P (aj |xt , w) = P (sj , yj |x, w) P ( yje ∈Yje∗ ψuel.cr e j ,vj ,yj ,y j y0e ∈Yje∗ 3.3 ψuel.cr e j ,vj ,yj ,y0 (5) Inference Given a new word sequence x and model weights w trained on a training set, the goal of inference is to find the best assignment, a∗ = argmaxa P (a|x, w) for x. We ex"
D15-1104,D11-1072,0,0.145004,"Missing"
D15-1104,W12-0508,0,0.0594618,"Missing"
D15-1104,W08-2121,0,0.0119505,"Missing"
D15-1104,P09-1116,0,0.0208154,"2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC Joint optimization is costly. It increases the problem complexity, is usually inefficient, and 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linki"
D15-1104,W03-0419,0,0.899416,"Missing"
D15-1104,W12-6327,0,0.015995,"in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linking usually takes mentions detected by NER as its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambiguation Challenge” which focused on the end to end performance of linking system 2 . Joint optimization models have been studied at great length. E.g. Dynamic CRF (McCallum et al., 2003) has been proposed to conduct Partof-Speech Tagging and Chunking tasks together. Finkel and Manning (2009) show how to model parsing an"
D15-1104,W09-1119,0,0.973501,"raged by several editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC Joint optimization is costly. It increases the problem complexity, is usually inefficient, and 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population t"
D15-1135,S07-1018,0,0.0121345,"e object. Liguda & Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars & Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee & Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to com"
D15-1135,W13-2322,0,0.0220911,"w semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 Approach Consider the first problem in Figure 1 (written below for convenience), One number is 16 more than another. If the smaller number is subtracted from 2/3 of the larger, the result is 1/4 of the sum of the two numbers. Find the numbers. To automatically solve this problem, th"
D15-1135,D13-1160,0,0.00887979,"ing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 A"
D15-1135,P14-1133,0,0.0147107,"mantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 Approach Consider the first problem in Figure 1 (w"
D15-1135,P13-1042,0,0.00942264,"much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math"
D15-1135,W04-2412,0,0.0108717,"on of the information gathered for one object. Liguda & Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars & Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee & Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. S"
D15-1135,J07-4004,0,0.0126457,"Missing"
D15-1135,P81-1022,0,0.747028,"classes from Freebase types and isa extraction results. We have over 50 manually defined math-related verb functions. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury & Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. 3.2.2 Parsing Parsing for CFG is a well-studied topic with lots of algorithms invented (Kasami, 1965; Earley, 1970). The core idea behind almost all the algorithms is exploiting dynamic programming to achieve efficient search through the space of possible parse trees. For syntactic parsing, a wellknown serious problem is ambiguity: the appearance of many syntactically correct but semantically unreasonable parse trees. Modern syntactic parsers reply on statistical information to reduce 4 VerbNet: http://verbs.colorado.edu/~mpalmer/projects/verbnet.html 1137 ambiguity. They are often based on probabilistic CFGs (PCFGs) or probabilistic lexicalized CFGs trained on hand-labeled TreeBanks. With the new set of D"
D15-1135,C92-2082,0,0.12496,"re manually built by referring to text books and online tu1136 torials. About 35 classes and 200 functions are obtained in this way. Additional instances of each element type are constructed in the ways below. Classes: Additional classes and grammar rules are obtained from two data sources: Freebase 3 types, and automatically extracted lexical semantic data. By treating Freebase types as DOL classes and the mapping from types to lexical names as grammar rules, we get the first version of grammar for classes. To improve coverage, we run a term peer similarity and hypernym extraction algorithm (Hearst, 1992; Shi et al., 2010; Zhang et al., 2011) on a web snapshot of 3 billion pages, and get a peer-similarity graph and a collection of is-a pairs. An is-a pair example is (Megan Fox, actress), where “Megan Fox” and “actress” are instance and type names respectively. In our peer similarity graph, “Megan Fox” and “Britney Spears” have a high similarity score. The peer similarity graph is used to clean the is-a data collection (with the idea that peer terms often share some common type names). Given the cleaned isa data, we sort the type names by weight and manually create classes for top-1000 type na"
D15-1135,kingsbury-palmer-2002-treebank,0,0.0365188,"$2) → {$2} {$1} mf.number.even → even mf.condition.if($1) → if {$1} mf.approximately → approximately |roughly education.university → university math.number → number math.integer → integer 3 Functions: Additional noun functions are automatically created from Freebase properties and attribute extraction results (Pasca et al., 2006; Durme et al., 2008), using a similar procedure with creating classes from Freebase types and isa extraction results. We have over 50 manually defined math-related verb functions. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury & Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. 3.2.2 Parsing Parsing for CFG is a well-studied topic with lots of algorithms invented (Kasami, 1965; Earley, 1970). The core idea behind almost all the algorithms is exploiting dynamic programming to achieve efficient search through the space of possible parse trees. For syntactic parsing, a wellknown serious problem is ambiguity:"
D15-1135,P14-1026,0,0.352942,"hool-level math word problems (i.e., math problems described in natural language). Efforts to automatically solve math word problems date back to the 1960s (Bobrow, 1964a, b). Previous work on this topic falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a, b; Charniak, 1968; Bakman, 2007; Liguda & Pfeiffer, 2012), math problem sentences are transformed to certain structures by pattern matching or verb categorization. Equations are then derived from the structures. Statistical learning methods are employed in two recent papers (Kushman et al., 2014; Hosseini et al., 2014). ______________________________________ * Work done while this author was an intern at Microsoft Research Figure 1: Number word problem examples In this paper, we present a computer system called SigmaDolphin which automatically solves math word problems by semantic parsing and reasoning. We design a meaning representation language called DOL (abbreviation of dolphin language) as the structured semantic representation of NL text. A semantic parser is implemented to transform math problem text into DOL trees. A reasoning module is included to derive math expressions fro"
D15-1135,D13-1161,0,0.0108997,"cture of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 Approach Consider the first"
D15-1135,W04-0902,0,0.201286,"Missing"
D15-1135,J08-2001,0,0.0116413,"mation gathered for one object. Liguda & Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars & Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee & Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based"
D15-1135,P07-1121,0,0.0293539,"rsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representatio"
D15-1135,D07-1071,0,0.0230829,".wolframalpha.com Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the"
D15-1135,P11-1116,1,0.919889,"Missing"
D15-1135,C10-1112,1,\N,Missing
D15-1135,C08-1116,0,\N,Missing
D15-1135,D14-1058,0,\N,Missing
D15-1135,J14-1002,0,\N,Missing
D15-1135,J02-3001,0,\N,Missing
D17-1084,D14-1058,0,0.10866,"[NUM]” to (1 − n1 ) ∗ n2 = x. In this way, we can decompose the templates and learn math concepts in a finer grain. Furthermore, we observe that problems of the same template share some common properties. By aggregating problems of the same template and 806 struction. their pre-defined semantic language. However, these methods are only effective in their designated math problem categories and are not scalable to other categories. For example, the method used by Shi et al. (2015) works extremely well for solving number word problems but not others. In the statistical machine learning approach, Hosseini et al. (2014) solves addition and subtraction problems by extracting quantities as states and derive math concepts from verbs in the training data. Kushman et al. (2014) and Zhou et al. (2015) generalize equations attached to problems with variable slots and number slots. They learn a probabilistic model for finding the best solution equation. Upadhyay et al. (2016) follows their approach and leverage math word problems without equation annotation as external resources. Seo et al. (2015) solves a set of SAT geometry questions with text and diagram provided. KoncelKedziorski et al. (2015) and Roy and Roth ("
D17-1084,P16-1084,1,0.338646,"near equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mitra and Baral (2016) considers addition and subtraction problems in three basic problem types: “Change”, “Part Whole” and “Comparison”. They manually design different features for each type, which is difficult to expand to more types. In summary, previous methods can achieve high accuracy in limited math problem categories, (i.e. (Kushman et al., 2014; Shi et al., 2015)), but do not scale or perform well in datasets containing various math problem types as in Huang et al. (2016), as their designed features are becoming sparse. Their process of acquiring mathematical knowledge is either sparse or based on certain assumptions of specific problem types. To alleviate this problem, we introduce our template sketch construction and fine-grained expressions learning in the next section. 3 3.1 Definition Template: It is first introduced in Kushman et al. (2014). It is a unique form of an equation system. For example, given an equation system as follows: 2 · x1 + 4 · x2 = 34 x1 + 4 = x2 This equation system is a solution for a specific math word problem. We replace the number"
D17-1084,Q15-1042,0,0.684985,"Missing"
D17-1084,P14-1026,0,0.568474,"osseini et al., 2014; Roy and Roth, 2015; KoncelKedziorski et al., 2015) derive math concepts based on observations from their dataset of specific types of problems, e.g. problems with one single equation. For example, Hosseini et al. (2014) assumes verbs and only verbs embed math concepts and map them to addition/subtraction. Roy and Roth (2015); Koncel-Kedziorski et al. (2015) assume there is only one unknown variable in the problem and cannot derive math concepts involving constants or more than one unknown variables, such as “the product of two unknown numbers”. Template-based approaches (Kushman et al., 2014; Zhou et al., 2015; Upadhyay et al., 2016), on the other hand, leverage the built-in composition structure of equation system templates to formulate all types of math concepts seen in training data, such as (1 − n1 ) ∗ n2 = x in Figure 1. However, they suffer from two major shortcomings. First, the math concepts they learned, which is expressed as an entire template, fails to capture a lot of useful information with sparse training instances. We argue that it would be more expressive if the math concept is learned in a finer granularity. Second, their learning processes rely heavily on lexica"
D17-1084,P16-1202,0,0.450844,"ions attached to problems with variable slots and number slots. They learn a probabilistic model for finding the best solution equation. Upadhyay et al. (2016) follows their approach and leverage math word problems without equation annotation as external resources. Seo et al. (2015) solves a set of SAT geometry questions with text and diagram provided. KoncelKedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mitra and Baral (2016) considers addition and subtraction problems in three basic problem types: “Change”, “Part Whole” and “Comparison”. They manually design different features for each type, which is difficult to expand to more types. In summary, previous methods can achieve high accuracy in limited math problem categories, (i.e. (Kushman et al., 2014; Shi et al., 2015)), but do not scale or perform well in datasets containing various math problem types as in Huang et al. (2016), as their designed features are becoming sparse. Their process of acquiring mathematical knowledge is either sparse or based on certain"
D17-1084,D15-1202,0,0.413901,"et al. (2014) solves addition and subtraction problems by extracting quantities as states and derive math concepts from verbs in the training data. Kushman et al. (2014) and Zhou et al. (2015) generalize equations attached to problems with variable slots and number slots. They learn a probabilistic model for finding the best solution equation. Upadhyay et al. (2016) follows their approach and leverage math word problems without equation annotation as external resources. Seo et al. (2015) solves a set of SAT geometry questions with text and diagram provided. KoncelKedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mitra and Baral (2016) considers addition and subtraction problems in three basic problem types: “Change”, “Part Whole” and “Comparison”. They manually design different features for each type, which is difficult to expand to more types. In summary, previous methods can achieve high accuracy in limited math problem categories, (i.e. (Kushman et al., 2014; Shi et al., 2015)), but do not scale or perform well"
D17-1084,D15-1171,0,0.0211217,"works extremely well for solving number word problems but not others. In the statistical machine learning approach, Hosseini et al. (2014) solves addition and subtraction problems by extracting quantities as states and derive math concepts from verbs in the training data. Kushman et al. (2014) and Zhou et al. (2015) generalize equations attached to problems with variable slots and number slots. They learn a probabilistic model for finding the best solution equation. Upadhyay et al. (2016) follows their approach and leverage math word problems without equation annotation as external resources. Seo et al. (2015) solves a set of SAT geometry questions with text and diagram provided. KoncelKedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mitra and Baral (2016) considers addition and subtraction problems in three basic problem types: “Change”, “Part Whole” and “Comparison”. They manually design different features for each type, which is difficult to expand to more types. In summary, previous methods can achieve high"
D17-1084,D15-1135,1,0.530141,"ncelKedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mitra and Baral (2016) considers addition and subtraction problems in three basic problem types: “Change”, “Part Whole” and “Comparison”. They manually design different features for each type, which is difficult to expand to more types. In summary, previous methods can achieve high accuracy in limited math problem categories, (i.e. (Kushman et al., 2014; Shi et al., 2015)), but do not scale or perform well in datasets containing various math problem types as in Huang et al. (2016), as their designed features are becoming sparse. Their process of acquiring mathematical knowledge is either sparse or based on certain assumptions of specific problem types. To alleviate this problem, we introduce our template sketch construction and fine-grained expressions learning in the next section. 3 3.1 Definition Template: It is first introduced in Kushman et al. (2014). It is a unique form of an equation system. For example, given an equation system as follows: 2 · x1 + 4 ·"
D17-1084,D16-1029,0,0.685273,"KoncelKedziorski et al., 2015) derive math concepts based on observations from their dataset of specific types of problems, e.g. problems with one single equation. For example, Hosseini et al. (2014) assumes verbs and only verbs embed math concepts and map them to addition/subtraction. Roy and Roth (2015); Koncel-Kedziorski et al. (2015) assume there is only one unknown variable in the problem and cannot derive math concepts involving constants or more than one unknown variables, such as “the product of two unknown numbers”. Template-based approaches (Kushman et al., 2014; Zhou et al., 2015; Upadhyay et al., 2016), on the other hand, leverage the built-in composition structure of equation system templates to formulate all types of math concepts seen in training data, such as (1 − n1 ) ∗ n2 = x in Figure 1. However, they suffer from two major shortcomings. First, the math concepts they learned, which is expressed as an entire template, fails to capture a lot of useful information with sparse training instances. We argue that it would be more expressive if the math concept is learned in a finer granularity. Second, their learning processes rely heavily on lexical and syntactic features, such as the depen"
D17-1084,D15-1096,0,0.500547,"Missing"
D18-1411,D10-1049,0,0.152059,"ddon et al., 2016; Murakami et al., 2017; Wiseman et al., 2017), without establishing underlying semantic correspondences. Texts generated thereby can be fluent but not conforming to the input data, unlike templatebased approaches where lexical choices could be directly controlled. In our work, we find the derived semantic correspondences between data and texts to be useful for template induction, either with simple heuristics to automatically extract description patterns (how to say) and corresponding triggers (what & when to say), or with more crafted discriminative learning approaches (cf. Angeli et al. (2010)). 3 Technical overview Task Let S be the set of all world states, W be the set of all texts, O be the set of all executable operators, and V be the output space of O. A world state s ∈ S is a table storing some information, or more specifically in this work, a tabular recording for a sports game. An operator o ∈ O can be executed on a world state to retrieve values, i.e., each o could be treated as a mapping of S → V. The result of an operation can be a string, a continuous values or a discrete value. Meanwhile, each world state s is accompanied with a piece of description w ∈ W. Here w consi"
D18-1411,N10-1083,0,0.0282992,"Missing"
D18-1411,J93-2003,0,0.133568,"is rather simple: For each sentence, we “encourage” at least a proportion of words to be aligned to NULL labels: Skipping null labels Preliminary experiments suggest that the initial model have too many words assigned to the NULL tag. Informative alignments may not be adjacent, which breaks the simplest Markov assumptions. In our model, the transition score of two non-NULL labels can be calculated by skipping all the NULLs in between, as shown in Figure 3. This is implemented without breaking the overall Markov property with the following trick used in earlier work on statistical alignments (Brown et al., 1993): Suppose we have m labels (i.e., m latent states), we can design m different NULL labels that share the same emission score, while preserving their original outward-transition probabilities. The types of NULL labels are inherited from the previous label. This might seem to be wasteful at first sight as we use two-fold latent states, but the Markov property is successfully preserved, therefore simplifying our implementation. 4.5 from statistical alignment (Brown et al., 1993), these words tend to be aligned to some irrelevant fields in the table which are rarely mentioned. We address this issu"
D18-1411,D16-1032,0,0.0185467,"r weak and distant form of supervision from paired tables and texts without annotations for fine-grained alignments between phrases and data cells. Similar modeling and learning strategies could potentially be useful for considerably large tag space derived from structured knowledge bases in the future (Choi et al., 2018). The feasibility of this work is partly due to the availability of data, mostly comes from the field of data-to-text language generation. Related work in data-to-text generation mainly focused on directly generating summary descriptions for structured data (Mei et al., 2016; Kiddon et al., 2016; Murakami et al., 2017; Wiseman et al., 2017), without establishing underlying semantic correspondences. Texts generated thereby can be fluent but not conforming to the input data, unlike templatebased approaches where lexical choices could be directly controlled. In our work, we find the derived semantic correspondences between data and texts to be useful for template induction, either with simple heuristics to automatically extract description patterns (how to say) and corresponding triggers (what & when to say), or with more crafted discriminative learning approaches (cf. Angeli et al. (20"
D18-1411,D14-1043,0,0.642868,"elds. Consider the example description for a basketball game shown in Figure 1. The phrase edged out in the first sentence implies the fact that the Toronto Raptors had beaten their opponent by a relatively narrow margin. This could only be derived from an operation of subtraction between two scores that correspond to the field PTS for both teams in the event table, which leads to a relatively small difference of only four points. Previous efforts on learning semantic correspondences relying on categorical distributions (Liang et al., 2009) or string pattern features (Hajishirzi et al., 2012; Koncel-Kedziorski et al., 2014) do not have the capability to accurately capture numerical information, especially for the part that does not appear explicitly in the table and needs to be inferred. 3761 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3761–3771 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Such kind of language grounding is important both for natural language understanding and for natural language generation. For language understanding, establishing explicit connections between symbols and values beyond ungrou"
D18-1411,D17-1160,0,0.0166601,"ed on reducing the amount of supervision required (Artzi and Zettlemoyer, 3762 2013). The intuition behind weakly supervised executable semantic parsing is that once the latent semantic representation has been executed, one could test whether the execution results could match the information with available weak supervision signals such as answers to natural language queries (Clarke et al., 2010; Liang et al., 2011), or task completion from instructional navigations (Misra et al., 2017). Such formulations have been adapted for question answering over structured tables (Pasupat and Liang, 2015; Krishnamurthy et al., 2017). However, the current research focus is to convert a natural language question into executable table queries and to directly retrieve results. They do not have the need of inference involving numerical commonsense implied by various lexical patterns. A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013) but only specific to translating language into queries in the highly structured database and cannot be applied to our domain. Our approach is implemented as assigning tag annotations over text spans, which is conceptually related to fine-grained named entity tagging (Ling a"
D18-1411,P18-1009,0,0.0283712,"ting language into queries in the highly structured database and cannot be applied to our domain. Our approach is implemented as assigning tag annotations over text spans, which is conceptually related to fine-grained named entity tagging (Ling and Weld, 2012). Our setting only requires a rather weak and distant form of supervision from paired tables and texts without annotations for fine-grained alignments between phrases and data cells. Similar modeling and learning strategies could potentially be useful for considerably large tag space derived from structured knowledge bases in the future (Choi et al., 2018). The feasibility of this work is partly due to the availability of data, mostly comes from the field of data-to-text language generation. Related work in data-to-text generation mainly focused on directly generating summary descriptions for structured data (Mei et al., 2016; Kiddon et al., 2016; Murakami et al., 2017; Wiseman et al., 2017), without establishing underlying semantic correspondences. Texts generated thereby can be fluent but not conforming to the input data, unlike templatebased approaches where lexical choices could be directly controlled. In our work, we find the derived seman"
D18-1411,P09-1011,0,0.276231,"s be accompanied by a context. Grounded language acquisition aims at learning the meaning of language in the context of an observed world state. A solution framework typically addresses the following subproblems: segmenting the text into meaningful phrasal units, determining which world state information is being referred to, and finding proper alignments from these units to the events of values in the world state. The task has attracted much attention from the NLP community with a special focus on aligning text descriptions onto processed, structured event records (Snyder and Barzilay, 2007; Liang et al., 2009; Hajishirzi et al., 2011). Various statistical models have been proposed, attempting at ∗ Contribution during internship at Microsoft Research Asia. 1 Our implementation is available at https: //github.com/hiaoxui/D2T-Grounding. characterizing the interaction between text spans and categorical values (e.g., direction=‘East’) or strings (e.g., person names). The previously addressed term semantic correspondences narrowly describes the process of aligning natural language spans to different data fields. However, there still exists a gap between alignment results and the underlying semantics. Pe"
D18-1411,P15-2019,0,0.0609053,"Missing"
D18-1411,P11-1060,0,0.0193656,"rsing started from fully supervised training with annotated meaning representations available (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 3762 2013). The intuition behind weakly supervised executable semantic parsing is that once the latent semantic representation has been executed, one could test whether the execution results could match the information with available weak supervision signals such as answers to natural language queries (Clarke et al., 2010; Liang et al., 2011), or task completion from instructional navigations (Misra et al., 2017). Such formulations have been adapted for question answering over structured tables (Pasupat and Liang, 2015; Krishnamurthy et al., 2017). However, the current research focus is to convert a natural language question into executable table queries and to directly retrieve results. They do not have the need of inference involving numerical commonsense implied by various lexical patterns. A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013) but only specific to translating language into queries in the hig"
D18-1411,W10-2903,0,0.0348332,"on. Early semantic parsing started from fully supervised training with annotated meaning representations available (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 3762 2013). The intuition behind weakly supervised executable semantic parsing is that once the latent semantic representation has been executed, one could test whether the execution results could match the information with available weak supervision signals such as answers to natural language queries (Clarke et al., 2010; Liang et al., 2011), or task completion from instructional navigations (Misra et al., 2017). Such formulations have been adapted for question answering over structured tables (Pasupat and Liang, 2015; Krishnamurthy et al., 2017). However, the current research focus is to convert a natural language question into executable table queries and to directly retrieve results. They do not have the need of inference involving numerical commonsense implied by various lexical patterns. A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013) but only specific to translating language in"
D18-1411,P06-2034,0,0.0210308,"texts. This will address the issue of the lack of consideration for the relationship between lexical terms and numerical values. Our approach makes a significant difference in that our framework could generalize to numerical values or value combinations that are unseen in training, and will not be simply reciting cooccurrence patterns of exact values in the training data. Our work relates to learning executable semantic parsers under weak supervision. Early semantic parsing started from fully supervised training with annotated meaning representations available (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 3762 2013). The intuition behind weakly supervised executable semantic parsing is that once the latent semantic representation has been executed, one could test whether the execution results could match the information with available weak supervision signals such as answers to natural language queries (Clarke et al., 2010; Liang et al., 2011), or task completion from instructional navigations (Misra et al., 2017). Such formulations have been adapted for question ans"
D18-1411,N16-1086,0,0.122449,"y requires a rather weak and distant form of supervision from paired tables and texts without annotations for fine-grained alignments between phrases and data cells. Similar modeling and learning strategies could potentially be useful for considerably large tag space derived from structured knowledge bases in the future (Choi et al., 2018). The feasibility of this work is partly due to the availability of data, mostly comes from the field of data-to-text language generation. Related work in data-to-text generation mainly focused on directly generating summary descriptions for structured data (Mei et al., 2016; Kiddon et al., 2016; Murakami et al., 2017; Wiseman et al., 2017), without establishing underlying semantic correspondences. Texts generated thereby can be fluent but not conforming to the input data, unlike templatebased approaches where lexical choices could be directly controlled. In our work, we find the derived semantic correspondences between data and texts to be useful for template induction, either with simple heuristics to automatically extract description patterns (how to say) and corresponding triggers (what & when to say), or with more crafted discriminative learning approaches ("
D18-1411,D17-1106,0,0.0189423,"esentations available (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 3762 2013). The intuition behind weakly supervised executable semantic parsing is that once the latent semantic representation has been executed, one could test whether the execution results could match the information with available weak supervision signals such as answers to natural language queries (Clarke et al., 2010; Liang et al., 2011), or task completion from instructional navigations (Misra et al., 2017). Such formulations have been adapted for question answering over structured tables (Pasupat and Liang, 2015; Krishnamurthy et al., 2017). However, the current research focus is to convert a natural language question into executable table queries and to directly retrieve results. They do not have the need of inference involving numerical commonsense implied by various lexical patterns. A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013) but only specific to translating language into queries in the highly structured database and cannot be applied to our domain. Our approac"
D18-1411,J10-3007,0,0.279933,"Missing"
D18-1411,P13-1038,0,0.028989,"t appear explicitly in the table and needs to be inferred. 3761 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3761–3771 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Such kind of language grounding is important both for natural language understanding and for natural language generation. For language understanding, establishing explicit connections between symbols and values beyond ungrounded symbolic meaning representations will be useful for acquisition and inference of numerical commonsense (Narisawa et al., 2013). For language generation, properly aligned information is the key to acquiring patterns of various lexical choices under different world states (Roy and Reiter, 2005). In this work, we make a step towards more explicit semantic correspondences between structured data and texts. Rather than only producing coarse alignments between data fields and text spans, we try to detect the latent semantics underlying these alignments by prompting explicit semantic annotations. We make the first attempt at utilizing publicly available datasets originally prepared for data-to-text language generation to pr"
D18-1411,P15-1142,0,0.021363,"ut more recent work focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 3762 2013). The intuition behind weakly supervised executable semantic parsing is that once the latent semantic representation has been executed, one could test whether the execution results could match the information with available weak supervision signals such as answers to natural language queries (Clarke et al., 2010; Liang et al., 2011), or task completion from instructional navigations (Misra et al., 2017). Such formulations have been adapted for question answering over structured tables (Pasupat and Liang, 2015; Krishnamurthy et al., 2017). However, the current research focus is to convert a natural language question into executable table queries and to directly retrieve results. They do not have the need of inference involving numerical commonsense implied by various lexical patterns. A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013) but only specific to translating language into queries in the highly structured database and cannot be applied to our domain. Our approach is implemented as assigning tag annotations over text spans, which is conceptually related to fine-grained"
D18-1411,P13-1092,0,0.0171202,"answers to natural language queries (Clarke et al., 2010; Liang et al., 2011), or task completion from instructional navigations (Misra et al., 2017). Such formulations have been adapted for question answering over structured tables (Pasupat and Liang, 2015; Krishnamurthy et al., 2017). However, the current research focus is to convert a natural language question into executable table queries and to directly retrieve results. They do not have the need of inference involving numerical commonsense implied by various lexical patterns. A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013) but only specific to translating language into queries in the highly structured database and cannot be applied to our domain. Our approach is implemented as assigning tag annotations over text spans, which is conceptually related to fine-grained named entity tagging (Ling and Weld, 2012). Our setting only requires a rather weak and distant form of supervision from paired tables and texts without annotations for fine-grained alignments between phrases and data cells. Similar modeling and learning strategies could potentially be useful for considerably large tag space derived from structured kn"
D18-1411,D09-1001,0,0.0278628,"ervision signals such as answers to natural language queries (Clarke et al., 2010; Liang et al., 2011), or task completion from instructional navigations (Misra et al., 2017). Such formulations have been adapted for question answering over structured tables (Pasupat and Liang, 2015; Krishnamurthy et al., 2017). However, the current research focus is to convert a natural language question into executable table queries and to directly retrieve results. They do not have the need of inference involving numerical commonsense implied by various lexical patterns. A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013) but only specific to translating language into queries in the highly structured database and cannot be applied to our domain. Our approach is implemented as assigning tag annotations over text spans, which is conceptually related to fine-grained named entity tagging (Ling and Weld, 2012). Our setting only requires a rather weak and distant form of supervision from paired tables and texts without annotations for fine-grained alignments between phrases and data cells. Similar modeling and learning strategies could potentially be useful for considerably large tag space derived from"
D18-1411,D17-1239,0,0.339063,"paired tables and texts without annotations for fine-grained alignments between phrases and data cells. Similar modeling and learning strategies could potentially be useful for considerably large tag space derived from structured knowledge bases in the future (Choi et al., 2018). The feasibility of this work is partly due to the availability of data, mostly comes from the field of data-to-text language generation. Related work in data-to-text generation mainly focused on directly generating summary descriptions for structured data (Mei et al., 2016; Kiddon et al., 2016; Murakami et al., 2017; Wiseman et al., 2017), without establishing underlying semantic correspondences. Texts generated thereby can be fluent but not conforming to the input data, unlike templatebased approaches where lexical choices could be directly controlled. In our work, we find the derived semantic correspondences between data and texts to be useful for template induction, either with simple heuristics to automatically extract description patterns (how to say) and corresponding triggers (what & when to say), or with more crafted discriminative learning approaches (cf. Angeli et al. (2010)). 3 Technical overview Task Let S be the s"
D18-1411,P13-1006,0,0.0316651,"n” that commonly appears in statistical alignment models (Sec 4.5), we add a soft statistical constraint via posterior regularization (Ganchev et al., 2010). As a by-product, we also show how the derived semantic annotations could be used to induce descriptive templates for data-to-text generation (Sec 5). Experimental results (Sec 6) suggest the feasibility of the setting in this study, and show the effectiveness of our proposed framework. 2 Related work Grounded language acquisition has aroused wide interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Gorniak and Roy, 2007; Yu and Siskind, 2013; Chrupała et al., 2015). Later work in the community of natural language processing also moved in this direction by relaxing the amount of supervision to enable a model to learn from ambiguous alignments (Kate and Mooney, 2007; Chen and Mooney, 2008). Some research aimed at establishing coarse alignments between simulated robot soccer game records and commentary sentences (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011). For weather forecast domain, Liang et al. (2009) used a hierarchical hidden Markov model in order to map utterances to world states, w"
D18-1422,D10-1049,0,0.448544,"ta and the results of inferring, respectively. whole system in an end-to-end fashion. As a result, end-to-end neural text generation has drawn increasing attention from the natural language research community (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Kiddon et al., 2016). Introduction Data-to-text generation is a classic language generation task that takes structured data (e.g., a table of statistics or a set of event records) as input, aiming at automatically producing texts that informatively, correctly and fluently describe the data (Kukich, 1983; Reiter and Dale, 1997; Angeli et al., 2010; Konstas and Lapata, 2012; PerezBeltrachini and Gardent, 2017). Traditionally, a data-to-text generation system should pay attention to the problem of content selection (i.e., what to say) and surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Modern neural generation systems avoid the distinction of these aspects by building over a standard encoder-decoder architecture (Sutskever et al., 2014) with the attention mechanism over input content (Bahdanau et al., 2015) and train the ∗ Contribution during internship at Microsoft. City Miami Atlanta However, a c"
D18-1422,P05-1018,0,0.0692782,"ponent. Fig. 3 shows an example of the gating weights at each time step in generation, where a darker cell means the incorporation of more information from operation results for decoding corresponding word. We can see that the gate weights are reasonable, as the gate values are large when deciding the team leader “horford” and the winner of the game “hawks”. 5 Related Work Data-to-text generation is a task of natural language generation (NLG) (Gatt and Krahmer, 2018). Previous research has focused on individual content selection (Kukich, 1983; Reiter and Dale, 1997; Dubou´e and McKeown, 2003; Barzilay and Lapata, 2005) and surface realization (Goldberg et al., 1994; Soricut and Marcu, 2006; Wong and Mooney, 2007). 3886 Recent work avoids the distinction of the content selection and sentence realization. Chen and Mooney (2008) use an SMT based approach to learn alignments between comments and their corresponding event records. Angeli et al. (2010) transform the problem into a sequence of local decisions using a log-linear model. Konstas and Lapata (2012) employ a PCFG to simultaneously optimize the content selection and surface realization problem. In the field of neural text generation, Mei et al. (2016) us"
D18-1422,E17-1060,0,0.209657,"sed approach to learn alignments between comments and their corresponding event records. Angeli et al. (2010) transform the problem into a sequence of local decisions using a log-linear model. Konstas and Lapata (2012) employ a PCFG to simultaneously optimize the content selection and surface realization problem. In the field of neural text generation, Mei et al. (2016) uses a neural encoder-decoder approach for end-to-end training. Some have focused on conditional language generation based on tables (Yang et al., 2017), short biographies generation from Wikipedia tables (Lebret et al., 2016; Chisholm et al., 2017) and comments generation based on stock prices (Murakami et al., 2017). However, none of these methods consider incorporating the facts that can be inferred from the input data to guide the process of generation. Murakami et al. (2017) post-process the price by extending the copy mechanism and replacing numerical values with defined arithmetic operations after generation. While our model, OpAtt utilizes information from pre-computed operations on raw data to guide the generation. Our work is related to research areas on deep learning models for program induction and question answering from a k"
D18-1422,W14-4012,0,0.166563,"Missing"
D18-1422,D14-1179,0,0.0104974,"Missing"
D18-1422,D18-2003,1,0.825789,"and human evaluation to evaluate the proposed model. Automatic Evaluation We employ BLEU-4 as the metric for automatic evaluation. Table 4 gives the automatic evaluation results for generation on two datasets. Our proposed model OpAtt outperforms neural network baselines (See et al., 2017; Wiseman et al., 2017). The results show that our method which incorporates the operations enables generating texts that are fidelity to facts and therefore yields the best performance. Seq2Seq+op+quant outper3 For the ROTOWIRE dataset, we adopt Wiseman et al. (2017)’s templates. For the ESPN dataset, we use Dou et al. (2018)’s system to extract templates. The template is constructed by emitting teams and players information in a sentence: <team1&gt; beats <team2&gt; with <point1&gt;<point2&gt;. 4 The authors have updated the dataset to fix some mistakes recently, so we cannot use the result which is reported in their paper and rerun this baseline with the authors’ code. 3884 Ref Seq2Seq+copy Seq2Seq+op Seq2Seq+op+quant OpAtt ESPN #Cont./#Supp. #Cont./#Supp. (input facts) (inferred facts) 0.00 / 4.90 0.00 / 1.12 0.44 / 4.61 0.16 / 1.25 0.24 / 3.97 0.07 / 1.08 0.21 / 4.88 0.03 / 1.10 0.04 / 5.00 0.02 / 1.27 #Cont. (unsupported"
D18-1422,W03-1016,0,0.366068,"Missing"
D18-1422,P16-1154,0,0.0668747,"the encoder and decoder. ct in Eq. 2 is the context vector at timestep t, computed as a weighted hidden vectors hj : tion. The joint probability for generating yt is formulated as follows: Pcopy (yt |y<t , S) = pgen P (yt |y<t , S) X +(1 − pgen ) αt,i (6) i:ri =yt ct = K X αt,j hj (4) j=1 where αt,j is computed by an attention scheme, typically implemented as a softmax distribution over scores calculated with a multi-layer perceptron (Bahdanau et al., 2015). 2.2 Copy Mechanism Recent work augments Seq2Seq models to copy words directly from the source information on which they are conditioned (Gu et al., 2016; See et al., 2017). These models usually introduce an additional binary variable zt into per-timestep target word distribution, which indicates whether the target word yt is copied from the source or is generated from the recurrent hidden states. We use the pointer-generator network (See et al., 2017) for the copy mechanism. Specifically, the binary variable zt is calculated from the context vector ct , the decoder state dt and the decoder input yt−1 : &gt; &gt; pgen = σ(w&gt; c ct + wd dt + wy yt−1 + bptr ) (5) where vectors wc , wd , wy and the scalar bptr are learnable parameters, and σ is the sigm"
D18-1422,D16-1032,0,0.023515,"ty of the generated texts to the input structured data. 1 Table 1: An example of generated texts from structured data. In this example, the wining team is not indicated explicitly, but can be inferred from the scores for hte wave :::: lines two teams. The words with underlining and ::::: are based on the facts from the input data and the results of inferring, respectively. whole system in an end-to-end fashion. As a result, end-to-end neural text generation has drawn increasing attention from the natural language research community (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Kiddon et al., 2016). Introduction Data-to-text generation is a classic language generation task that takes structured data (e.g., a table of statistics or a set of event records) as input, aiming at automatically producing texts that informatively, correctly and fluently describe the data (Kukich, 1983; Reiter and Dale, 1997; Angeli et al., 2010; Konstas and Lapata, 2012; PerezBeltrachini and Gardent, 2017). Traditionally, a data-to-text generation system should pay attention to the problem of content selection (i.e., what to say) and surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahme"
D18-1422,P12-1039,0,0.195489,"inferring, respectively. whole system in an end-to-end fashion. As a result, end-to-end neural text generation has drawn increasing attention from the natural language research community (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Kiddon et al., 2016). Introduction Data-to-text generation is a classic language generation task that takes structured data (e.g., a table of statistics or a set of event records) as input, aiming at automatically producing texts that informatively, correctly and fluently describe the data (Kukich, 1983; Reiter and Dale, 1997; Angeli et al., 2010; Konstas and Lapata, 2012; PerezBeltrachini and Gardent, 2017). Traditionally, a data-to-text generation system should pay attention to the problem of content selection (i.e., what to say) and surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Modern neural generation systems avoid the distinction of these aspects by building over a standard encoder-decoder architecture (Sutskever et al., 2014) with the attention mechanism over input content (Bahdanau et al., 2015) and train the ∗ Contribution during internship at Microsoft. City Miami Atlanta However, a critical issue for neural t"
D18-1422,P83-1022,0,0.830749,"based on the facts from the input data and the results of inferring, respectively. whole system in an end-to-end fashion. As a result, end-to-end neural text generation has drawn increasing attention from the natural language research community (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Kiddon et al., 2016). Introduction Data-to-text generation is a classic language generation task that takes structured data (e.g., a table of statistics or a set of event records) as input, aiming at automatically producing texts that informatively, correctly and fluently describe the data (Kukich, 1983; Reiter and Dale, 1997; Angeli et al., 2010; Konstas and Lapata, 2012; PerezBeltrachini and Gardent, 2017). Traditionally, a data-to-text generation system should pay attention to the problem of content selection (i.e., what to say) and surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Modern neural generation systems avoid the distinction of these aspects by building over a standard encoder-decoder architecture (Sutskever et al., 2014) with the attention mechanism over input content (Bahdanau et al., 2015) and train the ∗ Contribution during internship a"
D18-1422,D16-1128,0,0.333925,"be optimized in an end-to-end manner using back propagation. Given the batches of records {S}N and the standard natural language descriptions {Y }N , the objective function is to minimize the negative log-likelihood: L=− N Tk 1 XX k log p(ytk |y<t , Sk) N (20) k=1 t=1 where the superscript k indicates the index of the records-description pair, and Tk is the length of the k-th description. 4 4.1 Experiments Datasets Several benchmark datasets have been used in recent years for data-to-text generation (Liang et al., 2009; Chen and Mooney, 2008; Lebret et al., 2016). For instance, Lebret et al. (2016) have built a biography generation dataset from Wikipedia. However, a recent study by Perez-Beltrachini and Gardent (2017) shows that existing datasets have a few missing properties such as lacking syntactic and semantic diversity. To check whether the facts mentioned in the descriptions are based on input data, we identify the text spans which contain facts (e.g., in table 1, “Hawks” is a span contain fact) from the descriptions and divide each span into 3883 three categories: a) input facts (facts that can be directly found from the input), b) inferred facts (facts that can not be directly f"
D18-1422,P17-1003,0,0.148986,"de the whole generation process. However, there are two major challenges for incorporating pre-defined operations: (1) if we apply operations exhaustively on all fields with compatible value types in the table, it would create a huge search space in which mention worthy results are rare events and (2) it is difficult to establish the correspondences between specific spans of numeric results and lexical choices. For example, the word “edges” corresponds to the slight difference in score, i.e. 1, in Table. 1. Inspired by recent work that separates neural representations and symbolic operations (Liang et al., 2017), we propose a framework for neural data-to-text generation that is able to utilize information from pre-computed operations on raw data. Based on a standard sequence-to-sequence model with an attention and copying mechanism, we design a gating mechanism for the neural model to decide which part of the execution results should be used for generation. To address the second challenge, we also design a quantization layer to map numerical execution results into bins to guide different lexical choices according to different quantities of values. To examine the effectiveness of our proposed model, w"
D18-1422,P09-1011,0,0.558043,"Missing"
D18-1422,P17-1015,0,0.0299851,"l., 2017). However, none of these methods consider incorporating the facts that can be inferred from the input data to guide the process of generation. Murakami et al. (2017) post-process the price by extending the copy mechanism and replacing numerical values with defined arithmetic operations after generation. While our model, OpAtt utilizes information from pre-computed operations on raw data to guide the generation. Our work is related to research areas on deep learning models for program induction and question answering from a knowledge base (Neelakantan et al., 2016; Liang et al., 2017; Ling et al., 2017). Neelakantan et al. (2016) solve the problem of semantic parsing from structured data and generate programs using pre-defined arithmetic operations. Liang et al. (2017) design a set of executable operators and obtain the answers by the generated logic forms. Ling et al. (2017) design a set of operators to generate the latent program for math problem solving. However, data-to-text is a different task. The operations for these methods are designed to find the answers, while we use the operations to guide the process of generation. 6 Conclusion and Future Work In this work, we address the proble"
D18-1422,D15-1166,0,0.0492667,"ations as input. We design two attention layers to summarize information from both parts respectively, the overall context vector ct is balanced by a dynamic gate λt . ctx ct = (1 − λt )cop t + λt ct , (12) λt = σ(Wg dt + bg ), (13) ctx are the context vector of operwhere cop t and ct ation results and records, respectively. As there are two types of operation results which have quite different meanings, their context vectors are calculated separately and then put together by a nonlinear layer. The context vectors cscl t of operation results with scalar value at timestep t are constructed as (Luong et al., 2015): cscl t = N X scl αt,j ∗ hres j (14) = MLP(dt−1 , hop j ), scl αt,j = scl ) exp(βt,j P scl k exp(βt,k ) (15) (16) where MLP stands for standard 1-layer perceptron scl refers to the im(with tanh nonlinearity), and αt,j portance of j-th operations at the current timestep t. Eq. 14 is based on the attention mechanism which can be treated as mapping a query and a set of key-value pairs to an output. The output cscl t is computed as a weighted sum of the values hres j , where the weight assigned to each value is computed by a compatibility function of the query dt−1 with the corresponding key hop"
D18-1422,N16-1086,0,0.429373,"datasets show our proposed method clearly improves the fidelity of the generated texts to the input structured data. 1 Table 1: An example of generated texts from structured data. In this example, the wining team is not indicated explicitly, but can be inferred from the scores for hte wave :::: lines two teams. The words with underlining and ::::: are based on the facts from the input data and the results of inferring, respectively. whole system in an end-to-end fashion. As a result, end-to-end neural text generation has drawn increasing attention from the natural language research community (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Kiddon et al., 2016). Introduction Data-to-text generation is a classic language generation task that takes structured data (e.g., a table of statistics or a set of event records) as input, aiming at automatically producing texts that informatively, correctly and fluently describe the data (Kukich, 1983; Reiter and Dale, 1997; Angeli et al., 2010; Konstas and Lapata, 2012; PerezBeltrachini and Gardent, 2017). Traditionally, a data-to-text generation system should pay attention to the problem of content selection (i.e., what to say) and surface reali"
D18-1422,W17-3537,0,0.35103,"nce-to-sequence model with an attention and copying mechanism, we design a gating mechanism for the neural model to decide which part of the execution results should be used for generation. To address the second challenge, we also design a quantization layer to map numerical execution results into bins to guide different lexical choices according to different quantities of values. To examine the effectiveness of our proposed model, we collect a large dataset of sports headline generation for NBA basketball games1 . We also evaluate the models on the ROTOWIRE dataset released by Wiseman et al. (2017) which targets at generating short paragraphs. Experiments show that our model outperforms current state-of-theart neural methods in terms of both fluency and fidelity. In summary, we make the following contributions in this paper: • We propose a neural data-to-text framework that generate texts by additional processing over input data. Based on a basic sequenceto-sequence model with attention and copying, we design a gating mechanism to enable the model to decide which part of the executed results should be utilized. We also propose a novel quantization layer to map specific numerical values"
D18-1422,N07-1022,0,0.0400869,"r cell means the incorporation of more information from operation results for decoding corresponding word. We can see that the gate weights are reasonable, as the gate values are large when deciding the team leader “horford” and the winner of the game “hawks”. 5 Related Work Data-to-text generation is a task of natural language generation (NLG) (Gatt and Krahmer, 2018). Previous research has focused on individual content selection (Kukich, 1983; Reiter and Dale, 1997; Dubou´e and McKeown, 2003; Barzilay and Lapata, 2005) and surface realization (Goldberg et al., 1994; Soricut and Marcu, 2006; Wong and Mooney, 2007). 3886 Recent work avoids the distinction of the content selection and sentence realization. Chen and Mooney (2008) use an SMT based approach to learn alignments between comments and their corresponding event records. Angeli et al. (2010) transform the problem into a sequence of local decisions using a log-linear model. Konstas and Lapata (2012) employ a PCFG to simultaneously optimize the content selection and surface realization problem. In the field of neural text generation, Mei et al. (2016) uses a neural encoder-decoder approach for end-to-end training. Some have focused on conditional l"
D18-1422,D17-1197,0,0.271847,"}, and the operations are applied to the input records S to produce corresponding results at the preprocessing stage. The results of operations can be categorized into two types: opscl i denotes results with a type of scalar value and opidx denotes results with a type of ini dexing value. 3.2 Encoding Records We map each record r ∈ S into a vector r by concatenating the embedding of r.idx (e.g., row 2), r.f (e.g., column Points) and r.v (e.g., cell value &gt; 95), denoted as r = [eidx , ef , ev ] , where eidx , ef , ev are trainable word embeddings of r.idx, r.f and r.v respectively, similar to (Yang et al., 2017). We feed a set of record vectors r1 , ..., rK to a bidirectional GRU and yield the final record representactx tions hctx 1 , ..., hK as introduced in Section 2. We leave the exploring of different encoding methods as future work, as it would affect the performance. 3.3 Encoding Operations As shown in Fig. 1, each operation opi consists of: a) the name of the operation opi .t (e.g., minus); b) the column opi .c to which the operation applies (e.g., Points); and c) the row to which the operation applies, denoted as opi .arg = {ri .idx}A i=1 , where A is the count of arguments. We then encode ea"
D18-1422,W16-6642,0,0.0298218,"results, one is scalar results (e.g., the minus operation returns -1), the other is indexing results (e.g., the argmax operation returns the row number 2), and two encoders are designed to encode these results respectively. Scalar Results Representation In Table. 1, the word “edges” is generated based on the fact that the points gap of the two teams is -1. In fact, other value likes -2 or -3 is close to -1, and the word “edges” is also applicable to them. However, directly establishing the lexical choices on various sparse numeric values is not easy (Reiter et al., 2005; Smiley et al., 2016; Zarrieß and Schlangen, 2016). Reiter et al. (2005) use consistent data-toword rules for time-series weather forecast summary generation. In this paper, we aim to capture the data-to-word mapping automatically by a simple quantization unit. A quantization layer is designed to map the scalar values into several bins, namely quantization units. Specifically, we feed each scalar value opscl i to a softmax layer, and its is computed as the weighted representation hres i sum of all quantization embeddings: qi = Wq opscl i + bq , exp(qi,l ) µi,l = PL , exp(q ) i,j j=1 hres = i L X µi,l escl l (9) (10) (11) l=1 where Wq and bq a"
D18-1422,P17-1099,0,0.766209,"s under different conditions. • To focus our study on correct text generation, we collect a challenging dataset for NBA headline generation. • We conduct experiments on the NBA headline dataset as well as the ROTOWIRE dataset from previous work. Results show improvements on both correctness and fluency from our proposed framework over baseline systems. 2 Background: Attention-Based Neural Sequence-to-Sequence Model In this section, we briefly introduce the architecture of the attention-based sequence-to-sequence (Seq2Seq) (Cho et al., 2014b; Bahdanau et al., 2015) model with a copy mechanism (See et al., 2017), which is the basis of our proposed model. 2.1 RNN Encoder-Decoder The goal of data-to-text generation is to generate a natural language description for a given set of data records S = {rj }K j=1 . Usually, a Seq2Seq model consists of an encoder and a decoder with recurrent neural networks (RNN). First, each input record rj is encoded into a hidden vector hj with j ∈ {1, ..., K} using a bidirectional RNN. The decoder generates the description word by word using another RNN. In the training phase, given a record set and its corresponding natural language description (S, y), the Seq2Seq model m"
D18-1422,W16-6606,0,0.0267843,"produce two types of results, one is scalar results (e.g., the minus operation returns -1), the other is indexing results (e.g., the argmax operation returns the row number 2), and two encoders are designed to encode these results respectively. Scalar Results Representation In Table. 1, the word “edges” is generated based on the fact that the points gap of the two teams is -1. In fact, other value likes -2 or -3 is close to -1, and the word “edges” is also applicable to them. However, directly establishing the lexical choices on various sparse numeric values is not easy (Reiter et al., 2005; Smiley et al., 2016; Zarrieß and Schlangen, 2016). Reiter et al. (2005) use consistent data-toword rules for time-series weather forecast summary generation. In this paper, we aim to capture the data-to-word mapping automatically by a simple quantization unit. A quantization layer is designed to map the scalar values into several bins, namely quantization units. Specifically, we feed each scalar value opscl i to a softmax layer, and its is computed as the weighted representation hres i sum of all quantization embeddings: qi = Wq opscl i + bq , exp(qi,l ) µi,l = PL , exp(q ) i,j j=1 hres = i L X µi,l escl l (9) ("
D18-1422,P06-1139,0,0.0181184,"generation, where a darker cell means the incorporation of more information from operation results for decoding corresponding word. We can see that the gate weights are reasonable, as the gate values are large when deciding the team leader “horford” and the winner of the game “hawks”. 5 Related Work Data-to-text generation is a task of natural language generation (NLG) (Gatt and Krahmer, 2018). Previous research has focused on individual content selection (Kukich, 1983; Reiter and Dale, 1997; Dubou´e and McKeown, 2003; Barzilay and Lapata, 2005) and surface realization (Goldberg et al., 1994; Soricut and Marcu, 2006; Wong and Mooney, 2007). 3886 Recent work avoids the distinction of the content selection and sentence realization. Chen and Mooney (2008) use an SMT based approach to learn alignments between comments and their corresponding event records. Angeli et al. (2010) transform the problem into a sequence of local decisions using a log-linear model. Konstas and Lapata (2012) employ a PCFG to simultaneously optimize the content selection and surface realization problem. In the field of neural text generation, Mei et al. (2016) uses a neural encoder-decoder approach for end-to-end training. Some have"
D18-1422,D17-1239,0,0.168642,"Missing"
D18-2003,W13-0113,0,0.0160675,"example, we can say “with a massive 8 GB of memory” for a laptop computer while “a massive 2 GB” is inappropriate. Rule-based approaches (Moore and Paris, 1993; Hovy, 1993; Reiter and Dale, 2000; Belz, 2007; Bouayad-Agha et al., 2011) encode domain knowledge into the generation system and then produce high-quality texts, while the construction of the system is expensive and heavily depends on domain experts. Statistical approaches are employed to reduce extensive development time by learning rules from historical data (Langkilde and Knight, 1998; Liang et al., 2009; Duboue and McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-to-text generation, i.e., a technology which takes structured data as input and produces text that adequately and fluently describes this data as output, has various applications on the generation of sports news (Chen and Mooney, 2008; Kim and Mooney, 2010; Mei et al., 2016; Wiseman et al., 2017), product descriptions (Wang et al., 2017), weather reports (Liang et al., 2009; Angeli et al., 2010; Mei et al., 20"
D18-2003,C10-2062,0,0.0204373,"loyed to reduce extensive development time by learning rules from historical data (Langkilde and Knight, 1998; Liang et al., 2009; Duboue and McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-to-text generation, i.e., a technology which takes structured data as input and produces text that adequately and fluently describes this data as output, has various applications on the generation of sports news (Chen and Mooney, 2008; Kim and Mooney, 2010; Mei et al., 2016; Wiseman et al., 2017), product descriptions (Wang et al., 2017), weather reports (Liang et al., 2009; Angeli et al., 2010; Mei et al., 2016) and short biographies (Lebret et al., 2016; Chisholm et al., 2017). In another scenario, it is possible albeit a little awkward for a virtual assistant like Microsoft Cortana to read out structured data when responding to users’ queries. it is more user friendly for a virtual assistant to identify and read out the essential part of the structured data in natural language to make it easier to understand. In these cases, it is inefficien"
D18-2003,P13-1138,0,0.0567117,"Architecture 2.1 Fig. 1 shows the simplified architecture of the Data2Text Studio. It mainly consists of three components: 1) model training, 2) model revision and 3) text generation. For typical usages, developers can directly choose the pre-trained model to generate high-quality texts. To develop a customized text generation model: First, developModel Training We adopt the template-based solution for the Data2Text Studio. It can generate texts with high accuracy and fluency, which can be used in business applications directly. Several previous studies (Liang et al., 2009; Wang et al., 2017; Kondadadi et al., 2013) can be applied to extract templates from parallel data. To address the challenges introduced in Section 1, we propose a 1 http://automatedinsights.com, http:// arria.com and http://narrativescience.com 14 (a) Template revision. The center part shows the template with (b) Generated texts preview. Multiple headlines are generslots, and the bottom part shows the trigger conditions. ated for the same game to ensure variety. Figure 3: Data2Text Studio Interface tured data which contains the attributes of the game. The right-hand side shows multiple generated texts for this game to help developers"
D18-2003,P98-1116,0,0.334428,"ty: the generated texts should be consistent with the structured data. For example, we can say “with a massive 8 GB of memory” for a laptop computer while “a massive 2 GB” is inappropriate. Rule-based approaches (Moore and Paris, 1993; Hovy, 1993; Reiter and Dale, 2000; Belz, 2007; Bouayad-Agha et al., 2011) encode domain knowledge into the generation system and then produce high-quality texts, while the construction of the system is expensive and heavily depends on domain experts. Statistical approaches are employed to reduce extensive development time by learning rules from historical data (Langkilde and Knight, 1998; Liang et al., 2009; Duboue and McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-to-text generation, i.e., a technology which takes structured data as input and produces text that adequately and fluently describes this data as output, has various applications on the generation of sports news (Chen and Mooney, 2008; Kim and Mooney, 2010; Mei et al., 2016; Wiseman et al., 2017), product descriptions (Wang et al., 2017), weat"
D18-2003,D16-1128,0,0.171107,"Missing"
D18-2003,P09-1011,0,0.337894,"ld be consistent with the structured data. For example, we can say “with a massive 8 GB of memory” for a laptop computer while “a massive 2 GB” is inappropriate. Rule-based approaches (Moore and Paris, 1993; Hovy, 1993; Reiter and Dale, 2000; Belz, 2007; Bouayad-Agha et al., 2011) encode domain knowledge into the generation system and then produce high-quality texts, while the construction of the system is expensive and heavily depends on domain experts. Statistical approaches are employed to reduce extensive development time by learning rules from historical data (Langkilde and Knight, 1998; Liang et al., 2009; Duboue and McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-to-text generation, i.e., a technology which takes structured data as input and produces text that adequately and fluently describes this data as output, has various applications on the generation of sports news (Chen and Mooney, 2008; Kim and Mooney, 2010; Mei et al., 2016; Wiseman et al., 2017), product descriptions (Wang et al., 2017), weather reports (Liang e"
D18-2003,D10-1049,0,0.576735,"McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-to-text generation, i.e., a technology which takes structured data as input and produces text that adequately and fluently describes this data as output, has various applications on the generation of sports news (Chen and Mooney, 2008; Kim and Mooney, 2010; Mei et al., 2016; Wiseman et al., 2017), product descriptions (Wang et al., 2017), weather reports (Liang et al., 2009; Angeli et al., 2010; Mei et al., 2016) and short biographies (Lebret et al., 2016; Chisholm et al., 2017). In another scenario, it is possible albeit a little awkward for a virtual assistant like Microsoft Cortana to read out structured data when responding to users’ queries. it is more user friendly for a virtual assistant to identify and read out the essential part of the structured data in natural language to make it easier to understand. In these cases, it is inefficient and expensive to generate texts using human writers, To address the second challenge, we propose a Semi-HMMs model to automatically extract"
D18-2003,N16-1086,0,0.0489928,"ive development time by learning rules from historical data (Langkilde and Knight, 1998; Liang et al., 2009; Duboue and McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-to-text generation, i.e., a technology which takes structured data as input and produces text that adequately and fluently describes this data as output, has various applications on the generation of sports news (Chen and Mooney, 2008; Kim and Mooney, 2010; Mei et al., 2016; Wiseman et al., 2017), product descriptions (Wang et al., 2017), weather reports (Liang et al., 2009; Angeli et al., 2010; Mei et al., 2016) and short biographies (Lebret et al., 2016; Chisholm et al., 2017). In another scenario, it is possible albeit a little awkward for a virtual assistant like Microsoft Cortana to read out structured data when responding to users’ queries. it is more user friendly for a virtual assistant to identify and read out the essential part of the structured data in natural language to make it easier to understand. In these cases, it is inefficient and expensive to"
D18-2003,N07-1021,0,0.0423381,"ets for template extraction and text generation. The results show that our model achieves improvements on both tasks. 1 There are two main challenges for data-to-text generation systems: 1) Interactivity: For a developer, it should be able to customize the text generation model and control the generated texts. 2) Interpretability: the generated texts should be consistent with the structured data. For example, we can say “with a massive 8 GB of memory” for a laptop computer while “a massive 2 GB” is inappropriate. Rule-based approaches (Moore and Paris, 1993; Hovy, 1993; Reiter and Dale, 2000; Belz, 2007; Bouayad-Agha et al., 2011) encode domain knowledge into the generation system and then produce high-quality texts, while the construction of the system is expensive and heavily depends on domain experts. Statistical approaches are employed to reduce extensive development time by learning rules from historical data (Langkilde and Knight, 1998; Liang et al., 2009; Duboue and McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-"
D18-2003,J93-4004,0,0.117737,"ty applications. We conduct experiments on ROTOW IRE datasets for template extraction and text generation. The results show that our model achieves improvements on both tasks. 1 There are two main challenges for data-to-text generation systems: 1) Interactivity: For a developer, it should be able to customize the text generation model and control the generated texts. 2) Interpretability: the generated texts should be consistent with the structured data. For example, we can say “with a massive 8 GB of memory” for a laptop computer while “a massive 2 GB” is inappropriate. Rule-based approaches (Moore and Paris, 1993; Hovy, 1993; Reiter and Dale, 2000; Belz, 2007; Bouayad-Agha et al., 2011) encode domain knowledge into the generation system and then produce high-quality texts, while the construction of the system is expensive and heavily depends on domain experts. Statistical approaches are employed to reduce extensive development time by learning rules from historical data (Langkilde and Knight, 1998; Liang et al., 2009; Duboue and McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under var"
D18-2003,W11-2810,0,0.174524,"late extraction and text generation. The results show that our model achieves improvements on both tasks. 1 There are two main challenges for data-to-text generation systems: 1) Interactivity: For a developer, it should be able to customize the text generation model and control the generated texts. 2) Interpretability: the generated texts should be consistent with the structured data. For example, we can say “with a massive 8 GB of memory” for a laptop computer while “a massive 2 GB” is inappropriate. Rule-based approaches (Moore and Paris, 1993; Hovy, 1993; Reiter and Dale, 2000; Belz, 2007; Bouayad-Agha et al., 2011) encode domain knowledge into the generation system and then produce high-quality texts, while the construction of the system is expensive and heavily depends on domain experts. Statistical approaches are employed to reduce extensive development time by learning rules from historical data (Langkilde and Knight, 1998; Liang et al., 2009; Duboue and McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-to-text generation, i.e., a"
D18-2003,D18-1411,1,0.840082,"(l, π, w; θ), (s,w)∈ D l,π where D represents the whole training data. Once the model has been trained, we use Viterbi-like dynamic programming to perform the MAP inference to segment the texts and to assign the most likely tags for each span. We derive an expectation-maximization (EM) algorithm to perform maximum likelihood esti16 4.1 mation, and introduce a soft statistical regularization to guide the model towards a better solution. Specifically, we design a special NULL tag for unaligned words, and we “encourage” it to annotate at least half of the words. For more details, please refer to Qin et al. (2018). 3.2 We conduct an experiment and compare with Liang et al. (2009)’s system as the baseline. It is difficult to evaluate the accuracy of tag assignment for the whole dataset, since the executable tags are not annotated in the original data. We recruit three human annotators which are familiar with basketball games to label a random sample consisting of 300 sentences from the test set. The annotators were told to judge whether each word span is related to the table, and which label they are related to. Finally, we calculate the precision and recall for non-NULL tag assignments at word-level. T"
D18-2003,E17-1060,0,0.0665304,"enerating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-to-text generation, i.e., a technology which takes structured data as input and produces text that adequately and fluently describes this data as output, has various applications on the generation of sports news (Chen and Mooney, 2008; Kim and Mooney, 2010; Mei et al., 2016; Wiseman et al., 2017), product descriptions (Wang et al., 2017), weather reports (Liang et al., 2009; Angeli et al., 2010; Mei et al., 2016) and short biographies (Lebret et al., 2016; Chisholm et al., 2017). In another scenario, it is possible albeit a little awkward for a virtual assistant like Microsoft Cortana to read out structured data when responding to users’ queries. it is more user friendly for a virtual assistant to identify and read out the essential part of the structured data in natural language to make it easier to understand. In these cases, it is inefficient and expensive to generate texts using human writers, To address the second challenge, we propose a Semi-HMMs model to automatically extract templates and corresponding trigger conditions from parallel training data. Trigger c"
D18-2003,W03-1016,0,0.278434,"h the structured data. For example, we can say “with a massive 8 GB of memory” for a laptop computer while “a massive 2 GB” is inappropriate. Rule-based approaches (Moore and Paris, 1993; Hovy, 1993; Reiter and Dale, 2000; Belz, 2007; Bouayad-Agha et al., 2011) encode domain knowledge into the generation system and then produce high-quality texts, while the construction of the system is expensive and heavily depends on domain experts. Statistical approaches are employed to reduce extensive development time by learning rules from historical data (Langkilde and Knight, 1998; Liang et al., 2009; Duboue and McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-to-text generation, i.e., a technology which takes structured data as input and produces text that adequately and fluently describes this data as output, has various applications on the generation of sports news (Chen and Mooney, 2008; Kim and Mooney, 2010; Mei et al., 2016; Wiseman et al., 2017), product descriptions (Wang et al., 2017), weather reports (Liang et al., 2009; Angeli et al."
D18-2003,I17-2032,1,0.864647,"ngkilde and Knight, 1998; Liang et al., 2009; Duboue and McKeown, 2003; Howald et al., 2013). However, statistical approaches are prone to generating texts with mistakes, because they don’t know how to use specific phrases under various application conditions. Introduction Data-to-text generation, i.e., a technology which takes structured data as input and produces text that adequately and fluently describes this data as output, has various applications on the generation of sports news (Chen and Mooney, 2008; Kim and Mooney, 2010; Mei et al., 2016; Wiseman et al., 2017), product descriptions (Wang et al., 2017), weather reports (Liang et al., 2009; Angeli et al., 2010; Mei et al., 2016) and short biographies (Lebret et al., 2016; Chisholm et al., 2017). In another scenario, it is possible albeit a little awkward for a virtual assistant like Microsoft Cortana to read out structured data when responding to users’ queries. it is more user friendly for a virtual assistant to identify and read out the essential part of the structured data in natural language to make it easier to understand. In these cases, it is inefficient and expensive to generate texts using human writers, To address the second challe"
D18-2003,J10-3007,0,0.0847849,"Missing"
D18-2003,D17-1239,0,0.120304,"Missing"
D19-1299,H05-1042,0,0.186391,"attern doesn’t hold for this case, because he is British. Hopefully, such cases are rare since the birth place conforms to the nationality in most of cases, so our methods can bring improvement as indicated by the overall better performance across almost all Wikipedia categories. 5 Related Work Data-to-text generation is an important task in natural language generation which has been studied for decades (Kukich, 1983; Holmes-Higgin, 1994; Reiter and Dale, 1997). This task is broadly divided into two subproblems: content selection (Kukich, 1983; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005) and surface realization (Goldberg et al., 1994; Reiter et al., 2005). With the advent of neural text generation, the distinction between content selection and surface realization becomes blurred. For example, Mei et al. (2016) proposed an end-to-end encoder-aligner-decoder model to learn both content selection and surface realization jointly which shows good results on WeatherGov and RoboCub datasets. Wiseman et al. (2017) generate long descriptive game summaries from a database of basketball games where they show the current state-of-the-art neural models are quite good at generating fluent"
D19-1299,W03-1016,0,0.0900029,"However, this inference pattern doesn’t hold for this case, because he is British. Hopefully, such cases are rare since the birth place conforms to the nationality in most of cases, so our methods can bring improvement as indicated by the overall better performance across almost all Wikipedia categories. 5 Related Work Data-to-text generation is an important task in natural language generation which has been studied for decades (Kukich, 1983; Holmes-Higgin, 1994; Reiter and Dale, 1997). This task is broadly divided into two subproblems: content selection (Kukich, 1983; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005) and surface realization (Goldberg et al., 1994; Reiter et al., 2005). With the advent of neural text generation, the distinction between content selection and surface realization becomes blurred. For example, Mei et al. (2016) proposed an end-to-end encoder-aligner-decoder model to learn both content selection and surface realization jointly which shows good results on WeatherGov and RoboCub datasets. Wiseman et al. (2017) generate long descriptive game summaries from a database of basketball games where they show the current state-of-the-art neural models are quit"
D19-1299,W18-6539,0,0.0402782,"Missing"
D19-1299,P83-1022,0,0.717921,"performance. 1 Entity Linking Object country (P17) Algeria (Q262) defender (Q336286) instance of (P31) association football position (Q4611891) … … … MC El Eulma (Q2742749) league (P118) Algerian Ligue Professionnelle 1 (Q647746) Figure 1: An example of generating description from a Wikipedia infobox. External background knowledge expanded from the infobox is helpful for generation. Automatic text generation from structured data (data-to-text) is a classic task in natural language generation which aims to automatically generate fluent, truthful and informative texts based on structured data (Kukich, 1983; Holmes-Higgin, 1994; Reiter and Dale, 1997). Data-to-text is often formulated into two subproblems: content selection which decides what contents should be included in the text and surface realization which Contribution during internship at Microsoft Research. Relation Guelma (Q609871) External Background Knowledge from Wikidata Introduction ∗ Subject determines how to generate the text based on selected contents. Traditionally, these two subproblems have been tackled separately. In recent years, neural generation models, especially the encoder-decoder model, solve these two subproblems join"
D19-1299,D16-1128,0,0.127722,"Missing"
D19-1299,P17-1099,0,0.44058,"Our model takes a data table D (e.g., a Wikipedia infobox) and a relevant external knowledge base (KB) containing a set of facts F as input and generates a natural language text y = y1 , ..., yT consisting of T words. To augment the infobox with external knowledge, we preserve the Wikipedia internal hyperlink information in the field values of infobox, and track these hyperlinks to get their corresponding entities from Wikidata2 where we retrieve only one-hop facts. The backbone of our model is an attention based sequence-to-sequence model (Bahdanau et al., 2014) equipped with copy mechanism (See et al., 2017). As shown in Fig. 2, the model consists of four main components: a table encoder, a KB encoder, the dual attention mechanism and a decoder. We describe each component in the following sections. 2.1 Table Encoder In Fig. 2, the input data table D consists of several field name and field value pairs. We follow (Sha et al., 2017; Liu et al., 2017) to tokenize the field values and transform the input table into a flattened sequence {(ni , vi )}N i=1 , where each element is a token vi from a field value paired with its corresponding field name ni . To encode the flattened table, we map each (ni ,"
D19-1299,P14-5010,0,0.00469779,"Missing"
D19-1299,N16-1086,0,0.262803,"en formulated into two subproblems: content selection which decides what contents should be included in the text and surface realization which Contribution during internship at Microsoft Research. Relation Guelma (Q609871) External Background Knowledge from Wikidata Introduction ∗ Subject determines how to generate the text based on selected contents. Traditionally, these two subproblems have been tackled separately. In recent years, neural generation models, especially the encoder-decoder model, solve these two subproblems jointly and have achieved remarkable successes in several benchmarks (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Duˇsek et al., 2018; Nie et al., 2018). Such end-to-end data-to-text models rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that all writing knowledge can be learned from the training data. However, when people are writing, they will not only rely on the data contents themselves but also consider related knowledge, which is neglected by previous methods. For example, as shown in Fig. 1, an infobox about a person called Nacer Hammami is paired with its corresponding biography description from the Wiki"
D19-1299,P18-1076,0,0.0245316,"k to multi-sentence text generation, where they focus on bootstrapping generators from loosely aligned data. However, most of the work mentioned above assume all the writing knowledge can be learned from massive parallel pairs of training data. Different from the previous work, we exploit incorporating external 3029 knowledge into this task to improve the fidelity of generated text. Our work is also relevant to recent works on integrating external knowledge into neural models for other NLP tasks. The motivations of incorporating external knowledge range from enriching the context information (Mihaylov and Frank, 2018) in reading comprehension, improving the inference ability of models (Chen et al., 2018) in natural language inference, to providing the model a knowledge source to copy from in language modelling (Ahn et al., 2016). Our model, KBAtt, is most relevant to Mihaylov and Frank (2018), where they focus on similarity calculation but we focus on generation in this paper. Moreover, in addition to demonstrating the positive effect of incorporating external knowledge as previous work, we also design a new metric to quantify the potential gains of external knowledge for a specific dataset which can expla"
D19-1299,N18-1139,0,0.150009,"zation jointly which shows good results on WeatherGov and RoboCub datasets. Wiseman et al. (2017) generate long descriptive game summaries from a database of basketball games where they show the current state-of-the-art neural models are quite good at generating fluent outputs, but perform poorly in content selection and capturing long-term structure. Our work falls into the task of single sentence generation from Wikipedia infoboxes. The model structure ranges from feed-forward networks work (Lebret et al., 2016) to encoderdecoder models (Sha et al., 2017; Liu et al., 2017; Bao et al., 2018; Nema et al., 2018). Recently, Perez-Beltrachini and Lapata (2018) generalize this task to multi-sentence text generation, where they focus on bootstrapping generators from loosely aligned data. However, most of the work mentioned above assume all the writing knowledge can be learned from massive parallel pairs of training data. Different from the previous work, we exploit incorporating external 3029 knowledge into this task to improve the fidelity of generated text. Our work is also relevant to recent works on integrating external knowledge into neural models for other NLP tasks. The motivations of incorporatin"
D19-1299,D18-1422,1,0.862745,"should be included in the text and surface realization which Contribution during internship at Microsoft Research. Relation Guelma (Q609871) External Background Knowledge from Wikidata Introduction ∗ Subject determines how to generate the text based on selected contents. Traditionally, these two subproblems have been tackled separately. In recent years, neural generation models, especially the encoder-decoder model, solve these two subproblems jointly and have achieved remarkable successes in several benchmarks (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Duˇsek et al., 2018; Nie et al., 2018). Such end-to-end data-to-text models rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that all writing knowledge can be learned from the training data. However, when people are writing, they will not only rely on the data contents themselves but also consider related knowledge, which is neglected by previous methods. For example, as shown in Fig. 1, an infobox about a person called Nacer Hammami is paired with its corresponding biography description from the Wikipedia. However, the information in the infobox is not enough to cover all the facts"
D19-1299,P02-1040,0,0.103893,"ethods. We also compare our results with other published results using the WikiBio dataset. The model structure of our baseline model is most similar to Sha et al. (2017) by removing their specialized design on order planning which is not the focus of this paper. Since our aim is to verify the effectiveness of external knowledge for datato-text task, we keep our baseline model as general as possible without other specialized design. Our primary model is KBAtt: a model which integrates the background knowledge into baseline model through a KB encoder and KB attention mechanism. We employ BLEU (Papineni et al., 2002) as the automatic evaluation metric. In addition to BLEU, we conduct human evaluation to assess the factual accuracy of generated sentences. 6 Although the English Wikipedia has about 5 million entities, we totally parsed 1,656,458 infoboxes and drop some of them due to data noise which indicates nearly 2/3 entities have no infobox. 3026 Training Details The dimensions of all trainable word embeddings are set to 512, and the GRU hidden states sizes are set to 512. To limit the memory of our model, we set the maximum number of facts per table to 500. We initialize all the model parameters rando"
D19-1299,D17-1239,0,0.153643,"tent selection which decides what contents should be included in the text and surface realization which Contribution during internship at Microsoft Research. Relation Guelma (Q609871) External Background Knowledge from Wikidata Introduction ∗ Subject determines how to generate the text based on selected contents. Traditionally, these two subproblems have been tackled separately. In recent years, neural generation models, especially the encoder-decoder model, solve these two subproblems jointly and have achieved remarkable successes in several benchmarks (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Duˇsek et al., 2018; Nie et al., 2018). Such end-to-end data-to-text models rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that all writing knowledge can be learned from the training data. However, when people are writing, they will not only rely on the data contents themselves but also consider related knowledge, which is neglected by previous methods. For example, as shown in Fig. 1, an infobox about a person called Nacer Hammami is paired with its corresponding biography description from the Wikipedia. However, the information in the info"
D19-1299,N18-1137,0,0.0169804,"d results on WeatherGov and RoboCub datasets. Wiseman et al. (2017) generate long descriptive game summaries from a database of basketball games where they show the current state-of-the-art neural models are quite good at generating fluent outputs, but perform poorly in content selection and capturing long-term structure. Our work falls into the task of single sentence generation from Wikipedia infoboxes. The model structure ranges from feed-forward networks work (Lebret et al., 2016) to encoderdecoder models (Sha et al., 2017; Liu et al., 2017; Bao et al., 2018; Nema et al., 2018). Recently, Perez-Beltrachini and Lapata (2018) generalize this task to multi-sentence text generation, where they focus on bootstrapping generators from loosely aligned data. However, most of the work mentioned above assume all the writing knowledge can be learned from massive parallel pairs of training data. Different from the previous work, we exploit incorporating external 3029 knowledge into this task to improve the fidelity of generated text. Our work is also relevant to recent works on integrating external knowledge into neural models for other NLP tasks. The motivations of incorporating external knowledge range from enriching the c"
E17-1078,E06-1002,0,0.478319,"Missing"
E17-1078,P14-2076,0,0.0353975,"Missing"
E17-1078,D07-1074,0,0.876744,"stract whether they rank all candidates of all mentions simultaneously by incorporating a global coherence measure into the optimization goal (“global inference”). While linguistically well-founded in the concept of lexical cohesion (Halliday and Hasan, 1976), global inference approaches (Kulkarni et al., 2009; Hoffart et al., 2011a) do not scale well with number of mentions and number of candidate entities. In contrast, local approaches do not suffer from scalability issues, since they only optimize the similarity between mention context and candidate KB entry text (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), usually also including a popularity prior2 (Milne and Witten, 2008; Spitkovsky and Chang, 2012). Recent local approaches achieve state-of-the-art results by using convolutional neural networks to capture similarity at multiple context sizes (Francis-Landau et al., 2016), but, by definition, fail to take global coherence into account. To avoid the trade-off between the efficiency of local inference on the one hand and the coherence benefits of global inference on the other, we propose a two-stage approach: In the first stage, candidate entities are ranked by a fast, local inferencebased EL sy"
E17-1078,P15-4007,1,0.862146,"ference system models mention and entity context with a convolutional neural network (CNN). The CNN captures semantic similarity of a given mention’s context at different granularities (small context window, paragraph, document) and the entity context derived from the entity’s Wikipedia page. PH (Pershina et al., 2015): This global inference system applies Personal PageRank to a graph whose nodes represent candidate entities and whose edges indicate if a link between the corresponding Wikipedia articles exists. PH achieves the best CoNLL performance among the systems in our evaluation. TAC-1 (Heinzerling and Strube, 2015): This system uses local and pairwise inference in an easyfirst, incremental rule-based approach. Features are based on popularity priors, contextual occurrence of keywords, entity type, and relational evidence. TAC-2 (Sil et al., 2015): This system employs a global inference approach which partitions a document into sets of mentions that appear near each other. The partitioning is motivated by the intuition that a given mention’s immediate context provides the most salient information for disambiguation, and drastically reduces the search space during global optimization. TAC-3 (Dai et al., 2"
E17-1078,Q14-1037,0,0.127759,"Table 5: Results on CoNLL and TAC15 test sets. Baseline shows performance of the original systems, After verification shows performance after application of our automatic verification method, and ∆ shows the corresponding change. Bold font indicates best results for each metric and system. On CoNLL, the precision increase is less pronounced, arguably owing to the already higher baseline precision, which leaves less room for improvement. Since EL is usually performed as part of a larger task, such as knowledge base completion, search, or as part of a more comprehensive entity analysis system (Durrett and Klein, 2014), good precision is highly desirable in order to minimize error propagation to other system components and downstream applications. 3.3 System Prec Rec F1 FL baseline FL filter FL rerank 85.3 89.2 87.9 85.2 84.7 85.6 85.2 86.9 86.7 Table 6: Comparison of filtering and candidate entity reranking performance on the CoNLL test set. filtering, while reranking increases both precision and recall. Candidate Reranking 3.4 We resort to the binary decision of either retaining or removing an entity linked by an EL system if no candidate entities and no meaningful confidence scores are available. This is"
E17-1078,W14-5201,0,0.0231896,"Missing"
E17-1078,D11-1072,0,0.203629,"Missing"
E17-1078,N16-1150,0,0.423721,", global inference approaches (Kulkarni et al., 2009; Hoffart et al., 2011a) do not scale well with number of mentions and number of candidate entities. In contrast, local approaches do not suffer from scalability issues, since they only optimize the similarity between mention context and candidate KB entry text (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), usually also including a popularity prior2 (Milne and Witten, 2008; Spitkovsky and Chang, 2012). Recent local approaches achieve state-of-the-art results by using convolutional neural networks to capture similarity at multiple context sizes (Francis-Landau et al., 2016), but, by definition, fail to take global coherence into account. To avoid the trade-off between the efficiency of local inference on the one hand and the coherence benefits of global inference on the other, we propose a two-stage approach: In the first stage, candidate entities are ranked by a fast, local inferencebased EL system. In the second stage these results are used to create a semantic profile of the given text, derived from rich data the KB contains about the top-ranked candidates. Since the linking precision of current EL systems is relatively high, we trust that this profile is rea"
E17-1078,P14-5010,0,0.00479715,"Missing"
E17-1078,spitkovsky-chang-2012-cross,0,0.150255,"ng a global coherence measure into the optimization goal (“global inference”). While linguistically well-founded in the concept of lexical cohesion (Halliday and Hasan, 1976), global inference approaches (Kulkarni et al., 2009; Hoffart et al., 2011a) do not scale well with number of mentions and number of candidate entities. In contrast, local approaches do not suffer from scalability issues, since they only optimize the similarity between mention context and candidate KB entry text (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), usually also including a popularity prior2 (Milne and Witten, 2008; Spitkovsky and Chang, 2012). Recent local approaches achieve state-of-the-art results by using convolutional neural networks to capture similarity at multiple context sizes (Francis-Landau et al., 2016), but, by definition, fail to take global coherence into account. To avoid the trade-off between the efficiency of local inference on the one hand and the coherence benefits of global inference on the other, we propose a two-stage approach: In the first stage, candidate entities are ranked by a fast, local inferencebased EL system. In the second stage these results are used to create a semantic profile of the given text,"
E17-1078,S10-1071,0,0.0867794,"Missing"
E17-1078,P13-4007,0,0.0156959,"on. TAC-3 (Dai et al., 2015): This local inference system models mentions and entity context with a CNN and word embeddings. The systems were chosen for their popularity (AIDA, SL), performance on CoNLL (FL, PH), and performance on TAC15 (TAC systems). Unless stated otherwise, we use system output provided by authors for CoNLL systems, and provided by the workshop organizers for TAC15 systems.9 Our evaluation does not include (Globerson et al., 2016) and (Yamada et al., 2016), who report better performance on CoNLL than PH, but were unable to make system output available. 3.1 tion; DKPro WSD (Miller et al., 2013) for modeling entity mentions and links, and using Freebase (Bollacker et al., 2008) and YAGO (Hoffart et al., 2011a) as knowledge bases. After feature extraction, we train a random forest classifier10 for each dataset, one using FL system results for the CoNLL development set (216 documents) and one using TAC-1 results for the TAC15 training set (168 documents). For evaluation, we apply the verifier trained on FL CoNLL development results to the test set results of the FL and AIDA systems, and a verifier trained on PH training data to the PH test set results. For the test set output of TAC sy"
E17-1078,K16-1025,0,0.0132162,"mmediate context provides the most salient information for disambiguation, and drastically reduces the search space during global optimization. TAC-3 (Dai et al., 2015): This local inference system models mentions and entity context with a CNN and word embeddings. The systems were chosen for their popularity (AIDA, SL), performance on CoNLL (FL, PH), and performance on TAC15 (TAC systems). Unless stated otherwise, we use system output provided by authors for CoNLL systems, and provided by the workshop organizers for TAC15 systems.9 Our evaluation does not include (Globerson et al., 2016) and (Yamada et al., 2016), who report better performance on CoNLL than PH, but were unable to make system output available. 3.1 tion; DKPro WSD (Miller et al., 2013) for modeling entity mentions and links, and using Freebase (Bollacker et al., 2008) and YAGO (Hoffart et al., 2011a) as knowledge bases. After feature extraction, we train a random forest classifier10 for each dataset, one using FL system results for the CoNLL development set (216 documents) and one using TAC-1 results for the TAC15 training set (168 documents). For evaluation, we apply the verifier trained on FL CoNLL development results to the test set"
E17-1078,Q14-1019,0,0.0305856,"TAC15 dataset consists of different text genres: clean newswire articles, and noisy discussion forum threads. Analysis of verification performance on these two genres reveals that verification has the biggest impact on noisy text (Table 7, bottom), while the improvement is smaller for two systems on clean text, and even slightly negative for one system, namely the global inference system TAC-2 (Table 7, top). 4 Related Work Global coherence has been successfully employed for EL in a number of seminal works (Kulkarni et al., 2009; Hoffart et al., 2011b; Han et al., 2011), and more recently by Moro et al. (2014), Pershina et al. (2015), and Globerson et al. (2016), among others. These approaches maximize global coherence based on a general notion of semantic relatedness, while considering a fixed number of candidate entities for each mentions. Our approach differs from these in in two regards. Firstly, we introduce specific aspects of coherence, namely entity type coherence, geographic coherence, and tem835 Baseline Rec F1 After verification Prec Rec F1 Genre System Prec News TAC-1 TAC-2 TAC-3 66.5 69.7 63.0 60.3 59.9 59.1 63.2 64.4 61.0 75.8 79.3 71.3 57.0 53.9 54.3 Forum TAC-1 TAC-2 TAC-3 76.0 73.1"
E17-1078,N15-1026,0,0.0325173,"Missing"
E17-1078,D16-1201,0,0.0658539,"Missing"
E17-1078,P11-1138,0,0.269279,"achey et al., 2014) for TAC15. This metric measures precision, recall, and F 1 of matching entity links and mention spans. 3.2 Results and Discussion Evaluation results are shown in Table 5. Our method improves the linking performance of all evaluated EL systems. The impact is most noticeable for the systems that only use local and pairwise inference, namely FL (+1.9 F 1), TAC1 (+2.4 F 1), TAC-3 (+1.1 F 1). The improved TAC-1 result (68.1F 1) is the best published linking score on the TAC15 dataset. Improvements are smaller for the global inference systems, AIDA, HP, and TAC-2. In contrast to Ratinov et al. (2011), who report only a very small increase in linking performance when incorporating global features into a local inferencebased system, our results indicate that global features are useful and lead to considerable improvements. As expected, improvements are caused by increased precision, due to filtering out likely linking mistakes. The fact that this increase is not accompanied by a commensurate decrease in recall, shows that our method predicts wrong linking decisions with high accuracy. On TAC15, we observe considerable improvements in linking precision of up to 10.4 percent. Setup and Implem"
H01-1069,A00-2004,0,0.0116859,"estion find desired semantic type • Engines: IdentiFinder (BBN) CONTEX (Hermjakob) Segment Parsing • Steps: parse segment sentences • Engines: CONTEX (Hermjakob) Match segments against question Rank and prepare answers QA typology • Categorize QA types in taxonomy (Gerber) Output answers Constraint patterns • Identify likely answers in relation to other parts of the sentence (Gerber) Figure 1. Webclopedia architecture. S e g m e n t a t i o n : To decrease the amount of text to be processed, the documents are broken into semantically coherent segments. Two text segmenter—TexTiling [5] and C99 [2]—were tried; the first is used; see [9]. Ranking s e g m e n t s : For each segment, each sentence i s scored using a formula that rewards word and phrase overlap with the question and its expanded query words. Segments are ranked. See [9] Parsing s e g m e n t s : CONTEX parses each sentence of the top-ranked 100 segments (Section 4). Pinpointing: For each sentence, three steps of matching are performed (Section 5); two compare the analyses of the question and the sentence; the third uses the window method t o compute a goodness score. Ranking of answers: The candidate answers’ scores are com"
H01-1069,A00-2016,1,0.869289,"Missing"
H01-1069,W01-1203,1,0.739956,"de (1) significantly more training data; (2) a few additional features, some more treebank cleaning, a bit more background knowledge etc.; and (3) the 251 test questions on Oct. 16 were probably a little bit harder on average, because a few of the TREC-9 questions initially treebanked (and included in the October figures) were selected for early treebanking because they represented particular challenges, hurting subsequent Qtarget processing. 4.2 Parsing Potential Answers The semantic type ontology in CONTEX was extended t o include 115 Qtarget types, plus some combined types; more details in [8]. Beside the Qtargets that refer to concepts i n CONTEX’s concept ontology (see first example below), Qtargets can also refer to part of speech labels (first example), to constituent roles or slots of parse trees (second and third examples), and to more abstract nodes in the QA Typology (later examples). For questions with the Qtargets Q-WHYFAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, and others, the parser also provides Qargs—information helpful for matching (final examples). Semantic ontology types (I-EN-CITY) and part of speech labels (S-PROPER-NAME): What is the capital of Uganda? QTARGET: (((I"
H01-1069,P00-1071,0,0.0567015,"For each sentence, three steps of matching are performed (Section 5); two compare the analyses of the question and the sentence; the third uses the window method t o compute a goodness score. Ranking of answers: The candidate answers’ scores are compared and the winner(s) are output. 3. THE QA TYPOLOGY In order to perform pinpointing deeper than the word level, the system has to produce a representation of what the user i s asking. Some previous work in automated question answering has categorized questions by question word or by a mixture of question word and the semantic class of the answer [11, 10]. To ensure full coverage of all forms of simple question and answer, and to be able to factor in deviations and special requirements, we are developing a QA Typology. We motivate the Typology (a taxonomy of QA types) as follows. There are many ways to ask the same thing: What is the age o f the Queen of Holland? How old is the Netherlands’ queen? How long has the ruler of Holland been alive? Likewise, there are many ways of delivering the same answer: about 60; 63 years old; since January 1938. Such variations form a sort of semantic equivalence class of both questions and answers. Since the"
H01-1069,A00-1023,0,0.0224804,"For each sentence, three steps of matching are performed (Section 5); two compare the analyses of the question and the sentence; the third uses the window method t o compute a goodness score. Ranking of answers: The candidate answers’ scores are compared and the winner(s) are output. 3. THE QA TYPOLOGY In order to perform pinpointing deeper than the word level, the system has to produce a representation of what the user i s asking. Some previous work in automated question answering has categorized questions by question word or by a mixture of question word and the semantic class of the answer [11, 10]. To ensure full coverage of all forms of simple question and answer, and to be able to factor in deviations and special requirements, we are developing a QA Typology. We motivate the Typology (a taxonomy of QA types) as follows. There are many ways to ask the same thing: What is the age o f the Queen of Holland? How old is the Netherlands’ queen? How long has the ruler of Holland been alive? Likewise, there are many ways of delivering the same answer: about 60; 63 years old; since January 1938. Such variations form a sort of semantic equivalence class of both questions and answers. Since the"
H01-1069,P94-1002,0,\N,Missing
H01-1069,P97-1062,1,\N,Missing
H05-2003,P05-1037,1,0.8298,"discussion board with an open source board that is currently used by selected classes. The board provides a platform for evaluating new teaching and learning technologies. Within the discussion board teachers and students post messages about course-related topics. The discussions are organized chronologically within topics and higher-level forums. These ‘live’ discussions are now enabling a new opportunity, the opportunity to apply and evaluate advanced natural language processing (NLP) technology. Recently we designed a summarization system for technical chats and emails on the Linux kernel (Zhou and Hovy, 2005). It clusters discussions according to subtopic structures on the sub-message level, identifies immediate responding pairs using machine-learning methods, and generates subtopicbased mini-summaries for each chat log. Incorporation of this system into the ISI Discussion Board framework, called Classummary, benefits both distance learning and NLP communities. Summaries are created periodically and sent to students and teachers via their preferred medium (emails, text messages on mobiles, web, etc). This relieves users of the burden of reading through a large volume of messages before participati"
hovy-etal-2006-automated,W03-0508,0,\N,Missing
hovy-etal-2006-automated,P02-1040,0,\N,Missing
hovy-etal-2006-automated,N04-1019,0,\N,Missing
hovy-etal-2006-automated,N03-1020,1,\N,Missing
hovy-etal-2006-automated,W02-0406,1,\N,Missing
I05-2047,W98-1124,0,\N,Missing
I05-2047,W97-0703,0,\N,Missing
I05-2047,W97-0707,0,\N,Missing
I05-2047,J91-1002,0,\N,Missing
I05-2047,W97-0711,0,\N,Missing
I05-2047,A97-1042,1,\N,Missing
I05-2047,C94-1056,0,\N,Missing
I05-2047,J02-4004,0,\N,Missing
I05-2047,P95-1034,0,\N,Missing
I05-2047,W97-0704,1,\N,Missing
I13-1004,P98-1012,0,0.0177845,"Missing"
I13-1004,D07-1086,0,0.139347,"ultiple local transformations in the form of ‘wi wi+1 7→ wi |wi+1 ’ or ‘wi |wi+1 7→ wi wi+1 ’. ‘wi wi+1 7→ wi |wi+1 ’ means that S a does not include a segment boundary between tokens wi and wi+1 and S b does; Similarly, ‘wi |wi+1 7→ wi wi+1 ’ means the reverse. For example, for the first query in Table 1, we can have the local transformations ‘download adobe 7→ download |adobe’ and ‘adobe |writer 7→ adobe writer’. The proposed model estimates the score of every local transformation using a binary classifier and then aggregates the individual scores to reach its final decision. 2 Related Work Bergsma and Wang (2007) considered the decision to segment or not between each pair of adjacent words as a binary classification problem. Guo et al. (2008), Yu and Shi (2009), and Kiseleva et al. (2010) used methods based on CRF. As the cost of obtaining labeled data is high, they are usually not feasible to develop a set of labeled data covering all the domains on the web and then train a scalable QS model for web search. The work for web-scale QS are usually unsupervised and utilized various statistics such as mutual information (MI) and frequency count collected from various sources such as web data, query logs,"
I13-1004,P98-2186,0,0.00911503,"arg max x∈T (Sq1 7→Sqj ) f (x) as Contextual Features: POS Tag. Bergsma et al. (2007) show that part-of-speech (POS) tags are useful in their segmentation classification. We also exploit the POS tag pair of wi0 and wi0 +1 as features. For example, intuitively, “NN NN 7→ NN |NN” is more likely to occur than “JJ NN 7→ JJ |NN”. The POS tags that we consider include all types of POS tags. Note that this is different from Bergsma et al. (2007). As their segmentation model only takes care of noun phrase queries, their POS tags are restricted to determiners, adjectives, and nouns. The POS tagger by (Roth and Zelenko, 1998) is used in this paper. Mutual Information (MI). Following previous work (Section 2), we also adopt MI between wi0 and wi0 +1 as our feature. The work (Bergsma and Wang, 2007) also considered the case of a noun phrase with multiple modifiers (e.g. “female bus driver”). To make the segmentation decision between ‘female’ and ‘bus’, M I(‘female’, ‘driver’) is more suitable to represent the information of not separating them than M I(‘female’, ‘bus’). Thus, we also incorporate M I(wi0 −1 , wi0 +1 ) and M I(wi0 , wi0 +2 ) into our feature set. 1&lt;j≤n its index to replace the top-1 segmentation; Othe"
I13-1004,P09-2047,0,\N,Missing
I13-1004,C98-2181,0,\N,Missing
I17-2032,P12-1000,0,0.224966,"Missing"
I17-2032,D13-1157,0,0.0138458,"tistical framework. Our approach has three significant merits. (1) Coherent with fact: we proposed to learn structured knowledge from training dataset, and use it to choose important attributes and determine the structure of description; (2) Fluent: the proposed approach is templatebased which guarantees grammaticality of generated descriptions, and the proposed templated knowledge help to choose semantically correct template; (3) Highly automated: the proposed approach required only weak human intervention. Moreover, in addition to the standard metrics for data-to-text generation, e.g, BLEU (Konstas and Lapata, 2013; Lebret et al., 2016; Kiddon et al., 2016); to evaluate accuracy and fluency of generated descriptions, we propose to measure what to say and how to say separately. 2 Original Text: • The massive 8 GB of memory will allow you to have lots of files open at the same time. • The D520 laptop installed with Windows 7. Extracted Sentence Level Templates: • The massive [RAM Size] of memory will allow you to have lots of files open at the same time. • The D520 laptop installed with [Operating System]. Table 1: Extracted template examples. Words in bracket are aligned attributes; words with underline"
I17-2032,P98-1116,0,0.708152,"Missing"
I17-2032,D16-1128,0,0.0589412,"Missing"
I17-2032,P09-1011,0,0.128014,"as a Ranking Problem In the online stage, given the attributes of a product and the extracted templates, we first generate candidate descriptions by combining all valid templates which fit the given attributes, and then rank the candidate descriptions with the learned writing knowledge. After formulating it as a ranking problem, it is flexible to integrate all kinds of features to estimate the quality of the generated descriptions. Sentence Level Template Extraction Given a parallel dataset, we first align descriptions and theirs corresponding attributes to extract templates. Several studies (Liang et al., 2009; Kondadadi et al., 2013; Lebret et al., 2016) can be applied to solve this problem. In this paper, we follow the approach which is proposed by Kondadadi et al. (2013). Table 1 shows some sample extracted 3.1 Templated Knowledge At the first step of generating description in the online stage, we fill the extracted templates with the attributes of the input data. However, the extracted templates are with different quality or might have semantic gap with the filled values. Value Preference For the first extracted template shown in Table 1, the context words in this template depend on value of “R"
I17-2032,W03-1016,0,0.498883,"Missing"
I17-2032,P02-1040,0,0.0987669,"min of value preferences for all attributes in candidate document, and treat them as separated features; • # missing attribute: is described in Section 2. Structured knowledge: • Attribute prior: is the sum of attribute priors for attributes mentioned in candidate description. • Attribute dependency: is described by Eq. 2. The structured scores which based on different version of P (si , si−1 ) are treated as separated features; Table 2: Features of ranking model. 4 4.3 Overall Performance First of all, we show an example of generated description in Table 5. We adopt language similarity BLEU (Papineni et al., 2002) and retrieval accuracies top-K recall6 as our evaluation metrics, which are widely used in related work. Table 4 shows that both of structured information and template information help improve the overall performance, and the full model achieves the best performance. Notice that the upper bound in term of BLEU is only 31.5, so the above performance is acceptable. For Recall, both Value Preference and Attribute Prior are the most useful features for retrieving the groundtruth. Method Basic +Structured +Templated Full WordCount AttriCount OracleBLEU Experiments 4.1 ranked based on basic feature"
I17-2032,W13-0113,0,0.360595,"Missing"
I17-2032,D16-1032,0,0.0261017,"ificant merits. (1) Coherent with fact: we proposed to learn structured knowledge from training dataset, and use it to choose important attributes and determine the structure of description; (2) Fluent: the proposed approach is templatebased which guarantees grammaticality of generated descriptions, and the proposed templated knowledge help to choose semantically correct template; (3) Highly automated: the proposed approach required only weak human intervention. Moreover, in addition to the standard metrics for data-to-text generation, e.g, BLEU (Konstas and Lapata, 2013; Lebret et al., 2016; Kiddon et al., 2016); to evaluate accuracy and fluency of generated descriptions, we propose to measure what to say and how to say separately. 2 Original Text: • The massive 8 GB of memory will allow you to have lots of files open at the same time. • The D520 laptop installed with Windows 7. Extracted Sentence Level Templates: • The massive [RAM Size] of memory will allow you to have lots of files open at the same time. • The D520 laptop installed with [Operating System]. Table 1: Extracted template examples. Words in bracket are aligned attributes; words with underline are attributes missing in template extracti"
I17-2032,P13-1138,0,0.336971,"roducts from Amazon. Generating descriptions for the products which do not have descriptions, and explaining complex attributes of the product for better understanding are also valuable. Data-to-text generation renders structured records into natural language (Reiter and Dale, 2000), which is similar to this problem. Statistical approaches were employed to reduce extensive development time by learning rules from historical data (Langkilde and Knight, 1998; Liang et al., 2009). Duboue and McKeown (2003) proposed a statistical approach to mine content selection rules for biography descriptions; Kondadadi et al. (2013) and Howald et al. (2013) proposed a statistical approach to select appropriate templates for weather report generation. However, product description generation is different from above work. To generating a useful product description, a system needs to be aware of the relative importance among the attributes of a product and to maintain accuracy at the same time. Successful product description generation needs to address two major challenges: (1) What to say: decide which attributes should be included ∗ This work was done when the second author was an intern at Microsoft Research Asia. 187 Pro"
K18-1046,E06-1002,0,0.528691,"hing function, (b) the interaction-focused model which usually takes a simple representation learning function and uses a complex matching function. In the remaining of this section, we will present the details of a representation-focused model (M-CNN) and an interaction-focused model (K-NRM). We will also discuss the advantages of these two models in the entity linking task. learns contextual features for sequence tagging via recurrent neural networks (Lample et al., 2016). 2.2.2 Candidate Generation Given a mention m, we use several heuristic rules to generate candidate entities similar to (Bunescu and Pasca, 2006; Huang et al., 2014; Sun et al., 2015). Specifically, given a mention m, we retrieve an entity as a candidate from KB, if it matches one of the following conditions: (a) the entity title exactly matches the mention, (b) the anchor text of the entity exactly matches the mention, (c) the title of the entity’s redirected page exactly matches the mention Additionally, we add a special candidate NIL for each mention, which refers to a new entity out of KB. Given a mention, multiple candidates can be retrieved. Hence, we need to do entity disambiguation. 3 Semantic Matching 3.1.1 Convolution Neural"
K18-1046,D07-1074,0,0.841695,"e state-of-the-arts on public tweet datasets. 1 Table 1: An illustration of short text entity linking, with mention Trump underlined. One of the major challenges in entity linking task is ambiguity, where an entity mention could denote to multiple entities in a knowledge base. As shown in Table 1, the mention Trump can refer to U.S. president Donald Trump and also the card name Trump (card games). Many of recent approaches for long text entity linking take the advantage of global context which captures the coherence among the mapped entities for a set of related mentions in a single document (Cucerzan, 2007; Han et al., 2011; Globerson et al., 2016; Heinzerling et al., 2017). However, short texts like tweets are often concise and less coherent, which lack the necessary information for the global methods. In the NEEL dataset (Weller et al., 2016), there are only 3.4 mentions in each tweet on average. Several studies (Liu et al., 2013; Huang et al., 2014) investigate collective tweet entity linking by pre-collecting and considering multiple tweets simultaneously. However, multiple texts are not always available for collection and the process is time-consuming. Thus, we argue that an efficient enti"
K18-1046,K17-1008,0,0.0140787,"Huang et al. (2014) investigate the collective tweet entity linking by considering multiple tweets simultaneously. Meij et al. (2012) and Guo et al. (2013) perform joint detection and disambiguation of mentions for tweet entity linking using feature based learning methods. Recently, some neural network methods have been applied to entity linking to model the local contextual information. He et al. (2013) investigate Stacked Denoising Auto-encoders to learn entity representation. Sun et al. (2015); FrancisLandau et al. (2016) apply convolutional neural networks for entity linking. Eshel et al. (2017) use recurrent neural networks to model the mention contexts. Nie et al. (2018) uses a co-attention mechanism to select informative contexts and entity description for entity disambiguation. However, none of these methods consider combining representation- and interaction-focused semantic matching methods to capture the semantic similarity for entity linking, and use rank aggregation method to combine multiple semantic signals. 6 7 Acknowledgement We thank the anonymous reviewers for their helpful comments. We also thank Jin-Ge Yao, Zhirui Zhang, Shuangzhi Wu and Yin Lin for helpful conversati"
K18-1046,N16-1150,0,0.339801,"ormation is available. Recent neural approaches have shown their superiority in capturing rich semantic sim∗ Correspondence author is Rong Pan. This work was done when the first and second author were interns and the third author was an employee at Microsoft Research Asia. 1 https://en.wikipedia.org/wiki/Donald Trump 476 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 476–485 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics ilarities from mention contexts and entity contents. Sun et al. (2015); Francis-Landau et al. (2016) proposed using convolutional neural networks (CNN) with Siamese (symmetric) architecture to capture the similarity between texts. These approaches can be viewed as representation-focused semantic matching models. The representation-focused model first builds a representation for a single text (e.g., a context or an entity description) with a neural network, and then conducts matching between the abstract representation of two pieces of text. Even though such models capture distinguishable information from both mention and entity side, some concrete matching signals are lost (e.g., exact match"
K18-1046,D17-1277,0,0.0827917,"Missing"
K18-1046,N13-1122,0,0.36864,"fectiveness of jointly combining these two semantic matching signals. 4.1 Datasets & Evaluation Metric In our experiments, we evaluate our proposed model ASM on the following two datasets. NEEL Weller et al. (2016). We use the dataset of Named Entity Extraction & Linking Challenge 2016. The training dataset consists of 6,025 tweets and includes 6,374 non-NIL queries and 2,291 480 NIL queries. The validation dataset consists of 100 tweets and includes 253 non-NIL queries and 85 NIL queries. The testing dataset consists of 300 tweets and includes 738 non-NIL queries and 284 NIL queries. MSR-TEL Guo et al. (2013)3 . This dataset consists of 428 tweets and 770 non-NIL queries. Since the NEEL test dataset has distribution bias problem, we add MSR-TEL as another dataset for the evaluation. In the NEEL testing dataset, 384 out of 1022 queries refer to three entities: ‘Donald Trump’, ‘Star Wars’ and ‘Star Wars (The Force Awakens)’. In this paper, we use accuracy as the major evaluation metric for entity disambiguation. Formally, we denote N as the number of queries and M as the number of correctly linked mentions given the gold mention (the top-ranked entity is the golden entity), accuracy = M N . Besides,"
K18-1046,P13-2006,0,0.0196413,"wever, global approaches are difficult to apply in domains where only short and noisy text is available (e.g. tweets). Many techniques have been proposed to short texts including tweets. Liu et al. (2013) and Huang et al. (2014) investigate the collective tweet entity linking by considering multiple tweets simultaneously. Meij et al. (2012) and Guo et al. (2013) perform joint detection and disambiguation of mentions for tweet entity linking using feature based learning methods. Recently, some neural network methods have been applied to entity linking to model the local contextual information. He et al. (2013) investigate Stacked Denoising Auto-encoders to learn entity representation. Sun et al. (2015); FrancisLandau et al. (2016) apply convolutional neural networks for entity linking. Eshel et al. (2017) use recurrent neural networks to model the mention contexts. Nie et al. (2018) uses a co-attention mechanism to select informative contexts and entity description for entity disambiguation. However, none of these methods consider combining representation- and interaction-focused semantic matching methods to capture the semantic similarity for entity linking, and use rank aggregation method to comb"
K18-1046,E17-1078,1,0.843832,"n illustration of short text entity linking, with mention Trump underlined. One of the major challenges in entity linking task is ambiguity, where an entity mention could denote to multiple entities in a knowledge base. As shown in Table 1, the mention Trump can refer to U.S. president Donald Trump and also the card name Trump (card games). Many of recent approaches for long text entity linking take the advantage of global context which captures the coherence among the mapped entities for a set of related mentions in a single document (Cucerzan, 2007; Han et al., 2011; Globerson et al., 2016; Heinzerling et al., 2017). However, short texts like tweets are often concise and less coherent, which lack the necessary information for the global methods. In the NEEL dataset (Weller et al., 2016), there are only 3.4 mentions in each tweet on average. Several studies (Liu et al., 2013; Huang et al., 2014) investigate collective tweet entity linking by pre-collecting and considering multiple tweets simultaneously. However, multiple texts are not always available for collection and the process is time-consuming. Thus, we argue that an efficient entity disambiguation which requires only a single short text (e.g., a tw"
K18-1046,D11-1072,0,0.200774,"Missing"
K18-1046,P14-1036,1,0.963373,"ald Trump and also the card name Trump (card games). Many of recent approaches for long text entity linking take the advantage of global context which captures the coherence among the mapped entities for a set of related mentions in a single document (Cucerzan, 2007; Han et al., 2011; Globerson et al., 2016; Heinzerling et al., 2017). However, short texts like tweets are often concise and less coherent, which lack the necessary information for the global methods. In the NEEL dataset (Weller et al., 2016), there are only 3.4 mentions in each tweet on average. Several studies (Liu et al., 2013; Huang et al., 2014) investigate collective tweet entity linking by pre-collecting and considering multiple tweets simultaneously. However, multiple texts are not always available for collection and the process is time-consuming. Thus, we argue that an efficient entity disambiguation which requires only a single short text (e.g., a tweet) and can well utilize local contexts is better suited in real word applications. Introduction The task of entity linking aims to link a mention that appears in a piece of text to an entry (i.e. entity) in a knowledge base. For example, as shown in Table 1, given a mention Trump i"
K18-1046,P08-1030,0,0.0471281,"ntic matching method capture both compositional and concrete matching signals (e.g. exact match). Moreover, the pairwise rank aggregation is applied to better combine multiple semantic signals. We have shown the effectiveness of ASM over two datasets through comprehensive experiments. In the future, we will try our model for long text entity linking. Existing entity linking methods can roughly fall into two categories. Early work focus on local approaches, which identifies one mention each time, and each mention is disambiguated separately using hand-crafted features (Bunescu and Pasca, 2006; Ji and Grishman, 2008; Milne and Witten, 2008; Zheng et al., 2010). While recent work on entity linking has largely focus on global methods, which takes the mentions in the document as inputs and find their corresponding entities simultaneously by considering the coherency of entity assignments within a document. (Cucerzan, 2007; Hoffart et al., 2011; Globerson et al., 2016; Ganea and Hofmann, 2017). Global models can tap into highly discriminative semantic signals (e.g. coreference and entity relatedness) that are unavailable to local methods, and have significantly outperformed the local approach on standard dat"
K18-1046,N16-1030,0,0.0468736,"into two types: (a) the representation-focused model which takes a complex representation learning function and uses a relatively simple matching function, (b) the interaction-focused model which usually takes a simple representation learning function and uses a complex matching function. In the remaining of this section, we will present the details of a representation-focused model (M-CNN) and an interaction-focused model (K-NRM). We will also discuss the advantages of these two models in the entity linking task. learns contextual features for sequence tagging via recurrent neural networks (Lample et al., 2016). 2.2.2 Candidate Generation Given a mention m, we use several heuristic rules to generate candidate entities similar to (Bunescu and Pasca, 2006; Huang et al., 2014; Sun et al., 2015). Specifically, given a mention m, we retrieve an entity as a candidate from KB, if it matches one of the following conditions: (a) the entity title exactly matches the mention, (b) the anchor text of the entity exactly matches the mention, (c) the title of the entity’s redirected page exactly matches the mention Additionally, we add a special candidate NIL for each mention, which refers to a new entity out of KB"
K18-1046,P13-1128,0,0.537712,"U.S. president Donald Trump and also the card name Trump (card games). Many of recent approaches for long text entity linking take the advantage of global context which captures the coherence among the mapped entities for a set of related mentions in a single document (Cucerzan, 2007; Han et al., 2011; Globerson et al., 2016; Heinzerling et al., 2017). However, short texts like tweets are often concise and less coherent, which lack the necessary information for the global methods. In the NEEL dataset (Weller et al., 2016), there are only 3.4 mentions in each tweet on average. Several studies (Liu et al., 2013; Huang et al., 2014) investigate collective tweet entity linking by pre-collecting and considering multiple tweets simultaneously. However, multiple texts are not always available for collection and the process is time-consuming. Thus, we argue that an efficient entity disambiguation which requires only a single short text (e.g., a tweet) and can well utilize local contexts is better suited in real word applications. Introduction The task of entity linking aims to link a mention that appears in a piece of text to an entry (i.e. entity) in a knowledge base. For example, as shown in Table 1, gi"
K18-1046,N10-1072,0,0.0275719,"and concrete matching signals (e.g. exact match). Moreover, the pairwise rank aggregation is applied to better combine multiple semantic signals. We have shown the effectiveness of ASM over two datasets through comprehensive experiments. In the future, we will try our model for long text entity linking. Existing entity linking methods can roughly fall into two categories. Early work focus on local approaches, which identifies one mention each time, and each mention is disambiguated separately using hand-crafted features (Bunescu and Pasca, 2006; Ji and Grishman, 2008; Milne and Witten, 2008; Zheng et al., 2010). While recent work on entity linking has largely focus on global methods, which takes the mentions in the document as inputs and find their corresponding entities simultaneously by considering the coherency of entity assignments within a document. (Cucerzan, 2007; Hoffart et al., 2011; Globerson et al., 2016; Ganea and Hofmann, 2017). Global models can tap into highly discriminative semantic signals (e.g. coreference and entity relatedness) that are unavailable to local methods, and have significantly outperformed the local approach on standard datasets(Globerson et al., 2016). However, globa"
K18-1046,D15-1104,1,0.900022,"Missing"
L18-1566,P05-1045,0,0.0869125,"et 2 developed by (Riedel et al., 2010)3 . The NYT corpus contains about 1.8 million news articles. 1 2 3 https://https://www.mturk.com/ http://iesl.cs.umass.edu/riedel/ecml/ Apart from the NYT dataset released by (Riedel et al., 2010), Dataset DSTrainSmall DSTrainLarge DSTest HoffmannTest OurTest #sentences 126,184 522,611 172,448 881 2,040 #pairs 67,946 279,786 96,678 565 1,666 #facts 4,700 18,252 1,950 259 547 Table 1: Statistics about the datasets. When constructing the dataset, named entity mentions were first extracted from the text of NYT articles by using Stanford Named Entity Tagger (Finkel et al., 2005). Then, the named entity mentions were linked to the entities in Freebase by using exact string matching. If a sentence mentions two entities that have a relation in Freebase, then a corresponding instance will be generated and labeled as the relation type. Otherwise, an instance with a label NA which indicates that there is no relation between the entity pair, will be generated. Riedel et al. (2010) mainly focus on the relations related to “people”, “business”, “person” and “location”. There are 53 relation labels including the special label NA in the corpus. The Freebase relations were divid"
L18-1566,P11-1055,0,0.189001,"lation. As shown in Figure 1, the second sentence is a false positive instance. (ii) Multiple labels instances. An entity pair may preserve multiple relation types in a KB. For example, (Bill Gates, founderOf, Microsoft) and (Bill Gates, ceoOf, Microsoft) are clearly true. To deal with the two major issues, multi-instance multilabel learning (MIML) was proposed for RE by relaxing the distant supervision assumption and making the at-least-one assumption: if two entities preserve a relation in a KB, at least one sentence that mentions the entity pair expresses the relation (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). The previous work of MIML-based approaches can be mainly categorized into two folds: (i) feature-based approaches (Hoffmann et al., 2011; Surdeanu et al., 2012) and (ii) neural network-based approaches (Zeng et al., 2015; Lin et al., 2016). However, when we carefully examine the experimental settings of the previous MIML-based work (Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016), we find the following issues which may affect the existing conclusions: (i) When the model comparison experiments were conducted by Zeng et al. (2015; Lin et al. (2016), the comp"
L18-1566,P16-1200,0,0.111484,"Missing"
L18-1566,P09-1113,0,0.708093,"Missing"
L18-1566,D12-1042,0,0.553217,"ure 1, the second sentence is a false positive instance. (ii) Multiple labels instances. An entity pair may preserve multiple relation types in a KB. For example, (Bill Gates, founderOf, Microsoft) and (Bill Gates, ceoOf, Microsoft) are clearly true. To deal with the two major issues, multi-instance multilabel learning (MIML) was proposed for RE by relaxing the distant supervision assumption and making the at-least-one assumption: if two entities preserve a relation in a KB, at least one sentence that mentions the entity pair expresses the relation (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). The previous work of MIML-based approaches can be mainly categorized into two folds: (i) feature-based approaches (Hoffmann et al., 2011; Surdeanu et al., 2012) and (ii) neural network-based approaches (Zeng et al., 2015; Lin et al., 2016). However, when we carefully examine the experimental settings of the previous MIML-based work (Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016), we find the following issues which may affect the existing conclusions: (i) When the model comparison experiments were conducted by Zeng et al. (2015; Lin et al. (2016), the compared models were trained"
L18-1566,D15-1203,0,0.270759,"soft) are clearly true. To deal with the two major issues, multi-instance multilabel learning (MIML) was proposed for RE by relaxing the distant supervision assumption and making the at-least-one assumption: if two entities preserve a relation in a KB, at least one sentence that mentions the entity pair expresses the relation (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). The previous work of MIML-based approaches can be mainly categorized into two folds: (i) feature-based approaches (Hoffmann et al., 2011; Surdeanu et al., 2012) and (ii) neural network-based approaches (Zeng et al., 2015; Lin et al., 2016). However, when we carefully examine the experimental settings of the previous MIML-based work (Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016), we find the following issues which may affect the existing conclusions: (i) When the model comparison experiments were conducted by Zeng et al. (2015; Lin et al. (2016), the compared models were trained on the datasets with different size. Particularly, the neural network-based models (Lin et al., 2016) actually used a large training dataset containing 522, 611 sentences, while feature-based models (Hoffmann et al., 2011"
M93-1025,M92-1001,0,\N,Missing
N03-1020,W00-0408,0,0.0586575,"ent levels: all, most, some, hardly any, or none6. For example, as shown in Figure 1, an assessor marked system units 1.1 and 10.4 (red/dark underlines in the left pane) as sharing some content with the current model unit 2.2 (highlighted green/dark gray in the right). 2.3 Evaluation Metrics Recall at different compression ratios has been used in summarization research to measure how well an automatic system retains important content of original documents (Mani et al. 1998). However, the simple sentence recall measure cannot differentiate system performance appropriately, as is pointed out by Donaway et al. (2000). Therefore, instead of pure sentence recall score, we use coverage score C. We define it as follows7: (Number of MUs marked) • E (1) Total number of MUs in the model summary E, the ratio of completeness, ranges from 1 to 0: 1 for all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 for none. If we ignore E (set it to 1), we obtain simple sentence recall score. We use average coverage scores derived from human judgments as the references to evaluate various automatic scoring methods in the following sections. summary, the better it is. The question is: “Can we apply BLEU directly without"
N03-1020,W02-0406,1,0.24301,"Missing"
N03-1020,2001.mtsummit-papers.68,0,0.0500634,"ns to evaluate summaries as well?”. We first ran IBM’s BLEU evaluation script unmodified over the DUC 2001 model and peer summary set. The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman ρ for the multidocument task is 0.67 using one reference and 0.70 using three. These numbers indicate that they positively correlate at α = 0.018. Therefore, BLEU seems a promising automatic scoring metric for summary evaluation. According to Papineni et al. (2001), BLEU is essentially a precision metric. It measures how well a machine translation overlaps with multiple human translations using n-gram co-occurrence statistics. N-gram precision in BLEU is computed as follows: C= 3 BLEU and N-gram Co-Occurrence To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001). The NIST (NIST 2002) scoring metric is based on BLEU. The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translatio"
N03-1020,H01-1056,0,0.0218636,"Missing"
N03-1020,W00-0400,0,0.088529,"Missing"
N03-1020,P02-1040,0,\N,Missing
N06-1057,P05-1074,0,0.1557,"ry evaluation, a collection of less-strict paraphrases must be created and a matching strategy needs to be investigated. 4 Paraphrase Acquisition Paraphrases are alternative verbalizations for conveying the same information and are required by many Natural Language Processing (NLP) applications. In particular, summary creation and 449 evaluation methods need to recognize paraphrases and their semantic equivalence. Unfortunately, we have yet to incorporate into the evaluation framework previous findings in paraphrase identification and extraction (Barzilay and McKeown, 2001; Pang et al., 2003; Bannard and Callison-Burch, 2005). 4.1 Related Work on Paraphrasing Three major approaches in paraphrase collection are manual collection (domain-specific), collection utilizing existing lexical resources (i.e. WordNet), and derivation from corpora. Hermjakob et al. (2002) view paraphrase recognition as reformulation by pattern recognition. Pang et al. (2003) use word lattices as paraphrase representations from semantically equivalent translations sets. Using parallel corpora, Barzilay and McKeown (2001) identify paraphrases from multiple translations of classical novels, where as Bannard and Callison-Burch (2005) develop a p"
N06-1057,P01-1008,0,0.0474875,"found. To include paraphrase matching in summary evaluation, a collection of less-strict paraphrases must be created and a matching strategy needs to be investigated. 4 Paraphrase Acquisition Paraphrases are alternative verbalizations for conveying the same information and are required by many Natural Language Processing (NLP) applications. In particular, summary creation and 449 evaluation methods need to recognize paraphrases and their semantic equivalence. Unfortunately, we have yet to incorporate into the evaluation framework previous findings in paraphrase identification and extraction (Barzilay and McKeown, 2001; Pang et al., 2003; Bannard and Callison-Burch, 2005). 4.1 Related Work on Paraphrasing Three major approaches in paraphrase collection are manual collection (domain-specific), collection utilizing existing lexical resources (i.e. WordNet), and derivation from corpora. Hermjakob et al. (2002) view paraphrase recognition as reformulation by pattern recognition. Pang et al. (2003) use word lattices as paraphrase representations from semantically equivalent translations sets. Using parallel corpora, Barzilay and McKeown (2001) identify paraphrases from multiple translations of classical novels,"
N06-1057,J93-2003,0,0.0063894,"Missing"
N06-1057,P02-1033,0,0.0434437,"Missing"
N06-1057,N03-1020,1,0.713763,"sed. Naturally, there is a great amount of confidence in manual evaluation since humans can infer, paraphrase, and use world knowledge to relate text units with similar meanings, but which are worded differently. Human efforts are preferred if the evaluation task is easily conducted and managed, and does not need to be performed repeatedly. However, when resources are limited, automated evaluation methods become more desirable. For years, the summarization community has been actively seeking an automatic evaluation methodology that can be readily applied to various summarization tasks. ROUGE (Lin and Hovy, 2003) has gained popularity due to its simplicity and high correlation with human judgments. Even though validated by high correlations with human judgments gathered from previous Document Understanding Conference (DUC) experiments, current automatic procedures (Lin and Hovy, 2003; Hovy et al., 2005) only employ lexical n-gram matching. The lack of support for word or phrase matching that stretches beyond strict lexical matches has limited the expressiveness and utility of these methods. We need a mechanism that supplements literal matching—i.e. paraphrase and synonym—and approximates semantic clos"
N06-1057,N04-1019,0,0.182833,"way: Section 2 introduces previous work in summarization evaluation; Section 3 describes the motivation behind this work; paraphrase acquisition is discussed in Section 4; Section 5 explains in detail our summary comparison mechanism; Section 6 validates ParaEval with human summary judgments; and we conclude and discuss future work in Section 7. 2 Previous Work There has been considerable work in both manual and automatic summarization evaluations. Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). SEE provides a user-friendly environment in which human assessors evaluate the quality of system-produced peer summary by comparing it to a reference summary. Summaries are represented by a list of summary units (sentences, clauses, etc.). Assessors can assign full or partial content coverage score to peer summary units in comparison to the corresponding reference summary units. Grammaticality can also be graded unit-wise. The goal of the Factoid work is to compare the information content of different summaries of the same text and determine the minimum number of summaries, which was shown t"
N06-1057,J03-1002,0,0.00220072,"atistical Machine Translation (SMT) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (Och and Ney, 2004). The sentencebased translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory. This alignment process results in a corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other. We ran the alignment algorithm from (Och and Ney, 2003) on a Chinese-English parallel corpus of 218 million English words. Phrase pairs are extracted by following the method described in (Och and Ney, 2004) where all contiguous phrase pairs having consistent alignments are extraction candidates. The resulting phrase table is of high quality; both the alignment models and phrase extraction methFigure 2. An example of paraphrase extraction. ods have been shown to produce very good results for SMT. Using these pairs we build paraphrase sets by joining together all English phrases with the same Chinese translation. Figure 2 shows an example word/phras"
N06-1057,J04-4002,0,0.00402602,"nard and Callison-Burch (2005) develop a probabilistic representation for paraphrases extracted from large Machine Translation (MT) data sets. 4.2 Extracting Paraphrases Our method to automatically construct a large domain-independent paraphrase collection is based on the assumption that two different English phrases of the same meaning may have the same translation in a foreign language. Phrase-based Statistical Machine Translation (SMT) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (Och and Ney, 2004). The sentencebased translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory. This alignment process results in a corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other. We ran the alignment algorithm from (Och and Ney, 2003) on a Chinese-English parallel corpus of 218 million English words. Phrase pairs are extracted by following the method described in (Och and Ney, 2004) where all contiguous phrase pairs having"
N06-1057,N03-1024,0,0.16803,"Missing"
N06-1057,W03-0508,0,0.0210048,"Missing"
N06-1057,P02-1040,0,\N,Missing
N06-1059,W03-0508,0,\N,Missing
N06-1059,P02-1040,0,\N,Missing
N06-1059,N04-1019,0,\N,Missing
N06-1059,N03-1020,1,\N,Missing
N06-1059,W04-1013,1,\N,Missing
N15-1126,P11-1051,0,0.0155573,"Li et al., 2012). The main impact of our trigger scoping strategy is to narrow down the text span of searching for facts, from sentence-level 3 A poss−1 B means there is a possession modifier relation (poss) between B and A. 1207 to fragment-level. We only focus on analyzing the content which is likely to contain an answer. Our trigger scoping method is also partially inspired from the negation scope detection work (e.g., (Szarvas et al., 2008; Elkin et al., 2005; Chapman et al., 2001; Morante and Daelemans, 2009; Agarwal and Yu, 2010)) and reference scope identification in citing sentences (Abu-Jbara and Radev, 2011; AbuJbara and Radev, 2012). 5 Conclusions and Future Work In this paper we explore the role of triggers and their scopes in biographical fact extraction. We implement the trigger scoping strategy using two simple but effective methods. Experiments demonstrate that our approach outperforms state-of-the-art without any syntactic analysis and external knowledge bases. In the future, we will aim to explore how to generate a trigger list for a “surprise” new fact type within limited time. Acknowledgement This work was supported by the U.S. DARPA Award No. FA8750-13-2-0045 in the Deep Exploration a"
N15-1126,N12-1009,0,0.0328479,"Missing"
N15-1126,W14-2907,0,0.0253086,"Missing"
N15-1126,P14-5010,0,0.00539561,"types is regarded as the right boundary. The rule-based scoping result of the walk-through example is as follows: Paul Francis Conrad and his twin [<brother>, James, were] [<born> in Cedar Rapids, Iowa, on June 27, 1924,] [<sons> of Robert H. Conrad and Florence Lawler Conrad.] 2.2.2 Supervised Classification Alternatively we regard scope identification as a classification task. For each detected trigger, scope identification is performed as a binary classification of each token in the sentence as to whether it is within or outside of a trigger’s scope. We apply the Stanford CoreNLP toolkit (Manning et al., 2014) to annotate part-of-speech tags and names in each document. We design the following features to train a classifier. • Position: The feature takes value 1 if the word appears before the trigger, and 0 otherwise. • Distance: The distance (in words) between the word and the trigger. • POS: POS tags of the word and the trigger. • Name Entity: The name entity type of the word. • Interrupt: The feature takes value 1 if there is a verb or a trigger with other fact type between the trigger and the word, and 0 otherwise. Verbs and triggers with other fact types can effectively change the current topic"
N15-1126,W09-1304,0,0.0189502,"upon external knowledge bases and it is timeconsuming to manually write or edit patterns (Sun et al., 2011; Li et al., 2012). The main impact of our trigger scoping strategy is to narrow down the text span of searching for facts, from sentence-level 3 A poss−1 B means there is a possession modifier relation (poss) between B and A. 1207 to fragment-level. We only focus on analyzing the content which is likely to contain an answer. Our trigger scoping method is also partially inspired from the negation scope detection work (e.g., (Szarvas et al., 2008; Elkin et al., 2005; Chapman et al., 2001; Morante and Daelemans, 2009; Agarwal and Yu, 2010)) and reference scope identification in citing sentences (Abu-Jbara and Radev, 2011; AbuJbara and Radev, 2012). 5 Conclusions and Future Work In this paper we explore the role of triggers and their scopes in biographical fact extraction. We implement the trigger scoping strategy using two simple but effective methods. Experiments demonstrate that our approach outperforms state-of-the-art without any syntactic analysis and external knowledge bases. In the future, we will aim to explore how to generate a trigger list for a “surprise” new fact type within limited time. Ackn"
N15-1126,W08-0606,0,0.0120013,"ly expensive: Distant Supervision (Surdeanu et al., 2010) relies upon external knowledge bases and it is timeconsuming to manually write or edit patterns (Sun et al., 2011; Li et al., 2012). The main impact of our trigger scoping strategy is to narrow down the text span of searching for facts, from sentence-level 3 A poss−1 B means there is a possession modifier relation (poss) between B and A. 1207 to fragment-level. We only focus on analyzing the content which is likely to contain an answer. Our trigger scoping method is also partially inspired from the negation scope detection work (e.g., (Szarvas et al., 2008; Elkin et al., 2005; Chapman et al., 2001; Morante and Daelemans, 2009; Agarwal and Yu, 2010)) and reference scope identification in citing sentences (Abu-Jbara and Radev, 2011; AbuJbara and Radev, 2012). 5 Conclusions and Future Work In this paper we explore the role of triggers and their scopes in biographical fact extraction. We implement the trigger scoping strategy using two simple but effective methods. Experiments demonstrate that our approach outperforms state-of-the-art without any syntactic analysis and external knowledge bases. In the future, we will aim to explore how to generate"
P02-1058,J93-1003,0,0.0966277,"Missing"
P02-1058,C00-1072,1,0.481811,"EBCL -SIGN ATURE -KUCAN :L EX 0. 63636 36363 63636 ))))) )) Figure 1. Sample key concept structure. In a key step for locating important sentences, NeATS computes the likelihood ratio λ (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams1, using the on- topic document collection as the relevant set and the off-topic document collection as the irrelevant set. Figure 1 shows the top 5 concepts with their relevancy scores (-2λ) for the topic “Slovenia Secession from Yugoslavia ” in the DUC-2001 test collection. This is similar to the idea of topic signature introduced in (Lin and Hovy 2000). With the individual key concepts available, we proceed to cluster these concepts in order to identify major subtopics within the main topic. Clusters are formed through strict lexical connection. For example, Milan and Kucan are grouped as “Milan Kucan” since “Milan Kucan” is a key bigram concept; while Croatia, Yugoslavia, Slovenia, republic, and are joined due to the connections as follows: • Slovenia Croatia • Croatia Slovenia • Yugoslavia Slovenia • republic Slovenia 1 Closed class words (of, in, and, are, and so on) were ignored in constructing unigrams, bigrams and trigrams. • Croatia"
P02-1058,W01-0100,0,0.254104,"d in an effectiv e way? express all, most, some or hardly any of the content of the current model unit. 4 Evaluation Metrics One goal of DUC-2001 was to debug the evaluation procedures and identify stable metrics that could serve as common reference points. NIST did not define any official performance metric in DUC-2001. It released the raw evaluation results to DUC -2001 participants and encouraged them to propose metrics that would help progress the field. 4.1.1 Recall, Coverage, Retention and Weighted Retention Recall at different compression ratios has been used in summarization research (Mani 2001) to measure how well an automatic system retains important content of original documents. Assume we have a system summary Ss and a model summary Sm. The number of sentences occurring in both S s and S m is N a, the number of sentences in Ss is N s, and the number of sentences in Sm is N m. Recall is defined as Na/Nm. The Compression Ratio is defined as the length of a summary (by words or sentences) divided by the length of its original document. DUC-2001 set the compression lengths to 50, 100, 200, and 400 words for the multi-document summarization task. However, applying recall in DUC -2001"
P02-1058,J98-3005,0,\N,Missing
P03-2021,A97-1029,0,0.0135954,"ne and it is not used for summarization. Note also that the sentence that starts with “However,...” scored much lower than the selected two – its color is approximately half diluted into the background. There are quite a few sentences in the second part of the document that scored relatively high. However, these sentences are below the sentence position cutoff so they do not appear in the summary. We illustrate this by rendering such sentences in slanted style. 3.3 Alternative Summaries The bottom part of the summary panel is occupied by the map-based visualization. We use BBN’s IdentiFinder (Bikel et al., 1997) to detect the names of geographic locations in the document set. We then select the most frequently used location names and place them on world map. Each location is identified by a black dot followed by a frequency chart and the location name. The frequency chart is a bar chart where each bar corresponds to a document. The bar is painted using the document color and the length of the bar is proportional to the number of times the location name is used in the document. The document set we used in our example describes the progress of the hurricane Andrew and its effect on Florida, Louisiana,"
P03-2021,J93-1003,0,0.0305385,"ich addresses these three directions. The iNeATS system is built on top of the NeATS multi-document summarization system. In the following section we give a brief overview of the NeATS system and in Section 3 describe the interactive version. 2 NeATS NeATS (Lin and Hovy, 2002) is an extractionbased multi-document summarization system. It is among the top two performers in DUC 2001 and 2002 (Over, 2001). It consists of three main components: Content Selection The goal of content selection is to identify important concepts mentioned in a document collection. NeATS computes the likelihood ratio (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams and clusters these concepts in order to identify major subtopics within the main topic. Each sentence in the document set is then ranked, using the key concept structures. These n-gram key concepts are called topic signatures. Content Filtering NeATS uses three different filters: sentence position, stigma words, and redundancy filter. Sentence position has been used as a good important content filter since the late 60s (Edmundson, 1969). NeATS applies a simple sentence filter that only retains the N lead sentences. Some sentences sta"
P03-2021,P02-1058,1,\N,Missing
P03-2021,C94-2144,0,\N,Missing
P04-1077,2001.mtsummit-papers.3,0,0.0346222,"rict n-gram matching to skipbigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An"
P04-1077,N03-1003,0,0.0107407,"Missing"
P04-1077,2003.mtsummit-papers.32,0,0.0552346,"translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two r"
P04-1077,W04-1013,1,0.115126,"Missing"
P04-1077,C04-1072,1,0.629085,"Missing"
P04-1077,W95-0115,0,0.0222944,"Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two recent large-scale machine translation evaluations. Recently, Turian et al. (2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation. However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while"
P04-1077,J82-2005,0,0.673361,"Missing"
P04-1077,niessen-etal-2000-evaluation,1,0.175187,"rds in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that"
P04-1077,2001.mtsummit-papers.68,0,0.134315,"ences. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two recent large-scale machine translation evaluations. Recently, Turian et al. (2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation. However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram (n &gt; 1) matches achieved similar per"
P04-1077,C02-1073,0,0.00991641,"men et al. 1989). Given two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length. We can find the LCS of two sequences of length m and n using standard dynamic programming technique in O(mn) time. LCS has been used to identify cognate candidates during construction of N-best translation lexicons from parallel text. Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. He used as an approximate string matching algorithm. Saggion et al. (2002) used normalized pairwise LCS (NP-LCS) to compare similarity between two texts in automatic summarization evaluation. NP-LCS can be shown as a special case of Equation (6) with β = 1. However, they did not provide the correlation analysis of NP-LCS with 1 This is a real machine translation output. The “kill” in S2 or S3 does not match with “killed” in S1 in strict word-to-word comparison. 2 human judgments and its effectiveness as an automatic evaluation measure. To apply LCS in machine translation evaluation, we view a translation as a sequence of words. The intuition is that the longer the L"
P04-1077,C92-2067,0,0.03926,"vel structure similarity naturally and identifies longest co-occurring insequence n-grams automatically. The second method relaxes strict n-gram matching to skipbigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity b"
P04-1077,N03-2021,0,\N,Missing
P04-1077,P02-1040,0,\N,Missing
P04-1077,N03-1020,1,\N,Missing
P07-1011,P04-1022,1,0.832643,"Missing"
P07-1011,W05-0904,0,0.0123293,"e ratio of the number of erroneous LCs to the number of collocations in each sentence. Perplexity from Language Model (PLM) Perplexity measures are extracted from a trigram language model trained on a general English corpus using the SRILM-SRI Language Modeling Toolkit (Stolcke, 2002). We calculate two values for each sentence: lexicalized trigram perplexity and part of speech (POS) trigram perplexity. The erroneous sentences would have higher perplexity. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences (Liu and Gildea, 2005). To measure the coherence of sentences, we use a statistical parser Toolkit (Collins, 1997) to assign each sentence a parser’s score that is the related log probability of parsing. We assume that erroneous sentences with undesirable sentence structures are more likely to receive lower scores. Function Word Density (FWD) We consider the density of function words (Corston-Oliver et al., 2001), i.e. the ratio of function words to content words. This is inspired by the work (Corston-Oliver et al., 2001) showing that function word density can be effective in distinguishing between human references"
P07-1011,P06-1032,0,0.446282,"high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing 81 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003; Brockett et al., 2006) usually need errors to be specified and tagged in the training sentences, which requires expert help to be recruited and is ti"
P07-1011,P06-1031,0,0.0698495,"al errors in the writing of English learners. However, it could be expensive to write rules manually. Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing 81 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003;"
P07-1011,J93-2003,0,0.00558954,"pose a new approach to detecting erroneous sentences by integrating pattern discovery with supervised learning models. Experimental results show that our techniques are promising. 1 Gao Cong Introduction Detecting erroneous/correct sentences has the following applications. First, it can provide feedback for writers of English as a Second Language (ESL) as to whether a sentence contains errors. Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources for a wide range of applications, such as statistical machine translation (Brown et al., 1993) and cross-lingual information retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. ∗ Work done while the author was a visiting student at MSRA † Work done while the author was a visiting student at MSRA The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mis"
P07-1011,P98-1032,0,0.105684,"oyed to identify and correct writing errors (Brockett et al., 2006). This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contrast to existing statistical methods, our technique needs neither errors tagged nor parallel corpora, and is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a sentence is done by grammar, sentence structure, and lexical choice. Another related work is Machine Translation (MT) evaluation. Classification models are employed in (Corston-Oliver et al., 2001; Gamon et al., 2005) to evaluate the well-formedness of machine translation outputs. The writers of ESL and MT normally make different mistakes:"
P07-1011,A00-2019,0,0.804432,"Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing 81 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003; Brockett et al., 2006) usually need errors to be specified and tagged in the training sentences, which req"
P07-1011,P97-1003,0,0.0417279,"rom Language Model (PLM) Perplexity measures are extracted from a trigram language model trained on a general English corpus using the SRILM-SRI Language Modeling Toolkit (Stolcke, 2002). We calculate two values for each sentence: lexicalized trigram perplexity and part of speech (POS) trigram perplexity. The erroneous sentences would have higher perplexity. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences (Liu and Gildea, 2005). To measure the coherence of sentences, we use a statistical parser Toolkit (Collins, 1997) to assign each sentence a parser’s score that is the related log probability of parsing. We assume that erroneous sentences with undesirable sentence structures are more likely to receive lower scores. Function Word Density (FWD) We consider the density of function words (Corston-Oliver et al., 2001), i.e. the ratio of function words to content words. This is inspired by the work (Corston-Oliver et al., 2001) showing that function word density can be effective in distinguishing between human references and machine outputs. In this paper, we calculate the densities of seven kinds of function w"
P07-1011,P01-1020,0,0.163624,"nd is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a sentence is done by grammar, sentence structure, and lexical choice. Another related work is Machine Translation (MT) evaluation. Classification models are employed in (Corston-Oliver et al., 2001; Gamon et al., 2005) to evaluate the well-formedness of machine translation outputs. The writers of ESL and MT normally make different mistakes: in general, ESL writers can write overall grammatically correct sentences with some local mistakes while MT outputs normally produce locally well-formed phrases with overall grammatically wrong sentences. Hence, the manual features designed for MT evaluation are not applicable to detect erroneous sentences from ESL learners. LSPs differ from the traditional sequential patterns, e.g., (Agrawal and Srikant, 1995; Pei et al., 2001) in that LSPs are atta"
P07-1011,2005.eamt-1.15,0,0.196679,"Introduction Detecting erroneous/correct sentences has the following applications. First, it can provide feedback for writers of English as a Second Language (ESL) as to whether a sentence contains errors. Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources for a wide range of applications, such as statistical machine translation (Brown et al., 1993) and cross-lingual information retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. ∗ Work done while the author was a visiting student at MSRA † Work done while the author was a visiting student at MSRA The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mistakes (Yukio et al., 2001; Gui and Yang, 2003) made by ESL learners include spelling, lexical collocation, sentence structure, tense, agreement, verb formation, wrong PartOf-Speech (POS), art"
P07-1011,H05-1006,0,0.137165,"use of hand-crafted rules, e.g., template rules (Heidorn, 2000) and mal-rules in context-free grammars (Michaud et al., 2000; Bender et al., 2004). As discussed in Section 1, manual rule based methods have some shortcomings. The second category uses statistical techniques to detect erroneous sentences. An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS. The method (Izumi et al., 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs. They also require specifying error tags that can tell the specific errors and their corrections in the training corpus. The phrasal Statistical Machine Translation (SMT) technique is employed to identify and correct writing errors (Brockett et al., 2006). This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contr"
P07-1011,C98-1032,0,\N,Missing
P07-1011,P03-2026,0,\N,Missing
P07-1129,H05-1121,0,0.0304208,"nts to identify the relevant documents is time consuming and tends to become overwhelming. Individuals need to be able to retrieve the relevant consultation documents efficiently and effectively. Therefore, this work presents a novel mechanism to automatically retrieve the relevant consultation documents with respect to users' problems. Traditional information retrieval systems represent queries and documents using a bag-of-words approach. Retrieval models, such as the vector space model (VSM) (Baeza-Yates and RibeiroNeto, 1999) and Okapi model (Robertson et al., 1995; Robertson et al., 1996; Okabe et al., 2005), are then adopted to estimate the relevance between queries and documents. The VSM represents each query and document as a vector of words, and adopts the cosine measure to estimate their relevance. The Okapi model, which has been used on the Text REtrieval Conference (TREC) collections, developed a family of word-weighting functions Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024–1031, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Query: Consultation Document I broke up with my boyfriend. &lt;Depressed&gt; I o"
P07-1129,C04-1164,1,0.82514,"Missing"
P08-1019,P03-1003,0,0.0816273,"Missing"
P08-1019,J98-2002,0,0.018437,"arch Our approach to question search consists of two steps: (a) summarize questions in a data structure consisting of question topic and question focus; (b) 157 model question topic and question focus in a language modeling framework for search. In the step (a), we employ the MDL-based (Minimum Description Length) tree cut model for automatically identifying question topic and question focus. Thus, this section will begin with a brief review of the MDL-based tree cut model and then follow that by an explanation of steps (a) and (b). 2.1 The MDL-based tree cut model Formally, a tree cut model (Li and Abe, 1998) can be represented by a pair consisting of a tree cut , and a probability parameter vector of the same length, that is, where and , (1) , ,.. ,…, (2) are , , where , , … are classes determined by a cut 1. A ‘cut’ in a tree is in the tree and ∑ any set of nodes in the tree that defines a partition of all the nodes, viewing each node as representing the set of child nodes as well as itself. For example, the cut indicated by the dash line in Figure 1 corresponds to three classes: , , , , and , , , . Figure 1. An Example on the Tree Cut Model A straightforward way for determining a cut of a tree"
P08-1019,J93-2003,0,0.0108834,"guage modeling approach (for information retrieval) and translation-based approach (for information retrieval). Our idea of modeling question structure for search can naturally extend to Jeon et al.’s model. More specifically, by using translation probabilities, we can rewrite equation (11) and (12) as follow: ·  ·∑ 1 ·  ∑ · 1 |·  ·  | | ·  ·  | (13) (14) where | denotes the probability that topic term is the translation of . In our experiments, to estimate the probability |, we used the collections of question titles and question descriptions as the parallel corpus and the IBM model 1 (Brown et al., 1993) as the alignment model. Usually, users reiterate or paraphrase their questions (already described in question titles) in question descriptions. We utilized the new model elaborated by equation (13) and (14) for searching questions about ‘travel’ and ‘computers & internet’. The new model is denoted as ‘SMT-CUT’. Table 6 provides the evaluation results. The evaluation was conducted with exactly the same setting as in Section 3. From Table 6, we see that the performance of our approach can be further boosted by using translation probability. Data TRLTST CITST Methods MAP R-Precision LMIR-CUT 0.2"
P08-1019,P05-1026,0,0.00740134,"Burke et al., 1997) heuristically combines statistical similarities and semantic similarities between questions to rank FAQs. Conventional vector space models are used to calculate the statistical similarity and WordNet (Fellbaum, 1998) is used to estimate the semantic similarity. Sneiders (2002) proposed template based FAQ retrieval systems. Lai et al. (2002) proposed an approach to automatically mine FAQs from the Web. Jijkoun and Rijke (2005) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages, and treated Q/A pair retrieval as a fielded search task. Harabagiu et al. (2005) used a Question Answer Database (known as QUAB) to support interactive question answering. They compared seven different similarity metrics for selecting related questions from QUAB and found that the conceptbased metric performed best. Recently, the research of question search has been further extended to the community-based Q&A data. For example, Jeon et al. (Jeon et al., 2005a; Jeon et al., 2005b) compared four different retrieval methods, i.e. vector space model, Okapi, language model (LM), and translation-based model, for automatically fixing the lexical chasm between In this paper, we h"
P08-1019,C02-1011,1,0.224227,"owing, with a series of definitions, we will describe how a question tree is constructed from a collection of questions. Let’s begin with explaining the representation of a question. A straightforward method is to represent a question as a bag-of-words (possibly ignoring stop words). However, this method cannot discern ‘the hotels in Paris’ from ‘the Paris hotel’. Thus, we turn to use the linguistic units carrying on more semantic information. Specifically, we make use of two kinds of units: BaseNP (Base Noun Phrase) and WH-ngram. A BaseNP is defined as a simple and non-recursive noun phrase (Cao and Li, 2002). A WH-ngram is an ngram beginning with WH-words. The WH-words that we consider include ‘when’, ‘what’, ‘where’, ‘which’, and ‘how’. We refer to these two kinds of units as ‘topic terms’. With ‘topic terms’, we represent a question as a topic chain and a set of questions as a question tree. Definition 1 (Topic Profile) The topic profile of a topic term in a categorized question collection is a probability distribution of categories | where is a set of categories. , | ∑ (7) , where , is the frequency of the topic term within category . Clearly, we 1. have ∑ | By ‘categorized questions’, we refe"
P08-1019,W02-1904,0,0.0223412,"lly similar questions in an archive. 5 Related Work 6 The major focus of previous research efforts on question search is to tackle the lexical chasm problem between questions. The research of question search is first conducted using FAQ data. FAQ Finder (Burke et al., 1997) heuristically combines statistical similarities and semantic similarities between questions to rank FAQs. Conventional vector space models are used to calculate the statistical similarity and WordNet (Fellbaum, 1998) is used to estimate the semantic similarity. Sneiders (2002) proposed template based FAQ retrieval systems. Lai et al. (2002) proposed an approach to automatically mine FAQs from the Web. Jijkoun and Rijke (2005) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages, and treated Q/A pair retrieval as a fielded search task. Harabagiu et al. (2005) used a Question Answer Database (known as QUAB) to support interactive question answering. They compared seven different similarity metrics for selecting related questions from QUAB and found that the conceptbased metric performed best. Recently, the research of question search has been further extended to the community-based Q&A data."
P08-1081,N06-1027,0,0.0136922,". In contrast, given a forum thread, we extract questions, their contexts, and their answers as summaries. Shrestha and McKeown (2004)’s work on email summarization is closer to our work. They used RIPPER as a classifier to detect interrogative questions and their answers and used the resulting question and answer pairs as summaries. However, it did not consider contexts of questions and dependency between answer sentences. We also note the existing work on extracting knowledge from discussion threads. Huang et al.(2007) used SVM to extract input-reply pairs from forums for chatbot knowledge. Feng et al. (2006a) used cosine similarity to match students’ query with reply posts for discussion-bot. Feng et al. (2006b) identified the most important message in online classroom discussion board. Our problem is quite different from the above work. Detecting context for question in forums is related to the context detection problem raised in the QA roadmap paper commissioned by ARDA (Burger et al., 2006). To our knowledge, none of the previous work addresses the problem of context detection. The method of finding follow-up questions (Yang et al., 2006) from TREC context track could be adapted for context d"
P08-1081,W06-1643,0,0.106336,"Missing"
P08-1081,P06-1114,0,0.0132137,"ur knowledge, none of the previous work addresses the problem of context detection. The method of finding follow-up questions (Yang et al., 2006) from TREC context track could be adapted for context detection. However, the followup relationship is limited between questions while context is not. In our other work (Cong et al., 2008), we proposed a supervised approach for question detection and an unsupervised approach for answer detection without considering context detection. Extensive research has been done in questionanswering, e.g. (Berger et al., 2000; Jeon et al., 2005; Cui et al., 2005; Harabagiu and Hickl, 2006; Dang et al., 2007). They mainly focus on con712 structing answer for certain types of question from a large document collection, and usually apply sophisticated linguistic analysis to both questions and the documents in the collection. Soricut and Brill (2006) used statistical translation model to find the appropriate answers from their QA pair collections from FAQ pages for the posted question. In our scenario, we not only need to find answers for various types of questions in forum threads but also their contexts. 3 Context and Answer Detection A question is a linguistic expression used by"
P08-1081,W03-0430,0,0.00402016,"ndency relationship between sentences. To this end, we proposed a general framework to detect contexts and answers based on Conditional Random Fields (Lafferty et al., 2001) (CRFs) which are able to model the sequential dependencies between contiguous nodes. A CRF is an undirected graphical model G of the conditional distribution P (Y|X). Y are the random variables over the labels of the nodes that are globally conditioned on X, which are the random variables of the observations. (See Section 3.4 for more about CRFs) Linear CRF model has been successfully applied in NLP and text mining tasks (McCallum and Li, 2003; Sha and Pereira, 2003). However, our problem cannot be modeled with Linear CRFs in the same way as other NLP tasks, where one node has a unique label. In our problem, each node (sentence) might have multiple labels since one sentence could be the context of multiple questions in a thread. Thus, it is difficult to find a solution to tag context sentences for all questions in a thread in single pass. Here we assume that questions in a given thread are independent and are found, and then we can label a thread with m questions one-by-one in mpasses. In each pass, one question Qi is selected as f"
P08-1081,N04-4027,0,0.0138336,"ance for context detection. The rest of this paper is organized as follows: The next section discusses related work. Section 3 presents the proposed techniques. We evaluate our techniques in Section 4. Section 5 concludes this paper and discusses future work. 2 Related Work There is some research on summarizing discussion threads and emails. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into subtopics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. Carenini et al (2007) leveraged both quotation relation and clue words for email summarization. In contrast, given a forum thread, we extract questions, their contexts, and their answers as summaries. Shrestha and McKeown (2004)’s work on email summarization is closer to our work. They used RIPPER as a classifier to detect interrogative questions and their answers and used the resulting question and answer pairs as summaries. However, it did not consider contexts of questions and dependency between answer s"
P08-1081,N03-1028,0,0.0615982,"ween sentences. To this end, we proposed a general framework to detect contexts and answers based on Conditional Random Fields (Lafferty et al., 2001) (CRFs) which are able to model the sequential dependencies between contiguous nodes. A CRF is an undirected graphical model G of the conditional distribution P (Y|X). Y are the random variables over the labels of the nodes that are globally conditioned on X, which are the random variables of the observations. (See Section 3.4 for more about CRFs) Linear CRF model has been successfully applied in NLP and text mining tasks (McCallum and Li, 2003; Sha and Pereira, 2003). However, our problem cannot be modeled with Linear CRFs in the same way as other NLP tasks, where one node has a unique label. In our problem, each node (sentence) might have multiple labels since one sentence could be the context of multiple questions in a thread. Thus, it is difficult to find a solution to tag context sentences for all questions in a thread in single pass. Here we assume that questions in a given thread are independent and are found, and then we can label a thread with m questions one-by-one in mpasses. In each pass, one question Qi is selected as focus and each other sent"
P08-1081,C04-1128,0,0.62306,"ing discussion threads and emails. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into subtopics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. Carenini et al (2007) leveraged both quotation relation and clue words for email summarization. In contrast, given a forum thread, we extract questions, their contexts, and their answers as summaries. Shrestha and McKeown (2004)’s work on email summarization is closer to our work. They used RIPPER as a classifier to detect interrogative questions and their answers and used the resulting question and answer pairs as summaries. However, it did not consider contexts of questions and dependency between answer sentences. We also note the existing work on extracting knowledge from discussion threads. Huang et al.(2007) used SVM to extract input-reply pairs from forums for chatbot knowledge. Feng et al. (2006a) used cosine similarity to match students’ query with reply posts for discussion-bot. Feng et al. (2006b) identifie"
P08-1081,C04-1079,0,0.0221809,"achieves better performance for context detection. The rest of this paper is organized as follows: The next section discusses related work. Section 3 presents the proposed techniques. We evaluate our techniques in Section 4. Section 5 concludes this paper and discusses future work. 2 Related Work There is some research on summarizing discussion threads and emails. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into subtopics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. Carenini et al (2007) leveraged both quotation relation and clue words for email summarization. In contrast, given a forum thread, we extract questions, their contexts, and their answers as summaries. Shrestha and McKeown (2004)’s work on email summarization is closer to our work. They used RIPPER as a classifier to detect interrogative questions and their answers and used the resulting question and answer pairs as summaries. However, it did not consider contexts of questions and depen"
P08-1081,P94-1019,0,0.00466449,"BFGS (limited memory Broyden-Fletcher-Goldfarb-Shanno) can be used to optimize objective function Lλ , while for complicated CRFs, Loopy BP are used instead to calculate the marginal probability. 3.5 Features used in CRF models The main features used in Linear CRF models for context detection are listed in Table 3. The similarity feature is to capture the word similarity and semantic similarity between candidate contexts and answers. The word similarity is based on cosine similarity of TF/IDF weighted vectors. The semantic similarity between words is computed based on Wu and Palmer’s measure (Wu and Palmer, 1994) using WordNet (Fellbaum, 1998).1 The similarity between contiguous sentences will be used to capture the dependency for CRFs. In addition, to bridge the lexical gaps between question and context, we learned top-3 context terms for each question term from 300,000 question-description pairs obtained from Yahoo! Answers using mutual information (Berger et al., 2000) ( question description in Yahoo! Answers is comparable to contexts in fo1 The semantic similarity between sentences is calculated as in (Yang et al., 2006). 715 Similarity features: · Cosine similarity with the question · Similarity"
P08-1081,W06-3005,0,0.0882293,"ct input-reply pairs from forums for chatbot knowledge. Feng et al. (2006a) used cosine similarity to match students’ query with reply posts for discussion-bot. Feng et al. (2006b) identified the most important message in online classroom discussion board. Our problem is quite different from the above work. Detecting context for question in forums is related to the context detection problem raised in the QA roadmap paper commissioned by ARDA (Burger et al., 2006). To our knowledge, none of the previous work addresses the problem of context detection. The method of finding follow-up questions (Yang et al., 2006) from TREC context track could be adapted for context detection. However, the followup relationship is limited between questions while context is not. In our other work (Cong et al., 2008), we proposed a supervised approach for question detection and an unsupervised approach for answer detection without considering context detection. Extensive research has been done in questionanswering, e.g. (Berger et al., 2000; Jeon et al., 2005; Cui et al., 2005; Harabagiu and Hickl, 2006; Dang et al., 2007). They mainly focus on con712 structing answer for certain types of question from a large document c"
P08-1081,P05-1037,0,0.0156498,"ection; 2) Skip-chain CRFs outperform Linear CRFs for answer finding, which demonstrates that context improves answer finding; 3) 2D CRF model improves the performance of Linear CRFs and the combination of 2D CRFs and Skipchain CRFs achieves better performance for context detection. The rest of this paper is organized as follows: The next section discusses related work. Section 3 presents the proposed techniques. We evaluate our techniques in Section 4. Section 5 concludes this paper and discusses future work. 2 Related Work There is some research on summarizing discussion threads and emails. Zhou and Hovy (2005) segmented internet relay chat, clustered segments into subtopics, and identified responding segments of the first segment in each sub-topic by assuming the first segment to be focus. In (Nenkova and Bagga, 2003; Wan and McKeown, 2004; Rambow et al., 2004), email summaries were organized by extracting overview sentences as discussion issues. Carenini et al (2007) leveraged both quotation relation and clue words for email summarization. In contrast, given a forum thread, we extract questions, their contexts, and their answers as summaries. Shrestha and McKeown (2004)’s work on email summarizati"
P09-2071,N03-1028,0,0.132249,"the previous state-of-the-art features we expect the achievement of better accuracy. All our models were trained until parameter estimation converged with a Gaussian prior variance of 4. During training, a pseudo-likelihood parameter estimation (Sutton and McCallum, 2006) was used as an initial weight (estimated in 30 iterations). We used complete and dense input/output joint features for dense model (Dense), and only supported features that are used at least once in the training examples for sparse form better, the sparse model performs well in practice without significant loss of accuracy (Sha and Pereira, 2003). 3 Penn Treebank3: Catalog No. LDC99T42 4 http://www.cnts.ua.ac.be/conll2000/chunking/ 5 http://www.cnts.ua.ac.be/conll2003/ner/ 6 http://archive.ics.uci.edu/ml/ 7 8 283 Ver. 2.0 RC3, http://mallet.cs.umass.edu/ Ver. 0.51, http://crfpp.sourceforge.net/ 10000 20000 30000 40000 0 500 1500 −2000 −6000 Log−likelihood 0 500 1000 1500 (d) NetTalk (e) Communicator (f) Encyclopedia 5000 0 1000 Training time (sec) 3000 5000 Training time (sec) −20000 Log−likelihood Sparse Method 1 Method 2 Method 3 Method 4 −30000 −5500 Log−likelihood −7500 3000 −6500 −39000 1000 Dense Sparse Method 1 Method 2 Method"
P09-2071,P06-2054,1,\N,Missing
P10-1067,P02-1006,0,0.743531,"ed to comparative question identification and comparator mining from questions. However, their methods typically can achieve high precision but suffer from low recall (Jindal and Liu, 2006b) (J&L). However, ensuring high recall is crucial in our intended application scenario where users can issue arbitrary queries. To address this problem, we develop a weakly-supervised bootstrapping pattern learning method by effectively leveraging unlabeled questions. Bootstrapping methods have been shown to be very effective in previous information extraction research (Riloff, 1996; Riloff and Jones, 1999; Ravichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal & Liu 2006 In this subsection, we provide a brief summary of the comparative mining method proposed by Jindal and Liu (2006a and"
P10-1067,P08-1119,0,0.00454293,"r mining from questions. However, their methods typically can achieve high precision but suffer from low recall (Jindal and Liu, 2006b) (J&L). However, ensuring high recall is crucial in our intended application scenario where users can issue arbitrary queries. To address this problem, we develop a weakly-supervised bootstrapping pattern learning method by effectively leveraging unlabeled questions. Bootstrapping methods have been shown to be very effective in previous information extraction research (Riloff, 1996; Riloff and Jones, 1999; Ravichandran and Hovy, 2002; Mooney and Bunescu, 2005; Kozareva et al., 2008). Our work is similar to them in terms of methodology using bootstrapping technique to extract entities with a specific relation. However, our task is different from theirs in that it requires not only extracting entities (comparator extraction) but also ensuring that the entities are extracted from comparative questions (comparative question identification), which is generally not required in IE task. 651 2.2 Jindal & Liu 2006 In this subsection, we provide a brief summary of the comparative mining method proposed by Jindal and Liu (2006a and 2006b), which is used as baseline for comparison a"
P11-1116,N09-1003,0,0.275482,"to have many supporting sentences of different types. This is a big challenge for rare terms, due to their low frequency in sentences (and even lower frequency in supporting sentences because not all occurrences can be covered by patterns). With evidence propagation, we aim at discovering more supporting sentences for terms (especially rare terms). Evidence propagation is motivated by the following two observations: (I) Similar entities or coordinate terms tend to share some common hypernyms. (II) Large term similarity graphs are able to be built efficiently with state-of-the-art techniques (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). With the graphs, we can obtain the similarity between two terms without their hypernyms being available. The first observation motivates us to “borrow” the supporting sentences from other terms as auxiliary evidence of the term. The second observation means that new information is brought with the state-of-the-art term similarity graphs (in addition to the term-label information discovered with the patterns of Table 1). Our evidence propagation algorithm contains two phases. In phase I, some pseudo supporting sentences are constructed for a term from t"
P11-1116,C92-2082,0,0.405589,"gies adopted in our semantic search and mining system NeedleSeek2. In the next section, we discuss major related efforts and how they differ from our work. Section 3 is a brief description of the baseline approach. The probabilistic evidence combination model that we exploited is introduced in Section 4. Our main approach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.micro"
P11-1116,P08-1119,0,0.398193,"we exploited is introduced in Section 4. Our main approach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 The probabilistic evidence combination model that we exploit here was first proposed in (Shi et al., 2009), for combining the page in-link evidence in building a nonlinear static-rank computation algorith"
P11-1116,N04-1041,0,0.502779,"in our semantic search and mining system NeedleSeek2. In the next section, we discuss major related efforts and how they differ from our work. Section 3 is a brief description of the baseline approach. The probabilistic evidence combination model that we exploited is introduced in Section 4. Our main approach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needl"
P11-1116,C10-2110,0,0.0331854,"s illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 The probabilistic evidence combination model that we exploit here was first proposed in (Shi et al., 2009), for combining the page in-link evidence in building a nonlinear static-rank computation algorithm. We applied it to the hyponymy extraction proble"
P11-1116,C10-1112,1,0.770237,"erent types. This is a big challenge for rare terms, due to their low frequency in sentences (and even lower frequency in supporting sentences because not all occurrences can be covered by patterns). With evidence propagation, we aim at discovering more supporting sentences for terms (especially rare terms). Evidence propagation is motivated by the following two observations: (I) Similar entities or coordinate terms tend to share some common hypernyms. (II) Large term similarity graphs are able to be built efficiently with state-of-the-art techniques (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). With the graphs, we can obtain the similarity between two terms without their hypernyms being available. The first observation motivates us to “borrow” the supporting sentences from other terms as auxiliary evidence of the term. The second observation means that new information is brought with the state-of-the-art term similarity graphs (in addition to the term-label information discovered with the patterns of Table 1). Our evidence propagation algorithm contains two phases. In phase I, some pseudo supporting sentences are constructed for a term from the supporting sentences of its neighbors"
P11-1116,N04-1010,0,0.023712,"baseline approach. The probabilistic evidence combination model that we exploited is introduced in Section 4. Our main approach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 The probabilistic evidence combination model that we exploit here was first proposed in (Shi et al., 2009), for combining the page in-link e"
P11-1116,P06-1101,0,0.0385374,"eek2. In the next section, we discuss major related efforts and how they differ from our work. Section 3 is a brief description of the baseline approach. The probabilistic evidence combination model that we exploited is introduced in Section 4. Our main approach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1"
P11-1116,P10-1149,0,0.0113897,"f instance-label pairs with a TF*IDF like method, by exploiting clusters of semantically related phrases. The core idea is to keep a term-label pair (T, L) only if the number of terms having the label L in the term T’s cluster is above a threshold and if L is not the label of too many clusters (otherwise the pair will be discarded). In contrast, we are able to add new (high-quality) labels for a term with our evidence propagation method. On the other hand, low quality labels get smaller score gains via propagation and are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar & Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. Most existing work tends to utilize small-scale or private corpora, whereas the corpus that we used is publicly available and much larger than most of the existing work. We published our term sets (refer to Section 6.1) and their corresponding user judgments so researchers working on similar topics can reproduce our results. Type Hearst-I Hearst-II Hearst-III IsA-I IsA-II IsA-III Pattern NPL {,} (such as) {NP,}* {and|or} NP NPL {,} (include(s) |including) {NP,}* {and|or} NP NPL {,} (e.g.|e.g)"
P11-1116,D08-1061,0,0.494774,"erformance improvement in terms of P@5, MAP and R-Precision. 1 Introduction 1 An important task in text mining is the automatic extraction of entities and their lexical relations; this has wide applications in natural language processing and web search. This paper focuses on mining the hyponymy (or is-a) relation from largescale, open-domain web documents. From the viewpoint of entity classification, the problem is to automatically assign fine-grained class labels to terms. There have been a number of approaches (Hearst 1992; Pantel & Ravichandran 2004; Snow et al., 2005; Durme & Pasca, 2008; Talukdar et al., 2008) to address the problem. These methods typically exploited manually-designed or automatical* This work was performed when Fan Zhang and Shuqi Sun were interns at Microsoft Research Asia ly-learned patterns (e.g., “NP such as NP”, “NP like NP”, “NP is a NP”). Although some degree of success has been achieved with these efforts, the results are still far from perfect, in terms of both recall and precision. As will be demonstrated in this paper, even by processing a large corpus of 500 million web pages with the most popular patterns, we are not able to extract correct labels for many (especially"
P11-1116,P09-1050,0,0.220088,"Missing"
P11-1116,P09-1052,1,0.869968,"approach, a term is represented by a feature vector, with each feature corresponding to a context in which the term appears. The similarity between two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a list of carefully-designed (or automatically learned) patterns is exploited and applied to a text collection, with the hypothesis that the terms extracted by applying each of the patterns to a specific piece of text tend to be similar. Two categories of patterns have been studied in the literature (Heast 1992; Pasca 2004; Kozareva et al., 2008; Zhang et al., 2009): sentence lexical patterns, and HTML tag patterns. An example of sentence lexical patterns is “T {, T}*{,} (and|or) T”. HTML tag patterns include HTML tables, drop-down lists, and other tag repeat patterns. In this paper, we generate the DS and PB graphs by adopting the best-performed methods studied in (Shi et al., 2010). We will compare, by experiments, the propagation performance of utilizing the two categories 1164 of graphs, and also investigate the performance of utilizing both graphs for evidence propagation. 6 6.1 Experiments Experimental setup Corpus We adopt a publicly available dat"
P11-1116,D09-1098,0,\N,Missing
P14-1036,P11-1095,0,0.124343,"iu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-coll"
P14-1036,E06-1002,0,0.353752,"ion m as an n-gram from a specific tweet. Each concept has a set of textual representation fields (Meij et al., 2012), including title (the title of the article), sentence (the first sentence of the article), paragraph (the first paragraph of the article), content (the entire content of the article), and anchor (the set of all anchor texts with incoming links to the article). Wikipedia Lexicon Construction We first construct an offline lexicon with each entry as hm, {c1 , ..., ck }i, where {c1 , ..., ck } is the set of possible referent concepts for the mention m. Following the previous work (Bunescu, 2006; Cucerzan, 2007; Hachey et al., 2013), we extract the possible mentions for a given concept c using the following resources: the title of c; the aliases appearing in the introduction and infoboxes of c (e.g., The Evergreen State is an alias of Washington state); the titles of pages redirecting to c (e.g., State of Washington is a redirecting page of Washington (state)); the titles of the disambiguaIn order to address these unique challenges for wikification for the short tweets, we employ graph-based semi-supervised learning algorithms (Zhu et al., 2003; Smola and Kondor, 2003; Blum et al., 2"
P14-1036,C12-1028,1,0.900153,"ept mention detection, (ii) concept mention disambiguation. Wikification is a particularly useful task for short messages such as tweets because it allows a reader to easily grasp the related topics and enriched information from the KB. From a systemto-system perspective, wikification has demonstrated its usefulness in a variety of applications, including coreference resolution (Ratinov and Roth, 2012) and classification (Vitale et al., 2012). Sufficient labeled data is crucial for supervised models. However, manual wikification annotation for short documents is challenging and timeconsuming (Cassidy et al., 2012). The challenges are: (i) unlinkability, a valid concept may not exist in the KB. (ii) ambiguity, it is impossible to determine the correct concept due to the dearth of information within a single tweet or multiple correct answer. For instance, it would be difficult to determine the correct referent concept for “Gators” in t1 in Figure 1. Linking “UCONN” in t3 to University of Connecticut may also be acceptable since Connecticut Huskies is the athletic team of the university. (iii) prominence, it is challenging to select a set of linkable mentions that are important and relevant. It is not tri"
P14-1036,D11-1071,1,0.28354,"ollective methods usually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Han and Sun, 2011), while collective approaches further leverage the global coherence between concepts normally through supervised or graph-based re-ranking models (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Ferragina and Scaiella, 2010; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Han et al., 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Especially note that when applying the collective methods to short messages from social media, evidence from other messages usually needs to be considered (Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Our method is a collective approach with the following novel advancements: (i) A novel graph representation with fine-grained relations, (ii) A unified framework based on meta paths to explore richer relevant context, (iii) Joint identification and linking of mentions under semi-supervised setting."
P14-1036,P06-1017,0,0.0608106,"cea and Csomai, 2007; Milne and Witten, 2008b; Milne and Witten, 2008a; Kulkarni et al., 2009; He et al., 2011; Ratinov et al., 2011; Cassidy et al., 2012; Cheng and Roth, 2013), to the linking of a cluster of corefer0 .5 0 F 1 6.4 Parameter Analysis 4 0 0 L a b e le d T w e e t S iz e Figure 5: The effect of Labeled Tweet Size. 387 ple tweets. This work is also related to graph-based semisupervised learning (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004; Talukdar and Crammer, 2009), which has been successfully applied in many Natural Language Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011"
P14-1036,D13-1184,0,0.386453,"one is an abbreviation of the other, and at least one meta path exists between mi and mj . Then we define the weight matrix representing the coreferential relation as:  if mi and mj are coreferential,  1.0 and ci = cj Wijcoref =  0 Otherwise 4.4 log max(|Ci |, |Cj |) − log |Ci ∩ Cj | , log(|C|) − log min(|Ci |, |Cj |) Semantic Relatedness Ensuring topical coherence (Principle 3) has been beneficial for wikification on formal texts (e.g., News) by linking a set of semantically-related mentions to a set of semantically-related concepts simultaneously (Han et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013). However, the shortness of a single tweet means that it may not provide enough topical clues. Therefore, it is important to extend this evidence to capture semantic relatedness information from multiple tweets. We define the semantic relatedness score between two mentions as SR(mi , mj ) = 1.0 if at least one meta path exists between mi and mj , otherwise SR(mi , mj ) = 0. In order to compute the semantic relatedness of two concepts ci and cj , we adopt the approach proposed by (Milne and 384 The Combined Relational Graph gators, Florida Gators men&apos;s basketball 0.404 bucks, Milwaukee Bucks to"
P14-1036,D13-1041,0,0.0355207,"that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior popularity and context si"
P14-1036,D07-1074,0,0.950267,"ram from a specific tweet. Each concept has a set of textual representation fields (Meij et al., 2012), including title (the title of the article), sentence (the first sentence of the article), paragraph (the first paragraph of the article), content (the entire content of the article), and anchor (the set of all anchor texts with incoming links to the article). Wikipedia Lexicon Construction We first construct an offline lexicon with each entry as hm, {c1 , ..., ck }i, where {c1 , ..., ck } is the set of possible referent concepts for the mention m. Following the previous work (Bunescu, 2006; Cucerzan, 2007; Hachey et al., 2013), we extract the possible mentions for a given concept c using the following resources: the title of c; the aliases appearing in the introduction and infoboxes of c (e.g., The Evergreen State is an alias of Washington state); the titles of pages redirecting to c (e.g., State of Washington is a redirecting page of Washington (state)); the titles of the disambiguaIn order to address these unique challenges for wikification for the short tweets, we employ graph-based semi-supervised learning algorithms (Zhu et al., 2003; Smola and Kondor, 2003; Blum et al., 2004; Zhou et al."
P14-1036,P13-1107,1,0.761714,"nalysis 4 0 0 L a b e le d T w e e t S iz e Figure 5: The effect of Labeled Tweet Size. 387 ple tweets. This work is also related to graph-based semisupervised learning (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004; Talukdar and Crammer, 2009), which has been successfully applied in many Natural Language Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately a"
P14-1036,D11-1074,0,0.0166395,"We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior populari"
P14-1036,D11-1011,0,0.0273495,"sually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Han and Sun, 2011), while collective approaches further leverage the global coherence between concepts normally through supervised or graph-based re-ranking models (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Ferragina and Scaiella, 2010; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Han et al., 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Especially note that when applying the collective methods to short messages from social media, evidence from other messages usually needs to be considered (Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Our method is a collective approach with the following novel advancements: (i) A novel graph representation with fine-grained relations, (ii) A unified framework based on meta paths to explore richer relevant context, (iii) Joint identification and linking of mentions under semi-supervised setting. 8 Conclusions We have"
P14-1036,I11-1113,0,0.0531393,"ting a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Han and Sun, 2011), while collective approaches further leverage the global coherence between concepts normally through supervised or graph-based re-ranking models (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Ferragina and Scaiella, 2010; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Han et al., 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Especially note that when applying the collective methods to short messages from social media, evidence from other messages usually needs to be considered (Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Our method is a collective approach with the following novel advancements: (i) A novel graph representation with fine-grained relations, (ii) A unified framework based on meta paths to explore richer relevant context, (ii"
P14-1036,N13-1122,0,0.450775,"identification and disambiguation. We first introduce the following three principles that our approach relies on. Principle 1 (Local compatibility): Two pairs of hm, ci with strong local compatibility tend to 4.1 Local Compatibility We first compute local compatibility (Principle 1) by considering a set of novel local features to cap382 ture the importance and relevance of a mention m to a tweet t, as well as the correctness of its linkage to a concept c. We have designed a number of features which are similar to those commonly used in wikification and entity linking work (Meij et al., 2012; Guo et al., 2013; Mihalcea and Csomai, 2007). Mention Features We define the following features based on information from mentions. vectors vc and vt , and the average tf-idf value of common items in vc and vt , where vc and vt are the top 100 tf-idf word vectors in c and t. Local Compatibility Computation For each node vi = hmi , ci i, we collect its local features as a feature vector Fi = hf1 , f2 , ..., fd i. To avoid features with large numerical values that dominate other features, the value of each feature is re-scaled using feature standardization approach. The cosine similarity is then adopted to comp"
P14-1036,P13-1128,0,0.290658,"ated to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Wit"
P14-1036,C10-1145,0,0.0139728,"ully applied in many Natural Language Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant m"
P14-1036,I11-1063,0,0.0178731,"Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective ap"
P14-1036,P05-1049,0,0.0816585,"single text (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Milne and Witten, 2008a; Kulkarni et al., 2009; He et al., 2011; Ratinov et al., 2011; Cassidy et al., 2012; Cheng and Roth, 2013), to the linking of a cluster of corefer0 .5 0 F 1 6.4 Parameter Analysis 4 0 0 L a b e le d T w e e t S iz e Figure 5: The effect of Labeled Tweet Size. 387 ple tweets. This work is also related to graph-based semisupervised learning (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004; Talukdar and Crammer, 2009), which has been successfully applied in many Natural Language Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 20"
P14-1036,D09-1025,0,0.0644804,"formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Han and Sun, 2011), while collective approaches further leverage the global coherence between concepts normally through supervised or graph-based re-ranking models (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Ferragina and Scaiella, 2010; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Han et al., 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Especially note that when applying the collective methods to short messages from social media, evidence from other messages usually needs to be considered (Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Our method is a collective approach with the following novel advancements: (i) A novel graph representation wit"
P14-1036,D12-1113,0,0.0641886,"Missing"
P14-1036,P11-1138,0,0.90289,"e same surface form or one is an abbreviation of the other, and at least one meta path exists between mi and mj . Then we define the weight matrix representing the coreferential relation as:  if mi and mj are coreferential,  1.0 and ci = cj Wijcoref =  0 Otherwise 4.4 log max(|Ci |, |Cj |) − log |Ci ∩ Cj | , log(|C|) − log min(|Ci |, |Cj |) Semantic Relatedness Ensuring topical coherence (Principle 3) has been beneficial for wikification on formal texts (e.g., News) by linking a set of semantically-related mentions to a set of semantically-related concepts simultaneously (Han et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013). However, the shortness of a single tweet means that it may not provide enough topical clues. Therefore, it is important to extend this evidence to capture semantic relatedness information from multiple tweets. We define the semantic relatedness score between two mentions as SR(mi , mj ) = 1.0 if at least one meta path exists between mi and mj , otherwise SR(mi , mj ) = 0. In order to compute the semantic relatedness of two concepts ci and cj , we adopt the approach proposed by (Milne and 384 The Combined Relational Graph gators, Florida Gators men&apos;s basketball 0.404 bu"
P15-1057,W08-0336,0,0.0275193,"riginal meanings. 2 Problem Formulation Following the recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers t"
P15-1057,P06-1017,0,0.020688,"Missing"
P15-1057,P13-2006,0,0.0430093,"Missing"
P15-1057,W13-0908,0,0.0277964,"d malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detect"
P15-1057,P13-1107,1,0.928563,"ecoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, even for human, it is difficult to decode many morphs without certain historical, cultural, or political background knowledge (Zhang et al., 2014). For example, “The Hutt” can be used to refer to a fictional alien entity in the Star Wars universe (“The Hutt stayed and established himself as ruler of Nam Chorios”), or the governor of New Jersey, Chris Christie (“The Hutt announced a bid for a seat in the New Jersey General Assembly”). Huang et al. (2013) did a pioneering pilot study on morph resolution, but their approach assumed the entity morphs were already extracted and used a large amount of labeled data. In fact, they resolved morphs on corpus-level instead of mention-level and thus their approach was context-independent. A practical morph decoder, as depicted in Figure 1, consists of two problems: (1) Morph Extraction: given a corpus, extract morph mentions; and (2). Morph Resolution: For each morph mention, figure out the entity that it refers to. In this paper, we aim to solve the fundamental research problem of end-to-end morph deco"
P15-1057,P14-1036,1,0.900239,"Missing"
P15-1057,D14-1067,0,0.0149744,"(e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. A"
P15-1057,W13-0906,0,0.0253293,"avior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some comm"
P15-1057,P14-1038,1,0.800559,"2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and ca"
P15-1057,D14-1071,0,0.0150722,"s), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in secti"
P15-1057,P95-1026,0,0.257659,"s in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 593 cial to keep the genre"
P15-1057,P14-2105,0,0.0142968,"nd formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to t"
P15-1057,N07-1025,0,0.0387675,"ntext. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 593 cial to keep the genres consistent bet"
P15-1057,W03-1730,0,0.00977908,"4), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. For each morph mention, we discover a list of target candid"
P15-1057,P14-2115,1,0.48139,"{zhangb8,huangh9,panx2,jih,yener}@rpi.edu, 2 lisujian@pku.edu.cn, 3 cyl@microsoft.com 4 hanj@illinois.edu, 5 zhenwen@us.ibm.com, 6 yzsun@ccs.neu.edu, 7 hanj@illinois.edu Abstract a morph “Su-tooth” was created to refer to the Uruguay striker “Luis Suarez” for his habit of biting other players. Automatically decoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, even for human, it is difficult to decode many morphs without certain historical, cultural, or political background knowledge (Zhang et al., 2014). For example, “The Hutt” can be used to refer to a fictional alien entity in the Star Wars universe (“The Hutt stayed and established himself as ruler of Nam Chorios”), or the governor of New Jersey, Chris Christie (“The Hutt announced a bid for a seat in the New Jersey General Assembly”). Huang et al. (2013) did a pioneering pilot study on morph resolution, but their approach assumed the entity morphs were already extracted and used a large amount of labeled data. In fact, they resolved morphs on corpus-level instead of mention-level and thus their approach was context-independent. A practic"
P15-1057,P05-1049,0,0.0177967,"Missing"
P15-1057,D08-1063,0,0.0286454,"2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wide"
P15-1057,W12-4304,0,0.0148793,"; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic"
P15-1057,N03-1033,0,0.00839243,"e recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. F"
P16-1037,D13-1090,0,0.0147305,"the current slump in the UK lamb trade. Table 9: A correct prediction by TF-IDF but then changes into a mistake when including grounded entity features. The linked Wikipedia entries are indicated below the underlined entity mentions. 6.2 6.3 Paraphrase Identification Several hand-crafted features have proven helpful in modeling sentence/phrase similarity, e.g., string-based overlap (Xu et al., 2014), machine translation measures (Madnani et al., 2012), and dependency syntax (Wan et al., 2006; Wang et al., 2015). Using the combination and discriminative re-weighting of the mentioned features, Ji and Eisenstein (2013) manage to obtain more competitive results. More recent work has switched the focus onto neural methods. Socher et al. (2011) recursively encode the representations of sentences by the compositions of words. Convolutional neural nets (LeCun et al., 1998; Collobert and Weston, 2008) are also exploited in the tasks of paraphrase identification and sentence matching (Yin and Sch¨utze, 2015; He et al., 2015; Hu et al., 2014). Story link detection (SLD) is a similar task which aims to classify whether two news stories discuss the same event. Farahat et al. (2003) leverage part of speech tagging tec"
P16-1037,P15-2114,0,0.0649915,"hese differences make the collaborative filteringbased methods, which have been widely applied to paper citation recommendation, less available in our scenario. Therefore, in this paper we focus on content similarity-based methods to deal with the task of news citation recommendation. Previous studies use string-based overlap (Xu et al., 2014), machine translation measures (Madnani et al., 2012), and dependency syntax (Wan et al., 2006; Wang et al., 2015) to model text similarity. More recent work focuses on neural network methods (Yin and Sch¨utze, 2015; He et al., 2015; Hu et al., 2014; dos Santos et al., 2015; Lei et al., 2016). There are two major challenges rendering these approaches not suitable for this task: (i) the variety and (ii) the ambiguity of language. By variety, we mean that the same meaning may be expressed with different phrases. Taking the first row in Table 1 for example, Vlaar in the citing context refers to Ron Vlaar, a Dutch football player, who is referred to as Dutch star and Netherlands international in the cited article. By ambiguity, we mean that the same expression may have different meanings in different contexts. In the second example in Table 1, the mention tiger refe"
P16-1037,D15-1104,1,0.800375,"re associated with three words or less (usually names for persons or places, and lead to definition pages). We also discard those samples whose citing contexts contain or are exactly the same as the titles of the cited articles. For example, “READ MORE: The stories you need to read, in one handy email” links to an article titled “The stories you need to read, in one handy email”. The dataset is preprocessed with Stanford CoreNLP toolkit (Manning et al., 2014), including sentence splitting, tokenizing for whole passages, and POS-tagging for titles and lead paragraphs. We use the JERL system by Luo et al. (2015) for entity detection and grounding. It recognizes entity mentions and links them to Wikipedia entries. Feature Dealing with Variety 1 WMD Dealing with Ambiguity 2 Grounded Entity Overlap 3 Embedding-based Matching 4 Wikipedia Evidence Baselines 5 TF-IDF 6 Ungrounded Mention Overlap 7 Embedding-based Matching Full Article? # of features n 1 Word vector based earth mover’s distance. y y n 4 16 2 Precision and recall for grounded named entities. Minimized matching distance with LINE vectors. Precision and recall for evidence from Wikipedia. y y y 2 4 4 The cosine distance with TF-IDF. Precision"
P16-1037,P08-1019,1,0.609719,"in SLD. In our scenario, the query is typically a piece of context sentence instead of an entire article. Therefore, we find that document level methods yield sub-optimal performance when used to model the similarity of citing context and the articles. Besides, due to the fact that there might be multiple reports for a single event, we consider it reasonable to formulate our problem into a ranking task instead of classification. Question Retrieval The key problem in question retrieval lies in modeling questions’ similarity. Machine translation techniques (Jeon et al., 2005) and topic models (Duan et al., 2008) have been utilized by previous works. An alternative is representation learning. Zhou et al. (2015) use category-based meta-data to learn word embeddings. dos Santos et al. (2015) and Lei et al. (2016) obtain superior performance over hand-crafted features with CNN. News articles are more well-written than most documents in QA communities, which results in the feasibility of high-quality entity detection and grounding. 7 Discussions In this paper, we propose a novel problem of news citation recommendation, which aims to recommend news citations for references based on a citing context. We dev"
P16-1037,N12-1019,0,0.0857381,"ge, much less than the number of citations per academic paper (typically dozens); (b) the author-topic information is usually unavailable, since it is technically difficult to obtain author information from news articles. These differences make the collaborative filteringbased methods, which have been widely applied to paper citation recommendation, less available in our scenario. Therefore, in this paper we focus on content similarity-based methods to deal with the task of news citation recommendation. Previous studies use string-based overlap (Xu et al., 2014), machine translation measures (Madnani et al., 2012), and dependency syntax (Wan et al., 2006; Wang et al., 2015) to model text similarity. More recent work focuses on neural network methods (Yin and Sch¨utze, 2015; He et al., 2015; Hu et al., 2014; dos Santos et al., 2015; Lei et al., 2016). There are two major challenges rendering these approaches not suitable for this task: (i) the variety and (ii) the ambiguity of language. By variety, we mean that the same meaning may be expressed with different phrases. Taking the first row in Table 1 for example, Vlaar in the citing context refers to Ron Vlaar, a Dutch football player, who is referred to"
P16-1037,P03-1030,0,0.0385124,"hting of the mentioned features, Ji and Eisenstein (2013) manage to obtain more competitive results. More recent work has switched the focus onto neural methods. Socher et al. (2011) recursively encode the representations of sentences by the compositions of words. Convolutional neural nets (LeCun et al., 1998; Collobert and Weston, 2008) are also exploited in the tasks of paraphrase identification and sentence matching (Yin and Sch¨utze, 2015; He et al., 2015; Hu et al., 2014). Story link detection (SLD) is a similar task which aims to classify whether two news stories discuss the same event. Farahat et al. (2003) leverage part of speech tagging technique as well as task-specific similarity measures to boost the system’s performance. Shah et al. (2006) show that entity based document representation is a better choice compared to word-based representations in SLD. In our scenario, the query is typically a piece of context sentence instead of an entire article. Therefore, we find that document level methods yield sub-optimal performance when used to model the similarity of citing context and the articles. Besides, due to the fact that there might be multiple reports for a single event, we consider it rea"
P16-1037,P14-5010,0,0.00390864,"e 2.20 ground truth cited articles for each citing context in the dataset. In order to focus only on news events, we filter out those pairs whose hyperlinks are associated with three words or less (usually names for persons or places, and lead to definition pages). We also discard those samples whose citing contexts contain or are exactly the same as the titles of the cited articles. For example, “READ MORE: The stories you need to read, in one handy email” links to an article titled “The stories you need to read, in one handy email”. The dataset is preprocessed with Stanford CoreNLP toolkit (Manning et al., 2014), including sentence splitting, tokenizing for whole passages, and POS-tagging for titles and lead paragraphs. We use the JERL system by Luo et al. (2015) for entity detection and grounding. It recognizes entity mentions and links them to Wikipedia entries. Feature Dealing with Variety 1 WMD Dealing with Ambiguity 2 Grounded Entity Overlap 3 Embedding-based Matching 4 Wikipedia Evidence Baselines 5 TF-IDF 6 Ungrounded Mention Overlap 7 Embedding-based Matching Full Article? # of features n 1 Word vector based earth mover’s distance. y y n 4 16 2 Precision and recall for grounded named entities"
P16-1037,N13-1092,0,0.0621522,"Missing"
P16-1037,D15-1181,0,0.058891,"thor information from news articles. These differences make the collaborative filteringbased methods, which have been widely applied to paper citation recommendation, less available in our scenario. Therefore, in this paper we focus on content similarity-based methods to deal with the task of news citation recommendation. Previous studies use string-based overlap (Xu et al., 2014), machine translation measures (Madnani et al., 2012), and dependency syntax (Wan et al., 2006; Wang et al., 2015) to model text similarity. More recent work focuses on neural network methods (Yin and Sch¨utze, 2015; He et al., 2015; Hu et al., 2014; dos Santos et al., 2015; Lei et al., 2016). There are two major challenges rendering these approaches not suitable for this task: (i) the variety and (ii) the ambiguity of language. By variety, we mean that the same meaning may be expressed with different phrases. Taking the first row in Table 1 for example, Vlaar in the citing context refers to Ron Vlaar, a Dutch football player, who is referred to as Dutch star and Netherlands international in the cited article. By ambiguity, we mean that the same expression may have different meanings in different contexts. In the second"
P16-1037,U06-1019,0,0.108066,"academic paper (typically dozens); (b) the author-topic information is usually unavailable, since it is technically difficult to obtain author information from news articles. These differences make the collaborative filteringbased methods, which have been widely applied to paper citation recommendation, less available in our scenario. Therefore, in this paper we focus on content similarity-based methods to deal with the task of news citation recommendation. Previous studies use string-based overlap (Xu et al., 2014), machine translation measures (Madnani et al., 2012), and dependency syntax (Wan et al., 2006; Wang et al., 2015) to model text similarity. More recent work focuses on neural network methods (Yin and Sch¨utze, 2015; He et al., 2015; Hu et al., 2014; dos Santos et al., 2015; Lei et al., 2016). There are two major challenges rendering these approaches not suitable for this task: (i) the variety and (ii) the ambiguity of language. By variety, we mean that the same meaning may be expressed with different phrases. Taking the first row in Table 1 for example, Vlaar in the citing context refers to Ron Vlaar, a Dutch football player, who is referred to as Dutch star and Netherlands internatio"
P16-1037,Q14-1034,0,0.0590501,"of references per news article is 4.56 on average, much less than the number of citations per academic paper (typically dozens); (b) the author-topic information is usually unavailable, since it is technically difficult to obtain author information from news articles. These differences make the collaborative filteringbased methods, which have been widely applied to paper citation recommendation, less available in our scenario. Therefore, in this paper we focus on content similarity-based methods to deal with the task of news citation recommendation. Previous studies use string-based overlap (Xu et al., 2014), machine translation measures (Madnani et al., 2012), and dependency syntax (Wan et al., 2006; Wang et al., 2015) to model text similarity. More recent work focuses on neural network methods (Yin and Sch¨utze, 2015; He et al., 2015; Hu et al., 2014; dos Santos et al., 2015; Lei et al., 2016). There are two major challenges rendering these approaches not suitable for this task: (i) the variety and (ii) the ambiguity of language. By variety, we mean that the same meaning may be expressed with different phrases. Taking the first row in Table 1 for example, Vlaar in the citing context refers to R"
P16-1037,N15-1091,0,0.023332,"Missing"
P16-1037,P15-1025,0,0.0155436,"ticle. Therefore, we find that document level methods yield sub-optimal performance when used to model the similarity of citing context and the articles. Besides, due to the fact that there might be multiple reports for a single event, we consider it reasonable to formulate our problem into a ranking task instead of classification. Question Retrieval The key problem in question retrieval lies in modeling questions’ similarity. Machine translation techniques (Jeon et al., 2005) and topic models (Duan et al., 2008) have been utilized by previous works. An alternative is representation learning. Zhou et al. (2015) use category-based meta-data to learn word embeddings. dos Santos et al. (2015) and Lei et al. (2016) obtain superior performance over hand-crafted features with CNN. News articles are more well-written than most documents in QA communities, which results in the feasibility of high-quality entity detection and grounding. 7 Discussions In this paper, we propose a novel problem of news citation recommendation, which aims to recommend news citations for references based on a citing context. We develop a re-ranking system leveraging implicit and explicit semantics for content similarity. We const"
P16-1037,Q15-1025,0,\N,Missing
P16-1084,D15-1202,0,0.572838,"s human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results. 1 Introduction Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s (Bobrow, 1964a; Briars and Larkin, 1984; Fletcher, 1985). As early proposals seldom report empirical evaluation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches (Kushman et al., 2014; Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Roy and Roth, 2015) and semantic parsing methods (Shi et al., 2015). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems. The Dolphin1878 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. 2 Available from http://research.microsoft.com/enus/projects/dolphin/. ∗ Work done while this author was an intern at Microsoft Research. 887 Proceedings of the 54th Annual Meeting of the Association for Comp"
P16-1084,D15-1171,0,0.0327625,"structured answer text. We then conduct experiments to test the performance of some recent math problem solving systems on the dataset. We make the following main observations, Statistical machine learning methods have been proposed to solve math word problems since 2014. Hosseini et al. (2014) solve single step or multistep homogeneous addition and subtraction problems by learning verb categories from the training data. Kushman et al. (2014) and Zhou et al. (2015) solve a wide range of algebra word problems, given that systems of linear equations are attached to problems in the training set. Seo et al. (2015) focuses on SAT geometry questions with text and diagram provided. Koncel-Kedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. No empirical evaluation results are reported in most early publications on this topic. Although promising empirical results are reported in recent work, the datasets employed in their evaluation are small and lack diversity. For example, the Alg514 dataset used in Kushman et al. (2014) and Zhou et al. (2015) only contains 514 problems of 28 types. Please refer to Section 3.4 for more details about the d"
P16-1084,P08-1081,1,0.138135,"we need to develop algorithms which can utilize data more effectively. Our experiments indicate that the problem of automatic math word problem solving is still far from being solved. Good results obtained on small datasets may not be good indicators of high performance on larger and diverse datasets. For current methods, simply adding more training data is not an effective way to improve performance. New methodologies are required for this topic. 2 2.2 Our work on automatic answer and equation extraction is related to the recent CQA extraction work (Agichtein et al., 2008; Cong et al., 2008; Ding et al., 2008). Most of them aim to discover high-quality (question, answer text) pairs from CQA posts. We are different because we extract structured data (i.e., numbers and equation systems) inside the pieces of answer text. Related Work 2.1 Math Word Problem Solving Previous work on automatic math word problem solving falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a; Bobrow, 1964b; Charniak, 1968; Charniak, 1969; Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015), math problem sentences are transformed to certain structures (us"
P16-1084,D15-1135,1,0.692008,"the new dataset lead to interesting and surprising results. 1 Introduction Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s (Bobrow, 1964a; Briars and Larkin, 1984; Fletcher, 1985). As early proposals seldom report empirical evaluation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches (Kushman et al., 2014; Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Roy and Roth, 2015) and semantic parsing methods (Shi et al., 2015). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems. The Dolphin1878 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. 2 Available from http://research.microsoft.com/enus/projects/dolphin/. ∗ Work done while this author was an intern at Microsoft Research. 887 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 887–896, c Berlin,"
P16-1084,D14-1058,0,0.191765,"ers from the answer text provided by CQA users, which significantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results. 1 Introduction Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s (Bobrow, 1964a; Briars and Larkin, 1984; Fletcher, 1985). As early proposals seldom report empirical evaluation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches (Kushman et al., 2014; Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Roy and Roth, 2015) and semantic parsing methods (Shi et al., 2015). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems. The Dolphin1878 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. 2 Available from http://research.microsoft.com/enus/projects/dolphin/. ∗ Work done while this author was an intern at Microsoft Research"
P16-1084,D15-1096,0,0.493986,"Missing"
P16-1084,N16-1136,0,0.386408,"diagram provided. Koncel-Kedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. No empirical evaluation results are reported in most early publications on this topic. Although promising empirical results are reported in recent work, the datasets employed in their evaluation are small and lack diversity. For example, the Alg514 dataset used in Kushman et al. (2014) and Zhou et al. (2015) only contains 514 problems of 28 types. Please refer to Section 3.4 for more details about the datasets. Recently, a framework was presented in Koncel-Kedziorsk et al. (2016) for building an online repository of math word problems. The framework is initialized by including previous public available datasets. The largest dataset among them contains 1,155 problems. 1. All systems evaluated on the Dolphin18K dataset perform much worse than on their original small and less diverse datasets. 2. On the large dataset, a simple similaritybased method performs as well as more sophisticated statistical learning approaches. 3. System performance improves sub-linearly as more training data is used. This suggests that we need to develop algorithms which can utilize data more e"
P16-1084,Q15-1042,0,0.765136,"t provided by CQA users, which significantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results. 1 Introduction Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s (Bobrow, 1964a; Briars and Larkin, 1984; Fletcher, 1985). As early proposals seldom report empirical evaluation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches (Kushman et al., 2014; Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Roy and Roth, 2015) and semantic parsing methods (Shi et al., 2015). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems. The Dolphin1878 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. 2 Available from http://research.microsoft.com/enus/projects/dolphin/. ∗ Work done while this author was an intern at Microsoft Research. 887 Proceedings of the 54th An"
P16-1084,P14-1026,0,0.149841,"y extract problem answers from the answer text provided by CQA users, which significantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results. 1 Introduction Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s (Bobrow, 1964a; Briars and Larkin, 1984; Fletcher, 1985). As early proposals seldom report empirical evaluation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches (Kushman et al., 2014; Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Roy and Roth, 2015) and semantic parsing methods (Shi et al., 2015). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems. The Dolphin1878 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. 2 Available from http://research.microsoft.com/enus/projects/dolphin/. ∗ Work done while this author was an inter"
P16-1116,P11-1098,0,0.0369488,"from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one eve"
P16-1116,P15-1017,0,0.13171,"o and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works are either pattern-only or features-only. Moreover, all of these methods consider arguments sepa1225 rately while ignoring the relationship between arguments, which is also important for argument identification. Even the joint method (Li et al., 2013) does not model argument relations directly. We use trigger embedding, sentencelevel embedding, and pattern features tog"
P16-1116,P98-1067,0,0.0819726,", sentence-level embedding, and pattern features together as the our features for balancing. • We proposed a regularization-based method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and N"
P16-1116,P06-1061,0,0.041959,"ootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et"
P16-1116,P11-1113,0,0.84565,"aining (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate mod"
P16-1116,P11-1114,0,0.19134,"ed and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type,"
P16-1116,E12-1029,0,0.213239,"how that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et"
P16-1116,P08-1030,0,0.738218,"ents. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explore"
P16-1116,W05-0610,0,0.0178924,"f and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into conside"
P16-1116,P13-1008,0,0.390565,"2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works ar"
P16-1116,P10-1081,0,0.591789,"n the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff e"
P16-1116,E12-1030,0,0.0160451,"tern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other met"
P16-1116,P12-1088,0,0.0652043,"and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works are either pattern-onl"
P16-1116,P07-1075,0,0.0284876,"). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based me"
P16-1116,D07-1075,0,0.0301605,"ethods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for exam"
P16-1116,C00-2136,0,0.154785,"ed a regularization-based method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly super"
P16-1116,P05-1062,0,0.0399324,"3; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (M"
P16-1116,D09-1016,0,0.293776,"re patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012)"
P16-1116,P06-2094,0,0.0388605,"improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 200"
P16-1116,N06-1039,0,0.0146725,"n method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 200"
P16-1116,P05-1047,0,0.0304587,"o make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of e"
P16-1116,P03-1029,0,0.0553078,"d method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pat"
P16-1116,C98-1064,0,\N,Missing
P16-1124,P15-1127,0,0.0172451,"n via Coupled Path Ranking Quan Wang† , Jing Liu‡ , Yuanfei Luo† , Bin Wang† , Chin-Yew Lin‡ † Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China {wangquan,luoyuanfei,wangbin}@iie.ac.cn ‡ Microsoft Research, Beijing 100080, China {liudani,cyl}@microsoft.com Abstract their relations, typically stored as (head entity, relation, tail entity) triples, e.g., (Paris, capitalOf, France). Although such KBs can be impressively large, they are still quite incomplete and missing crucial facts, which may reduce their usefulness in downstream tasks (West et al., 2014; Choi et al., 2015). KB completion, i.e., automatically inferring missing facts by examining existing ones, has thus attracted increasing attention. Approaches to this task roughly fall into three categories: (i) path ranking algorithms (PRA) (Lao et al., 2011); (ii) embedding techniques (Bordes et al., 2013; Guo et al., 2015); and (iii) graphical models such as Markov logic networks (MLN) (Richardson and Domingos, 2006). This paper focuses on PRA, which is easily interpretable (as opposed to embedding techniques) and requires no external logic rules (as opposed to MLN). Knowledge bases (KBs) are often greatly i"
P16-1124,D07-1074,0,0.0704711,"ion type that exists between two entities. Given a specific relation, random walks are first employed to find paths between two entities that have the given relation. Here a path is a sequence of relations linking bornIn capitalOf two entities, e.g., h −−−−−→ e −−−−−−−−→ t. These paths are then used as features in a binary classifier to predict if new instances (i.e., entity pairs) have the given relation. Introduction Knowledge bases (KBs) like Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2014), and NELL (Carlson et al., 2010) are extremely useful resources for many NLP tasks (Cucerzan, 2007; Schuhmacher and Ponzetto, 2014). They provide large collections of facts about entities and While KBs are naturally composed of multiple relations, PRA models these relations separately during the inference phase, by learning an individual classifier for each relation. We argue, however, that it will be beneficial for PRA to model certain relations in a collective way, particularly when the relations are closely related to each other. For example, given two relations bornIn and livedIn, 1308 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1308–1"
P16-1124,D13-1107,0,0.0278819,"hat learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relatedness, through either parameter sharing (Evgeniou and Pontil, 2004; Ando and Zhang, 2005) or feature sharing (Argyriou et al., 2007; He et al., 2014). In recent years, there has been increasing work showing the benefits of multi-task learning in NLP-related tasks, such as relation extraction (Jiang, 2009; Carlson et al., 2010) and machine translation (Sennrich et al., 2013; Cui et al., 2013; Dong et al., 2015). This paper investigates the possibility of multi-task learning with PRA, in a parameter sharing manner. Connection with previous methods. Actually, modeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed features such as PRA paths usually perform better than latent features for KB completion."
P16-1124,P15-1166,0,0.021292,"ple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relatedness, through either parameter sharing (Evgeniou and Pontil, 2004; Ando and Zhang, 2005) or feature sharing (Argyriou et al., 2007; He et al., 2014). In recent years, there has been increasing work showing the benefits of multi-task learning in NLP-related tasks, such as relation extraction (Jiang, 2009; Carlson et al., 2010) and machine translation (Sennrich et al., 2013; Cui et al., 2013; Dong et al., 2015). This paper investigates the possibility of multi-task learning with PRA, in a parameter sharing manner. Connection with previous methods. Actually, modeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed features such as PRA paths usually perform better than latent features for KB completion. In this context, CP"
P16-1124,D15-1173,0,0.719743,"013; Jiang et al., 2012). This paper focuses on PRA, since it is easily interpretable (as opposed to embedding-based models) and requires no external logic rules (as opposed to MLN and its variants). PRA and its extensions. PRA is a random walk inference technique designed for predicting new relation instances in KBs, first proposed by Lao and Cohen (2010). Recently various extensions have been explored, ranging from incorporating a text corpus as additional evidence during inference (Gardner et al., 2013; Gardner et al., 2014), to introducing better schemes to generate more predictive paths (Gardner and Mitchell, 2015; Shi and Weninger, 2015), or using PRA in a broader context such as Google’s Knowledge Vault (Dong et al., 2014). All these approaches are based on some single-task version of PRA, while our work explores multi-task learning for it. Multi-task learning. Numerous studies have shown that learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relatedness, through either parameter sharing (Evgeniou and Pontil, 2004; Ando and Zhang, 2005) or"
P16-1124,D13-1080,0,0.0208147,"(iii) probabilistic graphical models such as the Markov logic network (MLN) and its variants (Pujara et al., 2013; Jiang et al., 2012). This paper focuses on PRA, since it is easily interpretable (as opposed to embedding-based models) and requires no external logic rules (as opposed to MLN and its variants). PRA and its extensions. PRA is a random walk inference technique designed for predicting new relation instances in KBs, first proposed by Lao and Cohen (2010). Recently various extensions have been explored, ranging from incorporating a text corpus as additional evidence during inference (Gardner et al., 2013; Gardner et al., 2014), to introducing better schemes to generate more predictive paths (Gardner and Mitchell, 2015; Shi and Weninger, 2015), or using PRA in a broader context such as Google’s Knowledge Vault (Dong et al., 2014). All these approaches are based on some single-task version of PRA, while our work explores multi-task learning for it. Multi-task learning. Numerous studies have shown that learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model th"
P16-1124,D14-1044,0,0.467605,"aphical models such as the Markov logic network (MLN) and its variants (Pujara et al., 2013; Jiang et al., 2012). This paper focuses on PRA, since it is easily interpretable (as opposed to embedding-based models) and requires no external logic rules (as opposed to MLN and its variants). PRA and its extensions. PRA is a random walk inference technique designed for predicting new relation instances in KBs, first proposed by Lao and Cohen (2010). Recently various extensions have been explored, ranging from incorporating a text corpus as additional evidence during inference (Gardner et al., 2013; Gardner et al., 2014), to introducing better schemes to generate more predictive paths (Gardner and Mitchell, 2015; Shi and Weninger, 2015), or using PRA in a broader context such as Google’s Knowledge Vault (Dong et al., 2014). All these approaches are based on some single-task version of PRA, while our work explores multi-task learning for it. Multi-task learning. Numerous studies have shown that learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relate"
P16-1124,P15-1009,1,0.717368,"relations, typically stored as (head entity, relation, tail entity) triples, e.g., (Paris, capitalOf, France). Although such KBs can be impressively large, they are still quite incomplete and missing crucial facts, which may reduce their usefulness in downstream tasks (West et al., 2014; Choi et al., 2015). KB completion, i.e., automatically inferring missing facts by examining existing ones, has thus attracted increasing attention. Approaches to this task roughly fall into three categories: (i) path ranking algorithms (PRA) (Lao et al., 2011); (ii) embedding techniques (Bordes et al., 2013; Guo et al., 2015); and (iii) graphical models such as Markov logic networks (MLN) (Richardson and Domingos, 2006). This paper focuses on PRA, which is easily interpretable (as opposed to embedding techniques) and requires no external logic rules (as opposed to MLN). Knowledge bases (KBs) are often greatly incomplete, necessitating a demand for KB completion. The path ranking algorithm (PRA) is one of the most promising approaches to this task. Previous work on PRA usually follows a single-task learning paradigm, building a prediction model for each relation independently with its own training data. It ignores"
P16-1124,P09-1114,0,0.00817837,"ores multi-task learning for it. Multi-task learning. Numerous studies have shown that learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relatedness, through either parameter sharing (Evgeniou and Pontil, 2004; Ando and Zhang, 2005) or feature sharing (Argyriou et al., 2007; He et al., 2014). In recent years, there has been increasing work showing the benefits of multi-task learning in NLP-related tasks, such as relation extraction (Jiang, 2009; Carlson et al., 2010) and machine translation (Sennrich et al., 2013; Cui et al., 2013; Dong et al., 2015). This paper investigates the possibility of multi-task learning with PRA, in a parameter sharing manner. Connection with previous methods. Actually, modeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed f"
P16-1124,P13-1082,0,0.0262895,"us studies have shown that learning multiple related tasks simulta1309 neously can provide significant benefits relative to learning them independently (Caruana, 1997). A key ingredient of multi-task learning is to model the notion of task relatedness, through either parameter sharing (Evgeniou and Pontil, 2004; Ando and Zhang, 2005) or feature sharing (Argyriou et al., 2007; He et al., 2014). In recent years, there has been increasing work showing the benefits of multi-task learning in NLP-related tasks, such as relation extraction (Jiang, 2009; Carlson et al., 2010) and machine translation (Sennrich et al., 2013; Cui et al., 2013; Dong et al., 2015). This paper investigates the possibility of multi-task learning with PRA, in a parameter sharing manner. Connection with previous methods. Actually, modeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed features such as PRA paths usually perform better than latent features"
P16-1124,D11-1049,0,0.327152,"Missing"
P16-1124,W15-4007,0,0.0659011,"h as relation extraction (Jiang, 2009; Carlson et al., 2010) and machine translation (Sennrich et al., 2013; Cui et al., 2013; Dong et al., 2015). This paper investigates the possibility of multi-task learning with PRA, in a parameter sharing manner. Connection with previous methods. Actually, modeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed features such as PRA paths usually perform better than latent features for KB completion. In this context, CPRA is designed in a way that gets the multi-relational benefit of embedding techniques while keeping PRA-style path features. Nickel et al. (2014) and Neelakantan et al. (2015) have tried similar ideas. However, their work focuses on improving embedding techniques with observed features, while our approach aims at improving PRA with multi-task learning. 3 Path Ranking Algorithm PRA was first proposed by Lao and Cohen (2010), and later slightly modified in various ways (Gardne"
P16-1124,P15-1016,0,0.114913,"odeling multiple relations collectively is a common practice in embedding-based approaches. In such a method, embeddings are learned jointly for all relations, over a set of shared latent features (entity embeddings), and hence can capture meaningful associations among different relations. As shown by (Toutanova and Chen, 2015), observed features such as PRA paths usually perform better than latent features for KB completion. In this context, CPRA is designed in a way that gets the multi-relational benefit of embedding techniques while keeping PRA-style path features. Nickel et al. (2014) and Neelakantan et al. (2015) have tried similar ideas. However, their work focuses on improving embedding techniques with observed features, while our approach aims at improving PRA with multi-task learning. 3 Path Ranking Algorithm PRA was first proposed by Lao and Cohen (2010), and later slightly modified in various ways (Gardner et al., 2014; Gardner and Mitchell, 2015). The key idea of PRA is to explicitly use paths that connect two entities as features to predict potential relations between them. Here a path is a sequence of relations ⟨r1 , r2 , · · · , rℓ ⟩ that link two entities. For example, ⟨bornIn, capitalOf⟩ i"
P17-2085,D15-1077,1,0.929384,"ent, Rensselaer Polytechnic Institute, Troy, NY, USA {liny9,jih}@rpi.edu 2 Microsoft Research, Beijing, China cyl@microsoft.com Abstract not well-known and usually absent in general KBs, we may be able to acquire lists of these entities from the local government as the target KB. Voice of the Customer. EL also plays an important role in mining customer opinions from data generated on social platforms and ecommerce websites, thereby helping companies better understand the needs and expectations of their customers. However, the target products are often not covered by general KBs. For example, (Cao et al., 2015) tested 32 names of General Motors car models and only found 4 in Wikipedia. Although some companies may choose to maintain a comprehensive product KB, it will be much more practical and less costly to provide only lists of product names. Traditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the KB may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call it as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which c"
P17-2085,C12-1028,1,0.894036,"Missing"
P17-2085,D13-1184,0,0.146291,"Missing"
P17-2085,P14-1036,1,0.932167,"Missing"
P17-2085,N15-1119,1,0.928285,"Missing"
P17-2085,C10-1112,0,0.0311579,"school on a scholarship at [[Harvard University|Harvard University]]... ∗ On October 6, 2012, [[Allison Harvard|Harvard]] made an appearance in an episode of... Because enwiki:Harvard_University is in the University list, the first mention will be considered as referential, whereas the second one is non-referential. We also apply matching rules in Table 1 to obtain more non-referential mentions. After that, we extract sentences around wikilinks as a document. Experiments 4.1 Data set In our experiment, the construction of data set consists of two steps: collecting name lists from NeedleSeek2 (Shi et al., 2010) and extracting documents from Wikipedia. NeedleSeek is a project aiming to mine semantic concepts from tera-scale data (ClueWeb09) and classify them into a wide range of semantic categories. For example, “KFC” is mined as a concept in the restaurant category, along with key sentences and attributes, such as employee number and founder. To obtain target name lists, we select 7 semantic categories (see Table 2) generated by NeedleSeek as target domains, and take the top concepts in each category as target entities. We manually map each name to its pertinent Wikipedia page as a target entity (e."
P18-1039,D13-1160,0,0.0470331,"l language which is more compact and is effective in facilitating better machine learning performance. Learning Framework In rule-based approaches (Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015), they map math problem description into structures with predefined grammars and rules. Feature-based approaches contain two stages: (1) generate equation candidates; They either re7.2 Semantic Parsing Our work is also related to the classic settings of learning executable semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011, 2013; Berant et al., 2013; Pasupat and Liang, 2016). Maximum marginal likelihood with beam search (Kwiatkowski et al., 2013; Pasupat and Liang, 2016; Ling et al., 2017) is traditionally used. It maximizes the marginal likelihood of all consistent logical forms being observed. Recently 426 reinforcement learning (Guu et al., 2017; Liang et al., 2017) has also been considered, which maximizes the expected reward over all possible logical forms. Different from them, we only consider one single consistent latent form per training instance by leveraging training signals from both the answer and the equation system, which s"
P18-1039,W10-2903,0,0.0423773,"sentences. Inspired by these representations, our work describes a new formal language which is more compact and is effective in facilitating better machine learning performance. Learning Framework In rule-based approaches (Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015), they map math problem description into structures with predefined grammars and rules. Feature-based approaches contain two stages: (1) generate equation candidates; They either re7.2 Semantic Parsing Our work is also related to the classic settings of learning executable semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011, 2013; Berant et al., 2013; Pasupat and Liang, 2016). Maximum marginal likelihood with beam search (Kwiatkowski et al., 2013; Pasupat and Liang, 2016; Ling et al., 2017) is traditionally used. It maximizes the marginal likelihood of all consistent logical forms being observed. Recently 426 reinforcement learning (Guu et al., 2017; Liang et al., 2017) has also been considered, which maximizes the expected reward over all possible logical forms. Different from them, we only consider one single consistent latent form per training instance by lever"
P18-1039,P16-1154,0,0.0333876,"unks Example Four times the sum of three and a number is 10. -&gt; math#product(4, math#sum(3, m))=10 Table 3: Examples of classes and functions in our intermediate representation. “ret” stands for return type. $1, $2 are arguments with its types. available math word problem datasets1 : 4 Model In this section, we describe (1) the basic sequenceto-sequence model, and (2) attention regularization. 4.1 Sequence-to-Sequence RNN Model Our baseline model is based on sequence-tosequence learning (Sutskever et al., 2014) with attention (Bahdanau et al., 2015) and copy mechanism (Gulcehre et al., 2016; Gu et al., 2016). Encoder: The encoder is implemented as a singlelayer bidirectional RNN with gated recurrent units (GRUs). It reads words one-by-one from the input problem, producing a sequence of hidden states hi = [hFi , hB i ] with: hFi = GRU (φin (xi ), hFi−1 ), (1) hB i (2) = GRU (φ in (xi ), hB i+1 ), where φin maps each input word xi to a fixeddimensional vector. Decoder with Copying: At each decoding step j, the decoder receives the word embedding of the previous word, and an attention function is applied to attend over the input words as follows: • Number Word Problem (NumWord) is created by Shi et"
P18-1039,P16-1014,0,0.0293933,"(cnt:$1:int): sum of $1 unks Example Four times the sum of three and a number is 10. -&gt; math#product(4, math#sum(3, m))=10 Table 3: Examples of classes and functions in our intermediate representation. “ret” stands for return type. $1, $2 are arguments with its types. available math word problem datasets1 : 4 Model In this section, we describe (1) the basic sequenceto-sequence model, and (2) attention regularization. 4.1 Sequence-to-Sequence RNN Model Our baseline model is based on sequence-tosequence learning (Sutskever et al., 2014) with attention (Bahdanau et al., 2015) and copy mechanism (Gulcehre et al., 2016; Gu et al., 2016). Encoder: The encoder is implemented as a singlelayer bidirectional RNN with gated recurrent units (GRUs). It reads words one-by-one from the input problem, producing a sequence of hidden states hi = [hFi , hB i ] with: hFi = GRU (φin (xi ), hFi−1 ), (1) hB i (2) = GRU (φ in (xi ), hB i+1 ), where φin maps each input word xi to a fixeddimensional vector. Decoder with Copying: At each decoding step j, the decoder receives the word embedding of the previous word, and an attention function is applied to attend over the input words as follows: • Number Word Problem (NumWord) is"
P18-1039,P17-1097,0,0.0278382,"Missing"
P18-1039,D14-1058,0,0.193297,"1, math#product(2 2, ordinal(2)) Table 8: Instance check of intermediate form for one math problem in several training iterations. 2 0 means the the first ‘2’ in the input and so on. Tokens with quote marks mean that they are incorrect. 7 Related Work place numbers of existing equations in the training data as new equations (Kushman et al., 2014; Zhou et al., 2015; Upadhyay et al., 2016), or enumerate possible combinations of math operators and numbers and variables (Koncel-Kedziorski et al., 2015), which leads to intractably huge search space. (2) predict equation with features. For example, Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Roy and Roth (2015); Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016); Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Roy and Roth (2017) focus on the dependencies between number units. These approaches requires manual feature design and the features may be difficult to be generalized to other tasks. Recently, there are a few works trying to build an end-to-end system with neural models. Ling et al. (2017) consider multiple-choice math problems and use a"
P18-1039,D17-1084,1,0.839493,"not exist in training data. In this paper, we propose a new method which adds an extra meaning representation and generate an intermediate form as output. Additionally, we observe that the attention weights of the seq2seq model repetitively concentrates on numbers in the problem description. To address the issue, we further propose to use a form of attention regularization. To train the model without explicit annotations of intermediate forms, we propose an iterative laIntroduction There is a growing interest in math word problem solving (Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Huang et al., 2017; Roy and Roth, 2018). It requires reasoning with respect to sets of numbers or variables, which is an essential capability in many other natural language understanding tasks. Consider the math problems shown in Table 1. To solve the problems, one needs to know how many numbers to be summed up (e.g. “2 numbers/3 numbers”), and the relation between ∗ Work done while this author was an intern at Microsoft Research. 419 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 419–428 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association f"
P18-1039,D11-1039,0,0.0309076,"ons, our work describes a new formal language which is more compact and is effective in facilitating better machine learning performance. Learning Framework In rule-based approaches (Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015), they map math problem description into structures with predefined grammars and rules. Feature-based approaches contain two stages: (1) generate equation candidates; They either re7.2 Semantic Parsing Our work is also related to the classic settings of learning executable semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011, 2013; Berant et al., 2013; Pasupat and Liang, 2016). Maximum marginal likelihood with beam search (Kwiatkowski et al., 2013; Pasupat and Liang, 2016; Ling et al., 2017) is traditionally used. It maximizes the marginal likelihood of all consistent logical forms being observed. Recently 426 reinforcement learning (Guu et al., 2017; Liang et al., 2017) has also been considered, which maximizes the expected reward over all possible logical forms. Different from them, we only consider one single consistent latent form per training instance by leveraging training signals from both the answer and t"
P18-1039,Q13-1005,0,0.0655272,"Missing"
P18-1039,P16-1084,1,0.799437,"ep j, the decoder receives the word embedding of the previous word, and an attention function is applied to attend over the input words as follows: • Number Word Problem (NumWord) is created by Shi et al. (2015). It contains 1,878 number word problems (verbally expressed number problems, such as the examples in Table 1). Its linear subset (subset of problems that can be solved by linear equation systems) has 986 problems, only involving four basic operations {+, −, ∗, /}. eji = v T tanh(Wh hi + Ws sj + battn ), exp(eji ) aji = Pm , i0 =1 exp(eji0 ) m X cj = aji hi , • Dolphin18K is created by Huang et al. (2016). It contains 18,711 math word problems collected from Yahoo! Answers2 . Since it contains some problems without equations, we only use the subset of 10,644 problems which are paired with their equation systems. (3) (4) (5) i=1 where sj is the decoder hidden state. Intuitively, aji defines the probability distribution of attention over the input words. They are computed from the unnormalized attention scores eji . cj is the context vector, which is the weighted sum of the encoder hidden states. 1 Other small datasets with 4 basic operations {+, −, ∗, /} and only one unknown variable are consid"
P18-1039,D15-1202,0,0.145608,"math problem in several training iterations. 2 0 means the the first ‘2’ in the input and so on. Tokens with quote marks mean that they are incorrect. 7 Related Work place numbers of existing equations in the training data as new equations (Kushman et al., 2014; Zhou et al., 2015; Upadhyay et al., 2016), or enumerate possible combinations of math operators and numbers and variables (Koncel-Kedziorski et al., 2015), which leads to intractably huge search space. (2) predict equation with features. For example, Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Roy and Roth (2015); Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016); Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Roy and Roth (2017) focus on the dependencies between number units. These approaches requires manual feature design and the features may be difficult to be generalized to other tasks. Recently, there are a few works trying to build an end-to-end system with neural models. Ling et al. (2017) consider multiple-choice math problems and use a seq2seq model to generate rationale and the final choice (i.e. A, B, C, D). Wang e"
P18-1039,P14-1026,0,0.279192,"power to generate equations of which problem types do not exist in training data. In this paper, we propose a new method which adds an extra meaning representation and generate an intermediate form as output. Additionally, we observe that the attention weights of the seq2seq model repetitively concentrates on numbers in the problem description. To address the issue, we further propose to use a form of attention regularization. To train the model without explicit annotations of intermediate forms, we propose an iterative laIntroduction There is a growing interest in math word problem solving (Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Huang et al., 2017; Roy and Roth, 2018). It requires reasoning with respect to sets of numbers or variables, which is an essential capability in many other natural language understanding tasks. Consider the math problems shown in Table 1. To solve the problems, one needs to know how many numbers to be summed up (e.g. “2 numbers/3 numbers”), and the relation between ∗ Work done while this author was an intern at Microsoft Research. 419 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 419–428 c Melbourn"
P18-1039,D16-1117,0,0.0267173,"Missing"
P18-1039,D13-1161,0,0.0457474,"Missing"
P18-1039,D15-1135,1,0.952702,"roblem solving as a sequence prediction task, taking the sequence of words in a math problem as input and generating a sequence of tokens in its corresponding intermediate form as output. We then execute the intermediate form to obtain the final answer. We evaluate the task using answer accuracy on two publicly We first discuss two meaning representation schemes for math problem solving. An equation system is a collection of one or more equations involving the same set of variables, which should be considered as highly abstractive symbolic representation. The Dolphin Language is introduced by Shi et al. (2015). It contains about 35 math-related classes and over 200 math-related functions, with additional classes and functions automatically mined from Freebase. 420 Problem 1: Find three consecutive integers with a sum of 267. Dolphin Language: vf.find(cat(‘integers’), count:3, adj.consecutive, (math#sum(pron.that, 267, det.a))) Equation: x + (x + 1) + (x + 2) = 267 This work: math#consecutive(3), math#sum(cnt: 3) = 267 Problem 2: What are 5 consecutive numbers total 95? Dolphin Language: wh.vf.math.total((cat(‘numbers’), count:5, pron.what, adj.consecutive), 95) Equation: x + (x + 1) + (x + 2) + (x"
P18-1039,P17-1003,0,0.0538859,"Missing"
P18-1039,P11-1060,0,0.0415219,"y these representations, our work describes a new formal language which is more compact and is effective in facilitating better machine learning performance. Learning Framework In rule-based approaches (Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015), they map math problem description into structures with predefined grammars and rules. Feature-based approaches contain two stages: (1) generate equation candidates; They either re7.2 Semantic Parsing Our work is also related to the classic settings of learning executable semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011, 2013; Berant et al., 2013; Pasupat and Liang, 2016). Maximum marginal likelihood with beam search (Kwiatkowski et al., 2013; Pasupat and Liang, 2016; Ling et al., 2017) is traditionally used. It maximizes the marginal likelihood of all consistent logical forms being observed. Recently 426 reinforcement learning (Guu et al., 2017; Liang et al., 2017) has also been considered, which maximizes the expected reward over all possible logical forms. Different from them, we only consider one single consistent latent form per training instance by leveraging training signa"
P18-1039,P16-1008,0,0.0151939,"issue, we regularize the accumulative attention weights for each input token using a rectified linear unit (ReLU) layer, leading to the regularization term: Ambiguity in Derivation For one equation system, several latent form derivations are possible. Take the following math problem as an example: Find 3 consecutive integers that 3 times the sum of the first and the third is 79. where λ is a hyper-parameter that controls the contribution of attention regularization in the loss. The format of our attention regularization term resembles the coverage mechanism used in neural machine translation (Tu et al., 2016; Cohn et al., 2016), which encourages the coverage or fertility control for input tokens. Given the annotation of its equation 3 ∗ (x + (x + 2)) = 79, there are two possible latent intermediate forms: 1) math#consecutive(3), math#product(3, math#sum(ordinal(1), ordinal(3)))=79 2) math#consecutive(3), math#product(3, math#sum(min(), max()))=79 There exist two types of ambiguities: a) operator ambiguity. (x + 2) may correspond to the operator “ordinal(3)” or “max()”; b) alignment ambiguity. For each “3” in the intermediate form, it is unclear which “3” in the input to be copied. Therefore, we m"
P18-1039,D16-1029,0,0.0149549,"oduct(“2 0”, “max()”) Intermediate form in 51st iteration (7) math#consecutive(2 0), ordinal(1) = math#sum(2 1, math#product(“2 0”, ordinal(2)) Intermediate form in 101st iteration (3) math#consecutive(2 0), ordinal(1) = math#sum(2 1, math#product(2 2, ordinal(2)) Table 8: Instance check of intermediate form for one math problem in several training iterations. 2 0 means the the first ‘2’ in the input and so on. Tokens with quote marks mean that they are incorrect. 7 Related Work place numbers of existing equations in the training data as new equations (Kushman et al., 2014; Zhou et al., 2015; Upadhyay et al., 2016), or enumerate possible combinations of math operators and numbers and variables (Koncel-Kedziorski et al., 2015), which leads to intractably huge search space. (2) predict equation with features. For example, Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Roy and Roth (2015); Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016); Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Roy and Roth (2017) focus on the dependencies between number units. These approaches requires manual feature"
P18-1039,P17-1015,0,0.142088,"predict equation with features. For example, Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Roy and Roth (2015); Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016); Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Roy and Roth (2017) focus on the dependencies between number units. These approaches requires manual feature design and the features may be difficult to be generalized to other tasks. Recently, there are a few works trying to build an end-to-end system with neural models. Ling et al. (2017) consider multiple-choice math problems and use a seq2seq model to generate rationale and the final choice (i.e. A, B, C, D). Wang et al. (2017) apply a seq2seq model to generate equations with the constraint of single unknown variable. Similarly, we use the seq2seq model but with novel attention regularization to address incorrect attention weights in the seq2seq model. Our work is related to two research areas: math word problem solving and semantic parsing. 7.1 Math Word Problem Solving There are two major components in this task: (1) meaning representation; (2) learning framework. Semantic"
P18-1039,D15-1096,0,0.231895,"Missing"
P18-1039,P16-1202,0,0.257849,"input and so on. Tokens with quote marks mean that they are incorrect. 7 Related Work place numbers of existing equations in the training data as new equations (Kushman et al., 2014; Zhou et al., 2015; Upadhyay et al., 2016), or enumerate possible combinations of math operators and numbers and variables (Koncel-Kedziorski et al., 2015), which leads to intractably huge search space. (2) predict equation with features. For example, Hosseini et al. (2014) design features to classify verbs to addition or subtraction. Roy and Roth (2015); Roy et al. (2016) leverage the tree structure of equations. Mitra and Baral (2016); Roy and Roth (2018) design features for a few math concepts (e.g. Part-Whole, Comparison). Roy and Roth (2017) focus on the dependencies between number units. These approaches requires manual feature design and the features may be difficult to be generalized to other tasks. Recently, there are a few works trying to build an end-to-end system with neural models. Ling et al. (2017) consider multiple-choice math problems and use a seq2seq model to generate rationale and the final choice (i.e. A, B, C, D). Wang et al. (2017) apply a seq2seq model to generate equations with the constraint of sing"
P18-1039,P16-1003,0,0.0131849,"ore compact and is effective in facilitating better machine learning performance. Learning Framework In rule-based approaches (Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015), they map math problem description into structures with predefined grammars and rules. Feature-based approaches contain two stages: (1) generate equation candidates; They either re7.2 Semantic Parsing Our work is also related to the classic settings of learning executable semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2011, 2013; Berant et al., 2013; Pasupat and Liang, 2016). Maximum marginal likelihood with beam search (Kwiatkowski et al., 2013; Pasupat and Liang, 2016; Ling et al., 2017) is traditionally used. It maximizes the marginal likelihood of all consistent logical forms being observed. Recently 426 reinforcement learning (Guu et al., 2017; Liang et al., 2017) has also been considered, which maximizes the expected reward over all possible logical forms. Different from them, we only consider one single consistent latent form per training instance by leveraging training signals from both the answer and the equation system, which should be more efficient fo"
P18-1039,Q18-1012,0,0.505625,"ng data. In this paper, we propose a new method which adds an extra meaning representation and generate an intermediate form as output. Additionally, we observe that the attention weights of the seq2seq model repetitively concentrates on numbers in the problem description. To address the issue, we further propose to use a form of attention regularization. To train the model without explicit annotations of intermediate forms, we propose an iterative laIntroduction There is a growing interest in math word problem solving (Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Huang et al., 2017; Roy and Roth, 2018). It requires reasoning with respect to sets of numbers or variables, which is an essential capability in many other natural language understanding tasks. Consider the math problems shown in Table 1. To solve the problems, one needs to know how many numbers to be summed up (e.g. “2 numbers/3 numbers”), and the relation between ∗ Work done while this author was an intern at Microsoft Research. 419 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 419–428 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Ling"
P19-1256,W18-6555,0,0.0510501,"o say”). Table 1 depicts an example, where the attribute Rating=5 out of 5 in the input meaning representation (MR) is not verbalised in a reference text written by human, while the word restaurant in the reference should refer to an attribute value EatType=Restaurant not contained in the MR. Without explicit alignments in between MRs and the corresponding utterances for guidance, neural systems trained on such data often produce unexpected errors. Previous work attempted at injecting indirect semantic control over the encoder-decoder architecture (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Agarwal et al., 2018) or encouraging consistency during training (Chisholm et al., 2017), without essentially changing to the noisy training data. One exception is the Slug2Slug system (Juraska et al., 2018), where the authors use an aligner with manually written heuristic rules to filter out unrealized attributes from data. In this paper, we propose a simple, automatic recipe towards reducing hallucination for neural surface realisers by enhancing the semantic equivalence between pairs of MRs and utterances. The steps include: (1) Build a language understanding module (ideally well-calibrated) that tries to parse"
P19-1256,P18-1060,0,0.0143559,", X)}N by MR confidence in re1 verse order 8: for i = 1 to bφ · N c do 9: for j = 1 to M do e |X ) &lt; p /N then 10: if p(ri,j i j e from Re 11: Remove ri,j i 12: end if 13: end for 14: De ← De ∪ (Rie , Xi ) 15: end for 16: Update θ with Eq. 3 on De 17: end for of MR-text pairs, and M is the number of wrong MR-text pairs which contain missing or conflict slots in the realization given its input MR. BLEU4 (Papineni et al., 2002) is also reported, although currently neither BLEU nor any other automatic metrics could be convincingly used for evaluating language generation (Novikova et al., 2017a; Chaganty et al., 2018; Reiter, 2018b, inter alia). Human Evaluation. We randomly sample 100 data-text pairs from test set and ask three crowd workers to manually annotate missed (M), added (A), and contradicted (C) slot values in NLG outputs with respect to the input MR, or exact match (E) if all slot values have been realized in the given utterance which contains no additional hallucinated information. When evaluating the NLU systems, missed and added slots refer to the opposite directions, respectively. Compared Systems. Systems in comparison: • TGen (Duˇsek et al., 2018): a sequence-tosequence (Seq2Seq) model w"
P19-1256,E17-1060,0,0.283567,". A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator. 1 Table 1: A loosely corresponded MR-text pair. Bolded phrases conforms to the MR, underlined words are domain-specific additional information, and italic values in the MR are not realised in the reference. Introduction Neural models for natural language generation (NLG) based on the encoder-decoder framework have become quite popular recently (Wen et al., 2015; Mei et al., 2016; Wiseman et al., 2017; Wen et al., 2017; Chisholm et al., 2017; Nie et al., 2018, inter alia). Albeit being appealing for producing fluent and diverse sentences, neural NLG models often suffer from a severe issue of content hallucination (Reiter, 2018a), which refers to the problem that the generated texts often contain information that is irrelevant to or contradicted with the input. Given that similar issues have been less reported or noticed in the latest neural machine translation systems, we believe that the origin of the issue for neural NLG comes from the data side. Current datasets used for training neural NLG systems often include instances that"
P19-1256,N16-1086,0,0.0355939,"relative unaligned noise from the original data-text pairs. A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator. 1 Table 1: A loosely corresponded MR-text pair. Bolded phrases conforms to the MR, underlined words are domain-specific additional information, and italic values in the MR are not realised in the reference. Introduction Neural models for natural language generation (NLG) based on the encoder-decoder framework have become quite popular recently (Wen et al., 2015; Mei et al., 2016; Wiseman et al., 2017; Wen et al., 2017; Chisholm et al., 2017; Nie et al., 2018, inter alia). Albeit being appealing for producing fluent and diverse sentences, neural NLG models often suffer from a severe issue of content hallucination (Reiter, 2018a), which refers to the problem that the generated texts often contain information that is irrelevant to or contradicted with the input. Given that similar issues have been less reported or noticed in the latest neural machine translation systems, we believe that the origin of the issue for neural NLG comes from the data side. Current datasets us"
P19-1256,D18-1422,1,0.855276,"-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator. 1 Table 1: A loosely corresponded MR-text pair. Bolded phrases conforms to the MR, underlined words are domain-specific additional information, and italic values in the MR are not realised in the reference. Introduction Neural models for natural language generation (NLG) based on the encoder-decoder framework have become quite popular recently (Wen et al., 2015; Mei et al., 2016; Wiseman et al., 2017; Wen et al., 2017; Chisholm et al., 2017; Nie et al., 2018, inter alia). Albeit being appealing for producing fluent and diverse sentences, neural NLG models often suffer from a severe issue of content hallucination (Reiter, 2018a), which refers to the problem that the generated texts often contain information that is irrelevant to or contradicted with the input. Given that similar issues have been less reported or noticed in the latest neural machine translation systems, we believe that the origin of the issue for neural NLG comes from the data side. Current datasets used for training neural NLG systems often include instances that do not contain th"
P19-1256,D17-1238,0,0.0776824,"Missing"
P19-1256,P16-2008,0,0.386926,"Missing"
P19-1256,W17-5525,0,0.252801,"Missing"
P19-1256,W18-6539,0,0.0607466,"Missing"
P19-1256,P02-1040,0,0.104198,"{} e , . . . , r e ) for ev4: Parse the MR Rie = (ri,1 i,M ery Xi using Eq. 2P e 5: Slot confid. pj = N i=1 p(ri,j |Xi ) for sj PM e |X ) for Re 6: MR confid. fi = j=1 p(ri,j i i 7: Sort {(Re , X)}N by MR confidence in re1 verse order 8: for i = 1 to bφ · N c do 9: for j = 1 to M do e |X ) &lt; p /N then 10: if p(ri,j i j e from Re 11: Remove ri,j i 12: end if 13: end for 14: De ← De ∪ (Rie , Xi ) 15: end for 16: Update θ with Eq. 3 on De 17: end for of MR-text pairs, and M is the number of wrong MR-text pairs which contain missing or conflict slots in the realization given its input MR. BLEU4 (Papineni et al., 2002) is also reported, although currently neither BLEU nor any other automatic metrics could be convincingly used for evaluating language generation (Novikova et al., 2017a; Chaganty et al., 2018; Reiter, 2018b, inter alia). Human Evaluation. We randomly sample 100 data-text pairs from test set and ask three crowd workers to manually annotate missed (M), added (A), and contradicted (C) slot values in NLG outputs with respect to the input MR, or exact match (E) if all slot values have been realized in the given utterance which contains no additional hallucinated information. When evaluating the NLU"
P19-1256,P16-1154,0,0.0527308,"been realized in the given utterance which contains no additional hallucinated information. When evaluating the NLU systems, missed and added slots refer to the opposite directions, respectively. Compared Systems. Systems in comparison: • TGen (Duˇsek et al., 2018): a sequence-tosequence (Seq2Seq) model with reranking. • Slug2Slug (Juraska et al., 2018): current state-of-the-art method on E2E challenge dataset. It is an ensemble model and uses a rule based aligner for data cleaning and reranking. 2675 • Seq2Seq: a basic Seq2Seq model trained on original MR-text pairs with the copy mechanism (Gu et al., 2016; See et al., 2017). • Seq2Seq+aug: Seq2Seq trained on the MRtext pairs reconstructed by pre-trained NLU. • Seq2Seq+aug+iter: Seq2Seq trained on the MR-text pairs reconstructed by NLU model with iterative data refinement algorithm. • Seq2Seq+aligner: Seq2Seq trained on the MR-text pairs produced by the rule based aligner (Juraska et al., 2018). Implementation Details. For all models, we use fixed pre-trained GloVe vectors (Pennington et al., 2014) and character embeddings (Hashimoto et al., 2017). The dimensions of trainable hidden units in LSTMs are all set to 400. The epochs for pre-training"
P19-1256,D14-1162,0,0.0820971,"d uses a rule based aligner for data cleaning and reranking. 2675 • Seq2Seq: a basic Seq2Seq model trained on original MR-text pairs with the copy mechanism (Gu et al., 2016; See et al., 2017). • Seq2Seq+aug: Seq2Seq trained on the MRtext pairs reconstructed by pre-trained NLU. • Seq2Seq+aug+iter: Seq2Seq trained on the MR-text pairs reconstructed by NLU model with iterative data refinement algorithm. • Seq2Seq+aligner: Seq2Seq trained on the MR-text pairs produced by the rule based aligner (Juraska et al., 2018). Implementation Details. For all models, we use fixed pre-trained GloVe vectors (Pennington et al., 2014) and character embeddings (Hashimoto et al., 2017). The dimensions of trainable hidden units in LSTMs are all set to 400. The epochs for pre-training Npre and bootstrapping Ntune are all set to 5 on validation. During training, we regularize all layers with a dropout rate of 0.1. We use stochastic gradient descent (SGD) for optimisation with learning rate 0.1. The gradient is truncated by 5. For hyper-parameter φ, we conduct experiments with different values (φ = 0.2, 0.4, 0.6, 0.8, 1.0), details in Appendix A. 3.2 Original data NLU refined data w/o self-training NLG Results. Table 4 presents"
P19-1256,D17-1206,0,0.075095,"Missing"
P19-1256,N18-1014,0,0.546552,"restaurant in the reference should refer to an attribute value EatType=Restaurant not contained in the MR. Without explicit alignments in between MRs and the corresponding utterances for guidance, neural systems trained on such data often produce unexpected errors. Previous work attempted at injecting indirect semantic control over the encoder-decoder architecture (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Agarwal et al., 2018) or encouraging consistency during training (Chisholm et al., 2017), without essentially changing to the noisy training data. One exception is the Slug2Slug system (Juraska et al., 2018), where the authors use an aligner with manually written heuristic rules to filter out unrealized attributes from data. In this paper, we propose a simple, automatic recipe towards reducing hallucination for neural surface realisers by enhancing the semantic equivalence between pairs of MRs and utterances. The steps include: (1) Build a language understanding module (ideally well-calibrated) that tries to parse the MR from an utterance; (2) Use it to reconstruct the correct attribute values revealed in the reference texts; (3) With proper confidence thresh2673 Proceedings of the 57th Annual Me"
P19-1256,W17-3537,0,0.0432426,"ences, neural NLG models often suffer from a severe issue of content hallucination (Reiter, 2018a), which refers to the problem that the generated texts often contain information that is irrelevant to or contradicted with the input. Given that similar issues have been less reported or noticed in the latest neural machine translation systems, we believe that the origin of the issue for neural NLG comes from the data side. Current datasets used for training neural NLG systems often include instances that do not contain the same amount of information from the input structure and the output text (Perez-Beltrachini and Gardent, 2017). There is no exception for datasets ∗ Contribution during internship at Microsoft. originally intended for surface realisation (“how to say”) without focusing on content selection (“what to say”). Table 1 depicts an example, where the attribute Rating=5 out of 5 in the input meaning representation (MR) is not verbalised in a reference text written by human, while the word restaurant in the reference should refer to an attribute value EatType=Restaurant not contained in the MR. Without explicit alignments in between MRs and the corresponding utterances for guidance, neural systems trained on s"
P19-1256,N18-1137,0,0.0696341,"Missing"
P19-1256,J18-3002,0,0.104289,"corresponded MR-text pair. Bolded phrases conforms to the MR, underlined words are domain-specific additional information, and italic values in the MR are not realised in the reference. Introduction Neural models for natural language generation (NLG) based on the encoder-decoder framework have become quite popular recently (Wen et al., 2015; Mei et al., 2016; Wiseman et al., 2017; Wen et al., 2017; Chisholm et al., 2017; Nie et al., 2018, inter alia). Albeit being appealing for producing fluent and diverse sentences, neural NLG models often suffer from a severe issue of content hallucination (Reiter, 2018a), which refers to the problem that the generated texts often contain information that is irrelevant to or contradicted with the input. Given that similar issues have been less reported or noticed in the latest neural machine translation systems, we believe that the origin of the issue for neural NLG comes from the data side. Current datasets used for training neural NLG systems often include instances that do not contain the same amount of information from the input structure and the output text (Perez-Beltrachini and Gardent, 2017). There is no exception for datasets ∗ Contribution during i"
P19-1256,P17-1099,0,0.0605418,"the given utterance which contains no additional hallucinated information. When evaluating the NLU systems, missed and added slots refer to the opposite directions, respectively. Compared Systems. Systems in comparison: • TGen (Duˇsek et al., 2018): a sequence-tosequence (Seq2Seq) model with reranking. • Slug2Slug (Juraska et al., 2018): current state-of-the-art method on E2E challenge dataset. It is an ensemble model and uses a rule based aligner for data cleaning and reranking. 2675 • Seq2Seq: a basic Seq2Seq model trained on original MR-text pairs with the copy mechanism (Gu et al., 2016; See et al., 2017). • Seq2Seq+aug: Seq2Seq trained on the MRtext pairs reconstructed by pre-trained NLU. • Seq2Seq+aug+iter: Seq2Seq trained on the MR-text pairs reconstructed by NLU model with iterative data refinement algorithm. • Seq2Seq+aligner: Seq2Seq trained on the MR-text pairs produced by the rule based aligner (Juraska et al., 2018). Implementation Details. For all models, we use fixed pre-trained GloVe vectors (Pennington et al., 2014) and character embeddings (Hashimoto et al., 2017). The dimensions of trainable hidden units in LSTMs are all set to 400. The epochs for pre-training Npre and bootstrap"
P19-1256,D15-1199,0,0.170571,"Missing"
P19-1256,E17-1042,0,0.0273224,"Missing"
P19-1256,P18-1135,0,0.0179711,"tentive Encoder. The encoder produces the vector representations of slot-value pairs in MR and its paired utterance. A slot-value pair r can be treated as a short sequence W = (w1 , . . . , wn ) by concatenating words in its slot and value. The word sequence W is first represented as a sequence of word embedding vectors (v1 , . . . , vn ) from a pre-trained embedding matrix E, and then passed through a bidirectional LSTM layer to yield the contextualized representations sv U sv = (usv 1 , . . . , un ). To produce a summary context vector for U sv , we adopt the same selfattention structure in Zhong et al. (2018) to obtain the sentence vector cs , due to the effectiveness of self-attention modules over variable-length sequences. Similarly, we obtain the contextualized = Output text for the utterance X. Attentive Scorer. The scorer calculates the semantic similarity between a slot-value pair r (e.g., Price=Cheap) and the utterance X (e.g., reference in Table 1). Firstly, an attention layer is applied to select the most salient words in X related to r, which yields the attentive representation d of utterance X. Given the sentence vector cs of the slot-value pair r and the attentive vector d of the utter"
P19-1524,C18-1139,0,0.157952,"Missing"
P19-1524,D18-1217,0,0.0903986,"Missing"
P19-1524,D14-1162,0,0.082083,"ted data, limiting their power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple module on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results. 1 Introduction In the past few years, neural models have become dominant in research on named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016, inter alia), as they effectively utilize distributed representations learned from large-scale unlabeled texts (Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018, inter alia), while avoiding the huge efforts required for designing hand-crafted features or gathering external lexicons. Results from modern neural NER models have achieved new state-of-the-art performance over standard benchmarks such as the popular CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003). An end-to-end model with the property of letting the data speak for itself seems to be appealing at first sight. However, given that the amount of labeled training data for NER is relatively small when compared with other tasks with m"
P19-1524,N18-1202,0,0.045919,"power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple module on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results. 1 Introduction In the past few years, neural models have become dominant in research on named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016, inter alia), as they effectively utilize distributed representations learned from large-scale unlabeled texts (Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018, inter alia), while avoiding the huge efforts required for designing hand-crafted features or gathering external lexicons. Results from modern neural NER models have achieved new state-of-the-art performance over standard benchmarks such as the popular CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003). An end-to-end model with the property of letting the data speak for itself seems to be appealing at first sight. However, given that the amount of labeled training data for NER is relatively small when compared with other tasks with millions of training e"
P19-1524,W13-3516,0,0.135468,"Missing"
P19-1524,N16-1030,0,0.143042,"urces or designing hand-crafted features. This could increase the chance of overfitting since the models cannot access any supervision signal beyond the small amount of annotated data, limiting their power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple module on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results. 1 Introduction In the past few years, neural models have become dominant in research on named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016, inter alia), as they effectively utilize distributed representations learned from large-scale unlabeled texts (Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018, inter alia), while avoiding the huge efforts required for designing hand-crafted features or gathering external lexicons. Results from modern neural NER models have achieved new state-of-the-art performance over standard benchmarks such as the popular CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003). An end-to-end model with the property of letting the da"
P19-1524,W09-1119,0,0.208701,"fer to locations.2 Data-driven end-to-end models trained on that dataset could implicitly bias towards predicting PERSON for most occurrences of Clinton even under some contexts when it refers to a location. On the other hand, for frequently studied languages such as English, people have already collected dictionaries or lexicons consisting of long lists of entity names, known as gazetteers. Gazetteers could be treated as an external source of knowledge that could guide models towards wider coverage beyond the annotated entities in NER datasets. In traditional log-linear named entity taggers (Ratinov and Roth, 2009; Luo et al., 2015), gazetteers are commonly used as discrete features in the form of whether the current token or current span is appearing in the gazetter or not. There does not seem to be any reason for a neural model not to utilize the off-the-shelf gazetters. In this paper, we make a simple attempt in utilizing gazetteers in neural NER. Building on a recently proposed architecture called hybrid semiMarkov conditional random fields (HSCRFs) where span-level scores are derived from tokenlabel scores, we introduce a simple additional module that scores a candidate entity span by the degree i"
P19-1524,I17-2017,0,0.0441381,"). The gazetteers were originally collected from the web and Wikipedia, consisting of around 1.5 million entities grouped into 79 fine-grained categories. We trimmed and mapped these groups into CoNLL-formatted NER tags (see Appendix for details) with about 1.3 million entities kept. Token-level Representation Scottish Labour Experiments (4) i=tj L where µi = ηi vi0 and b|li is the new weight parameter for token label li . The HSCRF model and the sub-tagger derived from it are linear in the way they calculate the span scores. Unlike other semi-CRF models (Zhuo et al., 2016; Zhai et al., 2017; Sato et al., 2017) which utilize neural approaches to derive span scores from word-level representations, HSCRF calculates span score by summing up word-level scores inside a span along BILOU paths constrained by tag mi ’s. This sub-tagger could be analogously treated as playing the role of soft dictionary look-ups, as opposed to the traditional way that activates a discrete feature only for hard token/span matches. Training Due to the space limit, we leave hyperparameter details to the supplementary materials. 4 Word representation The representation for a word consists of three parts: pretrained 50dimensional"
P19-1524,D15-1104,1,0.865136,"driven end-to-end models trained on that dataset could implicitly bias towards predicting PERSON for most occurrences of Clinton even under some contexts when it refers to a location. On the other hand, for frequently studied languages such as English, people have already collected dictionaries or lexicons consisting of long lists of entity names, known as gazetteers. Gazetteers could be treated as an external source of knowledge that could guide models towards wider coverage beyond the annotated entities in NER datasets. In traditional log-linear named entity taggers (Ratinov and Roth, 2009; Luo et al., 2015), gazetteers are commonly used as discrete features in the form of whether the current token or current span is appearing in the gazetter or not. There does not seem to be any reason for a neural model not to utilize the off-the-shelf gazetters. In this paper, we make a simple attempt in utilizing gazetteers in neural NER. Building on a recently proposed architecture called hybrid semiMarkov conditional random fields (HSCRFs) where span-level scores are derived from tokenlabel scores, we introduce a simple additional module that scores a candidate entity span by the degree it softly matches th"
P19-1524,D18-1230,0,0.0779221,"Missing"
P19-1524,P16-1101,0,0.164625,"nd-crafted features. This could increase the chance of overfitting since the models cannot access any supervision signal beyond the small amount of annotated data, limiting their power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple module on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results. 1 Introduction In the past few years, neural models have become dominant in research on named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016, inter alia), as they effectively utilize distributed representations learned from large-scale unlabeled texts (Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018, inter alia), while avoiding the huge efforts required for designing hand-crafted features or gathering external lexicons. Results from modern neural NER models have achieved new state-of-the-art performance over standard benchmarks such as the popular CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003). An end-to-end model with the property of letting the data speak for itself"
P19-1524,W14-1609,0,0.177396,"Missing"
P19-1524,P18-2038,0,0.0646657,"uffer from similar overfitting issues when trained on limited data, but in practice they could be easily spotted and fixed due to the transparency of linear feature weights. 2 See e.g., https://en.wikipedia.org/wiki/ Clinton_(disambiguation) 5301 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5301–5307 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Framework 2.1 where Hybrid semi-Markov CRFs φj = Our approach is by nature based on the hybrid semi-Markov conditional random fields (HSCRFs) proposed by Ye and Ling (2018), which connect traditional CRFs (Lafferty et al., 2001) and semi-Markov CRFs (Sarawagi and Cohen, 2005) by simultaneously leveraging token-level and segment-level scoring information. Let s = hs1 , . . . , sp i denote a segmentation of input sequence x = hx1 , . . . , xn i, where a segment sj = htj , uj , yj i represents a span with a start position tj , an end position uj , and a label yj ∈ Y . We assume that all segments have positive lengths and the start position of the first segment is always 1, then the segmentation s satisfies t1 = 1, up = n, uj − tj ≥ 0, and tj+1 = uj + 1 for 1 ≤ j &lt;"
P19-1524,P16-1134,0,0.054139,"UIUC NER system (Khashabi et al., 2018). The gazetteers were originally collected from the web and Wikipedia, consisting of around 1.5 million entities grouped into 79 fine-grained categories. We trimmed and mapped these groups into CoNLL-formatted NER tags (see Appendix for details) with about 1.3 million entities kept. Token-level Representation Scottish Labour Experiments (4) i=tj L where µi = ηi vi0 and b|li is the new weight parameter for token label li . The HSCRF model and the sub-tagger derived from it are linear in the way they calculate the span scores. Unlike other semi-CRF models (Zhuo et al., 2016; Zhai et al., 2017; Sato et al., 2017) which utilize neural approaches to derive span scores from word-level representations, HSCRF calculates span score by summing up word-level scores inside a span along BILOU paths constrained by tag mi ’s. This sub-tagger could be analogously treated as playing the role of soft dictionary look-ups, as opposed to the traditional way that activates a discrete feature only for hard token/span matches. Training Due to the space limit, we leave hyperparameter details to the supplementary materials. 4 Word representation The representation for a word consists o"
W02-0406,2001.mtsummit-papers.68,0,0.0282352,"imum retention score of x (row) is higher than the maximum retention score of y (column), a ‘-’ indicates the maximum retention score of x is lower than the minimum retention score of y, and a ‘~’ means x and y are indistinguishable. Table 2 shows relative system performance in the multi-document summarization task. Despite the instability of the manual evaluation, we discuss automatic summary evaluation in an attempt to approximate the human evaluation results in the next section. 5 Automatic Summary Evaluation Inspired by recent progress in automatic evaluation of machine translation (BLEU; Papineni et al. 2001), we would like to apply the same idea in the evaluation of summaries. Following BLEU, we used the automatically computed accumulative n-gram matching scores (NAMS) between a model unit (MU) and a system summary (S)4 as performance indicator, considering multi-document summaries. Only content words were used in forming n-grams. NAMS is defined as follows: a1·NAM1 + a2·NAM2 + a3·NAM3 + a4·NAM4 NAMn is n-gram hit ratio defined as: # of matched n - grams between MU and S total # of n - grams in MU We tested three different configurations of ai: 4 The whole system summary was used to compute NAMS"
W02-0406,P02-1040,0,\N,Missing
W03-0510,W00-0408,0,0.105548,"Missing"
W03-0510,J98-3005,0,0.0416223,"am co-occurrence scoring metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Paul & Liggett 2002) indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much. This leads to the be"
W03-0510,W02-0406,1,0.599598,"ndpipe to help her breathe.&lt;/S&gt; &lt;/TEXT&gt; &lt;/DOC&gt; Figure 3. A 150-word oracle extract for document AP900424-0035. good summary. It also makes system and human performance approach average since it is more likely to include some good sentences but not all of them. Empirical results shown in Section 5 confirm this and that leads us to the question of how to construct a corpus to evaluate summarization systems. We discuss this issue in the conclusion section. 4.3 Inter-Human Agreement and Its Effect on System Performance In this section we study how inter-human agreement affects system performance. Lin and Hovy (2002) reported that, compared to a manually created ideal, humans scored about 0.40 in average coverage score and the best system scored about 0.35. According to these numbers, we might assume that humans cannot agree to each other on what is important and the best system is almost as good as humans. If this is true then estimating an upper bound using oracle extracts is meaningless. No matter how high the estimated upper bounds may be, we probably would never be able to achieve that performance due to lack of agreement between humans: the oracle approximating one human would fail miserably with an"
W03-0510,H01-1054,0,0.0371477,"metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Paul & Liggett 2002) indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much. This leads to the belief that lead-base"
W03-0510,N03-1020,1,0.607198,"pants were required to create a generic 100-word summary. There were 30 test sets in DUC 2001 and each test set contained about 10 documents. For each document, one summary was created manually as the ëidealí model summary at approximately 100 words. We will refer to this manual summary as H1. Two other manual summaries were also created at about that length. We will refer to these two additional human summaries as H2 and H3. In addition, baseline summaries were created automatically by taking the first n sentences up to 100 words. We will refer this baseline extract as B1. In a recent study (Lin and Hovy 2003), we showed that the recall-based unigram co-occurrence automatic scoring metric correlated highly with human evaluation and has high recall and precision in predicting statistical significance of results comparing with its human counterpart. The idea is to measure the content similarity between a system extract and a manual summary using simple n-gram overlap. A similar idea called IBM BLEU score has proved successful in automatic machine translation evaluation (Papineni et al. 2001, NIST 2002). For summarization, we can express the degree of content overlap in terms of n-gram matches as the"
W03-0510,P02-1040,0,\N,Missing
W03-1101,J93-1003,0,0.0137263,"s of about 120 miles per hour and caused severe damage in small coastal centres such as Morgan City, Franklin and New Iberia.” ranked according to their lengthadjusted log-probability. 3 NeATS – a Multi-Document Summarization System NeATS (Lin and Hovy, 2002) is an extractionbased multi-document summarization system. It is among the top two performers in DUC 2001 and 2002 (Over and Liggett, 2002). It consists of three main components: • Content Selection – The goal of content selection is to identify important concepts mentioned in a document collection. NeATS computes the likelihood ratio λ (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams, and clusters these concepts in order to identify major subtopics within the main topic. Each sentence in the document set is then ranked, using the key concept structures. These n-gram key concepts are called topic signatures (Lin and Hovy 2000). We used key n-grams to rerank compressions in our experiments. • Content Filtering – NeATS uses three different filters: sentence position, stigma words, and maximum marginal relevancy. Sentence position has been used as a good content filter since the late 60s (Edmundson, 1969). We apply a"
W03-1101,A00-1043,0,0.0763121,"compression techniques to improve the performance of extraction-based automatic text summarization systems. Sentence compression aims to retain the most salient information of a sentence, rewritten in a short form (Knight and Marcu, 2000). It can be used to deliver compressed content to portable devices (Buyukkokten et al., 2001; Corston-Oliver, 2001) or as a reading aid for aphasic readers (Carroll et al., 1998) or the blind (Grefenstette, 1998). Earlier research in sentence compression focused on compressing single sentences, and were evaluated on a sentence by sentence basis. For example, Jing (2000) trained her system on a set of 500 sentences from the Benton Foundation (http://www.benton.org) and their reduced forms written by humans. The results were evaluated at the parse tree level against the reduced trees; while Knight and Marcu (2000) trained their system on a set of 1,067 sentences from Ziff-Davis magazine articles and evaluated their results on grammaticality and importance rated by humans. Both reported success in their evaluation criteria. However, neither of them reported their techniques’ effectiveness in improving the overall performance of automatic text summarization syst"
W03-1101,C00-1072,1,0.3145,"ization system. It is among the top two performers in DUC 2001 and 2002 (Over and Liggett, 2002). It consists of three main components: • Content Selection – The goal of content selection is to identify important concepts mentioned in a document collection. NeATS computes the likelihood ratio λ (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams, and clusters these concepts in order to identify major subtopics within the main topic. Each sentence in the document set is then ranked, using the key concept structures. These n-gram key concepts are called topic signatures (Lin and Hovy 2000). We used key n-grams to rerank compressions in our experiments. • Content Filtering – NeATS uses three different filters: sentence position, stigma words, and maximum marginal relevancy. Sentence position has been used as a good content filter since the late 60s (Edmundson, 1969). We apply a simple sentence filter that only retains the 10 lead sentences. Some sentences start with stigma words such as conjunctions, quotation marks, pronouns, and the verb “say” and its derivatives usually cause discontinuity in summaries. We simply reduce the scores of these sentences to demote their ranks and"
W03-1101,N03-1020,1,0.172444,"A sentence is added to the summary if and only if its content has less than X percent overlap with the summary. • Content Presentation – To ensure coherence of the summary, NeATS pairs each sentence with an introduction sentence. It then outputs the final sentences in their chronological order. We ran NeATS to generate summaries of different sizes that were used as our test bed. The topic signatures created in the process were used to rerank compressions. We describe the automatic evaluation metric used in our experiments in the next section. 4 Unigram Co-Occurrence Metric In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram cooccurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counterpart. The idea is to measure the content similarity between a system extract and a manual summary using simple n-gram overlap. A similar idea called IBM BLEU score has proved successful in automatic machine translation evaluation (NIST, 2002; Papineni et al., 2001). For summarization, we can express the degree of content overlap in terms of ngram matches a"
W03-1101,W03-0510,1,0.712953,"Missing"
W03-1101,J98-3005,0,0.0251118,"ems participating in the past Document Understanding Conference (DUC, 2002) (a large scale summarization evaluation effort sponsored by the United States government), and the Text Summarization Challenge (Fukusima and Okumura, 2001) (sponsored by Japanese government) are extraction based. Extraction-based automatic text summarization systems extract parts of original documents and output the results as summaries (Chen et al., 2003; Edmundson, 1969; Goldstein et al., 1999; Hovy and Lin, 1999; Kupiec et al., 1995; Luhn, 1969). Other systems based on information extraction (McKeown et al., 2002; Radev and McKeown, 1998; White et al., 2001) and discourse analysis (Marcu, 1999; Strzalkowski et al., 1999) also exist but they are not yet usable for general-domain summarization. Our study focuses on the effectiveness of applying sentence compression techniques to improve the performance of extraction-based automatic text summarization systems. Sentence compression aims to retain the most salient information of a sentence, rewritten in a short form (Knight and Marcu, 2000). It can be used to deliver compressed content to portable devices (Buyukkokten et al., 2001; Corston-Oliver, 2001) or as a reading aid for aph"
W03-1101,H01-1054,0,0.0148296,"past Document Understanding Conference (DUC, 2002) (a large scale summarization evaluation effort sponsored by the United States government), and the Text Summarization Challenge (Fukusima and Okumura, 2001) (sponsored by Japanese government) are extraction based. Extraction-based automatic text summarization systems extract parts of original documents and output the results as summaries (Chen et al., 2003; Edmundson, 1969; Goldstein et al., 1999; Hovy and Lin, 1999; Kupiec et al., 1995; Luhn, 1969). Other systems based on information extraction (McKeown et al., 2002; Radev and McKeown, 1998; White et al., 2001) and discourse analysis (Marcu, 1999; Strzalkowski et al., 1999) also exist but they are not yet usable for general-domain summarization. Our study focuses on the effectiveness of applying sentence compression techniques to improve the performance of extraction-based automatic text summarization systems. Sentence compression aims to retain the most salient information of a sentence, rewritten in a short form (Knight and Marcu, 2000). It can be used to deliver compressed content to portable devices (Buyukkokten et al., 2001; Corston-Oliver, 2001) or as a reading aid for aphasic readers (Carroll"
W03-1101,P02-1058,1,\N,Missing
W03-1101,P02-1040,0,\N,Missing
W03-1101,W02-0406,1,\N,Missing
W04-1013,N03-1020,1,0.147289,"summaries automatically has drawn a lot of attention in the summarization research community in recent years. For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries. These methods are: cosine similarity, unit overlap (i.e. unigram or bigram), and longest common subsequence. However, they did not show how the results of these automatic evaluation methods correlate to human judgments. Following the successful applic ation of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU , i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries. In this paper, we introduce a package, ROUGE, for automatic evaluation of summaries and its evaluations. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes several automatic evaluation methods that measure the similarity between summaries. We describe ROUGE-N in Section 2, ROUGE-L in Section 3, ROUGE-W in Section 4, and ROUGE-S in Section 5. Section 6 shows how these measures correlate with human judgments using DUC 2001, 2002, and 2003 data. Section"
W04-1013,P04-1077,1,0.146856,"Missing"
W04-1013,W01-0100,0,0.156851,"between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. 1 Introduction Traditionally evaluation of summarization involves human judgments of different quality metrics, for example, coherence, conciseness, grammaticality, readability, and content (Mani, 2001). However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts. This is very expensive and difficult to conduct in a frequent basis. Therefore, how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years. For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries. These methods ar"
W04-1013,W95-0115,0,0.147382,"based on longest common subsequences between two summaries. 3 ROUGE-L: Longest Common Subs equence A sequence Z = [z1 , z2, ..., zn ] is a subsequence of another sequence X = [x1 , x2 , ..., x m ], if there exists a strict increasing sequence [i1 , i2 , ..., ik] of indices of X such that for all j = 1, 2, ..., k, we have xij = zj (Cormen et al., 1989). Given two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length. LCS has been used in identifying cognate candidates during construction of N-best translation lexicon from parallel text. Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. He used LCS as an approximate string matching algorithm. Saggion et al. (2002) used normalized pairwise LCS to compare simila rity between two texts in automatic summarization evaluation. 3.1 Sentence-Level LCS To apply LCS in summarization evaluation, we view a summary sentence as a sequence of words. The intuition is that the longer the LCS of two summary sentences is, the more similar the two summaries are. We propose using LCS-based F"
W04-1013,N03-2021,0,0.145177,"Missing"
W04-1013,2001.mtsummit-papers.68,0,0.187347,"to conduct in a frequent basis. Therefore, how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years. For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries. These methods are: cosine similarity, unit overlap (i.e. unigram or bigram), and longest common subsequence. However, they did not show how the results of these automatic evaluation methods correlate to human judgments. Following the successful applic ation of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU , i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries. In this paper, we introduce a package, ROUGE, for automatic evaluation of summaries and its evaluations. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes several automatic evaluation methods that measure the similarity between summaries. We describe ROUGE-N in Section 2, ROUGE-L in Section 3, ROUGE-W in Section 4, and ROUGE-S in Section 5. Section 6 shows how these measures correlate with huma"
W04-1013,C02-1073,0,0.146243,"udgments of different quality metrics, for example, coherence, conciseness, grammaticality, readability, and content (Mani, 2001). However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts. This is very expensive and difficult to conduct in a frequent basis. Therefore, how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years. For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries. These methods are: cosine similarity, unit overlap (i.e. unigram or bigram), and longest common subsequence. However, they did not show how the results of these automatic evaluation methods correlate to human judgments. Following the successful applic ation of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU , i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries. In this"
W04-1013,P02-1040,0,\N,Missing
W06-1610,N03-1020,1,0.0666167,"at uses paraphrases to improve the quality of machine translation evaluations. Previous work has focused on fixed n-gram evaluation metrics coupled with lexical identity matching. ParaEval addresses three important issues: support for paraphrase/synonym matching, recall measurement, and correlation with human judgments. We show that ParaEval correlates significantly better than BLEU with human assessment in measurements for both fluency and adequacy. 1 Introduction The introduction of automated evaluation procedures, such as BLEU (Papineni et al., 2001) for machine translation (MT) and ROUGE (Lin and Hovy, 2003) for summarization, have prompted much progress and development in both of these areas of research in Natural Language Processing (NLP). Both evaluation tasks employ a comparison strategy for comparing textual units from machine-generated and gold-standard texts. Ideally, this comparison process would be performed manually, because of humans’ abilities to infer, paraphrase, and use world knowledge to relate differently worded pieces of equivalent information. However, manual evaluations are time consuming and expensive, thus making them a bottleneck in system development cycles. BLEU measures"
W06-1610,P04-1077,1,0.647219,"Missing"
W06-1610,J03-1002,0,0.00239997,"tistical Machine Translation (SMT) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (Och and Ney, 2004). The sentence-based translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory. This alignment process results in a corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other. We ran the alignment algorithm from (Och and Ney, 2003) on a Chinese-English parallel corpus of 218 million English words, available from the Linguistic Data Consortium (LDC). Phrase pairs are extracted by following the method described in (Och and Ney, 2004) where all contiguous phrase pairs having consistent alignments are extraction candidates. Using these pairs we build paraphrase sets by joining together all English phrases that have the same Chinese translation. Figure 3 shows an example word/phrase alignment for two parallel sentence pairs from our corpus where the phrases “blowing up” and “bombing” have the same Chinese translation. On the"
W06-1610,J04-4002,0,0.00709496,"involving Chinese) and has determined it to be a direct substitute of the phrase “bombing attack”, then the Chinese translation of “bombing attack” would be used in place of the translation for “bombing”. This substitution technique has shown some improvement in translation quality (Callison-Burch et al., 2006). Figure 3. An example of the paraphrase extraction process. foreign language. Phrase-based Statistical Machine Translation (SMT) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (Och and Ney, 2004). The sentence-based translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory. This alignment process results in a corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other. We ran the alignment algorithm from (Och and Ney, 2003) on a Chinese-English parallel corpus of 218 million English words, available from the Linguistic Data Consortium (LDC). Phrase pairs are extracted by following the method described in (Och an"
W06-1610,2003.mtsummit-papers.51,0,0.128697,"overall system and human ranking. In the upper left corner, human translators are grouped together, significantly separated from the automatic MT systems clustered into the lower right corner. Evaluating ParaEval To be effective in MT evaluations, an automated procedure should be capable of distinguishing good translation systems from bad ones, human translations from systems’, and human translations of differing quality. For a particular evaluation exercise, an evaluation system produces a ranking for system and human translations, and compares this ranking with one created by human judges (Turian et al., 2003). The closer a system’s ranking is to the human’s, the better the evaluation system is. 4.1 Validating ParaEval To test ParaEval’s ability, NIST 2003 Chinese MT evaluation results were used (NIST 2003). This collection consists of 100 source documents in Chinese, translations from eight individual translation systems, reference translations from four humans, and human assessments (on fluency and adequacy). The Spearman rank-order coefficient is computed as an indicator of how close a system ranking is to gold-standard human ranking. It should be noted that the 2003 MT data is separate from the"
W06-1610,P05-1074,0,0.454537,"uce domain-specific collections that are used for text generation and are application-specific. But operating in multiple domains and for multiple tasks translates into multiple manual collection efforts, which could be very time-consuming and costly. In order to facilitate smooth paraphrase utilization across a variety of NLP applications, we need an unsupervised paraphrase collection mechanism that can be easily conducted, and produces paraphrases that are of adequate quality and can be readily used with minimal amount of adaptation effort. Our method (Anonymous, 2006), also illustrated in (Bannard and Callison-Burch, 2005), to automatically construct a large domainindependent paraphrase collection is based on the assumption that two different phrases of the same meaning may have the same translation in a With the introduction of ParaEval, we will address two of these three issues, namely the paraphrasing problem and providing a recall measure. 79 in Figure 3, the paraphrase table would contain only “bombing” and “bombing attack”. Paraphrases that are direct substitutes of one another are useful when translating unknown phrases. For instance, if a MT system does not have the Chinese translation for the word “bom"
W06-1610,N06-1003,0,0.158728,"Missing"
W06-1610,E06-1032,0,0.0432685,"Missing"
W06-1610,2003.mtsummit-papers.32,0,0.110898,"Missing"
W06-1610,P02-1040,0,\N,Missing
W07-0736,2004.tmi-1.8,0,0.240032,"Missing"
W07-0736,W02-1039,0,0.0340365,"s the reference and the other three references as candidate translations. Presumably, since the candidate sentences are near-optimal translations, the BLEU scores obtained in such a way should be close to 1. But our analysis shows a mean BLEU of only 0.1456398, with a standard deviation of 0.1522381, which means that BLEU is not very predictive of sentence level evaluation. The BLEU score is, however, still informative in judging the average MT system’s translation. 4.2.2 Dependency Structure Matching Dependency relation information has been widely used in Machine Translation in recent years. Fox (2002) reported that dependency trees correspond better across translation pairs than constituent trees. The information summarization community has also seen successful implementation of ideas similar to the depedency structure. Zhou et al.(2005) and Hovy et al.(2005) reported using Basic Elements (BE) in text summarization and its evaluation. In the current 3 We added an extremely small number to both matched ngrams and total number of n-grams. paper, we match a candidate translation with a reference translation on the following ﬁve dependency structure (DS) types: • • • • • Agent - Verb Verb - Pa"
W07-0736,2005.eamt-1.15,0,0.108219,"Missing"
W07-0736,P04-1077,1,0.89288,"Missing"
W07-0736,W05-0904,0,0.150542,"Missing"
W07-0736,P06-2070,0,0.0759207,"Missing"
W07-0736,quirk-2004-training,0,0.0453361,"2005,2006) as well as propose ways to evaluate MT evaluation metrics (Lin, et al. 2004). Previous studies, however, have focused on MT evaluation at the document level in order to ﬁght n-gram sparseness problem. While document level correlation provides us with a general impression of the quality of an MT system, researchers desire to get more informative diagnostic evaluation at sentence level to improve the MT system instead of just an overall score that does not provide details. Recent years have seen several studies investigating MT evaluation at the sentence level (Liu et al., 2005,2006; Quirk, 2004). The state-of-the-art sentence level correlations reported in previous work between human assessments and automatic scoring are around 0.20. Kulesza et al.(2004) applied Support Vector Machine classiﬁcation learning to sentence level MT evaluation and reported improved correlation with human judgment over BLEU. However, the classiﬁcation taxonomy in their work is binary, being either machine translation or human translation. Additionally, as discussed above, the inconsistency from the human annotators weakens the legitimacy of the classiﬁcation approach. Gamon et al.(2005) reported a study of"
W07-0736,W06-1610,1,0.880606,"Missing"
W14-2706,P12-2012,1,0.923039,"ion that one would generally keep as secrets to himself. All other utterances, those that do not contain information about the self or someone close are categorized as G. Examples include gossip about celebrities or factual discourse about current events. Classifying self-disclosure level: Prior work on quantitatively analyzing self-disclosure has relied on user surveys (Trepte and Reinecke, 2013; Ledbetter et al., 2011) or human annotation (Barak and Gluck-Ofri, 2007). These methods consume much time and effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and 3 Self-Disclosure Topic Model This section describes our model, the selfdisclosure topic model (SDTM), for classifying self-disclosure level and discovering topics for each self-disclosure level. 3.1 Model We make two important assumptions based on our observations"
W14-2706,P13-1165,0,0.023724,"part classifies G vs. M/H levels with first-person pronouns (I, my, me). In the graphical model, y is the latent variable that represents this classification, and ω is the distribution over y. x is the observation of the firstperson pronoun in the tweets, and λ are the parameters learned from the maximum entropy classifier. With the annotated Twitter conversation dataset (described in Section 4.2), we experimented with several classifiers (Decision tree, Naive Bayes) and chose the maximum entropy classifier because it performed the best, similar to other joint topic models (Zhao et al., 2010; Mukherjee et al., 2013). Type Name Birthday Location Contact Occupation Education Family Trigram My name is, My last name My birthday is, My birthday party I live in, I lived in, I live on My email address, My phone number My job is, My new job My high school, My college is My dad is, My mom is, My family is Table 2: Example seed trigrams for identifying M level of SD. There are 51 of these used in SDTM. Utterances with H level express secretive wishes or sensitive information that exposes self or someone close (Barak and Gluck-Ofri, 2007). These are 44 Category physical appearance mental/physical condition Keywords"
W14-2706,N13-1039,0,0.0190454,"ongitudinal OSN usage (see Section 6.2), so SDTM is overall the best model for classifying self-disclosure levels. • Seed words and trigrams (SEED): Occurrence of seed words and trigrams which are described in section 3.3. • ASUM (Jo and Oh, 2011): A joint model of sentiment and topic using seed words. Each sentiment can be mapped to a SD level. Used in previous work (Bak et al., 2012). • First-person pronouns (FirstP): Occurrence of first-person pronouns which are described in section 3.2. To identify first-person pronouns, we tagged parts of speech in each tweet with the Twitter POS tagger (Owoputi et al., 2013). SEED, LIWC, LDA and FirstP cannot be used directly for classification, so we use Maximum entropy model with outputs of each of those models as features. We run MedLDA, ASUM and SDTM 20 times each and compute the average accuracies and F-measure for each level. We set 40 topics for LDA, MedLDA and ASUM, 60; 40; 40 topics for SDTM K G , K M and K H respectively, and set α = γ = 0.1. To incorporate the seed words and trigrams into ASUM and SDTM, we initialize β G , β M and β H differently. We assign a high value of 2.0 for each seed word and trigram for that level, and a low value of 10−6 for e"
W14-2706,N10-1020,0,0.0303645,"tion that exposes self or someone close (Barak and Gluck-Ofri, 2007). These are 44 Category physical appearance mental/physical condition Keywords acne, hair, overweight, stomach, chest, hand, scar, thighs, chubby, head, skinny addicted, bulimia, doctor, illness, alcoholic, disease, drugs, pills, anorexic 4 To answer our research questions, we need a large longitudinal dataset of conversations such that we can analyze the relationship between selfdisclosure behavior and conversation frequency over time. We chose to crawl Twitter because it offers a practical and large source of conversations (Ritter et al., 2010). Others have also analyzed Twitter conversations for natural language and social media research (Boyd et al., 2010; DanescuNiculescu-Mizil et al., 2011), but we collect conversations from the same set of dyads over several months for a unique longitudinal dataset. Table 3: Example words for identifying H level of SD. Categories are hand-labeled. generally keep as secrests. With this intuition, we crawled 26,523 secret posts from Six Billion Secrets 1 site where users post secrets anonymously. To extract seed words that might express secretive personal information, we compute mutual informatio"
W14-2706,D10-1006,0,0.0130636,"rts, and the first part classifies G vs. M/H levels with first-person pronouns (I, my, me). In the graphical model, y is the latent variable that represents this classification, and ω is the distribution over y. x is the observation of the firstperson pronoun in the tweets, and λ are the parameters learned from the maximum entropy classifier. With the annotated Twitter conversation dataset (described in Section 4.2), we experimented with several classifiers (Decision tree, Naive Bayes) and chose the maximum entropy classifier because it performed the best, similar to other joint topic models (Zhao et al., 2010; Mukherjee et al., 2013). Type Name Birthday Location Contact Occupation Education Family Trigram My name is, My last name My birthday is, My birthday party I live in, I lived in, I live on My email address, My phone number My job is, My new job My high school, My college is My dad is, My mom is, My family is Table 2: Example seed trigrams for identifying M level of SD. There are 51 of these used in SDTM. Utterances with H level express secretive wishes or sensitive information that exposes self or someone close (Barak and Gluck-Ofri, 2007). These are 44 Category physical appearance mental/ph"
W14-2706,J04-3002,0,\N,Missing
W19-8619,D16-1032,0,0.0504069,"structured input data (i.e., tables) (Kukich, 1983; Reiter and Dale, 1997; Barzilay and Lapata, 2005; Angeli et al., 2010; Kim and Mooney, 2010; Perez-Beltrachini and Gardent, 2017). Traditionally, it is divided into two subtasks: content selection (i.e., what to say) and the surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Recent neural generation systems ignore the distinction of these two subtasks using a single encoder-decoder model (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Kiddon et al., 2016; Chisholm et al., 2017). ∗ Johnny Allen Hendrix November 27, 1942 Seattle, Washington, U.S. Rock, psychedelic rock, hard rock, blues, R&B Musician, songwriter, producer Guitar, vocals Contribution during internship at Microsoft. 141 Proceedings of The 12th International Conference on Natural Language Generation, pages 141–146, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics pooling methods suggest that capturing dependencies among the input records is helpful. However, RNN based methods capture only the sequential relationships among the input data, which"
W19-8619,C10-2062,0,0.0200019,"e records by treating the input records as a sequence. As a result, it is common that two records in the input data are relevant. For example, as shown in Table 1, the input record “Instruments: Guitar, vocals” is related to “Occupation: Musician, songwriter, producer”. The improvements of RNN based methods over Introduction Data-to-text generation, one classic task of natural language generation, aims to produce a piece of texts that adequately and fluently describes its structured input data (i.e., tables) (Kukich, 1983; Reiter and Dale, 1997; Barzilay and Lapata, 2005; Angeli et al., 2010; Kim and Mooney, 2010; Perez-Beltrachini and Gardent, 2017). Traditionally, it is divided into two subtasks: content selection (i.e., what to say) and the surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Recent neural generation systems ignore the distinction of these two subtasks using a single encoder-decoder model (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Kiddon et al., 2016; Chisholm et al., 2017). ∗ Johnny Allen Hendrix November 27, 1942 Seattle, Washington, U.S. Rock, psychedelic rock, hard roc"
W19-8619,P17-4012,0,0.0433431,"twork with layer normalization (Vaswani et al.). Specifically, hli = 3.2 Implementation Details We tune all hyper-parameters according to the performance on the separated validation set. The dimension of trainable embeddings and hidden units in LSTMs are all set to 600. For the multi-layer and multi-head architecture, 3 layers and 4 multiattention heads are used. During training, we regularize all layers with a dropout 0.1. For optimization, we use Adam with learning rate 0.0002. The gradient is truncated by 1. All experiments use beam size of 5 in decoding. We use pytorch version of OpenNMT (Klein et al., 2017) for implementation. Qli hli (Kjl hlj )T esi,j √ = ; βi,j = PT (4) si,n dk n=1 e T X Datasets and Evaluation Metrics We conduct experiments on two datasets. E2E (Novikova et al., 2017) is a dataset for taskoriented language generation in the restaurant domain with 50,000 samples, the validation and test data are multi-reference; WIKIBIO (Lebret et al., 2016) contains 728,321 articles from English Wikipedia. It uses the first sentence of each article as the description of the corresponding infobox. The detailed statics of two datasets are listed in Table 2. For evaluation metrics, we use BLEU-4"
W19-8619,P83-1022,0,0.537574,"he input table independently while RNN based methods model the relationships among the records by treating the input records as a sequence. As a result, it is common that two records in the input data are relevant. For example, as shown in Table 1, the input record “Instruments: Guitar, vocals” is related to “Occupation: Musician, songwriter, producer”. The improvements of RNN based methods over Introduction Data-to-text generation, one classic task of natural language generation, aims to produce a piece of texts that adequately and fluently describes its structured input data (i.e., tables) (Kukich, 1983; Reiter and Dale, 1997; Barzilay and Lapata, 2005; Angeli et al., 2010; Kim and Mooney, 2010; Perez-Beltrachini and Gardent, 2017). Traditionally, it is divided into two subtasks: content selection (i.e., what to say) and the surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Recent neural generation systems ignore the distinction of these two subtasks using a single encoder-decoder model (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Kiddon et al., 2016; Chisholm et al., 2017). ∗ John"
W19-8619,D16-1128,0,0.176238,"Missing"
W19-8619,P09-1011,0,0.0391702,"ion model introduced in (Bahdanau et al., 2015) to compute the attention weight αt,j . The neural data-to-text generation is based on the encoder-decoder architecture. As shown in Figure 1 , there are multiple choices of table encoding that affect the generation decoder. We briefly introduce the backbone of the neural generation method in Section 2.1 and then introduce the details of three types of table encoders in Section 2.2. 2.2 Table Encoder Record Vectors: The input table can be viewed as a set of field-value records, where values are sequences of words corresponding to a certain field (Liang et al., 2009; Lebret et al., 2016; Yang et al., 2017). For instance, in the Table 1, the word “William” has the field “Birth name” and it is the first word in this field. Every word in the field is a record r and is presented as triple (rv , rf , rpos ), where rv , rf and rpos refer to the value (e.g., William), the field name (e.g., Birth name), the relative position in its field (e.g., 0). We map each record r ∈ S into a vector r by concatenating the embedding of rv , rf and rpos , denoted as r = [ev , ef , epos ], where ev , ef , epos are trainable word embeddings of rv , rf and rpos . Pooling Based En"
W19-8619,N16-1086,0,0.34088,"ts that adequately and fluently describes its structured input data (i.e., tables) (Kukich, 1983; Reiter and Dale, 1997; Barzilay and Lapata, 2005; Angeli et al., 2010; Kim and Mooney, 2010; Perez-Beltrachini and Gardent, 2017). Traditionally, it is divided into two subtasks: content selection (i.e., what to say) and the surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Recent neural generation systems ignore the distinction of these two subtasks using a single encoder-decoder model (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Kiddon et al., 2016; Chisholm et al., 2017). ∗ Johnny Allen Hendrix November 27, 1942 Seattle, Washington, U.S. Rock, psychedelic rock, hard rock, blues, R&B Musician, songwriter, producer Guitar, vocals Contribution during internship at Microsoft. 141 Proceedings of The 12th International Conference on Natural Language Generation, pages 141–146, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics pooling methods suggest that capturing dependencies among the input records is helpful. However, RNN based methods capture only the seque"
W19-8619,D18-1422,1,0.804687,"both models under different shuffling noise are more stable than RnnEnc. The experimental results further confirms that using order invariant encoding SelfAtt is stable and suitable for table encoder. 3.4 Limitations of Self-Attention Mechanism The self-attention table encoder achieves comparable performance with respect to recurrent table encoders on E2E and WIKIBIO datasets. However, the input of these two datasets are relatively short. To investigate the performance of selfattention mechanism on capturing long range dependencies, we conduct SelfAtt on a recently proposed NBA dataset ESPN (Nie et al., 2018), where the average input length 165.9 and average field number is 134.2. The results on ESPN are shown in Table 5. The SelfAtt has difficulty in learning such long range dependencies and performs worse than RnnEnc. When applying a restricted self-attention ReSAtt (Wang 144 et al., 2018), where we limit the self-attention mechanism to capture dependencies within a fixed window size (set to 10 in the experiments), the result of ReSAtt performs comparable with respect to RnnEnc, despite this type of method is not order invariant. Handling long range dependencies for input data that is non-sensit"
W19-8619,D10-1049,0,0.0362264,"elationships among the records by treating the input records as a sequence. As a result, it is common that two records in the input data are relevant. For example, as shown in Table 1, the input record “Instruments: Guitar, vocals” is related to “Occupation: Musician, songwriter, producer”. The improvements of RNN based methods over Introduction Data-to-text generation, one classic task of natural language generation, aims to produce a piece of texts that adequately and fluently describes its structured input data (i.e., tables) (Kukich, 1983; Reiter and Dale, 1997; Barzilay and Lapata, 2005; Angeli et al., 2010; Kim and Mooney, 2010; Perez-Beltrachini and Gardent, 2017). Traditionally, it is divided into two subtasks: content selection (i.e., what to say) and the surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Recent neural generation systems ignore the distinction of these two subtasks using a single encoder-decoder model (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Kiddon et al., 2016; Chisholm et al., 2017). ∗ Johnny Allen Hendrix November 27, 1942 Seattle, Washington, U.S. Rock, psyc"
W19-8619,W17-5525,0,0.0730239,"Missing"
W19-8619,H05-1042,0,0.066573,"N based methods model the relationships among the records by treating the input records as a sequence. As a result, it is common that two records in the input data are relevant. For example, as shown in Table 1, the input record “Instruments: Guitar, vocals” is related to “Occupation: Musician, songwriter, producer”. The improvements of RNN based methods over Introduction Data-to-text generation, one classic task of natural language generation, aims to produce a piece of texts that adequately and fluently describes its structured input data (i.e., tables) (Kukich, 1983; Reiter and Dale, 1997; Barzilay and Lapata, 2005; Angeli et al., 2010; Kim and Mooney, 2010; Perez-Beltrachini and Gardent, 2017). Traditionally, it is divided into two subtasks: content selection (i.e., what to say) and the surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Recent neural generation systems ignore the distinction of these two subtasks using a single encoder-decoder model (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Kiddon et al., 2016; Chisholm et al., 2017). ∗ Johnny Allen Hendrix November 27, 1942 Seattle, Washin"
W19-8619,P02-1040,0,0.104697,"or implementation. Qli hli (Kjl hlj )T esi,j √ = ; βi,j = PT (4) si,n dk n=1 e T X Datasets and Evaluation Metrics We conduct experiments on two datasets. E2E (Novikova et al., 2017) is a dataset for taskoriented language generation in the restaurant domain with 50,000 samples, the validation and test data are multi-reference; WIKIBIO (Lebret et al., 2016) contains 728,321 articles from English Wikipedia. It uses the first sentence of each article as the description of the corresponding infobox. The detailed statics of two datasets are listed in Table 2. For evaluation metrics, we use BLEU-4 (Papineni et al., 2002) to assess the generation quality automatically. Self-Attention Encoders: For data-to-text generation, input records are order invariant as input data should convey the same information regardless of the order of input records. The input records is a set, while the recurrent encoder makes strong hypothesis and treats it as a sequence. si,j Experiments βi,j (Vjl hlj ); hl+1 = f (hli + g(hli )) (5) i 3.3 Performance j=0 The results of different input encoding methods along with other competing systems on the test sets of three datasets are shown in Table 3. We compare three types of encoders (i."
W19-8619,E17-1060,0,0.0696273,"a (i.e., tables) (Kukich, 1983; Reiter and Dale, 1997; Barzilay and Lapata, 2005; Angeli et al., 2010; Kim and Mooney, 2010; Perez-Beltrachini and Gardent, 2017). Traditionally, it is divided into two subtasks: content selection (i.e., what to say) and the surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Recent neural generation systems ignore the distinction of these two subtasks using a single encoder-decoder model (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Kiddon et al., 2016; Chisholm et al., 2017). ∗ Johnny Allen Hendrix November 27, 1942 Seattle, Washington, U.S. Rock, psychedelic rock, hard rock, blues, R&B Musician, songwriter, producer Guitar, vocals Contribution during internship at Microsoft. 141 Proceedings of The 12th International Conference on Natural Language Generation, pages 141–146, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics pooling methods suggest that capturing dependencies among the input records is helpful. However, RNN based methods capture only the sequential relationships among the input data, which is sensitive to its inpu"
W19-8619,W17-3537,0,0.0178087,"the input records as a sequence. As a result, it is common that two records in the input data are relevant. For example, as shown in Table 1, the input record “Instruments: Guitar, vocals” is related to “Occupation: Musician, songwriter, producer”. The improvements of RNN based methods over Introduction Data-to-text generation, one classic task of natural language generation, aims to produce a piece of texts that adequately and fluently describes its structured input data (i.e., tables) (Kukich, 1983; Reiter and Dale, 1997; Barzilay and Lapata, 2005; Angeli et al., 2010; Kim and Mooney, 2010; Perez-Beltrachini and Gardent, 2017). Traditionally, it is divided into two subtasks: content selection (i.e., what to say) and the surface realization (i.e., how to say) (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Recent neural generation systems ignore the distinction of these two subtasks using a single encoder-decoder model (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Kiddon et al., 2016; Chisholm et al., 2017). ∗ Johnny Allen Hendrix November 27, 1942 Seattle, Washington, U.S. Rock, psychedelic rock, hard rock, blues, R&B Musician, songwriter, pr"
W19-8619,P16-2008,0,0.0465617,"Missing"
W19-8619,P17-1099,0,0.035585,"ndependently, therefore, it first applies a feed forward neural network layer over every record vector rj and yields the input hidden vector hj = tanh(W rj ), where W is a trainable parameter. The initial context vector c0 in Eq. 1 is calculated by the following maxpooling layer. Base Model Given a set of records S = {rj }K j=1 , the goal of data-to-text generation is to produce a description y = y1 , ..., yT . Usually, the encoder-decoder architecture consist of a table encoder and a recurrent neural network based decoder segmented with attention (Bahdanau et al., 2015) and conditional copy (See et al., 2017) mechanism. Firstly, each input record rj is encoded into a hidden vector hj using a specified table encoder, which is the focus of this paper and three encoders will be introduced in Section 2.2. Then, for the generated description y, the decoder generates the word yt at the t-th time step based on the previously generated words y&lt;t and the input hidden vectors H = {hj }K j=1 . Specifically, P (yt |y&lt;t , H) = softmax(f (dt , yt−1 , ct )) αt,j hj j=1 Method 2.1 K X (1) where f (.) is a tanh function and dt = LSTM(dt−1 , yt−1 , , ct−1 ) is the hidden state of the decoder at step t. ct in Eq. 1"
W19-8619,P17-1018,0,0.0346926,"nd other systems over three datasets. dependency among the records by treating the set of record vectors r1 , ..., rT as a sequence. The sequence of records are fed into a RNN yielding a sequence of input hidden vectors h1 , ...hT . We adopt a bidirectional LSTM by following (Mei et al., 2016). The initial context vector is set as the last hidden vector of the sequence c0 = hT . 3 3.1 Therefore an ideal table encoder has two desired properties: a) enable to capture relationships among the input records and b) is also order invariant. Recently proposed self-attention mechanism (Vaswani et al.; Wang et al., 2017) is able to learn interactions between arbitrary records and therefore is also irrelevant to the order of the records. For this purpose, we adapt the multi-layer selfattention mechanism for the encoding. Each layer has two sub-layers: one layer is for multi-head self-attention and the other one is a position wise feed-forward neural network with layer normalization (Vaswani et al.). Specifically, hli = 3.2 Implementation Details We tune all hyper-parameters according to the performance on the separated validation set. The dimension of trainable embeddings and hidden units in LSTMs are all set"
W19-8619,D18-1116,0,0.0674075,"Missing"
W19-8619,D17-1197,0,0.0226448,"2015) to compute the attention weight αt,j . The neural data-to-text generation is based on the encoder-decoder architecture. As shown in Figure 1 , there are multiple choices of table encoding that affect the generation decoder. We briefly introduce the backbone of the neural generation method in Section 2.1 and then introduce the details of three types of table encoders in Section 2.2. 2.2 Table Encoder Record Vectors: The input table can be viewed as a set of field-value records, where values are sequences of words corresponding to a certain field (Liang et al., 2009; Lebret et al., 2016; Yang et al., 2017). For instance, in the Table 1, the word “William” has the field “Birth name” and it is the first word in this field. Every word in the field is a record r and is presented as triple (rv , rf , rpos ), where rv , rf and rpos refer to the value (e.g., William), the field name (e.g., Birth name), the relative position in its field (e.g., 0). We map each record r ∈ S into a vector r by concatenating the embedding of rv , rf and rpos , denoted as r = [ev , ef , epos ], where ev , ef , epos are trainable word embeddings of rv , rf and rpos . Pooling Based Encoders: The pooling based encoder treats"
W97-0704,W97-0711,0,\N,Missing
W97-0704,A97-1042,1,\N,Missing
W97-0704,C92-2070,0,\N,Missing
W97-0704,P95-1034,0,\N,Missing
X98-1026,P98-1012,0,0.0398847,"Missing"
X98-1026,P98-1116,0,0.0136715,"Missing"
X98-1026,W97-0707,0,0.0146187,"Missing"
X98-1026,A97-1042,1,0.788819,"Missing"
X98-1026,A92-1006,0,0.0422787,"Missing"
X98-1026,W98-1124,0,0.0146725,"Missing"
X98-1026,W97-0711,0,\N,Missing
X98-1026,C98-1112,0,\N,Missing
X98-1026,C98-1012,0,\N,Missing
X98-1026,P95-1046,1,\N,Missing
X98-1026,A97-1039,0,\N,Missing
zhou-etal-2006-summarizing,W03-0508,0,\N,Missing
zhou-etal-2006-summarizing,A97-1042,1,\N,Missing
zhou-etal-2006-summarizing,N04-1019,0,\N,Missing
zhou-etal-2006-summarizing,P99-1072,0,\N,Missing
zhou-etal-2006-summarizing,P99-1071,0,\N,Missing
zhou-etal-2006-summarizing,W04-1013,1,\N,Missing
