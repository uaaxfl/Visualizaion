1994.bcs-1.8,J85-1002,0,0.0460177,"Missing"
1994.bcs-1.8,H94-1121,0,0.0286108,"Missing"
1994.bcs-1.8,J90-2002,0,0.0278424,"Missing"
1994.bcs-1.8,W93-0223,1,0.811269,"Missing"
1994.bcs-1.8,C94-2191,1,0.866621,"Missing"
1994.bcs-1.8,1994.bcs-1.7,0,0.0857451,"Missing"
1995.tmi-1.6,C88-1021,0,0.0644498,"Missing"
1995.tmi-1.6,A88-1003,0,0.0667661,"Missing"
1995.tmi-1.6,1994.bcs-1.7,0,0.0848101,"Missing"
1995.tmi-1.6,1988.tmi-1.7,1,0.848309,"Missing"
1995.tmi-1.6,1989.mtsummit-1.28,0,0.0478514,"Missing"
1995.tmi-1.6,C90-1016,0,0.0619971,"Missing"
1996.tc-1.6,1985.tc-1.13,0,0.201819,"Missing"
1996.tc-1.6,1995.tmi-1.6,1,0.795772,"Missing"
2005.eamt-1.28,W03-2201,0,0.0797706,"Missing"
2008.amta-papers.5,2008.amta-papers.5,1,0.0512178,"Missing"
2008.amta-papers.5,A97-1011,0,0.0797201,"these corpora is small by today’s standards, we should not that any previous corpus analysis on translation universals (e.g. Laviosa’s (2002) work on simplification) has covered even smaller data. 76 [8th AMTA conference, Hawaii, 21-25 October 2008] (v) They were produced during the same span of time (2005-2008). (vi) They are of a similar size (no. of tokens). richness, which is computed as the number of lemmas divided by the number of tokens present in the corpus and accounts for the variety of word use by an author. The lemma of every word is automatically returned by the Connexor parser (Tapanainen and Jarvinen, 1997). 3 Corpus features Previous studies on universals, unfortunately, have not accounted for what exactly classes as evidence in terms of different text features for their validity. To obtain an objective measure that would quantify the degree to which this or that universal holds, it is important to define features or parameters so that formal empirical studies can be conducted to compare texts in terms of simplification or similarity, and more specifically to verify our hypotheses. In the absence of any such guidelines, the first step to take in this study is to identify features of texts. We p"
2008.amta-papers.5,W06-1111,0,\N,Missing
2008.amta-papers.5,W06-1101,0,\N,Missing
2008.amta-papers.5,J01-4008,0,\N,Missing
2013.mtsummit-wmwumttt.4,P91-1022,0,0.18803,"ted words in order to identify collocations; otherwise, for example, collocations such as committing murder and committed murder would be treated separately, even though they are obviously the same (whose lemma is commit murder). The system relies on TreeTagger to annotate text sentences with both lemma and POS-tagging information. Its output is then transformed into the XML format (see Figure 2) by running a Python script, part of MWEToolkit. 3.2 Parallel corpora Classic approaches to translation using parallel corpora exploited the concepts of alignment and correspondence at sentence level (Brown et al., 1991; Gale and Church, 1993). Two methods were developed: length-based and translationbased (Varga et al., 2005). Collocation translation using parallel corpora has also been approached using transfer systems that rely on generative grammars, because of the notion that the base of a collocation determines its collocatives (Wehrli et al., 2009) and the assumption that source and target MWEs share their syntactic relation (Lü and Zhou, 2004). 3.3 Comparable Corpora Parallel resources are generally scarce and in many cases not available at all. The wider availability of comparable texts offers new op"
2013.mtsummit-wmwumttt.4,W06-1005,0,0.0702929,"Missing"
2013.mtsummit-wmwumttt.4,P89-1010,0,0.560079,"ions according to specific POSpatterns, and Hunalign (Varga et al., 2005) to align corpora at sentence level. Furthermore, it connects online to WordReference and uses it as a multilingual translation dictionary and thesaurus. Figure 1 illustrates the architecture of the system; its main modules will be described in greater detail in the following paragraphs. 3.1 Collocation retrieval Early work on collocation extraction focused on statistical processing. Choueka et al. (1983) developed an approach to retrieve sequences of words occurring together over a threshold in their corpora. Similarly, Church and Hanks (1989) proposed a correlation method based on the notion of mutual information. Smadja (1993), however, highlighted the importance of combining statistical and linguistic methods. In recent years, advances have been made (Ramisch et al., 2010; Seretan, 2011), many of them advocating rulebased and hybrid approaches (Hoang, Kim and Kam, 2009), and based on language-specific syntactic structures (Santana et al., 2011) or machine learning of lexical functions (Gelbukh and Kolesnikova, 2013). 4.1 Candidate selection module This module processes the SL corpus in order to format it to comply with the input"
2013.mtsummit-wmwumttt.4,P98-1069,0,0.146816,"tic processing, but they are not allinclusive. In fact, as is often the case, there are exceptions to the rules. For example, collocations in English such as copycat crime (delito inspirado en uno precedente or que trata de imitarlo, in Spanish) and to commit suicide (suicidarse in Spanish) cannot be translated using the proposed approach. 3 these do not allow for bridging between languages (Sharoff et al., 2009), research suggests (Rapp, 1995) that a word is closely associated with words in its context and that the association between a base and its collocatives is preserved in any language. Fung and Yuen (1998), for instance, argued that the first clue to the similarity between a word and its translation is the number of common words in their contexts. Similarly, Sharoff et al. (2009) proposed a methodology that relies on similarity classes. Related Work This section presents a brief review of existing techniques for the extraction and translation of collocations. It starts by outlining collocation extraction and then moves to translation. 4 System The system 1 employs the following three language-independent tools: TreeTagger to POS-tag corpora, the MWEToolkit (Ramisch et al., 2010) to extract coll"
2013.mtsummit-wmwumttt.4,J93-1004,0,0.607442,"o identify collocations; otherwise, for example, collocations such as committing murder and committed murder would be treated separately, even though they are obviously the same (whose lemma is commit murder). The system relies on TreeTagger to annotate text sentences with both lemma and POS-tagging information. Its output is then transformed into the XML format (see Figure 2) by running a Python script, part of MWEToolkit. 3.2 Parallel corpora Classic approaches to translation using parallel corpora exploited the concepts of alignment and correspondence at sentence level (Brown et al., 1991; Gale and Church, 1993). Two methods were developed: length-based and translationbased (Varga et al., 2005). Collocation translation using parallel corpora has also been approached using transfer systems that rely on generative grammars, because of the notion that the base of a collocation determines its collocatives (Wehrli et al., 2009) and the assumption that source and target MWEs share their syntactic relation (Lü and Zhou, 2004). 3.3 Comparable Corpora Parallel resources are generally scarce and in many cases not available at all. The wider availability of comparable texts offers new opportunities to both rese"
2013.mtsummit-wmwumttt.4,W09-2905,0,0.0262206,"Missing"
2013.mtsummit-wmwumttt.4,ramisch-etal-2010-mwetoolkit,0,0.0204893,"in any language. Fung and Yuen (1998), for instance, argued that the first clue to the similarity between a word and its translation is the number of common words in their contexts. Similarly, Sharoff et al. (2009) proposed a methodology that relies on similarity classes. Related Work This section presents a brief review of existing techniques for the extraction and translation of collocations. It starts by outlining collocation extraction and then moves to translation. 4 System The system 1 employs the following three language-independent tools: TreeTagger to POS-tag corpora, the MWEToolkit (Ramisch et al., 2010) to extract collocations according to specific POSpatterns, and Hunalign (Varga et al., 2005) to align corpora at sentence level. Furthermore, it connects online to WordReference and uses it as a multilingual translation dictionary and thesaurus. Figure 1 illustrates the architecture of the system; its main modules will be described in greater detail in the following paragraphs. 3.1 Collocation retrieval Early work on collocation extraction focused on statistical processing. Choueka et al. (1983) developed an approach to retrieve sequences of words occurring together over a threshold in their"
2013.mtsummit-wmwumttt.4,W12-3311,0,0.0484438,"Missing"
2013.mtsummit-wmwumttt.4,P95-1050,0,0.204309,"IN + NN Table 3: English-Spanish syntax comparison It is worth noting that these transfer rules are designed to aid us in our own approach to the task 19 of syntactic processing, but they are not allinclusive. In fact, as is often the case, there are exceptions to the rules. For example, collocations in English such as copycat crime (delito inspirado en uno precedente or que trata de imitarlo, in Spanish) and to commit suicide (suicidarse in Spanish) cannot be translated using the proposed approach. 3 these do not allow for bridging between languages (Sharoff et al., 2009), research suggests (Rapp, 1995) that a word is closely associated with words in its context and that the association between a base and its collocatives is preserved in any language. Fung and Yuen (1998), for instance, argued that the first clue to the similarity between a word and its translation is the number of common words in their contexts. Similarly, Sharoff et al. (2009) proposed a methodology that relies on similarity classes. Related Work This section presents a brief review of existing techniques for the extraction and translation of collocations. It starts by outlining collocation extraction and then moves to tra"
2013.mtsummit-wmwumttt.4,J93-1007,0,0.916411,"the expression I made my homework is not. The choice of using the verb to do and not the verb to make in this particular example can be thought of as an arbitrary convention. In addition, some collocates exhibit delexical and metaphorical meanings (to make an attempt, to toy with an idea). Similarly, collocations are cohesive lexical clusters. This means that the presence of one or several component words of a collocation in a phrase often suggests the existence of the remaining component words of that collocation. This property attributes particular statistical distributions to collocations (Smadja, 1993). For example, in a sample text containing the words bid and farewell, the probability of the two of them appearing together is higher than the probability of the two of them appearing individually. object) 2. NN or JJ + NN 3. NN + of + NN 4. RB + JJ 5. VB + RB 6. VB + IN + NN 7. VB + NN (subject) Examples to express concern, to bid farewell traumatic experience, copycat crime pinch of salt, pride of lions deadly serious, fast asleep to speak vaguely, to sob bitterly to take into consideration, to jump to a conclusion to break out <war&gt;, to crow <a cock&gt; Table 1: Typology of collocations in En"
2013.mtsummit-wmwumttt.4,W09-0415,0,0.0272285,"utput is then transformed into the XML format (see Figure 2) by running a Python script, part of MWEToolkit. 3.2 Parallel corpora Classic approaches to translation using parallel corpora exploited the concepts of alignment and correspondence at sentence level (Brown et al., 1991; Gale and Church, 1993). Two methods were developed: length-based and translationbased (Varga et al., 2005). Collocation translation using parallel corpora has also been approached using transfer systems that rely on generative grammars, because of the notion that the base of a collocation determines its collocatives (Wehrli et al., 2009) and the assumption that source and target MWEs share their syntactic relation (Lü and Zhou, 2004). 3.3 Comparable Corpora Parallel resources are generally scarce and in many cases not available at all. The wider availability of comparable texts offers new opportunities to both researchers and translators. While 1 Consisting of a series of Python scripts which handle text and XML representations, and implemented using the wxPython development environment for Mac OSX. 20 Figure 1: Architectural scheme of the system POS-pattern definition aims at applying syntactic constraints on collocation can"
2013.mtsummit-wmwumttt.4,P04-1022,0,0.40784,"Toolkit. 3.2 Parallel corpora Classic approaches to translation using parallel corpora exploited the concepts of alignment and correspondence at sentence level (Brown et al., 1991; Gale and Church, 1993). Two methods were developed: length-based and translationbased (Varga et al., 2005). Collocation translation using parallel corpora has also been approached using transfer systems that rely on generative grammars, because of the notion that the base of a collocation determines its collocatives (Wehrli et al., 2009) and the assumption that source and target MWEs share their syntactic relation (Lü and Zhou, 2004). 3.3 Comparable Corpora Parallel resources are generally scarce and in many cases not available at all. The wider availability of comparable texts offers new opportunities to both researchers and translators. While 1 Consisting of a series of Python scripts which handle text and XML representations, and implemented using the wxPython development environment for Mac OSX. 20 Figure 1: Architectural scheme of the system POS-pattern definition aims at applying syntactic constraints on collocation candidates. This stage is language-dependent: as long as a language can be POS-tagged and a typology"
2020.coling-main.445,2020.acl-main.747,0,0.253815,"Missing"
2020.coling-main.445,N19-1423,0,0.278988,"odel requires extensive predictor pre-training and relies on large parallel data and computational resources. In order to remove the dependency on large parallel data, which also entails the need for powerful computational resources, we propose to use crosslingual embeddings that are already fine-tuned to reflect properties between languages. We assume that by using them we will ease the burden of having complex neural network architectures. Over the last few years there has been significant work done in the area of crosslingual embeddings (Ruder et al., 2019). Since the introduction of BERT (Devlin et al., 2019), transformer models have been used successfully for various NLP tasks such as named entity recognition (Devlin et al., 2019), sentence classification (Sun et al., 2019), and question answering (Devlin et al., 2019), in many cases improving the state of the art. Most of the tasks were focused on English due to the fact that most of the pre-trained transformer models were trained on English data. Although there are several multilingual models like multilingual BERT (mBERT) (Devlin et al., 2019) and multilingual DistilBERT (mDistilBERT) (Sanh et al., 2019), researchers expressed some reservation"
2020.coling-main.445,N15-1124,0,0.030281,"4.2 Predicting DA Even though HTER has been typically used to assess quality in machine translations, the reliability of this metric for assessing the performance of quality estimation systems has been questioned by researchers (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2017), where raters evaluate the machine translation on a continuous 1-100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015). We used a recently created dataset to predict DA in machine translations which was released for the WMT 2020 quality estimation shared task 1 (Specia et al., 2020). The dataset is composed of data extracted from Wikipedia for six language pairs, consisting of high-resource English-German (En-De) and English-Chinese (En-Zh), medium-resource Romanian-English (Ro-En) and Estonian-English (EtEn), and low-resource Sinhala-English (Si-En) and Nepalese-English (Ne-En), as well as a Russian2 Language codes are available in ISO 639-1 Registration Authority Website Online - https://www.loc.gov/ standa"
2020.coling-main.445,C16-1294,0,0.022276,"English-Czech (En-Cs), English-German (EnDe), English-Russian (En-Ru), English-Latvian (En-Lv) and German-English (De-En). The texts are from a variety of domains and the translations were produced using both neural and statistical machine translation systems. More details about these datasets can be found in Table 1 and in (Specia et al., 2018; Fonseca et al., 2019). 4.2 Predicting DA Even though HTER has been typically used to assess quality in machine translations, the reliability of this metric for assessing the performance of quality estimation systems has been questioned by researchers (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2017), where raters evaluate the machine translation on a continuous 1-100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015). We used a recently created dataset to predict DA in machine translations which was released for the WMT 2020 quality estimation shared task 1 (Specia et al., 2020). The dataset is composed of data extracted from Wikipedia for si"
2020.coling-main.445,C18-1266,0,0.272386,"workflows as they have numerous potential uses. They can be employed to select the best translation when several translation engines are available or can inform the end user about the reliability of automatically translated content. In addition, QE systems can be used to decide whether a translation can be published as it is in a given context, or whether it requires human post-editing before publishing or even translation from scratch by a human (Kepler et al., 2019). The estimation of translation quality can be done at different levels: document level, sentence level and word/phrase level (Ive et al., 2018). In this research we focus on sentence-level quality estimation. As we discuss in Section 2, at present neural-based QE methods constitute the state of the art in quality estimation. However, these approaches are based on complex neural networks and require resourceintensive training. This resource-intensive nature of these deep-learning-based frameworks makes it expensive to have QE systems that work for several languages at the same time. Furthermore, these architectures require a large number of annotated instances for training, making the quality estimation task very difficult for low-res"
2020.coling-main.445,P19-3020,0,0.161523,"Missing"
2020.coling-main.445,W17-4763,0,0.0617344,"endent on linguistic processing and feature engineering to train traditional machine-learning algorithms like support vector regression and randomised decision trees (Specia et al., 2013). Even though, they provided good results, these traditional approaches are no longer the state of the art. In recent years, neural-based QE systems have consistently topped the leader boards in WMT quality estimation shared tasks (Kepler et al., 2019). For example, the best-performing system at the WMT 2017 shared task on QE was POSTECH, which is purely neural and does not rely on feature engineering at all (Kim et al., 2017). POSTECH revolves around an encoder-decoder Recurrent Neural Network (RNN) (referred to as the ’predictor’), stacked with a bidirectional RNN (the ’estimator’) that produces quality estimates. In the predictor, an encoderdecoder RNN model predicts words based on their context representations and in the estimator step there is a bidirectional RNN model to produce quality estimates for words, phrases and sentences based on representations from the predictor. To be effective, POSTECH requires extensive predictor pre-training, which means it depends on large parallel data and is computationally i"
2020.coling-main.445,W15-3037,0,0.0289313,"ir context representations and in the estimator step there is a bidirectional RNN model to produce quality estimates for words, phrases and sentences based on representations from the predictor. To be effective, POSTECH requires extensive predictor pre-training, which means it depends on large parallel data and is computationally intensive (Ive et al., 2018). The POSTECH architecture was later re-implemented in deepQuest (Ive et al., 2018). OpenKiwi (Kepler et al., 2019) is another open-source QE framework developed by Unbabel. It implements four different neural network architectures QUETCH (Kreutzer et al., 2015), N U QE (Martins et al., 2016), Predictor-Estimator (Kim et al., 2017) and a stacked model of those architectures. Both the QUETCH and N U QE architectures have simple neural network models that do not rely on 1 The public GitHub repository is available on https://github.com/tharindudr/transquest and the official documentation is available on https://tharindudr.github.io/TransQuest. 5071 additional parallel data, but do not perform that well. The Predictor-Estimator model is similar to the POSTECH architecture and relies on additional parallel data. In OpenKiwi, the best performance for sente"
2020.coling-main.445,W16-2387,0,0.0155901,"n the estimator step there is a bidirectional RNN model to produce quality estimates for words, phrases and sentences based on representations from the predictor. To be effective, POSTECH requires extensive predictor pre-training, which means it depends on large parallel data and is computationally intensive (Ive et al., 2018). The POSTECH architecture was later re-implemented in deepQuest (Ive et al., 2018). OpenKiwi (Kepler et al., 2019) is another open-source QE framework developed by Unbabel. It implements four different neural network architectures QUETCH (Kreutzer et al., 2015), N U QE (Martins et al., 2016), Predictor-Estimator (Kim et al., 2017) and a stacked model of those architectures. Both the QUETCH and N U QE architectures have simple neural network models that do not rely on 1 The public GitHub repository is available on https://github.com/tharindudr/transquest and the official documentation is available on https://tharindudr.github.io/TransQuest. 5071 additional parallel data, but do not perform that well. The Predictor-Estimator model is similar to the POSTECH architecture and relies on additional parallel data. In OpenKiwi, the best performance for sentence-level quality estimation wa"
2020.coling-main.445,N19-4009,0,0.0355003,"entences. In that column NMT indicates Neural Machine Translation and SMT indicates Statistical Machine Translation. Competition shows the quality estimation competition in which the data was released and the last column indicates the number of instances the train, development and test dataset had in each language pair respectively. English (En-Ru) dataset which combines articles from Wikipedia and Reddit (Fomicheva et al., 2020). These datasets have been collected by translating sentences sampled from source-language articles using state-of-the-art NMT models built using the fairseq toolkit (Ott et al., 2019) and annotated with DA scores by professional translators. Each translation was rated with a score from 0-100 according to the perceived translation quality by at least three translators (Specia et al., 2020). The DA scores were standardised using the z-score. The quality estimation systems evaluated on these datasets have to predict the mean DA z-scores of test sentence pairs. Each language pair has 7,000 sentence pairs in the training set, 1,000 sentence pairs in the development set and another 1,000 sentence pairs in the testing set. 5 Evaluation and discussion This section presents the eva"
2020.coling-main.445,P19-1493,0,0.0205766,"arious NLP tasks such as named entity recognition (Devlin et al., 2019), sentence classification (Sun et al., 2019), and question answering (Devlin et al., 2019), in many cases improving the state of the art. Most of the tasks were focused on English due to the fact that most of the pre-trained transformer models were trained on English data. Although there are several multilingual models like multilingual BERT (mBERT) (Devlin et al., 2019) and multilingual DistilBERT (mDistilBERT) (Sanh et al., 2019), researchers expressed some reservations about their ability to represent all the languages (Pires et al., 2019). In addition, although mBERT and mDistilBERT showed some crosslingual characteristics, they do not perform well on crosslingual benchmarks (K et al., 2020). XLM-RoBERTa (XML-R) was released in November 2019 (Conneau et al., 2020) as an update to the XLM-100 model (Conneau and Lample, 2019). XLM-R takes a step back from XLM, eschewing XLM’s Translation Language Modeling (TLM) objective since it requires a dataset of parallel sentences, which can be difficult to acquire. Instead, XLM-R trains RoBERTa(Liu et al., 2019) on a huge, multilingual dataset at an enormous scale: unlabelled text in 104"
2020.coling-main.445,R19-1116,1,0.618898,"tmax layer that predicts the quality score of the translation. We used mean-squared-error loss as the objective function. Early experiments we carried out demonstrated that the CLS-strategy leads to better results than the other two strategies for this architecture. Therefore, we used the embedding of the [CLS] token as the input of a softmax layer. 2. SiameseTransQuest (STransQuest): The second approach proposed in this paper relies on the Siamese architecture depicted in Figure 1b which has shown promising results in monolingual semantic textual similarity tasks (Reimers and Gurevych, 2019; Ranasinghe et al., 2019). In this case, we feed the original text and the translation into two separate XLM-R transformer models. Similar to the previous architecture we used the same three pooling strategies for the outputs of the transformer models. We then calculated the cosine similarity between the two outputs of the pooling strategy. We used mean-squared-error loss as the objective function. In initial experiments we carried out with this architecture, the MEAN-strategy showed better results than the other two strategies. For this reason, we used the MEAN-strategy for our experiments. Therefore, cosine similari"
2020.coling-main.445,2020.wmt-1.122,1,0.89248,"inning solutions submitted to recent shared tasks on 15 different language pairs on different aspects of quality estimation. In fact, a tuned version of TransQuest was declared the winner for all 8 tasks of the direct This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 5070 Proceedings of the 28th International Conference on Computational Linguistics, pages 5070–5081 Barcelona, Spain (Online), December 8-13, 2020 assessment sentence level QE shared task organised at WMT 2020 (for more details see (Ranasinghe et al., 2020) and Section 5.1. The main contributions of this paper are the following: 1. We introduce TransQuest, an open-source framework, and use it to implement two neural network architectures that outperform current state-of-the-art quality estimation methods in two different aspects of sentence-level quality estimation. 2. To the best of our knowledge this is the first neural-based method which develops a model capable of providing quality estimation for more than one language pair. In this way we address the problem of high costs required to maintain a multi-language-pair QE environment. 3. We tack"
2020.coling-main.445,D19-1410,0,0.114697,"us scale: unlabelled text in 104 languages is extracted from CommonCrawl datasets, totalling 2.5TB of text. It is trained using only RoBERTa’s (Liu et al., 2019) masked language modelling (MLM) objective. Surprisingly, this strategy provided better results in crosslingual tasks. XLM-R outperforms mBERT on a variety of crosslingual benchmarks such as crosslingual natural language inference and crosslingual question answering (Conneau et al., 2020). Both architectures proposed in TransQuest have been successfully applied in the monolingual semantic textual similarity tasks (Devlin et al., 2019; Reimers and Gurevych, 2019). When applied in monolingual experiments, both of them use monolingual transformer models like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) as the input. This inspired us to change the input in such a way that it can represent both the source and target sentences for which the quality of translation needs to be estimated, with the hope that the same architectures would also provide good results in the QE task. Our initial experiments showed that crosslingual embeddings like XLM-R provide better results than multilingual embeddings like mBERT. Therefore, in this research we explore t"
2020.coling-main.445,P13-4014,0,0.244407,"Missing"
2020.coling-main.445,P15-4020,0,0.440699,"followed by the evaluation and discussion in Section 5. The paper finishes with conclusions and ideas for future research directions. 2 Related Work During the past decade there has been tremendous progress in the field of quality estimation, largely as a result of the QE shared tasks organised annually by the Workshops on Statistical Machine Translation (WMT), more recently called the Conferences on Machine Translation, since 2012. The annotated datasets these shared tasks released each year have led to the development of many open-source QE systems like QuEst (Specia et al., 2013), QuEst++ (Specia et al., 2015), deepQuest (Ive et al., 2018), and OpenKiwi (Kepler et al., 2019). Before the neural network era, most of the quality estimation systems like QuEst (Specia et al., 2013) and QuEst++ (Specia et al., 2015) were heavily dependent on linguistic processing and feature engineering to train traditional machine-learning algorithms like support vector regression and randomised decision trees (Specia et al., 2013). Even though, they provided good results, these traditional approaches are no longer the state of the art. In recent years, neural-based QE systems have consistently topped the leader boards"
2020.coling-main.445,2020.wmt-1.79,0,0.107633,"Missing"
2020.eamt-1.19,1978.tc-1.0,0,0.63431,"Missing"
2020.eamt-1.19,S17-2001,0,0.0398336,"Missing"
2020.eamt-1.19,D18-2029,0,0.0257507,"Missing"
2020.eamt-1.19,W15-5204,0,0.0316142,"ible to improve the quality of matching by taking into consideration the syntactic structure of sentences. Unfortunately, the evaluation is carried out on only a handful of carefully selected segments. Another method which performs matching at level of syntactic trees is proposed in (Vanallemeersch and Vandeghinste, 2014). The results presented in their paper are preliminary and the authors notice that tree matching method is “prohibitively slow”. More recent work has focused on incorporating paraphrases into the matching and retrieving algorithm (Utiyama et al., 2011; Gupta and Orasan, 2014; Chatzitheodorou, 2015). Utiyama et al. (2011) proposed a finite transducer which considers paraphrases during the matching. The evaluation shows that the method improves both precision and recall of matching, but it was carried out with only one translator and focused only on segments with exactly the same meaning. Gupta and Orasan (2014) proposed a variant of the edit distance metric which incorporates paraphrases from PPDB5 using greedy approximation and dynamic programming. Both automatic evaluation and evaluation with translators show the advantages of using this approach (Gupta et al., 2016). Chatzitheodorou ("
2020.eamt-1.19,D17-1070,0,0.0555431,"ilarity between texts. It is easy to calculate how close or distant two vectors are by using well understood mathematical distance metrics. In addition, deep learning based methods proved more robust in numerous NLP applications. In this paper we propose a novel TM matching and retrieval method based on the Universal Sentence Encoder (Cer et al., 2018) which has the capability to capture semantically similar segments in TMs better than methods based on edit distance. We selected the Universal Sentence Encoder as our sentence encoder since it outperforms other sentence encoders like Infersent (Conneau et al., 2017) in many Natural Language Processing tasks including Semantic Retrieval (Cer et al., 2018). Also the recently release of Multilingual Universal Sentence Encoder 3 is available on 16 different languages (Yang et al., 2019). Since we are planning to expand our research to other language pairs than the English - Spanish pair investigated in this paper, the multilingual aspect of the Universal Sentence Encoder can prove very useful. The rest of the paper is organised as follows. Section 2 briefly describes several approaches used to improve the matching and retrieval in TMs. Section 3 contains inf"
2020.eamt-1.19,W14-3348,0,0.00843715,"trieve a match from the translation memory for one incoming sentence is just 1.6s, which is reasonable. In light of this, we decided to use the Transformer Architecture for future experiments since it is efficient enough and since it was reported that it provides better accuracy in semantic retrieval tasks than the DAN Architecture (Cer et al., 2018). 3.2.3 Results In order to compare the results obtained by our method with those of an existing translation memory tool we used Okapi which uses simple edit distance to retrieve matches from the translation memory. We calculated the METEOR score (Denkowski and Lavie, 2014) between the actual translation of the incoming segment and the match we retrieved from the translation memory with the transformer architecture of the Universal Sentence Encoder. We repeated the same process with the match we retrieved from Okapi. We used METEOR score since we believed it can capture the semantic similarity between two segments better than the BLEU score (Denkowski and Lavie, 2014). To understand the performance of our method, we first removed the segments where the match provided by Okapi and the Universal Sentence Encoder was same. Then, to have a better analysis of the res"
2020.eamt-1.19,2014.eamt-1.2,0,0.0377279,"007) show how it is possible to improve the quality of matching by taking into consideration the syntactic structure of sentences. Unfortunately, the evaluation is carried out on only a handful of carefully selected segments. Another method which performs matching at level of syntactic trees is proposed in (Vanallemeersch and Vandeghinste, 2014). The results presented in their paper are preliminary and the authors notice that tree matching method is “prohibitively slow”. More recent work has focused on incorporating paraphrases into the matching and retrieving algorithm (Utiyama et al., 2011; Gupta and Orasan, 2014; Chatzitheodorou, 2015). Utiyama et al. (2011) proposed a finite transducer which considers paraphrases during the matching. The evaluation shows that the method improves both precision and recall of matching, but it was carried out with only one translator and focused only on segments with exactly the same meaning. Gupta and Orasan (2014) proposed a variant of the edit distance metric which incorporates paraphrases from PPDB5 using greedy approximation and dynamic programming. Both automatic evaluation and evaluation with translators show the advantages of using this approach (Gupta et al.,"
2020.eamt-1.19,D15-1124,0,0.0261836,"Missing"
2020.eamt-1.19,P15-1162,0,0.0283772,"Missing"
2020.eamt-1.19,steinberger-etal-2012-dgt,0,0.0874939,"Missing"
2020.eamt-1.19,P15-1150,0,0.00804804,"the same idea. As a result, even if the TM contains a semantically similar segment, the retrieval algorithm will not be able to identify it in most of the cases. Researchers tried to address this shortcoming of the edit distance metric by employing similarity metrics that can identify semantically similar segments even when they are different at token level. Section 2 discusses some of the approaches proposed so far. Recent research on the topic of text similarity employed methods that rely on deep learning and various vector based representations used in this field (Ranasinghe et al., 2019b; Tai et al., 2015; Mueller and Thyagarajan, 2016). One of the reasons for this is that calculating the similarity between vectors is more straightforward than calculating the similarity between texts. It is easy to calculate how close or distant two vectors are by using well understood mathematical distance metrics. In addition, deep learning based methods proved more robust in numerous NLP applications. In this paper we propose a novel TM matching and retrieval method based on the Universal Sentence Encoder (Cer et al., 2018) which has the capability to capture semantically similar segments in TMs better than"
2020.eamt-1.19,2011.mtsummit-papers.37,0,0.0405215,"memories and Mitkov (2007) show how it is possible to improve the quality of matching by taking into consideration the syntactic structure of sentences. Unfortunately, the evaluation is carried out on only a handful of carefully selected segments. Another method which performs matching at level of syntactic trees is proposed in (Vanallemeersch and Vandeghinste, 2014). The results presented in their paper are preliminary and the authors notice that tree matching method is “prohibitively slow”. More recent work has focused on incorporating paraphrases into the matching and retrieving algorithm (Utiyama et al., 2011; Gupta and Orasan, 2014; Chatzitheodorou, 2015). Utiyama et al. (2011) proposed a finite transducer which considers paraphrases during the matching. The evaluation shows that the method improves both precision and recall of matching, but it was carried out with only one translator and focused only on segments with exactly the same meaning. Gupta and Orasan (2014) proposed a variant of the edit distance metric which incorporates paraphrases from PPDB5 using greedy approximation and dynamic programming. Both automatic evaluation and evaluation with translators show the advantages of using this"
2020.eamt-1.19,2014.tc-1.11,0,0.0265146,"lly rich languages like Hungarian. They also experiment with sentence skeletons in which NPs are automatically aligned between source and target. Unfortunately, the paper presents only preliminary results. Pekar 4 https://github.com/tharindudr/ intelligent-translation-memories and Mitkov (2007) show how it is possible to improve the quality of matching by taking into consideration the syntactic structure of sentences. Unfortunately, the evaluation is carried out on only a handful of carefully selected segments. Another method which performs matching at level of syntactic trees is proposed in (Vanallemeersch and Vandeghinste, 2014). The results presented in their paper are preliminary and the authors notice that tree matching method is “prohibitively slow”. More recent work has focused on incorporating paraphrases into the matching and retrieving algorithm (Utiyama et al., 2011; Gupta and Orasan, 2014; Chatzitheodorou, 2015). Utiyama et al. (2011) proposed a finite transducer which considers paraphrases during the matching. The evaluation shows that the method improves both precision and recall of matching, but it was carried out with only one translator and focused only on segments with exactly the same meaning. Gupta"
2020.eamt-1.19,1999.mtsummit-1.48,0,0.433734,"ngines, as well as project management features (Gupta et al., 2016). Even though retrieval of previously translated segments is a key feature in a TM system, this process is still very much limited to edit-distance based measures. Researchers working on natural language processing have proposed a number of methods which try to improve the existing matching and retrieval approaches used by translation memories. However, the majority of these approaches are not suitable for large TMs, like the ones normally employed by professional translators or were evaluated on very small number of segments. Planas and Furuse (1999) extend the edit distance metric to incorporate lemmas and part-of-speech information when calculating the similarity between two segments, but they test their approach on less than 150 segments from two domains using two translation memories with less than 40,000 segments in total. Lemmas and part-of-speech information is also used in (Hod´asz and Pohl, 2005) in order to improve matching, especially for morphologically rich languages like Hungarian. They also experiment with sentence skeletons in which NPs are automatically aligned between source and target. Unfortunately, the paper presents"
2020.eamt-1.19,R19-1115,1,0.928453,"tures are used to express the same idea. As a result, even if the TM contains a semantically similar segment, the retrieval algorithm will not be able to identify it in most of the cases. Researchers tried to address this shortcoming of the edit distance metric by employing similarity metrics that can identify semantically similar segments even when they are different at token level. Section 2 discusses some of the approaches proposed so far. Recent research on the topic of text similarity employed methods that rely on deep learning and various vector based representations used in this field (Ranasinghe et al., 2019b; Tai et al., 2015; Mueller and Thyagarajan, 2016). One of the reasons for this is that calculating the similarity between vectors is more straightforward than calculating the similarity between texts. It is easy to calculate how close or distant two vectors are by using well understood mathematical distance metrics. In addition, deep learning based methods proved more robust in numerous NLP applications. In this paper we propose a novel TM matching and retrieval method based on the Universal Sentence Encoder (Cer et al., 2018) which has the capability to capture semantically similar segments"
2020.eamt-1.19,R19-1116,1,0.910606,"tures are used to express the same idea. As a result, even if the TM contains a semantically similar segment, the retrieval algorithm will not be able to identify it in most of the cases. Researchers tried to address this shortcoming of the edit distance metric by employing similarity metrics that can identify semantically similar segments even when they are different at token level. Section 2 discusses some of the approaches proposed so far. Recent research on the topic of text similarity employed methods that rely on deep learning and various vector based representations used in this field (Ranasinghe et al., 2019b; Tai et al., 2015; Mueller and Thyagarajan, 2016). One of the reasons for this is that calculating the similarity between vectors is more straightforward than calculating the similarity between texts. It is easy to calculate how close or distant two vectors are by using well understood mathematical distance metrics. In addition, deep learning based methods proved more robust in numerous NLP applications. In this paper we propose a novel TM matching and retrieval method based on the Universal Sentence Encoder (Cer et al., 2018) which has the capability to capture semantically similar segments"
2020.eamt-1.19,D19-1410,0,0.0189511,"If we further analyse the fuzzy match score range 0.6-0.8, as shown in table 10, the mean semantic textual similarity for the sentences provided by Universal Sentence Encoder is 0.768. Therefore, we assume that the matches retrieved from the Universal Sentence Encoder in the fuzzy match score range 0.6-0.8 will help to improve the translation productivity. However, this is something that we plan to analyse further by carrying out evaluations with professional translators. In the future, we also plan to experiment with other sentence encoders such as Infersent (Conneau et al., 2017) and SBERT (Reimers and Gurevych, 2019) and with alternative algorithms which are capable to capture semantic textual similarity between two sentences. We will try unsupervised methods like word vector averaging Fuzzy score 0.8 - 1.0 0.6 - 0.8 0.4 - 0.6 0.2 - 0.4 0 - 0.2 Mean STS score 0.952 0.768 0.642 0.315 0.121 Table 10: Mean STS score for the sentences retrieved by Universal Sentence Encoder for each fuzzy match score. Fuzzy score column shows the fuzzy match score ranges and Mean STS score column shows that mean STS score for the sentence retrieved by Universal Sentence Encoder for that fuzzy match score range. and word movin"
2020.semeval-1.94,P19-4007,0,0.0505279,"Missing"
2020.semeval-1.94,W09-4406,0,0.0317659,"Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 717 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 717–723 Barcelona, Spain (Online), December 12, 2020. and a decision tree to further analyse the candidates. Finally, the results are ranked using heuristic rules. All aspects of the system were developed by analysing the Institut Universitari de Ling¨u´ıstica Aplicada technical corpus in Spanish, which is also used for evaluation. Machine learning algorithms have also been used for definition extraction. Gaudio and Branco (2009) describe an approach that is said to be language independent and test it with decision trees and Random Forest, as well as Na¨ıve Bayes, k-Nearest Neighbour and Support Vector Machines using different sampling techniques to varying degrees of success. Kobyli´nski and Przepi´orkowski (2008) process Polish texts and use Balanced Random Forests, which bootstrap equal sets of positive and negative training examples to the classifier, as opposed to a larger group of unequal sets of training examples. Overall, while the approach is said to increase run time, it does bring minor increases in perform"
2020.semeval-1.94,C92-2082,0,0.514864,"including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection. 1 Introduction Definition Extraction refers to the task in Natural Language Processing (NLP) of detecting and extracting a term and its definition in different types of text. A common use of automatic definition extraction is to help building dictionaries (Kobyli´nski and Przepi´orkowski, 2008), but it can be employed for many other applications. For example, ontology building can benefit from methods that extract definitions (Hearst, 1992; Malais´e et al., 2007), whilst the fields of definition extraction and information extraction can employ similar methodologies. It is therefore normal that there is growing interest in the task of definition extraction. This paper describes our system that participated in two of the three subtasks of Task 6 at SemEval 2020 (DeftEval), a shared task focused on definition extraction from a specialised corpus. Our method employs state-of-the-art neural architectures in combination with automatic methods which extend and clean the provided dataset. The remaining parts of this paper are structure"
2020.semeval-1.94,D14-1181,0,0.0181281,"Missing"
2020.semeval-1.94,N16-1030,0,0.0494826,"Missing"
2020.semeval-1.94,W04-1807,0,0.0775387,"Missing"
2020.semeval-1.94,R19-1116,1,0.758829,"nce classification task, we experimented with three different neural architectures: Convolutional Neural Network (CNN) (Kim, 2014), Recurrent Neural Network (RNN) (Cui et al., 2018) and Transformer (Devlin et al., 2018). After running various configurations, we found the Transformer architecture to perform best. With the introduction of BERT (Devlin et al., 2018) transformer architectures have shown a massive success in a wide range of NLP tasks. Transformer architectures have been trained on general tasks like language modelling and then fine-tuned for classification tasks (Sun et al., 2019; Ranasinghe et al., 2019b). Transformer models take an input of a sequence and output the representation of the sequence. The sequence has one or two segments that the first token of the sequence is always [CLS] which contains the special classification embedding and another special token [SEP] is used for separating segments. For text classification tasks, transformer models take the final hidden state h of the first token [CLS] as the representation of the whole sequence (Sun et al., 2019). The [CLS] token was then fed in to a simple softmax classifier to predict the label of the whole sentence: whether it contains"
2020.semeval-1.94,D19-1410,0,0.0128115,"ended data from Wikipedia was not useful, however, for a wider approach with higher recall, this could be more helpful. We also tried to participate in the final subtask, Relation Classification. However, due to time constraints, we were not able to achieve a valid submission for the this subtask. We approached it as a sequence pair classification task and employed a Siamese Neural Network which was shown to perform well in sequence pair classification tasks (Mueller and Thyagarajan, 2016; Ranasinghe et al., 2019a). The 721 architecture we employed is similar to the architecture presented in (Reimers and Gurevych, 2019). When two sequences have a relation, we extracted the sequences and provided them as the input for the Siamese transformer architecture. Then we used the objective function suggested as classification objective function in (Reimers and Gurevych, 2019) and optimised the cross-entropy loss. Due to the complexity of this task, we managed to run only a baseline of the proposed architecture which achieved very low evaluation scores on the development data. Therefore, we did not have a submission for this task and do not present any results here. In future, we hope to carry out further experiments"
2020.semeval-1.94,W19-4015,0,0.385007,"nguage independent and test it with decision trees and Random Forest, as well as Na¨ıve Bayes, k-Nearest Neighbour and Support Vector Machines using different sampling techniques to varying degrees of success. Kobyli´nski and Przepi´orkowski (2008) process Polish texts and use Balanced Random Forests, which bootstrap equal sets of positive and negative training examples to the classifier, as opposed to a larger group of unequal sets of training examples. Overall, while the approach is said to increase run time, it does bring minor increases in performance with some fine-tuning. Most recently, Spala et al. (2019) have created DEFT, a corpus for definition extraction from unstructured and semi-structured texts. Citing some of the pattern-based approaches also mentioned here, the authors argue that definitions have been well-defined and not necessarily representative of natural language. Therefore, a new corpus is presented that is said to more accurately represent natural language, and includes more messy examples of definitions. Parts of the DEFT corpus make up the dataset for this shared task, which is described in more detail in the following section. 3 Subtasks and Dataset The DeftEval shared task"
2020.wmt-1.122,2020.acl-main.747,0,0.135208,"Missing"
2020.wmt-1.122,2020.tacl-1.35,0,0.097489,"Missing"
2020.wmt-1.122,W19-5401,0,0.108216,"Missing"
2020.wmt-1.122,C18-1266,0,0.139288,"age pairs according to the WMT 2020 official results. 1 Introduction The goal of quality estimation (QE) systems is to determine the quality of a translation without having access to a reference translation. This makes it very useful in translation workflows where it can be used to determine whether an automatically translated sentence is good enough to be used for a given purpose, or if it needs to be shown to a human translator for translation from scratch or postediting (Kepler et al., 2019). Quality estimation can be done at different levels: document level, sentence level and word level (Ive et al., 2018). This paper presents TransQuest, a sentence-level quality estimation framework which is the winning solution in all the language pairs in the WMT 2020 Sentence-Level Direct Assessment shared task (Specia et al., 2020). In the past, high preforming quality estimation systems such as QuEst (Specia et al., 2013) and QuEst++ (Specia et al., 2015) were heavily dependent on linguistic processing and feature engineering. These features were fed into traditional machine-learning algorithms like support vector regression and randomised decision trees (Specia et al., 2013), which then determined the qu"
2020.wmt-1.122,P19-3020,0,0.154459,"Missing"
2020.wmt-1.122,W17-4763,0,0.22913,"ng and feature engineering. These features were fed into traditional machine-learning algorithms like support vector regression and randomised decision trees (Specia et al., 2013), which then determined the quality of a translation. Even though, these approaches provide good results, they are no longer the state of the art, being replaced in recent years by neural-based QE systems which usually rely on little or no linguistic processing. For example the best-performing system at the WMT 2017 shared task on QE was POSTECH, which is purely neural and does not rely on feature engineering at all (Kim et al., 2017). In order to achieve high results, approaches such as POSTECH require extensive pre-training, which means they depend on large parallel data and are computationally intensive (Ive et al., 2018). TransQuest, our QE framework removes this dependency on large parallel data by using crosslingual embeddings (Ranasinghe et al., 2020) that are already fine-tuned to reflect properties between languages (Ruder et al., 2019). Ranasinghe et al. (2020) show that by using them, TransQuest eases the burden of having complex neural network architectures, which in turn entails a reduction of the computationa"
2020.wmt-1.122,R19-1116,1,0.815984,"sed mean-squared-error loss as the objective function (Ranasinghe et al., 2020). Similar to Ranasinghe et al. (2020), the early experiments we carried out demonstrated that the CLS-strategy leads to better results than the other two strategies for this architecture. Therefore, we used the embedding of the [CLS] token as the input of a softmax layer. 2. SiameseTransQuest (STransQuest): The second approach proposed in TransQuest relies on the Siamese architecture depicted in Figure 1b which has shown promising results in monolingual semantic textual similarity tasks (Reimers and Gurevych, 2019; Ranasinghe et al., 2019). For this, we fed the original text and the translation into two separate XLM-R transformer models. Similarly to the previous architecture, we experimented with the same three pooling strategies for the outputs of the transformer models (Ranasinghe et al., 2020). TransQuest then calculates the cosine similarity between the two outputs of the pooling strategy. TransQuest used mean-squared-error loss as the objective function. Similar to Ranasinghe et al. (2020) in the initial experiments we carried out with this architecture the MEAN-strategy showed better results than the other two strategies"
2020.wmt-1.122,2020.coling-main.445,1,0.858605,"t, being replaced in recent years by neural-based QE systems which usually rely on little or no linguistic processing. For example the best-performing system at the WMT 2017 shared task on QE was POSTECH, which is purely neural and does not rely on feature engineering at all (Kim et al., 2017). In order to achieve high results, approaches such as POSTECH require extensive pre-training, which means they depend on large parallel data and are computationally intensive (Ive et al., 2018). TransQuest, our QE framework removes this dependency on large parallel data by using crosslingual embeddings (Ranasinghe et al., 2020) that are already fine-tuned to reflect properties between languages (Ruder et al., 2019). Ranasinghe et al. (2020) show that by using them, TransQuest eases the burden of having complex neural network architectures, which in turn entails a reduction of the computational resources. That paper also shows that TransQuest performs well in transfer learning settings where it can be trained on language pairs for which we have resources and applied successfully on less resourced language pairs. The remainder of the paper is structured as follows. The dataset used in the competition is briefly discus"
2020.wmt-1.122,D19-1410,0,0.0257331,"he translation. TransQuest used mean-squared-error loss as the objective function (Ranasinghe et al., 2020). Similar to Ranasinghe et al. (2020), the early experiments we carried out demonstrated that the CLS-strategy leads to better results than the other two strategies for this architecture. Therefore, we used the embedding of the [CLS] token as the input of a softmax layer. 2. SiameseTransQuest (STransQuest): The second approach proposed in TransQuest relies on the Siamese architecture depicted in Figure 1b which has shown promising results in monolingual semantic textual similarity tasks (Reimers and Gurevych, 2019; Ranasinghe et al., 2019). For this, we fed the original text and the translation into two separate XLM-R transformer models. Similarly to the previous architecture, we experimented with the same three pooling strategies for the outputs of the transformer models (Ranasinghe et al., 2020). TransQuest then calculates the cosine similarity between the two outputs of the pooling strategy. TransQuest used mean-squared-error loss as the objective function. Similar to Ranasinghe et al. (2020) in the initial experiments we carried out with this architecture the MEAN-strategy showed better results tha"
2020.wmt-1.122,2020.wmt-1.79,0,0.191132,"Missing"
2020.wmt-1.122,P15-4020,0,0.270513,"ood enough to be used for a given purpose, or if it needs to be shown to a human translator for translation from scratch or postediting (Kepler et al., 2019). Quality estimation can be done at different levels: document level, sentence level and word level (Ive et al., 2018). This paper presents TransQuest, a sentence-level quality estimation framework which is the winning solution in all the language pairs in the WMT 2020 Sentence-Level Direct Assessment shared task (Specia et al., 2020). In the past, high preforming quality estimation systems such as QuEst (Specia et al., 2013) and QuEst++ (Specia et al., 2015) were heavily dependent on linguistic processing and feature engineering. These features were fed into traditional machine-learning algorithms like support vector regression and randomised decision trees (Specia et al., 2013), which then determined the quality of a translation. Even though, these approaches provide good results, they are no longer the state of the art, being replaced in recent years by neural-based QE systems which usually rely on little or no linguistic processing. For example the best-performing system at the WMT 2017 shared task on QE was POSTECH, which is purely neural and"
2020.wmt-1.122,P13-4014,0,0.2828,"Missing"
2021.acl-short.55,N19-1388,0,0.0223633,"et al., 2020) where a simple linear layer is added on top of the transformer model to obtain the predictions (Lee, 2020). All of these approaches consider quality estimation as a language-specific task and build a different model for each language pair. This approach has many drawbacks in real-world applications, some of which are discussed in Section 1. Multilinguality Multilinguality allows training a single model to perform a task from and/or to multiple languages. Even though this has been applied to many tasks (Ranasinghe and Zampieri, 2020, 2021) including NMT (Nguyen and Chiang, 2017; Aharoni et al., 2019), multilingual approaches have been rarely used in QE (Sun et al., 2020). Shah and Specia (2016) explore QE models for more than one language where they use multitask learning with annotators or languages as multiple tasks. They show that multilingual models led to marginal improvements over bilingual ones with a traditional black-box, feature-based approach. In a recent study, Ranasinghe et al. (2020b) show that multilingual QE models based on transformers trained on high-resource languages can be used for zeroshot, sentence-level QE in low-resource languages. In a similar architecture, but w"
2021.acl-short.55,2020.acl-main.747,0,0.163753,"Missing"
2021.acl-short.55,N19-1423,0,0.0239418,"Estimation Early approaches in wordlevel QE were based on features fed into a traditional machine learning algorithm. Systems like QuEst++ (Specia et al., 2015) and MARMOT (Logacheva et al., 2016) were based on features used with Conditional Random Fields to perform wordlevel QE. With deep learning models becoming popular, the next generation of word-level QE algorithms were based on bilingual word embeddings fed into deep neural networks. Such approaches can be found in OpenKiwi (Kepler et al., 2019). However, the current state of the art in word-level QE is based on transformers like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) where a simple linear layer is added on top of the transformer model to obtain the predictions (Lee, 2020). All of these approaches consider quality estimation as a language-specific task and build a different model for each language pair. This approach has many drawbacks in real-world applications, some of which are discussed in Section 1. Multilinguality Multilinguality allows training a single model to perform a task from and/or to multiple languages. Even though this has been applied to many tasks (Ranasinghe and Zampieri, 2020, 2021) including NMT (Nguyen"
2021.acl-short.55,2020.wmt-1.116,0,0.0389242,"Missing"
2021.acl-short.55,L16-1582,0,0.0237408,"odels as part of an open-source framework1 . 1 Documentation is available on http://tharindu. co.uk/TransQuest/ 434 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 434–440 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Model Architecture 2 Related Work Quality Estimation Early approaches in wordlevel QE were based on features fed into a traditional machine learning algorithm. Systems like QuEst++ (Specia et al., 2015) and MARMOT (Logacheva et al., 2016) were based on features used with Conditional Random Fields to perform wordlevel QE. With deep learning models becoming popular, the next generation of word-level QE algorithms were based on bilingual word embeddings fed into deep neural networks. Such approaches can be found in OpenKiwi (Kepler et al., 2019). However, the current state of the art in word-level QE is based on transformers like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) where a simple linear layer is added on top of the transformer model to obtain the predictions (Lee, 2020). All of these approaches consider qu"
2021.acl-short.55,I17-2050,0,0.018743,"2019) and XLM-R (Conneau et al., 2020) where a simple linear layer is added on top of the transformer model to obtain the predictions (Lee, 2020). All of these approaches consider quality estimation as a language-specific task and build a different model for each language pair. This approach has many drawbacks in real-world applications, some of which are discussed in Section 1. Multilinguality Multilinguality allows training a single model to perform a task from and/or to multiple languages. Even though this has been applied to many tasks (Ranasinghe and Zampieri, 2020, 2021) including NMT (Nguyen and Chiang, 2017; Aharoni et al., 2019), multilingual approaches have been rarely used in QE (Sun et al., 2020). Shah and Specia (2016) explore QE models for more than one language where they use multitask learning with annotators or languages as multiple tasks. They show that multilingual models led to marginal improvements over bilingual ones with a traditional black-box, feature-based approach. In a recent study, Ranasinghe et al. (2020b) show that multilingual QE models based on transformers trained on high-resource languages can be used for zeroshot, sentence-level QE in low-resource languages. In a simi"
2021.acl-short.55,2020.wmt-1.122,1,0.950933,"which they have to focus more. Word-level QE is generally framed as a supervised ML problem (Kepler et al., 2019; Lee, 2020) trained on data in which the correctness of translation is labelled at word-level (i.e. good, bad, gap). The training data publicly available to build wordlevel QE models is limited to very few language pairs, which makes it difficult to build QE models for many languages. From an application perspective, even for the languages with resources, it is difficult to maintain separate QE models for each language since the state-of-the-art neural QE models are large in size (Ranasinghe et al., 2020b). In our paper, we address this problem by developing multilingual word-level QE models which perform competitively in different domains, MT types and language pairs. In addition, for the first time, we propose word-level QE as a zero-shot crosslingual transfer task, enabling new avenues of research in which multilingual models can be trained once and then serve a multitude of languages and domains. The main contributions of this paper are the following: i We introduce a simple architecture to perform word-level quality estimation that predicts the quality of the words in the source sentence"
2021.acl-short.55,2020.coling-main.445,1,0.951303,"which they have to focus more. Word-level QE is generally framed as a supervised ML problem (Kepler et al., 2019; Lee, 2020) trained on data in which the correctness of translation is labelled at word-level (i.e. good, bad, gap). The training data publicly available to build wordlevel QE models is limited to very few language pairs, which makes it difficult to build QE models for many languages. From an application perspective, even for the languages with resources, it is difficult to maintain separate QE models for each language since the state-of-the-art neural QE models are large in size (Ranasinghe et al., 2020b). In our paper, we address this problem by developing multilingual word-level QE models which perform competitively in different domains, MT types and language pairs. In addition, for the first time, we propose word-level QE as a zero-shot crosslingual transfer task, enabling new avenues of research in which multilingual models can be trained once and then serve a multitude of languages and domains. The main contributions of this paper are the following: i We introduce a simple architecture to perform word-level quality estimation that predicts the quality of the words in the source sentence"
2021.acl-short.55,2020.emnlp-main.470,1,0.798925,"Missing"
2021.acl-short.55,C18-1266,0,0.0152413,"e to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios. 1 Introduction Quality Estimation (QE) is the task of assessing the quality of a translation without having access to a reference translation (Specia et al., 2009). Translation quality can be estimated at different levels of granularity: word, sentence and document level (Ive et al., 2018). So far the most popular task has been sentence-level QE (Specia et al., 2020), in which QE models provide a score for each pair of source and target sentences. A more challenging task, which is currently receiving a lot of attention from the research community, is word-level quality estimation. This task provides more fine-grained information about the quality of a translation, indicating which words from the source have been incorrectly translated in the target, and whether the words inserted between these words are correct (good vs bad gaps). This information can be useful for post-editors"
2021.acl-short.55,2021.naacl-demos.17,1,0.828743,"Missing"
2021.acl-short.55,P19-3020,0,0.0328944,"Missing"
2021.acl-short.55,2020.wmt-1.118,0,0.632328,"nces. A more challenging task, which is currently receiving a lot of attention from the research community, is word-level quality estimation. This task provides more fine-grained information about the quality of a translation, indicating which words from the source have been incorrectly translated in the target, and whether the words inserted between these words are correct (good vs bad gaps). This information can be useful for post-editors by indicating the parts of a sentence on which they have to focus more. Word-level QE is generally framed as a supervised ML problem (Kepler et al., 2019; Lee, 2020) trained on data in which the correctness of translation is labelled at word-level (i.e. good, bad, gap). The training data publicly available to build wordlevel QE models is limited to very few language pairs, which makes it difficult to build QE models for many languages. From an application perspective, even for the languages with resources, it is difficult to maintain separate QE models for each language since the state-of-the-art neural QE models are large in size (Ranasinghe et al., 2020b). In our paper, we address this problem by developing multilingual word-level QE models which perfor"
2021.acl-short.55,P15-4020,0,0.0303708,"se the code and the pre-trained models as part of an open-source framework1 . 1 Documentation is available on http://tharindu. co.uk/TransQuest/ 434 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 434–440 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Model Architecture 2 Related Work Quality Estimation Early approaches in wordlevel QE were based on features fed into a traditional machine learning algorithm. Systems like QuEst++ (Specia et al., 2015) and MARMOT (Logacheva et al., 2016) were based on features used with Conditional Random Fields to perform wordlevel QE. With deep learning models becoming popular, the next generation of word-level QE algorithms were based on bilingual word embeddings fed into deep neural networks. Such approaches can be found in OpenKiwi (Kepler et al., 2019). However, the current state of the art in word-level QE is based on transformers like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) where a simple linear layer is added on top of the transformer model to obtain the predictions (Lee, 2020)."
2021.acl-short.55,2009.eamt-1.5,0,0.0577206,"form on par with the current language-specific models. In the cases of zeroshot and few-shot QE, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios. 1 Introduction Quality Estimation (QE) is the task of assessing the quality of a translation without having access to a reference translation (Specia et al., 2009). Translation quality can be estimated at different levels of granularity: word, sentence and document level (Ive et al., 2018). So far the most popular task has been sentence-level QE (Specia et al., 2020), in which QE models provide a score for each pair of source and target sentences. A more challenging task, which is currently receiving a lot of attention from the research community, is word-level quality estimation. This task provides more fine-grained information about the quality of a translation, indicating which words from the source have been incorrectly translated in the target, and"
2021.acl-short.55,2020.aacl-main.39,0,0.059407,"Missing"
2021.latechclfl-1.12,J19-2006,0,0.0477863,"Missing"
2021.latechclfl-1.12,W15-2119,0,0.0240671,"ombinations. The features include morphological forms (past tense, passive voice form, etc.), syntactic features (e.g. number of clauses per sentence, sentence length), word classes (e.g. types of pronominal function words, adverbial quantifiers), and dependency relations (e.g. adjectival clause, clausal complement). Besides, we include two features reflecting sentence complexity: mean hierarchical 104 3 We provide the full list of features in Appendix A. Figure 1: Distribution of chunk sizes (in tokens) in each subcorpus after lemmatisation distance and mean dependency distance (described in Jing and Liu, 2015) and two features reflecting richness of vocabulary - type-to-token ratio and lexical density. The normalisation basis varies depending on the type of feature as recommended in (Evert and Neumann, 2017): total tokens for word classes; number of sentences for conjunctions and modal predicates; total verbs for verb forms; total number of dependencies in the sentence for the selected types of dependencies. is six (book-aware 6-fold cross-validation). In the multiclass scenario, we used standard 10-fold crossvalidation instead. 3.3 For all ML experiments below we report accuracy and macro F1-score"
2021.latechclfl-1.12,2020.lrec-1.505,1,0.843499,"Missing"
2021.latechclfl-1.12,2020.coling-main.532,0,0.0678354,"Missing"
2021.latechclfl-1.12,C12-2076,0,0.0196897,"Missing"
2021.latechclfl-1.12,R11-1091,0,0.0754877,"Missing"
2021.latechclfl-1.12,P17-1049,0,0.0279663,"Missing"
2021.latechclfl-1.12,Q15-1030,0,0.0501831,"Missing"
2021.latechclfl-1.12,N16-1110,1,0.896664,"Missing"
2021.latechclfl-1.12,R19-1130,0,0.027627,"Missing"
2021.latechclfl-1.12,K17-3009,0,0.0130249,"rva and Petroni, 2008). Figure 1 also shows a subcorpus of non-translations (marked as Russian). It was built following the sampling and chunking principles described above from the monolingual part of the RNC. All translational subcorpora are in one-sentenceper-line format. We discarded by-lines and headings, such as “Chapter 5”, “Jane Eyre” and “Charlotte Bronte”, empty lines and lines without alpha, including cases where the absence of text was marked with “—”. The 11 translational subcorpora and non-translations were annotated within Universal Dependencies (UD) framework using UDPipe (v1, Straka and Straková, 2017).2 The chunks in the resulting conllu format were used as input to our feature extraction module. 3.2 lationese detection. In a previous study based on Russian, these structural features were shown to work much better than lexical features (such as ngram ranks and PMI scores, see Kunilovskaya and Corpas Pastor, 2021). Of several dozens of UD relations, we use the subset of seven relations that was shown to perform well for translationese detection in English-to-Russian mass-media texts (Kunilovskaya and Kutuzov, 2018). We also included features that are susceptible to change in translation, ac"
barbu-etal-2002-corpus,C96-1021,0,\N,Missing
barbu-etal-2002-corpus,W01-0716,1,\N,Missing
barbu-etal-2002-corpus,W97-1306,0,\N,Missing
barbu-etal-2002-corpus,J94-4002,0,\N,Missing
barbu-etal-2002-corpus,P01-1006,1,\N,Missing
barbu-etal-2002-corpus,P98-2143,1,\N,Missing
barbu-etal-2002-corpus,C98-2138,1,\N,Missing
C02-1027,P98-2143,1,0.920244,"A - an architecture for language processing in Bulgarian LINGUA is a text processing framework for Bulgarian which automatically performs tokenisation, sentence splitting, part-of-speech tagging, parsing, clause segmentation, sectionheading identification and resolution for third person personal pronouns (Figure 1). All modules of LINGUA are original and purposebuilt, except for the module for morphological analysis which uses Krushkov’s morphological analyser BULMORPH (Krushkov, 1997). The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998). LINGUA was used in a number of projects covering automatic text abridging, word semantic extraction (Totkov and Tanev, 1999) and term extraction. The following sections outline the basic language processing functions, provided by the language engine. 2.1 Text segmentation: tokenisation, sentence splitting and paragraph identification The first stage of every text processing task is the segmentation of text in terms of tokens, sentences and paragraphs. LINGUA performs text segmentation by operating within an input window of 30 tokens, applying rule-based algorithm for token synthesis, sentenc"
C02-1027,E95-1022,0,\N,Missing
C02-1027,C98-2138,1,\N,Missing
C12-2112,H05-1103,0,0.0511358,"Missing"
C12-2112,levy-andrew-2006-tregex,0,0.0535524,"Missing"
C12-2112,W03-0203,1,0.839805,"Missing"
C94-2191,P93-1021,0,0.0357687,"Missing"
C94-2191,P87-1022,0,0.418412,"Missing"
C94-2191,C88-1021,0,0.705933,"Missing"
C94-2191,J86-3001,0,0.0643405,"Missing"
C94-2191,A88-1003,0,0.777915,"Missing"
C94-2191,P89-1031,0,0.0499352,"Missing"
C94-2191,J81-2001,0,\N,Missing
C94-2191,J81-4001,0,\N,Missing
C94-2191,P89-1032,0,\N,Missing
C98-2138,C90-3063,0,0.939653,"Missing"
C98-2138,P89-1032,0,0.0648748,"Missing"
C98-2138,J94-4002,0,0.767113,"ack of syntactic information, for instance, means giving up c-command constraints and subject preference (or on other occasions object preference, see Mitkov 1995) which could be used in center tracking. Syntactic parallelism, useful in discriminating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb semantics and semantic parallelism. Our evaluation, however, suggests that much less is lost than might be feared. In fact, our evaluation shows that the results are comparable to syntax-based methods (Lappin & Leass 1994). We believe that the good success rate is due to the fact that a number of antecedent indicators are taken into account and no factor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith 1998) or syntactic and semantic parallelism preferences (see below). 3.1 Evaluation A Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-an"
C98-2138,A88-1003,0,0.939872,"Missing"
C98-2138,J90-4001,0,\N,Missing
C98-2138,C96-1021,0,\N,Missing
C98-2138,C88-1021,0,\N,Missing
C98-2138,C94-2191,1,\N,Missing
C98-2138,W97-1306,0,\N,Missing
C98-2138,P87-1022,0,\N,Missing
D18-1528,P16-2094,0,0.0313724,"ically significant difference. In addition, examining the discriminatory power of specific gaze features provides valuable information about the human processing of the pronoun. We make our data, code and annotation available at https: //github.com/victoria-ianeva/ It-Classification. The GECO eye-tracking corpus is available at http: //expsy.ugent.be/downloads/geco/. 2 Related Work Gaze data in NLP While eye-movement data has been traditionally used to gain understanding of the cognitive processing of text, it was recently applied to a number of technical tasks such as part-of-speech tagging (Barrett et al., 2016), detection of multi-word expressions (Rohanian et al., 2017; Yaneva et al., 2017), sentence compression (Klerke et al., 2016), complex-word identification ˇ (Stajner et al., 2017), and sentiment analysis (Rotsztejn, 2018). Eye movements were also shown to carry valuable information about the reader and were used to detect specific conditions affecting reading such as autism (Yaneva et al., 2018, 2016) and dyslexia (Rello and Ballesteros, 2015). The motivation behind these approaches is two-fold. First, eye tracking is already making its way into everyday use with interfaces and devices that f"
D18-1528,W05-0406,0,0.711157,"Missing"
D18-1528,guillou-etal-2014-parcor,0,0.0334567,"Missing"
D18-1528,N16-1179,0,0.0700178,"rmation about the human processing of the pronoun. We make our data, code and annotation available at https: //github.com/victoria-ianeva/ It-Classification. The GECO eye-tracking corpus is available at http: //expsy.ugent.be/downloads/geco/. 2 Related Work Gaze data in NLP While eye-movement data has been traditionally used to gain understanding of the cognitive processing of text, it was recently applied to a number of technical tasks such as part-of-speech tagging (Barrett et al., 2016), detection of multi-word expressions (Rohanian et al., 2017; Yaneva et al., 2017), sentence compression (Klerke et al., 2016), complex-word identification ˇ (Stajner et al., 2017), and sentiment analysis (Rotsztejn, 2018). Eye movements were also shown to carry valuable information about the reader and were used to detect specific conditions affecting reading such as autism (Yaneva et al., 2018, 2016) and dyslexia (Rello and Ballesteros, 2015). The motivation behind these approaches is two-fold. First, eye tracking is already making its way into everyday use with interfaces and devices that feature eye-tracking navigation (e.g. Windows Eye Control3 ). Second, linguistic annotation by gaze is faster than traditional"
D18-1528,P16-3020,0,0.361344,"the automatic classification of it. We show that by using gaze data and a POS-tagger we are able to significantly outperform a common baseline and classify between three categories of it with an accuracy comparable to that of linguisticbased approaches. In addition, the discriminatory power of specific gaze features informs the way humans process the pronoun, which, to the best of our knowledge, has not been explored using data from a natural reading task. 1 Introduction Anaphora resolution is both one of the most important and one of the least developed tasks in Natural Language Processing (Lee et al., 2016). A particularly difficult case for anaphora resolution systems is the pronoun it, as it may refer to a specific noun phrase or an entire clause, or it may even refer to nothing at all, as in sentences 1 - 3 below1 . 1. “I couldnt say exactly, sir, but it wasnt teatime by a long way.” (Pleonastic it (nonreferential)). 2. Now, as to this quarrel. When was the first time you heard of it? (Nominal anaphoric) 3. You have been with your mistress many years, is it not so? (Clause anaphoric2 .) 1 Extracted from the GECO corpus (Cop et al., 2016) Some authors also distinguish other, less-common types"
D18-1528,D17-1137,0,0.151473,"Missing"
D18-1528,E06-1007,0,0.0995085,"Missing"
D18-1528,rohanian-etal-2017-using,1,0.739164,"scriminatory power of specific gaze features provides valuable information about the human processing of the pronoun. We make our data, code and annotation available at https: //github.com/victoria-ianeva/ It-Classification. The GECO eye-tracking corpus is available at http: //expsy.ugent.be/downloads/geco/. 2 Related Work Gaze data in NLP While eye-movement data has been traditionally used to gain understanding of the cognitive processing of text, it was recently applied to a number of technical tasks such as part-of-speech tagging (Barrett et al., 2016), detection of multi-word expressions (Rohanian et al., 2017; Yaneva et al., 2017), sentence compression (Klerke et al., 2016), complex-word identification ˇ (Stajner et al., 2017), and sentiment analysis (Rotsztejn, 2018). Eye movements were also shown to carry valuable information about the reader and were used to detect specific conditions affecting reading such as autism (Yaneva et al., 2018, 2016) and dyslexia (Rello and Ballesteros, 2015). The motivation behind these approaches is two-fold. First, eye tracking is already making its way into everyday use with interfaces and devices that feature eye-tracking navigation (e.g. Windows Eye Control3 )."
D18-1528,W17-5030,1,0.81727,"Missing"
D18-1528,L16-1077,1,0.898675,"Missing"
E03-1066,W00-0408,0,0.110325,"Missing"
E03-1066,J95-2003,0,0.0669656,"tences&apos; score in order to be considered important Indicating phrases: Paice (1981) noticed that it is possible to identify phrases which can be used to assess the importance of a sentence. The list of indicating phrases can be loaded, saved and modified in the tool. Surface clues: Several factors such as sentence location and length can be taken into consideration to decide the importance of the sentence. Lexical cohesion: Lexical cohesion as proposed in (Hoey, 1991) is used to produce extracts. Discourse information: Our own summarisation method uses information provided by Centering Theory (Grosz et al., 1995) to produce extracts. The automatic methods are used not only to identify important sentences, but also to remove sentences which do not contain important information. For example, it is possible to remove sentences which contain certain indicating phrases or have a TF-IDF score lower than a given threshold. As in the case of important sentences, the user can review the system&apos;s decisions. In order to offer maximum portability CAST, is written in Java, its input being XML. We decided not to include any preprocessing module in CAST (e.g. sentence splitter, PoS tagger, etc.), so all the necessar"
E03-1066,narita-2000-constructing,0,0.088708,"information overload by reducing it. At present the most common type of summarised information is textual information, but unfortunately the quality of the automatic summaries is not of a very high level. Apart from a working paper of ours in the mid 90s (Mitkov, 1995), the only relevant research we could find in this field is that of Craven (Craven, 1996). However, Craven&apos;s approach takes a rather simplistic view because it uses only methods which extract keywords from the text and not complete sentences or even phrases. Another tool which aids humans in producing summaries is presented in (Narita, 2000). This tool does not employ any automatic methods to help humans, but gives them the option to access a corpus of human produced abstracts which can function as templates, providing grammatical patterns and collocations common to abstracts. Endres-Niggemeyer (Endres-Niggemeyer, 1998) identifies three stages in human summarisation: document exploration, relevance assessment and summary production. In the first &apos;This figure includes books, newspapers, scholarly journals, office documents, etc. 135 two stages the summariser identifies the overall structure of the text and the main topics, whereas"
E03-1066,C96-2166,0,0.225137,"ght important sentences in the text. These sentences can then be saved as gold standard. The tool can also be used to teach students about summarisation methods. As the tool incorporates several methods, they can be run on the same text, making it possible to compare results. All these methods are highly customisable and the tool enables us to see the influence of different parameters on them. As aforementioned, the tool relies on several automatic methods to identify the important sentences. At present, these methods are: Keyword method: Uses TF-IDF scores to weight sentences as proposed in (Zechner, 1996). The user can modify the list of terms and indicate thresholds for sentences&apos; score in order to be considered important Indicating phrases: Paice (1981) noticed that it is possible to identify phrases which can be used to assess the importance of a sentence. The list of indicating phrases can be loaded, saved and modified in the tool. Surface clues: Several factors such as sentence location and length can be taken into consideration to decide the importance of the sentence. Lexical cohesion: Lexical cohesion as proposed in (Hoey, 1991) is used to produce extracts. Discourse information: Our o"
E12-1072,J08-4004,0,0.0302455,"Missing"
E12-1072,P08-1002,0,0.0342287,"Missing"
E12-1072,W05-0406,0,0.148805,"o siguiente. (It) will be what is established in the next section. 1 All the examples provided are taken from our corpus. In the examples, explicit subjects are presented in italics. Zero subjects are presented by the symbol Ø and in the English translations the subjects which are elided in Spanish are marked with parentheses. Impersonal constructions are not explicitly indicated. 3 Related Work Identification of non-referential pronouns, although a crucial step in co-reference and anaphora resolution systems (Mitkov, 2010),2 has been applied only to the pleonastic it in English (Evans, 2001; Boyd et al., 2005; Bergsma et al., 2008) and expletive pronouns in French (Danlos, 2005). Machine learning methods are known to perform better than rule-based techniques for identifying non-referential expressions (Boyd et al., 2005). However, there is some debate as to which approach may be optimal in anaphora resolution systems (Mitkov and Hallett, 2007). Both English and French texts use an explicit word, with some grammatical information (a third person pronoun), which is non-referential (Mitkov, 2010). By contrast, in Spanish, nonreferential expressions are not realized by expletive or pleonastic pronouns"
E12-1072,I05-2013,0,0.0766859,"Missing"
E12-1072,P00-1022,0,0.465731,"Missing"
E12-1072,E06-1007,0,0.0602231,"Missing"
E12-1072,C02-1139,0,0.138111,"vy, 2009). The main contributions of this study are: • A public annotated corpus in Spanish to compare different strategies for detecting explicit subjects, zero subjects and impersonal constructions. Introduction Subject ellipsis is the omission of the subject in a sentence. We consider not only missing referential subject (zero subject) as manifestation of ellipsis, but also non-referential impersonal constructions. Various natural language processing (NLP) tasks benefit from the identification of elliptical subjects, primarily anaphora resolution (Mitkov, 2002) and co-reference resolution (Ng and Cardie, 2002). The difficulty in detecting missing subjects and non-referential pronouns has been acknowledged since the first studies on ∗ This work was partially funded by a ‘La Caixa’ grant for master students. Ruslan Mitkov Research Group in Computational Linguistics Univ. of Wolverhampton, UK • The first ML based approach to this problem in Spanish and a thorough analysis regarding features, learnability, genre and errors. • The best performing algorithms to automatically detect explicit subjects and impersonal constructions in Spanish. The remainder of the paper is organized as follows. Section 2 des"
E12-1072,R09-2011,1,0.906825,"e 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 706–715, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics genre impact and the error analysis are all detailed in Section 6. Finally, in Section 7, conclusions are drawn and plans for future work are discussed. This work is an extension of the first author master’s thesis (Rello, 2010) and a preliminary version of the algorithm was presented in Rello et al. (2010). 2 Classes of Spanish Subjects Literature related to ellipsis in NLP (Ferr´andez and Peral, 2000; Rello and Illisei, 2009a; Mitkov, 2010) and linguistic theory (Bosque, 1989; Brucart, 1999; Real Academia Espa˜nola, 2009) has served as a basis for establishing the classes of this work. Explicit subjects are phonetically realized and their syntactic position can be pre-verbal or postverbal. In the case of post-verbal subjects (a), the syntactic position is restricted by some conditions (Real Academia Espa˜nola, 2009). (a) Carecer´an de validez las disposiciones que contradigan otra de rango superior.1 The dispositions which contradict higher range ones will not be valid. Zero subjects (b) appear as the result of a"
E12-1072,A97-1011,0,0.123824,"Missing"
E12-1072,D07-1057,0,0.165909,"Missing"
ha-etal-2008-mutual,C94-2167,0,\N,Missing
ha-etal-2008-mutual,C94-1084,0,\N,Missing
ha-etal-2008-mutual,J00-2004,0,\N,Missing
ha-etal-2008-mutual,P98-1074,0,\N,Missing
ha-etal-2008-mutual,C98-1071,0,\N,Missing
ha-etal-2008-mutual,W03-0305,0,\N,Missing
J01-4001,W97-1306,0,0.0206517,"ed for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain a n d / o r linguistic knowledge and reported promising results in knowledge-poor operational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; Mitkov 1996, 1998b). The drive toward knowledge-poor and robust approaches was further motivated by the emergence of cheaper and more reliable corpus-based NLP tools such as partof-speech taggers and shallow parsers, alongside the increasing availability of corpora and other NLP resources (e.g., ontologies). In fact, the availability of corpora, both raw and annotated with coreferential links, provided a strong impetus to anaphora resolu* School of Humanities, Language and Social Sciences, Stafford Street, Wolverhampton WV1 1SB, UK. E-maih r.mitkov@wlv.ac.uk t 30 Saw Mill River Road, Hawthor"
J01-4001,M95-1015,0,0.0607149,"Missing"
J01-4001,P87-1022,0,0.280699,"Missing"
J01-4001,W98-1119,0,0.0448818,"Missing"
J01-4001,P97-1014,0,0.0443166,"Missing"
J01-4001,A00-1020,0,0.0586317,"Missing"
J01-4001,W97-1307,0,0.0407624,"Missing"
J01-4001,W97-0319,0,0.039376,"Missing"
J01-4001,C96-1021,1,0.814809,"ich required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain a n d / o r linguistic knowledge and reported promising results in knowledge-poor operational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; Mitkov 1996, 1998b). The drive toward knowledge-poor and robust approaches was further motivated by the emergence of cheaper and more reliable corpus-based NLP tools such as partof-speech taggers and shallow parsers, alongside the increasing availability of corpora and other NLP resources (e.g., ontologies). In fact, the availability of corpora, both raw and annotated with coreferential links, provided a strong impetus to anaphora resolu* School of Humanities, Language and Social Sciences, Stafford Street, Wolverhampton WV1 1SB, UK. E-maih r."
J01-4001,J94-4002,1,0.791556,"th to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain a n d / o r linguistic knowledge and reported promising results in knowledge-poor operational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; Mitkov 1996, 1998b). The drive toward knowledge-poor and robust approaches was further motivated by the emergence of cheaper and more reliable corpus-based NLP tools such as partof-speech taggers and shallow parsers, alongside the increasing availability of corpora and other NLP resources (e.g., ontologies). In fact, the availability of corpora, both raw and annotated with coreferential links, provided a strong impetus to anaphora resolu* School of Humanities, Language and Social Sciences, Stafford St"
J01-4001,P98-2143,1,0.913522,"Missing"
J01-4001,W98-1502,1,0.840212,"Missing"
J01-4001,C94-2189,0,0.0165787,"process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain a n d / o r linguistic knowledge and reported promising results in knowledge-poor operational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; Mitkov 1996, 1998b). The drive toward knowledge-poor and robust approaches was further motivated by the emergence of cheaper and more reliable corpus-based NLP tools such as partof-speech taggers and shallow parsers, alongside the increasing availability of corpora and other NLP resources (e.g., ontologies). In fact, the availability of corpora, both raw and annotated with coreferential links, provided a strong impetus to anaphora resolu* School of Humanities, Language and Social Sciences, Stafford Street, Wolverham"
J01-4001,A88-1003,0,0.0339572,"ourse Representation Theory and Centering Theory inspired new research on the computational treatment of anaphora. The drive toward corpus-based robust NLP solutions further stimulated interest in alternative a n d / o r data-enriched approaches. Last, but not least, application-driven research in areas such as automatic abstracting and information extraction independently highlighted the importance of anaphora and coreference resolution, boosting research in this area. Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain a n d / o r linguistic knowledge and reported promising results in knowledge-poor op"
J01-4001,P98-2204,0,0.0391486,"Missing"
J01-4001,P96-1036,0,0.0359719,"Missing"
J01-4001,P99-1079,0,0.0233287,"Missing"
J01-4001,C94-2184,0,\N,Missing
J01-4001,C88-1021,0,\N,Missing
J01-4001,C90-3063,0,\N,Missing
J01-4001,P95-1017,0,\N,Missing
J01-4001,P93-1021,0,\N,Missing
J01-4001,C98-2138,1,\N,Missing
J01-4001,C98-2199,0,\N,Missing
J01-4001,W99-0611,0,\N,Missing
J99-4007,P87-1022,0,0.138321,"Missing"
J99-4007,P83-1007,0,0.337819,"Missing"
J99-4007,J95-2003,0,0.172361,"Missing"
J99-4007,P97-1014,0,0.0326492,"algorithm (Walker). Centering has proved to be a powerful tool for accounting for discourse coherence and has been used successfully in anaphora resolution; however, as with every theory in linguistics, it has its limitations. Some chapters suggest extensions of or amendments to the centering theory with a view to achieving a more comprehensive and successful model (e.g., the chapters by Kameyama, Roberts, and Walker). Ideally, in addition to papers such as Kameyama's and Walker's, this collection could perhaps also have featured extended versions of papers, such as those of Kehler (1997) and Hahn and Strube (1997), that highlight certain weaknesses of the original centering model or suggest extensions or alternative solutions (Strube 1998). It must be acknowledged here that the production schedule of this volume may have been a factor in not including some of this recent work, and also that space limits might not have allowed all possible areas of centering to be covered. These are, however, minor points; the volume is a very well edited collection of excellent papers, and I recommend it unreservedly as a valuable source to anyone interested in centering, discourse, or computational linguistics in gene"
J99-4007,J97-3006,0,0.0146029,"ith the centering algorithm (Walker). Centering has proved to be a powerful tool for accounting for discourse coherence and has been used successfully in anaphora resolution; however, as with every theory in linguistics, it has its limitations. Some chapters suggest extensions of or amendments to the centering theory with a view to achieving a more comprehensive and successful model (e.g., the chapters by Kameyama, Roberts, and Walker). Ideally, in addition to papers such as Kameyama's and Walker's, this collection could perhaps also have featured extended versions of papers, such as those of Kehler (1997) and Hahn and Strube (1997), that highlight certain weaknesses of the original centering model or suggest extensions or alternative solutions (Strube 1998). It must be acknowledged here that the production schedule of this volume may have been a factor in not including some of this recent work, and also that space limits might not have allowed all possible areas of centering to be covered. These are, however, minor points; the volume is a very well edited collection of excellent papers, and I recommend it unreservedly as a valuable source to anyone interested in centering, discourse, or comput"
J99-4007,P98-2204,0,0.0420709,"Missing"
konstantinova-etal-2012-review,taboada-etal-2006-methods,1,\N,Missing
konstantinova-etal-2012-review,W11-0141,0,\N,Missing
konstantinova-etal-2012-review,P07-1125,0,\N,Missing
konstantinova-etal-2012-review,W10-3110,0,\N,Missing
konstantinova-etal-2012-review,R11-2022,1,\N,Missing
L16-1045,W10-1001,0,0.0789116,". We also discuss the role of Simple Wikipedia (Zhu et al., 2010) as a widely-used accessibility benchmark, in light of our finding that it is significantly more complex than both the EasyRead and the LocalNews corpora. Keywords: readability, text simplification, cognitive disabilities, autism, easy-to-read 1. Introduction The main task of automatic text simplification (TS) is to convert texts into a more understandable form for readers with lower than average reading skills, without changing the original meaning of the text. These readers could include children and people with poor literacy (Aluisio et al., 2010), learning disabilities (Feng, 2009), neurodevelopmental disorders (Rello et al., 2013), second language learners (Petersen, 2007), among others. The evaluation of TS output could be defined through the tasks of quality and accessibility (readability) estimation. Quality estimation is performed at sentence level and its purpose is to establish whether the output sentences are grammatical, meaningful and simpler. Readability estimation on the other hand is done at text level and is concerned with measuring how understandable the output is for the target reader population. In this paper we focus"
L16-1045,W03-1004,0,0.0126349,"sing the Weekly Reader corpus (Allen, 2009) as training data and then evaluating on the LocalNews corpus (Feng, 2009). LocalNews consists of 11 original and 11 simplified news stories that have been assessed in terms of their understandability by people with MID. In Section 4 we compare the complexity of this corpus to the complexity of our easy-read corpus. 2.2. Corpora Used for Text Simplification Currently the output of most text simplification systems is compared to gold standards such as the widely-used Simple English Wikipedia (Zhu et al., 2010) or Encyclopaedia Britannica for Children (Barzilay and Elhadad, 2003), where texts have been manually adapted by humans to match the reading abilities of children and contain two complexity levels:original and simplified documents (Table 1, column 4); in the cases of the Weekly Reader corpus (Allen, 2009) or Literacy Works corpus (Petersen and Ostendorf, 2007) the simplification has been done for second language learners (Table 1). Corpus Newsella SimpleWiki Britannica WeeklyReader Literacy works FIRST LocalNews Target readers L2 learners Various Children L2 learners L2 learners Autism MID Domain News Encylcl Encylcl News General News Articles 1,130 x 4 In prog"
L16-1045,C10-2032,0,0.0563868,"aders. While the latter is certainly the most reliable way to measure text accessibility and harness insights into user preferences, it is very time-consuming and expensive to perform, especially in cases where the target readers are people with cognitive disabilities (intellectual disability, autism, hyperactivity, etc.). To address this issue, research has been focusing on developing automatic methods for readability evaluation, which could account for the specific reading difficulties of various reader populations (Dubay, 2004; Benjamin, 2012), including people with cognitive disabilities (Feng et al., 2010; Yaneva and Evans, 2015). However, relying on readability formulae to measure text accessibility is an approach that has many drawbacks (Benjamin, 2012; Dubay, 2004; Siddharthan, 2004), mainly related to the fact that readability formulae only employ surface text features such as word or sentence length. Thus for example, the Flesch formula (Flesch, 1948) presented below is very widely-used but cannot take into account the difficulty autistic readers have in using abstraction (Minshew et al., 2002) or the difficulty aphasic readers have with passive voice or other syntactic constructions (Ber"
L16-1045,Q15-1021,0,0.0377213,"syRead corpus. Table 1: English-language corpora used in text simplification research While these corpora are undoubtedly very useful for developing TS systems because they contain pairs of matched original and simplified sentences, they have never been evaluated by the targeted readers (except for LocalNews, Section 2.1). The lack of evaluations casts doubt on the quality of the resource, with evidence showing that Simˇ ple Wikipedia is not actually that accessible (Stajner et al., 2012). Some researchers appeal for “the community to drop it as the standard benchmark set for simplification” (Xu et al., 2015). In Section 4 of this paper we test this further by comparing a subset of the SimpleWiki corpus to LocalNews and the EasyRead corpora. In addition, more research is needed to testify whether the quality of texts produced through manual simplification with the primary objective of developing aligned corpora is similar to the quality of accessible documents produced Easy-to-read Documents and Cognitive Disabilities 3. The EasyRead Corpus An initial collection of 372 English-language easy-to-read documents was obtained and their text complexity assessed using the Flesh Reading Ease formula (Fles"
L16-1045,R15-1089,1,0.923557,"tter is certainly the most reliable way to measure text accessibility and harness insights into user preferences, it is very time-consuming and expensive to perform, especially in cases where the target readers are people with cognitive disabilities (intellectual disability, autism, hyperactivity, etc.). To address this issue, research has been focusing on developing automatic methods for readability evaluation, which could account for the specific reading difficulties of various reader populations (Dubay, 2004; Benjamin, 2012), including people with cognitive disabilities (Feng et al., 2010; Yaneva and Evans, 2015). However, relying on readability formulae to measure text accessibility is an approach that has many drawbacks (Benjamin, 2012; Dubay, 2004; Siddharthan, 2004), mainly related to the fact that readability formulae only employ surface text features such as word or sentence length. Thus for example, the Flesch formula (Flesch, 1948) presented below is very widely-used but cannot take into account the difficulty autistic readers have in using abstraction (Minshew et al., 2002) or the difficulty aphasic readers have with passive voice or other syntactic constructions (Berndt et al., 1996). F lesc"
L16-1045,R15-2005,1,0.739934,"from very easy (n=54), easy (n=37), medium (n=23) and even difficult (n=4) and very difficult (n=2). The control participants had a predominant rate of very easy (n=117) and easy (n=20) and none of them ranked any text as difficult or very difficult (Yaneva et al., 2015). We then investigated the compliance of a 150-document sample of easy-to-read documents available on the Web to the official guidelines for easy-to-read document production, affirming that the majority of them meet the required standards and that the Web could be used as a source for obtaining good-quality easy-to-read texts (Yaneva, 2015). Finally, we compared the level of text complexity in the 150-document sample of easy-to-read documents to corpora evaluated on people with mild intellectual disability (LocalNews corpus, (Feng, 2009)) or developed for people with autism (FIRST corpus (Jordanova, 2013), showing that there were no major differences between the text complexity of these corpora (Yaneva, 2015). Section 3 presents the main characteristics of the EasyRead corpus. Table 1: English-language corpora used in text simplification research While these corpora are undoubtedly very useful for developing TS systems because t"
L16-1045,C10-1152,0,0.426692,"r people with cognitive disabilities. We then compare the EasyRead corpus to the simplified output contained in the LocalNews corpus (Feng, 2009), the accessibility of which has been evaluated through reading comprehension experiments including 20 adults with mild intellectual disability. This comparison is made on the basis of 13 disability-specific linguistic features. The comparison reveals that there are no major differences between the two corpora, which shows that the EasyRead corpus is to a similar reading level as the user-evaluated texts. We also discuss the role of Simple Wikipedia (Zhu et al., 2010) as a widely-used accessibility benchmark, in light of our finding that it is significantly more complex than both the EasyRead and the LocalNews corpora. Keywords: readability, text simplification, cognitive disabilities, autism, easy-to-read 1. Introduction The main task of automatic text simplification (TS) is to convert texts into a more understandable form for readers with lower than average reading skills, without changing the original meaning of the text. These readers could include children and people with poor literacy (Aluisio et al., 2010), learning disabilities (Feng, 2009), neurod"
N19-1275,W17-1717,0,0.160024,"(Sag et al., 2002; Baldwin and Kim, 2010). As they are fraught with syntactic and semantic idiosyncrasies, their automatic identification remains a major challenge (Constant et al., 2017). Occurrences of discontinuous MWEs are particularly elusive as they involve relationships between non-adjacent tokens (e.g. put one of the blue masks on). While some previous studies disregard discontinuous MWEs (Legrand and Collobert, 2016), others stress the importance of factoring them in (Schneider et al., 2014). Using a CRF-based and a transition-based approach respectively, Moreau et al. (2018) and Al Saied et al. (2017) try to ∗ *The first two authors contributed equally. The code is available on https://github.com/ omidrohanian/gappy-mwes. 1 capture discontinuous occurrences with help from dependency parse information. Previously explored neural MWE identification models (Gharbieh et al., 2017) suffer from limitations in dealing with discontinuity, which can be attributed to their inherently sequential nature. More sophisticated architectures are yet to be investigated (Constant et al., 2017). Graph convolutional neural networks (GCNs) (Kipf and Welling, 2017) and attention-based neural sequence labeling (T"
N19-1275,K18-2005,0,0.0757411,"Missing"
N19-1275,J17-4005,0,0.235499,"Missing"
N19-1275,S17-1006,0,0.159451,"between non-adjacent tokens (e.g. put one of the blue masks on). While some previous studies disregard discontinuous MWEs (Legrand and Collobert, 2016), others stress the importance of factoring them in (Schneider et al., 2014). Using a CRF-based and a transition-based approach respectively, Moreau et al. (2018) and Al Saied et al. (2017) try to ∗ *The first two authors contributed equally. The code is available on https://github.com/ omidrohanian/gappy-mwes. 1 capture discontinuous occurrences with help from dependency parse information. Previously explored neural MWE identification models (Gharbieh et al., 2017) suffer from limitations in dealing with discontinuity, which can be attributed to their inherently sequential nature. More sophisticated architectures are yet to be investigated (Constant et al., 2017). Graph convolutional neural networks (GCNs) (Kipf and Welling, 2017) and attention-based neural sequence labeling (Tan et al., 2018) are methodologies suited for modeling non-adjacent relations and are hence adapted to MWE identification in this study. Conventional GCN (Kipf and Welling, 2017) uses a global graph structure for the entire input. We modify it such that GCN filters convolve nodes"
N19-1275,D14-1181,0,0.00250851,"nly prove superior to existing 2692 Proceedings of NAACL-HLT 2019, pages 2692–2698 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics methods in terms of overall performance but also are more robust in handling cases with gaps. 2 Methodology To specifically target discontinuity, we explore two mechanisms both preceding a Bi-LSTM: 1) a GCN layer to act as a syntactic ngram detector, 2) an attention mechanism to learn long-range dependencies. 2.1 Graph Convolution as Feature Extraction Standard convolutional filters act as sequential ngram detectors (Kim, 2014). Such filters might prove inadequate in modeling complex language units like discontinuous MWEs. One way to overcome this problem is to consider non-sequential relations by attending to syntactic information in parse trees through the application of GCNs. GCN is defined as a directed multi-node graph G(V, E) where vi ∈ V and (vi , r, vj ) ∈ E are entities (words) and edges (relations) respectively. By defining a vector xv as the feature representation for the word v, the convolution equation in GCN can be defined as a non-linear activation function f and a filter W with a bias term b as: c ="
N19-1275,W16-1810,0,0.0645005,"Missing"
N19-1275,D17-1159,0,0.140857,"ttention, on the other hand, learns representations by relating different parts of the same sequence. Each position in a sequence is linked to any other position with O(1) operations, minimising maximum path (compared to RNN’s O(n)) which facilitates gradient flow and makes it theoretically well-suited for learning long-range dependencies (Vaswani et al., 2017). The difference in the two approaches motivates our attempt to incorporate them into a hybrid model with an eye to exploiting their individual strengths. Other studies that used related syntax-aware methods in sequence labeling include Marcheggiani and Titov (2017) and Strubell et al. (2018) where GCN and self-attention were separately applied to semantic role labelling. Our contribution in this study, is to show for the first time, how GCNs can be successfully applied to MWE identification, especially to tackle discontinuous ones. Furthermore, we propose a novel architecture that integrates GCN with selfattention outperforming state-of-the-art. The resulting models not only prove superior to existing 2692 Proceedings of NAACL-HLT 2019, pages 2692–2698 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics method"
N19-1275,W18-4926,0,0.0313354,"emantics of their components (Sag et al., 2002; Baldwin and Kim, 2010). As they are fraught with syntactic and semantic idiosyncrasies, their automatic identification remains a major challenge (Constant et al., 2017). Occurrences of discontinuous MWEs are particularly elusive as they involve relationships between non-adjacent tokens (e.g. put one of the blue masks on). While some previous studies disregard discontinuous MWEs (Legrand and Collobert, 2016), others stress the importance of factoring them in (Schneider et al., 2014). Using a CRF-based and a transition-based approach respectively, Moreau et al. (2018) and Al Saied et al. (2017) try to ∗ *The first two authors contributed equally. The code is available on https://github.com/ omidrohanian/gappy-mwes. 1 capture discontinuous occurrences with help from dependency parse information. Previously explored neural MWE identification models (Gharbieh et al., 2017) suffer from limitations in dealing with discontinuity, which can be attributed to their inherently sequential nature. More sophisticated architectures are yet to be investigated (Constant et al., 2017). Graph convolutional neural networks (GCNs) (Kipf and Welling, 2017) and attention-based"
N19-1275,N18-2068,0,0.0496309,"Missing"
N19-1275,N18-1202,0,0.0730646,"Missing"
N19-1275,Q14-1016,0,0.261323,"c units composed of more than one word whose meanings cannot be fully determined by the semantics of their components (Sag et al., 2002; Baldwin and Kim, 2010). As they are fraught with syntactic and semantic idiosyncrasies, their automatic identification remains a major challenge (Constant et al., 2017). Occurrences of discontinuous MWEs are particularly elusive as they involve relationships between non-adjacent tokens (e.g. put one of the blue masks on). While some previous studies disregard discontinuous MWEs (Legrand and Collobert, 2016), others stress the importance of factoring them in (Schneider et al., 2014). Using a CRF-based and a transition-based approach respectively, Moreau et al. (2018) and Al Saied et al. (2017) try to ∗ *The first two authors contributed equally. The code is available on https://github.com/ omidrohanian/gappy-mwes. 1 capture discontinuous occurrences with help from dependency parse information. Previously explored neural MWE identification models (Gharbieh et al., 2017) suffer from limitations in dealing with discontinuity, which can be attributed to their inherently sequential nature. More sophisticated architectures are yet to be investigated (Constant et al., 2017). Gr"
N19-1275,D18-1548,0,0.0220045,"s representations by relating different parts of the same sequence. Each position in a sequence is linked to any other position with O(1) operations, minimising maximum path (compared to RNN’s O(n)) which facilitates gradient flow and makes it theoretically well-suited for learning long-range dependencies (Vaswani et al., 2017). The difference in the two approaches motivates our attempt to incorporate them into a hybrid model with an eye to exploiting their individual strengths. Other studies that used related syntax-aware methods in sequence labeling include Marcheggiani and Titov (2017) and Strubell et al. (2018) where GCN and self-attention were separately applied to semantic role labelling. Our contribution in this study, is to show for the first time, how GCNs can be successfully applied to MWE identification, especially to tackle discontinuous ones. Furthermore, we propose a novel architecture that integrates GCN with selfattention outperforming state-of-the-art. The resulting models not only prove superior to existing 2692 Proceedings of NAACL-HLT 2019, pages 2692–2698 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics methods in terms of overall perfo"
N19-1275,D18-1244,0,0.0217486,"e how much information should pass or be modified. For simplicity Cr is defined as 1 − T r. We apply a block of J stacked highway layers (the section inside the blue dotted square in Figure 1). Each layer regulates its input x using the two gates and a feedforward layer H as follows: y = T r H + (1 − T r) x (4) where denotes the Hadamard product and T r is defined as σ(WT r x + bT r ). We set bT r to a negative number to reinforce carry behavior which helps the model learn temporal dependencies early in the training. Our architecture bears some resemblance to Marcheggiani and Titov (2017) and Zhang et al. (2018) in its complementary view of GCN and BiLSTM. However there are some important differences. In these works, BiLSTM is applied prior to GCN in order to encode contextualised information and to enhance the teleportation capability of GCN. Marcheggiani and Titov (2017) stack a few BiLSTM layers with the idea that the resulting representation would enable GCN to consider nodes that are multiple hops away in the input graph. Zhang et al. (2018) use a similar encoder, however the model employs single BiLSTM and GCN layers, and the graph of relations is undirected. In our work, we use pre-trained con"
P01-1006,P95-1017,0,0.12318,"it is the antecedent of the current pronoun. A score is calculated based on these indicators and the discourse referent with the highest aggregate value is selected as antecedent. 3.2 Evaluation measures used The workbench incorporates an automatic scoring system operating on an XML input file where the correct antecedents for every anaphor have been marked. The annotation scheme recognised by the system at this moment is MUC, but support for the MATE annotation scheme is currently under developement as well. We have implemented four measures for evaluation: precision and recall as defined by Aone and Bennett (1995)4 as well as success rate and critical success rate as defined in (Mitkov, 2000a). These four measures are calculated as follows: • Precision = number of correctly resolved anaphor / number of anaphors attempted to be resolved • Recall = number of correctly resolved anaphors / number of all anaphors identified by the system • Success rate = number of correctly resolved anaphors / number of all anaphors • Critical success rate = number of correctly resolved anaphors / number of anaphors with more than one antecedent after a morphological filter was applied The last measure is an important crite"
P01-1006,W97-1306,0,0.846742,"full analysis). Future versions of the workbench will include access to semantic information (WordNet) to accommodate approaches incorporating such types of knowledge. 3 Comparative evaluation of knowledge-poor anaphora resolution approaches The first phase of our project included comparison of knowledge-poorer approaches which share a common pre-processing philosophy. We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev’s parserfree version of Lappin and Leass’ RAP (Kennedy and Boguraev, 1996), Baldwin’s pronoun resolution method (Baldwin, 1997) and Mitkov’s knowledge-poor pronoun resolution approach (Mitkov, 1998b). All three of these algorithms share a similar pre-processing methodology: they do not rely on a parser to process the input and instead use POS taggers and NP extractors; nor do any of the methods make use of semantic or real-world knowledge. We re-implemented all three algorithms based on their original description and personal consultation with the authors to avoid misinterpretations. Since the original version of CogNiac is non-robust and resolves only anaphors that obey certain rules, for fairer and comparable result"
P01-1006,C90-3063,0,0.808023,"Missing"
P01-1006,W98-1119,0,0.182481,"Missing"
P01-1006,C96-1021,0,0.893858,"esulting from the pre-processing phase. A list of noun phrases is explicitly kept in the file representation. Each entry in this list consists of a record containing: • the word form • the lemma of the word or of the head of the noun phrase • the starting position in the text • the ending position in the text • the part of speech • the grammatical function • the index of the sentence that contains the referent • the index of the verb whose argument this referent is Each of the algorithms implemented for the workbench enriches this set of data with information relevant to its particular needs. Kennedy and Boguraev (1996), for example, need additional information about whether a certain discourse referent is embedded or not, plus a pointer to the COREF class associated to the referent, while Mitkov’s approach needs a score associated to each noun phrase. Apart from the pre-processing tools, the implementation of the algorithms included in the workbench is built upon a common programming interface, which allows for some basic processing functions to be shared as well. An example is the morphological filter applied over the set of possible antecedents of an anaphor. 2.3 Usability of the workbench The evaluation"
P01-1006,J94-4002,0,0.701828,"Missing"
P01-1006,P98-2143,1,0.960412,"of data. The paper proposes an evaluation environment for comparing anaphora resolution algorithms which is illustrated by presenting the results of the comparative evaluation of three methods on the basis of several evaluation measures. 1 Introduction The evaluation of any NLP algorithm or system should indicate not only its efficiency or performance, but should also help us discover what a new approach brings to the current state of play in the field. To this end, a comparative evaluation with other well-known or similar approaches would be highly desirable. We have already voiced concern (Mitkov, 1998a), (Mitkov, 2000b) that the evaluation of anaphora resolution algorithms and systems is bereft of any common ground for comparison due not only to the difference of the evaluation data, but also due to the diversity of pre-processing tools employed by each anaphora resolution system. The evaluation picture would not be accurate even if we compared anaphora resolution systems on the basis of the same data since the pre-processing errors which would be carried over to the systems’ outputs might vary. As a way forward we have proposed Ruslan Mitkov School of Humanities, Languages and Social Scie"
P01-1006,mitkov-2000-towards,1,0.64749,"er proposes an evaluation environment for comparing anaphora resolution algorithms which is illustrated by presenting the results of the comparative evaluation of three methods on the basis of several evaluation measures. 1 Introduction The evaluation of any NLP algorithm or system should indicate not only its efficiency or performance, but should also help us discover what a new approach brings to the current state of play in the field. To this end, a comparative evaluation with other well-known or similar approaches would be highly desirable. We have already voiced concern (Mitkov, 1998a), (Mitkov, 2000b) that the evaluation of anaphora resolution algorithms and systems is bereft of any common ground for comparison due not only to the difference of the evaluation data, but also due to the diversity of pre-processing tools employed by each anaphora resolution system. The evaluation picture would not be accurate even if we compared anaphora resolution systems on the basis of the same data since the pre-processing errors which would be carried over to the systems’ outputs might vary. As a way forward we have proposed Ruslan Mitkov School of Humanities, Languages and Social Sciences University o"
P01-1006,A97-1011,0,0.086357,"Missing"
P01-1006,P99-1079,0,0.0464724,"Missing"
P01-1006,C98-2138,1,\N,Missing
P98-2143,C90-3063,0,0.922285,"f view of the centering theory. The latter proposes the ranking &quot;subject, direct object, indirect object&quot; (Brennan et al. 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects. Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0). The collocation preference here is restricted to the patterns &quot;noun phrase (pronoun), verb&quot; and &quot;verb, noun phrase (pronoun)&quot;. Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (Dagan & Itai 1990). Example: Press the key i down and turn the volume up... Press it i again. Immediate reference In technical manuals the &quot;immediate reference&quot; clue can often be useful in identifying the antecedent. The heuristics used is that in constructions of the form &quot;...(You) V l NP ... con (you) V 2 it (con (you) V 3 it)&quot;, where con ~ {and/or/before/after...}, the noun phrase immediately after V l is a very likely candidate for antecedent of the pronoun &quot;it&quot; immediately following V 2 and is therefore given preference (scores 2 and 0). This preference can be viewed as a modification of the collocation pr"
P98-2143,P89-1032,0,0.0604656,"Missing"
P98-2143,J94-4002,0,0.736091,"ack of syntactic information, for instance, means giving up c-command constraints and subject preference (or on other occasions object preference, see Mitkov 1995) which could be used in center tracking. Syntactic parallelism, useful in discriminating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb semantics and semantic parallelism. Our evaluation, however, suggests that much less is lost than might be feared. In fact, our evaluation shows that the results are comparable to syntax-based methods (Lappin & Leass 1994). We believe that the good success rate is due to the fact that a number of antecedent indicators are taken into account and no factor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith 1998) or syntactic and semantic parallelism preferences (see below). 3.1 Evaluation A Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-an"
P98-2143,A88-1003,0,0.935971,"Missing"
P98-2143,J90-4001,0,\N,Missing
P98-2143,C96-1021,0,\N,Missing
P98-2143,C88-1021,0,\N,Missing
P98-2143,C94-2191,1,\N,Missing
P98-2143,W97-1306,0,\N,Missing
P98-2143,P87-1022,0,\N,Missing
pekar-etal-2004-categorizing,C00-2136,0,\N,Missing
pekar-etal-2004-categorizing,A97-1011,0,\N,Missing
R19-1115,S16-1081,0,0.0199338,"the art STS methods rely on word embeddings one way or another. The recently introduced contextualised word embeddings have proved more effective than standard word embeddings in many natural language processing tasks. This paper evaluates the impact of several contextualised word embeddings on unsupervised STS methods and compares it with the existing supervised/unsupervised STS methods for different datasets in different languages and different domains. 1 Introduction Measuring Semantic Textual Similarity (STS) is calculating the degree of semantic equivalence between two snippets of text (Agirre et al., 2016). Earlier, STS tasks largely focused on similarity between short texts such as abstracts and product descriptions (Li et al., 2006; Mihalcea et al., 2006). Recently, STS tasks at the International Workshops on Semantic Evaluation (SemEval) focused on measuring STS between full sentence pairs. The introduction of competitive STS tasks led to the development of standard datasets like the SICK corpus (Bentivogli et al., 2016) and standardised the similarity score as a numerical value between 1 and 5 (Agirre et al., 2014). Having a good STS metric is crucial for many natural language processing ap"
R19-1115,N19-1078,0,0.0291762,"semantic similarity between a pair of sentences, takes the average of the word embeddings of all words in the two sentences, and calculates the cosine similarity between the resulting embeddings. This is a common way to acquire sentence embeddings from word embeddings. Obviously, this simple baseline leaves considerable room for variation. We have investigated the effects of ignoring stopwords and computing an average weighted by tf-idf in particular and reported them in the 4 section. 2.2.3 Stacked Embeddings Stacked Embeddings are obtained by concatenating different embeddings. According to Akbik et al. (2019) stacking the embeddings can provide a powerful embeddings to represent words. We represent the stacked embeddings in section 4 with ’+’ between the used models. As an example if the model name says ELMo + BERT, it is a stacked embedding of ELMo and BERT. For ELMo + BERT model we used pre-trained ’bertlarge-uncased’ model and ’original’ pre-trained ELMo model to represent each word as a 4096 + 3072 vector. 3.2 Standard Word Representations In order to compare the results of contextualised word embeddings, we used a standard word representation model in each experiment as a baseline. In this re"
R19-1115,C18-1139,0,0.0370311,"al. (2015). Word Mover’s Distance uses the word embeddings of the words in two texts to measure the minimum distance that the words in one text need to “travel” in semantic space to reach the words in the other text as shown in Figure 1. Kusner et al. (2015) says that this is a good approach than vector averaging since this technique keeps the word vectors as it is through out the operation. We have investigated the effects of considering/ ignoring stop words before calculating the word mover’s distance. 2.2.4 Flair Flair is another type of popular contextualised word embeddings introduced in Akbik et al. (2018). It takes a different approach by using a character level language model rather than the word level language model used in ELMo and BERT. The recommended way to use Flair embeddings is to stack pre-trained ’news-forward’ embeddings and pre-trained ’news-backward’ embeddings with Glove (Pennington et al., 2014) word embeddings (Akbik et al., 2018). We used the stacked model to represent each word as a 4196 lengthened vector. 2.3 Cosine Similarity on Average Vectors Figure 1: The Word Mover’s Distance between two documents 3.3 Experiments Cosine Similarity Using Smooth Inverse Frequency The thi"
R19-1115,S15-2017,1,0.897019,"Missing"
R19-1115,S14-2085,0,0.0248716,"dataset, 0.547 Pearson correlation for Newswire dataset and 0.570 weighted mean from both of them. The best performing model that participated in SemEval 2015 task 2, had 0.705 Pearson correlation for Wikipedia, 0.683 for Newswire and 0.690 weighted mean (Agirre et al., 2015). Our approach would rank fifth out of 17 team in the final results, which is the best result for an unsupervised approach. As with the English model, this one also surpasses other complex supervised models. As an example RTM-DCU-1stST.tree uses a supervised machine learning algorithm with Referential Translation Machines(Biici and Way, 2014) and our fairly simple unsupervised approach outperform them by a significant margin. Comparing the results we can safely assume that our approach works well with Spanish language STS too. 5.2 Bio-Medical STS In order to evaluate our approach in a different domain, we experimented it on Bio-medical STS dataset explained in 2.1.3. As in the previous experiments we applied all unsupervised approaches mentioned. We used ELMo embeddings trained on a biomedical domain corpora (e.g., PubMed abstracts, PMC full-text articles) (Peters et al., 1000 1 https://github.com/google-research/bert 2018) and Bi"
R19-1115,K18-2005,0,0.0314486,"nd it does not need a training set as the approach is unsupervised. As a result, the approach is easily portable to other languages and domains given the availability of ELMo and BERT models in that particular language or domain. In order to observe how well the method performs in other languages and domains we applied it to Spanish STS dataset and Biomedical STS dataset described in section 3. 5.1 Spanish STS We run all the unsupervised STS methods described in section 2 on the Spanish STS dataset explained in section 2.1.2. For the ELMo embeddings we used Spanish ELMo embeddings provided in Che et al. (2018), while for the BERT embeddings we used ”BERT-Base, Multilingual Cased” 1 model which has been built on the top 100 languages with the largest Wikipedias which includes Spanish language too. The predictions from the experiment were rescaled to lie ∈ [0,4] as the GOLD standards. Organisers have used only one evaluation metric in this Spanish STS task: Pearson correlation (τ ) against the predictions and GOLD standard. They have calculated Pearson correlation for each test set: Spanish news and Spanish wiki, separately and has taken the weighted average to give the final rankings in the leader b"
R19-1115,D17-1070,0,0.178057,"on 6 would briefly describe the related work done for STS. The paper finishes with conclusions. not be trained properly. Given the amount of human labour required to produce datasets for STS, it is not possible to have high quality large training datasets. As a result researches working in the field have also considered unsupervised methods for STS. Recent unsupervised approaches use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them. Such approaches have used cosine similarity on sent2vec (Pagliardini et al., 2018), InferSent (Conneau et al., 2017), Word Mover’s Distance (Kusner et al., 2015), Doc2Vec (Le and Mikolov, 2014) and Smooth Inverse Frequency with GloVe vectors (Arora et al., 2017). While these approaches have produced decent results in the final rankings of shared tasks, they have also provided strong baselines for the STS task. 2 Settings of the Experiments 2.1 Data Sets The experiments presented in this paper were carried out using several datasets which will be explained in next subsections. In order to prove the portability of the approaches, the proposed architectures were also tested on an English Biomedical STS dataset"
R19-1115,S14-2139,0,0.0609997,"Missing"
R19-1115,D15-1181,0,0.0271166,"efore the shift of interest in neural networks, most of the proposed methods relied heavily on feature engineering. With the introduction of word embedding models, researchers focused more on neural representation for this task. There are two main approaches which employ neural representation models: supervised and unsupervised. Unsupervised approaches use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them while supervised approaches uses a machine learning model trained to predict the similarity using word embeddings. ConvNet (He et al., 2015), Skip Thought vectors (Kiros et al., 2015), Dependency Tree-LSTM (Tai et al., 2015) and Siamese Neural Networks (Mueller and Thyagarajan, 2016) can be considered as the most successful architectures employed for calculating STS. These supervised approaches always suffer from less training data problem which is common in STS tasks. As a result the researches have also considered unsupervised approaches. The three unsupervised STS methods explored in this paper: Cosine similarity on average vectors, Word Mover’s Distance and Cosine similarity using Smooth Inverse Frequency are the most common u"
R19-1115,S14-2001,0,0.0261713,"for these evaluation metrics. For this reason, we applied a parametric regression step to obtain better-calibrated predictions. We trained a regression model on the SICK train data and predicted on the SICK test data. This calibration step served as a minor correction for our restrictively simple similarity function. However, this regression calibration improved the Pearson correlation by 0.01 for the SICK test set. Our unsupervised method had 0.762 Pearson correlation score, whilst the best result in the International Workshop on Semantic Evaluation 2014 Task 1 had 0.828 Pearson correlation (Marelli et al., 2014). Our approach would be ranked on the ninth position from the top results out of 18 participants, and it is the best unsupervised STS method among the results. Our method even outperformed systems that rely on additional feature generation (e.g. dependency parses) or data augmentation schemes. As an example, our method is just above the UoW system which relied on 20 linguistics features fed in to a Support Vector Machine and obtained a 0.714 Pearson correlation (Gupta et al., 2014). Compared to these complex approaches our simple approach provides a strong baseline to STS tasks. 5 Portability"
R19-1115,L18-1008,0,0.0297489,"ctures were also tested on an English Biomedical STS dataset. In addition, the language independence of the method is tested by applying it to a Spanish STS dataset. Word vectors are used to determine a representation of a sentence in approaches like Word Mover’s Distance (Kusner et al., 2015) and Smooth Inverse Frequency (Arora et al., 2017). The main weakness of word vectors is that each word has the same unique vector regardless of the context it appears. For an example, the word ”play” has several meanings, but in standard word embeddings such as Glove (Pennington et al., 2014), FastText (Mikolov et al., 2018) or Word2Vec (Mikolov et al., 2013) each instance of the word has the same representation regardless of the meaning which is used. However, contextualised word embedding models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) etc. generate embeddings for a word based on the context it appears, thus generating slightly different embeddings for each of its occurrence. The recent applications in areas such as question answering and textual entailment show that contextualised word embeddings perform better than the traditional word embeddings (Devlin et al., 2018). 2.1.1 English-Engl"
R19-1115,S17-2016,0,0.0187802,"cessing. For example in Biomedical Informatics, it can be used to compare genes (Ferreira and Couto, 2010). Given the growing importance of having a good STS metric and as a result of the SemEval workshops, researchers have proposed numerous STS methods. Most of the early approaches were based on traditional machine learning and involved heavy feature engineering (B´echara et al., 2015). With the advances of word embeddings, and as a result of the success neural networks have achieved in other fields, most of the methods proposed in recent years rely on neural architectures (Tai et al., 2015; Shao, 2017). Neural networks are preferred over traditional machine learning models as they generally tend to perform better than traditional machine learning models. They also do not rely on explicit linguistics features which have to be extracted before the ML model is learnt. Determining the best linguistic features for calculating STS is not an easy task as it requires a good understanding of the linguistic phenomenon and relies on researchers’ intuition. In addition, calculating these features is usually not an easy task, especially for languages other than English. Therefore, in contrast to traditi"
R19-1115,P15-1150,0,0.182274,"Missing"
R19-1115,S15-2001,0,0.0706766,"d Representations Tharindu Ranasinghe, Constantin Or˘asan and Ruslan Mitkov Research Group in Computational Linguistics University of Wolverhampton, UK {t.d.ranasinghehettiarachchige, c.orasan, r.mitkov }@wlv.ac.uk Abstract et al., 2011) and text classification (Rocchio, 1971). Semantic similarity also contributes to many semantic web applications like community extraction, ontology generation and entity disambiguation (Li et al., 2006), and it is also useful for Twitter search (Salton et al., 1997), where it is required to accurately measure semantic relatedness between concepts or entities (Xu et al., 2015). STS is not limited only to natural language processing. For example in Biomedical Informatics, it can be used to compare genes (Ferreira and Couto, 2010). Given the growing importance of having a good STS metric and as a result of the SemEval workshops, researchers have proposed numerous STS methods. Most of the early approaches were based on traditional machine learning and involved heavy feature engineering (B´echara et al., 2015). With the advances of word embeddings, and as a result of the success neural networks have achieved in other fields, most of the methods proposed in recent years"
R19-1115,P11-1076,0,0.0742506,"Missing"
R19-1115,N18-1049,0,0.0677019,"guages and domains in section 5. Section 6 would briefly describe the related work done for STS. The paper finishes with conclusions. not be trained properly. Given the amount of human labour required to produce datasets for STS, it is not possible to have high quality large training datasets. As a result researches working in the field have also considered unsupervised methods for STS. Recent unsupervised approaches use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them. Such approaches have used cosine similarity on sent2vec (Pagliardini et al., 2018), InferSent (Conneau et al., 2017), Word Mover’s Distance (Kusner et al., 2015), Doc2Vec (Le and Mikolov, 2014) and Smooth Inverse Frequency with GloVe vectors (Arora et al., 2017). While these approaches have produced decent results in the final rankings of shared tasks, they have also provided strong baselines for the STS task. 2 Settings of the Experiments 2.1 Data Sets The experiments presented in this paper were carried out using several datasets which will be explained in next subsections. In order to prove the portability of the approaches, the proposed architectures were also tested on"
R19-1115,D14-1162,0,0.102888,"the approaches, the proposed architectures were also tested on an English Biomedical STS dataset. In addition, the language independence of the method is tested by applying it to a Spanish STS dataset. Word vectors are used to determine a representation of a sentence in approaches like Word Mover’s Distance (Kusner et al., 2015) and Smooth Inverse Frequency (Arora et al., 2017). The main weakness of word vectors is that each word has the same unique vector regardless of the context it appears. For an example, the word ”play” has several meanings, but in standard word embeddings such as Glove (Pennington et al., 2014), FastText (Mikolov et al., 2018) or Word2Vec (Mikolov et al., 2013) each instance of the word has the same representation regardless of the meaning which is used. However, contextualised word embedding models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) etc. generate embeddings for a word based on the context it appears, thus generating slightly different embeddings for each of its occurrence. The recent applications in areas such as question answering and textual entailment show that contextualised word embeddings perform better than the traditional word embeddings (Devlin"
R19-1115,N18-1202,0,0.307667,"ation of a sentence in approaches like Word Mover’s Distance (Kusner et al., 2015) and Smooth Inverse Frequency (Arora et al., 2017). The main weakness of word vectors is that each word has the same unique vector regardless of the context it appears. For an example, the word ”play” has several meanings, but in standard word embeddings such as Glove (Pennington et al., 2014), FastText (Mikolov et al., 2018) or Word2Vec (Mikolov et al., 2013) each instance of the word has the same representation regardless of the meaning which is used. However, contextualised word embedding models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) etc. generate embeddings for a word based on the context it appears, thus generating slightly different embeddings for each of its occurrence. The recent applications in areas such as question answering and textual entailment show that contextualised word embeddings perform better than the traditional word embeddings (Devlin et al., 2018). 2.1.1 English-English STS Data Set For the experiments carried out on English STS, we used the SICK dataset. (Bentivogli et al., 2016). The SICK data contains 9927 sentence pairs with a 5,000/4,927 training/test split which were"
R19-1116,S15-2017,1,0.897921,"Missing"
R19-1116,S14-2114,0,0.0256167,"Missing"
R19-1116,S17-2001,0,0.0606945,"Missing"
R19-1116,S14-2055,0,0.0654058,"Missing"
R19-1116,D17-1070,0,0.0226762,"kind of neural network architecture which employed word embeddings (Shao, 2017). As an example, Maharjan et al. (2017) used an ensemble of traditional machine learning models and deep learning models in their top performing system at Semeval 2017 STS task. There are two main approaches which employ neural representation models: supervised and unsupervised. Unsupervised approaches use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them. Such approaches have used cosine similarity on sent2vec (Pagliardini et al., 2018), InferSent (Conneau et al., 2017), Doc2Vec (Le and Mikolov, 2014) and smooth inverse frequency with GloVe vectors (Arora et al., 2017). Supervised approaches use neural networks to project word embeddings to fixed dimensional vectors which are trained to capture the semantic meaning of the sentence. Recently, many neural network architectures have been used to calculate sentence similarity. He et al. (2015) propose an elaborate convolutional network (ConvNet) variant which infers sentence similarity by integrating various differences across many convolutions at varying scales. Kiros et al. (2015) propose the skip-thoughts mod"
R19-1116,S14-2139,0,0.0512514,"Missing"
R19-1116,2014.tc-1.10,1,0.901711,"unity. The SemEval tasks also led to the development of standard datasets like the SICK corpus (Bentivogli et al., 2016) and standardised the similarity score as a numerical value between 1 and 5 (Agirre et al., 2014). Having a good STS metric is very important in many natural language processing (NLP) applications. As an example, for certain types of question answering systems, having an accurate STS component is the key to success since the questions with similar meanings can be answered similarly (Majumder et al., 2016). STS is also important in translation memories retrieval and matching (Gupta et al., 2014b). Translation memories help translators by finding in the database they maintain previously translated sentences, which are similar to the one to be translated, and retrieving their translations. Hence, accurate STS methods are beneficial for translation memory. Given the growing importance of having a good STS metric and as a result of the SemEval workshops, researchers have proposed numerous STS methods. Most of the early approaches were based on traditional machine learning and involved heavy feature engineering (B´echara et al., 2015). With the advances of word embeddings, and as a resul"
R19-1116,S17-2015,0,0.0361576,"Missing"
R19-1116,D15-1181,0,0.0205158,"use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them. Such approaches have used cosine similarity on sent2vec (Pagliardini et al., 2018), InferSent (Conneau et al., 2017), Doc2Vec (Le and Mikolov, 2014) and smooth inverse frequency with GloVe vectors (Arora et al., 2017). Supervised approaches use neural networks to project word embeddings to fixed dimensional vectors which are trained to capture the semantic meaning of the sentence. Recently, many neural network architectures have been used to calculate sentence similarity. He et al. (2015) propose an elaborate convolutional network (ConvNet) variant which infers sentence similarity by integrating various differences across many convolutions at varying scales. Kiros et al. (2015) propose the skip-thoughts model, which extends the skip-gram approach of word2vec from the word to sentence level. This model feeds each sentence into an Recurrent Neural Network (RNN) encoder-decoder with Gated Recurrent Unit (GRU) activations. They attempt to reconstruct the immediately preceding and following sentences. For the sentence similarity task, they obtain skip-thought vectors for sentence p"
R19-1116,S14-2131,0,0.0615653,"Missing"
R19-1116,S14-2001,0,0.022563,"ing model to GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018) or concatenating them with word2vec model did not improve the results either. For this reason, none of these results are presented here. The Siamese neural network with GRU was tested with a cyclical learning rate (Smith, 2015), which has the advantage of forcing the model to find another local minimum if the current minimum is not robust and makes the model generalize better to unseen data. However, neither cyclical learning rate nor reducing learning rate on plateau increased the per3 These results are reported in Marelli et al. (2014) Approach τ FCICU (Hassan et al., 2017) BIT (Wu et al., 2017) ECNU (Tian et al., 2017) DT Team (Maharjan et al., 2017) Bi-directional LSTM† GRU + Capsule + Flatten† RTV 0.8540 0.8545 0.8547 MALSTM LSTM: Adagrad† GRU + Attention† LSTM + Attention† Bi-directional GRU† GRU† 0.8651 0.8692 0.8725 0.8743 0.8750 0.8792 0.8280 0.8400 0.8518 0.8536 Table 2: Pearson correlation (τ ) for STS2017 test set. formance further. We do not report these results too. Table 2 shows the results obtained for STS2017 test dataset comparing our experiments with other top performing models in SemEval 2017 Task 1 (Cer e"
R19-1116,L18-1008,0,0.076203,"Missing"
R19-1116,H92-1116,0,0.610984,"redicting STS 1 . 3 3.1 Settings of the Experiments Data Sets The experiments presented in this paper were carried out using the SICK dataset (Bentivogli et al., 2016) and SemEval 2017 Task 1 dataset (Cer et al., 2017) which we will refer as STS2017 dataset. The SICK data contains 9927 sentence pairs with a 5,000/4,927 training/test split which were employed in the SemEval tasks. Each pair is annotated with a relatedness score between [1,5] corresponding to the average relatedness judged by 10 different individuals. In order to generate more training data we used thesaurus-based augmentation (Miller, 1992) and added 10,022 additional training examples. Evaluation was done with the SICK test data. Mueller and Thyagarajan (2016) uses the same thesaurus-based data augmentation in their research. The STS2017 test datset had 250 sentence pairs annotated with a relatedness score between [1,5]. As the training data for the competition, participants were encouraged to make use of all existing data sets from prior STS evaluations including all previously released trial, training and evaluation data 2 . Once we combined all datasets from prior STS tasks we had 8277 sentence pairs for training. 3.2 Propos"
R19-1116,W16-1617,0,0.054413,"ghts. In addition, parameter updating is mirrored across these sub-networks. Siamese networks are popular among tasks that involve finding similarity or a relationship between two comparable things. They have been proven successful in tasks like signature verification (Bromley et al., 1993), face verification (Chopra et al., 2005), image similarity (Koch et al., 2015) and have been re1004 Proceedings of Recent Advances in Natural Language Processing, pages 1004–1011, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_116 cently used successfully in sentence similarity (Neculoiu et al., 2016). Siamese architectures are good in these tasks because, when the inputs are of the same kind, it makes sense to use a similar model to process similar inputs. In this way the networks will have representation vectors with the same semantics, making them easier to compare pairs of sentences. Given that the weights are shared across sub networks there are fewer parameters to train, which in turn means they require less training data and less tendency to over-fit. Given the amount of human labour required to produce datasets for STS, Siamese neural networks can prove the ideal solution for the S"
R19-1116,N18-1049,0,0.0127484,"he STS task at Semeval 2017 used some kind of neural network architecture which employed word embeddings (Shao, 2017). As an example, Maharjan et al. (2017) used an ensemble of traditional machine learning models and deep learning models in their top performing system at Semeval 2017 STS task. There are two main approaches which employ neural representation models: supervised and unsupervised. Unsupervised approaches use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them. Such approaches have used cosine similarity on sent2vec (Pagliardini et al., 2018), InferSent (Conneau et al., 2017), Doc2Vec (Le and Mikolov, 2014) and smooth inverse frequency with GloVe vectors (Arora et al., 2017). Supervised approaches use neural networks to project word embeddings to fixed dimensional vectors which are trained to capture the semantic meaning of the sentence. Recently, many neural network architectures have been used to calculate sentence similarity. He et al. (2015) propose an elaborate convolutional network (ConvNet) variant which infers sentence similarity by integrating various differences across many convolutions at varying scales. Kiros et al. (2"
R19-1116,D14-1162,0,0.0822554,"ctures with bidirectional GRU, LSTM with Attention and GRU with Attention also surpassed the benchmark. However uni-directional GRU out performed them on all 3 evaluation metrics. We experimented with other similarity functions and other embedding models. Using Euclidean distance for the similarity function instead of Manhattan distance did not improve the results for the model because semantically different sentences could end up being represented by nearly identical vectors due to the vanishing gradients of the Euclidean distance (Chopra et al., 2005). Changing the embedding model to GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018) or concatenating them with word2vec model did not improve the results either. For this reason, none of these results are presented here. The Siamese neural network with GRU was tested with a cyclical learning rate (Smith, 2015), which has the advantage of forcing the model to find another local minimum if the current minimum is not robust and makes the model generalize better to unseen data. However, neither cyclical learning rate nor reducing learning rate on plateau increased the per3 These results are reported in Marelli et al. (2014) Approach τ FCICU (Hass"
R19-1116,P13-4028,0,0.0323165,"S) is an important research area in natural language processing which plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. This paper evaluates Siamese recurrent architectures, a special type of neural networks, which are used here to measure STS. Several variants of the architecture are compared with existing methods. 1 Introduction Measuring Semantic Textual Similarity (STS) is the task of calculating the similarity between a pair of texts using both direct and indirect relationships between them (Rus et al., 2013). Originally, the work on STS largely focused on similarity between short texts such as abstracts and product descriptions (Li et al., 2006; Mihalcea et al., 2006). The introduction of the STS tasks at the International Workshops on Semantic Evaluation (SemEval) lead to an increase of the interest that the field received from the research community. The SemEval tasks also led to the development of standard datasets like the SICK corpus (Bentivogli et al., 2016) and standardised the similarity score as a numerical value between 1 and 5 (Agirre et al., 2014). Having a good STS metric is very imp"
R19-1116,S17-2016,0,0.0763167,"ated, and retrieving their translations. Hence, accurate STS methods are beneficial for translation memory. Given the growing importance of having a good STS metric and as a result of the SemEval workshops, researchers have proposed numerous STS methods. Most of the early approaches were based on traditional machine learning and involved heavy feature engineering (B´echara et al., 2015). With the advances of word embeddings, and as a result of the success neural networks have achieved in other fields, most of the methods proposed in recent years rely on neural architectures (Tai et al., 2015; Shao, 2017). Neural networks are preferred over traditional machine learning models as they generally tend to perform better than traditional machine learning models. They also do not rely on linguistic features which means they can be easily applied to languages other than English. The architecture employed in this paper is a special class of neural networks called Siamese neural networks. These networks contain two or more identical sub-networks. The networks are identical in the sense that they have the same configuration with the same parameters and weights. In addition, parameter updating is mirrore"
R19-1116,P15-1150,0,0.0865406,"Missing"
R19-1116,S17-2028,0,0.0332185,"Missing"
R19-1116,S17-2007,0,0.0415386,"Missing"
R19-1116,S14-2044,0,0.141732,"periments carried out in this paper including the datasets employed here and the different architectures explored. The architectures are evaluated in Section 4. The paper finishes with conclusions. 2 Related Work Given that a good STS metric is required for a variety of NLP fields, researchers have proposed a large number of such metrics. Before the shift of interest in neural networks, most of the proposed methods relied heavily on feature engineering. A typical example is (Gupta et al., 2014a) which employed 20 linguistic features fed into a support vector machine regressor. The top system (Zhao et al., 2014) in task 1 in SemEval 2014 has used seven types of features including text difference measures, common text similarity measures etc. (Zhao et al., 2014). Then they have fed it in to several learning algorithms like support vector machine regressor, Random Forest, Gradient boosting etc (Zhao et al., 2014). With the introduction of word embedding models, researchers focused more on neural representation for this task. Many of the leading teams in the STS task at Semeval 2017 used some kind of neural network architecture which employed word embeddings (Shao, 2017). As an example, Maharjan et al."
S15-2017,S14-2139,1,0.418054,"Missing"
S15-2017,N03-1017,0,0.0103338,"entify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6 . We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the colloc"
S15-2017,P07-2045,0,0.0167899,"e available by the Apache OpenNLP library were used (i.e. we used models to identify dates, locations, money, organisations, percentages, persons and time). We also used all the pre-trained NER models for Spanish (in this case, we used models to identify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available t"
S15-2017,2005.mtsummit-papers.11,0,0.0236061,"anslation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6 . We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the collocation modules of the 3 https://code.google.com/p/tt4j http://"
S15-2017,W02-0109,0,0.0206325,"es Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6 . We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the collocation modules of the 3 https://code.google.com/p/tt4j http://snowball.tartarus.org 5 http://opennlp.apache.org 6 https://github.com/hpcosta/stopwords 4 NLTK package (Loper and Bird, 2002), and sorted by the degree of likelihood association between their components. 2.2 Extracted Features This section details the features that our system uses to measure the semantic textual similarity between two sentences. The system uses the same features for both Subtask 2a and Subtask 2b. In addition to the baseline features used in Gupta et al., 2014, we introduced a set of Distributional, Semantic and Conceptual Similarity Measures, as well as a feature reflecting MWEs across sentences. 2.2.1 Baseline Features The system is built on the baseline system developed for SemEval2014, which con"
S15-2017,J03-1002,0,0.0048888,"o used all the pre-trained NER models for Spanish (in this case, we used models to identify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6 . We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5"
S15-2017,P03-1021,0,0.0111434,"as available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6 . We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the collocation modules of the 3 https://code.google.com/p/tt4j http://snowball.tartarus.org 5 http://opennlp.apache.org 6 http"
S15-2017,P13-1132,0,0.0233604,"tely for NLP purposes, including processing semantic similarities. For the purpose of our experiments, we focused on two more common types of MWEs in English and Spanish: verb noun combinations and verb particle constructions. Whenever a verb+noun or a verb+particle combination occurs in our sentence pair, we search a prepared list MWEs, sorted according to their likelihood measures of association. The degree of association of these combinations served as a feature in our ML system. 3 Semantic Similarity Measures This feature takes advantage of the Align, Disambiguate and Walk (ADW)9 library (Pilehvar et al., 2013), a WordNet-based approach for measuring semantic similarity of arbitrary pairs of lexical items. It is important to mention that this feature is the only one that only works for English, which explains why we have a translation model (see section 2.1.3). In other words, when we are dealing 8 with Spanish text, we use the trained model to translate from Spanish to English. As the ADW library permits us to measure the semantic similarity between two raw English sentences, either by using disambiguation or not, we used both options to calculate all the comparison methods made available by the li"
S15-2017,C10-3015,0,0.0256287,"fter computing all the co-occurrences, we used these values to calculate the Jaccard’ (Jaccard, 1901), Lin’ (Lin, 1998) and PMI’ (Turney, 2001) scores. 2.2.4 9 2.2.5 Multiword Expressions Multiword Expressions (MWEs) are meaningful lexical units whose distinct idiosyncratic properties call for special treatment within a computational system. Non-compositionality is one of the properties of MWEs. The degree of association between the components of a MWE has been proved to be a promising approach to find out how much they are non-compostional and therefore how probable they are acceptable MWEs (Ramisch et al., 2010). The more non-compositional a MWE is, the more important is not to treat its components separately for NLP purposes, including processing semantic similarities. For the purpose of our experiments, we focused on two more common types of MWEs in English and Spanish: verb noun combinations and verb particle constructions. Whenever a verb+noun or a verb+particle combination occurs in our sentence pair, we search a prepared list MWEs, sorted according to their likelihood measures of association. The degree of association of these combinations served as a feature in our ML system. 3 Semantic Simila"
S15-2017,N03-1033,0,0.0173387,"Missing"
S15-2017,P06-4018,0,\N,Missing
S16-1096,S13-1004,0,0.0131576,"ion Semantic Textual Similarity (STS) is the task of assigning a real number score to quantify the semantic likeness of two text snippets. Similarity measures play a crucial role in various areas of text processing and translation technologies ranging from improving information retrieval rankings (Lin and Hovy, 2003; Corley and Mihalcea, 2005) and text summarization to machine translation evaluation and enhancing matches in translation memory and terminologies (Resnik and others, 1999; Ma et al., 2011; Banchs et al., 2015; Vela and Tan, 2015). The annual SemEval STS task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) provides a platform where systems are evaluated on the same data and evaluation criteria. (1) = |{i : [∃j : (i, j) ∈ Al] and wi |{i : (1) wi ∈ C}| ∈ C}| (1) where is the monotonic proportion of the semantic unit alignment from a set of alignments Al that maps the positions of the words (i, j) between sentences S (1) and S (2) , given that the aligned units belong to a set of content words, C. Since the proportion is monotonic, the equation above only provides the proportion of semantic unit alignments for S (1) . The Al alignments pairs are automatic"
S16-1096,P14-1023,0,0.315427,"or changing but the denominator combined the proportions using harmonic mean: changes with it. The difference between v(S (1) ) and (1) (2) (2) 2 ∗ propAl ∗ propAl sim(S (1) , S (2) ) = (3) v(S ) contributes to distributional semantic dis(1) (2) tance. propAl + propAl To calculate a real value similarity score between Instead of simply using the alignment propor- the sentence vectors, we take the dot product betions, Sultan et al. (2015) extended their hypothe- tween the vectors to compute the cosine similarity sis by leveraging pre-trained neural net embeddings between the sentence vectors: (Baroni et al., 2014). They posited that the semantics of the sentence can be captured by the centroid v(S (1) ) · v(S (2) ) sim(S (1) , S (2) ) = (5) of its content words1 computed by the element-wise |v(S (1) ) ||v(S (2) )| sum of the content word embeddings normalized by the number of content words in the sentence. ToThere was no clear indication of which vector gether with the similarity scores from Equation 3 space Sultan et al. (2015) have chosen to compute and the cosine similarity between two sentence em- the similarity score from Equation 5. Thus we combeddings, they trained a Bayesian ridge regressor to"
S16-1096,S13-1020,0,0.03134,"Missing"
S16-1096,W05-1203,0,0.0616982,"ions. We compared the difference in performance of the replicated system and the extended variants. Our variants to the replicated system show improved correlation scores and all of our submissions outperform the median scores from all participating systems. (1) propAl 1 Introduction Semantic Textual Similarity (STS) is the task of assigning a real number score to quantify the semantic likeness of two text snippets. Similarity measures play a crucial role in various areas of text processing and translation technologies ranging from improving information retrieval rankings (Lin and Hovy, 2003; Corley and Mihalcea, 2005) and text summarization to machine translation evaluation and enhancing matches in translation memory and terminologies (Resnik and others, 1999; Ma et al., 2011; Banchs et al., 2015; Vela and Tan, 2015). The annual SemEval STS task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) provides a platform where systems are evaluated on the same data and evaluation criteria. (1) = |{i : [∃j : (i, j) ∈ Al] and wi |{i : (1) wi ∈ C}| ∈ C}| (1) where is the monotonic proportion of the semantic unit alignment from a set of alignments Al that maps the positions of the w"
S16-1096,N13-1092,0,0.100661,"Missing"
S16-1096,S14-2139,1,0.890069,"Missing"
S16-1096,D15-1124,1,0.89722,"Missing"
S16-1096,S14-2102,0,0.0241025,"ared to Sultan et al. (2015) results of 0.8015 we are 0.04 points short of their results which should technically rank our system at 20th out of 70+ submissions to the STS 2015 task4 . Machine Translation (MT) evaluation metrics have shown competitive performance in previous 4 Our replication attempt obtained better results compared to our STS-2015 submission (MiniExperts) that used a Support Vector Machine regressor trained on a number of linguistically motivated features (Gupta et al., 2014); it achieved 0.7216 mean score (B´echara et al., 2015). 637 STS tasks (Barr´on-Cede˜no et al., 2013; Huang and Chang, 2014; Bertero and Fung, 2015; Tan et al., 2015). Tan et al. (2016) annotated the STS datasets with MT metrics scores for every pair of sentence in the training and evaluation data. We extend our XGBoost model with these MT metric annotations and achieved a higher score for every domain leading to an overall Pearson correlation score of 0.73050 (+Saarsheff in Table 1). 6 Conclusion In this paper, we have presented our findings on replicating the top system in the STS 2014 and 2015 task and evaluated our replica of the system in the English STS task of SemEval-2016. We have introduced variants and e"
S16-1096,W14-1618,0,0.0369043,"d context window, 10 negative samples, subsampling, 400 dimensions • 2-word context window, PMI weighting, no compression, 300K dimensions In this case, we extracted two similarity features for every sentence pair. With the harmonic proportion feature from Equation 3 and the similarity scores from Equation 5, we trained a boosted tree ensemble on the 3 features using the STS 2012 to 2015 datasets and submitted the outputs from this model as our baseline submission in the English STS Task in SemEval 2016. 3.1 Replacing COMPOSES with GloVe Pennington et al. (2014) handles semantic regularities (Levy et al., 2014) explicitly by using a global log-bilinear regression model which combines the global matrix factorization and the local context vectors when training word embeddings. Instead of using the COMPOSES vector space, we experimented with replacing the v(wi ) component in Equation 4 with the GloVe vectors,2 vglove (wi ) such that: simglove (S (1) , S (2) ) = distributions p and pˆθ using regularised KullbackLeibler (KL) divergence. vglove (S (1) ) · vglove (S (2) ) |vglove (S (1) ) ||vglove (S (2) )| (6) The novelty lies in the usage of the global matrix to capture corpus wide phenomena that might n"
S16-1096,N03-1020,0,0.0860567,"sentence representations. We compared the difference in performance of the replicated system and the extended variants. Our variants to the replicated system show improved correlation scores and all of our submissions outperform the median scores from all participating systems. (1) propAl 1 Introduction Semantic Textual Similarity (STS) is the task of assigning a real number score to quantify the semantic likeness of two text snippets. Similarity measures play a crucial role in various areas of text processing and translation technologies ranging from improving information retrieval rankings (Lin and Hovy, 2003; Corley and Mihalcea, 2005) and text summarization to machine translation evaluation and enhancing matches in translation memory and terminologies (Resnik and others, 1999; Ma et al., 2011; Banchs et al., 2015; Vela and Tan, 2015). The annual SemEval STS task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) provides a platform where systems are evaluated on the same data and evaluation criteria. (1) = |{i : [∃j : (i, j) ∈ Al] and wi |{i : (1) wi ∈ C}| ∈ C}| (1) where is the monotonic proportion of the semantic unit alignment from a set of alignments Al that"
S16-1096,P11-1124,1,0.889794,"Missing"
S16-1096,D14-1162,0,0.0785412,"(personal communication with Arafat Sultan). 635 • 5-word context window, 10 negative samples, subsampling, 400 dimensions • 2-word context window, PMI weighting, no compression, 300K dimensions In this case, we extracted two similarity features for every sentence pair. With the harmonic proportion feature from Equation 3 and the similarity scores from Equation 5, we trained a boosted tree ensemble on the 3 features using the STS 2012 to 2015 datasets and submitted the outputs from this model as our baseline submission in the English STS Task in SemEval 2016. 3.1 Replacing COMPOSES with GloVe Pennington et al. (2014) handles semantic regularities (Levy et al., 2014) explicitly by using a global log-bilinear regression model which combines the global matrix factorization and the local context vectors when training word embeddings. Instead of using the COMPOSES vector space, we experimented with replacing the v(wi ) component in Equation 4 with the GloVe vectors,2 vglove (wi ) such that: simglove (S (1) , S (2) ) = distributions p and pˆθ using regularised KullbackLeibler (KL) divergence. vglove (S (1) ) · vglove (S (2) ) |vglove (S (1) ) ||vglove (S (2) )| (6) The novelty lies in the usage of the global ma"
S16-1096,Q14-1018,0,0.396864,"1: Replicating the Success of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity Hanna Bechara, Rohit Gupta, Liling Tan, Constantin Or˘asan, Ruslan Mitkov, Josef van Genabith University of Wolverhampton / UK, Universit¨at des Saarlandes / Germany Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz / Germany {hanna.bechara, r.gupta, corsasan, r.mitkov}@wlv.ac.uk, liling.tan@uni-saarland.de, josef.van genabith@dfki.de 2 Abstract DLS System from STS 2014 and 2015 For the past two editions of the STS task, the top performing submissions are from the DLS@CU team (Sultan et al., 2014b; Sultan et al., 2015). Their STS2014 submission is based on the proportion of overlapping content words between the two sentences treating semantic similarity as a monotonically increasing function of the degree to which two sentences contain semantically similar units and these units occur in similar semantic contexts (Sultan et al., 2014b). Essentially, their semantic metric is based on the proportion of aligned content words between two sentences, formally defined as: This paper describes the WOLVESAAR systems that participated in the English Semantic Textual Similarity (STS) task in SemE"
S16-1096,S14-2039,0,0.166692,"1: Replicating the Success of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity Hanna Bechara, Rohit Gupta, Liling Tan, Constantin Or˘asan, Ruslan Mitkov, Josef van Genabith University of Wolverhampton / UK, Universit¨at des Saarlandes / Germany Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz / Germany {hanna.bechara, r.gupta, corsasan, r.mitkov}@wlv.ac.uk, liling.tan@uni-saarland.de, josef.van genabith@dfki.de 2 Abstract DLS System from STS 2014 and 2015 For the past two editions of the STS task, the top performing submissions are from the DLS@CU team (Sultan et al., 2014b; Sultan et al., 2015). Their STS2014 submission is based on the proportion of overlapping content words between the two sentences treating semantic similarity as a monotonically increasing function of the degree to which two sentences contain semantically similar units and these units occur in similar semantic contexts (Sultan et al., 2014b). Essentially, their semantic metric is based on the proportion of aligned content words between two sentences, formally defined as: This paper describes the WOLVESAAR systems that participated in the English Semantic Textual Similarity (STS) task in SemE"
S16-1096,S15-2027,0,0.275822,"cess of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity Hanna Bechara, Rohit Gupta, Liling Tan, Constantin Or˘asan, Ruslan Mitkov, Josef van Genabith University of Wolverhampton / UK, Universit¨at des Saarlandes / Germany Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz / Germany {hanna.bechara, r.gupta, corsasan, r.mitkov}@wlv.ac.uk, liling.tan@uni-saarland.de, josef.van genabith@dfki.de 2 Abstract DLS System from STS 2014 and 2015 For the past two editions of the STS task, the top performing submissions are from the DLS@CU team (Sultan et al., 2014b; Sultan et al., 2015). Their STS2014 submission is based on the proportion of overlapping content words between the two sentences treating semantic similarity as a monotonically increasing function of the degree to which two sentences contain semantically similar units and these units occur in similar semantic contexts (Sultan et al., 2014b). Essentially, their semantic metric is based on the proportion of aligned content words between two sentences, formally defined as: This paper describes the WOLVESAAR systems that participated in the English Semantic Textual Similarity (STS) task in SemEval2016. We replicated"
S16-1096,P15-1150,0,0.0335968,"2 Similarity Using Tree LSTM Recurrent Neural Nets (RNNs) allow arbitrarily sized sentence lengths (Elman, 1990) but early work on RNNs suffered from the vanishing/exploding gradients problem (Bengio et al., 1994). Hochreiter and Schmidhuber (1997) introduced multiplicative input and output gate units to solve the vanishing gradients problem. While RNN and LSTM process sentences in a sequential manner, Tree-LSTM extends the LSTM architecture by processing the input sentence through a syntactic structure of the sentence. We use the ReVal metric (Gupta et al., 2015) implementation of Tree-LSTM (Tai et al., 2015) to generate the similarity score. ReVal represents both sentences (h1 , h2 ) using Tree-LSTMs and predicts a similarity score yˆ based on a neural network which considers both distance and angle between h1 and h2 : n   λ 1X (i) KL p(i) pˆθ + ||θ||22 n 2 for 1 ≤ j ≤ K, where, y ∈ [1, K] is the similarity score of a training pair. This gives us a similarity score between [1, K] which is mapped between [0, 1].3 Please refer to Gupta et al. (2015) for training details. 4 Submission We submitted three models based on the original replication of the Sultan et al. (2014b) and Sultan et al. (2015)"
S16-1096,S15-2015,1,0.890832,"Missing"
S16-1096,S16-1095,1,0.857884,"Missing"
S16-1096,W15-3051,1,0.839378,"he median scores from all participating systems. (1) propAl 1 Introduction Semantic Textual Similarity (STS) is the task of assigning a real number score to quantify the semantic likeness of two text snippets. Similarity measures play a crucial role in various areas of text processing and translation technologies ranging from improving information retrieval rankings (Lin and Hovy, 2003; Corley and Mihalcea, 2005) and text summarization to machine translation evaluation and enhancing matches in translation memory and terminologies (Resnik and others, 1999; Ma et al., 2011; Banchs et al., 2015; Vela and Tan, 2015). The annual SemEval STS task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) provides a platform where systems are evaluated on the same data and evaluation criteria. (1) = |{i : [∃j : (i, j) ∈ Al] and wi |{i : (1) wi ∈ C}| ∈ C}| (1) where is the monotonic proportion of the semantic unit alignment from a set of alignments Al that maps the positions of the words (i, j) between sentences S (1) and S (2) , given that the aligned units belong to a set of content words, C. Since the proportion is monotonic, the equation above only provides the proportion of sem"
S16-1096,S14-2010,0,\N,Missing
S16-1096,S15-2017,1,\N,Missing
S16-1096,S15-2004,0,\N,Missing
S16-1096,S12-1051,0,\N,Missing
S18-1090,W16-6208,0,0.0396344,"Missing"
S18-1090,S15-2080,0,0.0903873,"part to increasing popularity of social media and the availability of data from websites like Twitter and Reddit. Some recent work focus exclusively on irony or sarcasm in isolation (Joshi et al., 2016), under the assumption that sarcasm has a stronger impact on changing the sentiment of the overall message. However in many cases, these terms are taken to be practically synonymous (Pozzi et al., 2016; Wallace et al., 2014; Pt´acˇ ek et al., 2014). SemEval has a long-standing shared task on sentiment analysis that has also involved processing of figurative language including irony and sarcasm (Ghosh et al., 2015; Nakov et al., 2016). Results from recent tasks on sentiment analysis confirm that the top performing teams increasingly employ deep learning methodologies, while classical machine learning models like SVM and logistic regression remain popular (Ghosh et al., 2015; Rosenthal et al., 2017). In our observation of the training data, we noticed that tweets often follow a fairly consistent spatial pattern. Informative words are more likely to cluster at both ends of a tweet. Hashtags, while scattered throughout the whole text, tend to occur at the end. In ironic tweets, negative sentiments are mor"
S18-1090,W15-4322,0,0.0611345,"Missing"
S18-1090,S17-2126,0,0.0330906,"can be realised through contrast between different elements of the tweet. The elements of a tweet are: text, hashtagged tokens, and emojis. We adopt a more inclusive stance with regards to the concept of contrast with the following scenarios: Pre-processing Tweets were tokenised using NLTK’s tweet tokeniser (Loper and Bird, 2002). Additional preprocessing was done to obtain a subset of the features that concerned surface orthographic features (e.g. all capitals, elongations, emoticons, etc) and pattern-based named entities (e.g. time, place, user, etc). For this we used the ekphrasis toolkit (Baziotis et al., 2017). It employs an XML-based annotation scheme that made it easy to extract this information. For sentiment features and embeddings, however, pre-processing beyond tokenisation was deemed unnecessary as our emoji and word vectors were pre-trained on raw tweets. 1. Contrast between different parts of the same 554 and FR patterns. For these features, we rely on information from Vader sentiment lexicon (Gilbert, 2014). For dense vectors we use word2vec embeddings pretrained on a large twitter corpus as described in Godin et al. (2015). One limitation of these embeddings is that they don’t contain in"
S18-1090,S18-1005,0,0.0350317,"Missing"
S18-1090,P14-2084,0,0.105138,", Shiva Taslimipoor, Richard Evans and Ruslan Mitkov Research Group in Computationa Linguistics University of Wolverhampton Wolverhampton, UK {omid.rohanian,shiva.taslimi,r.j.evans,r.mitkov}@wlv.ac.uk Abstract employed for different purposes. It is widely accepted that sarcasm involves some degree of verbal aggression and ridicule directed at the hearer, whilst irony can simply be used for humorous or emphatic effect. It has been shown that computational processing of irony and sarcasm requires some knowledge of the context in which they appear, sometimes including paralinguistic information (Wallace et al., 2014). Exploring ironicity has practical implications, since performance of sentiment analysis systems is directly affected by knowledge about irony and sarcasm (Pozzi et al., 2016). As part of the 12th workshop on semantic evaluation (SemEval-2018), Shared Task 3 defines two subtasks with regards to irony detection in English tweets (Van Hee et al., 2018). Subtask A involves binary classification. The objective is to train a system that can label tweets as ironic or not. Subtask B is a multi-class classification problem with the objective to label tweets with one of the four specified labels descr"
S18-1090,U16-1013,0,0.0129768,"github.com/omidrohanian/ irony_detection “Do not say what you believe to be false.” 553 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 553–559 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 2 Related Work 3.2 Feature Representation There has been a recent surge of interest in the tasks of irony and sarcasm detection due in large part to increasing popularity of social media and the availability of data from websites like Twitter and Reddit. Some recent work focus exclusively on irony or sarcasm in isolation (Joshi et al., 2016), under the assumption that sarcasm has a stronger impact on changing the sentiment of the overall message. However in many cases, these terms are taken to be practically synonymous (Pozzi et al., 2016; Wallace et al., 2014; Pt´acˇ ek et al., 2014). SemEval has a long-standing shared task on sentiment analysis that has also involved processing of figurative language including irony and sarcasm (Ghosh et al., 2015; Nakov et al., 2016). Results from recent tasks on sentiment analysis confirm that the top performing teams increasingly employ deep learning methodologies, while classical machine le"
S18-1090,P14-5010,0,0.00247394,"n emojis (Eisner et al., 2016). element of a tweet a. antithetical emojis b. antithetical hashtagged tokens 2. Contrast between two different elements of a tweet c. text and hashtagged tokens d. text and emojis e. hashtagged tokens and emojis 3.3 A sizable proportion of the tweets contain multiword hashtags, such as #NotExcitedAboutThisAtAll or #goodluck, that require segmentation. For this we used ekphrasis’ hashtag segmentation tool (Baziotis et al., 2017). We separate the tweet and its segmented hashtagged tokens and run each group through the sentiment analysis tool from Stanford CoreNLP (Manning et al., 2014). CoreNLP assigns to an input any of 5 sentiment classes from very negative to very positive (0 to 4). If the resulting hashtag and text scores are on opposite sides of this spectrum, we consider this as contrast type c. as defined in 2. For d. and e. we follow a similar procedure. To approximate the sentiments present in emoji tokens, we use Emoji Sentiment Ranking (Kralj Novak et al., 2015). This is a lexicon of 751 emojis whose sentiments are ranked based on human annotation of 70,000 tweets in 13 European languages. The resulting contrast feature is a binary value that is set to True if an"
S18-1090,S16-1001,0,0.0874049,"opularity of social media and the availability of data from websites like Twitter and Reddit. Some recent work focus exclusively on irony or sarcasm in isolation (Joshi et al., 2016), under the assumption that sarcasm has a stronger impact on changing the sentiment of the overall message. However in many cases, these terms are taken to be practically synonymous (Pozzi et al., 2016; Wallace et al., 2014; Pt´acˇ ek et al., 2014). SemEval has a long-standing shared task on sentiment analysis that has also involved processing of figurative language including irony and sarcasm (Ghosh et al., 2015; Nakov et al., 2016). Results from recent tasks on sentiment analysis confirm that the top performing teams increasingly employ deep learning methodologies, while classical machine learning models like SVM and logistic regression remain popular (Ghosh et al., 2015; Rosenthal et al., 2017). In our observation of the training data, we noticed that tweets often follow a fairly consistent spatial pattern. Informative words are more likely to cluster at both ends of a tweet. Hashtags, while scattered throughout the whole text, tend to occur at the end. In ironic tweets, negative sentiments are more likely to be preced"
S18-1090,D13-1066,0,0.293229,"Missing"
S18-1160,D14-1162,0,0.0810285,"st popular word embedding method is word2vec (with the skip-gram architecture) which learns dense vector representations for words using an unsupervised model. Word2vec’s training objective is based on DH, defined so that the model can learn word vectors that are good at predicting nearby words (Mikolov et al., 2013). Another popular embedding technique is GloVe, which like word2vec, preserves semantic analogies in the vector space. One major difference between the two models is that GloVe utilises corpus statistics by training on global co-occurrence counts rather than local context windows (Pennington et al., 2014). In our system we use a concatenation of two sets of pre-trained embeddings. The first is trained on English Wikipedia using a variation of word2vec (Bojanowski et al., 2016). The other called ConceptNet Numberbatch (Speer and Lowry-Duda, 2017), is an ensemble of pre-trained Glove and word2vec vectors whose values are readjusted using a technique called retrofitting (Faruqui et al., 2014). In retrofitting, the values of the embeddings are updated using a training function that considers relational knowledge. Using each word embedding, we compute cosine similarity between each word in a triple"
S18-1160,J90-1003,0,0.483366,"the other two words. For each triple in this task, collocational behaviour of the attribute word with the first two words is measured to see whether the first word can be a better collocate than the other. To this end, we use several different association measures to compute the outputs of the Score function in Eq. 1. We measure the association of two words based on their co-occurrence in the span of 5 words. We use SketchEngine (Kilgarriff et al., 2004) to extract these statistics from the huge enTenTen corpus (Jakub´ıcˇ ek et al., 2013). Specifically, for each pair of words, we extract PMI (Church and Hanks, 1990) (known as MI in SketcEngine), MI3 (Oakes, 1998), log-likelihood (Dunning, 1993), Tscore (Krenn and Evert, 2001), log-Dice (Dice, 1945), and Salience (Kilgarriff et al., 2004) all as defined in SketchEngine. Approach Our goal is to define a simple interpretable metric, using which we can gauge semantic difference and identify discriminative attributes. We hypothesise that for a triple in this task, a stronger relation between the first word and the attribute (in comparison with the second word and the attribute)2 is indicative of the attribute word being discriminative between the two words. F"
S18-1160,J93-1003,0,0.280669,"te word with the first two words is measured to see whether the first word can be a better collocate than the other. To this end, we use several different association measures to compute the outputs of the Score function in Eq. 1. We measure the association of two words based on their co-occurrence in the span of 5 words. We use SketchEngine (Kilgarriff et al., 2004) to extract these statistics from the huge enTenTen corpus (Jakub´ıcˇ ek et al., 2013). Specifically, for each pair of words, we extract PMI (Church and Hanks, 1990) (known as MI in SketcEngine), MI3 (Oakes, 1998), log-likelihood (Dunning, 1993), Tscore (Krenn and Evert, 2001), log-Dice (Dice, 1945), and Salience (Kilgarriff et al., 2004) all as defined in SketchEngine. Approach Our goal is to define a simple interpretable metric, using which we can gauge semantic difference and identify discriminative attributes. We hypothesise that for a triple in this task, a stronger relation between the first word and the attribute (in comparison with the second word and the attribute)2 is indicative of the attribute word being discriminative between the two words. For each triple we define a discriminative score Disc Score(w1, w2, attr) as foll"
S18-1160,S17-2008,0,0.017604,"word vectors that are good at predicting nearby words (Mikolov et al., 2013). Another popular embedding technique is GloVe, which like word2vec, preserves semantic analogies in the vector space. One major difference between the two models is that GloVe utilises corpus statistics by training on global co-occurrence counts rather than local context windows (Pennington et al., 2014). In our system we use a concatenation of two sets of pre-trained embeddings. The first is trained on English Wikipedia using a variation of word2vec (Bojanowski et al., 2016). The other called ConceptNet Numberbatch (Speer and Lowry-Duda, 2017), is an ensemble of pre-trained Glove and word2vec vectors whose values are readjusted using a technique called retrofitting (Faruqui et al., 2014). In retrofitting, the values of the embeddings are updated using a training function that considers relational knowledge. Using each word embedding, we compute cosine similarity between each word in a triple and the attribute word to account for the statistics Score(w1, attr) and Score(w2, attr) in Eq. 1. 3.4 4 Experimental Settings We use the data as provided by the organisers of the shared task. We train our model on the train set and find the op"
S18-1160,W15-0107,0,0.0296965,"een two related words. One such example is dolphin and narwhal that only differ in having a horn (Krebs and Paperno, 2016). Therefore, combining linguistic and conceptual information would potentially strengthen a semantic model in capturing meaning of a word. To tackle this issue, some studies rely on human annotated list of different attributes related to a concept which are called feature norms (McRae et al., 2005). Despite their strength in encoding semantic knowledge, feature norms have not been widely used in practice because they are small in size and require a lot of work to assemble (Fagarasan et al., 2015). Lazaridou et al. (2016) is an earlier attempt at identification of discriminative features which focuses on visual attributes. 3 Association-based Score Statistical association measures have a long history in language processing. With the availability of huge corpora, these measures can be even more effective than before in finding collocations and associations between words. Collocational behaviour between two words is a strong signal that suggests one of the words can identify the other. As an example, in the triple (hair, body, curly), the association score in (hair, curly) is much more t"
S18-1160,W16-2509,0,0.0161173,"Linguistics 3.1 way, semantic difference can be modeled as the subtraction of vectors from semantically related words. As a classic example, subtraction of word vectors for king and man is similar to that of queen from woman (Mikolov et al., 2013). However not all semantic differences can be adequately captured using this method. There are many cases where the difference between two words originates from the lack or presence of a feature that cannot be directly mapped to the vector difference between two related words. One such example is dolphin and narwhal that only differ in having a horn (Krebs and Paperno, 2016). Therefore, combining linguistic and conceptual information would potentially strengthen a semantic model in capturing meaning of a word. To tackle this issue, some studies rely on human annotated list of different attributes related to a concept which are called feature norms (McRae et al., 2005). Despite their strength in encoding semantic knowledge, feature norms have not been widely used in practice because they are small in size and require a lot of work to assemble (Fagarasan et al., 2015). Lazaridou et al. (2016) is an earlier attempt at identification of discriminative features which"
S18-1160,P16-2035,0,0.0688439,"Missing"
S19-2228,N18-2031,0,0.0864053,"above performs poorly on classification tasks such as sentiment analysis, because it loses the word order in the same way it happens with the standard bag-ofwords model, and fails to recognise many sophisticated linguistic phenomena (Le and Mikolov, 2014). For this reason, the second approach relies on neural networks which receive as input the embedding vectors corresponding to the context, but without performing any modification on it. Keras was used to implement these neural architectures. Two neural architectures were developed. The first one was adopted from text classification research (Coates and Bollegala, 2018). As depicted in figure 1 it contains variants of Long Short-Term Memories (LSTMs) with self attention followed by average pooling and max pooling layers. It also has a dropout (Srivastava et al., 2014) between 2 dense layers after the concatenate layer. The model was trained with cyclical learning rate (Smith, 2017). The pooling layers in the first architecture are considered as a very primitive type of routing mechanism. The solution that is proposed is a capsule network (Sabour et al., 2017). A capsule network with a bi-directional GRU was also experimented with for this data set. The compl"
S19-2228,P13-1144,0,0.377721,"location, is a non-trivial task closely related to named entity recognition (NER) (Piskorski and Yangarber, 2013). For this reason, using an NER system to detect and assign location tags could seem a good way forward. However, NER systems may not be able to detect whether a name refers to a actual location or not (e.g., London in London Bus Company). In addition, location names are usually ambiguous, which means it is crucial that these are disambiguated in order to assign the correct coordinates. While in the past the focus in toponym resolution has been on rule and gazetteer driven methods (Speriosu and Baldridge, 2013), more recent approaches also consider ML-based techniques. DeLozier et al. (2015) describe their ML-based approach, which does not require a gazetteer. The approach calculates the geographical profile of each word, which is refined using Wikipedia statistics, and then fed into an ML classifier. Speriosu and Baldridge (2013) also make ∗ The first two authors contributed equally to the paper. use of an ML classifier which is text-driven. Geotags of documents are used to automatically generate a training set. Although the two previous approaches used two standard corpora for toponym resolution,"
S19-2228,S19-2155,0,0.051076,"Missing"
S19-2228,P16-1089,0,0.0623409,"Missing"
temnikova-etal-2012-clcm,J00-4006,0,\N,Missing
temnikova-etal-2012-clcm,W09-0441,0,\N,Missing
temnikova-etal-2012-clcm,C96-2183,0,\N,Missing
temnikova-etal-2012-clcm,P02-1040,0,\N,Missing
temnikova-etal-2012-clcm,temnikova-2010-cognitive,1,\N,Missing
temnikova-etal-2012-clcm,R11-1094,1,\N,Missing
temnikova-etal-2012-clcm,W09-1318,0,\N,Missing
tutin-etal-2004-annotation,J98-2001,0,\N,Missing
tutin-etal-2004-annotation,A00-1020,0,\N,Missing
tutin-etal-2004-annotation,W03-2120,0,\N,Missing
tutin-etal-2004-annotation,M98-1029,0,\N,Missing
W03-0203,W99-0410,0,\N,Missing
W03-0203,A97-1011,0,\N,Missing
W06-1416,H05-1103,0,0.229078,"the main tasks involved in this process and how our system was evaluated by domain experts. 1 Introduction Although Multiple-Choice Test Items (MCTIs) are used daily for assessment, authoring them is a laborious task. This gave rise to a relatively new research area within the emerging field of Textto-Text Generation (TTG) called Multiple-Choice Test Item Generation (MCTIG).1 Mitkov et al. (2006) developed a system which detects the important concepts in a text automatically and produces MCTIs testing explicitly conveyed factual knowledge.2 This differs from most related work in MCTIG such as Brown et al. (2005) and the papers in BEAUNLPII (2005) which deploy various NLP techniques to produce MCTIs for vocabulary assessment, often using preselected words as the input (see Mitkov et al. for more extensive comparisons). The approach of Mitkov et al. is semi-automatic since the MCTIs have to be reviewed by domain experts to assess their usability. They report that semi-automatic MCTIG can be more than 3 times quicker than authoring of MCTIs without the aid of their system. 2 Multiple-Choice Test Item Generation A MCTI such as the one in example (1) typically consists of a question or stem, the correct a"
W09-0207,O97-1002,0,0.151186,"Missing"
W09-0207,A97-1011,0,0.0179374,"following is an example of an item generated of the program and then post-edited. &quot;Which type of clause might contain verb and dependent words? i) verb clause ii) adverb clause iii) adverbial clause iv) multiple subordinate clause v) subordinate clause&quot;. 5 It should be noted that there were cases where the different selection/similarity strategies picked the same distractors. 6 51 http://search.cpan.org/~tpederse/WordNet-Similarity parser, these two words are said to be “cooccuring”8. The corpus used to collect the cooccurance vector was the BNC and the dependency parsed used the FDG parser (Tapanainen and Järvinen, 1997). The Information Radius (JS) is calculated using (5). (1) where len is the number of edges on the shortest path in the taxonomy between the two concepts and MAX is the depth of the taxonomy. Jiang and Conrath’s measure compares the sum of the information content of the individual concepts with that of their lowest common subsumer: (4) (5) (2) where 3.4 where IC(c) is the information content (Patwardhan et al., 2003) of the concept c, and lcs denotes the lowest common subsumer, which represents the most specific concept that the two concepts have in common. The Lin measure scales the informati"
W09-0207,P97-1008,0,0.0200551,"the principle of term based evaluation where each term is given a Soundex code. Each Soundex code itself consists of a letter and three numbers between 0 and 6. By way of example the Soundex code of verb is V610 (the first character in the code is always the first letter of the word encoded). Vowels are not used and digits are based on the consonants as illustrate by the following table: (3) 3.3 1. 2. 3. 4. 5. 6. Distributional similarity For computing distributional similarity we made use of Viktor Pekar&apos;s implementation7 based on Information Radius, which according to a comparative study by Dagan et al. (1997) performs consistently better than the other similar measures. Information Radius (or Jensen-Shannon divergence) is a variant of Kullback-Leiber divergence measuring similarity between two words as the amount of information contained in the difference between the two corresponding co-occurrence vectors. Every word wj is presented by the set of words wi1...n with which it co-occurs. The semantics of wj are modelled as a vector in an n-dimensional space where n is the number of words co-occurring with wj, and the features of the vector are the probabilities of the cooccurrences established from"
W09-0207,W03-0203,1,\N,Missing
W09-0207,P97-1012,0,\N,Missing
W09-0207,P97-1009,0,\N,Missing
W11-4112,2008.amta-papers.5,1,0.883804,"Missing"
W11-4112,P97-1032,0,0.120473,"Missing"
W14-1201,R13-2011,1,0.875676,"Missing"
W14-1201,E99-1042,0,0.198958,"ality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum ˇ disorder (Stajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the origin"
W14-1201,W03-1602,0,0.0740564,"sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum ˇ disorder (Stajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Wood• It is the first study to explore the possibil"
W14-1201,C96-2183,0,0.779222,"tences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum ˇ disorder (Stajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Wood• It is the first study to explore the possibility of replacing human assessment of the quality of TS system output with automatic evaluation. • It is the first study to investigate the correlation of human assessment of TS system output with MT evaluation metrics. • It proposes a decision-making procedu"
W14-1201,W11-1601,0,0.139643,"Missing"
W14-1201,P03-1054,0,0.00867588,"hu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 ˇ Table 1: IAA from (Glavaˇs and Stajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003). Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event mentions, while the event-wise simplification transforms each factual event mention into a separate sentence of the output. The last simplification scheme (pronominal anaphora) additionally employs pronominal anaphora resolution on top of the event-wise simplification scheme.3 Methodology 3.2 All experiments were conducted on a freely available sentence-level dataset1 , fully described in ˇ (Glavaˇs and Stajner, 2013), and the two datasets we derived from"
W14-1201,P02-1040,0,0.106571,"ree of simplification. Woodsend and Lapata (2011b), and Glavaˇs ˇ and Stajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 ˇ Table 1: IAA from (Glavaˇs and Stajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003). Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event menti"
W14-1201,D11-1038,0,0.180995,"n for Computational Linguistics 2 Category weighted κ Pearson MAE Grammaticality 0.68 0.77 0.18 Meaning 0.53 0.67 0.37 Simplicity 0.54 0.60 0.28 Related Work The output of the TS system proposed by Siddharthan (2006) was rated for grammaticality and meaning preservation by three human evaluators. Similarly, Drndarevic et al. (2013) evaluated the grammaticality and the meaning preservation of automatically simplified Spanish sentences on a Likert scale with the help of twenty-five human annotators. Additionally, the authors used seven readability metrics to assess the degree of simplification. Woodsend and Lapata (2011b), and Glavaˇs ˇ and Stajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002),"
W14-1201,P12-1107,0,0.436511,"Missing"
W14-1201,W11-2112,0,0.0548789,"fully automatic manner – using some readability measures or average sentence length as features ˇ (as in (Drndarevi´c et al., 2013; Glavaˇs and Stajner, 4 2013) for example). 3.4 3.5 Goal After we obtained the six automatic metrics (cosine, METEOR, TERp, TINE, T-BLEU, and SRL), we performed two sets of experiments, trying to answer two main questions: Features: MT Evaluation Metrics In all experiments, we focused on six commonly used MT evaluation metrics. These are cosine similarity (using the bag-of-words representation), METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011), and two components of TINE: T-BLEU (which differs from the standard BLEU (Papineni et al., 2002) by using 3-grams, 2-grams, and 1-grams when there are no 4-grams found, where the “original” BLEU would give score ‘0’) and SRL (which is the component of TINE based on semantic role labeling using SENNA4 ). Although these two components contribute equally to TINE (thus being linearly correlated with TINE), we wanted to investigate which one of them contributes more to the correlation of TINE with human judgements. Given their different natures, we expect T-BLEU to contribute more to the correlat"
W14-1201,C10-1152,0,0.652024,"012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Wood• It is the first study to explore the possibility of replacing human assessment of the quality of TS system output with automatic evaluation. • It is the first study to investigate the correlation of human assessment of TS system output with MT evaluation metrics. • It proposes a decision-making procedure for the classification of simplified sentences into: (1) those which are acceptable; (2) those which need further post-editing; and (3) those which should be discarded. 1 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @"
W14-1201,ruiter-etal-2010-human,0,0.038341,"Missing"
W14-1201,W09-0441,0,0.384081,"vaˇs ˇ and Stajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 ˇ Table 1: IAA from (Glavaˇs and Stajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003). Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event mentions, while the event-wise simplification transforms each"
W14-1201,W11-2107,0,\N,Missing
W14-5604,P02-1040,0,0.130489,"so recorded the time needed to simplify each text as an indication of, among other things, ease of use of (and clarity for) each set of rules and its productivity in general; these results are reported in Tables 6 and 7 of the following section. Several experiments were conducted to assess the inter-annotator agreement. We believe that the interannotator agreement is another good indicator as to how straightforward it is to apply a specific set of simplification rules and how reliable the simplification process is in general. We compute the interannotator agreement in terms of the BLEU score (Papineni et al., 2002). BLEU score is widely used in MT to compare the reference translation with the output of the system (translation hypothesis). Here we use the BLEU score to compare the simple sentences produced by one annotator with the corresponding sentences of another annotator. We measure the inter-annotator agreement for all three pairs of annotators (Table 8). In addition, we examined how many times each of the rules was selected by each writer which in our view would be not only a way of accounting for agreement and but also assessing the usefulness of every rule and how balanced a set of rules is in g"
W14-5604,I13-1043,1,0.826895,"eriments. In order to minimise potential familiarity effect (texts which already have been simplified are expected to be simplified faster and more efficiently as they are familiar to the writers), we allowed a few days interval between each time a specific text was simplified using different rules. We applied the Spauldings Spanish Readability index – SSR (Spaulding, 1956) as well as the Lexical Complexity index – LC (Anula, 2007) to assess the readability of the simplified texts. Both metrics have ˇ shown a good correlation with the possible reading obstacles for various target populations (Stajner and Saggion, 2013), and were used for the evaluation of the automatic TS system in Simplext (Drndarevi´c et al., 2013). We also asked a third writer to simplify samples from the texts used by the first two writers which were pre-assessed to be of comparable complexity, with a view to establishing whether familiarisation has an effect on the output. The results of these readability experiments are presented in Tables 4 and 5 of the following section. We also recorded the time needed to simplify each text as an indication of, among other things, ease of use of (and clarity for) each set of rules and its productiv"
W15-5203,macklovitch-russell-2000-whats,0,0.182995,"Missing"
W15-5203,J08-4005,0,0.0825882,"Missing"
W17-1718,E06-2001,0,0.0209946,"the verb, noun, and their two following adjacent words while preserving the original order. In other words, given each expression, the context feature consists of a combined vector with the dimension of 4 * 300 = 1200. Concatenated feature vectors are fed into a logistic regression classifier. The details with regards to training the classifier are explained in Section 6. (Taslimipoor et al., 2016). The resource focuses on four most frequent Italian verbs: fare, dare, prendere and trovare. It includes all the concordances of these verbs when followed by any noun, taken from the itWaC corpus (Baroni and Kilgarriff, 2006) using SketchEngine (Kilgarriff et al., 2004). The concordances include windows of ten words before and after an expression; hence, there are contexts around each Verb + Noun expression to be used for the classification task4 . 30, 094 concordances are annotated by two native speakers and can be used as the gold-standard for this research. The Kappa measure of interannotator agreement between the two annotators on the whole list of concordances is 0.65 with the observed agreement of 0.85 (Taslimipoor et al., 2016). Since the agreement is substantial, we continue with the first annotator’s anno"
W17-1718,E06-1042,0,0.109284,"Missing"
W17-1718,C14-1071,0,0.0130273,"the type-based investigation of MWEs (Rondon et al., 2015; Farahmand and Martins, 2014; Salehi and Cook, 1 Type refers to the canonical form of an expression, while token refers to each instance (usage) of the expression in any morphological form in text. 133 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 133–138, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics a small sample of human annotated expressions. Their method uses grammatical knowledge about the canonical form of expressions. There is some recent interest in segmenting texts (Brooke et al., 2014; Schneider et al., 2014) based on MWEs. Brook et al. (2014) propose an unsupervised approach for identifying the types of MWEs and tagging all the token occurrences of identified expressions as MWEs. This methodology might be more useful in the case of longer idiomatic expressions that is the focus of that study. Nevertheless for expressions with fewer words, the aforementioned challenges regarding opacity of tokens limit the efficacy of such techniques. The supervised approach posited by Schneider et al. (2014) results in a corpus of automatically annotated MWEs. However, the literal/idiomat"
W17-1718,J09-1005,1,0.882013,"expression is literal or MWE. Lexical and syntactic context features derived from vector representations are shown to be more effective over traditional statistical measures to identify tokens of MWEs. 1 Introduction Multiword expressions (MWEs) belong to a class of phraseological phenomena that is ubiquitous in the study of language (Baldwin and Kim, 2010). Scholarly research in MWEs immensely benefit both NLP applications and end users (Granger and Meunier, 2008). Context of an expression has been shown to be discriminative in determining whether a particular token is idiomatic or literal (Fazly et al., 2009; Tu and Roth, 2011). However, in-context investigation of MWEs is an underexplored area. The most common approach to treat MWEs computationally in any language is by examining corpora using statistical measures (Evert and Krenn, 2005; Ramisch et al., 2010; Villavicencio, 2005). These measures are broadly applied to identifying the types 1 of MWEs. While there is ongoing research to improve the type-based investigation of MWEs (Rondon et al., 2015; Farahmand and Martins, 2014; Salehi and Cook, 1 Type refers to the canonical form of an expression, while token refers to each instance (usage) of"
W17-1718,W16-1817,0,0.0265634,"Missing"
W17-1718,W06-1203,0,0.0540037,"ole minori. ‘In order to improve the transportation system, the government should build bridges both to and from the smaller islands.’ 2) Affinch possiamo migliorare la convivenza fra popoli diversi, bisognerebbe creare ponti, non sollevare nuovi muri! ‘In order to improve coexistence among different people, we should build bridges not raise new walls!’ 3 Related Work 4 With regards to context-based identification of idiomatic expressions, Birke and Sakar (2006) use a slightly modified version of an existing word sense disambiguation algorithm for supervised tokenbased identification of MWEs. Katz and Giesbrecht (2006) rely primarily on the local context of a token without considering linguistic properties of expressions. Fazly et al. (2009) take into account both linguistic properties and local context in their analysis of MWE tokens. They have employed and evaluated an unsupervised approach on Methodology Our goal is to classify tokens of Verb + Noun expressions into literal and idiomatic categories. To this end, we exploit the information contained in the concordance of each occurrence of an expression. Given each concordance, we extract vector representations for several of its words to act as syntactic"
W17-1718,W11-0807,0,0.0656959,"al or MWE. Lexical and syntactic context features derived from vector representations are shown to be more effective over traditional statistical measures to identify tokens of MWEs. 1 Introduction Multiword expressions (MWEs) belong to a class of phraseological phenomena that is ubiquitous in the study of language (Baldwin and Kim, 2010). Scholarly research in MWEs immensely benefit both NLP applications and end users (Granger and Meunier, 2008). Context of an expression has been shown to be discriminative in determining whether a particular token is idiomatic or literal (Fazly et al., 2009; Tu and Roth, 2011). However, in-context investigation of MWEs is an underexplored area. The most common approach to treat MWEs computationally in any language is by examining corpora using statistical measures (Evert and Krenn, 2005; Ramisch et al., 2010; Villavicencio, 2005). These measures are broadly applied to identifying the types 1 of MWEs. While there is ongoing research to improve the type-based investigation of MWEs (Rondon et al., 2015; Farahmand and Martins, 2014; Salehi and Cook, 1 Type refers to the canonical form of an expression, while token refers to each instance (usage) of the expression in an"
W17-1718,ramisch-etal-2010-mwetoolkit,0,0.212714,"to a class of phraseological phenomena that is ubiquitous in the study of language (Baldwin and Kim, 2010). Scholarly research in MWEs immensely benefit both NLP applications and end users (Granger and Meunier, 2008). Context of an expression has been shown to be discriminative in determining whether a particular token is idiomatic or literal (Fazly et al., 2009; Tu and Roth, 2011). However, in-context investigation of MWEs is an underexplored area. The most common approach to treat MWEs computationally in any language is by examining corpora using statistical measures (Evert and Krenn, 2005; Ramisch et al., 2010; Villavicencio, 2005). These measures are broadly applied to identifying the types 1 of MWEs. While there is ongoing research to improve the type-based investigation of MWEs (Rondon et al., 2015; Farahmand and Martins, 2014; Salehi and Cook, 1 Type refers to the canonical form of an expression, while token refers to each instance (usage) of the expression in any morphological form in text. 133 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 133–138, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics a small sample of human annotated expressi"
W17-1718,W15-0908,0,0.012765,"r and Meunier, 2008). Context of an expression has been shown to be discriminative in determining whether a particular token is idiomatic or literal (Fazly et al., 2009; Tu and Roth, 2011). However, in-context investigation of MWEs is an underexplored area. The most common approach to treat MWEs computationally in any language is by examining corpora using statistical measures (Evert and Krenn, 2005; Ramisch et al., 2010; Villavicencio, 2005). These measures are broadly applied to identifying the types 1 of MWEs. While there is ongoing research to improve the type-based investigation of MWEs (Rondon et al., 2015; Farahmand and Martins, 2014; Salehi and Cook, 1 Type refers to the canonical form of an expression, while token refers to each instance (usage) of the expression in any morphological form in text. 133 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 133–138, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics a small sample of human annotated expressions. Their method uses grammatical knowledge about the canonical form of expressions. There is some recent interest in segmenting texts (Brooke et al., 2014; Schneider et al., 2014) based on MWEs"
W17-1718,S13-1039,0,0.0582496,"Missing"
W17-1718,Q14-1016,0,0.0243107,"tigation of MWEs (Rondon et al., 2015; Farahmand and Martins, 2014; Salehi and Cook, 1 Type refers to the canonical form of an expression, while token refers to each instance (usage) of the expression in any morphological form in text. 133 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 133–138, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics a small sample of human annotated expressions. Their method uses grammatical knowledge about the canonical form of expressions. There is some recent interest in segmenting texts (Brooke et al., 2014; Schneider et al., 2014) based on MWEs. Brook et al. (2014) propose an unsupervised approach for identifying the types of MWEs and tagging all the token occurrences of identified expressions as MWEs. This methodology might be more useful in the case of longer idiomatic expressions that is the focus of that study. Nevertheless for expressions with fewer words, the aforementioned challenges regarding opacity of tokens limit the efficacy of such techniques. The supervised approach posited by Schneider et al. (2014) results in a corpus of automatically annotated MWEs. However, the literal/idiomatic usages of expressions"
W17-1718,W04-0401,1,0.625749,"l measures. (Ramisch et al., 2010). We target syntactic features by extracting vectors for the verb and the noun contained in the expression. Here we extract the vectors of the verb and the noun components in their raw form hoping to indirectly learn lexical and syntactic features for each occurrence of an expression. We believe that the structure of the verb component is important in extracting fixedness information for an expression. Also, the distributional representation of the noun component is informative since Verb + Noun expressions are known to have some degrees of semi-productivity (Stevenson et al., 2004). Additionally, we extract vectors for cooccurring words around a target expression. Specifically, we focus on the two words immediately following the Verb + Noun expression. We expect the arguments of the verb and the noun components that occur following the expression to play a distinguishing role in these kinds of so-called complex predicates2 (Samek-Lodovici, 2003). The word vectors in this study come from the Italian word2vec embedding which is available online3 . The generated word embedding approach has applied Gensim’s skipgram word2vec model with the window size of 10 to extract vecto"
W17-1718,W14-0802,0,\N,Missing
W17-5030,P15-2011,1,0.880809,"Missing"
W17-5030,C12-1023,0,0.029599,"s and sentences (Just and Carpenter, 1980). A series of studies investigating the effects of word frequency, verb complexity and lexical ambiguity (Juhasz and Rayner, 2003; Rayner et al., 2012), as well as contextual effects on word perception (Ehrlich and Rayner, 1981) concluded that long, rare and ambiguous words are more likely to be fixated longer and their processing requires more cognitive effort from the reader. These are also words that are likely to be replaced with shorter and more frequent ones during lexical simplification aimed at making text more accessible to wider populations (Bott et al., 2012; Glavaˇs and ˇ Stajner, 2015). Eye tracking has also been extensively used for the investigation of reading-related disorders owing to its capacity to provide information about the online processing of the text. For example, aphasic readers show “qualitatively different gaze fixation patterns” when answering comprehension questions (Dickey et al., 2007) and readers with dyslexia have been found to exhibit longer fixation durations and less efficient scanning techniques (Kim and Lombardino, 2016). In spite of the decades-long tradition of using gaze data to investigate word processing among ne"
W17-5030,S12-1066,0,0.0292614,"72 http://alt.qcri.org/semeval2016/task11/ https://simple.wikipedia.org feature (Wr´obel, 2016). In an earlier organised shared task on English Lexical Substitution at SemEval-2012,4 which had the aim of providing a framework for evaluation of lexical simplification systems, for each given sentence containing one target ‘complex’ word and four substitution candidates, participating systems were competing in ranking the four given substitution candidates according to their simplicity, i.e. how easy they are to be understood by fluent but non-native English speakers. The best performing system (Jauhar and Specia, 2012) used a combination of collocational features and four psycholinguistic measures extracted from the MRC (Machine Readable Dictionary) Psycholinguistic Database (Coltheart, 1981): Next, we identify which particular words (in their specific contexts) impose heavier cognitive load on each group of participants by clustering them as challenging or not, based on viewing time of each participant individually (Section 4.1), and then classifying them into four classes depending on the number of participants who found them challenging (Section 4.2). Finally, we investigate the lexical properties which"
W17-5030,N16-1050,0,0.18899,"utistic and non-autistic adolescents while reading individual sentences, suggesting that the reading task imposed an overall heavier cognitive load on the participants from the ASD group. Brock et al. (2008) also used gaze data1 and showed that both the ASD and the control participants were able to use context to successfully dis1.2 Complex Word Identification Complex Word Identification (CWI) task received high attention only recently, with findings suggesting that using a CWI module at the beginning of a lexical simplification (LS) pipeline significantly improves performances of LS systems (Paetzold and Specia, 2016c) and with the recently organised SemEval-2016 CWI shared task.2 The goal of the shared task was building CWI systems which would identify challenging words for non-native English speakers. The dataset consisted of sentences (without context), each with one content word (noun, verb, adjective, or adverb) marked as a target word. The training dataset contained 200 sentences, where each target word was annotated by 20 non-native English speakers as ‘easy’ or ‘complex’, depending on whether they understood its meaning or not. The participants were asked to mark the word as ‘complex’ even if they"
W17-5030,L16-1491,0,0.197677,"utistic and non-autistic adolescents while reading individual sentences, suggesting that the reading task imposed an overall heavier cognitive load on the participants from the ASD group. Brock et al. (2008) also used gaze data1 and showed that both the ASD and the control participants were able to use context to successfully dis1.2 Complex Word Identification Complex Word Identification (CWI) task received high attention only recently, with findings suggesting that using a CWI module at the beginning of a lexical simplification (LS) pipeline significantly improves performances of LS systems (Paetzold and Specia, 2016c) and with the recently organised SemEval-2016 CWI shared task.2 The goal of the shared task was building CWI systems which would identify challenging words for non-native English speakers. The dataset consisted of sentences (without context), each with one content word (noun, verb, adjective, or adverb) marked as a target word. The training dataset contained 200 sentences, where each target word was annotated by 20 non-native English speakers as ‘easy’ or ‘complex’, depending on whether they understood its meaning or not. The participants were asked to mark the word as ‘complex’ even if they"
W17-5030,S16-1146,0,0.027834,"Missing"
W19-8703,aziz-etal-2012-pet,1,0.86805,"Missing"
W19-8703,W19-6627,0,0.128545,"Missing"
W19-8703,W19-6622,0,0.144069,"Missing"
W19-8703,W16-3401,0,0.169036,"Missing"
W19-8703,2008.amta-papers.5,1,0.73301,"fidelity to the source text and, on the other hand, fluency in the target language. The term “translationese” had been put forward by Gellerstam (1986), but it was Baker (1993, 1996) who proposed and described the linguistic and stylistic natures of translationese, naming them Translation Universals. Translation Universals are hypotheses of linguistic features common to all translated texts regardless of the source and target languages. The hypothetical features proposed by Baker are: Simplification, Explicitation, Normalisation (or Conservatism) and Levelling out (or Convergence, as named by Pastor et al. (2008).1 Simplification means that translated texts are easier to understand than original texts because translators tend to simplify the language of the original text for the readers. Explicitation is the tendency to spell things out in translation; consequently, translations tend to be longer than original texts. Moreover, linguistic features that are typical of the source language are more explicit on the surface of the translation even though they are optional. Levelling out or Convergence means that there is less variation among translated texts than among non-translated texts. In other words,"
W19-8703,Q15-1030,0,0.337248,"Missing"
W19-8703,2006.amta-papers.25,0,0.106596,"Missing"
W97-1303,A88-1003,0,0.240898,"Missing"
W97-1303,J90-3001,0,0.0513012,"Missing"
W97-1303,C88-1021,0,0.105842,"Missing"
W97-1303,J94-4002,0,0.841225,"Missing"
W97-1303,C94-2191,1,0.887073,"Missing"
W97-1312,E95-1022,0,0.0212973,"Missing"
W97-1312,C90-3063,0,0.0776328,"Missing"
W97-1312,C96-1021,0,0.024492,"Missing"
W97-1312,J94-4002,0,0.0174705,"Missing"
W97-1312,J81-2001,0,0.0381678,"Missing"
W97-1312,C94-2191,1,0.897544,"Missing"
W97-1312,C94-2189,0,0.0452809,"Missing"
W98-1502,C94-2184,0,0.44107,"Missing"
W98-1502,P87-1022,0,0.118744,"Missing"
W98-1502,C88-1021,0,0.291474,"Missing"
W98-1502,P89-1032,0,0.0580421,"Missing"
W98-1502,C96-1021,0,0.0392262,"en proposed, making use of e.g. neural networks, a situation semantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et a!. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic processing of growing language resources. , Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linguistic knowledge (Baldwin 1997; Dagan & llai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams eta!. 1996). Our work is a continuation of these latest trends in the search for inexpensive, rapid and reliable procedures for anaph~ ora resolution. It shows how pronouns in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing, benefiting instead from corpus-based NLP techniques such as sentence splitting and part-ofspeech tagging. On the other hand, none of the projects reported so far, has looked at the multilingual aspects of the approaches that have been developed, or, in particu~"
W98-1502,J90-4001,0,0.0389455,"Missing"
W98-1502,J94-4002,0,0.106155,"Missing"
W98-1502,C94-2191,1,0.886091,"Missing"
W98-1502,W97-1302,0,0.0221653,"Missing"
W98-1502,A92-1028,0,0.0377426,"Missing"
W98-1502,1995.tmi-1.7,0,0.0867686,"Missing"
W98-1502,C94-2189,0,0.111123,"networks, a situation semantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et a!. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic processing of growing language resources. , Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linguistic knowledge (Baldwin 1997; Dagan & llai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams eta!. 1996). Our work is a continuation of these latest trends in the search for inexpensive, rapid and reliable procedures for anaph~ ora resolution. It shows how pronouns in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing, benefiting instead from corpus-based NLP techniques such as sentence splitting and part-ofspeech tagging. On the other hand, none of the projects reported so far, has looked at the multilingual aspects of the approaches that have been developed, or, in particu~ lar, how a specific approac"
W98-1502,W97-1314,0,0.0386543,"Missing"
W98-1502,A88-1003,0,0.251172,"Missing"
W98-1502,W97-1305,0,0.0107596,", knowledge poor anaphora resolution and multilingual NLP For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain 7 While anaphora resolution projects have been reported for French (Popescu-Belis & Robba 1997, Rolbert 1989), German (Dunker & Umbach 1993; Fischer eta!. 1996; Leass & Schwa!l1991; Stuckardt 1996; Stuckardt 1997), Japanese (Mori eta!. 1997; Nakaiwa & Ikehara 1992; Nakaiwa & Ikehara 1995; Nakaiwa et a!. 1995; Nakaiwa et a!. 1996; Wakao 1994 ), Portuguese (Abra9os & Lopes 1994 ), Swedish (Fraurud, 1988) and Turkish (Tin & Akman, 1994), the research on languages other than English constitutes only a small part of all the work in this field. In contrast to previous work in the field, our project has a tmly multilingual character. We have developed a knowledge-poor, robust approach which we propose as a platform for multilingual pronoun resolution in technical manuals. The approach was initially developed"
W98-1502,C94-2185,0,0.0281356,"methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain 7 While anaphora resolution projects have been reported for French (Popescu-Belis & Robba 1997, Rolbert 1989), German (Dunker & Umbach 1993; Fischer eta!. 1996; Leass & Schwa!l1991; Stuckardt 1996; Stuckardt 1997), Japanese (Mori eta!. 1997; Nakaiwa & Ikehara 1992; Nakaiwa & Ikehara 1995; Nakaiwa et a!. 1995; Nakaiwa et a!. 1996; Wakao 1994 ), Portuguese (Abra9os & Lopes 1994 ), Swedish (Fraurud, 1988) and Turkish (Tin & Akman, 1994), the research on languages other than English constitutes only a small part of all the work in this field. In contrast to previous work in the field, our project has a tmly multilingual character. We have developed a knowledge-poor, robust approach which we propose as a platform for multilingual pronoun resolution in technical manuals. The approach was initially developed and tested for English, but we have also adapted and tested it for Polish and Arabic. We found that the approach could be adapted"
W98-1502,C90-3063,0,\N,Missing
W98-1502,W97-1306,0,\N,Missing
