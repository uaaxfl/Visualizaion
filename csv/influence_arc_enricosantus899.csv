2020.cogalex-1.5,W11-2501,0,0.246629,"essential also for the creation of linguistic resources, such as ontologies and thesauri (Grefenstette, 1994), and this is especially true for specialized domains. Research on semantic relations benefited from the success of Distributional Semantic Models (Budanitsky and Hirst, 2006; Turney and Pantel, 2010), since they allow to easily generate semantic representations for words from text, in the form of semantic vectors. However, the semantic similarity measured by vector models is an underspecified relation, and it is not easy to tell, given two similar words, in which way they are similar (Baroni and Lenci, 2011; Chersoni et al., 2016; Schulte Im Walde, 2020). In the previous edition of the CogALex workshop, co-located with COLING 2016 in Osaka, the organizers set up a shared task dedicated to the corpus-based identification of semantic relations for English (Santus et al., 2016c). For the first time, systems were being evaluated in a shared task on the classification of multiple relations at once and, not surprisingly, the task proved to be challenging for computational models. For this new edition of the workshop, we have decided to launch a new version of the same shared task, adding more language"
2020.cogalex-1.5,E12-1004,0,0.0294467,"tion of semantic relations were being released, including relations such as hypernymy, cohyponymy and antonymy (Baroni and Lenci, 2011; Lenci and Benotto, 2012; Scheible and Schulte Im Walde, 2014; Weeds et al., 2014; Santus et al., 2015). In a second phase, following the increasing popularity of publicly-available frameworks for training word embeddings such as Word2Vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014), the focus quickly shifted on the usage of these vectors as features for supervised classifiers. Some of these methods train classifiers directly on pairs of vectors (Baroni et al., 2012; Weeds et al., 2014), while others compute DSMs-based metrics first and then use them as features (Santus et al., 2016b). Late attempts to conciliate similarity metrics and word embeddings brought to proposals such as APSyn (Santus et al., 2018). Some of the more recent contributions proposed even more sophisticated classification approaches. (Shwartz et al., 2016; Roller and Erk, 2016) aim at integrating word embeddings with information coming from lexical patterns, which proved to be extremely accurate for detecting relations. Other researchers introduced modifications to the structure of t"
2020.cogalex-1.5,P99-1008,0,0.0489817,"XLM-R, in all the four languages, and both in the monolingual and in the multilingual setting. This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 46 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 46–53 Barcelona, Spain (Online), December 12, 2020 Licence details: 2 Related Work The earlier methods for identifying semantic relations were based on patterns. Patterns are generally very precise for identifying relations such as hypernymy-hyponymy (Hearst, 1992; Snow et al., 2004) and meronymy (Berland and Charniak, 1999; Girju et al., 2006), or even multiple relations at once (Pantel and Pennacchiotti, 2006), but their limit is that the two related words have to occur together in a corpus, and thus their recall is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clark"
2020.cogalex-1.5,J06-1003,0,0.0238009,"re available at https://sites.google.com/site/ cogalexvisharedtask/. 1 Introduction Determining whether two words are related and what kind of relations holds between them is an important task in Natural Language Processing, and it has inspired a lot of research for more than one decade (Santus, 2016). Discovering relations between words is essential also for the creation of linguistic resources, such as ontologies and thesauri (Grefenstette, 1994), and this is especially true for specialized domains. Research on semantic relations benefited from the success of Distributional Semantic Models (Budanitsky and Hirst, 2006; Turney and Pantel, 2010), since they allow to easily generate semantic representations for words from text, in the form of semantic vectors. However, the semantic similarity measured by vector models is an underspecified relation, and it is not easy to tell, given two similar words, in which way they are similar (Baroni and Lenci, 2011; Chersoni et al., 2016; Schulte Im Walde, 2020). In the previous edition of the CogALex workshop, co-located with COLING 2016 in Osaka, the organizers set up a shared task dedicated to the corpus-based identification of semantic relations for English (Santus e"
2020.cogalex-1.5,W16-5313,1,0.76517,"reation of linguistic resources, such as ontologies and thesauri (Grefenstette, 1994), and this is especially true for specialized domains. Research on semantic relations benefited from the success of Distributional Semantic Models (Budanitsky and Hirst, 2006; Turney and Pantel, 2010), since they allow to easily generate semantic representations for words from text, in the form of semantic vectors. However, the semantic similarity measured by vector models is an underspecified relation, and it is not easy to tell, given two similar words, in which way they are similar (Baroni and Lenci, 2011; Chersoni et al., 2016; Schulte Im Walde, 2020). In the previous edition of the CogALex workshop, co-located with COLING 2016 in Osaka, the organizers set up a shared task dedicated to the corpus-based identification of semantic relations for English (Santus et al., 2016c). For the first time, systems were being evaluated in a shared task on the classification of multiple relations at once and, not surprisingly, the task proved to be challenging for computational models. For this new edition of the workshop, we have decided to launch a new version of the same shared task, adding more languages to the evaluation and"
2020.cogalex-1.5,W09-0215,0,0.0152169,"1999; Girju et al., 2006), or even multiple relations at once (Pantel and Pennacchiotti, 2006), but their limit is that the two related words have to occur together in a corpus, and thus their recall is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Benotto, 2012; Santus et al., 2014a), synonymy (Santus et al., 2016a) or antonymy (Santus et al., 2014b), or looked for specific differences in their distributional contexts (Scheible et al., 2013). In parallel, the first large datasets for evaluating the identification of semantic relations were being released, including relations such as hypernymy, cohyponymy and antonymy (Baroni and Lenci, 2011; Lenci and Benotto, 2012; Scheible and Schulte Im Walde, 2014; Weeds et al., 2014; Santus et al., 2015). In a second phase, following the increasing popularity of publicly-available frame"
2020.cogalex-1.5,2020.cogalex-1.6,0,0.0274903,"as a test set, with no ground truth. Detailed class statistics can be found in Table 2 (no training and validation data was provided for Italian). SYN ANT HYP RANDOM TOTAL train 842 916 898 2554 5210 English valid test 259 266 308 306 292 279 877 887 1736 1738 train 782 829 841 2430 4882 German valid 272 275 294 786 1627 test 265 281 286 796 1628 train 402 361 421 1330 2514 Chinese valid 129 136 145 428 838 test 122 142 129 445 838 Italian test 187 144 153 523 1007 Table 2: Dataset Statistics 3.3 Participating Teams Three participants submitted their system to CogALex-VI shared task: HSemID (Colson, 2020), Text2CS (Wachowiak et al., 2020) and TransDNN (Karmakar and McCrae, 2020). All teams took part in subtask 1, while only two of them participated in subtask 2. Text2TCS exploited a multilingual language model based on XLM-RoBERTa (Conneau et al., 2020), which is pretrained on 100 different languages using CommonCrawl data. To adapt the system to the task, the authors appended a linear layer to XLM-R, followed by a softmax for the classification. This system was fine-tuned on the three training set from different languages simultaneously. TransDNN proposed an architecture combining BERT (Devli"
2020.cogalex-1.5,J06-1005,0,0.106204,"uages, and both in the monolingual and in the multilingual setting. This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 46 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 46–53 Barcelona, Spain (Online), December 12, 2020 Licence details: 2 Related Work The earlier methods for identifying semantic relations were based on patterns. Patterns are generally very precise for identifying relations such as hypernymy-hyponymy (Hearst, 1992; Snow et al., 2004) and meronymy (Berland and Charniak, 1999; Girju et al., 2006), or even multiple relations at once (Pantel and Pennacchiotti, 2006), but their limit is that the two related words have to occur together in a corpus, and thus their recall is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Be"
2020.cogalex-1.5,P19-1476,0,0.0156161,"at a time, with rare attempts of tackling the problem in a multiclass setting. The shared task organized in coincidence with CogALex 2016 (Zock et al., 2016) was one of the few exceptions, and the low results achieved by most systems (the top F-score being 0.44) showed the difficulty of distinguishing between multiple relations at once. For this reason, we have decided to propose a similar challenge, yet including another factor of complexity: multilingualism. Considering the recent approaches that have been introduced for semantic relations in multilingual (Wang et al., 2019), crosslingual (Glavaš and Vulic, 2019) and meta learning (Yu et al., 2020) settings, we provided datasets in multiple languages (English, German, Chinese and Italian) and encouraged the participants to train their systems for both monolingual and multilingual evaluation. 3 Shared Task The CogALex-VI shared task was organized as a friendly competition: participants had access to both training and testing datasets, which were respectively released on August 1 and September 1, 2020. The scores of the participating systems were evaluated with the official scripts, and each team had to submit a short paper containing the system descrip"
2020.cogalex-1.5,C92-2082,0,0.591806,"ormance was achieved by a RoBERTa-based system, XLM-R, in all the four languages, and both in the monolingual and in the multilingual setting. This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 46 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 46–53 Barcelona, Spain (Online), December 12, 2020 Licence details: 2 Related Work The earlier methods for identifying semantic relations were based on patterns. Patterns are generally very precise for identifying relations such as hypernymy-hyponymy (Hearst, 1992; Snow et al., 2004) and meronymy (Berland and Charniak, 1999; Girju et al., 2006), or even multiple relations at once (Pantel and Pennacchiotti, 2006), but their limit is that the two related words have to occur together in a corpus, and thus their recall is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation"
2020.cogalex-1.5,W19-4310,0,0.0234319,"Missing"
2020.cogalex-1.5,2020.cogalex-1.8,0,0.0211918,"s can be found in Table 2 (no training and validation data was provided for Italian). SYN ANT HYP RANDOM TOTAL train 842 916 898 2554 5210 English valid test 259 266 308 306 292 279 877 887 1736 1738 train 782 829 841 2430 4882 German valid 272 275 294 786 1627 test 265 281 286 796 1628 train 402 361 421 1330 2514 Chinese valid 129 136 145 428 838 test 122 142 129 445 838 Italian test 187 144 153 523 1007 Table 2: Dataset Statistics 3.3 Participating Teams Three participants submitted their system to CogALex-VI shared task: HSemID (Colson, 2020), Text2CS (Wachowiak et al., 2020) and TransDNN (Karmakar and McCrae, 2020). All teams took part in subtask 1, while only two of them participated in subtask 2. Text2TCS exploited a multilingual language model based on XLM-RoBERTa (Conneau et al., 2020), which is pretrained on 100 different languages using CommonCrawl data. To adapt the system to the task, the authors appended a linear layer to XLM-R, followed by a softmax for the classification. This system was fine-tuned on the three training set from different languages simultaneously. TransDNN proposed an architecture combining BERT (Devlin et al., 2018), LSTM and CNN, in which the BERT embeddings are passed to a"
2020.cogalex-1.5,S12-1012,0,0.0263254,"t al., 2006), or even multiple relations at once (Pantel and Pennacchiotti, 2006), but their limit is that the two related words have to occur together in a corpus, and thus their recall is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Benotto, 2012; Santus et al., 2014a), synonymy (Santus et al., 2016a) or antonymy (Santus et al., 2014b), or looked for specific differences in their distributional contexts (Scheible et al., 2013). In parallel, the first large datasets for evaluating the identification of semantic relations were being released, including relations such as hypernymy, cohyponymy and antonymy (Baroni and Lenci, 2011; Lenci and Benotto, 2012; Scheible and Schulte Im Walde, 2014; Weeds et al., 2014; Santus et al., 2015). In a second phase, following the increasing popularity of publicly-available frameworks for training word e"
2020.cogalex-1.5,P16-2074,0,0.0145453,"Santus et al., 2018). Some of the more recent contributions proposed even more sophisticated classification approaches. (Shwartz et al., 2016; Roller and Erk, 2016) aim at integrating word embeddings with information coming from lexical patterns, which proved to be extremely accurate for detecting relations. Other researchers introduced modifications to the structure of the vector spaces with the goal of identifying a specific type of semantic relation, for example by modifying the objective function of the Word2Vec training to inject external knowledge from a lexical resource (e.g. WordNet) (Nguyen et al., 2016; Nguyen et al., 2017), or by adding an extra postprocessing step that projects the word vectors into a new space, expressly specialized for modeling the target relation (Vuli´c and Korhonen, 2018) or even more refined techniques of vector space specialisation (e.g. adversarial specialisation) (Kamath et al., 2019). However, these contributions mostly tried to address one relation at a time, with rare attempts of tackling the problem in a multiclass setting. The shared task organized in coincidence with CogALex 2016 (Zock et al., 2016) was one of the few exceptions, and the low results achieve"
2020.cogalex-1.5,P06-1015,0,0.0487534,"l setting. This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 46 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 46–53 Barcelona, Spain (Online), December 12, 2020 Licence details: 2 Related Work The earlier methods for identifying semantic relations were based on patterns. Patterns are generally very precise for identifying relations such as hypernymy-hyponymy (Hearst, 1992; Snow et al., 2004) and meronymy (Berland and Charniak, 1999; Girju et al., 2006), or even multiple relations at once (Pantel and Pennacchiotti, 2006), but their limit is that the two related words have to occur together in a corpus, and thus their recall is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Benotto, 2012; Santus et al., 2014a), synonymy (Santus et al., 2016a) o"
2020.cogalex-1.5,D14-1162,0,0.0859858,"ntonymy (Santus et al., 2014b), or looked for specific differences in their distributional contexts (Scheible et al., 2013). In parallel, the first large datasets for evaluating the identification of semantic relations were being released, including relations such as hypernymy, cohyponymy and antonymy (Baroni and Lenci, 2011; Lenci and Benotto, 2012; Scheible and Schulte Im Walde, 2014; Weeds et al., 2014; Santus et al., 2015). In a second phase, following the increasing popularity of publicly-available frameworks for training word embeddings such as Word2Vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014), the focus quickly shifted on the usage of these vectors as features for supervised classifiers. Some of these methods train classifiers directly on pairs of vectors (Baroni et al., 2012; Weeds et al., 2014), while others compute DSMs-based metrics first and then use them as features (Santus et al., 2016b). Late attempts to conciliate similarity metrics and word embeddings brought to proposals such as APSyn (Santus et al., 2018). Some of the more recent contributions proposed even more sophisticated classification approaches. (Shwartz et al., 2016; Roller and Erk, 2016) aim at integrating wor"
2020.cogalex-1.5,D16-1234,0,0.013283,"., 2013) and Glove (Pennington et al., 2014), the focus quickly shifted on the usage of these vectors as features for supervised classifiers. Some of these methods train classifiers directly on pairs of vectors (Baroni et al., 2012; Weeds et al., 2014), while others compute DSMs-based metrics first and then use them as features (Santus et al., 2016b). Late attempts to conciliate similarity metrics and word embeddings brought to proposals such as APSyn (Santus et al., 2018). Some of the more recent contributions proposed even more sophisticated classification approaches. (Shwartz et al., 2016; Roller and Erk, 2016) aim at integrating word embeddings with information coming from lexical patterns, which proved to be extremely accurate for detecting relations. Other researchers introduced modifications to the structure of the vector spaces with the goal of identifying a specific type of semantic relation, for example by modifying the objective function of the Word2Vec training to inject external knowledge from a lexical resource (e.g. WordNet) (Nguyen et al., 2016; Nguyen et al., 2017), or by adding an extra postprocessing step that projects the word vectors into a new space, expressly specialized for mode"
2020.cogalex-1.5,E14-4008,1,0.831619,"tiple relations at once (Pantel and Pennacchiotti, 2006), but their limit is that the two related words have to occur together in a corpus, and thus their recall is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Benotto, 2012; Santus et al., 2014a), synonymy (Santus et al., 2016a) or antonymy (Santus et al., 2014b), or looked for specific differences in their distributional contexts (Scheible et al., 2013). In parallel, the first large datasets for evaluating the identification of semantic relations were being released, including relations such as hypernymy, cohyponymy and antonymy (Baroni and Lenci, 2011; Lenci and Benotto, 2012; Scheible and Schulte Im Walde, 2014; Weeds et al., 2014; Santus et al., 2015). In a second phase, following the increasing popularity of publicly-available frameworks for training word embeddings such as Wor"
2020.cogalex-1.5,Y14-1018,1,0.807923,"tiple relations at once (Pantel and Pennacchiotti, 2006), but their limit is that the two related words have to occur together in a corpus, and thus their recall is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Benotto, 2012; Santus et al., 2014a), synonymy (Santus et al., 2016a) or antonymy (Santus et al., 2014b), or looked for specific differences in their distributional contexts (Scheible et al., 2013). In parallel, the first large datasets for evaluating the identification of semantic relations were being released, including relations such as hypernymy, cohyponymy and antonymy (Baroni and Lenci, 2011; Lenci and Benotto, 2012; Scheible and Schulte Im Walde, 2014; Weeds et al., 2014; Santus et al., 2015). In a second phase, following the increasing popularity of publicly-available frameworks for training word embeddings such as Wor"
2020.cogalex-1.5,W15-4208,1,0.862062,"g apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Benotto, 2012; Santus et al., 2014a), synonymy (Santus et al., 2016a) or antonymy (Santus et al., 2014b), or looked for specific differences in their distributional contexts (Scheible et al., 2013). In parallel, the first large datasets for evaluating the identification of semantic relations were being released, including relations such as hypernymy, cohyponymy and antonymy (Baroni and Lenci, 2011; Lenci and Benotto, 2012; Scheible and Schulte Im Walde, 2014; Weeds et al., 2014; Santus et al., 2015). In a second phase, following the increasing popularity of publicly-available frameworks for training word embeddings such as Word2Vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014), the focus quickly shifted on the usage of these vectors as features for supervised classifiers. Some of these methods train classifiers directly on pairs of vectors (Baroni et al., 2012; Weeds et al., 2014), while others compute DSMs-based metrics first and then use them as features (Santus et al., 2016b). Late attempts to conciliate similarity metrics and word embeddings brought to proposals such as"
2020.cogalex-1.5,L16-1723,1,0.930353,"st, 2006; Turney and Pantel, 2010), since they allow to easily generate semantic representations for words from text, in the form of semantic vectors. However, the semantic similarity measured by vector models is an underspecified relation, and it is not easy to tell, given two similar words, in which way they are similar (Baroni and Lenci, 2011; Chersoni et al., 2016; Schulte Im Walde, 2020). In the previous edition of the CogALex workshop, co-located with COLING 2016 in Osaka, the organizers set up a shared task dedicated to the corpus-based identification of semantic relations for English (Santus et al., 2016c). For the first time, systems were being evaluated in a shared task on the classification of multiple relations at once and, not surprisingly, the task proved to be challenging for computational models. For this new edition of the workshop, we have decided to launch a new version of the same shared task, adding more languages to the evaluation and encouraging the participants to evaluate their system also in a multilingual setting. Among the three teams that submitted their systems, the top performance was achieved by a RoBERTa-based system, XLM-R, in all the four languages, and both in the"
2020.cogalex-1.5,L16-1722,1,0.891422,"st, 2006; Turney and Pantel, 2010), since they allow to easily generate semantic representations for words from text, in the form of semantic vectors. However, the semantic similarity measured by vector models is an underspecified relation, and it is not easy to tell, given two similar words, in which way they are similar (Baroni and Lenci, 2011; Chersoni et al., 2016; Schulte Im Walde, 2020). In the previous edition of the CogALex workshop, co-located with COLING 2016 in Osaka, the organizers set up a shared task dedicated to the corpus-based identification of semantic relations for English (Santus et al., 2016c). For the first time, systems were being evaluated in a shared task on the classification of multiple relations at once and, not surprisingly, the task proved to be challenging for computational models. For this new edition of the workshop, we have decided to launch a new version of the same shared task, adding more languages to the evaluation and encouraging the participants to evaluate their system also in a multilingual setting. Among the three teams that submitted their systems, the top performance was achieved by a RoBERTa-based system, XLM-R, in all the four languages, and both in the"
2020.cogalex-1.5,W16-5309,1,0.896948,"st, 2006; Turney and Pantel, 2010), since they allow to easily generate semantic representations for words from text, in the form of semantic vectors. However, the semantic similarity measured by vector models is an underspecified relation, and it is not easy to tell, given two similar words, in which way they are similar (Baroni and Lenci, 2011; Chersoni et al., 2016; Schulte Im Walde, 2020). In the previous edition of the CogALex workshop, co-located with COLING 2016 in Osaka, the organizers set up a shared task dedicated to the corpus-based identification of semantic relations for English (Santus et al., 2016c). For the first time, systems were being evaluated in a shared task on the classification of multiple relations at once and, not surprisingly, the task proved to be challenging for computational models. For this new edition of the workshop, we have decided to launch a new version of the same shared task, adding more languages to the evaluation and encouraging the participants to evaluate their system also in a multilingual setting. Among the three teams that submitted their systems, the top performance was achieved by a RoBERTa-based system, XLM-R, in all the four languages, and both in the"
2020.cogalex-1.5,P18-2088,1,0.837835,"second phase, following the increasing popularity of publicly-available frameworks for training word embeddings such as Word2Vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014), the focus quickly shifted on the usage of these vectors as features for supervised classifiers. Some of these methods train classifiers directly on pairs of vectors (Baroni et al., 2012; Weeds et al., 2014), while others compute DSMs-based metrics first and then use them as features (Santus et al., 2016b). Late attempts to conciliate similarity metrics and word embeddings brought to proposals such as APSyn (Santus et al., 2018). Some of the more recent contributions proposed even more sophisticated classification approaches. (Shwartz et al., 2016; Roller and Erk, 2016) aim at integrating word embeddings with information coming from lexical patterns, which proved to be extremely accurate for detecting relations. Other researchers introduced modifications to the structure of the vector spaces with the goal of identifying a specific type of semantic relation, for example by modifying the objective function of the Word2Vec training to inject external knowledge from a lexical resource (e.g. WordNet) (Nguyen et al., 2016;"
2020.cogalex-1.5,W14-5814,0,0.0610671,"Missing"
2020.cogalex-1.5,I13-1056,0,0.026263,"l is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Benotto, 2012; Santus et al., 2014a), synonymy (Santus et al., 2016a) or antonymy (Santus et al., 2014b), or looked for specific differences in their distributional contexts (Scheible et al., 2013). In parallel, the first large datasets for evaluating the identification of semantic relations were being released, including relations such as hypernymy, cohyponymy and antonymy (Baroni and Lenci, 2011; Lenci and Benotto, 2012; Scheible and Schulte Im Walde, 2014; Weeds et al., 2014; Santus et al., 2015). In a second phase, following the increasing popularity of publicly-available frameworks for training word embeddings such as Word2Vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014), the focus quickly shifted on the usage of these vectors as features for supervised classifiers. S"
2020.cogalex-1.5,P16-1226,0,0.0633399,"eedings of the Workshop on Cognitive Aspects of the Lexicon, pages 46–53 Barcelona, Spain (Online), December 12, 2020 Licence details: 2 Related Work The earlier methods for identifying semantic relations were based on patterns. Patterns are generally very precise for identifying relations such as hypernymy-hyponymy (Hearst, 1992; Snow et al., 2004) and meronymy (Berland and Charniak, 1999; Girju et al., 2006), or even multiple relations at once (Pantel and Pennacchiotti, 2006), but their limit is that the two related words have to occur together in a corpus, and thus their recall is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Benotto, 2012; Santus et al., 2014a), synonymy (Santus et al., 2016a) or antonymy (Santus et al., 2014b), or looked for specific differences in their distributional contexts (Scheible et al., 2013). In parallel"
2020.cogalex-1.5,W18-3018,0,0.0345802,"Missing"
2020.cogalex-1.5,2020.cogalex-1.7,0,0.0329548,"ground truth. Detailed class statistics can be found in Table 2 (no training and validation data was provided for Italian). SYN ANT HYP RANDOM TOTAL train 842 916 898 2554 5210 English valid test 259 266 308 306 292 279 877 887 1736 1738 train 782 829 841 2430 4882 German valid 272 275 294 786 1627 test 265 281 286 796 1628 train 402 361 421 1330 2514 Chinese valid 129 136 145 428 838 test 122 142 129 445 838 Italian test 187 144 153 523 1007 Table 2: Dataset Statistics 3.3 Participating Teams Three participants submitted their system to CogALex-VI shared task: HSemID (Colson, 2020), Text2CS (Wachowiak et al., 2020) and TransDNN (Karmakar and McCrae, 2020). All teams took part in subtask 1, while only two of them participated in subtask 2. Text2TCS exploited a multilingual language model based on XLM-RoBERTa (Conneau et al., 2020), which is pretrained on 100 different languages using CommonCrawl data. To adapt the system to the task, the authors appended a linear layer to XLM-R, followed by a softmax for the classification. This system was fine-tuned on the three training set from different languages simultaneously. TransDNN proposed an architecture combining BERT (Devlin et al., 2018), LSTM and CNN, in"
2020.cogalex-1.5,W03-1011,0,0.0794512,"(Berland and Charniak, 1999; Girju et al., 2006), or even multiple relations at once (Pantel and Pennacchiotti, 2006), but their limit is that the two related words have to occur together in a corpus, and thus their recall is limited (Shwartz et al., 2016). Distributional Models, which do not suffer from such limitations, became then the first choice for the NLP research on semantic relations. In a first phase, researchers focused on the similarity metric, proposing alternatives to cosine that can be more efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Benotto, 2012; Santus et al., 2014a), synonymy (Santus et al., 2016a) or antonymy (Santus et al., 2014b), or looked for specific differences in their distributional contexts (Scheible et al., 2013). In parallel, the first large datasets for evaluating the identification of semantic relations were being released, including relations such as hypernymy, cohyponymy and antonymy (Baroni and Lenci, 2011; Lenci and Benotto, 2012; Scheible and Schulte Im Walde, 2014; Weeds et al., 2014; Santus et al., 2015). In a second phase, following the increasing popularity of publicly-a"
2020.cogalex-1.5,C14-1212,0,0.0171418,"efficient in setting apart a specific semantic relation from the others, e.g. hypernymy (Weeds and Weir, 2003; Clarke, 2009; Lenci and Benotto, 2012; Santus et al., 2014a), synonymy (Santus et al., 2016a) or antonymy (Santus et al., 2014b), or looked for specific differences in their distributional contexts (Scheible et al., 2013). In parallel, the first large datasets for evaluating the identification of semantic relations were being released, including relations such as hypernymy, cohyponymy and antonymy (Baroni and Lenci, 2011; Lenci and Benotto, 2012; Scheible and Schulte Im Walde, 2014; Weeds et al., 2014; Santus et al., 2015). In a second phase, following the increasing popularity of publicly-available frameworks for training word embeddings such as Word2Vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014), the focus quickly shifted on the usage of these vectors as features for supervised classifiers. Some of these methods train classifiers directly on pairs of vectors (Baroni et al., 2012; Weeds et al., 2014), while others compute DSMs-based metrics first and then use them as features (Santus et al., 2016b). Late attempts to conciliate similarity metrics and word embeddings brought"
2020.cogalex-1.5,2020.acl-main.336,0,0.0344424,"the problem in a multiclass setting. The shared task organized in coincidence with CogALex 2016 (Zock et al., 2016) was one of the few exceptions, and the low results achieved by most systems (the top F-score being 0.44) showed the difficulty of distinguishing between multiple relations at once. For this reason, we have decided to propose a similar challenge, yet including another factor of complexity: multilingualism. Considering the recent approaches that have been introduced for semantic relations in multilingual (Wang et al., 2019), crosslingual (Glavaš and Vulic, 2019) and meta learning (Yu et al., 2020) settings, we provided datasets in multiple languages (English, German, Chinese and Italian) and encouraged the participants to train their systems for both monolingual and multilingual evaluation. 3 Shared Task The CogALex-VI shared task was organized as a friendly competition: participants had access to both training and testing datasets, which were respectively released on August 1 and September 1, 2020. The scores of the participating systems were evaluated with the official scripts, and each team had to submit a short paper containing the system description. Among the three participants t"
2020.fever-1.7,N18-1074,0,0.38107,"wo modules: (i) a span extractor that aims to identify in the evidence the pieces of relevant information that are informative with respect to the claim; (ii) a classifier that uses the claim, evidence and extracted spans to predict whether the evidence is supporting, refuting or containing insufficient information. The spans extracted by the first module are useful to enhance the classifier and inform the user. Humans can in fact exploit the spans to effectively understand why a claim is true or false. We evaluate our pipeline with three highly performing neural models on the F EVER dataset (Thorne et al., 2018), comparing the uninformed to the informed setting. While this dataset includes Introduction The increased quantity of information that circulates in social media and on the Web every day, together with the high cost of assessing its veracity, has demanded the application of natural language processing (NLP) techniques to the task of fact verification. In the last years, the NLP community has proposed a large number of datasets and approaches for addressing this task, facing complicated challenges that are still far from being solved. The task of fact verification can be split into (i) retriev"
2020.fever-1.7,N19-1423,0,0.277998,"dapted to the task of fact verification (Tokala et al., 2019). BiDAF combines LSTMs with both a context-to-query and query-tocontext attention, to produce a query-aware context representation at multiple hierarchical levels. Nie et al. (2019) introduced the Neural Semantic Matching Networks (NSMNs), which aligns two encoded texts and computes the semantic matching between the aligned representations with LSTMs and used it to earn the first place in the first competitions organized on the F EVER dataset. Soleimani et al. (2019) exploits the contextualized representations of a pre-trained BERT (Devlin et al., 2019) model for both sentence selection and fact verification. Related Work Fake news detection has recently gained interest in the NLP community. Most of the initial works have focused on style (Feng et al., 2012) and linguistic approaches (P´erez-Rosas and Mihalcea, 2015). Despite the good performance in synthetic datasets, these methods failed when applied to real-world data. New approaches based on fact verification over retrieved evidence have therefore taken the stage in the literature. Datasets. Several fact verification datasets were developed over the last decade. Vlachos and Riedel (2014)"
2020.fever-1.7,N19-1230,0,0.0204259,"Missing"
2020.fever-1.7,P12-2034,0,0.0289098,"hical levels. Nie et al. (2019) introduced the Neural Semantic Matching Networks (NSMNs), which aligns two encoded texts and computes the semantic matching between the aligned representations with LSTMs and used it to earn the first place in the first competitions organized on the F EVER dataset. Soleimani et al. (2019) exploits the contextualized representations of a pre-trained BERT (Devlin et al., 2019) model for both sentence selection and fact verification. Related Work Fake news detection has recently gained interest in the NLP community. Most of the initial works have focused on style (Feng et al., 2012) and linguistic approaches (P´erez-Rosas and Mihalcea, 2015). Despite the good performance in synthetic datasets, these methods failed when applied to real-world data. New approaches based on fact verification over retrieved evidence have therefore taken the stage in the literature. Datasets. Several fact verification datasets were developed over the last decade. Vlachos and Riedel (2014) created a dataset which consisted of 221 statements and hyperlinks to pieces of evidence of various formats. Many datasets were created in the following years, with collections of claims of increasing size an"
2020.fever-1.7,W14-2508,0,0.0262939,"ERT (Devlin et al., 2019) model for both sentence selection and fact verification. Related Work Fake news detection has recently gained interest in the NLP community. Most of the initial works have focused on style (Feng et al., 2012) and linguistic approaches (P´erez-Rosas and Mihalcea, 2015). Despite the good performance in synthetic datasets, these methods failed when applied to real-world data. New approaches based on fact verification over retrieved evidence have therefore taken the stage in the literature. Datasets. Several fact verification datasets were developed over the last decade. Vlachos and Riedel (2014) created a dataset which consisted of 221 statements and hyperlinks to pieces of evidence of various formats. Many datasets were created in the following years, with collections of claims of increasing size and various kinds of additional information. Among them Ferreira and Vlachos (2016)’s debunking dataset (300 rumoured claims and 2,595 associated news articles) and Wang (2017)’s L IAR dataset (12,836 short statements labeled for veracity, topic and various metadata on the speaker). In the last years, most systems have been developed over F EVER (Thorne et al., 2018), a large-scale dataset"
2020.fever-1.7,N16-1138,0,0.0240121,"2015). Despite the good performance in synthetic datasets, these methods failed when applied to real-world data. New approaches based on fact verification over retrieved evidence have therefore taken the stage in the literature. Datasets. Several fact verification datasets were developed over the last decade. Vlachos and Riedel (2014) created a dataset which consisted of 221 statements and hyperlinks to pieces of evidence of various formats. Many datasets were created in the following years, with collections of claims of increasing size and various kinds of additional information. Among them Ferreira and Vlachos (2016)’s debunking dataset (300 rumoured claims and 2,595 associated news articles) and Wang (2017)’s L IAR dataset (12,836 short statements labeled for veracity, topic and various metadata on the speaker). In the last years, most systems have been developed over F EVER (Thorne et al., 2018), a large-scale dataset for Fact Extraction and VERification that consists of 185,445 claims and their related evidence, labeled as either supporting, refuting or not containing enough information. Approaches. There has been a large development since the first approaches for fact verification (Ferreira and Vlacho"
2020.fever-1.7,P17-2067,0,0.0963582,"ata. New approaches based on fact verification over retrieved evidence have therefore taken the stage in the literature. Datasets. Several fact verification datasets were developed over the last decade. Vlachos and Riedel (2014) created a dataset which consisted of 221 statements and hyperlinks to pieces of evidence of various formats. Many datasets were created in the following years, with collections of claims of increasing size and various kinds of additional information. Among them Ferreira and Vlachos (2016)’s debunking dataset (300 rumoured claims and 2,595 associated news articles) and Wang (2017)’s L IAR dataset (12,836 short statements labeled for veracity, topic and various metadata on the speaker). In the last years, most systems have been developed over F EVER (Thorne et al., 2018), a large-scale dataset for Fact Extraction and VERification that consists of 185,445 claims and their related evidence, labeled as either supporting, refuting or not containing enough information. Approaches. There has been a large development since the first approaches for fact verification (Ferreira and Vlachos, 2016; Wang, 2017; Long et al., 2017). To provide a strong base3 Method Given a claim C = {"
2020.fever-1.7,N19-1357,0,0.019483,"ication layer which maps the representation of the [CLS] token to the output labels. stead, pretrained on an entailment task over a multigenre corpus (i.e. three-label classification: entailment/neutral/contradiction on the M ULTI NLI dataset (Williams et al., 2018)). The choice of using a rationale-style extractor (Shah et al., 2020) is due to its ability to provide informative spans that can be used as explanations to the relation of the evidence with the claim. This approach was shown to perform better than simply relying on the internal attention weights of a classifier (Lei et al., 2016; Jain and Wallace, 2019). 3.2 Classifiers To test our assumption, we consider three neural network architectures that have achieved the best performance on the first FEVER shared Task recently: BiDAF (Seo et al., 2016b), NSMN (Nie et al., 2019) and BERT (Devlin et al., 2019). Note that the architecture of Mclassifier is independent of Mspan . The spans extracted by Mclassifier are forwarded to the classifier by concatenating them to the original evidence, followed by a separator token. BiDAF consists of four layers: (i) the embedding layer, which encodes two raw text sequences (i.e. ˆ (ii) C and E) into two vector se"
2020.fever-1.7,N18-1101,0,0.0276916,"here each token is replaced with an embedding capturing its relationship with the other words in Ii−1 . The output of enci becomes the input of enci+1 . I0 is set as the concatenation of C and E, preceded by the special [CLS] token. The output of the last encoder enc12 is therefore an highly embedded representation of C and E. It is passed to the classification layer which maps the representation of the [CLS] token to the output labels. stead, pretrained on an entailment task over a multigenre corpus (i.e. three-label classification: entailment/neutral/contradiction on the M ULTI NLI dataset (Williams et al., 2018)). The choice of using a rationale-style extractor (Shah et al., 2020) is due to its ability to provide informative spans that can be used as explanations to the relation of the evidence with the claim. This approach was shown to perform better than simply relying on the internal attention weights of a classifier (Lei et al., 2016; Jain and Wallace, 2019). 3.2 Classifiers To test our assumption, we consider three neural network architectures that have achieved the best performance on the first FEVER shared Task recently: BiDAF (Seo et al., 2016b), NSMN (Nie et al., 2019) and BERT (Devlin et al"
2020.fever-1.7,D16-1011,0,0.13241,"Missing"
2020.fever-1.7,2021.ccl-1.108,0,0.10601,"Missing"
2020.fever-1.7,I17-2043,0,0.0205004,"(300 rumoured claims and 2,595 associated news articles) and Wang (2017)’s L IAR dataset (12,836 short statements labeled for veracity, topic and various metadata on the speaker). In the last years, most systems have been developed over F EVER (Thorne et al., 2018), a large-scale dataset for Fact Extraction and VERification that consists of 185,445 claims and their related evidence, labeled as either supporting, refuting or not containing enough information. Approaches. There has been a large development since the first approaches for fact verification (Ferreira and Vlachos, 2016; Wang, 2017; Long et al., 2017). To provide a strong base3 Method Given a claim C = {c1 , . . . , cn } and a piece of evidence E = {e1 , . . . , em }, two word sequences of length n and m respectively, the fact verification problem requires to predict the relation rel = {(S)upports, (R)efutes, (I)nsufficient} between E and C. Framework. We propose a pipeline of two modules: a span extractor Mspan and a classifier Mclassifier . The goal of Mspan (C, E) is to identify polarizing pieces of information {ei1 , . . . , eiN } in E without which rel(E, C) would be neutral (i.e. C would neither be entailed nor contradicted by E). Th"
2020.fever-1.7,D15-1133,0,0.0479311,"Missing"
2020.lrec-1.700,J10-4006,1,0.940389,"e-filler pairs. McRae and Pad´o include, respectively, 1,444 and 414 scores for agents and patients (e.g., doctor-advise and hit-ball), whereas Ferretti includes judgements for 274 instruments and 248 locations (e.g., cut-mower and teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are"
2020.lrec-1.700,P14-1023,0,0.32911,"e Learning Methods 1. Introduction In recent years, vectors derived from neural network training have quickly replaced the old, count-based Distributional Semantic Models (DSMs) as a de facto standard for word representation in NLP.1 Tools such as Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) have provided the research community with an efficient and scalable method for training vector representations, generally referred to as word embeddings. Moreover, the embeddings have been reported to have an advantage over the old count models also in terms of performance in several NLP tasks (Baroni et al., 2014).2 In this scenario, thematic fit estimation represents an exception. Concretely, the task consists in estimating a typicality score for a filler noun given a verb role (e.g., a system has to predict how plausible a cake is as a patient of the verb to eat). It is generally evaluated by assessing the correlation between collections of human judgements and DSM outputs, and it represents an important benchmark for the capacity of the models of capturing compositional meaning (Lenci, 2018). In a systematic comparison between count-based models and neural embeddings, (Baroni et al., 2014) showed th"
2020.lrec-1.700,Q17-1010,0,0.0525211,"Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the subject for the agent) to extract its typical fillers. The vectors of the typical fillers were then summed to create distributional representations of the prototypical fillers, and the thematic fit of a noun for a role was finally assessed as the cosine similarity between its filler vector and the role prototype. While word embeddings were taking distributional semantics by storm (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), it is surprising that, after the early studies, such vector representations have not been tested anymore on thematic fit. Moreover, the few works were carried out only with the original Word2Vec embeddings, trained on window-based contexts, and were limited to the Continuous-Bag-of-Words (CBOW) tested on two datasets, in which only the agent and the patient roles are represented. To the best of our knowledge, Skip-Gram vectors have never been tested on thematic fit estimation. Finally, the embeddings trained on syntactic dependencies by (Levy and Goldberg, 2014) proved to be efficient in mod"
2020.lrec-1.700,N19-1423,0,0.0438305,"Missing"
2020.lrec-1.700,N15-1003,0,0.655069,"ectively, 1,444 and 414 scores for agents and patients (e.g., doctor-advise and hit-ball), whereas Ferretti includes judgements for 274 instruments and 248 locations (e.g., cut-mower and teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved"
2020.lrec-1.700,D17-1138,0,0.0157339,"tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different: discrete semantic types in the former case, gradient compatibility between arguments and thematic roles in the latter one (Lebani and Lenci, 2018). In the literature using DSMs for modeling thematic fit, the method by (Baroni and Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the subject for the agent) to extract its typical fillers. The vectors of the typic"
2020.lrec-1.700,K19-1050,0,0.0297538,"Missing"
2020.lrec-1.700,S18-2002,0,0.318261,"tti ones, even without syntactic fillers. Our scores were obtained simply by training the model with standard parameters and with no refined context selection. Thus, we conclude that word embeddings are not always a bad fit for the thematic fit task. Given the growing interest for the psychological plausibility of word embeddings and for their performance on cognitively-motivated benchmarks (Søgaard, 2016; Mandera et al., 2017; Bakarov, 2018; Schwartz and Mitchell, 9 It should be noticed that state-of-the-art neural systems for this task are trained on semantic role labels (Tilk et al., 2016; Hong et al., 2018), and thus they avoid -at least in theory- the problem of dealing with the ambiguity of the prepositions. 5711 2019; Hollenstein et al., 2019), future experiments might add thematic fit estimation to the set of tasks in which they could be tested, by carefully taking into account the impact of factors such as the size of training data and the linguistic information available to the models. Another possible direction of work could aim at adapting the recently-introduced contextualized embeddings (Radford et al., 2018; Peters et al., 2018; Devlin et al., 2019) to the task. 6. Acknowledgements We"
2020.lrec-1.700,E17-2063,0,0.240928,"tors have never been tested on thematic fit estimation. Finally, the embeddings trained on syntactic dependencies by (Levy and Goldberg, 2014) proved to be efficient in modeling the functional similarity between words that tend to have the same function or structural role in a sentence, and thus they seem good candidates to perform well in the task. 4 Here, we present a complete evaluation of the abovementioned models on datasets including agents, patients, instruments and locations. Moreover, given the recent claims that incorporating syntax in DSMs does not lead to significant improvements (Lapesa and Evert, 2017), we pay specific attention to a question not addressed yet in the literature: how essential is syntactic information for building good-quality semantic role prototypes? 1 2 Datasets. We tested our models on three standard datasets derived from (McRae et al., 1998), (Ferretti et al., 2001) and (Pad´o, 2007), containing plausibility judgments for AGENTS specific world knowledge in sentence comprehension, as in the psycholinguistic literature of reference. 4 See also (Turney, 2012) for the distinction between functional similarity and domain similarity. PATIENTS Role Figure 3: Scores distributio"
2020.lrec-1.700,P14-2050,0,0.305943,"t models based on count vectors. Despite the progress made in the recent literature and the introduction of several new architectures and improvements, the studies following the first evaluation attempts have only focused on count models. In the present contribution, we propose a more systematic comparison between embeddings and count-based models on thematic fit estimation. Compared to earlier evaluations, which only tested CBOW vectors on agents and patients datasets, we evaluate both the Word2Vec architectures on a wider variety of roles, as well as the dependency-based word embeddings by (Levy and Goldberg, 2014). Additionally, since the best thematic fit models make use of syntactic information to build ’prototypical’ representations of the verb roles, we test the importance of such information for the model performance. 2. Related Work According to a long tradition of psycholinguistic studies, human semantic memory stores a generalized knowledge about events and their participants (McRae et al., 1998; McRae et al., 2005; Hare et al., 2009). The typicality of the combinations of verbs and arguments has important consequences for sentence processing, as typical combinations require less effort from hu"
2020.lrec-1.700,D17-1257,0,0.0228285,"7 0.248 0.203 0.261 Table 4: Spearman correlations for the Instruments and Locations dataset for all models with all filler sets. This is not surprising: DM is a carefully crafted syntactic DSM, and the addition of lexical syntactic patterns has been hypothesized to have a positive impact in this task (Sayeed et al., 2015). However, the less-refined DEPS model perform similarly to the SG embeddings, which in turn perform always better than the CBOW ones. The performance of LG-DEPS is also close to the SG one: syntactic dependencies, as suggested by some recent contributions in the literature (Li et al., 2017; Lapesa and Evert, 2017), do not improve model performance. As for the fillers, syntax seems instead to play an important role: if we compare models with BOWF fillers with those making use of the ”syntactic” sets, we observe large and significant drops for every model on both datasets, to the point that many correlations on McRae become non-significant. DM fillers are clearly better than the DEPS one, suggesting that syntactic information is more useful to select typical contexts. The results for the other roles (Table 4) instead show a clear advantage of the BOW embeddings over count-based m"
2020.lrec-1.700,D14-1162,0,0.087969,"the method by (Baroni and Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the subject for the agent) to extract its typical fillers. The vectors of the typical fillers were then summed to create distributional representations of the prototypical fillers, and the thematic fit of a noun for a role was finally assessed as the cosine similarity between its filler vector and the role prototype. While word embeddings were taking distributional semantics by storm (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), it is surprising that, after the early studies, such vector representations have not been tested anymore on thematic fit. Moreover, the few works were carried out only with the original Word2Vec embeddings, trained on window-based contexts, and were limited to the Continuous-Bag-of-Words (CBOW) tested on two datasets, in which only the agent and the patient roles are represented. To the best of our knowledge, Skip-Gram vectors have never been tested on thematic fit estimation. Finally, the embeddings trained on syntactic dependencies by (Levy and Goldberg, 2014) pro"
2020.lrec-1.700,N18-1202,0,0.107127,"Missing"
2020.lrec-1.700,D16-1099,1,0.843203,"cant. DM fillers are clearly better than the DEPS one, suggesting that syntactic information is more useful to select typical contexts. The results for the other roles (Table 4) instead show a clear advantage of the BOW embeddings over count-based models. SG embeddings are again the best, followed by the CBOW ones, and LG-DEPS vectors, unexpectedly, are again lagging behind their BOW counterparts. Disappointing results for dependency-based embeddings have also been reported by (Gamallo, 2017), in comparison to syntactic count-based vectors. A possible cause, as suggested by (Asr et al., 2016; Sahlgren and Lenci, 2016), could be found in the fact that embedding models are suboptimal when trained on smaller data sizes, and this could be especially true with sparse dependency contexts. It is also interesting to observe that, with instruments and locations, dropping syntactically-selected fillers does not always cause huge correlation drops for the embedding models, and in some cases it even leads to improvements (cf. the SG and CBOW scores for Locations). Probably, using prepositions to select the fillers turns out to be a rough approximation, and the prototypes are so noisy that there are no gains with respe"
2020.lrec-1.700,D17-1068,1,0.923154,"scores for agents and patients (e.g., doctor-advise and hit-ball), whereas Ferretti includes judgements for 274 instruments and 248 locations (e.g., cut-mower and teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different:"
2020.lrec-1.700,W16-2518,0,0.480117,"ce count between a verb v and a filler f and the expected count under independence (the formula is the same for the syntactic method, but adding the third element of the syntactic relation r). For each DSM and filler type, the vectors of the top scoring k fillers for each role were summed to build the prototypes. After testing with k = 10, 20, 30, 40, 50, we observed that the number of fillers did not significantly affect the performance, coherently with the findings of (Greenberg et al., 2015). The reported results have been obtained with k = 20, as in the works by (Baroni et al., 2014) and (Sayeed et al., 2016). The 5 DSMs have been evaluated with all 3 filler types, making 15 different models. Finally, we measured the cosine similarity between roles prototypes and fillers in the datasets, and we computed the Spearman correlation between scores and human judgements. Role Prototypes As in (Baroni and Lenci, 2010), we extract typical fillers for each verb role and sum them to cre5 Ov,f Ev,f LM I(v, f ) = log 4. Results and Discussion The scores for agents and patients datasets and those for instruments and locations follow quite different patterns. In Table 3, DM turns out to be by far the best model"
2020.lrec-1.700,N19-1005,0,0.0434075,"Missing"
2020.lrec-1.700,W16-2521,0,0.012326,"the task, performing similarly to a standard dependency-based model on the Pad´o and McRae datasets (it only lags behind the carefullycrafted DM), and outperforming all count-based competitors on the Ferretti ones, even without syntactic fillers. Our scores were obtained simply by training the model with standard parameters and with no refined context selection. Thus, we conclude that word embeddings are not always a bad fit for the thematic fit task. Given the growing interest for the psychological plausibility of word embeddings and for their performance on cognitively-motivated benchmarks (Søgaard, 2016; Mandera et al., 2017; Bakarov, 2018; Schwartz and Mitchell, 9 It should be noticed that state-of-the-art neural systems for this task are trained on semantic role labels (Tilk et al., 2016; Hong et al., 2018), and thus they avoid -at least in theory- the problem of dealing with the ambiguity of the prepositions. 5711 2019; Hollenstein et al., 2019), future experiments might add thematic fit estimation to the set of tasks in which they could be tested, by carefully taking into account the impact of factors such as the size of training data and the linguistic information available to the model"
2020.lrec-1.700,D16-1017,0,0.374475,"titors on the Ferretti ones, even without syntactic fillers. Our scores were obtained simply by training the model with standard parameters and with no refined context selection. Thus, we conclude that word embeddings are not always a bad fit for the thematic fit task. Given the growing interest for the psychological plausibility of word embeddings and for their performance on cognitively-motivated benchmarks (Søgaard, 2016; Mandera et al., 2017; Bakarov, 2018; Schwartz and Mitchell, 9 It should be noticed that state-of-the-art neural systems for this task are trained on semantic role labels (Tilk et al., 2016; Hong et al., 2018), and thus they avoid -at least in theory- the problem of dealing with the ambiguity of the prepositions. 5711 2019; Hollenstein et al., 2019), future experiments might add thematic fit estimation to the set of tasks in which they could be tested, by carefully taking into account the impact of factors such as the size of training data and the linguistic information available to the models. Another possible direction of work could aim at adapting the recently-introduced contextualized embeddings (Radford et al., 2018; Peters et al., 2018; Devlin et al., 2019) to the task. 6."
2020.lrec-1.700,J13-3006,0,0.0300509,"re tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different: discrete semantic types in the former case, gradient compatibility between arguments and thematic roles in the latter one (Lebani and Lenci, 2018). In the literature using DSMs for modeling thematic fit, the method by (Baroni and Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the su"
2020.lrec-1.700,P19-1071,0,0.421821,"teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different: discrete semantic types in the former case, gradient compatibility between arguments and thematic roles in the latter one (Lebani and Lenci, 2018). In the lite"
2021.cmcl-1.7,C16-1066,0,0.0186748,"s Affair at Styles, for a total of 54, 364 tokens, it contains eye-tracking data from 33 subjects, both English native speakers (14) and bilingual speakers of Dutch and English (19), and comes with the Dutch counterpart. The Provo corpus (Luke and Christianson, 2017) contains 55 short English texts about various topics, with 2.5 sentences and 50 words on average, for a total of 2, 689 tokens, and eye-tracking measures collected from 85 subjects. Annotated eye-tracking corpora are also available for other languages, including German (Kliegl et al., 2006), Hindi (Husain et al., 2015), Japanese (Asahara et al., 2016) and Russian (Laurinavichyute et al., 2019), among others. 3 Feature N F IX FFD GPT TRT FIX P ROP min max mean (std) 0.0 0.0 0.0 0.0 0.0 7.25 296.8 2424.9 996.2 1.0 1.1 (0.7) 77.3 (34.4) 154.1 (143.6) 128.8 (88.6) 0.67 (0.26) Table 1: Minimum, maximum, mean and standard deviation of the feature values before scaling in both training and test data, after averaging across readers. Feature min max mean (std) N F IX FFD GPT TRT FIX P ROP 0.0 0.0 0.0 0.0 0.0 100.0 12.2 100.0 41.1 100.0 15.1 (9.5) 3.2 (1.4) 6.4 (5.9) 5.3 (3.7) 67.1 (26.0) Table 2: Minimum, maximum, mean and standard deviation of the"
2021.cmcl-1.7,2021.cmcl-1.17,0,0.0950795,"Missing"
2021.cmcl-1.7,P16-2094,0,0.0756797,"orpus (ZuCo). Eyetracking data were recorded during natural reading of English sentences. In total, we received submissions from 13 registered teams, whose systems include boosting algorithms with handcrafted features, neural models leveraging transformer language models, or hybrid approaches. The winning system used a range of linguistic and psychometric features in a gradient boosting framework. 1 Figure 1: Example sentence from the ZuCo corpus read by a single reader. The blue dots mark fixations on the corresponding words above, a wider diameter represent a longer fixation duration. data (Barrett et al., 2016; Hollenstein et al., 2019; Toneva and Wehbe, 2019). Thanks to the recent introduction of a standardized dataset (Hollenstein et al., 2018, 2020), it is now possible to compare the capabilities of machine learning approaches to model and analyze human patterns of reading. In this shared task, we present the challenge of predicting eye word-level tracking-based metrics recorded during English sentence processing. We encouraged submissions concerning both cognitive modeling and linguistically motivated approaches (e.g., language models). All data files are available on the competition website.1"
2021.cmcl-1.7,2021.cmcl-1.14,0,0.034899,"osting methods using tree-based algorithms with extensive feature extraction (e.g., CatBoost2 or LightGBM3 ), 7 Results In this section, we describe the prediction performance achieved by the participating teams. The official results of this shared task are presented in Table 3. The best results were achieved by a linguistic feature-based approach (Bestgen, 2021). As described above, other teams opted for neural 2 https://catboost.ai/ https://lightgbm.readthedocs.io/en/ latest/ 3 75 References approaches (e.g., Li and Rudzicz, 2021 and Oh, 2021) or hybrid approaches (e.g., Yu et al., 2021 and Choudhary et al., 2021), combining linguistic features and state-of-the-art language representations. Raksha Agarwal and Niladri Chatterjee. 2021. LangResearchLab_NC at CMCL2021 Shared Task: Predicting Gaze Behaviour using Linguistic Features and Tree Regressors. In Proceedings of the NAACL Workshop on Cognitive Modeling and Computational Linguistics. The difficulty of predicting the individual eyetracking features is analogous in all submitted systems. FFD is the most accurately predicted feature. This seems to suggest that the models are more capable to capture early processing stages of lexical access compared to"
2021.cmcl-1.7,N06-1038,0,0.0202231,"e 2: Boxplot showing the feature value distributions of both training and test sets. Below each box is the median value of each feature. contain any information that can be linked to the participants. The eye-tracking data was recorded with an EyeLink 1000 system in a series of naturalistic reading experiments. Full sentences were presented at the same position on the screen one at a time. The participants read each sentence at their own reading speed. The reading material included sentences from movie reviews from the Stanford Sentiment Treebank (Socher et al., 2013) and a Wikipedia dataset (Culotta et al., 2006). For a detailed description of the data acquisition, please refer to the original publications. An example sentence is presented in Figure 1. We use the normal reading paradigms from ZuCo, i.e, Task 1 and Task 2 from ZuCo 1.0, and all tasks from ZuCo 2.0. We extracted the eyetracking data from all 12 subjects from ZuCo 1.0 and all 18 subjects from ZuCo 2.0. The dataset contains 990 sentences. All sentences were shuffled randomly before splitting into training and test sets. The training data contains 800 sentences, and the test data 190 sentences. 4.1 endings are marked with an &lt;EOS&gt; symbol a"
2021.cmcl-1.7,2021.cmcl-1.13,0,0.0888806,"Missing"
2021.cmcl-1.7,N19-1423,0,0.0272424,"res. 5.2 Features The features included for training the systems include surface features (e.g., word length, sentence length, word positions in the sentence), lexical features (e.g., lemmas, named entities) token probability features (word frequency and ngram metrics), syntactic features (e.g., part-ofspeech tags and dependency parsing), text complexity metrics, behavioral measures, (e.g., concreteness, familiarity, age of acquisition), context features (i.e., information about the preceding and following tokens) as well as representations from state-of-the-art language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019). Mean Baseline We use the mean central tendency as a baseline for this regression problem, i.e., we calculate the mean value for each feature from the training data and use it as a prediction for all words in the test data. Table 3 shows the MAE scores achieved by this mean baseline for each eye-tracking feature. 6 Participating Teams & Systems 13 teams and a total of 42 participants registered on the competition website. All 13 teams, including 26 registered participants, submitted their predictions during the evaluation phase. Each"
2021.cmcl-1.7,W12-1706,0,0.027643,"prisal theory (Hale, 2001; Levy, 2008), which claims that the processing difficulty of a word is proportional to its surprisal, i.e., the negative logarithm of the probabil1 https://competitions.codalab.org/ competitions/28176 72 Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 72–78 Online Event, June 10, 2021. ©2021 Association for Computational Linguistics ity of the word given the context. Surprisal theory was the reference framework for several studies on language models and eye-tracking data prediction (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012). These studies use the data from the Dundee Corpus (Kennedy et al., 2003), which consists of sentences from British newspapers with eye-tracking measurements from 10 participants, as one of the earliest and most popular benchmarks. Later work on the topic found that the perplexity of a language model is the primary factor determining the fit to human reading times (Goodkind and Bicknell, 2018), a result that was confirmed also by the recent investigations involving neural language models such as GRU networks (Aurnhammer and Frank, 2019) and Transformers (Merkx and Frank, 2020; Wilcox et al.,"
2021.cmcl-1.7,2021.cmcl-1.9,0,0.0304026,"he training data and use it as a prediction for all words in the test data. Table 3 shows the MAE scores achieved by this mean baseline for each eye-tracking feature. 6 Participating Teams & Systems 13 teams and a total of 42 participants registered on the competition website. All 13 teams, including 26 registered participants, submitted their predictions during the evaluation phase. Each team was allowed three submissions during the evaluation phase. Finally, 10 teams published system description papers outlining their approach (see Table 3 for all references). Additional data Only one team (Li and Rudzicz, 2021) used external eye-tracking data, leveraging the Provo corpus (Luke and Christianson, 2017) for additional word-level eye movement samples. Methods The participating teams submitted predictions generated from various approaches. Mainly two methods were used: (1) Boosting methods using tree-based algorithms with extensive feature extraction (e.g., CatBoost2 or LightGBM3 ), 7 Results In this section, we describe the prediction performance achieved by the participating teams. The official results of this shared task are presented in Table 3. The best results were achieved by a linguistic feature-"
2021.cmcl-1.7,W18-0102,0,0.0165043,"guistics ity of the word given the context. Surprisal theory was the reference framework for several studies on language models and eye-tracking data prediction (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012). These studies use the data from the Dundee Corpus (Kennedy et al., 2003), which consists of sentences from British newspapers with eye-tracking measurements from 10 participants, as one of the earliest and most popular benchmarks. Later work on the topic found that the perplexity of a language model is the primary factor determining the fit to human reading times (Goodkind and Bicknell, 2018), a result that was confirmed also by the recent investigations involving neural language models such as GRU networks (Aurnhammer and Frank, 2019) and Transformers (Merkx and Frank, 2020; Wilcox et al., 2020; Hao et al., 2020). Using an alternative approach, Bautista and Naval (2020) obtained good results for the prediction of eye movements with autoencoders. In addition to the ZuCo corpus used for this shared task (see Section 4), there are several other resources of eye-tracking data for English. The Ghent Eye-Tracking Corpus (GECO; Cop et al., 2017) is composed of the entire Agatha Christie"
2021.cmcl-1.7,2021.ccl-1.108,0,0.058169,"Missing"
2021.cmcl-1.7,N01-1021,0,0.498484,"ocessing and computer vision. Not only can it reveal the workings of the underlying cognitive processes of language understanding, but the performance of computational models can also be improved if their inductive bias is adjusted using human cognitive signals such as eye-tracking, fMRI, or EEG 2 Related Work Research on naturalistic reading has shown that fixation patterns are influenced by the predictability of words in their sentence context (Ehrlich and Rayner, 1981). In natural language processing and psycholinguistics, the most influential account of the phenomenon is surprisal theory (Hale, 2001; Levy, 2008), which claims that the processing difficulty of a word is proportional to its surprisal, i.e., the negative logarithm of the probabil1 https://competitions.codalab.org/ competitions/28176 72 Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 72–78 Online Event, June 10, 2021. ©2021 Association for Computational Linguistics ity of the word given the context. Surprisal theory was the reference framework for several studies on language models and eye-tracking data prediction (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012). T"
2021.cmcl-1.7,2020.cmcl-1.10,0,0.0217989,"e studies use the data from the Dundee Corpus (Kennedy et al., 2003), which consists of sentences from British newspapers with eye-tracking measurements from 10 participants, as one of the earliest and most popular benchmarks. Later work on the topic found that the perplexity of a language model is the primary factor determining the fit to human reading times (Goodkind and Bicknell, 2018), a result that was confirmed also by the recent investigations involving neural language models such as GRU networks (Aurnhammer and Frank, 2019) and Transformers (Merkx and Frank, 2020; Wilcox et al., 2020; Hao et al., 2020). Using an alternative approach, Bautista and Naval (2020) obtained good results for the prediction of eye movements with autoencoders. In addition to the ZuCo corpus used for this shared task (see Section 4), there are several other resources of eye-tracking data for English. The Ghent Eye-Tracking Corpus (GECO; Cop et al., 2017) is composed of the entire Agatha Christie’s novel The Mysterious Affair at Styles, for a total of 54, 364 tokens, it contains eye-tracking data from 33 subjects, both English native speakers (14) and bilingual speakers of Dutch and English (19), and comes with the Du"
2021.cmcl-1.7,2021.cmcl-1.11,0,0.0337718,"m various approaches. Mainly two methods were used: (1) Boosting methods using tree-based algorithms with extensive feature extraction (e.g., CatBoost2 or LightGBM3 ), 7 Results In this section, we describe the prediction performance achieved by the participating teams. The official results of this shared task are presented in Table 3. The best results were achieved by a linguistic feature-based approach (Bestgen, 2021). As described above, other teams opted for neural 2 https://catboost.ai/ https://lightgbm.readthedocs.io/en/ latest/ 3 75 References approaches (e.g., Li and Rudzicz, 2021 and Oh, 2021) or hybrid approaches (e.g., Yu et al., 2021 and Choudhary et al., 2021), combining linguistic features and state-of-the-art language representations. Raksha Agarwal and Niladri Chatterjee. 2021. LangResearchLab_NC at CMCL2021 Shared Task: Predicting Gaze Behaviour using Linguistic Features and Tree Regressors. In Proceedings of the NAACL Workshop on Cognitive Modeling and Computational Linguistics. The difficulty of predicting the individual eyetracking features is analogous in all submitted systems. FFD is the most accurately predicted feature. This seems to suggest that the models are more"
2021.cmcl-1.7,2021.cmcl-1.12,0,0.0571858,"Missing"
2021.cmcl-1.7,D13-1170,0,0.00305362,"ixProp (a) Training data. (b) Test data. Figure 2: Boxplot showing the feature value distributions of both training and test sets. Below each box is the median value of each feature. contain any information that can be linked to the participants. The eye-tracking data was recorded with an EyeLink 1000 system in a series of naturalistic reading experiments. Full sentences were presented at the same position on the screen one at a time. The participants read each sentence at their own reading speed. The reading material included sentences from movie reviews from the Stanford Sentiment Treebank (Socher et al., 2013) and a Wikipedia dataset (Culotta et al., 2006). For a detailed description of the data acquisition, please refer to the original publications. An example sentence is presented in Figure 1. We use the normal reading paradigms from ZuCo, i.e, Task 1 and Task 2 from ZuCo 1.0, and all tasks from ZuCo 2.0. We extracted the eyetracking data from all 12 subjects from ZuCo 1.0 and all 18 subjects from ZuCo 2.0. The dataset contains 990 sentences. All sentences were shuffled randomly before splitting into training and test sets. The training data contains 800 sentences, and the test data 190 sentences"
2021.cmcl-1.7,2020.lrec-1.18,1,0.7841,"Missing"
2021.cmcl-1.7,2021.cmcl-1.16,0,0.0867029,"Missing"
2021.cmcl-1.7,2021.cmcl-1.15,0,0.0332678,"ds were used: (1) Boosting methods using tree-based algorithms with extensive feature extraction (e.g., CatBoost2 or LightGBM3 ), 7 Results In this section, we describe the prediction performance achieved by the participating teams. The official results of this shared task are presented in Table 3. The best results were achieved by a linguistic feature-based approach (Bestgen, 2021). As described above, other teams opted for neural 2 https://catboost.ai/ https://lightgbm.readthedocs.io/en/ latest/ 3 75 References approaches (e.g., Li and Rudzicz, 2021 and Oh, 2021) or hybrid approaches (e.g., Yu et al., 2021 and Choudhary et al., 2021), combining linguistic features and state-of-the-art language representations. Raksha Agarwal and Niladri Chatterjee. 2021. LangResearchLab_NC at CMCL2021 Shared Task: Predicting Gaze Behaviour using Linguistic Features and Tree Regressors. In Proceedings of the NAACL Workshop on Cognitive Modeling and Computational Linguistics. The difficulty of predicting the individual eyetracking features is analogous in all submitted systems. FFD is the most accurately predicted feature. This seems to suggest that the models are more capable to capture early processing stages o"
2021.eacl-main.149,E17-1014,0,0.0224698,"for tackling the task (Wu et al., 2018; Nikhil and Mundra, 2018), and finally, it was the turn of Transformer-based models such as BERT (Devlin et al., 2019) and BioBERT (Lee et al., 2020), which are the building blocks of most of the top performing systems in the recent competitions (Chen et al., 2019; Mahata et al., 2019; Miftahutdinov et al., 2019). At the same time, the task has been independently tackled also by researchers in Named Entity Recognition, since ADE detection represents a classical case of a challenging task where the entities can be composed by discontinuous spans of text (Stanovsky et al., 2017; Dai et al., 2020; Wunnava et al., 2020). 2.2 Transformers Architectures in NLP There is little doubt that Transformers (Vaswani et al., 2017) have been the dominant class of NLP systems in the last few years. The “golden child” of this revolution is BERT (Devlin et al., 2019), which was the first system to apply the bidirectional training of a Transformer to a language modeling task. More specifically, BERT is trained with a Masked Language Modeling objective: random words in the input sentences are replaced by a [MASK] token and the model attempts to predict the masked token based on the su"
2021.findings-emnlp.289,W18-5446,0,0.0722848,"Missing"
2021.findings-emnlp.289,2021.emnlp-main.552,0,0.0539991,"Missing"
2021.findings-emnlp.300,W19-1909,0,0.0663705,"Missing"
2021.findings-emnlp.300,D19-1371,0,0.0411571,"Missing"
2021.findings-emnlp.300,2020.acl-main.520,0,0.190435,"aset size. Therefore, the proba- access restriction in the platform. Splits are stratbility of drawing a sample from task t is computed ified, to maintain an equal ratio of positive and t ,Nt ) as θt = Pmin(γ , where N corresponds to negative examples (see Table 2). t t min(γt ,Nt ) 3537 CADEC CADEC contains 1,250 medical forum posts annotated with patient-reported AEs. In this dataset, texts are long and informal, often deviating from English syntax and punctuation rules. Forum posts may contain more than one AE. For our goals, we adopted the training, validation, and test splits proposed by Dai et al. (2020) (see Table 2). SMM4H-French The SMM4H French Dataset contains a total of 1,941 samples out of which 31 samples belong to AE (positive) class and 1,910 samples have the label Non-AE (negative class). This dataset is only used for testing the zero-shot transfer (see Table 2). 4.2 ADE corpus v2 This dataset (Gurulingappa et al., 2012) contains case reports extracted from MEDLINE and it was used for multi-task training, as it contains annotations for all tasks in Table 1, i.e. drugs, dosage, AE detection and extraction. Splits are stratified, to maintain an equal ratio of positive and negative ex"
2021.findings-emnlp.300,W18-5913,0,0.0339462,"Missing"
2021.findings-emnlp.300,K16-1028,0,0.0233522,"yi . Finally, the output of the decoder passes through a SoftMax layer over the vocabulary. Raffel et al. (2019) proposed to add a prefix in front of the input sequence to inform the model about which task to perform (e.g. summarization, question answering, classification etc.; see Figure 1).The model was trained on the Colossal Clean Crawled Corpus (C4), a massive corpus (about 750 GB) of web-extracted and cleaned text. GLUE (Wang et al., 2018) and the SuperGLUE (Wang et al., 2019) benchmarks for natural language understanding, the abstractive summarization data by Hermann et al. (2015) and Nallapati et al. (2016), the SQUAD question answering dataset (Rajpurkar et al., 2016) and the WMT translation benchmarks for translation from English to French, from English to German and from English to Romanian. The tasks were all treated as a single task in the sequence-to-sequence format, by concatenating all the datasets together and appending the task-specific prefixes to the instances. T5 comes in versions, small (60 million parameters), base (220 million parameters), large (770 million parametrs), 3B (3 billion parameters) and 11B (11 billion parameters). In the paper we will use the term T5 to either refer"
2021.findings-emnlp.300,2021.naacl-main.200,0,0.0576618,"Missing"
2021.findings-emnlp.300,2021.eacl-main.149,1,0.720839,"on 1.14 million papers, randomly selected from semantic scholar, with an 18-82 ratio between computer science and biomedical papers. PubMedBERT (Gu et al., 2020) was pre-trained 3538 from scratch on PubMed abstracts, without building upon the vocabulary of the original BERT. SpanBERT (Joshi et al., 2020) adopts a different pre-training objective from BERT. This model is trained by masking full contiguous spans instead of single words or subwords, which allows it to encode span-level information. 4.3.2 AE Extraction Task Baselines For the AE EXTRACTION task, we use the four models described in Portelli et al. (2021a), namely BERT, BERT+CRF, SpanBERT, and SpanBERT+CRF. The authors reported state-ofthe-art performance with the SpanBERT models on SMM4H, and their implementation is publicly available at https://github.com/ ailabUdineGit/ADE. 4.3.3 Multi-Task Learning For Multi-Task Learning, we use as baseline the T5 model fine-tuned with the original training strategies by Raffel et al. (2019), which balance across tasks (TB, task balancing) but do not account for multi-dataset learning (DB, dataset balancing). We refer to them as T5TB-PM for proportional mixing and T5TB-TS for temperature scaling. We refe"
2021.findings-emnlp.300,D16-1264,0,0.0470087,"ax layer over the vocabulary. Raffel et al. (2019) proposed to add a prefix in front of the input sequence to inform the model about which task to perform (e.g. summarization, question answering, classification etc.; see Figure 1).The model was trained on the Colossal Clean Crawled Corpus (C4), a massive corpus (about 750 GB) of web-extracted and cleaned text. GLUE (Wang et al., 2018) and the SuperGLUE (Wang et al., 2019) benchmarks for natural language understanding, the abstractive summarization data by Hermann et al. (2015) and Nallapati et al. (2016), the SQUAD question answering dataset (Rajpurkar et al., 2016) and the WMT translation benchmarks for translation from English to French, from English to German and from English to Romanian. The tasks were all treated as a single task in the sequence-to-sequence format, by concatenating all the datasets together and appending the task-specific prefixes to the instances. T5 comes in versions, small (60 million parameters), base (220 million parameters), large (770 million parametrs), 3B (3 billion parameters) and 11B (11 billion parameters). In the paper we will use the term T5 to either refer to the architecture or to the T5-Base, as opposed to T5-Small,"
2021.findings-emnlp.300,W18-5446,0,0.0628041,"Missing"
2021.findings-emnlp.300,W19-3203,0,0.052335,"Missing"
2021.findings-emnlp.300,W18-5904,0,0.0597,"r balancing data across tasks and datasets in a multi-task setting, which leads to F1-score improvements on all benchmarks; iii) we test our model in a crosslingual transfer (English to French) scenario, showing that it outperforms Multilingual BERT in zero-shot learning. AE detection from social media data used a combination of various classifiers along with word embeddings as features (Sarker and Gonzalez, 2015; Nikfarjam et al., 2015; Daniulaityte et al., 2016; Metke-Jimenez and Karimi, 2016). After the introduction of challenges such as Social Media Mining for Health Applications (SMM4H) (Weissenbacher et al., 2018, 2019) and CADEC (Karimi et al., 2015), most works focused on neural networks (Sarker et al., 2018; Minard et al., 2018). With the development of attention mechanism (Vaswani et al., 2017), Transformerbased language models such as BERT (Devlin et al., 2019) and its biomedical (e.g., BioBERT (Lee et al., 2020), ClinicalBERT (Alsentzer et al., 2019) and PubMedBERT (Gu et al., 2020)) and non-biomedical variants (e.g., SpanBERT (Joshi et al., 2020)) obtained state-of-the-art performance in AE detection (Weissenbacher et al., 2019; Klein et al., 2020; Portelli et al., 2021a,b). Models like BERT an"
2021.starsem-1.1,J10-4006,1,0.542773,"mitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by"
2021.starsem-1.1,W15-1106,0,0.0158776,"rchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by the tuple (cf. Table 1). The dataset includes tuples of different lengt"
2021.starsem-1.1,N15-1003,0,0.0147145,"rchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by the tuple (cf. Table 1). The dataset includes tuples of different lengt"
2021.starsem-1.1,W16-4102,1,0.847282,"port) and the other sentence expressing a plausible but less typical one (The mechanic is checking the report), and the task is to assign a higher thematic fit/typicality score to the former. Notice that the two sentences differ only for one argument, and that the “atypical” one might, however, be a common filler with respect to the verb target role (e.g., report is a typical patient for check, it is just less plausible in combination with mechanic as an agent). Several models have tried to tackle the “dynamic” version of the thematic fit task, either based on classical distributional spaces (Chersoni et al., 2016, 2019) or on more sophisticated neural network architectures (Tilk et al., 2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physic"
2021.starsem-1.1,S18-2002,0,0.0120126,"report), and the task is to assign a higher thematic fit/typicality score to the former. Notice that the two sentences differ only for one argument, and that the “atypical” one might, however, be a common filler with respect to the verb target role (e.g., report is a typical patient for check, it is just less plausible in combination with mechanic as an agent). Several models have tried to tackle the “dynamic” version of the thematic fit task, either based on classical distributional spaces (Chersoni et al., 2016, 2019) or on more sophisticated neural network architectures (Tilk et al., 2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physical properties of the participants) (Wang et al., 2018; Porada et al., 2019; Ko et al., 2019). Contri"
2021.starsem-1.1,S17-1021,1,0.855068,"optimal generalization abilities. 1 Introduction People can discriminate between typical (e.g., A cop arrested a thief ) and atypical events (e.g., A thief arrested a cop) and exploit this ability in online sentence processing to anticipate the upcoming linguistic input. Brains have been claimed to be “prediction machines” (Clark, 2013) and psycholinguistic research has shown that a crucial ingredient 1 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 1–11 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics from corpora (Chersoni et al., 2017, 2021). Language Models are trained to make predictions given a context, and thus, they can also be viewed as models of GEK. This approach is promising if one considers the success of recent Transformer-based Language Models (henceforth TLM S), which are trained on huge corpora and contain a massive number of parameters. Even if these models receive extensive training and have been shown to capture linguistic properties (Jawahar et al., 2019; Goldberg, 2019), it is not obvious whether they acquire the aspects of GEK that have been modeled explicitly in previous approaches. To the best of our"
2021.starsem-1.1,2020.lrec-1.700,1,0.716625,"well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by the tuple (cf. Table 1). The dataset includes tuples of different lengths, and the typicality of a given argument depends on its interaction"
2021.starsem-1.1,N19-1349,0,0.0169179,"2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physical properties of the participants) (Wang et al., 2018; Porada et al., 2019; Ko et al., 2019). Contributions: 1. we propose a methodology to adapt TLM S to the dynamic estimation of thematic fit, using a dataset that contains several types of argument combinations differing for their typicality; 2. we present a comprehensive evaluation of various TLM S on this task, performed by comparing them to a strong distributional baseline; 3. we conduct further analysis aimed at identifying the potential limitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM"
2021.starsem-1.1,W11-0607,1,0.831728,"2011) have shown that humans are able to combine and dynamically update their expectations during sentence processing: for example, their expectations given the sequence The barber cut the differ from the ones given The lumberjack cut the , since the integration of knowledge “cued” by the agent argument with the verb will lead to the activation of different event scenarios. In Distributional Semantics, sophisticated models of the GEK have been proposed to make predictions on upcoming arguments by integrating the cues coming from the verb and the previously-realized arguments in the sentence (Lenci, 2011; Chersoni et al., 2019). Since such knowledge is acquired from both first-hand and linguistic experience (McRae and Matsuki, 2009), an important assumption of this literature is that, at least for its ”linguistic subset”, the GEK can be modeled with distributional information extracted Given the recent success of Transformers Language Models (TLMs), we decided to test them on a benchmark for the dynamic estimation of thematic fit. The evaluation of these models was performed in comparison with SDM, a framework specifically designed to integrate events in sentence meaning representations, and"
2021.starsem-1.1,2021.ccl-1.108,0,0.0279667,"Missing"
2021.starsem-1.1,N19-1423,0,0.016492,"the models that we used, we decided to disregard its tuple and the respective typical/atypical counterpart. For this reason, the final results only take in consideration a subset of the original datasets, which varies from model to model. Additionally, we computed a baseline for each Transformer model, where the model is prevented from attending to the other tokens in the sequence when making predictions. 3.2.2 Transformer-based Language Models We experimented with four TLM S to test how different architectures, training objectives, and sizes of the training corpus affect performance.3 BERT (Devlin et al., 2019) consists of a series of stacked Transformer encoders. It was trained using both a masked language modeling objective (i.e., 2 https://fasttext.cc/docs/en/ english-vectors.html 3 For all experiments involving TLM S, we use pre-trained models available in the HuggingFace’s Python library Transformers (Wolf et al., 2019). 4 Coverage SDM BERT-base(line) BERT-large ROBERTA-large GPT-2 medium AgentDTFit 105/134 0.58 0.46 (0.1) 0.53 0.64 PatientDTFit 323/402 0.62 0.59 (0.06) 0.64 0.64 0.63 InstrumentDTFit 31/100 0.58 0.52 (0.08) 0.53 0.5 0.5 TimeDTFit 89/100 0.58 0.63 (0.06) 0.64 0.66 0.66 LocationD"
2021.starsem-1.1,P07-1028,0,0.0615047,"a strong distributional baseline; 3. we conduct further analysis aimed at identifying the potential limitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that diffe"
2021.starsem-1.1,N18-2049,0,0.347046,"ral network architectures (Tilk et al., 2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physical properties of the participants) (Wang et al., 2018; Porada et al., 2019; Ko et al., 2019). Contributions: 1. we propose a methodology to adapt TLM S to the dynamic estimation of thematic fit, using a dataset that contains several types of argument combinations differing for their typicality; 2. we present a comprehensive evaluation of various TLM S on this task, performed by comparing them to a strong distributional baseline; 3. we conduct further analysis aimed at identifying the potential limitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as thos"
2021.starsem-1.1,2020.coling-main.109,0,0.0529502,"Missing"
2021.starsem-1.1,P19-1071,0,0.0142322,"dberg, 2019), it is not obvious whether they acquire the aspects of GEK that have been modeled explicitly in previous approaches. To the best of our knowledge, Transformers have never been tested on dynamic thematic fit modeling, nor their performance has been compared with traditional distributional models. Our current work is addressing this issue. 2020). Performance in the thematic fit task is typically measured with the correlation between the output scores of the model and human-elicited typicality judgments for verb-argument pairs (McRae et al., 1998; Ferretti et al., 2001; Pad´o, 2007; Zhang et al., 2019; Marton and Sayeed, 2021). In the simplest and most common version of this task, the typicality of verb argument-pairs is evaluated in isolation. Thematic fit is instead a dynamic concept: The expectations for an argument in a given verb role do not depend just on the verb, but also on the compositional combination with the other arguments in the sentence (Bicknell et al., 2010). To check the ability of computational models to account for the compositional update of argument expectations, Lenci (2011) framed the problem as a binary classification task: A system is presented a sentence pair, w"
2021.starsem-1.1,D19-6015,0,0.0193332,"ctures (Tilk et al., 2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physical properties of the participants) (Wang et al., 2018; Porada et al., 2019; Ko et al., 2019). Contributions: 1. we propose a methodology to adapt TLM S to the dynamic estimation of thematic fit, using a dataset that contains several types of argument combinations differing for their typicality; 2. we present a comprehensive evaluation of various TLM S on this task, performed by comparing them to a strong distributional baseline; 3. we conduct further analysis aimed at identifying the potential limitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as those working on applicat"
2021.starsem-1.1,2020.aacl-main.26,1,0.745247,"ected by these variations by design, since its predictions are based on semantic roles derived from the syntactic analysis of the sentence, which is explicitly provided to the model. 6 designed for this task, DTFit. Results show that TLM S scores positively correlate with human judgments. However, they do not significantly outperform the distributional prototype-based model (SDM) that we selected for comparison. This confirms the ability of SDM to dynamically update the semantic representation of a sentence, which was recently shown for the challenging task of logical metonymy interpretation (Rambelli et al., 2020). However, we decided to go beyond the simple evaluation against human judgments. We carried out several additional small-scale experiments with the specific aim to understand which factors could affect the predictions of TLM S. The results suggest that models are often too dependent on what they observe during training and lack some key aspects of human event knowledge. In particular, we observed that, in some cases, they are unable to compose all elements of the input to make predictions, and they tend to rely more on salient local associations between words. However, further analysis is nee"
2021.starsem-1.1,D17-1068,1,0.776185,"ilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by the tuple (cf. Table 1). The dataset includes tuples of different lengths, and the typicality of a given argument de"
2021.starsem-1.1,W16-2518,0,0.014262,"ing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by the tuple (cf. Table 1). The dataset includes tuples of different lengths, and the typicality o"
2021.starsem-1.1,D16-1017,0,0.0195321,"ic is checking the report), and the task is to assign a higher thematic fit/typicality score to the former. Notice that the two sentences differ only for one argument, and that the “atypical” one might, however, be a common filler with respect to the verb target role (e.g., report is a typical patient for check, it is just less plausible in combination with mechanic as an agent). Several models have tried to tackle the “dynamic” version of the thematic fit task, either based on classical distributional spaces (Chersoni et al., 2016, 2019) or on more sophisticated neural network architectures (Tilk et al., 2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physical properties of the participants) (Wang et al., 2018; Porada et al., 2019; Ko e"
2021.starsem-1.1,E09-1094,0,0.0557104,"stributional baseline; 3. we conduct further analysis aimed at identifying the potential limitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which"
2021.tacl-1.5,C14-1218,0,0.0202342,"arge amounts of monolingual data. The size of data for both languages is key: High-quality monolingual embeddings are required for successful matching. This assumption, however, does not hold for ancient languages, where we can typically access a few thousands of words at most. Decoding Cipher Texts Man-made ciphers have been the focal point for most of the early work on decipherment. They usually use EM algorithms, which are tailored towards these specific types of ciphers, most prominently substitution ciphers (Knight and Yamada, 1999; Knight et al., 2006). Later work by Nuhn et al. (2013), Hauer et al. (2014), and Kambhatla et al. (2018) addresses the problem using a heuristic search procedure, guided by a pretrained language model. To the best of our knowledge, these methods developed for tackling man-made ciphers have so far not been successfully applied to archaeological data. One contributing factor could be the inherent complexity in the evolution of natural languages. Deciphering Ancient Scripts Our research is most closely aligned with computational decipherment of ancient scripts. Prior work has already featured several successful instances of ancient language decipherment previously done"
2021.tacl-1.5,D18-1102,0,0.0130824,"al data. The size of data for both languages is key: High-quality monolingual embeddings are required for successful matching. This assumption, however, does not hold for ancient languages, where we can typically access a few thousands of words at most. Decoding Cipher Texts Man-made ciphers have been the focal point for most of the early work on decipherment. They usually use EM algorithms, which are tailored towards these specific types of ciphers, most prominently substitution ciphers (Knight and Yamada, 1999; Knight et al., 2006). Later work by Nuhn et al. (2013), Hauer et al. (2014), and Kambhatla et al. (2018) addresses the problem using a heuristic search procedure, guided by a pretrained language model. To the best of our knowledge, these methods developed for tackling man-made ciphers have so far not been successfully applied to archaeological data. One contributing factor could be the inherent complexity in the evolution of natural languages. Deciphering Ancient Scripts Our research is most closely aligned with computational decipherment of ancient scripts. Prior work has already featured several successful instances of ancient language decipherment previously done by human experts (Snyder et a"
2021.tacl-1.5,D13-1087,0,0.0197793,"the problem using a heuristic search procedure, guided by a pretrained language model. To the best of our knowledge, these methods developed for tackling man-made ciphers have so far not been successfully applied to archaeological data. One contributing factor could be the inherent complexity in the evolution of natural languages. Deciphering Ancient Scripts Our research is most closely aligned with computational decipherment of ancient scripts. Prior work has already featured several successful instances of ancient language decipherment previously done by human experts (Snyder et al., 2010; Berg-Kirkpatrick and Klein, 2013; Luo et al., 2019). Our work incorporates many linguistic insights about the structure of valid alignments introduced in prior work, such as monotonicity. We further expand the linguistic foundation by incorporating phonetic regularities that have been beneficial in early, pre-neural decipherment work (Knight et al., 70 2006). However, our model is designed to handle challenging cases not addressed by prior work, where segmentation of the ancient scripts is unknown. Moreover, we are interested in dead languages without a known relative and introduce an unsupervised measure of language closene"
2021.tacl-1.5,P10-1107,1,0.554942,"al. (2018) addresses the problem using a heuristic search procedure, guided by a pretrained language model. To the best of our knowledge, these methods developed for tackling man-made ciphers have so far not been successfully applied to archaeological data. One contributing factor could be the inherent complexity in the evolution of natural languages. Deciphering Ancient Scripts Our research is most closely aligned with computational decipherment of ancient scripts. Prior work has already featured several successful instances of ancient language decipherment previously done by human experts (Snyder et al., 2010; Berg-Kirkpatrick and Klein, 2013; Luo et al., 2019). Our work incorporates many linguistic insights about the structure of valid alignments introduced in prior work, such as monotonicity. We further expand the linguistic foundation by incorporating phonetic regularities that have been beneficial in early, pre-neural decipherment work (Knight et al., 70 2006). However, our model is designed to handle challenging cases not addressed by prior work, where segmentation of the ancient scripts is unknown. Moreover, we are interested in dead languages without a known relative and introduce an unsupe"
2021.tacl-1.5,L18-1429,0,0.0580564,"Missing"
2021.tacl-1.5,P13-1154,0,0.0315018,"s constructed from large amounts of monolingual data. The size of data for both languages is key: High-quality monolingual embeddings are required for successful matching. This assumption, however, does not hold for ancient languages, where we can typically access a few thousands of words at most. Decoding Cipher Texts Man-made ciphers have been the focal point for most of the early work on decipherment. They usually use EM algorithms, which are tailored towards these specific types of ciphers, most prominently substitution ciphers (Knight and Yamada, 1999; Knight et al., 2006). Later work by Nuhn et al. (2013), Hauer et al. (2014), and Kambhatla et al. (2018) addresses the problem using a heuristic search procedure, guided by a pretrained language model. To the best of our knowledge, these methods developed for tackling man-made ciphers have so far not been successfully applied to archaeological data. One contributing factor could be the inherent complexity in the evolution of natural languages. Deciphering Ancient Scripts Our research is most closely aligned with computational decipherment of ancient scripts. Prior work has already featured several successful instances of ancient language decipher"
D16-1205,N09-1003,0,0.141123,"Missing"
D16-1205,J10-4006,1,0.880419,"sts that verbs activate expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010). Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998), supports the idea of a mental lexicon arranged as a web of mutual expectations. Some past works in computational linguistics (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015) modeled thematic fit estimations by means of dependencybased or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants, but they do not encode directly the relation between participants co-occurring in the same event. Another trend of studies in the NLP community aimed at the introduction of richer contextual features i"
D16-1205,J90-1003,0,0.591713,"ical association measure. 4 4.1 Evaluation Corpus and DSMs We trained our DSMs on the RCV1 corpus, which contains approximately 150 million words (Lewis et al., 2004). The corpus was tagged with the tagger described in Dell’Orletta (2009) and dependencyparsed with DeSR (Attardi et al., 2009). RCV1 was chosen for two reasons: i) to show that our joint context-based representation can deal with data 1969 sparseness even with a training corpus of limited size; ii) to allow a comparison with the results reported by Melamud et al. (2014). All DSMs adopt Positive Pointwise Mutual Information (PPMI; Church and Hanks (1990)) as a context weighting scheme and vary according to three main parameters: i) type of contexts; ii) number of dimensions; iii) application of Singular Value Decomposition (SVD; see Landauer et al. (1998)). For what concerns the first parameter, we developed three types of DSMs: a) traditional bag-ofwords DSMs, where the features are content words co-occurring with the target in a window of width 2; b) dependency-based DSMs, where the features are words in a direct dependency relation with the target; c) joint context-based DSMs, using the joint features described in the previous section. The"
D16-1205,N15-1003,0,0.0186207,"nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010). Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998), supports the idea of a mental lexicon arranged as a web of mutual expectations. Some past works in computational linguistics (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015) modeled thematic fit estimations by means of dependencybased or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants, but they do not encode directly the relation between participants co-occurring in the same event. Another trend of studies in the NLP community aimed at the introduction of richer contextual features in DSMs, mostly based on word windows. The first example was the"
D16-1205,J15-4004,0,0.051174,"l. (2014) have proposed the Probabilistic Distributional Similarity (PDS), based on the intuition that two words, w1 and w2 , are similar if they are likely to occur in each other’s contexts. PDS assigns a high similarity score when both p(w1 |contexts of w2 ) and p(w2 | contexts of w1 ) are high. We tried to test variations of this measure with our representation, but we were not able to achieve satisfying results. Therefore, we report here only the scores with the cosine. 4.3 Datasets The DSMs are evaluated on two test sets: VerbSim (Yang and Powers, 2006) and the verb subset of SimLex-999 (Hill et al., 2015). The former includes 130 verb pairs, while the latter includes 222 verb pairs. Both datasets are annotated with similarity judgements, so we measured the Spearman correlation between them and the scores assigned by the model. The VerbSim dataset allows for comparison with Melamud et al. (2014), since they also evaluated their model on this test set, achieving a Spearman correlation score of 0.616 and outperforming all the baseline methods. The verb subset of SimLex-999, at the best of our knowledge, has never been used as a benchmark dataset for verb similarity. The SimLex dataset is known fo"
D16-1205,W11-0607,1,0.892111,"expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010). Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998), supports the idea of a mental lexicon arranged as a web of mutual expectations. Some past works in computational linguistics (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015) modeled thematic fit estimations by means of dependencybased or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants, but they do not encode directly the relation between participants co-occurring in the same event. Another trend of studies in the NLP community aimed at the introduction of richer contextual features in DSMs, mostl"
D16-1205,W14-1619,0,0.128219,"measure such proximity, even though other measures have been proposed (Weeds et al., 2004; Santus et al., 2016). Most of DSMs adopt a bag-of-words approach, that is they turn a text span (i.e., a word window or a parsed sentence) into a set of words and they register separately the co-occurrence of each word with a given target. The problem with this approach is that valuable information concerning word interrelations in a context gets lost, because words co-occurring with a target are treated as independent features. This is why works like Ruiz-Casado et al. (2005), Agirre et al. (2009) and Melamud et al. (2014) proposed to introduce richer contexts in distributional spaces, by using entire word windows as features. These richer contexts proved to be helpful to semantically represent verbs, which are characterized by highly context-sensitive meanings, and complex argument structures. In fact, two verbs may share independent words as features despite being very dissimilar from the semantic point of view. For instance kill and heal share the same object nouns in The doctor healed the patient and the The poison killed the patient, but are highly different if we consider their joint dependencies as a sin"
D16-1205,Y16-2021,1,0.794205,"pus. 1 Introduction Distributional Semantic Models (DSMs) rely on the Distributional Hypothesis (Harris, 1954; Sahlgren, 2008), stating that words occurring in similar contexts have similar meanings. On such theoretical grounds, word co-occurrences extracted from corpora are used to build semantic representations in the form of vectors, which have become very popular in the NLP community. Proximity between word vectors is taken as an index of meaning similarity, and vector cosine is generally adopted to measure such proximity, even though other measures have been proposed (Weeds et al., 2004; Santus et al., 2016). Most of DSMs adopt a bag-of-words approach, that is they turn a text span (i.e., a word window or a parsed sentence) into a set of words and they register separately the co-occurrence of each word with a given target. The problem with this approach is that valuable information concerning word interrelations in a context gets lost, because words co-occurring with a target are treated as independent features. This is why works like Ruiz-Casado et al. (2005), Agirre et al. (2009) and Melamud et al. (2014) proposed to introduce richer contexts in distributional spaces, by using entire word windo"
D16-1205,C04-1146,0,0.156904,"Missing"
D16-1205,N15-1050,0,\N,Missing
D17-1068,W16-2506,0,0.0164198,"ler vectors for that verb-specific role. Differently from Baroni and Lenci, the core and novel aspect of our proposal, described in the following subsections, is that we do not simply measure the correlation between all the features of candidate and prototype vectors (as vector cosine would do on unsorted vectors), but rather we rank and filter the features, computing the weighted overlap with a rank-based similarity measure inspired by AP Syn, a recent proposal by Santus 5 For an overview on the limitations of vector cosine, see: Li and Han (2013); Dinu et al. (2015); Schnabel et al. (2015); Faruqui et al. (2016); Santus et al. (2016a). 6 An early proposal going in this direction is the predication theory by Kintsch (2001), which exploited Latent Semantic Analysis to select only the vector features that are appropriate for predicate-argument composition. 7 See also the so-called ’strong version’ of the Distributional Hypothesis (Miller and Charles, 1991; Lenci, 2008). 650 et al. (2016a,b,c) which has shown interesting results in synonymy detection and similarity estimation. As we will show in the next sections, the new metric assigns high scores to candidate fillers sharing many salient contexts with"
D17-1068,W15-1106,0,0.649512,"redictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009). Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space”. The present wo"
D17-1068,P14-1023,0,0.20658,"several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space”. The present work sets itself among the unsupervised approaches to thematic fit estimation. By relying on explicit and interpretable count-based vector representations, we propose a simple, cognitively-inspired, and efficient thematic fit model using information extracted from dependency-pa"
D17-1068,N15-1003,0,0.495651,"redictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009). Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space”. The present wo"
D17-1068,J10-4006,1,0.0718801,"tensively used in sentence comprehension studies on constraint-based models, mainly as a predictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009). Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularl"
D17-1068,W11-0607,1,0.856257,"ce comprehension studies on constraint-based models, mainly as a predictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009). Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive t"
D17-1068,P07-1028,0,0.0834578,"ning a performance comparable to Lenci (2011). Related Work Erk et al. (2010) were, at the best of our knowledge, the first authors to measure the correlation between human-elicited thematic fit ratings and the scores assigned by a syntax-based Distributional Semantic Model (DSM). More specifically, their gold standard consisted of the human judgments collected by McRae et al. (1998) and Pad´o (2007). The plausibility of each verb-filler pair was computed as the similarity between new candidate nouns and previously attested exemplars for each specific verb-role pairing (as already proposed in Erk (2007)). Baroni and Lenci (2010) evaluated their Distributional Memory (henceforth DM)4 framework on the same datasets, adopting an approach to the task that has become dominant in the literature: for each verb role, they built a prototype vector by averaging the dependency-based vectors of its most typical fillers. The higher the similarity of a noun with a role prototype, the higher its plausibility as a filler for that role. Lenci (2011) has later extended the model to account for the dynamic update of the expectations on an argument, depending on how another role is filled. By using the same DM"
D17-1068,J10-4007,0,0.269326,"Missing"
D17-1068,W16-2518,0,0.70671,"have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space”. The present work sets itself among the unsupervised approaches to thematic fit estimation. By relying on explicit and interpretable count-based vector representations, we propose a simple, cognitively-inspired, and efficient thematic fit model using information extracted from dependency-parsed corpora. The key features of our proposal are a) prototypical representations of verb-specific thematic roles, based on feature weighting and filtering of second order cont"
D17-1068,D15-1036,0,0.0134542,"of the most typical filler vectors for that verb-specific role. Differently from Baroni and Lenci, the core and novel aspect of our proposal, described in the following subsections, is that we do not simply measure the correlation between all the features of candidate and prototype vectors (as vector cosine would do on unsorted vectors), but rather we rank and filter the features, computing the weighted overlap with a rank-based similarity measure inspired by AP Syn, a recent proposal by Santus 5 For an overview on the limitations of vector cosine, see: Li and Han (2013); Dinu et al. (2015); Schnabel et al. (2015); Faruqui et al. (2016); Santus et al. (2016a). 6 An early proposal going in this direction is the predication theory by Kintsch (2001), which exploited Latent Semantic Analysis to select only the vector features that are appropriate for predicate-argument composition. 7 See also the so-called ’strong version’ of the Distributional Hypothesis (Miller and Charles, 1991; Lenci, 2008). 650 et al. (2016a,b,c) which has shown interesting results in synonymy detection and similarity estimation. As we will show in the next sections, the new metric assigns high scores to candidate fillers sharing many"
D17-1068,D16-1017,0,0.762572,"s they computed the scores also for the instruments and for the locations of the Ferretti datasets (Ferretti et al., 2001). Greenberg et al. (2015a,b) further developed the TypeDM and the role-based models, investigating the effects of verb polysemy on human thematic fit judgments and introducing a hierarchical agglomerative clustering algorithm into the prototype creation process. Their goal was to cluster together typical fillers into multiple prototypes, corresponding to different verb senses, and their results showed constant improvements of the performance of the DM-based model. Finally, Tilk et al. (2016) presented two neural network architectures for generating probability distributions over selectional preferences for each thematic role. Their models took advantage of supervised training on two role-labeled corpora to optimize the distributional representation for thematic fit modeling, and managed to obtain significant improvements over the other systems on almost all the evaluation datasets. They also evaluated their model on the task of composing and updating verb argument expectations, obtaining a performance comparable to Lenci (2011). Related Work Erk et al. (2010) were, at the best of"
D17-1068,E09-1094,0,0.123359,"rlap Enrico Santus1 , Emmanuele Chersoni2 , Alessandro Lenci3 and Philippe Blache2 enrico santus@sutd.edu.sg emmanuelechersoni@gmail.com alessandro.lenci@unipi.it philippe.blache@univ-amu.fr 1 Singapore University of Technology and Design 2 Aix-Marseille University 3 University of Pisa Abstract has been extensively used in sentence comprehension studies on constraint-based models, mainly as a predictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009). Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit"
D17-1068,Y16-2021,1,0.913379,"rb-specific role. Differently from Baroni and Lenci, the core and novel aspect of our proposal, described in the following subsections, is that we do not simply measure the correlation between all the features of candidate and prototype vectors (as vector cosine would do on unsorted vectors), but rather we rank and filter the features, computing the weighted overlap with a rank-based similarity measure inspired by AP Syn, a recent proposal by Santus 5 For an overview on the limitations of vector cosine, see: Li and Han (2013); Dinu et al. (2015); Schnabel et al. (2015); Faruqui et al. (2016); Santus et al. (2016a). 6 An early proposal going in this direction is the predication theory by Kintsch (2001), which exploited Latent Semantic Analysis to select only the vector features that are appropriate for predicate-argument composition. 7 See also the so-called ’strong version’ of the Distributional Hypothesis (Miller and Charles, 1991; Lenci, 2008). 650 et al. (2016a,b,c) which has shown interesting results in synonymy detection and similarity estimation. As we will show in the next sections, the new metric assigns high scores to candidate fillers sharing many salient contexts with the verb-specific rol"
D17-1068,L16-1723,1,0.934831,"rb-specific role. Differently from Baroni and Lenci, the core and novel aspect of our proposal, described in the following subsections, is that we do not simply measure the correlation between all the features of candidate and prototype vectors (as vector cosine would do on unsorted vectors), but rather we rank and filter the features, computing the weighted overlap with a rank-based similarity measure inspired by AP Syn, a recent proposal by Santus 5 For an overview on the limitations of vector cosine, see: Li and Han (2013); Dinu et al. (2015); Schnabel et al. (2015); Faruqui et al. (2016); Santus et al. (2016a). 6 An early proposal going in this direction is the predication theory by Kintsch (2001), which exploited Latent Semantic Analysis to select only the vector features that are appropriate for predicate-argument composition. 7 See also the so-called ’strong version’ of the Distributional Hypothesis (Miller and Charles, 1991; Lenci, 2008). 650 et al. (2016a,b,c) which has shown interesting results in synonymy detection and similarity estimation. As we will show in the next sections, the new metric assigns high scores to candidate fillers sharing many salient contexts with the verb-specific rol"
D17-1068,J90-1003,0,\N,Missing
D17-1068,Q15-1016,0,\N,Missing
D19-1306,N15-1042,0,0.01281,"oblem in NLG, where most work has focused on studying the stylistic variation in text (Gatt and Krahmer, 2018). Early contributions in this area defined stylistic features using rules to vary generation (Brooke et al., 2010). For instance, Sheikha and Inkpen (2011) proposed an adaptation of the SimpleNLG realiser (Gatt et al., 2009) to handle formal versus informal language via constructing lexicons of formality (e.g., are not vs. aren’t). More contemporary approaches have tended to eschew rules in favour of data-driven methods to identify relevant linguistic features to stylistic attributes (Ballesteros et al., 2015; Di Fabbrizio et al., 2008; Krahmer and van Deemter, 2012). For example, Mairesse and Walker’s PERSONAGE system (Mairesse and Walker, 2011) uses machine-learning models to take as inputs a list of real-valued style parameters and generate sentences to project different personality traits. In the past few years, attribute-controlled NLG has witnessed renewed interest by researchers working on neural approaches to generation (Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Mueller et al., 2017; Zhang et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Among them, many att"
D19-1306,W16-2915,0,0.0133732,"e sentence pair with the smallest cost from each other approximates minimization of the content shift. Advantages of the WMD over other basic measures of sentence similarity include the fact that it has no hyperparameters to tune, appropriately handles sentences with unequal number of words (via weighted word matchings that sum to 1), accounts for synonymic-similarity at the word-level (through use of pretrained word embedding vectors), and considers the entire contents of each sentence (every word must be somehow matched). Furthermore, WMD has produced high accuracy in information retrieval (Brokos et al., 2016; Kim et al., 2017), where measuring content-similarity is critical (as in our attribute transfer task). Semantic Sentence Representation For the matching process in Step 1, the cosine similarity is computed between each pair of sentences. There is no perfect way of semantic representation of a sentence, but a state-of-the-art method is to use the sentence embeddings obtained by averaging the ELMo embeddings of all the words in the sentence (Peters et al., 2018). ELMo uses a deep, bi-directional LSTM model to create word representations within the context that they are 3100 used. Perone et al."
D19-1306,C10-2011,0,0.0279462,"t performance on two text rewriting tasks involving sentiment modification task and formality conversion. • We release an additional set of 800 sentences rewritten by humans tasked with the sentiment transfer task for the Y ELP review test set. This enables future researchers to evaluate more diverse transfer outputs. 2 Related Work Attribute-controlled text rewriting remains a longstanding problem in NLG, where most work has focused on studying the stylistic variation in text (Gatt and Krahmer, 2018). Early contributions in this area defined stylistic features using rules to vary generation (Brooke et al., 2010). For instance, Sheikha and Inkpen (2011) proposed an adaptation of the SimpleNLG realiser (Gatt et al., 2009) to handle formal versus informal language via constructing lexicons of formality (e.g., are not vs. aren’t). More contemporary approaches have tended to eschew rules in favour of data-driven methods to identify relevant linguistic features to stylistic attributes (Ballesteros et al., 2015; Di Fabbrizio et al., 2008; Krahmer and van Deemter, 2012). For example, Mairesse and Walker’s PERSONAGE system (Mairesse and Walker, 2011) uses machine-learning models to take as inputs a list of re"
D19-1306,D18-2029,0,0.0230839,"ay of semantic representation of a sentence, but a state-of-the-art method is to use the sentence embeddings obtained by averaging the ELMo embeddings of all the words in the sentence (Peters et al., 2018). ELMo uses a deep, bi-directional LSTM model to create word representations within the context that they are 3100 used. Perone et al. (2018) have shown that this approach can efficiently represent semantic and linguistic features of a sentence, outperforming more elaborate approaches such as Skip-Thought (Kiros et al., 2015), InferSent (Conneau et al., 2017a) and Universal Sentence Encoder (Cer et al., 2018). 4 4.1 Experiments Datasets We evaluate the proposed model on two representative tasks: sentiment modification on the Y ELP dataset, and text formality conversion on the F OR MALITY dataset. A careful human assessment in Section 4.1.1 shows that these two datasets are significantly more suitable than the other three popular “style transfer” datasets, namely the political slant transfer dataset (Prabhumoye et al., 2018; Tian et al., 2018), gender transfer dataset (Prabhumoye et al., 2018), and humorous-to-romantic transfer dataset (Li et al., 2018). Dataset Y ELP F ORMALITY Style Positive Nega"
D19-1306,D17-1070,0,0.198019,"pproaches, its performance is dependent on the availability of an accurate word identifier, a precise word replacement selector and a perfect language model to fix the grammatical errors introduced by the crude swap. Recent work improves upon adversarial approaches by additionally leveraging the idea of back translation (dos Santos et al., 2018; Logeswaran et al., 2018; Lample et al., 2019; Prabhumoye et al., 2018). It was previously used for unsupervised Statistical Machine Translation (SMT) (Fung and Yee, 1998; Munteanu et al., 2004; Smith et al., 2010) and Neural Machine Translation (NMT) (Conneau et al., 2017b; Lample et al., 2017; Artetxe et al., 2017), where it iteratively takes the pseudo pairs to train a translation model and then use the refined model to generate new pseudo-parallel pairs with enhanced quality. However, the success of this method relies on good quality of the pseudo-parallel pairs. Our approach proposes using retrieved sentences from the corpus based on semantic similarity as a decent starting point and then refining them using the trained translation models iteratively. 3 3.1 Method Task Formulation Given two mono-style corpora X = {x1 , · · · , xn } with attribute a1 and Y"
D19-1306,W08-2120,0,0.107031,"Missing"
D19-1306,P98-1069,0,0.27117,"ain a neural language model to combine them in a natural way. Despite outperforming the adversarial approaches, its performance is dependent on the availability of an accurate word identifier, a precise word replacement selector and a perfect language model to fix the grammatical errors introduced by the crude swap. Recent work improves upon adversarial approaches by additionally leveraging the idea of back translation (dos Santos et al., 2018; Logeswaran et al., 2018; Lample et al., 2019; Prabhumoye et al., 2018). It was previously used for unsupervised Statistical Machine Translation (SMT) (Fung and Yee, 1998; Munteanu et al., 2004; Smith et al., 2010) and Neural Machine Translation (NMT) (Conneau et al., 2017b; Lample et al., 2017; Artetxe et al., 2017), where it iteratively takes the pseudo pairs to train a translation model and then use the refined model to generate new pseudo-parallel pairs with enhanced quality. However, the success of this method relies on good quality of the pseudo-parallel pairs. Our approach proposes using retrieved sentences from the corpus based on semantic similarity as a decent starting point and then refining them using the trained translation models iteratively. 3 3"
D19-1306,W17-4902,0,0.0632128,"es have tended to eschew rules in favour of data-driven methods to identify relevant linguistic features to stylistic attributes (Ballesteros et al., 2015; Di Fabbrizio et al., 2008; Krahmer and van Deemter, 2012). For example, Mairesse and Walker’s PERSONAGE system (Mairesse and Walker, 2011) uses machine-learning models to take as inputs a list of real-valued style parameters and generate sentences to project different personality traits. In the past few years, attribute-controlled NLG has witnessed renewed interest by researchers working on neural approaches to generation (Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Mueller et al., 2017; Zhang et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Among them, many attribute-controlled text rewriting methods similarly employ GANbased models to disentangle the content and style of text in a shared latent space (Shen et al., 2017; Fu et al., 2018). However, existing work that applies these ideas to text suffers from both training difficulty (Salimans et al., 2016; Arjovsky and Bottou, 2017; Bousmalis et al., 2017), and ineffective manipulation of the latent space which leads to content loss (Li et al., 2018) and generation of gr"
D19-1306,W16-6010,0,0.0201414,"qual Contribution The enriched test set is available at https:// github.com/zhijing-jin/IMaT 1 Di Jin∗ MIT jindi15@mit.edu Juola and Vescovi, 2011), and written communication assistance (Pavlick and Tetreault, 2016). One typical example of a text attribute is the linguistic style, which refers to features of lexicons, syntax and phonology that collectively contribute to the identifiability of an instance of language (Gatt and Krahmer, 2018). For instance, given text written in an older style like the Shakespearean genre, a system may be tasked to convert it into modern-day colloquial English (Kabbara and Cheung, 2016). In general, a text attribute transfer system must be able to: (1) produce sentences that conform to the target attribute, (2) preserve the source sentence’s content, and (3) generate fluent language. Satisfying these requirements is challenging due to the typical absence of supervised parallel corpora exemplifying the desired attribute transformations. With access to only non-parallel data, most existing approaches aim to tease apart content and attribute in a latent sentence representation. For instance, Shen et al. (2017) and Fu et al. (2018) utilize Generative Adversarial Networks (GANs)"
D19-1306,P06-2058,0,0.0120827,"lable test set with human-generated transfer references.1 . 1 Introduction An intelligent natural language generation (NLG) system should be able to flexibly control linguistic attributes of the text it generates. For instance, a certain level of formality should be maintained in serious situations, while informal conversations can be improved through a more relaxed style. The ability to generate or rewrite a certain text with some attributes controlled or transferred to meet pragmatic goals is widely needed in applications such as dialogue generation (Oraby et al., 2018), author obfuscation (Kacmarcik and Gamon, 2006; ∗ Equal Contribution The enriched test set is available at https:// github.com/zhijing-jin/IMaT 1 Di Jin∗ MIT jindi15@mit.edu Juola and Vescovi, 2011), and written communication assistance (Pavlick and Tetreault, 2016). One typical example of a text attribute is the linguistic style, which refers to features of lexicons, syntax and phonology that collectively contribute to the identifiability of an instance of language (Gatt and Krahmer, 2018). For instance, given text written in an older style like the Shakespearean genre, a system may be tasked to convert it into modern-day colloquial Engl"
D19-1306,D14-1181,0,0.00330762,"ge rewriting quality via automated methods, following the practice in (Li et al., 2018; Lample et al., 2019). Here, content preservation is measured by the BLEU score between model outputs and multiple human references for each sentence. Since the previous work only introduces single human reference for Y ELP, we enriched the test set with four extra human references for each test sentence, in total 800 rewrites. The diverse human rewrites ensures a more variety-tolerant measurement of model outputs. Attribute accuracy is assessed via the classification results of a CNN-based text classifier (Kim, 2014). To automatically gauge fluency, we use pretrained language models (LM) to get the perplexity of output sentences. The details of the classifier, LM and the collection of human references are elaborated in Appendix A.1. 4.3 Baselines To compare against multiple baselines, we reimplemented three recently-proposed methods (described in Sections 1 and 2): C ROSS A LIGN MENT (CA) from Shen et al. (2017), M ULTI D E CODER (MD) from Fu et al. (2018), and D ELETE A ND R ETRIEVE (DAR) from Li et al. (2018). Experimental settings For the translation process, we used an off-theshelf Seq2Seq model, a 2-"
D19-1306,P17-4012,0,0.0133706,"e, some unrelated words happen to be wrongly inserted by approach of Li et al. (2018), dramatically upsetting the sentence content as seen in Table 1. In this paper, we propose the Iterative Matching and Translation (IMaT) framework which addresses the aforementioned limitations with regards to content inconsistency and ungrammaticality. Our approach first constructs a pseudoparallel corpus by matching a subset of semantically similar sentences from the source and the target corpora (which possess different attributes), then applies a standard sequence-to-sequence (Seq2Seq) translation model (Klein et al., 2017) to learn the attribute transfer. We then use the translation results of the trained Seq2Seq model (from the previous iteration) to update the previously made pseudo-parallel corpus, so as to refine its quality. Such a matching-translation-refine procedure is iterated repeatedly until performance plateaus. The proposed methodology is simpler and more robust than previous GAN/autoencoder techniques, and is free of manual heuristics such as those used by Li et al. (2018). We validate our Iterative Matching and Translation (IMaT) model in two attribute-controlled text rewriting tasks that aim to"
D19-1306,W18-5019,0,0.0459128,"Missing"
D19-1306,W04-3250,0,0.119531,"Missing"
D19-1306,Q16-1005,0,0.0297326,"instance, a certain level of formality should be maintained in serious situations, while informal conversations can be improved through a more relaxed style. The ability to generate or rewrite a certain text with some attributes controlled or transferred to meet pragmatic goals is widely needed in applications such as dialogue generation (Oraby et al., 2018), author obfuscation (Kacmarcik and Gamon, 2006; ∗ Equal Contribution The enriched test set is available at https:// github.com/zhijing-jin/IMaT 1 Di Jin∗ MIT jindi15@mit.edu Juola and Vescovi, 2011), and written communication assistance (Pavlick and Tetreault, 2016). One typical example of a text attribute is the linguistic style, which refers to features of lexicons, syntax and phonology that collectively contribute to the identifiability of an instance of language (Gatt and Krahmer, 2018). For instance, given text written in an older style like the Shakespearean genre, a system may be tasked to convert it into modern-day colloquial English (Kabbara and Cheung, 2016). In general, a text attribute transfer system must be able to: (1) produce sentences that conform to the target attribute, (2) preserve the source sentence’s content, and (3) generate fluen"
D19-1306,J12-1006,0,0.0189939,"Missing"
D19-1306,N18-1169,0,0.197041,"the model of Shen Input CA MD DAR Ours Positive Sentiment ↔ Negative Sentiment I love this place, the service is always great! I know this place, the food is just a horrible! I love this place, the service is always great! I did not like the homework of lasagna, not like it, . I used to love this place, but the service is horrible now. Table 1: Comparing outputs of transfer models. Our approach both transfers sentiment and preserves content. CrossAlignment (CA) by Shen et al. (2017) loses content, MultiDecoder (MD) by Fu et al. (2018) fails to modify sentiment, and DeleteAndRetrieve (DAR) by Li et al. (2018) produces ungrammatical output. 3097 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3097–3109, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics et al. (2017) changes key content words from “the service” to the unrelated “the food”. In an effort to avoid the issues of GAN/autoencoder approaches, Li et al. (2018) recently demonstrated that direct implementation of heuristic transformations– such as modifying high polarity words in reviews"
D19-1306,D15-1166,0,0.0524307,"Missing"
D19-1306,J11-3002,0,0.028989,"s area defined stylistic features using rules to vary generation (Brooke et al., 2010). For instance, Sheikha and Inkpen (2011) proposed an adaptation of the SimpleNLG realiser (Gatt et al., 2009) to handle formal versus informal language via constructing lexicons of formality (e.g., are not vs. aren’t). More contemporary approaches have tended to eschew rules in favour of data-driven methods to identify relevant linguistic features to stylistic attributes (Ballesteros et al., 2015; Di Fabbrizio et al., 2008; Krahmer and van Deemter, 2012). For example, Mairesse and Walker’s PERSONAGE system (Mairesse and Walker, 2011) uses machine-learning models to take as inputs a list of real-valued style parameters and generate sentences to project different personality traits. In the past few years, attribute-controlled NLG has witnessed renewed interest by researchers working on neural approaches to generation (Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Mueller et al., 2017; Zhang et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Among them, many attribute-controlled text rewriting methods similarly employ GANbased models to disentangle the content and style of text in a shared latent sp"
D19-1306,D14-1162,0,0.086346,"Missing"
D19-1306,N18-1202,0,0.0138966,"contents of each sentence (every word must be somehow matched). Furthermore, WMD has produced high accuracy in information retrieval (Brokos et al., 2016; Kim et al., 2017), where measuring content-similarity is critical (as in our attribute transfer task). Semantic Sentence Representation For the matching process in Step 1, the cosine similarity is computed between each pair of sentences. There is no perfect way of semantic representation of a sentence, but a state-of-the-art method is to use the sentence embeddings obtained by averaging the ELMo embeddings of all the words in the sentence (Peters et al., 2018). ELMo uses a deep, bi-directional LSTM model to create word representations within the context that they are 3100 used. Perone et al. (2018) have shown that this approach can efficiently represent semantic and linguistic features of a sentence, outperforming more elaborate approaches such as Skip-Thought (Kiros et al., 2015), InferSent (Conneau et al., 2017a) and Universal Sentence Encoder (Cer et al., 2018). 4 4.1 Experiments Datasets We evaluate the proposed model on two representative tasks: sentiment modification on the Y ELP dataset, and text formality conversion on the F OR MALITY datas"
D19-1306,P18-1080,0,0.149791,"Missing"
D19-1306,N18-1012,0,0.198196,"US Y ELP F ORMALITY Table 2: Statistics of Y ELP and F ORMALITY datasets. Y ELP The commonly used Y ELP review dataset for sentiment modification (Shen et al., 2017; Li et al., 2018; Prabhumoye et al., 2018) contains a positive corpus of reviews rated above three and a negative corpus of reviews rated below three. This task requires flipping high polarity sentences such as “The food is good” and “The food is bad”. We use the same train/test split as Shen et al. (2017); Li et al. (2018) (see Table 2). F ORMALITY The F ORMALITY dataset stems from an aligned set of formal and informal sentences (Rao and Tetreault, 2018). It demands changes in subtle linguistic features such as “want to” to “wanna”. We obtained a non-parallel dataset by shuffling the two originally aligned corpora (removing duplicates and sentences that exceed 100 words). Table 2 describes statistics of the resulting two corpora. The development/test sets are provided with four human-generated attribute transfer rewrites for each sentence (i.e. the gold standard). Dataset Quality Assessment Unde. Rate 49.0 88.0 66.0 11.0 0.5 Dis. Rate 33.0 14.0 31.0 16.0 17.0 F1 67.2 53.6 79.3 93.2 87.6 Table 3: Comparison of dataset quality. “Unde. Rate” ind"
D19-1306,P18-2031,0,0.0609239,"sarial training altogether. Li 3098 et al. (2018) proposed a much simpler approach: identify style-carrying n-grams, replace them with phrases of the opposite style, and train a neural language model to combine them in a natural way. Despite outperforming the adversarial approaches, its performance is dependent on the availability of an accurate word identifier, a precise word replacement selector and a perfect language model to fix the grammatical errors introduced by the crude swap. Recent work improves upon adversarial approaches by additionally leveraging the idea of back translation (dos Santos et al., 2018; Logeswaran et al., 2018; Lample et al., 2019; Prabhumoye et al., 2018). It was previously used for unsupervised Statistical Machine Translation (SMT) (Fung and Yee, 1998; Munteanu et al., 2004; Smith et al., 2010) and Neural Machine Translation (NMT) (Conneau et al., 2017b; Lample et al., 2017; Artetxe et al., 2017), where it iteratively takes the pseudo pairs to train a translation model and then use the refined model to generate new pseudo-parallel pairs with enhanced quality. However, the success of this method relies on good quality of the pseudo-parallel pairs. Our approach proposes usi"
D19-1306,W11-2826,0,0.0331988,"tasks involving sentiment modification task and formality conversion. • We release an additional set of 800 sentences rewritten by humans tasked with the sentiment transfer task for the Y ELP review test set. This enables future researchers to evaluate more diverse transfer outputs. 2 Related Work Attribute-controlled text rewriting remains a longstanding problem in NLG, where most work has focused on studying the stylistic variation in text (Gatt and Krahmer, 2018). Early contributions in this area defined stylistic features using rules to vary generation (Brooke et al., 2010). For instance, Sheikha and Inkpen (2011) proposed an adaptation of the SimpleNLG realiser (Gatt et al., 2009) to handle formal versus informal language via constructing lexicons of formality (e.g., are not vs. aren’t). More contemporary approaches have tended to eschew rules in favour of data-driven methods to identify relevant linguistic features to stylistic attributes (Ballesteros et al., 2015; Di Fabbrizio et al., 2008; Krahmer and van Deemter, 2012). For example, Mairesse and Walker’s PERSONAGE system (Mairesse and Walker, 2011) uses machine-learning models to take as inputs a list of real-valued style parameters and generate s"
D19-1306,N04-1034,0,0.0940916,"e model to combine them in a natural way. Despite outperforming the adversarial approaches, its performance is dependent on the availability of an accurate word identifier, a precise word replacement selector and a perfect language model to fix the grammatical errors introduced by the crude swap. Recent work improves upon adversarial approaches by additionally leveraging the idea of back translation (dos Santos et al., 2018; Logeswaran et al., 2018; Lample et al., 2019; Prabhumoye et al., 2018). It was previously used for unsupervised Statistical Machine Translation (SMT) (Fung and Yee, 1998; Munteanu et al., 2004; Smith et al., 2010) and Neural Machine Translation (NMT) (Conneau et al., 2017b; Lample et al., 2017; Artetxe et al., 2017), where it iteratively takes the pseudo pairs to train a translation model and then use the refined model to generate new pseudo-parallel pairs with enhanced quality. However, the success of this method relies on good quality of the pseudo-parallel pairs. Our approach proposes using retrieved sentences from the corpus based on semantic similarity as a decent starting point and then refining them using the trained translation models iteratively. 3 3.1 Method Task Formulat"
D19-1306,N10-1063,0,0.0105771,"in a natural way. Despite outperforming the adversarial approaches, its performance is dependent on the availability of an accurate word identifier, a precise word replacement selector and a perfect language model to fix the grammatical errors introduced by the crude swap. Recent work improves upon adversarial approaches by additionally leveraging the idea of back translation (dos Santos et al., 2018; Logeswaran et al., 2018; Lample et al., 2019; Prabhumoye et al., 2018). It was previously used for unsupervised Statistical Machine Translation (SMT) (Fung and Yee, 1998; Munteanu et al., 2004; Smith et al., 2010) and Neural Machine Translation (NMT) (Conneau et al., 2017b; Lample et al., 2017; Artetxe et al., 2017), where it iteratively takes the pseudo pairs to train a translation model and then use the refined model to generate new pseudo-parallel pairs with enhanced quality. However, the success of this method relies on good quality of the pseudo-parallel pairs. Our approach proposes using retrieved sentences from the corpus based on semantic similarity as a decent starting point and then refining them using the trained translation models iteratively. 3 3.1 Method Task Formulation Given two mono-st"
D19-1306,Q18-1027,0,0.0312282,"listic attributes (Ballesteros et al., 2015; Di Fabbrizio et al., 2008; Krahmer and van Deemter, 2012). For example, Mairesse and Walker’s PERSONAGE system (Mairesse and Walker, 2011) uses machine-learning models to take as inputs a list of real-valued style parameters and generate sentences to project different personality traits. In the past few years, attribute-controlled NLG has witnessed renewed interest by researchers working on neural approaches to generation (Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Mueller et al., 2017; Zhang et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Among them, many attribute-controlled text rewriting methods similarly employ GANbased models to disentangle the content and style of text in a shared latent space (Shen et al., 2017; Fu et al., 2018). However, existing work that applies these ideas to text suffers from both training difficulty (Salimans et al., 2016; Arjovsky and Bottou, 2017; Bousmalis et al., 2017), and ineffective manipulation of the latent space which leads to content loss (Li et al., 2018) and generation of grammatically-incorrect sentences. Other lines of research avoid adversarial training altogether. Li 3098 et al."
D19-1306,C98-1066,0,\N,Missing
D19-1306,D18-1549,0,\N,Missing
D19-1341,W18-5513,0,0.0655748,"Missing"
D19-1341,D15-1075,0,0.0382168,"rs. Again, this can be explained by the bias in the training data that translates to the development set, allowing FEVER-trained models to leverage it. Applying the regularization method, using the same training data, helps to train a more robust model that performs better on our test set, where verification in context is a key requirement. 6 Related Work Large scale datasets are fraught with give-away phrases (McCoy et al., 2019; Niven and Kao, 2019). Crowd workers tend to adopt heuristics when creating examples, introducing bias in the dataset. In SNLI (Stanford Natural Language Inference) (Bowman et al., 2015), entailment based solely on the hypothesis forms a very strong baseline (Poliak et al., 2018; Gururangan et al., 2018). Similarly, as shown by Kaushik and Lipton (2018), reading comprehension models that rely only on the question (or only on the passage referred to by the question) perform exceedingly well on several popular datasets (Weston et al., 2016; Onishi et al., 2016; Hill et al., 2016). To address deficiencies in the SQuAD dataset (Jia R.W LMI·10−6 144 30 67 55 31 9 32 8 10 41 R.W p(l|w) 0.35 0.33 0.35 0.35 0.33 0.31 0.33 0.30 0.32 0.35 Table 4: Re-weighted statistics (l = R EFUTES)"
D19-1341,D18-1546,0,0.0208476,"regularization method, using the same training data, helps to train a more robust model that performs better on our test set, where verification in context is a key requirement. 6 Related Work Large scale datasets are fraught with give-away phrases (McCoy et al., 2019; Niven and Kao, 2019). Crowd workers tend to adopt heuristics when creating examples, introducing bias in the dataset. In SNLI (Stanford Natural Language Inference) (Bowman et al., 2015), entailment based solely on the hypothesis forms a very strong baseline (Poliak et al., 2018; Gururangan et al., 2018). Similarly, as shown by Kaushik and Lipton (2018), reading comprehension models that rely only on the question (or only on the passage referred to by the question) perform exceedingly well on several popular datasets (Weston et al., 2016; Onishi et al., 2016; Hill et al., 2016). To address deficiencies in the SQuAD dataset (Jia R.W LMI·10−6 144 30 67 55 31 9 32 8 10 41 R.W p(l|w) 0.35 0.33 0.35 0.35 0.33 0.31 0.33 0.30 0.32 0.35 Table 4: Re-weighted statistics (l = R EFUTES) for the bigrams from Table 1. The weights were obtained following the optimization of Eq. 3 on the training set which contains three labels. and Liang, 2017), researcher"
D19-1341,P17-1152,0,0.0962021,"Missing"
D19-1341,P19-1334,0,0.0703286,"Missing"
D19-1341,N19-1423,0,0.0714446,"Missing"
D19-1341,N18-2017,0,0.125943,"Missing"
D19-1341,D17-1215,0,0.123267,"Missing"
D19-1341,P19-1459,0,0.0679511,"Missing"
D19-1341,D16-1241,0,0.0313133,"are fraught with give-away phrases (McCoy et al., 2019; Niven and Kao, 2019). Crowd workers tend to adopt heuristics when creating examples, introducing bias in the dataset. In SNLI (Stanford Natural Language Inference) (Bowman et al., 2015), entailment based solely on the hypothesis forms a very strong baseline (Poliak et al., 2018; Gururangan et al., 2018). Similarly, as shown by Kaushik and Lipton (2018), reading comprehension models that rely only on the question (or only on the passage referred to by the question) perform exceedingly well on several popular datasets (Weston et al., 2016; Onishi et al., 2016; Hill et al., 2016). To address deficiencies in the SQuAD dataset (Jia R.W LMI·10−6 144 30 67 55 31 9 32 8 10 41 R.W p(l|w) 0.35 0.33 0.35 0.35 0.33 0.31 0.33 0.30 0.32 0.35 Table 4: Re-weighted statistics (l = R EFUTES) for the bigrams from Table 1. The weights were obtained following the optimization of Eq. 3 on the training set which contains three labels. and Liang, 2017), researchers have proposed approaches for augmenting the existing dataset (Rajpurkar et al., 2018). In most cases, these augmentations are done manually, and involve constructing challenging examples for existing systems"
D19-1341,D14-1162,0,0.0806448,"Missing"
D19-1341,N18-1202,0,0.0746687,"Missing"
D19-1341,S18-2023,0,0.071565,"Missing"
D19-1341,P18-2124,0,0.0212556,"r only on the passage referred to by the question) perform exceedingly well on several popular datasets (Weston et al., 2016; Onishi et al., 2016; Hill et al., 2016). To address deficiencies in the SQuAD dataset (Jia R.W LMI·10−6 144 30 67 55 31 9 32 8 10 41 R.W p(l|w) 0.35 0.33 0.35 0.35 0.33 0.31 0.33 0.30 0.32 0.35 Table 4: Re-weighted statistics (l = R EFUTES) for the bigrams from Table 1. The weights were obtained following the optimization of Eq. 3 on the training set which contains three labels. and Liang, 2017), researchers have proposed approaches for augmenting the existing dataset (Rajpurkar et al., 2018). In most cases, these augmentations are done manually, and involve constructing challenging examples for existing systems. 7 Conclusion This paper demonstrates that the FEVER dataset contains idiosyncrasies that can be easily exploited by fact-checking classifiers to obtain high classification accuracies. Evaluating the claimevidence reasoning of these models necessitates unbiased datasets. Therefore, we suggest a way to turn the evaluation FEVER pairs into symmetric combinations for which a decision that is solely based on the claim is equivalent to a random guess. Tested on these pairs, FEV"
D19-1341,N19-1421,0,0.0120812,"nverse relations (see Figure 1). Examples of generated sentences are provided in Table 2. This new test set completely eliminates the ability of models to rely on cues from claims. Considering the two labels of this test set5 , the probability of a label given the existence of any n-gram in the claim or in the evidence is p(l|w) = 0.5, by construction. Also, as the example in Figure 1 demonstrates, in order to perform well on this dataset, a fact verification classifier may still take advantage of world 4 We use InferSent because BERT, being pretrained on Wikipedia, comprises world knowledge (Talmor et al., 2019). 5 N OT E NOUGH I NFO cases are easy to generate so we focus on the two other labels. knowledge (e.g. geographical locations), but reasoning should only be with respect to the context. 4 Towards Unbiased Training Creating a large symmetric dataset for training is outside the scope of this paper as it would be too expensive. Instead, we propose an algorithmic solution to alleviate the bias introduced by ‘give-away’ n-grams present in the claims. We reweight the instances in the dataset to flatten the correlation of claim n-grams with respect to the labels. Specifically, for ‘give-away’ phrases"
D19-1341,N18-1074,0,0.0634953,"Missing"
E14-4008,J10-4006,1,0.410132,"c Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). 38 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we introduce SLQS, a new entropy-based distributional measure that aims to identify hypernyms by providing a distributional characterization of their semantic generality. We assess it with two tasks: (i.) the identification of the broader term in hyponym-hypernym pairs (directionality task); (ii.) the discrimination of hypernymy among other semantic relations (detectio"
E14-4008,J07-2002,0,0.0337174,"Distributional Semantic Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). 38 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we introduce SLQS, a new entropy-based distributional measure that aims to identify hypernyms by providing a distributional characterization of their semantic generality. We assess it with two tasks: (i.) the identification of the broader term in hyponym-hypernym pairs (directionality task); (ii.) the discrimination of hypernymy among other sem"
E14-4008,W09-0215,0,0.851838,"Missing"
E14-4008,P05-1014,0,0.914436,"Missing"
E14-4008,W03-1011,0,0.848579,"we propose SLQS, which measures the semantic generality of a word by the entropy of its statistically most prominent contexts. Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: , = SLQS: A new entropy-based measure For every term wi we identify the N most associated contexts c (where N is a parameter empirically set to 50)1. The association strength has been calculated with Local Mutual Information (LMI; Evert, 2005). For each"
E14-4008,C04-1146,0,0.70813,"h measures the semantic generality of a word by the entropy of its statistically most prominent contexts. Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: , = SLQS: A new entropy-based measure For every term wi we identify the N most associated contexts c (where N is a parameter empirically set to 50)1. The association strength has been calculated with Local Mutual Information (LMI; Evert, 2005). For each selected context c,"
E14-4008,S12-1012,1,0.739596,"Missing"
E14-4008,C92-2082,0,\N,Missing
E14-4008,W11-2501,1,\N,Missing
E14-4008,W09-0200,0,\N,Missing
E17-1007,J10-4006,0,0.0143056,"the sentence cute cats drink milk, with the target word cats. The dependencybased contexts are drink-v:nsubj and cute-a:amod−1 . The joint-dependency context is drink-v#milk-n. Differently from Chersoni et al. (2016), we exclude the dependency tags to mitigate the sparsity of contexts. Out-of-vocabulary words are filtered out before applying the window. We experimented with window sizes 2 and 5, directional and indirectional (win2, win2d, win5, win5d). • Dependency-based contexts: rather than adjacent words in a window, we consider neighbors in a dependency parse tree (Pad´o and Lapata, 2007; Baroni and Lenci, 2010). The contexts of a target word wi are its parent and daughter nodes in the dependency tree (dep). We also experimented with a joint dependency context inspired by Chersoni et al. (2016), in which the contexts of a target word are the parent-sister pairs in the dependency tree (joint). See Figure 1 for an illustration. Feature Weighting Each distributional semantic space is spanned by a matrix M in which each row corresponds to a target word while each column corresponds to a context. The value of each cell Mi,j represents the association between the target word wi and the context cj . We expe"
E17-1007,C92-2082,0,0.638996,"n In the last two decades, the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy. Such effort is motivated by the role this semantic relation plays in a large number of tasks, such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011) and recognizing textual entailment (Dagan et al., 2013). The task has appeared to be, however, a challenging one, and the numerous approaches proposed to tackle it have often shown limitations. Early corpus-based methods have exploited patterns that may indicate hypernymy (e.g. “animals such as dogs”) (Hearst, 1992; Snow et al., 2005), but the recall limitation of this approach, requiring both words to co-occur in a sentence, motivated the development of methods that rely on 65 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 65–75, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ants and context-types are tested for the first time.1 We demonstrate that since each of these measures captures a different aspect of the hypernymy relation, there is no single measure that consistently p"
E17-1007,P15-2020,0,0.290882,"Missing"
E17-1007,E12-1004,0,0.209296,"e Precision (AP) and Mean Average Precision (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010) and the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features"
E17-1007,S12-1012,0,0.0752966,"de and data are available at: https://github.com/vered1986/UnsupervisedHypernymy 66 • Positive LMI (PLMI) - positive local mutual information (PLMI) (Evert, 2005; Evert, 2008). PPMI was found to have a bias towards rare events. PLMI simply balances PPMI by multiplying it by the co-occurrence frequency of w and c: P LM I(w, c) = f req(w, c) · P P M I(w, c). 3 • Weeds Precision (Weeds and Weir, 2003) A directional precision-based similarity measure. This measure quantifies the weighted inclusion of x’s contexts by y’s contexts: Σc∈~vx ∩~vy ~vx [c] W eedsP rec(x → y) = Σc∈~vx ~vx [c] • cosWeeds (Lenci and Benotto, 2012) Geometric mean of cosine similarity and Weeds precision: p Unsupervised Hypernymy Detection Measures cosW eeds(x → y) = • ClarkeDE (Clarke, 2009) Computes degree of inclusion, by quantifying weighted coverage of the hyponym’s contexts by those of the hypernym: We experiment with a large number of unsupervised measures proposed in the literature for distributional hypernymy detection, with some new variants. In the following section, ~vx and ~vy denote x and y’s word vectors (rows in the matrix M ). We consider the scores as measuring to what extent y is a hypernym of x (x → y). 3.1 CDE(x → y)"
E17-1007,P14-2050,0,0.128893,"Missing"
E17-1007,D16-1205,1,0.886738,"Missing"
E17-1007,Q15-1016,0,0.477411,"represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features (Santus et al., 2016a), and a neural method integrating path-based and distributional information (Shwartz et al., 2016). In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection, using several distributional semantic models that differ by context type and feature weighting. Some measure variThe fundamental role of hypernymy in NLP has motivated the development o"
E17-1007,J90-1003,0,0.372426,"corpus we used a concatenation of the following two corpora: ukWaC (Ferraresi, 2007), a 2-billion word corpus constructed by crawling the .uk domain, and WaCkypedia EN (Baroni et al., 2009), a 2009 dump of the English Wikipedia. Both corpora include POS, lemma and dependency parse annotations. Our vocabulary (of target and context words) includes only nouns, verbs and adjectives that occurred at least 100 times in the corpus. • Frequency - raw frequency (no weighting): Mi,j is the number of co-occurrences of wi and cj in the corpus. • Positive PMI (PPMI) - pointwise mutual information (PMI) (Church and Hanks, 1990) is defined as the log ratio between the joint probability of w and c and the product of their marginal probabilities: P M I(w, c) = ˆ log P (w,c) , where Pˆ (w), Pˆ (c), and Pˆ (w, c) Pˆ (w)Pˆ (c) Context Type We use several context types: are estimated by the relative frequencies of a word w, a context c and a word-context pair (w, c), respectively. To handle unseen pairs (w, c), yielding P M I(w, c) = log(0) = −∞, PPMI (Bullinaria and Levy, 2007) assigns zero to negative PMI scores: P P M I(w, c) = max(P M I(w, c), 0). • Window-based contexts: the contexts of a target word wi are the words"
E17-1007,N15-1098,0,0.804753,"represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features (Santus et al., 2016a), and a neural method integrating path-based and distributional information (Shwartz et al., 2016). In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection, using several distributional semantic models that differ by context type and feature weighting. Some measure variThe fundamental role of hypernymy in NLP has motivated the development o"
E17-1007,W09-0215,0,0.297353,"Missing"
E17-1007,P13-2080,0,0.0232974,"ing instances, hurting their reliability. Being based on general linguistic hypotheses and independent from training data, unsupervised measures are more robust, and therefore are still useful artillery for hypernymy detection. 1 dominik.schlechtweg@gmx.de Introduction In the last two decades, the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy. Such effort is motivated by the role this semantic relation plays in a large number of tasks, such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011) and recognizing textual entailment (Dagan et al., 2013). The task has appeared to be, however, a challenging one, and the numerous approaches proposed to tackle it have often shown limitations. Early corpus-based methods have exploited patterns that may indicate hypernymy (e.g. “animals such as dogs”) (Hearst, 1992; Snow et al., 2005), but the recall limitation of this approach, requiring both words to co-occur in a sentence, motivated the development of methods that rely on 65 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 65–75, c Valencia, Spain, April 3-"
E17-1007,J07-2002,0,0.0492155,"Missing"
E17-1007,P16-1226,1,0.846088,"Missing"
E17-1007,D14-1162,0,0.116257,"he distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features (Santus et al., 2016a), and a neural method integrating path-based and distributional information (Shwartz et al., 2016). In this paper we perform an extensive evaluation of various unsupervised distributional"
E17-1007,E14-1054,0,0.0703737,"ns of the distributional hypothesis (Harris, 1954). The first distributional approaches were unsupervised, assigning a score for each (x, y) wordpair, which is expected to be higher for hypernym pairs than for negative instances. Evaluation is performed using ranking metrics inherited from information retrieval, such as Average Precision (AP) and Mean Average Precision (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010) and the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use"
E17-1007,P06-1101,0,0.028322,"the unsupervised ones, the former are sensitive to the distribution of training instances, hurting their reliability. Being based on general linguistic hypotheses and independent from training data, unsupervised measures are more robust, and therefore are still useful artillery for hypernymy detection. 1 dominik.schlechtweg@gmx.de Introduction In the last two decades, the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy. Such effort is motivated by the role this semantic relation plays in a large number of tasks, such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011) and recognizing textual entailment (Dagan et al., 2013). The task has appeared to be, however, a challenging one, and the numerous approaches proposed to tackle it have often shown limitations. Early corpus-based methods have exploited patterns that may indicate hypernymy (e.g. “animals such as dogs”) (Hearst, 1992; Snow et al., 2005), but the recall limitation of this approach, requiring both words to co-occur in a sentence, motivated the development of methods that rely on 65 Proceedings of the 15th Conference of the European Chapter of the Association for Computation"
E17-1007,D16-1234,0,0.665717,"Missing"
E17-1007,C14-1097,0,0.616799,"Missing"
E17-1007,W03-1011,0,0.0966364,"University, Hong Kong 4 University of Stuttgart, Stuttgart, Germany {vered1986,esantus}@gmail.com, Abstract adaptations of the distributional hypothesis (Harris, 1954). The first distributional approaches were unsupervised, assigning a score for each (x, y) wordpair, which is expected to be higher for hypernym pairs than for negative instances. Evaluation is performed using ranking metrics inherited from information retrieval, such as Average Precision (AP) and Mean Average Precision (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010) and the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word emb"
E17-1007,C14-1212,0,0.426282,"n (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010) and the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features (Santus et al., 2016a), and a neural meth"
E17-1007,E14-4008,1,0.724451,"m, Abstract adaptations of the distributional hypothesis (Harris, 1954). The first distributional approaches were unsupervised, assigning a score for each (x, y) wordpair, which is expected to be higher for hypernym pairs than for negative instances. Evaluation is performed using ranking metrics inherited from information retrieval, such as Average Precision (AP) and Mean Average Precision (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010) and the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to th"
E17-1007,L16-1722,1,0.877285,"Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features (Santus et al., 2016a), and a neural method integrating path-based and distributional information (Shwartz et al., 2016). In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection, using several distributional semantic models that differ by context type and feature weighting. Some measure variThe fundamental role of hypernymy in NLP has motivated the development of many methods for the automatic identification of this relation, most of which rely on word distribution. We investigate an extensive number of such unsupervised measures, using several dist"
K17-1036,W09-2402,0,0.0825866,"Missing"
K17-1036,D16-1229,0,0.138991,"ering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet. Related Work There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordava and Baroni, 2011; Kim et al., 2014; Xu and Kemp, 2015; Eger and Mehler, 2016; Hellrich and Hahn, 2016; Hamilton et al., 2016a,b) and (ii), word sense induction models (WSI) inferring for each word a probability distribution over different word senses (or topics) in turn modeled as a distribution over words (Wang and Mccallum, 2006; Bamman and Crane, 2011; Wijaya and Yeniterzi, 2011; Lau et al., 2012; Mihalcea and Nastase, 2012; Frermann and Lapata, 2016). Most of the similarity models seem to be limited to quantify the degree of overall change rather than being able to qualify different types of semantic change.2 Similarity metrics, in particular, were shown not to distinguish well between words on different levels"
K17-1036,E06-1042,0,0.0254003,"us approaches to language change exploit the notion of entropy. Juola (2003) describes language change on a very general level by computing the relative entropy (or KL-divergence) of language stages, i.e. intuitively speaking, measuring how well later stages of English encode a prior stage. Kisselew et al. (2016) are interested in the diachronic properties of conversion using— among other measures—a word entropy measure. Finally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009), classification relying on abstractness cues (Turney et al., 2011; K¨oper and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al., 2014), and soft clustering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet. Related Work There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and t"
K17-1036,P16-1141,0,0.0717024,"Missing"
K17-1036,E12-1060,0,0.256208,"hods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordava and Baroni, 2011; Kim et al., 2014; Xu and Kemp, 2015; Eger and Mehler, 2016; Hellrich and Hahn, 2016; Hamilton et al., 2016a,b) and (ii), word sense induction models (WSI) inferring for each word a probability distribution over different word senses (or topics) in turn modeled as a distribution over words (Wang and Mccallum, 2006; Bamman and Crane, 2011; Wijaya and Yeniterzi, 2011; Lau et al., 2012; Mihalcea and Nastase, 2012; Frermann and Lapata, 2016). Most of the similarity models seem to be limited to quantify the degree of overall change rather than being able to qualify different types of semantic change.2 Similarity metrics, in particular, were shown not to distinguish well between words on different levels of the semantic hierarchy (Shwartz et al., 2016). Thus, we cannot expect diachronic similarity models to reflect changes in the semantic generality of a word over time, which was described to be a central effect of semantic change (cf. Bybee, 2015, p. 197). Additionally, they"
K17-1036,D09-1033,0,0.0201518,"on of entropy. Juola (2003) describes language change on a very general level by computing the relative entropy (or KL-divergence) of language stages, i.e. intuitively speaking, measuring how well later stages of English encode a prior stage. Kisselew et al. (2016) are interested in the diachronic properties of conversion using— among other measures—a word entropy measure. Finally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009), classification relying on abstractness cues (Turney et al., 2011; K¨oper and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al., 2014), and soft clustering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet. Related Work There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a simil"
K17-1036,P14-1024,0,0.025241,"intuitively speaking, measuring how well later stages of English encode a prior stage. Kisselew et al. (2016) are interested in the diachronic properties of conversion using— among other measures—a word entropy measure. Finally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009), classification relying on abstractness cues (Turney et al., 2011; K¨oper and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al., 2014), and soft clustering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet. Related Work There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordava and Baroni, 2011; Kim et al., 2014; Xu and Kemp, 2015; Eger and Mehler, 2016; Hellrich"
K17-1036,P12-2051,0,0.0570291,"s mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordava and Baroni, 2011; Kim et al., 2014; Xu and Kemp, 2015; Eger and Mehler, 2016; Hellrich and Hahn, 2016; Hamilton et al., 2016a,b) and (ii), word sense induction models (WSI) inferring for each word a probability distribution over different word senses (or topics) in turn modeled as a distribution over words (Wang and Mccallum, 2006; Bamman and Crane, 2011; Wijaya and Yeniterzi, 2011; Lau et al., 2012; Mihalcea and Nastase, 2012; Frermann and Lapata, 2016). Most of the similarity models seem to be limited to quantify the degree of overall change rather than being able to qualify different types of semantic change.2 Similarity metrics, in particular, were shown not to distinguish well between words on different levels of the semantic hierarchy (Shwartz et al., 2016). Thus, we cannot expect diachronic similarity models to reflect changes in the semantic generality of a word over time, which was described to be a central effect of semantic change (cf. Bybee, 2015, p. 197). Additionally, they often pose the problem of ve"
K17-1036,D11-1063,0,0.0234796,"l level by computing the relative entropy (or KL-divergence) of language stages, i.e. intuitively speaking, measuring how well later stages of English encode a prior stage. Kisselew et al. (2016) are interested in the diachronic properties of conversion using— among other measures—a word entropy measure. Finally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009), classification relying on abstractness cues (Turney et al., 2011; K¨oper and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al., 2014), and soft clustering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet. Related Work There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordav"
K17-1036,E14-1054,0,0.0128119,"bly abstract) radically’ (MB ) as in (2). recall that metaphor involves a mapping between two different domains (as introduced in Lakoff and Johnson 1980) in contrast to other types of meaning change, which is why we would expect a relatively strong effect on the contextual distribution here. Moreover, not only the range of a word’s meanings influences the range of contexts it occurs in, but also the particular nature of the individual meanings has an influence. As research in hypernymy detection shows, words at different levels of semantic generality have different distributional properties (Rimell, 2014; Santus et al., 2014; Shwartz et al., 2016). According to the distributional informativeness hypothesis, semantically more general words are less informative than special words as they occur in more general contexts (Rimell, 2014; Santus et al., 2014). Hence, differences in semantic generality of source and target concept should be reflected by their contextual distribution.6 Such differences occur particularly with taxonomic meaning changes like generalization and specialization, but also with metaphoric change, as it often results in the emergence of more abstract meanings of a word. Consid"
K17-1036,W03-1011,0,0.0549213,"Missing"
K17-1036,W09-0214,0,0.0798965,"arning (CoNLL 2017), pages 354–367, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics metaphoric change test set for German. Section 7 illustrates how the measures’ predictions shall be evaluated. The results are presented and discussed in Section 8. Section 9 will then conclude and give a short outlook to further research objectives. 2 to detect meaning changes where no new senses can be induced as, e.g., in grammaticalization. Moreover, some models require elaborate training (e.g., Frermann and Lapata, 2016). Apart from similarity and WSI models, Sagi et al. (2009) measure semantic broadening and narrowing of words (shifting upwards and downwards in the semantic taxonomy respectively) via semantic density calculated as the average cosine of its context word vectors. Just as word entropy, semantic density is based on the measurement of linguistic context dispersion (see Section 3.1). However, this method is only applied in a case study with very limited scope in terms of the number of phenomena covered and there is no verification of the test items via annotation. Hence, it remains to be shown that the method can generally distinguish broadening and narr"
K17-1036,E14-4008,1,0.765644,"radically’ (MB ) as in (2). recall that metaphor involves a mapping between two different domains (as introduced in Lakoff and Johnson 1980) in contrast to other types of meaning change, which is why we would expect a relatively strong effect on the contextual distribution here. Moreover, not only the range of a word’s meanings influences the range of contexts it occurs in, but also the particular nature of the individual meanings has an influence. As research in hypernymy detection shows, words at different levels of semantic generality have different distributional properties (Rimell, 2014; Santus et al., 2014; Shwartz et al., 2016). According to the distributional informativeness hypothesis, semantically more general words are less informative than special words as they occur in more general contexts (Rimell, 2014; Santus et al., 2014). Hence, differences in semantic generality of source and target concept should be reflected by their contextual distribution.6 Such differences occur particularly with taxonomic meaning changes like generalization and specialization, but also with metaphoric change, as it often results in the emergence of more abstract meanings of a word. Consider, e.g., the develop"
K17-1036,J13-3003,0,\N,Missing
K17-1036,W09-0215,0,\N,Missing
K17-1036,P09-1002,0,\N,Missing
K17-1036,P09-2018,0,\N,Missing
K17-1036,Q16-1003,0,\N,Missing
K17-1036,N16-1039,1,\N,Missing
K17-1036,P16-2009,0,\N,Missing
K17-1036,W11-2508,0,\N,Missing
K17-1036,W16-2015,0,\N,Missing
K17-1036,C16-1262,0,\N,Missing
L16-1722,W11-2501,1,0.38469,"Missing"
L16-1722,E12-1004,0,0.22534,"Missing"
L16-1722,W09-0215,0,0.0557322,"Missing"
L16-1722,C92-2082,0,0.209128,"Missing"
L16-1722,P13-2078,0,0.00781986,"Missing"
L16-1722,N15-1098,0,0.476188,"Missing"
L16-1722,E14-1054,0,0.0390832,"Missing"
L16-1722,C14-1097,0,0.715404,"Missing"
L16-1722,E14-4008,1,0.754471,"close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The fe"
L16-1722,Y14-1018,1,0.121991,"ntiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011). The ablation test has show"
L16-1722,W15-4208,1,0.746309,"s et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011). The ablation test has shown that four out of thirteen feat"
L16-1722,C08-1107,0,0.029533,"Missing"
L16-1722,W03-1011,0,0.807271,"ting hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly"
L16-1722,C14-1212,0,0.148115,"ymy in fact represents a key organization principle of semantic memory (Murphy, 2002), the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005). Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have cl"
L16-1722,Y15-1021,1,0.87055,"Missing"
L16-1723,W11-2501,1,0.863897,"Missing"
L16-1723,J90-1003,0,0.489356,"ty measures play a fundamental role in tasks such as Information Retrieval (IR), Text Classification (TC), Text Summarization (TS), Question Answering (QA), Sentiment Analysis (SA), and so on (Terra and Clarke, 2003; Tungthamthiti et al., 2015). They can be either knowledge-based or corpus-based (Gomaa and Fahmy, 2013). The former rely on lexicons or semantic networks, such as WordNet (Fellbaum, 1998), measuring the distance between the nodes in the network. The latter, instead, compute the similarity between words relying on statistical information about their distributions in large corpora (Church and Hanks, 1990). Knowledge based approaches generally exploit hand-crafted resources. While being hand-crafted ensures high quality, it also entails arbitrariness and high development and update costs. This is the main reason why these resources are known for their limited coverage (Santus et al., 2015b). Such limitation has often prompted researchers to pursue hybrid approaches (Turney, 2001). A key assumption of corpus-based approaches is that similarity between words can be measured by looking at words co-occurrences. In particular, following the Distributional Hypothesis (Harris, 1954; Firth, 1957), thes"
L16-1723,C92-2082,0,0.177797,"approaches generally exploit the Distributional Hypothesis, according to which words that occur in similar contexts also have similar meanings (Harris, 1954). Although these approaches extract statistics from large corpora, they vary in the way they define what has to be considered context (i.e. lexical context, syntactic context, documents, etc.), how the association with such context is measured (e.g. frequency of co-occurrence, association measures like Pointwise Mutual Information, etc.), and how the association with the contexts is used to identify the similarity (Terra and Clarke, 2003; Hearst, 1992; Santus et al., 2014a; Santus et al., 2014b; Santus et al., 2016a). A common way to represent word meaning in NLP is by using vectors to encode the association between the target words and their contexts. The resulting vector space is generally referred as Vector Space Model (VSM) or, more specifically, as Distributional Semantic Model (DSM). In such vector space, word similarity can be calculated by using the Vector Cosine, which measures the angle between the vectors (Turney and Pantel, 2010). Other measures – such as Manhattan Distance, Dice’s Coefficient, Euclidean Distance, Jaccard Simil"
L16-1723,Q15-1016,0,0.605479,"ctor Cosine is generally considered to be the optimal choice (Bullinaria and Levy 2007). Another common way to represent word meaning is using word embeddings, which are vector-space word representations that are implicitly learned by the input-layer weights of neural networks. These models have shown a strong ability to capture synonymy and analogies (such as in the famous “King - Man + Woman = Queen” example, where Mikolov et al. (2013) subtract the vector of “Man” from the one of “King”, and then add the vector of “Woman”, obtaining a very similar vector to the one of “Queen”), even though Levy et al. (2015) have claimed that traditional count-based DSMs can achieve the same results if their hyperparameters are properly optimized. A well-known problem with the distributional approaches is that they rely on a very loose definition of similarity. In fact, vectors have as nearest neighbours not only synonyms, but also hypernyms, co-hyponyms, antonyms, as well as a wide range of other semantically related items (Santus et al., 2015). For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures. Among the most common ones, there are the English a"
L16-1723,J07-2002,0,0.0420041,"Missing"
L16-1723,2003.mtsummit-papers.42,0,0.263036,"Missing"
L16-1723,W15-4208,1,0.935386,"Missing"
L16-1723,E14-4008,1,0.92777,"Missing"
L16-1723,N03-1032,0,0.173981,"Missing"
L16-1723,Y15-1021,1,0.830439,"Missing"
L16-1723,C08-1114,0,0.186322,"Missing"
L16-1726,N09-1003,0,0.0438364,"among different relationships between words (Santus et al., 2014c). In recent years, DSMs have been adopted in several semantic tasks, including the identification of semantic relatedness and similarity. The former task is concerned with whether two words are related or not, independently from their paradigmatic similarity. The identification of semantic similarity, instead, is a more specific task and it consists in identifying words that are paradigmatically related (Sun, 2014; Murphy, 2003). DSMs have successfully addressed these two tasks by using vector cosine as a measure of similarity (Agirre et al., 2009). Unfortunately, however, DSMs are not yet able to discriminate the several semantic relations that exist between words (Santus et al., 2014c). For example, a word such as ⿓ (long, dragon) can be in relation with several other words: 1. Antonymy: ⿓ (long, dragon) vs. 鳳 1 (feng, phoenix) 2. Hypernymy: ⿓ (long, dragon) vs. (shuizhongshengwu, aquatic life) ⽔中⽣物 3. Nearsynonymy 2 : ⿓ (long, dragon) vs 兔 (tu, rabbit) 1 In Chinese culture, dragon and phoenix roughly mean male and female, or king and queen. 2 This term, defined in Chinese WordNet, implies relatedness 4. Synonymy: 不 能 (buneng, cannot)"
L16-1726,Y96-1018,1,0.667444,"pecially pointed out that there is currently no dataset detailing part-whole relations that can be used for the evaluation of their methodology for Mandarin. The current paper addresses the necessity and importance of the construction of a practical dataset for the training and evaluation of DSMs. 3. Construction of Dataset The word pairs from which EVALution-MAN was constructed came from Chinese WordNet (CWN: (Huang et al., 2010)). CWN is a knowledge system modeled on the original Princeton WordNet. The system’s word 4584 sense examples and lexical semantic relations came from Sinica Corpus (Chen et al., 1996). We extracted pairs holding the following relations: synonymy (i.e. words belonging to the same CWN synset); antonymy (i.e. words that have the opposite synset in CWN); hyponymy (i.e. one word’s CWN synset is the subordinate of another’s); meronymy (i.e. one word’s CWN synset is a part, member or substance of another’s); nearsynonymy (i.e. words sharing in relatedness rather than within the same synset). Table 1: Numbers of Checking results 3.1. Data Collection In order to include only prototypical pairs in EVALutionMAN, we filtered the CWN pairs and then further assessed them through manual"
L16-1726,J06-1005,0,0.0543596,"e to) 不可以 5. Meronymy: 墻 (jianzhuwu, wall) vs. (jianzhuwu, building) 建築物 A DSM might be able to identify these words as similar/related, but it will struggle to to discriminate the paradigmatic relations they hold. While this is indeed still a challenging task, the NLP community has adopted several approaches (Jia et al., 2014). The new approaches can be classified according to the need of annotated resources to achieve their goal of identifying multiple relations between given word pairs: supervised, which is a method of discrimination that relies on large scale datasets to train the models (Girju et al., 2006; Van Hage et al., 2006); semi-supervised, which uses a small dataset that acts as a seed to extract more relation instances and patterns iteratively (Pantel and Pennacchiotti, 2006); and unsupervised which does not require any manually annotated dataset to train the model (Jia et al., 2014; Santus et al., 2014a; Santus et al., 2014b; Santus et al., 2014c). While a dataset is necessary for training the former two approaches, the third approach will also need a dataset for testing. Because of this necessity, several banchmarks have been constructued for English. To meet the demand, several benc"
L16-1726,W97-0802,0,0.603536,"Missing"
L16-1726,J15-4004,0,0.0395602,"ALution-MAN, a new resource for training and evaluating Mandarin DSMs. EVALutionMAN is a traditional Chinese version of the English EVALution 1.0 (Santus et al., 2015), as it was built following a very similar methodology. instead of similarity. 4583 2. Related Work 2.1. Datasets for DSMs For the training and evaluation of DSMs, several datasets have been widely used. While general-purpose datasets are still being made use of, a recent trend in the creation of benchmarks for the training of DSMs has been their construction from data derived from specific tasks performed by human participants (Hill et al., 2015). The general purpose resources used for the training and evaluation of DSMs have followed the model laid out by WordNet (Fellbaum, 1998). For Mandarin, Chinese, there is Chinese WordNet (CWN: (Huang et al., 2010)) and How-Net (Liu and Li, 2002). CWN consists of more than 50,000 word relation pairs covering the relationships of antonymy, synonymy, hyponymy/hypernymy, meronymy/holonymy, paranymy, nearsynonymy and variant. Paranymy in this context refers to co-hyponymy, i.e., lexical items that share hypernym pairs, while variant refers to pairs that are identical in meaning and use but differ i"
L16-1726,Y06-1024,1,0.74854,"ecific” while 城市 (chengshi, city) can be tagged as ”General”; 3. Abstract/Concrete: 玫瑰 (meigui, rose) can be tagged as ”Concrete” while 概念 (gainian, concept) can be tagged as ”Abstract”; 4. Event/Action/Time/Space/Object/Animal /Plant/Food/Color/People/Attribute: such as 轉變成 (zhuanbiancheng, transform into) can be tagged as ”Action”. Only relata that showed agreement between at least two of the taggers were treated as positive results, leaving the total 4585 set of semantically tagged relata at 373. Meanwhile, frequency information was calculated in a combined corpus of Chen et al. (1996) and Hong and Huang (2006). 4. Conclusion and Future Work EVALution-MAN is a dataset of Mandarin word relation pairs in Traditional Chinese for training and evaluation of DSMs or other applications. It has been manually rated according to relation pairs, and tagged for semantic type by native Mandarin speakers. It is freely available online at ”https://github.com/LHongchao/EVALution_MAN”. Future work will focus on extending the number of manually tagged relation pairs through extracting existing pairs through other ontology resources and/or through the use of behavioral methods that elicit semantic types of given words"
L16-1726,huang-etal-2004-sinica,1,0.610431,"was constructed from Taiwanese Mandarin language sources. Thus, variances in terminology such as, 北市 (beishi, North city) vs. 台北市 (taibei shi, Taipei city) did not fit the background of our raters, whom were all from Mainland China. Relata Synonymy Antonymy Hyponymy Meronymy Nearsynonymy Total 61 34 185 19 61 360 114 50 247 32 51 494 3.3. Semantic Tagging As a further step in describing the dataset, we identified the semantic type information of the positive pairs. For the 376 relata, their total frequency and PoS distribution were calculated in a combined corpus that included Sinica Corpus (Huang et al., 2004) and Chinese GigaWord (Huang et al., 2010). An additional three Ph.D. students were then asked to tag these relata according to their semantic types: 1. Basic/Subordinate/Superordinate: 研討會 (yantaohui, seminar) can be tagged as ”Basic” while 會議 (huiyi, meeting) can be tagged as ”Superodinate”; The next step in constructing the dataset involved the rating of relatedness between word pairs. We divided the 494 word relation pairs extracted from CWN into two groups equalling 250 and 244 word relation pairs. Each word relation pair was placed in carrier sentences that represented a specific relatio"
L16-1726,O02-2003,0,0.530534,"4583 2. Related Work 2.1. Datasets for DSMs For the training and evaluation of DSMs, several datasets have been widely used. While general-purpose datasets are still being made use of, a recent trend in the creation of benchmarks for the training of DSMs has been their construction from data derived from specific tasks performed by human participants (Hill et al., 2015). The general purpose resources used for the training and evaluation of DSMs have followed the model laid out by WordNet (Fellbaum, 1998). For Mandarin, Chinese, there is Chinese WordNet (CWN: (Huang et al., 2010)) and How-Net (Liu and Li, 2002). CWN consists of more than 50,000 word relation pairs covering the relationships of antonymy, synonymy, hyponymy/hypernymy, meronymy/holonymy, paranymy, nearsynonymy and variant. Paranymy in this context refers to co-hyponymy, i.e., lexical items that share hypernym pairs, while variant refers to pairs that are identical in meaning and use but differ in orthography (Huang et al., 2010). Nearsynonymy here refers to two words are related instead of similar. Because CWN was constructed using WordNet, it carries all its limitations (e.g. arbitrariness). This recently has been criticized, as it do"
L16-1726,D08-1103,0,0.0368024,"mation such as agent, event, patient, location, etc. While structurally distinct from WordNet, the relation pairs that can be extracted from its graph structure also have not been evaluated against human judgment. A well-known benchmark for the evaluation of DSM is the set of eighty multiple choice synonym questions from the Test of English as a Foreign Language (TOEFL). This dataset was introduced for the first time by (Landauer and Dumais, 1997) and it allowed for the comparision of computer performance (and other computational models) against that of the performance of college applicants. (Mohammad et al., 2008) used a similar paradigm for their dataset, built from 162 questions from the Graduate Record Examination (GRE) that targeted antonymy. Both datasets address only one semantic relation. Their sizes and focus on single relations make them inappropriate for an extensive evaluation of DSMs. BLESS (Baroni and Lenci Evaluation of Semantic Spaces) was the first English dataset especially designed for the evaluation of multiple semantic word relations. It features 200 basic target concepts instantiated by 26,554 relata. It contains five different word relations: co-hypernymy, hypernymy, meronymy, att"
L16-1726,P06-1015,0,0.0178279,"iminate the paradigmatic relations they hold. While this is indeed still a challenging task, the NLP community has adopted several approaches (Jia et al., 2014). The new approaches can be classified according to the need of annotated resources to achieve their goal of identifying multiple relations between given word pairs: supervised, which is a method of discrimination that relies on large scale datasets to train the models (Girju et al., 2006; Van Hage et al., 2006); semi-supervised, which uses a small dataset that acts as a seed to extract more relation instances and patterns iteratively (Pantel and Pennacchiotti, 2006); and unsupervised which does not require any manually annotated dataset to train the model (Jia et al., 2014; Santus et al., 2014a; Santus et al., 2014b; Santus et al., 2014c). While a dataset is necessary for training the former two approaches, the third approach will also need a dataset for testing. Because of this necessity, several banchmarks have been constructued for English. To meet the demand, several benchmarks have been constructed for English TOEFL (Landauer and Dumais, 1997), BLESS (Baroni and Lenci, 2011), LENCI/Benotto (Benotto, 2015) and EVALution 1.0 (Santus et al., 2015). How"
L16-1726,E14-4008,1,0.931977,"importance for tasks such as word sense disambiguation, lexical replacement, dictionary construction, and entailment understanding, to name a few (Sun, 2014). DSMs are founded on the assumption that the lexical similarity between words depends on their distributed context. According to the Distributional Hypothesis, if compared concepts or words have similar distributional features (hence, context), they are likely to have higher semantic similarity (Harris, 1954). One major shortcoming of current DSMs is that they cannot be used for discrimination among different relationships between words (Santus et al., 2014c). In recent years, DSMs have been adopted in several semantic tasks, including the identification of semantic relatedness and similarity. The former task is concerned with whether two words are related or not, independently from their paradigmatic similarity. The identification of semantic similarity, instead, is a more specific task and it consists in identifying words that are paradigmatically related (Sun, 2014; Murphy, 2003). DSMs have successfully addressed these two tasks by using vector cosine as a measure of similarity (Agirre et al., 2009). Unfortunately, however, DSMs are not yet a"
L16-1726,Y14-1018,1,0.929371,"importance for tasks such as word sense disambiguation, lexical replacement, dictionary construction, and entailment understanding, to name a few (Sun, 2014). DSMs are founded on the assumption that the lexical similarity between words depends on their distributed context. According to the Distributional Hypothesis, if compared concepts or words have similar distributional features (hence, context), they are likely to have higher semantic similarity (Harris, 1954). One major shortcoming of current DSMs is that they cannot be used for discrimination among different relationships between words (Santus et al., 2014c). In recent years, DSMs have been adopted in several semantic tasks, including the identification of semantic relatedness and similarity. The former task is concerned with whether two words are related or not, independently from their paradigmatic similarity. The identification of semantic similarity, instead, is a more specific task and it consists in identifying words that are paradigmatically related (Sun, 2014; Murphy, 2003). DSMs have successfully addressed these two tasks by using vector cosine as a measure of similarity (Agirre et al., 2009). Unfortunately, however, DSMs are not yet a"
L16-1726,W15-4208,1,0.896217,"DSMs. BLESS (Baroni and Lenci Evaluation of Semantic Spaces) was the first English dataset especially designed for the evaluation of multiple semantic word relations. It features 200 basic target concepts instantiated by 26,554 relata. It contains five different word relations: co-hypernymy, hypernymy, meronymy, attribute, and event. The structure of the dataset is in the form of “concept-relation-word”tuples. For each concept, BLESS also features semantically unrelated words. The shortcoming of BLESS, however, is that the dataset didn’t take synonymy and antonymy into consideration. Benotto (2015) constructed an English dataset targeting hypernymy, synonymy, and antonymy through elicitation experiments following the method introduced by (Paradis et al., 2009). Target words were firstly selected from sources such as WordNet and GermaNet (Hamp et al., 1997). An elicitation experiment was then conducted through Amazon Mechanical Turk to ask every participant to produce antonyms, synonyms and hyponyms of each word. In this way, 8,910 word relation pairs were collected. A DSMs-oriented resource that overcomes the shortcomings of the above datasets for English is EVALution 1.0 (Santus et al."
L16-1726,W11-2501,0,0.0645677,"Missing"
N19-1082,P14-1016,0,0.0205677,"skovec and Mcauley, 2012), i.e. when we extract information about one user, we consider the subgraph formed by the user and his/her direct neighbors. Each node corresponds to a Twitter user, who is represented by the set of posted tweets.7 Edges are defined by the followedby link, under the assumption that connected users are more likely to come from the same university or company. An example of the social media graph is reported in the appendices. Social media information extraction refers to the task of extracting information from users’ posts in online social networks (Benson et al., 2011; Li et al., 2014). In this paper, we aim at extracting education and job information from users’ tweets. Given a set of tweets posted by a user, the goal is to extract mentions of the organizations to which they belong. The fact that the tweets are short, highly contextualized and show special linguistic features makes this task particularly challenging. Dataset We construct two datasets, E DUCA TION and J OB , from the Twitter corpus released by Li et al. (2014). The original corpus contains millions of tweets generated by ≈ 10 thousand users, where the education and job mentions are annotated using distant s"
N19-1082,P13-1008,0,0.0602827,"Missing"
N19-1082,P11-1040,1,0.723765,"h as ego-networks (Leskovec and Mcauley, 2012), i.e. when we extract information about one user, we consider the subgraph formed by the user and his/her direct neighbors. Each node corresponds to a Twitter user, who is represented by the set of posted tweets.7 Edges are defined by the followedby link, under the assumption that connected users are more likely to come from the same university or company. An example of the social media graph is reported in the appendices. Social media information extraction refers to the task of extracting information from users’ posts in online social networks (Benson et al., 2011; Li et al., 2014). In this paper, we aim at extracting education and job information from users’ tweets. Given a set of tweets posted by a user, the goal is to extract mentions of the organizations to which they belong. The fact that the tweets are short, highly contextualized and show special linguistic features makes this task particularly challenging. Dataset We construct two datasets, E DUCA TION and J OB , from the Twitter corpus released by Li et al. (2014). The original corpus contains millions of tweets generated by ≈ 10 thousand users, where the education and job mentions are annotat"
N19-1082,P16-1101,0,0.0494878,"Task 2 and Task 3. The encoder and decoder BiLSTMs have the same dimension as the graph convolution layer. In Task 3, we concatenate a positional encoding to each text box’s representation by transforming its bounding box coordinates to a vector of length 32, and then applying a tanh activation. Baseline and Our Method We implement a two-layer BiLSTM with a conditional random fields (CRF) tagger as the sequential baseline (SeqIE). This architecture and its variants have been extensively studied and demonstrated to be successful in previous work on information extraction (Lample et al., 2016; Ma and Hovy, 2016). In the textual IE task (Task 1), our baseline is shown to obtain competitive results with the state-of-the-art method in the CONLL03 dataset. In the visual IE task (Task 3), in order to further increase the competitiveness of the baseline, we sequentially concatenate the horizontally aligned text boxes, therefore fully modeling the horizontal edges of the graph. Our baseline shares the same encoder and decoder architecture with GraphIE, but without the graph module. Both architectures have similar 6 6.1 Results Task 1: Textual Information Extraction Table 4 describes the NER accuracy on the"
N19-1082,P05-1045,0,0.155504,"involved in a wide range of duties for Washington ’s request … Figure 1: Example of the entity extraction task with an ambiguous entity mention (i.e. “...for Washington’s request...”). Aside from the sentential forward and backward edges (green, solid) which aggregate local contextual information, non-local relations — such as the co-referent edges (red, dashed) and the identicalmention edges (blue, dotted) — provide additional valuable information to reduce tagging ambiguity (i.e. P ERSON or L OCATION). Best viewed in color. Introduction the output space in a structured prediction framework (Finkel et al., 2005; Reichart and Barzilay, 2012; Hu et al., 2016). Such approaches, however, mostly overlook the richer set of structural relations in the input space. With reference to the example in Figure 1, the co-referent dependencies would not be readily exploited by simply constraining the output space, as they would not necessarily be labeled as entities (e.g. pronouns). In the attempt to capture non-local dependencies in the input space, alternative approaches define a graph that outlines the input structure and engineer features to describe it (Quirk and Poon, 2017). Designing effective features is ho"
N19-1082,P09-1113,0,0.0420489,"per, we aim at extracting education and job information from users’ tweets. Given a set of tweets posted by a user, the goal is to extract mentions of the organizations to which they belong. The fact that the tweets are short, highly contextualized and show special linguistic features makes this task particularly challenging. Dataset We construct two datasets, E DUCA TION and J OB , from the Twitter corpus released by Li et al. (2014). The original corpus contains millions of tweets generated by ≈ 10 thousand users, where the education and job mentions are annotated using distant supervision (Mintz et al., 2009). We sample the tweets from each user, maintaining the ratio between positive and negative posts.6 The obtained E DUCATION dataset consists of 443, 476 tweets generated by 7, 208 users, and the J OB dataset contains 176, 043 tweets generated by 1, 772 users. Dataset statistics are reported in Table 3. 5.3 Task 3: Visual Information Extraction Visual information extraction refers to the extraction of attribute values from documents formatted in various layouts. Examples include invoices and forms, whose format can be exploited to infer valuable information to support extraction. Dataset The cor"
N19-1082,P16-1105,0,0.216366,"Missing"
N19-1082,Q17-1008,0,0.101661,"Missing"
N19-1082,D14-1162,0,0.0835136,"IE with sentence-level graph module (cf. Figure 2(b)). 5.5 Implementation Details The models are trained with Adam (Kingma and Ba, 2014) to minimize the CRF objective. For regularization, we choose dropout with a ratio of 0.1 on both the input word representation and the hidden layer of the decoder. The learning rate is set to 0.001. We use the development set for early-stopping and the selection of the best performing hyperparameters. For CharCNN, we use 64-dimensional character embeddings and 64 filters of width 2 to 4 (Kim et al., 2016). The 100dimensional pretrained GloVe word embeddings (Pennington et al., 2014) are used in Task 1 and 2, and 64-dimensional randomly initialized word embeddings are used in Task 3. We use a twolayer GCN in Task 1, and a one-layer GCN in Task 2 and Task 3. The encoder and decoder BiLSTMs have the same dimension as the graph convolution layer. In Task 3, we concatenate a positional encoding to each text box’s representation by transforming its bounding box coordinates to a vector of length 32, and then applying a tanh activation. Baseline and Our Method We implement a two-layer BiLSTM with a conditional random fields (CRF) tagger as the sequential baseline (SeqIE). This a"
N19-1082,N18-1202,0,0.0617987,"Missing"
N19-1082,E17-1110,0,0.0223399,"in a structured prediction framework (Finkel et al., 2005; Reichart and Barzilay, 2012; Hu et al., 2016). Such approaches, however, mostly overlook the richer set of structural relations in the input space. With reference to the example in Figure 1, the co-referent dependencies would not be readily exploited by simply constraining the output space, as they would not necessarily be labeled as entities (e.g. pronouns). In the attempt to capture non-local dependencies in the input space, alternative approaches define a graph that outlines the input structure and engineer features to describe it (Quirk and Poon, 2017). Designing effective features is however challenging, arbitrary and time consuming, especially when the underlying structure is complex. Moreover, these approaches have limited capacity of capturing node interactions informed by the graph structure. Most modern Information Extraction (IE) systems are implemented as sequential taggers. While such models effectively capture relations in the local context, they have limited capability of exploiting non-local and non-sequential dependencies. In many applications, however, such dependencies can greatly reduce tagging ambiguity, thereby improving o"
N19-1082,N16-1030,0,0.365853,"representation, i.e. gi = Enc(si ), and conducts graph convolution on every node, propagating information between its neighbors, and integrating such information into a new hidden representation. Specifically, each layer of 4.3 Decoder To support tagging, the learned representation is propagated to the decoder. 3 We choose this simple normalization strategy instead of the two-sided normalization in Kipf and Welling (2016), as it performs better in the experiments. The same strategy is also adopted by Zhang et al. (2018). 754 (i) In our work, the decoder is instantiated as a BiLSTM+CRF tagger (Lample et al., 2016). The output representation of the graph module, GCN(si ), is split into two vectors of the same length, which are used as the initial hidden states for the forward and backward LSTMs, respectively. In this way, the graph contextual information is propagated to each word through the LSTM. Specifically, we have   (i) (i) z1:k = RNN h1:k ; GCN(si ), Θdec , (5) 5 (i) where h1:k are the output hidden states of the encoder, GCN(si ) represents the initial state, and Θdec is the decoder parameters. A simpler way to incorporate the graph representation into the decoder is concatenating with its inp"
N19-1082,N12-1008,1,0.865342,"nge of duties for Washington ’s request … Figure 1: Example of the entity extraction task with an ambiguous entity mention (i.e. “...for Washington’s request...”). Aside from the sentential forward and backward edges (green, solid) which aggregate local contextual information, non-local relations — such as the co-referent edges (red, dashed) and the identicalmention edges (blue, dotted) — provide additional valuable information to reduce tagging ambiguity (i.e. P ERSON or L OCATION). Best viewed in color. Introduction the output space in a structured prediction framework (Finkel et al., 2005; Reichart and Barzilay, 2012; Hu et al., 2016). Such approaches, however, mostly overlook the richer set of structural relations in the input space. With reference to the example in Figure 1, the co-referent dependencies would not be readily exploited by simply constraining the output space, as they would not necessarily be labeled as entities (e.g. pronouns). In the attempt to capture non-local dependencies in the input space, alternative approaches define a graph that outlines the input structure and engineer features to describe it (Quirk and Poon, 2017). Designing effective features is however challenging, arbitrary"
N19-1082,W08-0602,0,0.0979603,"Missing"
N19-1082,R11-1004,0,0.052394,"Missing"
N19-1082,P15-1150,0,0.140715,"Missing"
N19-1082,W03-0419,0,0.319986,"Missing"
N19-1082,P18-2038,0,0.0295615,"Missing"
N19-1082,D18-1244,0,0.167599,"context, which — instead of being used directly for classification — is projected to the decoder to enrich local information and perform sequence tagging. A handful of other information extraction approaches have used graph-based neural networks. Miwa and Bansal (2016) applied Tree LSTM (Tai et al., 2015) to jointly represent sequences and dependency trees for entity and relation extraction. On the same line of work, Peng et al. (2017) and Song et al. (2018) introduced Graph LSTM, which extended the traditional LSTM to graphs by enabling a varied number of incoming edges at each memory cell. Zhang et al. (2018) exploited graph convolutions to pool information over pruned dependency trees, outperforming existing sequence and dependency-based neural models in a relation extraction task. These studies differ from ours in several respects. First, they can only model wordlevel graphs, whereas our framework can learn non-local context either from word- or sentencelevel graphs, using it to reduce ambiguity during tagging at the word level. Second, all these studies achieved improvements only when using dependency trees. We extend the graph-based approach to validate the benefits of using other types of rel"
P18-2088,N09-1003,0,0.172175,"Missing"
P18-2088,P14-1023,0,0.0478418,"ille University 4 ISTD, Singapore University of Technology and Design Abstract ing the semantic content of a word. Such a limitation led to the introduction of alternative metrics based on feature ranking, which have been reported to outperform vector cosine in several similarity tasks (Santus et al., 2016a,b). Recently, the focus of the research on word representations has been shifting onto the so-called word embeddings (WE), which are dense vectors obtained by means of neural network training that achieved significant improvements in several similarity-related tasks (Mikolov et al., 2013a; Baroni et al., 2014). Although the representation type of the embeddings was helpful for reducing the sparsity of traditional count vectors, their nature does not sensibly differ (Levy et al., 2015). Most research works involving WE still adopt vector cosine for similarity estimation, yet little experimentation has been done on alternative metrics for comparing dense representations (exceptions include Camacho-Collados et al. (2015)). Some attempts to directly transfer rank-based measures from traditional DSMs to WE have faced difficulties (see, for example, Jebbara et al. (2018)). In this paper, we suggest a pos"
P18-2088,W16-2502,0,0.02045,"rity about them), as opposed to ‘genuine’ semantic similarity (i.e. the relation holding between concepts such as coffee and tea) (Agirre et al., 2009; Hill et al., 2015; Gladkova and Drozd, 2016). Therefore, when testing a DSM, it is important to pay attention to what type of semantic relation is actually modeled by the evaluation dataset. Moreover, researchers pointed out that similarity estimation alone does not constitute a strong benchmark, as the inter-annotator agreement is relatively low in all datasets and the performance of several automated systems is already above the upper bound (Batchkarov et al., 2016). As a consequence, workshops such as RepEval have been organized with the explicit purpose of finding alternative evaluation tasks for DSMs. A recent proposal is the challenging outlier detection task (Camacho-Collados and Navigli, 2016; Blair et al., 2016), which consists in the recognition of cluster membership, as well as of a relative degree of semantic dissimilarity. The task is described as follows: given a group of words, identify the outlier, namely the word that does not belong to the group (i.e. the one that is less similar to the others). On top of its potential applications (e.g."
P18-2088,D14-1162,0,0.0879466,"it on the similarity subset of WordSim dataset, obtaining the optimal value of p = 0.1, which has been successfully used in all evaluations, under all settings (i.e. embedding types and training corpora). Such regularity allows us to consider p = 0.1 as a constant, therefore dropping p. Since in WE we can drop also the parameter N by defining N = |f |, AP SynP can be not parametrized at all. 4 4.1 Datasets Evaluation Settings Embeddings For our experiments, we used two popular word embeddings architectures: the Skip-Gram with negative sampling (Mikolov et al., 2013a,b) and the GloVe vectors (Pennington et al., 2014) (standard hyperparameter settings: 300 dimensions, 4 We also performed experiments with CBOW embeddings (Mikolov et al., 2013b), but results were irregular and inconsistent. We leave therefore their analysis to future work. 5 Dump of Nov. 2014, approx. 1.7 billion words. 554 Cosine AP Syn AP SynP Skip-Gram WordSim-353 MEN Simlex-999 0.736 0.758 0.364 0.599 0.643 0.343 0.710 0.737 0.369 WordSim-353 0.511 0.356 0.607 GloVe MEN 0.640 0.393 0.670 Simlex-999 0.311 0.174 0.335 Table 1: Similarity Estimation, Spearman Correlation by Setting. Embeddings trained on Wikipedia. CC − Cos AP Syn AP SynP P"
P18-2088,D17-1068,1,0.848381,"y are likely to be useful for setting the outliers apart. In fact, a cohesive cluster should be mostly characterized by the same ‘salient’ dimensions, and thus, basing word comparisons on such dimensions should lead to more reliable estimates of cluster membership. In our contribution, we propose to adapt AP Syn, a metric originally proposed by Santus et al. (2016a,b), to dense word embeddings representations.3 AP Syn was shown to perform well on both synonymy detection and similarity estimation tasks, and it was recently adapted to achieve state-of-the-art results in thematic fit estimation (Santus et al., 2017). The original AP Syn formula is shown in equation 1. i=N AP Syn(wx , wy ) = X i=0 1 AV G(rsx (fi ), rssy (fi )) (1) For every feature fi in the intersection between the top N features of two vectors wx and wy , we 3 The number of dimensions in word embeddings is in the scale of hundreds, and thus the dimensionality is way lower than in the original DSMs used by Santus and colleagues. 553 add the inverse of the average rank of such feature, rsx (fx ) and rsy (fy ), in the two decreasingly value-sorted vectors sx and sy (in traditional vectors, often the parameter N ≥ 1000, but in WE N = |f |)."
P18-2088,Y16-2021,1,0.893749,"d Embeddings Enrico Santus1 , Hongmin Wang2 , Emmanuele Chersoni3 and Yue Zhang4 esantus@mit.edu hongmin wang@cs.ucsb.edu emmanuelechersoni@gmail.com yue zhang@sutd.edu.sg 1 Computer Science and Artificial Intelligence Lab, MIT 2 Department of Computer Science, University of California Santa Barbara 3 Aix-Marseille University 4 ISTD, Singapore University of Technology and Design Abstract ing the semantic content of a word. Such a limitation led to the introduction of alternative metrics based on feature ranking, which have been reported to outperform vector cosine in several similarity tasks (Santus et al., 2016a,b). Recently, the focus of the research on word representations has been shifting onto the so-called word embeddings (WE), which are dense vectors obtained by means of neural network training that achieved significant improvements in several similarity-related tasks (Mikolov et al., 2013a; Baroni et al., 2014). Although the representation type of the embeddings was helpful for reducing the sparsity of traditional count vectors, their nature does not sensibly differ (Levy et al., 2015). Most research works involving WE still adopt vector cosine for similarity estimation, yet little experiment"
P18-2088,W16-2508,0,0.275329,", it is important to pay attention to what type of semantic relation is actually modeled by the evaluation dataset. Moreover, researchers pointed out that similarity estimation alone does not constitute a strong benchmark, as the inter-annotator agreement is relatively low in all datasets and the performance of several automated systems is already above the upper bound (Batchkarov et al., 2016). As a consequence, workshops such as RepEval have been organized with the explicit purpose of finding alternative evaluation tasks for DSMs. A recent proposal is the challenging outlier detection task (Camacho-Collados and Navigli, 2016; Blair et al., 2016), which consists in the recognition of cluster membership, as well as of a relative degree of semantic dissimilarity. The task is described as follows: given a group of words, identify the outlier, namely the word that does not belong to the group (i.e. the one that is less similar to the others). On top of its potential applications (e.g. ontology learning), detecting outliers in clusters is a goal that poses a more strict quality requirement on the distributional representations compared to tests based simply on pairwise comparisons, as it is required that similar words"
P18-2088,L16-1723,1,0.844757,"d Embeddings Enrico Santus1 , Hongmin Wang2 , Emmanuele Chersoni3 and Yue Zhang4 esantus@mit.edu hongmin wang@cs.ucsb.edu emmanuelechersoni@gmail.com yue zhang@sutd.edu.sg 1 Computer Science and Artificial Intelligence Lab, MIT 2 Department of Computer Science, University of California Santa Barbara 3 Aix-Marseille University 4 ISTD, Singapore University of Technology and Design Abstract ing the semantic content of a word. Such a limitation led to the introduction of alternative metrics based on feature ranking, which have been reported to outperform vector cosine in several similarity tasks (Santus et al., 2016a,b). Recently, the focus of the research on word representations has been shifting onto the so-called word embeddings (WE), which are dense vectors obtained by means of neural network training that achieved significant improvements in several similarity-related tasks (Mikolov et al., 2013a; Baroni et al., 2014). Although the representation type of the embeddings was helpful for reducing the sparsity of traditional count vectors, their nature does not sensibly differ (Levy et al., 2015). Most research works involving WE still adopt vector cosine for similarity estimation, yet little experiment"
P18-2088,N15-1059,0,0.147072,"led word embeddings (WE), which are dense vectors obtained by means of neural network training that achieved significant improvements in several similarity-related tasks (Mikolov et al., 2013a; Baroni et al., 2014). Although the representation type of the embeddings was helpful for reducing the sparsity of traditional count vectors, their nature does not sensibly differ (Levy et al., 2015). Most research works involving WE still adopt vector cosine for similarity estimation, yet little experimentation has been done on alternative metrics for comparing dense representations (exceptions include Camacho-Collados et al. (2015)). Some attempts to directly transfer rank-based measures from traditional DSMs to WE have faced difficulties (see, for example, Jebbara et al. (2018)). In this paper, we suggest a possible solution to this problem by adapting AP Syn, a rank-based similarity metric originally proposed for sparse vectors (Santus et al., 2016b,a), to low-dimensional word embeddings. This goal is achieved by removing the parameter N (the extent of the feature overlap to be taken into account) and adding a smoothing parameter that is proven to be constant under multiple settings, therefore making the measure unsup"
P18-2088,Y14-1018,1,0.862126,"ne as a baseline and we test an adaptation of a rank-based measure to the dense features of the word embeddings. Vector cosine computes the correlation between all the vector dimensions, independently of their relevance for a given word pair or for a semantic cluster, and this could be a limitation for discerning different degrees of dissimilarity. The alternative rank-based measure is based on the hypothesis that similarity consists of sharing many relevant features, whereas dissimilarity can be described as either the non-sharing of relevant features or the sharing of non-relevant features (Santus et al., 2014, 2016b). This hypothesis could turn out to be very helpful for a task like the outlier detection, where prominent features might be the key to improve clustering quality: semantic dimensions that are shared by many of the cluster elements should be weighted more, as they are likely to be useful for setting the outliers apart. In fact, a cohesive cluster should be mostly characterized by the same ‘salient’ dimensions, and thus, basing word comparisons on such dimensions should lead to more reliable estimates of cluster membership. In our contribution, we propose to adapt AP Syn, a metric origi"
P18-2088,W16-2507,0,0.0195833,"es assigned to the pairs by the subjects and the cosines of the corresponding vectors (similary estimation task). Similarity as modeled by DSMs has been under debate, as its definition is underspecified. It in fact includes an ambiguity with the more generic notion of semantic relatedness, which is present also in many popular datasets (i.e. the concepts of coffee and cup are certainly related, but there is very little similarity about them), as opposed to ‘genuine’ semantic similarity (i.e. the relation holding between concepts such as coffee and tea) (Agirre et al., 2009; Hill et al., 2015; Gladkova and Drozd, 2016). Therefore, when testing a DSM, it is important to pay attention to what type of semantic relation is actually modeled by the evaluation dataset. Moreover, researchers pointed out that similarity estimation alone does not constitute a strong benchmark, as the inter-annotator agreement is relatively low in all datasets and the performance of several automated systems is already above the upper bound (Batchkarov et al., 2016). As a consequence, workshops such as RepEval have been organized with the explicit purpose of finding alternative evaluation tasks for DSMs. A recent proposal is the chall"
P18-2088,S13-1005,0,0.0282831,"nstant value ranging between 0 and 1 (excluded), as shown in equation 2, such that now the number of ranks contributing to the final score is widen to all features (see the smoother curve of AP SynP ower in Figure 1). We name this variant AP SynP ower or, shortly, AP SynP . i=|f | AP SynP (wx , wy ) = X i=0 Figure 1: Comparison of weight per feature rank in AP Syn and AP SynP (p = 0.1) across feature ranks ranging from 1 to 300. context size of 10 and negative sampling).4 For comparison with Camacho-Collados and Navigli (2016) on outlier detection, we used the same training corpora: the UMBC (Han et al., 2013) and the English Wikipedia.5 4.2 As for the similarity estimation task, we evaluate the Spearman correlation between systemgenerated scores and human judgments. We used three popular benchmark datasets: WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). It is important to point out that SimLex-999 is the only one specifically built for targeting genuine semantic similarity, while the others tend to mix similarity and relatedness scores. As for outlier detection, we evaluate our DSMs on the 8-8-8 dataset (Camacho-Collados and Navigli, 2016). The"
P18-2088,J15-4004,0,0.349061,"en the average scores assigned to the pairs by the subjects and the cosines of the corresponding vectors (similary estimation task). Similarity as modeled by DSMs has been under debate, as its definition is underspecified. It in fact includes an ambiguity with the more generic notion of semantic relatedness, which is present also in many popular datasets (i.e. the concepts of coffee and cup are certainly related, but there is very little similarity about them), as opposed to ‘genuine’ semantic similarity (i.e. the relation holding between concepts such as coffee and tea) (Agirre et al., 2009; Hill et al., 2015; Gladkova and Drozd, 2016). Therefore, when testing a DSM, it is important to pay attention to what type of semantic relation is actually modeled by the evaluation dataset. Moreover, researchers pointed out that similarity estimation alone does not constitute a strong benchmark, as the inter-annotator agreement is relatively low in all datasets and the performance of several automated systems is already above the upper bound (Batchkarov et al., 2016). As a consequence, workshops such as RepEval have been organized with the explicit purpose of finding alternative evaluation tasks for DSMs. A r"
S15-2113,W14-2608,0,0.145372,"ifying sarcasm which goes far behind the surface of the text and takes into account features on four levels: signatures, degree of unexpectedness, style, and emotional scenarios. They have demonstrated that these features do not help the identification in isolation. However, they do if they are combined in a complex framework. Barbieri and Saggion (2014) focused their approach on the use of lexical and semantic features, such as the frequency of the words in different reference corpora, the length of the words, and the number of related synsets in WordNet (Miller and Fellbaum, 1998). Finally, Buschmeier et al. (2014) assessed the impact of features used in previous studies, and they provide an important baseline for irony detection in English. Many datasets for the study of irony and sarcasm in Twitter are nowadays available. Thanks to the use of hashtags, it is easier to collect data with specific characteristics in Twitter. Reyes et al. (2013), for example, created a corpus of 40.000 tweets with four categories: Irony, Education, Humour, and Politics. Among the other resources, it is worth mentioning the sarcastic Amazon product reviews collected by Filatova (2012) and the Italian examples collected and"
S15-2113,W10-2914,0,0.0735455,"relying on the presence of emoticons, onomatopoeic expressions, and heavy punctuation in the text surface. Hao and Veale (2010) have investigated similes of the form “x as y” in a large corpus, proposing a method to automatically discriminate ironic from non-ironic similes. Tsur et al. (2010) proposed a semi-supervised approach for the automatic recognition of sarcasm in Amazon product reviews, exploiting some features that were specific to Amazon. Their method employed two modules: a semi-supervised acquisition of sarcastic patterns and a classifier. This method was then applied to tweets by Davidov et al. (2010), achieving even better results. Gonz´alez-Ib´an˜ ez et al. (2011) constructed a corpus of sarcastic tweets and used it to compare judgements made by humans and machine learning algorithms, concluding that none of them performed well. 674 More recently, Reyes et al. (2013) defined a complex model for identifying sarcasm which goes far behind the surface of the text and takes into account features on four levels: signatures, degree of unexpectedness, style, and emotional scenarios. They have demonstrated that these features do not help the identification in isolation. However, they do if they a"
S15-2113,filatova-2012-irony,0,0.0226777,"and Fellbaum, 1998). Finally, Buschmeier et al. (2014) assessed the impact of features used in previous studies, and they provide an important baseline for irony detection in English. Many datasets for the study of irony and sarcasm in Twitter are nowadays available. Thanks to the use of hashtags, it is easier to collect data with specific characteristics in Twitter. Reyes et al. (2013), for example, created a corpus of 40.000 tweets with four categories: Irony, Education, Humour, and Politics. Among the other resources, it is worth mentioning the sarcastic Amazon product reviews collected by Filatova (2012) and the Italian examples collected and annotated by Gianti et al. (2012), later used in Bosco et al. (2013). 3 Methodology 3.1 Data Pre-processing Considering the unregulated and arbitrary nature of the texts we are working with, we use some heuristic rules to pre-process them. These rules help us get more reliable syntactic structures when calling the syntactic parser. Twitter users often use repeated vowels (e.g. “loooove”) or capitalization (e.g. “LOVE”) to emphasize certain sentiments or emotions. The normalization consists of removing the repeated vowels (e.g. from “loooove” to “love”) a"
S15-2113,S15-2080,0,0.0253939,"d slang (“slng”) is not a self-evident task. The reason why none of these studies has proved to be the representative method that could widely be adopted and applied by other researchers is that they have not yet reached optimal results. Thus, the devising of a computational model able to accurately detect polarity is very much on-going. 673 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 673–678, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics This paper describes the model we developed for Task 11 of SemEval-2015 (Ghosh et al., 2015), which is concerned with the Sentiment Analysis of Figurative Language in Twitter. Our model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashi"
S15-2113,P11-2102,0,0.158153,"f polarity shifting in sentiment analysis still requires much research. For example, Li, et.al. (2010), explores the polarity shifters in English which significantly improve the performance of sentiment analysis. Besides, figurative uses of Currently, there is no method that can guarantee the unequivocal recognition of irony or sarcasm. Training a computer to perform such a highly pragmatic task does indeed pose a challenge to computational linguists. A good number of studies have been recently devoted to finding a solution to the problem. Most of them have focused on tweets (Gonz´alez-Ib´an˜ ez et al., 2011; Reyes et al., 2013; Liebrecht et al., 2013; Riloff et al., 2013; Barbieri et al., 2014; Vanzo et al., 2014). Identifying figurative language in short messages (generally consisting of no more than 140 characters) that do not make use of conventional language, but employ “little space-consuming” elements, such as emoticons (“:D”), abbreviations (“abbr.”) and slang (“slng”) is not a self-evident task. The reason why none of these studies has proved to be the representative method that could widely be adopted and applied by other researchers is that they have not yet reached optimal results. Th"
S15-2113,P03-1054,0,0.0163421,"orm “?!”. We also compiled an emoticon dictionary based on training data and internet resources. Another step that we considered relevant at this point is the maximal matching segmentation. The segmentation is, in fact, often lost in tweets, as white spaces and punctuation are not always used in their customary format (e.g. “yeahright”). In order to get rid of this problem, we tried to segment all the out of vocabulary tokens through a maximal matching algorithm according to an English dictionary (e.g. the token “yeahright” would be segmented as “yeah right”). Finally, we use Stanford parser (Klein and Manning, 2003) to get the POS tags and dependency structures of the normalized tweets. 3.2 Feature Set After the pre-processing, we then extract features of the following kinds. UniToken Token uni-grams are the basic features in our approach. The normalized forms of the emphasized tokens are put in a special bag with tags describing their emphasis types {duplicate vowel, capitalized, heavy punctuation, emoticon} BiToken Bi-grams of the normalized tokens are also used as features. DepTokenPair The “parent-child” pairs based on dependency structures are also used as features. PolarityWin In order to identify"
S15-2113,W07-0101,0,0.121857,"Missing"
S15-2113,C10-1072,1,0.84487,"Missing"
S15-2113,W13-1605,0,0.0768567,"Missing"
S15-2113,D13-1066,0,0.102715,"Missing"
S15-2113,Y14-1047,0,0.237799,"r Task 11 of SemEval-2015 (Ghosh et al., 2015), which is concerned with the Sentiment Analysis of Figurative Language in Twitter. Our model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashion, sarcasm is generally understood as the use of irony “to mock or convey contempt” (Stevenson, 2010). While irony and sarcasm are well studied in linguistics and psychology, their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to automatically identify irony and sarcasm in tweets and comments. Carvalho et a"
S15-2113,C14-1221,0,0.0609998,"xplores the polarity shifters in English which significantly improve the performance of sentiment analysis. Besides, figurative uses of Currently, there is no method that can guarantee the unequivocal recognition of irony or sarcasm. Training a computer to perform such a highly pragmatic task does indeed pose a challenge to computational linguists. A good number of studies have been recently devoted to finding a solution to the problem. Most of them have focused on tweets (Gonz´alez-Ib´an˜ ez et al., 2011; Reyes et al., 2013; Liebrecht et al., 2013; Riloff et al., 2013; Barbieri et al., 2014; Vanzo et al., 2014). Identifying figurative language in short messages (generally consisting of no more than 140 characters) that do not make use of conventional language, but employ “little space-consuming” elements, such as emoticons (“:D”), abbreviations (“abbr.”) and slang (“slng”) is not a self-evident task. The reason why none of these studies has proved to be the representative method that could widely be adopted and applied by other researchers is that they have not yet reached optimal results. Thus, the devising of a computational model able to accurately detect polarity is very much on-going. 673 Proce"
S15-2113,baccianella-etal-2010-sentiwordnet,0,\N,Missing
S15-2113,E14-3007,0,\N,Missing
S18-1115,S13-1005,0,0.16065,"Missing"
S18-1115,S18-1116,0,0.0792436,"Missing"
S18-1115,S18-1149,0,0.0363408,"Missing"
S18-1115,E17-2013,0,0.185703,"Missing"
S18-1115,C92-2082,0,0.323886,"systems for any individual subtask. Along with a specific source corpus and vocabulary, each subtask features its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification"
S18-1115,S15-2151,1,0.928124,"i♥ Luis Espinosa-Anke♣ Sergio Oramas♦ Tommaso Pasini♥ Enrico Santus♥ Vered Shwartz♠ Roberto Navigli♥ Horacio Saggion♦ ♣ School of Computer Science and Informatics, Cardiff University, United Kingdom ♥ Computer Science Department, Sapienza University of Rome, Italy ♦ Pompeu Fabra University, Barcelona, Spain ♥ MIT, United States ♠ Bar-Ilan University, Ramat Gan, Israel ♣ {camachocolladosj,espinosa-ankel}@cardiff.ac.uk, ♥ {dellibovi,pasini,navigli}@di.uniroma1.it, ♦ {name.surname}@upf.edu, ♥ esantus@mit.edu, ♠ vered1986@gmail.com Abstract web retrieval, website navigation or records management (Bordea et al., 2015). This paper describes the SemEval 2018 Shared Task on Hypernym Discovery. We put forward this task as a complementary benchmark for modeling hypernymy, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input. Instead, our reformulated task is defined as follows: given an input term, retrieve (or discover) its suitable hypernyms from a target corpus. We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were al"
S18-1115,S16-1168,0,0.263061,"s either a concept or a 4 As an example, the term apple could either refer to a fruit (if labeled as concept) or to a company (if labeled as named entity). 5 http://ebiquity.umbc. edu/blogger/2013/05/01/ umbc-webbase-corpus-of-3b-english-words/ 6 http://dbpubs.stanford.edu:8091/ testbed/doc2/WebBase/ ˜ 3 In fact, WordNet encodes hypernym and instance as two separate semantic relations. Instances are always leaf (terminal) nodes in their hierarchies. 714 sources of information with respect to the corpora used in previous tasks, such as Wikipedia in the SemEval 2016 task on taxonomy extraction (Bordea et al., 2016). In fact, the encyclopedic nature of Wikipedia has been exploited in a wide variety of works (Ponzetto and Strube, 2007; Flati et al., 2016; Gupta et al., 2016), and differs substantially from the web-based corpus we put forward here. As source corpus for the Italian subtask (1B) we instead used the 1.3-billion-word itWac corpus7 (Baroni et al., 2009), extracted from different sources of the web within the .it domain. Finally, as source corpus for the Spanish subtask (1C) we considered the 1.8-billion-word Spanish corpus8 (Cardellino, 2016), which also contains heterogeneous documents from di"
S18-1115,S18-1150,0,0.0250373,"Missing"
S18-1115,E17-2036,1,0.898611,"Missing"
S18-1115,N15-1098,0,0.0369984,"ttps://competitions. codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within do"
S18-1115,D16-1041,1,0.888795,"Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications (Camacho-Collados, 2017). In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains. The ma"
S18-1115,S18-1151,0,0.0726421,"14 We used the open-source code available at https:// bitbucket.org/luisespinosa/taxoembed 15 https://github.com/vered1986/ UnsupervisedHypernymy 16 Following the conclusions from Shwartz et al. (2017), we set the hyper-parameters to: SLQS: median, PLMI, N = 100 and APSyn: N = 500. 13 Although only P@5 is displayed in the tables due to lack of space, the other thresholds were used in the official evaluation as well. 717 5.2 Participant Systems in general they were outperformed by supervised systems, in some cases their performance came close, especially for concepts. For instance, the ADAPT (Maldonado and Klubika, 2018) system, which is based on a simple similarity measure applied to word embeddings, achieved a very decent 8.13 MAP percentage performance on the medical dataset, using neither supervision nor external resources. Supervised systems produced a larger gap for entities, probably due, as mentioned above, to the lower diversity of possible hypernyms. Table 3 shows a summary of all participant systems, displaying their main features with respect to supervison and external resources used, if any. 5.3 Results A summary of the results is provided in tables 3 to 7, respectively describing results for Eng"
S18-1115,P14-1113,0,0.377885,"t al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathe"
S18-1115,P16-2081,1,0.844897,"Missing"
S18-1115,P10-1134,1,0.838694,"y individual subtask. Along with a specific source corpus and vocabulary, each subtask features its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni"
S18-1115,D16-1234,0,0.0290792,"rrent research in hypernymy modeling with this novel discovery setting. Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition. It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (Shwartz et al., 2016): indeed, successfully identifying this lexical relation substantially improves Question Answering applications (Prager et al., 2008; Yahya et al., 2013), Textual Entailment and Semantic Search systems (Hoffart et al., 2014; Roller et al., 2014; Roller and Erk, 2016). In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (Yu et al., 2015), which are in turn useful resources for downstream tasks such as 712 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 712–724 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 1A: English 1B: Italian 1C: Spanish Term sorrow Nina Simone guacamole 2A: Medical pulmonary embolism 2B: Music Green Day Hypernym(s) sadness, unhappiness musicista, pianista, persona salsa para mojar, salsa, alimento"
S18-1115,D17-1022,0,0.0580762,"Missing"
S18-1115,C14-1097,0,0.0705885,"Missing"
S18-1115,S18-1146,0,0.019286,"Missing"
S18-1115,E17-2064,0,0.0114909,"stributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathered from different and heterogeneous sources. A system operating in this setting r"
S18-1115,L16-1722,1,0.929363,". codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld ap"
S18-1115,L16-1528,1,0.817501,"t appeared too vague or general, as well as terms with mis-attributed domains. Domain-specific corpora. As source corpus for the medical domain (subtask 2A) we provided a combination of texts drawn from the MEDLINE9 (Medical Literature Analysis and Retrieval System) repository, which contains academic documents such as scientific publications and paper abstracts. This corpus contains 130 million words. As regards the music domain (subtask 2B), instead, the source corpus we compiled is a concatenation of several music-specific corpora, i.e. music biographies from Last.fm contained in ELMD 2.0 (Oramas et al., 2016), articles from the music branch of Wikipedia, and a corpus of album customer reviews from Amazon (Oramas et al., 2017). The resulting corpus reaches 100 million words in total. 4.1.2 Term Collection Vocabulary Creation With the aim of simplifying the task for participants by providing a unified hypernym search space, we built a series of vocabulary files including all the possible hypernyms on each dataset. Each vocabulary was constructed by considering all the words occurring at least N times across the source corpus of the corresponding subtask. We set N to five and three in the general-pur"
S18-1115,E14-4008,1,0.929727,"i Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose te"
S18-1115,P17-1192,0,0.0129572,"es its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al."
S18-1115,P16-1226,1,0.918452,"017). In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains. The main goal of this task is that of complementing current research in hypernymy modeling with this novel discovery setting. Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition. It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (Shwartz et al., 2016): indeed, successfully identifying this lexical relation substantially improves Question Answering applications (Prager et al., 2008; Yahya et al., 2013), Textual Entailment and Semantic Search systems (Hoffart et al., 2014; Roller et al., 2014; Roller and Erk, 2016). In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (Yu et al., 2015), which are in turn useful resources for downstream tasks such as 712 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 712–724 New Orleans, Louisiana, June 5–6, 201"
S18-1115,E17-1007,1,0.68931,"ions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications (Camacho-Coll"
S18-1115,S18-1148,0,0.0202622,"Missing"
S18-1115,J17-4004,0,0.0330605,"Missing"
S18-1115,S17-1004,0,0.0128295,"ish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were allowed to compete in any or all of the subtasks. Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks. Data, results and further information about the task can be found at https://competitions. codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2"
S18-1115,D17-1123,0,0.0255651,"Missing"
S18-1115,C14-1212,0,0.0535272,"a et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathered from different a"
S18-1115,P10-2021,0,0.026139,"aining set, separately for each subtask and measure. where Q is a sample of experiment runs, AP(·) refers to average precision, i.e. an average of the correctness of each individual obtained hypernym from the search space. Mean Reciprocal Rank (MRR). MRR rewards the position of the first correct result in a ranked list of outcomes, and is defined as: |Q| 1 X 1 MRR = |Q| ranki i=1 where ranki refers to the rank position of the first relevant outcome for the ith run. While its main field of application is Information Retrieval, it has also been used in NLP tasks such as collocation recognition (Wu et al., 2010; Rodr´ıguezFern´andez et al., 2016). In addition to the above, we also provide results according to P@k, i.e. the number of correctly retrieved hypernyms at different cut-off thresholds, specifically k ∈ {1, 3, 5, 15}.13 5.1 Baselines We compared the participating systems with both supervised and unsupervised baselines for each subtask, inspired by recent work on hypernym detection and discovery. In this section we briefly describe each of them. 5.1.1 Unsupervised Baselines Supervised Baselines We first used a na¨ıve most frequent hypernym (MFH) baseline, which simply returns, for each input"
S18-1115,S18-1147,0,0.0304331,"Missing"
S18-1163,N15-1098,1,0.827444,"• total score: sum of the scores of w1 −f eat if prediction by score is 1, of w2 − f eat otherwise; 3.1 Experiments Choosing the Training Set During the practice phase, we noticed that the training set and the validation set show very different distributions. Running 5-fold cross validation experiments on either dataset, we obtained very high scores (sometimes close to 0.95). However, such scores did not generalize to the other dataset, where they dropped to about 0.60. This was only partially due to lexical memorization (some lexemes were present in multiple triples of the same dataset, cf. Levy et al. (2015); Santus et al. (2016b)). In fact, investigating the frequency of the words in the triples, we found that, on average, in our index, the first and the second words, w1 and w2, were about four times more frequent in the validation than in the training set (respectively 4.7M and 5.4M versus 0.9M and 1M ); similarly, the third word (i.e. f eat) was almost twice more frequent in the validation than in the training set (i.e. 3.9M versus 2.9M ). When the test set was made available, we could verify that its frequency distribution resembled the one in the validation set, with the first and second wor"
S18-1163,P14-1023,0,0.508854,"ana over watermelon). The system relies on an XGB classifier trained on carefully engineered graph-, pattern- and word embeddingbased features. It participated in the SemEval2018 Task 10 on Capturing Discriminative Attributes, achieving an F1 score of 0.73 and ranking 2nd out of 26 participant systems. 1 Introduction The recent introduction of popular software packages for training neural word embeddings (Mikolov et al., 2013a,b; Levy and Goldberg, 2014) has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks (Baroni et al., 2014). However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement (Batchkarov et al., 2016; Faruqui et al., 2016; Santus et al., 2016a). Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead"
S18-1163,W16-2502,0,0.061441,"Mikolov et al., 2013a,b; Levy and Goldberg, 2014) has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks (Baroni et al., 2014). However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement (Batchkarov et al., 2016; Faruqui et al., 2016; Santus et al., 2016a). Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead on the extraction of semantic differences between lexical meanings (Krebs and Paperno, 2016): given two words and an attribute (i.e., a discrete semantic feature), a system has to predict whether the attribute describes a difference between the corresponding concepts or not (e.g. wing is an attribute of plane, but not of helicopter). 2 2.1 Capturing Discriminative Attributes Task and Dataset Description The"
S18-1163,L18-1286,1,0.827694,"Missing"
S18-1163,D14-1162,0,0.0818355,"ple times in different POSdependency combinations. However, we found that f eat rarely appears in the top N features to the elements of the triples as w1, w2 and f eat. A training and validation set have been provided for system development (figures in Table 2). 2.2 Methodology Embeddings and Graphs For the Discriminative Attributes task, we combined word embeddings, patterns and information extracted from a graph-based distributional model. Concerning the word embeddings, we used the vectors produced by two popular frameworks for word embeddings: Word2Vec (Mikolov et al., 2013a,b) and GloVe (Pennington et al., 2014).2 The Word2Vec Skip-Gram architecture is a singlelayer neural network, based on the dot-product between word vectors, in which the vector representation is optimized to predict the context of a target word given the word itself. The context generally consists of a word window of a fixed width around the target. The other framework, GloVe, is similar to traditional count models based on matrix factorization (Turney and Pantel, 2010; Baroni et al., 2014), in the sense that vectors are trained on global word-word co-occurrence counts. In the case of GloVe, the training objective is to learn word"
S18-1163,Y16-2021,1,0.930479,"4) has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks (Baroni et al., 2014). However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement (Batchkarov et al., 2016; Faruqui et al., 2016; Santus et al., 2016a). Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead on the extraction of semantic differences between lexical meanings (Krebs and Paperno, 2016): given two words and an attribute (i.e., a discrete semantic feature), a system has to predict whether the attribute describes a difference between the corresponding concepts or not (e.g. wing is an attribute of plane, but not of helicopter). 2 2.1 Capturing Discriminative Attributes Task and Dataset Description The task of capturing discriminative attributes"
S18-1163,L16-1722,1,0.933311,"4) has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks (Baroni et al., 2014). However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement (Batchkarov et al., 2016; Faruqui et al., 2016; Santus et al., 2016a). Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead on the extraction of semantic differences between lexical meanings (Krebs and Paperno, 2016): given two words and an attribute (i.e., a discrete semantic feature), a system has to predict whether the attribute describes a difference between the corresponding concepts or not (e.g. wing is an attribute of plane, but not of helicopter). 2 2.1 Capturing Discriminative Attributes Task and Dataset Description The task of capturing discriminative attributes"
S18-1163,W16-2506,0,0.0527279,"Levy and Goldberg, 2014) has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks (Baroni et al., 2014). However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement (Batchkarov et al., 2016; Faruqui et al., 2016; Santus et al., 2016a). Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead on the extraction of semantic differences between lexical meanings (Krebs and Paperno, 2016): given two words and an attribute (i.e., a discrete semantic feature), a system has to predict whether the attribute describes a difference between the corresponding concepts or not (e.g. wing is an attribute of plane, but not of helicopter). 2 2.1 Capturing Discriminative Attributes Task and Dataset Description The task of capturing disc"
S18-1163,W16-2509,0,0.616184,"rsoni@gmail.com 1 Massachussetts Institute of Technology, 2 Universit¨at Hamburg, 3 Aix-Marseille University Abstract Since even related words may differ for some non-shared attributes (e.g. hypernyms and hyponyms), the ability of automatically recognize discriminative features would be an extremely useful addition for the creation of ontologies and other types of lexical resources and would make machine decisions interpretable, enabling human validation (Biemann et al., 2018). Moreover, one can think to applications to many other NLP domains, such as machine translation and dialogue systems (Krebs and Paperno, 2016). In the present contribution, we describe the BomJi classification system, which we used for the identification of discriminative features between concept pairs. According to the official evaluation results provided by the organizers1 , our system ranked second out of 26 participants. Our score, F 1 = 0.73 lags slightly behind the best score of 0.75. After the evaluation period, we run further experiments including all investigated features and found that the system can achieve up to 0.75 F1 score. This paper describes BomJi, a supervised system for capturing discriminative attributes in word"
W15-4208,N09-1003,0,0.0525288,"Missing"
W15-4208,W11-2501,1,0.772704,"onym questions of the TOEFL as a benchmark in the synonyms identification task. Although good results in such set (Rapp, 2003) may have a strong impact on the audience, its small size and the fact that it contains only synonyms cannot make it an accurate benchmark to evaluate DSMs. For what concerns antonymy, based on similar principles to the TOEFL, Mohammed et al. (2008) proposes a dataset containing 950 closest-opposite questions, where five alternatives are provided for every target word. Their data are collected starting from 162 questions in the Graduate Record Examination (GRE). BLESS (Baroni and Lenci, 2011) contains several relations, such as hypernymy, co-hyponymy, meronymy, event, attribute, etc. This dataset covers 200 concrete and unambiguous concepts divided in 17 categories (e.g. vehicle, ground mammal, etc.). Every concept is linked through the various semantic relations to several relata (which can be either nouns, adjectives or verbs). Unfortunately this dataset does not contain synonymy and antonymy related pairs. With respect to entailment, Baroni et al.(2012) Related Work Up to now, DSMs performance has typically been evaluated against benchmarks developed for purposes other than DSM"
W15-4208,E12-1004,0,0.733867,"Missing"
W15-4208,P99-1008,0,0.0361593,"similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a"
W15-4208,W09-0215,0,0.154042,"Missing"
W15-4208,D08-1103,0,0.0551206,"Missing"
W15-4208,P06-1015,0,0.154748,", in fact, can be similar in many ways. Dog and animal are similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synony"
W15-4208,2003.mtsummit-papers.42,0,0.0194325,"inconsistencies in the way semantic relations have been encoded. Simply looking at the hypernymy relation (Cruse, 1986), for example, we can see that it is used in both a taxonomical (i.e. dog is a hyponym of animal) and a vague and debatable way (i.e. silly is a hyponym of child). ConceptNet (Liu and Singh, 2004) may be considered even less homogeneous, given its size and the automatic way in which it was developed. Landauer and Dumais (1997) introduces the 80 multiple-choice synonym questions of the TOEFL as a benchmark in the synonyms identification task. Although good results in such set (Rapp, 2003) may have a strong impact on the audience, its small size and the fact that it contains only synonyms cannot make it an accurate benchmark to evaluate DSMs. For what concerns antonymy, based on similar principles to the TOEFL, Mohammed et al. (2008) proposes a dataset containing 950 closest-opposite questions, where five alternatives are provided for every target word. Their data are collected starting from 162 questions in the Graduate Record Examination (GRE). BLESS (Baroni and Lenci, 2011) contains several relations, such as hypernymy, co-hyponymy, meronymy, event, attribute, etc. This data"
W15-4208,gheorghita-pierrel-2012-towards,0,0.149614,"Missing"
W15-4208,E14-4008,1,0.740615,"a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth"
W15-4208,C92-2082,0,0.607621,"lations. Words, in fact, can be similar in many ways. Dog and animal are similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pa"
W15-4208,Y14-1018,1,0.947947,"a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth"
W15-4208,W09-2415,0,0.0355443,"Missing"
W15-4208,W14-5814,0,0.117044,"Missing"
W15-4208,Q14-1041,0,0.0527536,"used for either filtering the pairs or performing an in-depth analysis of the results. The tuples were extracted from a combination of ConceptNet 5.0 and WordNet 4.0, and subsequently filtered through automatic methods and crowdsourcing in order to ensure their quality. The dataset is freely downloadable1 . An extension in RDF format, including also scripts for data processing, is under development. 1 Introduction Distributional Semantic Models (DSMs) represent lexical meaning in vector spaces by encoding corpora derived word co-occurrences in vectors (Sahlgren, 2006; Turney and Pantel, 2010; Lapesa and Evert, 2014). These models are based on the assumption that meaning can be inferred from the contexts in which terms occur. Such assumption is 1 The resource is available http://colinglab.humnet.unipi.it/resources/ and https://github.com/esantus at at 64 Proceedings of the 4th Workshop on Linked Data in Linguistics (LDL-2015), pages 64–69, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2009; Weeds et al., 2004; Weeds and Weir, 2003). Both the abovementioned approaches need to rely on datasets containing semantic relations"
W15-4208,W03-1011,0,0.421294,"corpora derived word co-occurrences in vectors (Sahlgren, 2006; Turney and Pantel, 2010; Lapesa and Evert, 2014). These models are based on the assumption that meaning can be inferred from the contexts in which terms occur. Such assumption is 1 The resource is available http://colinglab.humnet.unipi.it/resources/ and https://github.com/esantus at at 64 Proceedings of the 4th Workshop on Linked Data in Linguistics (LDL-2015), pages 64–69, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2009; Weeds et al., 2004; Weeds and Weir, 2003). Both the abovementioned approaches need to rely on datasets containing semantic relations for training and/or evaluation. EVALution is a dataset designed to support DSMs on both processes. This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results. The qu"
W15-4208,C04-1146,0,0.471585,"spaces by encoding corpora derived word co-occurrences in vectors (Sahlgren, 2006; Turney and Pantel, 2010; Lapesa and Evert, 2014). These models are based on the assumption that meaning can be inferred from the contexts in which terms occur. Such assumption is 1 The resource is available http://colinglab.humnet.unipi.it/resources/ and https://github.com/esantus at at 64 Proceedings of the 4th Workshop on Linked Data in Linguistics (LDL-2015), pages 64–69, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2009; Weeds et al., 2004; Weeds and Weir, 2003). Both the abovementioned approaches need to rely on datasets containing semantic relations for training and/or evaluation. EVALution is a dataset designed to support DSMs on both processes. This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis"
W15-4208,S12-1012,1,0.886767,"ked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results. The tuples were extract"
W15-4208,W06-1104,0,0.011487,"di Pisa Pisa, Italy Chu-Ren Huang The Hong Kong Polytechnic University Hong Kong alessandro.lenci@ling.unipi.it churen.huang@polyu.edu.hk Abstract typically referred to as the distributional hypothesis (Harris, 1954). DSMs are broadly used in Natural Language Processing (NLP) because they allow systems to automatically acquire lexical semantic knowledge in a fully unsupervised way and they have been proved to outperform other semantic models in a large number of tasks, such as the measurement of lexical semantic similarity and relatedness. Their geometric representation of semantic distance (Zesch and Gurevych, 2006) allows its calculation through mathematical measures, such as the vector cosine. A related but more complex task is the identification of semantic relations. Words, in fact, can be similar in many ways. Dog and animal are similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them u"
W16-5309,P14-1023,0,0.234255,". Licence details: http:// semantic relation. Participants were provided with training and test datasets extracted from EVALution 1.0 (Santus et al., 2015b), as well as a scoring script for evaluating the output of their systems. The shared task has been intended and designed as a “friendly competition”: the goal was to identify strengths and weaknesses of various methods, rather than just “crowning” the best-performing model. In total, seven systems participated in the shared task. Most of them exploited Distributional Semantic Models (DSMs), either of the count-based or word-embedding type (Baroni et al., 2014). Most of them relied on distance or nearest neighbors in subtask 1, and on machine learning classifiers (e.g., Support Vector Machine (SVM), Convolutional Neural Network (CNN) and Random Forest (RF)) in subtask 2. Some systems enriched the DSM representation by adopting patterns (e.g., LexNet, the best system in subtask 2) or extracting distributional properties with unsupervised measures (e.g., ROOT18). This paper reports the results achieved by the participating systems, providing insights about their respective strengths and weaknesses. It is organized as follows. Section 2 surveys similar"
W16-5309,S15-2151,0,0.0565631,"Missing"
W16-5309,S16-1168,0,0.0528522,"Missing"
W16-5309,S07-1003,0,0.0961225,"Missing"
W16-5309,N16-2002,1,0.850221,"as a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits and demerits of countbased distributional models and word embeddings (which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic relations (Gladkova et al., 2016). Levy et al. (2015a) have also shown that optimization of hyperparameters can make a bigger difference than the choice between different models. Finally, very recently, several scholars have investigated the possibility of integrating different kinds of information. Kiela et al. (2015) have used image generality for hypernymy detection, while Shwartz et al. (2016) have tried to identify the same relation by combining pattern-based and distributional information. 3 3.1 Shared task Task description The CogALex-V shared task was conducted as a “friendly competition” where participants had access"
W16-5309,C92-2082,0,0.387764,"for the Identification of Semantic Relations Up to this date, several corpus-based approaches to the identification of semantic relations have been proposed. Most of them, however, focus on a single semantic relation with the ambitious objective of isolating it from all the others. Dealing with multiple relations has been found particularly challenging, and few systems have attempted multi-class classifications. The exceptions include Turney (2008) and Pantel and Pennacchiotti (2006). Early approaches rely on lexical-syntactic patterns (e.g. “tools such as hammers”). After the seminal work of Hearst (1992) who sketched methods for pattern discovery, Snow et al. (2004) adopted machine learning over dependency-paths-based features. While these approaches focused on hypernyms, Pantel and Pennacchiotti (2006) introduced Espresso, able to identify several semantic relations (i.e. hypernymy, part-of, succession, reaction and production) as well as to maximize recall by using the Web and precision by assessing the reliability of the patterns. Other pattern-based approaches to synonymy and antonymy are reported by Lin et al. (2003), Turney (2008), Wang et al. (2010) and Lobanova et al. (2010). The majo"
W16-5309,S10-1006,0,0.0774016,"Missing"
W16-5309,P15-2020,0,0.0161834,"(which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic relations (Gladkova et al., 2016). Levy et al. (2015a) have also shown that optimization of hyperparameters can make a bigger difference than the choice between different models. Finally, very recently, several scholars have investigated the possibility of integrating different kinds of information. Kiela et al. (2015) have used image generality for hypernymy detection, while Shwartz et al. (2016) have tried to identify the same relation by combining pattern-based and distributional information. 3 3.1 Shared task Task description The CogALex-V shared task was conducted as a “friendly competition” where participants had access to both training and testing datasets, released on the 8th and the 27th of September 2016, respectively. The participants were asked to evaluate the output of their system with the official evaluation script, released with the test set together with random and majority baselines. Each"
W16-5309,Q14-1041,1,0.84926,"the corpus. In subtask 1, LexNet is combined with vector cosine (calculated on word2vec embeddings trained on Google News) through weights that were learned on a validation set. In subtask 2, in order to avoid a bias towards the majority class RANDOM, a Multi-Layer Perceptron (MLP) is trained and applied only on pairs that were classified as related in subtask 1. The third system, Mach5, investigates the structure and hyperparameters of two traditional dependency-filtered and dependency-structured DSMs trained on a Web corpus of 9.5 billion words. The author sets most parameters according to Lapesa and Evert (2014), focusing on feature selection and optimization of SVD dimensions. Distance information is used directly in subtask 1, while for subtask 2 a linear SVM classifier is applied to 1200-dimensional vectors representing partial Euclidean distance in the two SVD-reduced spaces. Given the competitive results in subtask 1 and the much lower performance achieved in subtask 2, it is evident that Mach5 was optimized for identifying non-random pairs rather than for recognizing and discriminating specific semantic relations. The other systems include ROOT18, which relies on several unsupervised features e"
W16-5309,S12-1012,1,0.87775,"The major limitation of pattern-based approaches is that they require words to co-occur in the same sentence, strongly impacting the recall. Distributional approaches have therefore been adopted to reduce such limitations. They are based on the Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general seman"
W16-5309,Q15-1016,0,0.0399784,"nsisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits and demerits of countbased distributional models and word embeddings (which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic r"
W16-5309,N15-1098,0,0.0544365,"nsisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits and demerits of countbased distributional models and word embeddings (which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic r"
W16-5309,J10-3003,0,0.034749,"valuation metrics, the seven participating systems and their results. The best performing system in subtask 1 is GHHH (F1 = 0.790), while the best system in subtask 2 is LexNet (F1 = 0.445). The dataset and the task description are available at https://sites.google.com/site/cogalex2016/home/shared-task. 1 Introduction Determining automatically if words are semantically related, and in what way, is important for Natural Language Processing (NLP) applications such as thesaurus generation (Grefenstette, 1994), ontology learning (Zouaq and Nkambou, 2008), paraphrase generation and identification (Madnani and Dorr, 2010), as well as for drawing inferences (Martinez-G´omez et al., 2016). Many NLP applications make use of handcrafted resources such as WordNet (Fellbaum, 1998). However, creating these resources is expensive and time-consuming; they are available for only a few languages, and their coverage inevitably lags behind the lexical and conceptual proliferation. In the last decades, a number of corpus-based approaches have investigated the possibility of identifying lexical semantic relations by observing word usage. Even though these methods are still far from being able to provide a comprehensive model"
W16-5309,P16-4015,0,0.0228902,"Missing"
W16-5309,P16-2074,0,0.0481365,"rth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns"
W16-5309,P06-1015,0,0.206698,"well as further information about the shared task are available at https://sites.google. com/site/cogalex2016/home/shared-task. 70 2.2 Methods for the Identification of Semantic Relations Up to this date, several corpus-based approaches to the identification of semantic relations have been proposed. Most of them, however, focus on a single semantic relation with the ambitious objective of isolating it from all the others. Dealing with multiple relations has been found particularly challenging, and few systems have attempted multi-class classifications. The exceptions include Turney (2008) and Pantel and Pennacchiotti (2006). Early approaches rely on lexical-syntactic patterns (e.g. “tools such as hammers”). After the seminal work of Hearst (1992) who sketched methods for pattern discovery, Snow et al. (2004) adopted machine learning over dependency-paths-based features. While these approaches focused on hypernyms, Pantel and Pennacchiotti (2006) introduced Espresso, able to identify several semantic relations (i.e. hypernymy, part-of, succession, reaction and production) as well as to maximize recall by using the Web and precision by assessing the reliability of the patterns. Other pattern-based approaches to sy"
W16-5309,D14-1162,0,0.0882794,"nal publicly available word embeddings trained on huge corpora (Google News, Common Crawl and Wikipedia + Gigaword 5). The authors found that linear regression works better in subtask 1 (i.e. binary 74 classification), while multi-task CNN performs better in subtask 2, which involves multi-class classification. Analogy was instead found less appropriate for semantic relation identification. LexNet relies on Wikipedia + Gigaword 5 and Google News corpora, leveraging the combination of distributional and path-based information. The authors merged the 50-dimensional GloVe pre-trained embeddings (Pennington et al., 2014) for the words in the pairs with the average embedding vector – created using a LSTM (Hochreiter and Schmidhuber, 1997) – of all the dependency paths that connect them in the corpus. In subtask 1, LexNet is combined with vector cosine (calculated on word2vec embeddings trained on Google News) through weights that were learned on a validation set. In subtask 2, in order to avoid a bias towards the majority class RANDOM, a Multi-Layer Perceptron (MLP) is trained and applied only on pairs that were classified as related in subtask 1. The third system, Mach5, investigates the structure and hyperpa"
W16-5309,D16-1234,0,0.0642002,"Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongo"
W16-5309,C14-1097,0,0.0666826,"hey are based on the Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al.,"
W16-5309,W15-4208,1,0.913468,"m gladkova@phiz.c.u-tokyo.ac.jp Stefan Evert FAU Erlangen-N¨urnberg, Germany stefan.evert@fau.de Alessandro Lenci University of Pisa, Italy alessandro.lenci@unipi.it Abstract The shared task of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V) aims at providing a common benchmark for testing current corpus-based methods for the identification of lexical semantic relations (synonymy, antonymy, hypernymy, part-whole meronymy) and at gaining a better understanding of their respective strengths and weaknesses. The shared task uses a challenging dataset extracted from EVALution 1.0 (Santus et al., 2015b), which contains word pairs holding the above-mentioned relations as well as semantically unrelated control items (random). The task is split into two subtasks: (i) identification of related word pairs vs. unrelated ones; (ii) classification of the word pairs according to their semantic relation. This paper describes the subtasks, the dataset, the evaluation metrics, the seven participating systems and their results. The best performing system in subtask 1 is GHHH (F1 = 0.790), while the best system in subtask 2 is LexNet (F1 = 0.445). The dataset and the task description are available at ht"
W16-5309,L16-1722,1,0.853809,"sis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the N"
W16-5309,P16-1226,0,0.369399,"occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits an"
W16-5309,C08-1114,0,0.0585021,"and test data as well as further information about the shared task are available at https://sites.google. com/site/cogalex2016/home/shared-task. 70 2.2 Methods for the Identification of Semantic Relations Up to this date, several corpus-based approaches to the identification of semantic relations have been proposed. Most of them, however, focus on a single semantic relation with the ambitious objective of isolating it from all the others. Dealing with multiple relations has been found particularly challenging, and few systems have attempted multi-class classifications. The exceptions include Turney (2008) and Pantel and Pennacchiotti (2006). Early approaches rely on lexical-syntactic patterns (e.g. “tools such as hammers”). After the seminal work of Hearst (1992) who sketched methods for pattern discovery, Snow et al. (2004) adopted machine learning over dependency-paths-based features. While these approaches focused on hypernyms, Pantel and Pennacchiotti (2006) introduced Espresso, able to identify several semantic relations (i.e. hypernymy, part-of, succession, reaction and production) as well as to maximize recall by using the Web and precision by assessing the reliability of the patterns."
W16-5309,C14-1212,0,0.0769981,"such limitations. They are based on the Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched p"
W16-5313,P14-1023,0,0.0710946,"Missing"
W16-5313,J90-1003,0,0.32909,"mpurity index as the splitting criterion and has 10 as the maximum tree depth. The half of the total number of features were considered for each split. 3.1 Data Our data come from ukWaC (Baroni et al., 2009), a 2 billion tokens corpus of English built by crawling the .uk Internet domain. For the extraction of our features, we generated several distributional spaces, which differ according to the window size and to the statistical association measure that was used to weight raw co-occurrences. Since we obtained the best performances with window size 2 and Positive Pointwise Mutual Information (Church and Hanks, 1990), we report the results only for this setting. 99 3.2 Features Frequency It is a basic property of words and it is a very discriminative information. In this type of task, it proved to be competitive in identifying the directionality of pairs of hypernyms (Weeds and Weir, 2003), since we expect hypernyms to have higher frequency than hyponyms. For each pair, we computed three features: the frequency of each word (Freq1,2) and their difference (DiffFreq). Co-occurrence We compute the co-occurrence frequency (Cooc) between the two terms in each pair. This measure has been claimed to be particula"
W16-5313,N16-2002,0,0.0219028,"Missing"
W16-5313,N15-1098,0,0.0556602,"Missing"
W16-5313,P16-2074,0,0.0913729,"Missing"
W16-5313,D16-1234,0,0.0904482,"Missing"
W16-5313,Y14-1018,1,0.927782,"words to cooccur in the same sentence). DSMs, on the other hand, offer higher recall at the cost of lower precision: while they are strong in identifying distributionally similar words (i.e. nearest neighbors), they do not offer any principled way to discriminate between semantic relations (i.e. the nearest neighbors of a word are not only its synonyms, but they also include antonyms, hypernyms, and so on). The attempts to provide DSMs with the ability of automatically identifying semantic relations include a large number of unsupervised methods (Weeds and Weir, 2003; Lenci and Benotto, 2012; Santus et al., 2014), which are unfortunately far from achieving the perfect accuracy. In order to achieve higher performance, supervised methods have been recently adopted, also thanks to their ease (Weeds et al., 2014; Roller et al., 2014; Kruszewski et al., 2015; Roller and Erk, 2016; Santus et al., 2016a; Nguyen et al., 2016; Shwartz et al., 2016). Many of them rely on distributional word vectors, either concatenated or combined through algebraic functions. Others use as features either patterns or scores from the above-mentioned unsupervised methods. While these systems generally obtain high performance in c"
W16-5313,W15-4208,1,0.936692,") are likely to be antonyms. For each pair, we computed APSyn and APAnt for the top 1000 and for the top 100 contexts. Same POS We realized that many of the random pairs in the data included words with different parts of speech. Therefore, we decided to add a boolean value to our set of features: 1 if the most frequent POS of the words in the pair were the same, 0 otherwise. 100 3.3 Evaluation dataset The task organizers provided a training and a test set extracted from EVALution 1.0, a resource that was specifically designed for evaluating systems on the identification of semantic relations (Santus et al., 2015). EVALution 1.0 was derived from WordNet (Fellbaum, 1998) and ConceptNet (Liu and Singh, 2004) and it consists of almost 7500 word pairs, instantiating several semantic relations. The training and the test set included, respectively, 3054 and 4260 word pairs and they are lexical-split, that is, the two sets do not share any pair. Since words were not tagged, we performed POS-tagging with the TreeTagger (Schmid, 1995). 4 Results Model Random Baseline Cosine Baseline ROOT18(100) ROOT18(500) ROOT18(1000) P (task1) 0.283 0.589 0.818 0.818 0.823 R (task1) 0.503 0.573 0.657 0.650 0.657 F (task1) 0.3"
W16-5313,L16-1723,1,0.890506,"s (i.e. the nearest neighbors of a word are not only its synonyms, but they also include antonyms, hypernyms, and so on). The attempts to provide DSMs with the ability of automatically identifying semantic relations include a large number of unsupervised methods (Weeds and Weir, 2003; Lenci and Benotto, 2012; Santus et al., 2014), which are unfortunately far from achieving the perfect accuracy. In order to achieve higher performance, supervised methods have been recently adopted, also thanks to their ease (Weeds et al., 2014; Roller et al., 2014; Kruszewski et al., 2015; Roller and Erk, 2016; Santus et al., 2016a; Nguyen et al., 2016; Shwartz et al., 2016). Many of them rely on distributional word vectors, either concatenated or combined through algebraic functions. Others use as features either patterns or scores from the above-mentioned unsupervised methods. While these systems generally obtain high performance in classification tasks involving a single semantic relation, they have rarely been used on multiclass relation classification. On top of it, some scholars have questioned their ability to really learn semantic relations (Levy et al., 2015), claiming that they rather learn some lexical prope"
W16-5313,P16-1226,0,0.25801,"they have a wide range of applications, such as textual entailment, text summarization, sentiment analysis, ontology learning, and so on. For this reason, several systems This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ 98 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 98–103, Osaka, Japan, December 11-17 2016. Licence details: http: over the last few years have been proposed to tackle this problem, using both unsupervised and supervised approaches (see the works of Lenci and Benotto (2012) and Shwartz et al. (2016) on hypernymy; Weeds et al. (2014) and Santus et al. (2016a) on hypernymy and co-hyponymy; Mohammad et al. (2013) and Santus et al. (2014) on antonymy). However, many of these works focus on a single semantic relation, e.g. antonymy, and describe methods or measures to set it apart from other relations. There have not been many attempts, at the best of our knowledge, to deal with corpus-based semantic relation identification in a multiclass classification task. Few exceptions include the works by Turney (2008) on similarity, antonymy and analogy, and by Pantel and Pernacchiotti (2006) on Espre"
W16-5313,C08-1114,0,0.0983661,"Missing"
W16-5313,W03-1011,0,0.576732,"r from lower recall (i.e. they in fact require words to cooccur in the same sentence). DSMs, on the other hand, offer higher recall at the cost of lower precision: while they are strong in identifying distributionally similar words (i.e. nearest neighbors), they do not offer any principled way to discriminate between semantic relations (i.e. the nearest neighbors of a word are not only its synonyms, but they also include antonyms, hypernyms, and so on). The attempts to provide DSMs with the ability of automatically identifying semantic relations include a large number of unsupervised methods (Weeds and Weir, 2003; Lenci and Benotto, 2012; Santus et al., 2014), which are unfortunately far from achieving the perfect accuracy. In order to achieve higher performance, supervised methods have been recently adopted, also thanks to their ease (Weeds et al., 2014; Roller et al., 2014; Kruszewski et al., 2015; Roller and Erk, 2016; Santus et al., 2016a; Nguyen et al., 2016; Shwartz et al., 2016). Many of them rely on distributional word vectors, either concatenated or combined through algebraic functions. Others use as features either patterns or scores from the above-mentioned unsupervised methods. While these"
W16-5313,C14-1212,0,0.0897857,"Missing"
W16-5313,Q15-1016,0,\N,Missing
W16-5313,J13-3004,0,\N,Missing
W16-5313,Q15-1027,0,\N,Missing
W16-5313,L16-1722,1,\N,Missing
W16-5313,S12-1012,0,\N,Missing
W17-6803,J10-4006,1,0.660569,"their structural roles, which are computed later. For example, the difference between typical agents and typical patients, according to this account, would not be included in the representation of an event. In the last few years, a related issue has been debated in the field of distributional semantics, i.e. whether there is any added value in using structured representations of linguistic contexts over bag-ofwords ones (e.g., contexts represented as co-occurrence windows). While structured models have been shown to outperform the latter in a number of semantic tasks (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014), some bag-of-word models proved to be extremely competitive, at least under certain parameter settings (Baroni et al., 2014). A recent paper by Lapesa and Evert (2017) explicitly addressed the question of whether using structured distributional semantic models is worth the effort, by comparing the performance of syntax-based and window-based distributional models on four different tasks. The authors showed that, even after extensive parameter tuning, the former have a significant advantage only in one task out of four (i.e., noun clustering). Interestingly, in the di"
W17-6803,W16-4102,1,0.853824,"was measured as the cosine similarity between its vector, and the context-vector obtained by averaging the vectors of the preceding words. Ettinger and colleagues tested their method on the sentences used in the ERP study by Federmeier and Kutas (1999), in which three different conditions were defined, and they observed that the context-target similarity scores across conditions were following the same pattern of the N400 amplitudes of the original experiment. Thus, this work shows how data on N400 variations can be modeled even by means of vectors with minimal or no syntactic information. 2 Chersoni et al. (2016) presented a research work testing a similar method on the Bicknell dataset. However, their model does not really update argument expectations on the basis of other arguments, computing instead a global score of semantic coherence for the entire event representation, on the basis of the mutual typicality between all the participants. 3 Experiments Rationale. Baroni and Lenci (2010) computed the thematic fit for a candidate f iller (e.g., policeman) in an argument slot (e.g., agent) of an input lexical item (e.g., arrest) as the similarity score between the vector of the candidate f iller and a"
W17-6803,P13-4006,0,0.0270416,"of the Wacky (Baroni et al., 2009) corpus. Both were parsed with the Maltparser (Nivre and Hall, 2005). From this concatenation, we built a dependency-based DSMs, where the tuples are weighted by means of Positive Local Mutual Information (PLMI, Evert (2004)). Given the cooccurrence count Otrf of the target t, the syntactic relation r and the filler f , we computed the expected count Etrf (i.e., the simple joint probability of indipendent variables, corresponding to the product of the probabilities of the single events).4 4 The DSM were built by means of the scripts of the DISSECT framework (Dinu et al., 2013) The PLMI for each target-relation-filler tuple is computed as follows:  LM I(t, r, f ) = log Otrf Etrf  ∗ Otrf P LM I(t, r, f ) = max(LM I(t, r, f ), 0) (3) (4) Our DSM contains 28,817 targets (i.e., all nouns and verbs with frequency above 1000 in the training corpora), and all syntactic relations were included.5 We also built a window-based DSM to extract cooccurrence information for the BOW model, counting only the co-occurrences between the nouns and the verbs of the list above within a word window of width 2. Prototypes The prototypes of all models were built out of the vectors of the"
W17-6803,N15-1003,0,0.287639,"tional information coming from the agent and from the predicate of an agent-verb-patient triple (e.g., butcher–cut–meat), generating a prototype vector which represents the expectations on the patient filler, given the agent filler. The triples of the Bicknell dataset (Bicknell et al., 2010), which were used for the first time to evaluate such a model, are still today, at the best of our knowledge, the only existing gold standard for this type of task. Although the ‘structured-approach’ to thematic fit was influential for a number of other works (Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015; Sayeed et al., 2016; Santus et al., 2017), the task of modeling the update of the argument expectations has received relatively little attention. An exception is the work by Tilk et al. (2016), who trained a neural network on a role-labeled corpus in order to optimize the distributional representation for thematic fit estimation. Their model was also tested on the task of the composition and update of argument expectations, where it was able to achieve a performance comparable to Lenci (2011) on the triples of the Bicknell dataset.2 Notice that both the models of Lenci (2011) and Tilk et al."
W17-6803,E17-2063,0,0.0110316,"n of an event. In the last few years, a related issue has been debated in the field of distributional semantics, i.e. whether there is any added value in using structured representations of linguistic contexts over bag-ofwords ones (e.g., contexts represented as co-occurrence windows). While structured models have been shown to outperform the latter in a number of semantic tasks (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014), some bag-of-word models proved to be extremely competitive, at least under certain parameter settings (Baroni et al., 2014). A recent paper by Lapesa and Evert (2017) explicitly addressed the question of whether using structured distributional semantic models is worth the effort, by comparing the performance of syntax-based and window-based distributional models on four different tasks. The authors showed that, even after extensive parameter tuning, the former have a significant advantage only in one task out of four (i.e., noun clustering). Interestingly, in the discussion they leave open the question of whether their results can generalize to linguistically challenging task such as the prediction of thematic fit ratings. In this paper, we specifically in"
W17-6803,W11-0607,1,0.786635,"lity by means of a Local Mutual Information score (Evert, 2004) computed between verb, arguments and syntactic relations. The basic assumption is that the higher the distributional similarity of a candidate argument with a role prototype, the higher its predictability as a filler for that role will be. As a gold standard, the authors used the human-elicited thematic fit ratings collected by McRae et al. (1998) and Pad´o (2007), and they evaluated the performance by measuring the correlation between these ratings and the scores generated by the model (as already proposed by Erk et al. (2010)). Lenci (2011) later extended this ‘structured-approach’ to account for the dynamic update of the expectations on an argument, which depends on how other roles in the sentences are filled. For instance, given the agent butcher the expected patient of the verb cut is likely to be meat, while given the agent coiffeur the expected patient of the same verb is likely to be hair. By means of the same DM tensor, this study tested an additive and a multiplicative model (Mitchell and Lapata, 2010) to compose the distributional information coming from the agent and from the predicate of an agent-verb-patient triple ("
W17-6803,P14-2050,0,0.0132482,"which are computed later. For example, the difference between typical agents and typical patients, according to this account, would not be included in the representation of an event. In the last few years, a related issue has been debated in the field of distributional semantics, i.e. whether there is any added value in using structured representations of linguistic contexts over bag-ofwords ones (e.g., contexts represented as co-occurrence windows). While structured models have been shown to outperform the latter in a number of semantic tasks (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014), some bag-of-word models proved to be extremely competitive, at least under certain parameter settings (Baroni et al., 2014). A recent paper by Lapesa and Evert (2017) explicitly addressed the question of whether using structured distributional semantic models is worth the effort, by comparing the performance of syntax-based and window-based distributional models on four different tasks. The authors showed that, even after extensive parameter tuning, the former have a significant advantage only in one task out of four (i.e., noun clustering). Interestingly, in the discussion they leave open t"
W17-6803,J07-2002,0,0.0547974,"Missing"
W17-6803,D17-1068,1,0.772801,"from the predicate of an agent-verb-patient triple (e.g., butcher–cut–meat), generating a prototype vector which represents the expectations on the patient filler, given the agent filler. The triples of the Bicknell dataset (Bicknell et al., 2010), which were used for the first time to evaluate such a model, are still today, at the best of our knowledge, the only existing gold standard for this type of task. Although the ‘structured-approach’ to thematic fit was influential for a number of other works (Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015; Sayeed et al., 2016; Santus et al., 2017), the task of modeling the update of the argument expectations has received relatively little attention. An exception is the work by Tilk et al. (2016), who trained a neural network on a role-labeled corpus in order to optimize the distributional representation for thematic fit estimation. Their model was also tested on the task of the composition and update of argument expectations, where it was able to achieve a performance comparable to Lenci (2011) on the triples of the Bicknell dataset.2 Notice that both the models of Lenci (2011) and Tilk et al. (2016) necessarily rely on the hypothesis"
W17-6803,W16-2518,0,0.0126561,"g from the agent and from the predicate of an agent-verb-patient triple (e.g., butcher–cut–meat), generating a prototype vector which represents the expectations on the patient filler, given the agent filler. The triples of the Bicknell dataset (Bicknell et al., 2010), which were used for the first time to evaluate such a model, are still today, at the best of our knowledge, the only existing gold standard for this type of task. Although the ‘structured-approach’ to thematic fit was influential for a number of other works (Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015; Sayeed et al., 2016; Santus et al., 2017), the task of modeling the update of the argument expectations has received relatively little attention. An exception is the work by Tilk et al. (2016), who trained a neural network on a role-labeled corpus in order to optimize the distributional representation for thematic fit estimation. Their model was also tested on the task of the composition and update of argument expectations, where it was able to achieve a performance comparable to Lenci (2011) on the triples of the Bicknell dataset.2 Notice that both the models of Lenci (2011) and Tilk et al. (2016) necessarily r"
W17-6803,D16-1017,0,0.0643391,"nt filler, given the agent filler. The triples of the Bicknell dataset (Bicknell et al., 2010), which were used for the first time to evaluate such a model, are still today, at the best of our knowledge, the only existing gold standard for this type of task. Although the ‘structured-approach’ to thematic fit was influential for a number of other works (Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015; Sayeed et al., 2016; Santus et al., 2017), the task of modeling the update of the argument expectations has received relatively little attention. An exception is the work by Tilk et al. (2016), who trained a neural network on a role-labeled corpus in order to optimize the distributional representation for thematic fit estimation. Their model was also tested on the task of the composition and update of argument expectations, where it was able to achieve a performance comparable to Lenci (2011) on the triples of the Bicknell dataset.2 Notice that both the models of Lenci (2011) and Tilk et al. (2016) necessarily rely on the hypothesis that the arguments are structurally distinct, since they are trained either on argument tuples containing fine-grained dependency information, or on se"
W17-6803,P14-1023,0,\N,Missing
Y14-1018,J10-4006,1,0.804961,"of the contrasting pairs in GRE closest-to-opposite questions are not listed as opposites in WordNet”. Copyright 2014 by Enrico Santus, Qin Lu, Alessandro Lenci and Chu-Ren Huang 28th Pacific Asia Conference on Language, Information and Computation pages 135–144 !135 PACLIC 28 The automatic identification of semantic relations is a core task in computational semantics. Distributional Semantic Models (DSMs) have often been exploited for their well known ability to identify semantically similar lexemes using corpus-derived co-occurrences encoded as distributional vectors (Santus et al., 2014a; Baroni and Lenci, 2010; Turney and Pantel, 2010; Padó and Lapata, 2007; Sahlgren, 2006). These models are based on the Distributional Hypothesis (Harris, 1954) and represent lexical semantic similarity in function of distributional similarity, which can be measured by vector cosine (Turney and Pantel, 2010). However, these models are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponym"
Y14-1018,P10-1018,0,0.0497849,"Missing"
Y14-1018,2020.inlg-1.14,0,0.0238601,"Missing"
Y14-1018,C92-2082,0,0.398782,"gether with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are similar to synonyms"
Y14-1018,J91-1001,0,0.690848,"l need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are similar to synonyms in every dimension of meaning except one. Our hypothesis is that the different dimension of meaning is a salient one and it can be identified with DSMs and exploited for discriminating antonyms from synonyms. The rest of the paper is organized as follows. Section 2 gives the definition and illustrates the various types of antonyms. Section 3 gives a brief o"
Y14-1018,D13-1169,0,0.0689646,"Missing"
Y14-1018,S12-1012,1,0.925115,"are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Ch"
Y14-1018,W11-2128,0,0.0412439,"Missing"
Y14-1018,H05-1067,0,0.0744627,"Missing"
Y14-1018,D08-1103,0,0.428385,"makes use of Average Precision to estimate the extent and salience of the intersection among the most descriptive contexts of two target words. Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. APAnt outperforms the vector cosine and a baseline model implementing the cooccurrence hypothesis. 1 Introduction Antonymy is one of the fundamental relations shaping the organization of the semantic lexicon and its identification is very challenging for computational models (Mohammad et al., 2008; Deese, 1965; Deese, 1964). Yet, antonymy is essential for many Natural Language Processing (NLP) applications, such as Information Retrieval (IR), Ontology Learning (OL), Machine Translation (MT), Sentiment Analysis (SA) and Dialogue Systems (Roth and Schulte im Walde, 2014; Mohammad et al., 2013). In particular, the automatic identification of semantic opposition is a crucial component for the detection and generation of paraphrases (Marton et al., 2011), the understanding of contradictions (de Marneffe et al., 2008) and the detection of humor (Mihalcea and Strapparava, 2005). Several exist"
Y14-1018,J07-2002,0,0.0433936,"ite questions are not listed as opposites in WordNet”. Copyright 2014 by Enrico Santus, Qin Lu, Alessandro Lenci and Chu-Ren Huang 28th Pacific Asia Conference on Language, Information and Computation pages 135–144 !135 PACLIC 28 The automatic identification of semantic relations is a core task in computational semantics. Distributional Semantic Models (DSMs) have often been exploited for their well known ability to identify semantically similar lexemes using corpus-derived co-occurrences encoded as distributional vectors (Santus et al., 2014a; Baroni and Lenci, 2010; Turney and Pantel, 2010; Padó and Lapata, 2007; Sahlgren, 2006). These models are based on the Distributional Hypothesis (Harris, 1954) and represent lexical semantic similarity in function of distributional similarity, which can be measured by vector cosine (Turney and Pantel, 2010). However, these models are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like"
Y14-1018,P06-1015,0,0.714471,"e brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are simi"
Y14-1018,P14-2086,0,0.361997,"Missing"
Y14-1018,W14-5814,0,0.121769,"Missing"
Y14-1018,C02-1061,0,0.103734,"Missing"
Y14-1018,C08-1114,0,0.435435,"r lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a dis"
Y14-1018,E14-4008,1,0.918306,"wo lexemes are antonyms or synonyms by looking at the extent and salience of this intersection: the broader and more salient the intersection, the higher the probability that the lexemes are synonyms; vice versa the narrower and less salient the intersection, the higher the probability that the lexemes are antonyms. To verify this hypothesis, we select the N most salient contexts of the two target words (N=1001). We define the salience of a context for a specific target word by ranking the contexts through Local Mutual Information (LMI; Evert, 2005) and picking the first N, as already done by Santus et al. (2014a). Once the N most salient contexts for the two target words have been identified, we verify the extent and the salience of the contexts shared by both the target words. We predict that synonyms share a significantly higher number of salient contexts than antonyms. To estimate the extent and the salience of the shared contexts, we adapt the Average Precision measure (AP; Voorhees and Harman, 1999), a common Information Retrieval (IR) evaluation metric already used by Kotlerman et al. (2010) to identify lexical entailment. In IR systems, this measure is used to evaluate the ranked documents re"
Y14-1018,J13-3004,0,\N,Missing
Y14-1018,P08-1118,0,\N,Missing
Y15-1021,baccianella-etal-2010-sentiwordnet,0,0.00970063,"s is to measure the contributions on polarity values by different POS tags. – The “PolarityDep” is similar to “PolarityWin”, but it differs in that the negation is checked based on the dependency structure. – The “PolarShiftWin” measures the difference between the most positive item and the most negative item in a window of size 5. – The “PolarShiftDep” measures the polarity difference of “parent-child” pairs in the dependency structures of the tweets. Four sentiment dictionaries were used: Opinion Lexicon (Hu and Liu, 2004), Afinn (Nielsen, 2011), MPQA (Wiebe et al., 2005), and SentiWordnet (Baccianella et al., 2010). The union and intersection of the four dictionaries are also used as two additional dictionaries. Formally, the polarity feature can be represented as a (key, val) pair, where the key is <pos, dict&gt;, or <dict&gt;. For example, (<adj, mpqa&gt;, 1.0) means that according to the dictionary MPQA, adjectives contribute to the polarity value 3 http://nlp.stanford.edu/software/corenlp.shtml 180 http://nlp.stanford.edu/software/lex-parser.shtml PACLIC 29 Figure 1: Flowchart of overall process of our method for 1.0. Finally, features that occur less than three times are excluded. In feature normalization,"
Y15-1021,S15-2080,0,0.0300927,"sitive or negative. Later on, the task was extended to address more challenging and complex goals, such as the identification of the sentiment of the messages or the writer’s affective state in a more fine scale, with labels including anger, happiness or depression. Such extension could not avoid considering one of the most pervasive tools used in communication, namely figurative language. In fact, this expressive tool is not only very frequent in various kinds of texts, but it also strongly affects the sentiment expressed in the text, often completely reversing its polarity (Xu et al., 2015; Ghosh et al., 2015). Because figurative language is used in unpredictable ways in communication (i.e. either in crystallized forms or in creative ways) and it can involve several linguistic and extra-linguistic levels (i.e. from syntax to concepts and pragmatics), its identification and understanding is often difficult, even for human beings. If humans are able to rely on prosody (e.g. stress or intonation), kinesis (e.g. facial gestures), co-text (i.e. immediate textual environment) and context (i.e. wider environment), as well as cultural background, machines cannot access the same type of information. These d"
Y15-1021,pak-paroubek-2010-twitter,0,0.0475299,"the task on ironic and sarcastic tweets. 1 Introduction Whenever a message is encoded into linguistic form for being communicated – either in a spoken or written text1 – information revealing judgments, evaluations, attitudes and emotions is also encoded (Martin and White, 2005). This is true for both informal and formal texts, independently of how much attention the writer pays in cleaning such information out. This is also true for texts posted on social networks (i.e. Facebook, Twitter, etc.), where judgments, evaluations, attitudes and emotions constitute an important part of the message (Pak and Paroubek, 2010). Sentiment analysis (also known as opinion mining and subjectivity analysis) is a Natural Language Processing (NLP) task that focuses on identification of 1 In this paper we will mainly refer to written texts, but most of what is said is also applicable to spoken ones. such judgments, evaluations, attitudes and emotions. It can be compared to other classification tasks, as it consists in associating the analyzed texts with a label that represents the sentiment of the message or the affective state of the writer (Hart, 2013). In its earliest incarnations, sentiment analysis was limited to the"
Y15-1021,J01-4004,0,0.0657435,"le 2 identifies coherence in a tweet and uses it to evaluate the similarity between them. A as a feature. method proposed by Resnik (1995) is used There are several studies related to coherence to define the similarity between two synsets identification. A set of heuristic rules based on based on the information content of their lowgrammatical relations was proposed to identify coest super-ordinate (most specific common subherence in tweets (Tungthamthiti et al., 2014). A sumer). more complex method, based on machine learning, • The feature is activated when the similarity of was presented by Soon et al. (2001) to link coreone of synset pairs is greater than a threshold. ferring noun phrases both within and across senIt is set to 1.37 by our intuition. tences. However, such method would not be ap9. Number agreement feature – w1 and w2 agree in propriate for our scope, because it focuses specifinumber (i.e., they are both singular or plural) cally on coreference resolution, rather than identify10. Acronyms and abbreviation – A tweet contains ing the coherence relationship. Nevertheless, it proan acronym or abbreviation (i.e., “lol”, “ynwa”). vides some useful insights, which can be exploited 11. Emot"
Y15-1021,S15-2113,1,0.795892,"bel was either positive or negative. Later on, the task was extended to address more challenging and complex goals, such as the identification of the sentiment of the messages or the writer’s affective state in a more fine scale, with labels including anger, happiness or depression. Such extension could not avoid considering one of the most pervasive tools used in communication, namely figurative language. In fact, this expressive tool is not only very frequent in various kinds of texts, but it also strongly affects the sentiment expressed in the text, often completely reversing its polarity (Xu et al., 2015; Ghosh et al., 2015). Because figurative language is used in unpredictable ways in communication (i.e. either in crystallized forms or in creative ways) and it can involve several linguistic and extra-linguistic levels (i.e. from syntax to concepts and pragmatics), its identification and understanding is often difficult, even for human beings. If humans are able to rely on prosody (e.g. stress or intonation), kinesis (e.g. facial gestures), co-text (i.e. immediate textual environment) and context (i.e. wider environment), as well as cultural background, machines cannot access the same type of"
Y15-1021,E14-3007,0,\N,Missing
Y15-1021,W14-2608,0,\N,Missing
Y15-1021,D13-1066,0,\N,Missing
Y15-1021,Y14-1047,1,\N,Missing
Y16-2021,N09-1003,0,0.387296,"Missing"
Y16-2021,J10-4006,1,0.894609,"Missing"
Y16-2021,P14-1023,0,0.703018,"shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012), against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in Hill et al. (2015). In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable. Results prove its high performance also when calculated on large corpora, such as those used by Baroni et al. (2014). On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine. Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015), we test the ability of the models to quantify genuine semantic similarity. 2 2.1 Background DSMs, Measures of Association and Dimensionality Reduction Count-based DSMs are built in an unsupervised way. Starting from large preprocessed corpora, a matrix M(m×n) is built, in which each row is a vector representing a"
Y16-2021,J90-1003,0,0.702972,"of the Distributional Hypothesis (DH), which claims that words occurring in the same contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Sahlgren, 2008). Such hypothesis provides the theoretical ground for Distributional Semantic Models (DSMs), which represent word meaning by means of high-dimensional vectors encoding corpus-extracted co-occurrences between targets and their linguistic contexts (Turney and Pantel, 2010). Traditional DSMs initialize vectors with cooccurrence frequencies. Statistical measures, such as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word"
Y16-2021,W16-2506,0,0.162529,"ting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems (Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are actually shared by the vectors. Finally, Cosine is affected by the hubness effect (Dinu et al., 2014; Schn30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 229 abel et al., 2015), i.e. the fact that words with high frequency tend to be universal neighbours. Even though other measures have been proposed in the literature (Deza and Deza, 2009), Vector Cosine is still by far the most popular one (Turney and Pantel"
Y16-2021,J15-4004,0,0.0945737,", 2010). However, in a recent paper of Santus et al. (2016b), the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words. The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets. In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation - namely WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). For comparison, Vector Cosine is also calculated on several countbased DSMs. We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application. The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012), against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in Hill et al. (201"
Y16-2021,P12-1092,0,0.686827,"wise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems (Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are actually shared by the"
Y16-2021,W14-1503,0,0.0315309,"0.335 0.519 0.525 0.564 0.546 0.562 0.553 WSim (REL) 2 3 0.167 0.175 0.251 0.269 0.378 0.396 0.051 0.084 0.141 0.151 0.325 0.323 0.259 0.241 0.261 0.284 0.233 0.27 0.337 0.397 0.361 0.382 0.287 0.309 Table 3: Spearman correlation scores for our eight models trained on RCV1, in the two subsets of WordSim353. might depend on the different type of similarity encoded in SimLex-999 (i.e. genuine similarity). On top of it, despite Hill et al. (2015)’s claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014), we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015), using the words of the SimLex-999 dataset as query words and collecting for each of them the top 1000 nearest neighbors. Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora. Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list. It can be easily noticed that the highest ranke"
Y16-2021,Q15-1016,0,0.382486,"s that words occurring in the same contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Sahlgren, 2008). Such hypothesis provides the theoretical ground for Distributional Semantic Models (DSMs), which represent word meaning by means of high-dimensional vectors encoding corpus-extracted co-occurrences between targets and their linguistic contexts (Turney and Pantel, 2010). Traditional DSMs initialize vectors with cooccurrence frequencies. Statistical measures, such as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models lear"
Y16-2021,L16-1723,1,0.788938,"higher values and the inability of considering how many features are actually shared by the vectors. Finally, Cosine is affected by the hubness effect (Dinu et al., 2014; Schn30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 229 abel et al., 2015), i.e. the fact that words with high frequency tend to be universal neighbours. Even though other measures have been proposed in the literature (Deza and Deza, 2009), Vector Cosine is still by far the most popular one (Turney and Pantel, 2010). However, in a recent paper of Santus et al. (2016b), the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words. The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets. In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation - namely WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). For comparison, Vector Cosine is also"
Y16-2021,D15-1036,0,0.145843,"andauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004; Mikolov et al., 2013; Levy et al., 2015), looks at the normalized correlation between the dimensions of two word vectors, w1 and w2 and returns a score between -1 and 1. It is described by the following equation: n × f2i i=1 f1 i n i=1 f1 i × i=1 f2 i cos(w1 , w2 ) = n (3) where fi x is the i-th dimension in the vector x. Despite its extensive usage, Vector Cosine has been recently criticized for its hyper sensibility to features with high values and for the inability of identifying the actual feature intersection (Li and Han, 2013; Schnabel et al., 2015). Recalling an example by Li and Han (2013), the Vector Cosine for the toy-vectors a = [1, 2, 0] and b = [0, 1, 0] (i.e. 0.8944) is unexpectedly higher than the one for a and c = [2, 1, 0] (i.e. 0.8000), and even higher than the one for the toy-vectors a and d = [1, 2, 1] (i.e. 0.6325), which instead share a larger feature intersection. Since the Vector Cosine is a distance measure, it is also subject to the hubness problem, which was shown by Radovanovic et al. (2010) to be an inherent property of data distributions in highdimensional vector space. The problem consists in the fact that vector"
Y16-2021,P10-1040,0,0.0618988,"uch as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems (Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are act"
