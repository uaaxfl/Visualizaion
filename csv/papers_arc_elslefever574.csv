2021.semeval-1.145,{LT}3 at {S}em{E}val-2021 Task 6: Using Multi-Modal Compact Bilinear Pooling to Combine Visual and Textual Understanding in Memes,2021,-1,-1,2,1,2022,pranaydeep singh,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"Internet memes have become ubiquitous in social media networks today. Due to their popularity, they are also a widely used mode of expression to spread disinformation online. As memes consist of a mixture of text and image, they require a multi-modal approach for automatic analysis. In this paper, we describe our contribution to the SemEval-2021 Detection of Persuasian Techniques in Texts and Images Task. We propose a Multi-Modal learning system, which incorporates {``}memebeddings{''}, viz. joint text and vision features by combining them with compact bilinear pooling, to automatically identify rhetorical and psychological disinformation techniques. The experimental results show that the proposed system constantly outperforms the competition{'}s baseline, and achieves the 2nd best Macro F1-score and 14th best Micro F1-score out of all participants."
2021.latechclfl-1.15,A Pilot Study for {BERT} Language Modelling and Morphological Analysis for Ancient and Medieval {G}reek,2021,-1,-1,3,1,2022,pranaydeep singh,"Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"This paper presents a pilot study to automatic linguistic preprocessing of Ancient and Byzantine Greek, and morphological analysis more specifically. To this end, a novel subword-based BERT language model was trained on the basis of a varied corpus of Modern, Ancient and Post-classical Greek texts. Consequently, the obtained BERT embeddings were incorporated to train a fine-grained Part-of-Speech tagger for Ancient and Byzantine Greek. In addition, a corpus of Greek Epigrams was manually annotated and the resulting gold standard was used to evaluate the performance of the morphological analyser on Byzantine Greek. The experimental results show very good perplexity scores (4.9) for the BERT language model and state-of-the-art performance for the fine-grained Part-of-Speech tagger for in-domain data (treebanks containing a mixture of Classical and Medieval Greek), as well as for the newly created Byzantine Greek gold standard data set. The language models and associated code are made available for use at https://github.com/pranaydeeps/Ancient-Greek-BERT"
2020.semeval-1.135,{LT}3 at {S}em{E}val-2020 Task 7: Comparing Feature-Based and Transformer-Based Approaches to Detect Funny Headlines,2020,-1,-1,4,0,15185,bram vanroy,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper presents two different systems for the SemEval shared task 7 on Assessing Humor in Edited News Headlines, sub-task 1, where the aim was to estimate the intensity of humor generated in edited headlines. Our first system is a feature-based machine learning system that combines different types of information (e.g. word embeddings, string similarity, part-of-speech tags, perplexity scores, named entity recognition) in a Nu Support Vector Regressor (NuSVR). The second system is a deep learning-based approach that uses the pre-trained language model RoBERTa to learn latent features in the news headlines that are useful to predict the funniness of each headline. The latter system was also our final submission to the competition and is ranked seventh among the 49 participating teams, with a root-mean-square error (RMSE) of 0.5253."
2020.semeval-1.153,{LT}3 at {S}em{E}val-2020 Task 8: Multi-Modal Multi-Task Learning for Memotion Analysis,2020,-1,-1,3,1,2022,pranaydeep singh,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Internet memes have become a very popular mode of expression on social media networks today. Their multi-modal nature, caused by a mixture of text and image, makes them a very challenging research object for automatic analysis. In this paper, we describe our contribution to the SemEval-2020 Memotion Analysis Task. We propose a Multi-Modal Multi-Task learning system, which incorporates {``}memebeddings{''}, viz. joint text and vision features, to learn and optimize for all three Memotion subtasks simultaneously. The experimental results show that the proposed system constantly outperforms the competition{'}s baseline, and the system setup with continual learning (where tasks are trained sequentially) obtains the best classification F1-scores."
2020.semeval-1.173,{LT}3 at {S}em{E}val-2020 Task 9: Cross-lingual Embeddings for Sentiment Analysis of {H}inglish Social Media Text,2020,-1,-1,2,1,2022,pranaydeep singh,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper describes our contribution to the SemEval-2020 Task 9 on Sentiment Analysis for Code-mixed Social Media Text. We investigated two approaches to solve the task of Hinglish sentiment analysis. The first approach uses cross-lingual embeddings resulting from projecting Hinglish and pre-trained English FastText word embeddings in the same space. The second approach incorporates pre-trained English embeddings that are incrementally retrained with a set of Hinglish tweets. The results show that the second approach performs best, with an F1-score of 70.52{\%} on the held-out test data."
2020.lrec-1.504,Identifying Cognates in {E}nglish-{D}utch and {F}rench-{D}utch by means of Orthographic Information and Cross-lingual Word Embeddings,2020,0,0,1,1,2023,els lefever,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper investigates the validity of combining more traditional orthographic information with cross-lingual word embeddings to identify cognate pairs in English-Dutch and French-Dutch. In a first step, lists of potential cognate pairs in English-Dutch and French-Dutch are manually labelled. The resulting gold standard is used to train and evaluate a multi-layer perceptron that can distinguish cognates from non-cognates. Fifteen orthographic features capture string similarities between source and target words, while the cosine similarity between their word embeddings represents the semantic relation between these words. By adding domain-specific information to pretrained fastText embeddings, we are able to obtain good embeddings for words that did not yet have a pretrained embedding (e.g. Dutch compound nouns). These embeddings are then aligned in a cross-lingual vector space by exploiting their structural similarity (cf. adversarial learning). Our results indicate that although the classifier already achieves good results on the basis of orthographic information, the performance further improves by including semantic information in the form of cross-lingual word embeddings."
2020.computerm-1.12,{T}erm{E}val 2020: Shared Task on Automatic Term Extraction Using the Annotated Corpora for Term Extraction Research ({ACTER}) Dataset,2020,0,0,4,1,21025,ayla terryn,Proceedings of the 6th International Workshop on Computational Terminology,0,"The TermEval 2020 shared task provided a platform for researchers to work on automatic term extraction (ATE) with the same dataset: the Annotated Corpora for Term Extraction Research (ACTER). The dataset covers three languages (English, French, and Dutch) and four domains, of which the domain of \textit{heart failure} was kept as a held-out test set on which final f1-scores were calculated. The aim was to provide a large, transparent, qualitatively annotated, and diverse dataset to the ATE research community, with the goal of promoting comparative research and thus identifying strengths and weaknesses of various state-of-the-art methodologies. The results show a lot of variation between different systems and illustrate how some methodologies reach higher precision or recall, how different systems extract different types of terms, how some are exceptionally good at finding rare terms, or are less impacted by term length. The current contribution offers an overview of the shared task with a comparative evaluation, which complements the individual papers by all participants."
2020.calcs-1.6,Sentiment Analysis for {H}inglish Code-mixed Tweets by means of Cross-lingual Word Embeddings,2020,0,0,2,1,2022,pranaydeep singh,Proceedings of the The 4th Workshop on Computational Approaches to Code Switching,0,"This paper investigates the use of unsupervised cross-lingual embeddings for solving the problem of code-mixed social media text understanding. We specifically investigate the use of these embeddings for a sentiment analysis task for Hinglish Tweets, viz. English combined with (transliterated) Hindi. In a first step, baseline models, initialized with monolingual embeddings obtained from large collections of tweets in English and code-mixed Hinglish, were trained. In a second step, two systems using cross-lingual embeddings were researched, being (1) a supervised classifier and (2) a transfer learning approach trained on English sentiment data and evaluated on code-mixed data. We demonstrate that incorporating cross-lingual embeddings improves the results (F1-score of 0.635 versus a monolingual baseline of 0.616), without any parallel data required to train the cross-lingual embeddings. In addition, the results show that the cross-lingual embeddings not only improve the results in a fully supervised setting, but they can also be used as a base for distant supervision, by training a sentiment model in one of the source languages and evaluating on the other language projected in the same space. The transfer learning experiments result in an F1-score of 0.556, which is almost on par with the supervised settings and speak to the robustness of the cross-lingual embeddings approach."
2020.argmining-1.2,"Annotating Topics, Stance, Argumentativeness and Claims in {D}utch Social Media Comments: A Pilot Study",2020,-1,-1,2,1,15209,nina bauwelinck,Proceedings of the 7th Workshop on Argument Mining,0,"One of the major challenges currently facing the field of argumentation mining is the lack of consensus on how to analyse argumentative user-generated texts such as online comments. The theoretical motivations underlying the annotation guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. This pilot study reports on the annotation of a corpus of 100 Dutch user comments made in response to politically-themed news articles on Facebook. The annotation covers topic and aspect labelling, stance labelling, argumentativeness detection and claim identification. Our IAA study reports substantial agreement scores for argumentativeness detection (0.76 Fleiss{'} kappa) and moderate agreement for claim labelling (0.45 Fleiss{'} kappa). We provide a clear justification of the theories and definitions underlying the design of our guidelines. Our analysis of the annotations signal the importance of adjusting our guidelines to include allowances for missing context information and defining the concept of argumentativeness in connection with stance. Our annotated corpus and associated guidelines are made publicly available."
S19-2077,{LT}3 at {S}em{E}val-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in {T}witter (hat{E}val),2019,0,0,4,1,15209,nina bauwelinck,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"This paper describes our contribution to the SemEval-2019 Task 5 on the detection of hate speech against immigrants and women in Twitter (hatEval). We considered a supervised classification-based approach to detect hate speech in English tweets, which combines a variety of standard lexical and syntactic features with specific features for capturing offensive language. Our experimental results show good classification performance on the training data, but a considerable drop in recall on the held-out test set."
R19-1071,A Classification-Based Approach to Cognate Detection Combining Orthographic and Semantic Similarity Information,2019,0,0,2,1,3283,sofie labat,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"This paper presents proof-of-concept experiments for combining orthographic and semantic information to distinguish cognates from non-cognates. To this end, a context-independent gold standard is developed by manually labelling English-Dutch pairs of cognates and false friends in bilingual term lists. These annotated cognate pairs are then used to train and evaluate a supervised binary classification system for the automatic detection of cognates. Two types of information sources are incorporated in the classifier: fifteen string similarity metrics capture form similarity between source and target words, while word embeddings model semantic similarity between the words. The experimental results show that even though the system already achieves good results by only incorporating orthographic information, the performance further improves by including semantic information in the form of embeddings."
R19-1117,Analysing the Impact of Supervised Machine Learning on Automatic Term Extraction: {HAMLET} vs {T}ermo{S}tat,2019,0,0,4,1,21025,ayla terryn,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Traditional approaches to automatic term extraction do not rely on machine learning (ML) and select the top n ranked candidate terms or candidate terms above a certain predefined cut-off point, based on a limited number of linguistic and statistical clues. However, supervised ML approaches are gaining interest. Relatively little is known about the impact of these supervised methodologies; evaluations are often limited to precision, and sometimes recall and f1-scores, without information about the nature of the extracted candidate terms. Therefore, the current paper presents a detailed and elaborate analysis and comparison of a traditional, state-of-the-art system (TermoStat) and a new, supervised ML approach (HAMLET), using the results obtained for the same, manually annotated, Dutch corpus about dressage."
W18-3101,Economic Event Detection in Company-Specific News Text,2018,0,4,2,1,19358,gilles jacobs,Proceedings of the First Workshop on Economics and Natural Language Processing,0,"This paper presents a dataset and supervised classification approach for economic event detection in English news articles. Currently, the economic domain is lacking resources and methods for data-driven supervised event detection. The detection task is conceived as a sentence-level classification task for 10 different economic event types. Two different machine learning approaches were tested: a rich feature set Support Vector Machine (SVM) set-up and a word-vector-based long short-term memory recurrent neural network (RNN-LSTM) set-up. We show satisfactory results for most event types, with the linear kernel SVM outperforming the other experimental set-ups"
S18-1005,{S}em{E}val-2018 Task 3: Irony Detection in {E}nglish Tweets,2018,0,28,2,1,440,cynthia hee,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper presents the first shared task on irony detection: given a tweet, automatic natural language processing systems should determine whether the tweet is ironic (Task A) and which type of irony (if any) is expressed (Task B). The ironic tweets were collected using irony-related hashtags (i.e. {\#}irony, {\#}sarcasm, {\#}not) and were subsequently manually annotated to minimise the amount of noise in the corpus. Prior to distributing the data, hashtags that were used to collect the tweets were removed from the corpus. For both tasks, a training corpus of 3,834 tweets was provided, as well as a test set containing 784 tweets. Our shared tasks received submissions from 43 teams for the binary classification Task A and from 31 teams for the multiclass Task B. The highest classification scores obtained for both subtasks are respectively F1= 0.71 and F1= 0.51 and demonstrate that fine-grained irony classification is much more challenging than binary irony detection."
L18-1284,A Gold Standard for Multilingual Automatic Term Extraction from Comparable Corpora: Term Structure and Translation Equivalents,2018,0,1,3,1,21025,ayla terryn,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1321,{MI}s{A}: Multilingual {``}{I}s{A}{''} Extraction from Corpora,2018,0,0,2,0,17226,stefano faralli,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1521,Discovering the Language of Wine Reviews: A Text Mining Account,2018,0,0,1,1,2023,els lefever,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"It is widely held that smells and flavors are impossible to put into words. In this paper we test this claim by seeking predictive patterns in wine reviews, which ostensibly aim to provide guides to perceptual content. Wine reviews have previously been critiqued as random and meaningless. We collected an English corpus of wine reviews with their structured metadata, and applied machine learning techniques to automatically predict the wine's color, grape variety, and country of origin. To train the three supervised classifiers, three different information sources were incorporated: lexical bag-of-words features, domain-specific terminology features, and semantic word embedding features. In addition, using regression analysis we investigated basic review properties, i.e., review length, average word length, and their relationship to the scalar values of price and review score. Our results show that wine experts do share a common vocabulary to describe wines and they use this in a consistent way, which makes it possible to automatically predict wine characteristics based on the review text alone. This means that odors and flavors may be more expressible in language than typically acknowledged."
J18-4010,We Usually Don{'}t Like Going to the Dentist: Using Common Sense to Detect Irony on {T}witter,2018,54,4,2,1,440,cynthia hee,Computational Linguistics,0,"Although common sense and connotative knowledge come naturally to most people, computers still struggle to perform well on tasks for which such extratextual information is required. Automatic approaches to sentiment analysis and irony detection have revealed that the lack of such world knowledge undermines classification performance. In this article, we therefore address the challenge of modeling implicit or prototypical sentiment in the framework of automatic irony detection. Starting from manually annotated connoted situation phrases (e.g., {``}flight delays,{''} {``}sitting the whole day at the doctor{'}s office{''}), we defined the implicit sentiment held towards such situations automatically by using both a lexico-semantic knowledge base and a data-driven method. We further investigate how such implicit sentiment information affects irony detection by assessing a state-of-the-art irony classifier before and after it is informed with implicit sentiment information."
W17-5218,Towards an integrated pipeline for aspect-based sentiment analysis in various domains,2017,0,3,2,0.779814,424,orphee clercq,"Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"This paper presents an integrated ABSA pipeline for Dutch that has been developed and tested on qualitative user feedback coming from three domains: retail, banking and human resources. The two latter domains provide service-oriented data, which has not been investigated before in ABSA. By performing in-domain and cross-domain experiments the validity of our approach was investigated. We show promising results for the three ABSA subtasks, aspect term extraction, aspect category classification and aspect polarity classification."
S16-1168,{S}em{E}val-2016 Task 13: Taxonomy Extraction Evaluation ({TE}x{E}val-2),2016,24,33,2,0,17236,georgeta bordea,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes the second edition of the shared task on Taxonomy Extraction Evaluation organised as part of SemEval 2016. This task aims to extract hypernym-hyponym relations between a given list of domain-specific terms and then to construct a domain taxonomy based on them. TExEval-2 introduced a multilingual setting for this task, covering four different languages including English, Dutch, Italian and French from domains as diverse as environment, food and science. A total of 62 runs submitted by 5 different teams were evaluated using structural measures, by comparison with gold standard taxonomies and by manual quality assessment of novel relations."
P16-2050,Very quaffable and great fun: Applying {NLP} to wine reviews,2016,25,6,2,0,16715,iris hendrickx,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We automatically predict properties of wines on the basis of smell and flavor descriptions from experts' wine reviews. We show wine experts are capable of describing their smell and flavor experiences in wine reviews in a sufficiently consistent manner, such that we can use their descriptions to predict properties of a wine based solely on language. The experimental results show promising F-scores when using lexical and semantic information to predict the color, grape variety, country of origin, and price of a wine. This demonstrates, contrary to popular opinion, that wine experts' reviews really are informative."
L16-1051,A Classification-based Approach to Economic Event Detection in {D}utch News Text,2016,0,1,1,1,2023,els lefever,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Breaking news on economic events such as stock splits or mergers and acquisitions has been shown to have a substantial impact on the financial markets. As it is important to be able to automatically identify events in news items accurately and in a timely manner, we present in this paper proof-of-concept experiments for a supervised machine learning approach to economic event detection in newswire text. For this purpose, we created a corpus of Dutch financial news articles in which 10 types of company-specific economic events were annotated. We trained classifiers using various lexical, syntactic and semantic features. We obtain good results based on a basic set of shallow features, thus showing that this method is a viable approach for economic event detection in news text."
L16-1283,Exploring the Realization of Irony in {T}witter Data,2016,20,2,2,1,440,cynthia hee,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Handling figurative language like irony is currently a challenging task in natural language processing. Since irony is commonly used in user-generated content, its presence can significantly undermine accurate analysis of opinions and sentiment in such texts. Understanding irony is therefore important if we want to push the state-of-the-art in tasks such as sentiment analysis. In this research, we present the construction of a Twitter dataset for two languages, being English and Dutch, and the development of new guidelines for the annotation of verbal irony in social media texts. Furthermore, we present some statistics on the annotated corpora, from which we can conclude that the detection of contrasting evaluations might be a good indicator for recognizing irony."
C16-1257,{M}onday mornings are my fave :) {\\#}not Exploring the Automatic Recognition of Irony in {E}nglish tweets,2016,0,4,2,1,440,cynthia hee,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Recognising and understanding irony is crucial for the improvement natural language processing tasks including sentiment analysis. In this study, we describe the construction of an English Twitter corpus and its annotation for irony based on a newly developed fine-grained annotation scheme. We also explore the feasibility of automatic irony recognition by exploiting a varied set of features including lexical, syntactic, sentiment and semantic (Word2Vec) information. Experiments on a held-out test set show that our irony classifier benefits from this combined information, yielding an F1-score of 67.66{\%}. When explicit hashtag information like {\#}irony is included in the data, the system even obtains an F1-score of 92.77{\%}. A qualitative analysis of the output reveals that recognising irony that results from a polarity clash appears to be (much) more feasible than recognising other forms of ironic utterances (e.g., descriptions of situational irony)."
S15-2115,{LT}3: Sentiment Analysis of Figurative Tweets: piece of cake {\\#}{N}ot{R}eally,2015,14,9,2,1,440,cynthia hee,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes our contribution to the SemEval-2015 Task 11 on sentiment analysis of figurative language in Twitter. We considered two approaches, classification and regression, to provide fine-grained sentiment scores for a set of tweets that are rich in sarcasm, irony and metaphor. To this end, we combined a variety of standard lexical and syntactic features with specific features for capturing figurative content. All experiments were done using supervised learning with LIBSVM. For both runs, our system ranked fourth among fifteen submissions."
S15-2122,{LT}3: Applying Hybrid Terminology Extraction to Aspect-Based Sentiment Analysis,2015,27,4,3,1,424,orphee clercq,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"The LT3 system perceives ABSA as a task consisting of three main subtasks, which have to be tackled incrementally, namely aspect term extraction, classification and polarity classification. For the first two steps, we see that employing a hybrid terminology extraction system leads to promising results, especially when it comes to recall. For the polarity classification, we show that it is possible to gain satisfying accuracies, even on out-ofdomain data, with a basic model employing only lexical information."
S15-2157,{LT}3: A Multi-modular Approach to Automatic Taxonomy Construction,2015,14,10,1,1,2023,els lefever,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes our contribution to the SemEval-2015 task 17 on xe2x80x9cTaxonomy Extraction Evaluationxe2x80x9d. We propose a hypernym detection system combining three modules: a lexico-syntactic pattern matcher, a morphosyntactic analyzer and a module retrieving hypernym relations from structured lexical resources. Our system ranked first in the competition when considering the gold standard and manual evaluation, and second in the overall ranking. In addition, the experimental results show that all modules contribute to finding hypernym relations between terms."
R15-1086,Detection and Fine-Grained Classification of Cyberbullying Events,2015,33,26,2,1,440,cynthia hee,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"In the current era of online interactions, both positive and negative experiences are abundant on the Web. As in real life, negative experiences can have a serious impact on youngsters. Recent studies have reported cybervictimization rates among teenagers that vary between 20% and 40%. In this paper, we focus on cyberbullying as a particular form of cybervictimization and explore its automatic detection and fine-grained classification. Data containing cyberbullying was collected from the social networking site Ask.fm. We developed and applied a new scheme for cyberbullying annotation, which describes the presence and severity of cyberbullying, a post author's role (harasser, victim or bystander) and a number of fine-grained categories related to cyberbullying, such as insults and threats. We present experimental results on the automatic detection of cyberbullying and explore the feasibility of detecting the more fine-grained cyberbullying categories in online posts. For the first task, an F-score of 55.39% is obtained. We observe that the detection of the fine-grained categories (e.g. threats) is more challenging, presumably due to data sparsity, and because they are often expressed in a subtle and implicit way."
S14-2005,{S}em{E}val 2014 Task 5 - {L}2 Writing Assistant,2014,10,3,4,0,38941,maarten gompel,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We present a new cross-lingual task for SemEval concerning the translation of L1 fragments in an L2 context. The task is at the boundary of Cross-Lingual Word Sense Disambiguation and Machine Translation. It finds its application in the field of computer-assisted translation, particularly in the context of second language learning. Translating L1 fragments in an L2 context allows language learners when writing in a target language (L2) to fall back to their native language (L1) whenever they are uncertain of the right word or phrase."
S14-2070,{LT}3: Sentiment Classification in User-Generated Content Using a Rich Feature Set,2014,17,6,4,1,440,cynthia hee,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper describes our contribution to the SemEval-2014 Task 9 on sentiment analysis in Twitter. We participated in both strands of the task, viz. classification at message-level (subtask B), and polarity disambiguation of particular text spans within a message (subtask A). Our experiments with a variety of lexical and syntactic features show that our systems benefit from rich feature sets for sentiment analysis on user-generated content. Our systems ranked ninth among 27 and sixteenth among 50 submissions for task A and B respectively."
lefever-etal-2014-evaluation,Evaluation of Automatic Hypernym Extraction from Technical Corpora in {E}nglish and {D}utch,2014,29,5,1,1,2023,els lefever,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this research, we evaluate different approaches for the automatic extraction of hypernym relations from English and Dutch technical text. The detected hypernym relations should enable us to semantically structure automatically obtained term lists from domain- and user-specific data. We investigated three different hypernymy extraction approaches for Dutch and English: a lexico-syntactic pattern-based approach, a distributional model and a morpho-syntactic method. To test the performance of the different approaches on domain-specific data, we collected and manually annotated English and Dutch data from two technical domains, viz. the dredging and financial domain. The experimental results show that especially the morpho-syntactic approach obtains good results for automatic hypernym extraction from technical and domain-specific texts."
S13-2029,{S}em{E}val-2013 Task 10: Cross-lingual Word Sense Disambiguation,2013,-1,-1,1,1,2023,els lefever,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,None
R13-1024,Normalization of {D}utch User-Generated Content,2013,19,17,4,1,424,orphee clercq,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"This paper describes a phrase-based machine translation approach to normalize Dutch user-generated content (UGC). We compiled a corpus of three different social media genres (text messages, message board posts and tweets) to have a sample of this recent domain. We describe the various characteristics of this noisy text material and explain how it has been manually normalized using newly developed guidelines. For the automatic normalization task we focus on text messages, and find that a cascaded SMT system where a token-based module is followed by a translation at the character level gives the best word error rate reduction. After these initial experiments, we investigate the systemxe2x80x99s robustness on the complete domain of UGC by testing it on the other two social media genres, and find that the cascaded approach performs best on these genres as well. To our knowledge, we deliver the first proof-of-concept system for Dutch UGC normalization, which can serve as a baseline for future work."
R13-1078,A Combined Pattern-based and Distributional Approach for Automatic Hypernym Detection in {D}utch.,2013,36,4,2,0,41335,gwendolijn schropp,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"This paper proposes a two-step approach to find hypernym relations between pairs of noun phrases in Dutch text. We first apply a pattern-based approach that combines lexical and shallow syntactic information to extract a list of candidate hypernym pairs from the input text. In a second step, distributional similarity information is used to filter the obtained list of candidate pairs. Evaluation of the system shows encouraging results and reveals that the distributional information particularly helps to improve the precision for context dependent hypernym pairs. The proposed hypernym module is considered an important step in building a semantic structure for automatically extracted terminology. As our approach does not require external lexical resources, it can be applied for any given Dutch input text and is particularly well suited for domain and user specific text."
lefever-etal-2012-discovering,Discovering Missing {W}ikipedia Inter-language Links by means of Cross-lingual Word Sense Disambiguation,2012,13,4,1,1,2023,els lefever,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Wikipedia pages typically contain inter-language links to the corresponding pages in other languages. These links, however, are often incomplete. This paper describes a set of experiments in which the viability of discovering such missing inter-language links for ambiguous nouns by means of a cross-lingual Word Sense Disambiguation approach is investigated. The input for the inter-language link detection system is a set of Dutch pages for a given ambiguous noun and the output of the system is a set of links to the corresponding pages in three target languages (viz. French, Spanish and Italian). The experimental results show that although it is a very challenging task, the system succeeds to detect missing inter-language links between Wikipedia documents for a manually labeled test set. The final goal of the system is to provide a human editor with a list of possible missing links that should be manually verified."
W11-1006,An Evaluation and Possible Improvement Path for Current {SMT} Behavior on Ambiguous Nouns,2011,31,0,1,1,2023,els lefever,"Proceedings of Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Mistranslation of an ambiguous word can have a large impact on the understandability of a given sentence. In this article, we describe a thorough evaluation of the translation quality of ambiguous nouns in three different setups. We compared two statistical Machine Translation systems and one dedicated Word Sense Disambiguation (WSD) system. Our WSD system incorporates multilingual information and is independent from external lexical resources. Word senses are derived automatically from word alignments on a parallel corpus. We show that the two WSD classifiers that were built for these experiments (English--French and English--Dutch) outperform the SMT system that was trained on the same corpus. This opens perspectives for the integration of our multilingual WSD module in a statistical Machine Translation framework, in order to improve the automated translation of ambiguous words, and by consequence make the translation output more understandable."
P11-2055,{P}ara{S}ense or How to Use Parallel Corpora for Word Sense Disambiguation,2011,26,30,1,1,2023,els lefever,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"This paper describes a set of exploratory experiments for a multilingual classification-based approach to Word Sense Disambiguation. Instead of using a predefined monolingual sense-inventory such as WordNet, we use a language-independent framework where the word senses are derived automatically from word alignments on a parallel corpus. We built five classifiers with English as an input language and translations in the five supported languages (viz. French, Dutch, Italian, Spanish and German) as classification output. The feature vectors incorporate both the more traditional local context features, as well as binary bag-of-words features that are extracted from the aligned translations. Our results show that the ParaSense multilingual WSD system shows very competitive results compared to the best systems that were evaluated on the SemEval-2010 Cross-Lingual Word Sense Disambiguation task for all five target languages."
S10-1003,{S}em{E}val-2010 Task 3: Cross-Lingual Word Sense Disambiguation,2010,-1,-1,1,1,2023,els lefever,Proceedings of the 5th International Workshop on Semantic Evaluation,0,None
lefever-hoste-2010-construction,Construction of a Benchmark Data Set for Cross-lingual Word Sense Disambiguation,2010,14,13,1,1,2023,els lefever,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Given the recent trend to evaluate the performance of word sense disambiguation systems in a more application-oriented set-up, we report on the construction of a multilingual benchmark data set for cross-lingual word sense disambiguation. The data set was created for a lexical sample of 25 English nouns, for which translations were retrieved in 5 languages, namely Dutch, German, French, Italian and Spanish. The corpus underlying the sense inventory was the parallel data set Europarl. The gold standard sense inventory was based on the automatic word alignments of the parallel corpus, which were manually verified. The resulting word alignments were used to perform a manual clustering of the translations over all languages in the parallel corpus. The inventory then served as input for the annotators of the sentences, who were asked to provide a maximum of three contextually relevant translations per language for a given focus word. The data set was released in the framework of the SemEval-2010 competition."
W09-2413,{S}em{E}val-2010 Task 3: Cross-lingual Word Sense Disambiguation,2009,29,88,1,1,2023,els lefever,Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009),0,"We propose a multilingual unsupervised Word Sense Disambiguation (WSD) task for a sample of English nouns. Instead of providing manually sensetagged examples for each sense of a polysemous noun, our sense inventory is built up on the basis of the Europarl parallel corpus. The multilingual setup involves the translations of a given English polysemous noun in five supported languages, viz. Dutch, French, German, Spanish and Italian.n n The task targets the following goals: (a) the manual creation of a multilingual sense inventory for a lexical sample of English nouns and (b) the evaluation of systems on their ability to disambiguate new occurrences of the selected polysemous nouns. For the creation of the hand-tagged gold standard, all translations of a given polysemous English noun are retrieved in the five languages and clustered by meaning. Systems can participate in 5 bilingual evaluation subtasks (English -- Dutch, English -- German, etc.) and in a multilingual subtask covering all language pairs.n n As WSD from cross-lingual evidence is gaining popularity, we believe it is important to create a multilingual gold standard and run cross-lingual WSD benchmark tests."
E09-1057,Language-Independent Bilingual Terminology Extraction from a Multilingual Parallel Corpus,2009,15,26,1,1,2023,els lefever,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We present a language-pair independent terminology extraction module that is based on a sub-sentential alignment system that links linguistically motivated phrases in parallel texts. Statistical filters are applied on the bilingual list of candidate terms that is extracted from the alignment output.n n We compare the performance of both the alignment and terminology extraction module for three different language pairs (French-English, French-Italian and French-Dutch) and highlight language-pair specific problems (e.g. different compounding strategy in French and Dutch).n n Comparisons with standard terminology extraction programs show an improvement of up to 20% for bilingual terminology extraction and competitive results (85% to 90% accuracy) for monolingual terminology extraction, and reveal that the linguistically based alignment module is particularly well suited for the extraction of complex multiword terms."
hoste-etal-2008-learning,Learning-based Detection of Scientific Terms in Patient Information,2008,29,1,2,0,441,veronique hoste,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper, we investigate the use of a machine-learning based approach to the specific problem of scientific term detection in patient information. Lacking lexical databases which differentiate between the scientific and popular nature of medical terms, we used local context, morphosyntactic, morphological and statistical information to design a learner which accurately detects scientific medical terms. This study is the first step towards the automatic replacement of a scientific term by its popular counterpart, which should have a beneficial effect on readability. We show a F-score of 84{\%} for the prediction of scientific terms in an English and Dutch EPAR corpus. Since recasting the term extraction problem as a classification problem leads to a large skewedness of the resulting data set, we rebalanced the data set through the application of some simple TF-IDF-based and Log-likelihood-based filters. We show that filtering indeed has a beneficial effect on the learnerÂs performance. However, the results of the filtering approach combined with the learning-based approach remain below those of the learning-based approach."
C08-1067,Linguistically-Based Sub-Sentential Alignment for Terminology Extraction from a Bilingual Automotive Corpus,2008,10,12,2,0,17620,lieve macken,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,We present a sub-sentential alignment system that links linguistically motivated phrases in parallel texts based on lexical correspondences and syntactic similarity. We compare the performance of our sub-sentential alignment system with different symmetrization heuristics that combine the GIZA alignments of both translation directions. We demonstrate that the aligned linguistically motivated phrases are a useful means to extract bilingual terminology and more specifically complex multiword terms.
S07-1019,{AUG}: A combined classification and clustering approach for web people disambiguation,2007,8,12,1,1,2023,els lefever,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper presents a combined supervised and unsupervised approach for multi-document person name disambiguation. Based on feature vectors reflecting pairwise comparisons between web pages, a classification algorithm provides linking information about document pairs, which leads to initial clusters. In addition, two different clustering algorithms are fed with matrices of weighted keywords. In a final step the seed clusters are combined with the results of the clustering algorithms. Results on the validation data show that a combined classification and clustering approach doesn't always compare favorably to those obtained by the different algorithms separately."
