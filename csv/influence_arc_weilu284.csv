2009.iwslt-evaluation.14,I05-3025,1,0.891679,"follows: 1. Introduction This is the first year that the National University of Singapore (NUS) participated in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT). We submitted a run for the Chinese-English BTEC task1 , where we were ranked second out of twelve participating teams, based on the average of the normalized scores of ten automatic evaluation metrics. We adopted a phrase-based statistical machine translation (SMT) approach, and we investigated the effectiveness of different Chinese word segmentation standards. Using a maximum entropy model [1] and various data sources, we trained six different Chinese word segmenters. Each segmenter was then used to preprocess the Chinese side of the training/development/testing bi-texts, from which a separate phrase-based SMT system was built. Some of the resulting six systems yielded substantial translation performance gains as compared to a system that used the default segmentation provided by the organizers. Finally, we combined the output of all seven systems. The rest of this paper is organized as follows: Section 2 introduces the phrase-based SMT model, Section 3 presents our pre-processing"
2009.iwslt-evaluation.14,N03-1017,0,0.0069836,"Missing"
2009.iwslt-evaluation.14,P03-1021,0,0.0499017,"Missing"
2009.iwslt-evaluation.14,2002.tmi-tutorials.2,0,0.0814266,"Missing"
2009.iwslt-evaluation.14,P07-2045,0,0.0126909,"Missing"
2009.iwslt-evaluation.14,W04-1118,0,0.068797,"ned and used as follows: 4. Word Segmentation and Re-ranking In this section, we describe our experiments with different Chinese word segmentations and how we combine them into a single system using re-ranking. 4.1. Chinese Word Segmentation 1. We ran all seven candidate systems on the development data. The output included the English translation and thirteen associated scores from the SMT toolkit, which we used as features: Chinese word segmentation (CWS) has been shown conclusively as an essential step in machine translation, at least as far as current phrase-based SMT methods are concerned [6]. However, CWS is complicated by the fact that a word is not a well-defined concept in Chinese, where characters, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has b"
2009.iwslt-evaluation.14,2005.iwslt-1.18,0,0.037429,"efined concept in Chinese, where characters, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentat"
2009.iwslt-evaluation.14,P08-1115,0,0.0349163,"efined concept in Chinese, where characters, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentat"
2009.iwslt-evaluation.14,W08-0336,0,0.0478417,"ers, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentation provided by the IWSLT organizers,"
2009.iwslt-evaluation.14,W08-0335,0,0.0323456,"continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentation provided by the IWSLT organizers, and ICTCLAS-generated [11] segmentati"
2009.iwslt-evaluation.14,W03-1730,0,0.0713535,"erpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentation provided by the IWSLT organizers, and ICTCLAS-generated [11] segmentation respectively. Although ICTCLAS was also based on the PKU standard, the output seemed different enough from our PKU segmenter to be included as a separate candidate. (a) five (5) from the distortion model; (b) two (2) from the phrase translation model; (c) two (2) from the lexical translation model; (d) one (1) for the language model; (e) one (1) for the phrase penalty; (f) one (1) for the word penalty; and (g) one (1) for the final overall translation score (as calculated by Moses from all individual scores above and the MERT-tuned parameters). 2. A global fourteenth feature repe"
2009.iwslt-evaluation.14,P09-1104,0,0.0301126,"d various parameters of the phrase-based SMT system. We further describe a novel retraining technique yielding sizeable improvements in BLEU. 5.1. Parameter Tuning 1. We used the training bi-text to build a phrase table and to train an English language model. The Moses phrase-based SMT toolkit has a large number of options. While it comes with very sensible defaults, we found experimentally that varying some of them had a significant impact on the translation quality. Table 1 shows some non-standard settings used in our submission. Note that, for word alignments, we used the Berkeley Aligner2 [12] in unsupervised mode, which we found to outperform GIZA++ significantly. We used the default parameters of the aligner, except that we increased the number of iterations to 40. 2. We used the development dataset to tune the weights of the log-linear model of the phrase-based SMT system using MERT. 3. We concatenated the training and the development datasets; we then re-built the phrase table and retrained the language model on this new dataset. 4. We repeated the above three steps for each of the seven Chinese word segmenters, thus obtaining seven candidate systems. 5.2. Re-training on the De"
2009.iwslt-evaluation.14,P07-1005,1,0.881248,"Missing"
2020.acl-main.141,H05-1091,0,0.209583,"epresentation of an entity node is computed as the average of its mentions. To build a document-level graph, existing approaches use all nodes in the dependency tree of a sentence (Sahu et al., 2019) or one sentence-level node by averaging all token representations of the sentence (Christopoulou et al., 2019). Alternatively, we use tokens on the shortest dependency path between mentions in the sentence. The shortest dependency path has been widely used in the sentence-level relation extraction as it is able to effectively make use of relevant information while ignoring irrelevant information (Bunescu and Mooney, 2005; Xu et al., 2015a,b). Unlike sentence-level extraction, where each sentence only has two entities, each sentence here may involve multiple mentions. 2.2 Dynamic Reasoner The dynamic reasoner has two modules, structure induction and multi-hop reasoning as shown in Figure 3. The structure induction module is used to learn a latent structure of a document-level graph. The multi-hop reasoning module is used to perform inference on the induced latent structure, where representations of each node will be updated based on the information aggregation scheme. We stack N blocks in order to iteratively"
2020.acl-main.141,P16-1072,0,0.0572035,"ics. Ign F1 denotes F1 scores excluding relational facts shared by the training and dev/test sets. F1 scores for intra- and intersentence entity pairs are also reported. Evaluation on the test set is done through CodaLab3 . 3.3 Main Results We compare our proposed LSR with the following three types of competitive models on the DocRED dataset, and show the main results in Table 2. • Sequence-based Models. These models leverage different neural architectures to encode sentences in the document, including convolutional neural networks (CNN) (Zeng et al., 2014), LSTM, bidirectional LSTM (BiLSTM) (Cai et al., 2016) and attention-based LSTM (ContextAware) (Sorokin and Gurevych, 2017). • Graph-based Models. These models construct task-specific graphs for inference. GCNN (Sahu et al., 2019) constructs a document-level graph by co-reference links, and then applies relational GCNs for reasoning. EoG (Christopoulou et al., 2019) is the state-of-the-art document-level relation extraction model in biomedical domain. EoG first uses heuristics to construct the graph, then leverages an edge-oriented model to perform inference. GCNN and EoG are based on static structures. GAT (Veliˇckovi´c et al., 2018) is able to"
2020.acl-main.141,P18-2014,0,0.332318,"ics et al., 2012). On the other hand, structural information has been used to perform better reasoning since it models the non-local dependencies that are obscure from the surface form alone. Peng et al. (2017) construct dependency graph to capture interactions among n-ary entities for cross-sentence extraction. Sahu et al. (2019) extend this approach by using co-reference links to connect dependency trees of sentences to construct the document-level graph. Instead, Christopoulou et al. (2019) construct a heterogeneous graph based on a set of heuristics, and then apply an edge-oriented model (Christopoulou et al., 2018) to perform inference. Unlike previous methods, where a documentlevel structure is constructed by co-references and rules, our proposed model treats the graph structure as a latent variable and induces it in an end-to-end fashion. Our model is built based on the structured attention (Kim et al., 2017; Liu and Lapata, 2018). Using a variant of Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007), our model is able to generate task-specific dependency structures for capturing non-local interactions between entities. We further develop an iterative refinement strategy, which enables our model to d"
2020.acl-main.141,D19-1498,0,0.17869,"f the Association for Computational Linguistics, pages 1546–1557 c July 5 - 10, 2020. 2020 Association for Computational Linguistics et al., 2012). On the other hand, structural information has been used to perform better reasoning since it models the non-local dependencies that are obscure from the surface form alone. Peng et al. (2017) construct dependency graph to capture interactions among n-ary entities for cross-sentence extraction. Sahu et al. (2019) extend this approach by using co-reference links to connect dependency trees of sentences to construct the document-level graph. Instead, Christopoulou et al. (2019) construct a heterogeneous graph based on a set of heuristics, and then apply an edge-oriented model (Christopoulou et al., 2018) to perform inference. Unlike previous methods, where a documentlevel structure is constructed by co-references and rules, our proposed model treats the graph structure as a latent variable and induces it in an end-to-end fashion. Our model is built based on the structured attention (Kim et al., 2017; Liu and Lapata, 2018). Using a variant of Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007), our model is able to generate task-specific dependency structures for cap"
2020.acl-main.141,N19-1240,0,0.0382912,"Missing"
2020.acl-main.141,N19-1423,0,0.565751,"de constructor encodes sentences in a document into contextual representations and constructs representations of mention nodes, entity nodes and meta dependency paths (MDP) nodes, as shown in Figure 2. Here MDP indicates a set of shortest dependency paths for all mentions in a sentence, and tokens in the MDP are extracted as MDP nodes. 2.1.1 Context Encoding Given a document d, each sentence di in it is fed to the context encoder, which outputs the contextualized representations of each word in di . The context encoder can be a bidirectional LSTM (BiLSTM) (Schuster and Paliwal, 1997) or BERT (Devlin et al., 2019). Here we use the BiLSTM as an example: ← − ← − hij = LSTMl (hi j+1 , γji ) (1) − →i → −i hj = LSTMr (h j−1 , γji ) (2) ← − ← − − → → − where hij , hi j+1 , hij and hi j−1 represent the hidden representations of the j-th, (j+1)-th and (j-1)th token in the sentence di of two directions, and γji denotes the word embedding of the j-th token. Contextual representation of each token in the sentence ← − − → is represented as hij = [hij ; hij ] by concatenating hidden states of two directions, where hij ∈ Rd and d is the dimension. 1547 AVG Lutsenko is a former minister of internal affairs. Context H"
2020.acl-main.141,N18-2007,0,0.0335345,"et al., 2018; Jia et al., 2019; Sahu et al., 2019; Christopoulou et al., 2019) in the biomedical domain by only considering a few relations among chemicals. Unlike previous work, we focus on document-level relation extraction datasets (Yao et al., 2019; Li et al., 2016a; Wu et al., 2019) from different domains with a large number of relations and entities, which require understanding a document and performing multi-hop reasoning. Structure-based relational reasoning. Structural information has been widely used for relational reasoning in various NLP applications including question answering (Dhingra et al., 2018; De Cao et al., 2019; Song et al., 2018a) and relation extraction (Sahu et al., 2019; Christopoulou et al., 2019). Song et al. (2018a) and (De Cao et al., 2019) leverage co-reference information and set of rules to construct document-level entity graph. GCNs (Kipf and Welling, 2017) or GRNs (Song et al., 2018b) are applied to perform reasoning for multi-hop question answering (Welbl et al., 2018). Sahu et al. (2019) also utilize co-reference links to construct the dependency graph and use labelled edge GCNs (Marcheggiani and Titov, 2017) for document-level relation extraction. Instead of usin"
2020.acl-main.141,doddington-etal-2004-automatic,0,0.400355,"Missing"
2020.acl-main.141,P19-1062,0,0.0188248,"ment-level graph. With the help of dense connections, we are able to train a deeper model, allowing richer local and non-local information to be captured for learning a better graph representation. The computations on each graph convolution layer is similar to Equation (9). 2.2.3 Iterative Refinement Though structured attention (Kim et al., 2017; Liu and Lapata, 2018) is able to automatically induce a latent structure, recent research efforts show that the induced structure is relatively shallow and may not be able to model the complex dependencies for document-level input (Liu et al., 2019b; Ferracane et al., 2019). Unlike previous work (Liu and Lapata, 2018) that only induces the latent structure once, we repeatedly refine the document-level graph based on the updated representations, allowing the model to infer a more informative structure that goes beyond simple parent-child relations. As shown in Figure 3, we stack N blocks of the dynamic reasoner in order to induce the documentlevel structure N times. Intuitively, the reasoner 1549 induces a shallow structure at early iterations since the information propagates mostly between neighboring nodes. As the structure gets more refined by interactions wit"
2020.acl-main.141,P19-1024,1,0.875476,"an be represented with an n × n adjacency matrix A induced by the previous structure induction module, the convolution computation for the node i at the l-th layer, which takes the representation ul−1 from previous layer as input and outputs the i updated representations uli , can be defined as: uli n X = σ( Aij Wl ul−1 + bl ) i (9) j=1 where Wl and bl are the weight matrix and bias vector for the l-th layer, respectively. σ is the ReLU (Nair and Hinton, 2010) activation function. u0i ∈ Rd is the initial contextual representation of the i-th node constructed by the node constructor. Following Guo et al. (2019b), we use dense connections to the GCNs in order to capture more structural information on a large document-level graph. With the help of dense connections, we are able to train a deeper model, allowing richer local and non-local information to be captured for learning a better graph representation. The computations on each graph convolution layer is similar to Equation (9). 2.2.3 Iterative Refinement Though structured attention (Kim et al., 2017; Liu and Lapata, 2018) is able to automatically induce a latent structure, recent research efforts show that the induced structure is relatively sha"
2020.acl-main.141,Q19-1019,1,0.916342,"an be represented with an n × n adjacency matrix A induced by the previous structure induction module, the convolution computation for the node i at the l-th layer, which takes the representation ul−1 from previous layer as input and outputs the i updated representations uli , can be defined as: uli n X = σ( Aij Wl ul−1 + bl ) i (9) j=1 where Wl and bl are the weight matrix and bias vector for the l-th layer, respectively. σ is the ReLU (Nair and Hinton, 2010) activation function. u0i ∈ Rd is the initial contextual representation of the i-th node constructed by the node constructor. Following Guo et al. (2019b), we use dense connections to the GCNs in order to capture more structural information on a large document-level graph. With the help of dense connections, we are able to train a deeper model, allowing richer local and non-local information to be captured for learning a better graph representation. The computations on each graph convolution layer is similar to Equation (9). 2.2.3 Iterative Refinement Though structured attention (Kim et al., 2017; Liu and Lapata, 2018) is able to automatically induce a latent structure, recent research efforts show that the induced structure is relatively sha"
2020.acl-main.141,W09-2415,0,0.124296,"Missing"
2020.acl-main.141,N19-1370,0,0.219219,"ns across sentence boundaries in real-world scenarios (Peng et al., 2017). Therefore, the scope ∗ † Equally Contributed. Work done during internship at SUTD. Object:Ukrainian Relation: country of citizenship of extraction in biomedical domain has recently been expanded to cross-sentence level (Quirk and Poon, 2017; Gupta et al., 2018; Song et al., 2019). A more challenging, yet practical extension, is the document-level relation extraction, where a system needs to comprehend multiple sentences to infer the relations among entities by synthesizing relevant information from the entire document (Jia et al., 2019; Yao et al., 2019). Figure 1 shows an example adapted from the recently proposed document-level dataset DocRED (Yao et al., 2019). In order to infer the inter-sentence relation (i.e., country of citizenship) between Yulia Tymoshenko and Ukrainian, one first has to identify the fact that Lutsenko works with Yulia Tymoshenko. Next we identify that Lutsenko manages internal affairs, which is a Ukrainian authority. After incrementally connecting the evidence in the document and performing the step-by-step reasoning, we are able to infer that Yulia Tymoshenko is also a Ukrainian. Prior efforts sho"
2020.acl-main.141,D07-1015,0,0.736979,"ees of sentences to construct the document-level graph. Instead, Christopoulou et al. (2019) construct a heterogeneous graph based on a set of heuristics, and then apply an edge-oriented model (Christopoulou et al., 2018) to perform inference. Unlike previous methods, where a documentlevel structure is constructed by co-references and rules, our proposed model treats the graph structure as a latent variable and induces it in an end-to-end fashion. Our model is built based on the structured attention (Kim et al., 2017; Liu and Lapata, 2018). Using a variant of Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007), our model is able to generate task-specific dependency structures for capturing non-local interactions between entities. We further develop an iterative refinement strategy, which enables our model to dynamically build the latent structure based on the last iteration, allowing the model to incrementally capture the complex interactions for better multi-hop reasoning (Welbl et al., 2018). Experiments show that our model significantly outperforms the existing approaches on DocRED, a large-scale document-level relation extraction dataset with a large number of entities and relations, and also y"
2020.acl-main.141,D19-1282,0,0.0231485,"the i-th and the j-th node. Then, Aij can be derived based on Equation (8), where δ is the Kronecker delta (Koo et al., 2007). ˆ −1 ]ij Aij = (1 − δ1,j )Pij [L ˆ −1 ]ji −(1 − δi,1 )Pij [L (8) Here, A ∈ Rn×n can be interpreted as a weighted adjacency matrix of the document-level entity graph. Finally, we can feed A ∈ Rn×n into the multi-hop reasoning module to update the representations of nodes in the latent structure. 2.2.2 Multi-hop Reasoning Graph neural networks have been widely used in different tasks to perform multi-hop reasoning (Song et al., 2018a; Yang et al., 2019; Tu et al., 2019; Lin et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme. Specifically, our model is based on graph convolutional networks (GCNs) (Kipf and Welling, 2017) to perform reasoning. Formally, given a graph G with n nodes, which can be represented with an n × n adjacency matrix A induced by the previous structure induction module, the convolution computation for the node i at the l-th layer, which takes the representation ul−1 from previous layer as input and outputs the i updated representations uli , can be defined as: uli n X = σ( Aij Wl ul−1 + bl ) i"
2020.acl-main.141,P19-1629,0,0.146151,"ion on a large document-level graph. With the help of dense connections, we are able to train a deeper model, allowing richer local and non-local information to be captured for learning a better graph representation. The computations on each graph convolution layer is similar to Equation (9). 2.2.3 Iterative Refinement Though structured attention (Kim et al., 2017; Liu and Lapata, 2018) is able to automatically induce a latent structure, recent research efforts show that the induced structure is relatively shallow and may not be able to model the complex dependencies for document-level input (Liu et al., 2019b; Ferracane et al., 2019). Unlike previous work (Liu and Lapata, 2018) that only induces the latent structure once, we repeatedly refine the document-level graph based on the updated representations, allowing the model to infer a more informative structure that goes beyond simple parent-child relations. As shown in Figure 3, we stack N blocks of the dynamic reasoner in order to induce the documentlevel structure N times. Intuitively, the reasoner 1549 induces a shallow structure at early iterations since the information propagates mostly between neighboring nodes. As the structure gets more r"
2020.acl-main.141,Q18-1005,0,0.343133,"9) extend this approach by using co-reference links to connect dependency trees of sentences to construct the document-level graph. Instead, Christopoulou et al. (2019) construct a heterogeneous graph based on a set of heuristics, and then apply an edge-oriented model (Christopoulou et al., 2018) to perform inference. Unlike previous methods, where a documentlevel structure is constructed by co-references and rules, our proposed model treats the graph structure as a latent variable and induces it in an end-to-end fashion. Our model is built based on the structured attention (Kim et al., 2017; Liu and Lapata, 2018). Using a variant of Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007), our model is able to generate task-specific dependency structures for capturing non-local interactions between entities. We further develop an iterative refinement strategy, which enables our model to dynamically build the latent structure based on the last iteration, allowing the model to incrementally capture the complex interactions for better multi-hop reasoning (Welbl et al., 2018). Experiments show that our model significantly outperforms the existing approaches on DocRED, a large-scale document-level relation extr"
2020.acl-main.141,N19-1173,0,0.150013,"ion on a large document-level graph. With the help of dense connections, we are able to train a deeper model, allowing richer local and non-local information to be captured for learning a better graph representation. The computations on each graph convolution layer is similar to Equation (9). 2.2.3 Iterative Refinement Though structured attention (Kim et al., 2017; Liu and Lapata, 2018) is able to automatically induce a latent structure, recent research efforts show that the induced structure is relatively shallow and may not be able to model the complex dependencies for document-level input (Liu et al., 2019b; Ferracane et al., 2019). Unlike previous work (Liu and Lapata, 2018) that only induces the latent structure once, we repeatedly refine the document-level graph based on the updated representations, allowing the model to infer a more informative structure that goes beyond simple parent-child relations. As shown in Figure 3, we stack N blocks of the dynamic reasoner in order to induce the documentlevel structure N times. Intuitively, the reasoner 1549 induces a shallow structure at early iterations since the information propagates mostly between neighboring nodes. As the structure gets more r"
2020.acl-main.141,P15-2047,0,0.0299762,"ng relation for hJapan, World War IIi that requires reasoning across sentences. However, LSR also attends to the mention New Ireland with a high score, thus failing to predict that the entity pair hNew Ireland, World War IIi actually has no relation (NIL type). 4 Related Work Document-level relation extraction. Early efforts focus on predicting relations between entities within a single sentence by modeling interactions in the input sequence (Zeng et al., 2014; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Guo et al., 2020) or the corresponding dependency tree (Xu et al., 2015a,b; Liu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2018). These approaches do not consider interactions across mentions and ignore relations expressed across sentence boundaries. Recent work begins to explore cross-sentence extraction (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2018; Song et al., 2018c, 2019). Instead of using discourse structure understanding techniques (Liu et al., 2019a; Lei et al., 2017, 2018), these approaches leverage the dependency graph to capture inter-sentence interactions, and their scope is still limited to several sentences. More recently, the extraction scope has"
2020.acl-main.141,D17-1159,0,0.0352523,"soning in various NLP applications including question answering (Dhingra et al., 2018; De Cao et al., 2019; Song et al., 2018a) and relation extraction (Sahu et al., 2019; Christopoulou et al., 2019). Song et al. (2018a) and (De Cao et al., 2019) leverage co-reference information and set of rules to construct document-level entity graph. GCNs (Kipf and Welling, 2017) or GRNs (Song et al., 2018b) are applied to perform reasoning for multi-hop question answering (Welbl et al., 2018). Sahu et al. (2019) also utilize co-reference links to construct the dependency graph and use labelled edge GCNs (Marcheggiani and Titov, 2017) for document-level relation extraction. Instead of using GNNs, Christopoulou et al. (2019) use the edgeoriented model (Christopoulou et al., 2018) for logical inference based on a heterogeneous graph constructed by heuristics. Unlike previous approaches that use syntactic trees, co-references or heuristics, LSR model treats the document-level structure as a latent variable and induces it in an iteratively refined fashion, allowing the model to dynamically construct the graph for better relational reasoning. 5 Conclusion We introduce a novel latent structure refinement (LSR) model for better r"
2020.acl-main.141,P16-1105,0,0.0531685,"apan, World War IIi that requires reasoning across sentences. However, LSR also attends to the mention New Ireland with a high score, thus failing to predict that the entity pair hNew Ireland, World War IIi actually has no relation (NIL type). 4 Related Work Document-level relation extraction. Early efforts focus on predicting relations between entities within a single sentence by modeling interactions in the input sequence (Zeng et al., 2014; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Guo et al., 2020) or the corresponding dependency tree (Xu et al., 2015a,b; Liu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2018). These approaches do not consider interactions across mentions and ignore relations expressed across sentence boundaries. Recent work begins to explore cross-sentence extraction (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2018; Song et al., 2018c, 2019). Instead of using discourse structure understanding techniques (Liu et al., 2019a; Lei et al., 2017, 2018), these approaches leverage the dependency graph to capture inter-sentence interactions, and their scope is still limited to several sentences. More recently, the extraction scope has been expanded to the e"
2020.acl-main.141,W18-2314,0,0.12291,"additional training data and/or incorporate external tools. the latent structure induced by our model is indeed capable of synthesizing the information across multiple sentences of a document. Furthermore, LSR with GloVe also proves better in the inter-sentence setting compared with two BERT-based (Wang et al., 2019) models, indicating latent structure’s superiority in resolving longrange dependencies across the whole document compared with the BERT encoder. 3.5 Results on the Biomedical Datasets Table 3 depicts the comparisons with state-ofthe-art models on the CDR dataset. Gu et al. (2017); Nguyen and Verspoor (2018); Verga et al. (2018) leverage sequence-based models. Convolutional neural networks and self-attention networks are used as the encoders. Sahu et al. (2019); Christopoulou et al. (2019) use graph-based models. As shown in Table 3, our LSR performs worse than the state-of-the-art models. It is challenging for an off-the-shelf parser to get high quality dependency trees in the biomedical domain, as we observe that the MDP nodes extracted by the spaCy parser from the CDR dataset contains much less informative context compared with the nodes from DocRED. Here we introduce a simplified LSR model in"
2020.acl-main.141,D19-1020,0,0.25309,"pplications. Early research efforts focus on predicting relations between entities within the sentence (Zeng et al., 2014; Xu et al., 2015a,b). However, valuable relational information between entities, such as biomedical findings, is expressed by multiple mentions across sentence boundaries in real-world scenarios (Peng et al., 2017). Therefore, the scope ∗ † Equally Contributed. Work done during internship at SUTD. Object:Ukrainian Relation: country of citizenship of extraction in biomedical domain has recently been expanded to cross-sentence level (Quirk and Poon, 2017; Gupta et al., 2018; Song et al., 2019). A more challenging, yet practical extension, is the document-level relation extraction, where a system needs to comprehend multiple sentences to infer the relations among entities by synthesizing relevant information from the entire document (Jia et al., 2019; Yao et al., 2019). Figure 1 shows an example adapted from the recently proposed document-level dataset DocRED (Yao et al., 2019). In order to infer the inter-sentence relation (i.e., country of citizenship) between Yulia Tymoshenko and Ukrainian, one first has to identify the fact that Lutsenko works with Yulia Tymoshenko. Next we iden"
2020.acl-main.141,P18-1150,0,0.357333,"the marginal probability of the dependency edge between the i-th and the j-th node. Then, Aij can be derived based on Equation (8), where δ is the Kronecker delta (Koo et al., 2007). ˆ −1 ]ij Aij = (1 − δ1,j )Pij [L ˆ −1 ]ji −(1 − δi,1 )Pij [L (8) Here, A ∈ Rn×n can be interpreted as a weighted adjacency matrix of the document-level entity graph. Finally, we can feed A ∈ Rn×n into the multi-hop reasoning module to update the representations of nodes in the latent structure. 2.2.2 Multi-hop Reasoning Graph neural networks have been widely used in different tasks to perform multi-hop reasoning (Song et al., 2018a; Yang et al., 2019; Tu et al., 2019; Lin et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme. Specifically, our model is based on graph convolutional networks (GCNs) (Kipf and Welling, 2017) to perform reasoning. Formally, given a graph G with n nodes, which can be represented with an n × n adjacency matrix A induced by the previous structure induction module, the convolution computation for the node i at the l-th layer, which takes the representation ul−1 from previous layer as input and outputs the i updated representations ul"
2020.acl-main.141,D18-1246,0,0.241518,"the marginal probability of the dependency edge between the i-th and the j-th node. Then, Aij can be derived based on Equation (8), where δ is the Kronecker delta (Koo et al., 2007). ˆ −1 ]ij Aij = (1 − δ1,j )Pij [L ˆ −1 ]ji −(1 − δi,1 )Pij [L (8) Here, A ∈ Rn×n can be interpreted as a weighted adjacency matrix of the document-level entity graph. Finally, we can feed A ∈ Rn×n into the multi-hop reasoning module to update the representations of nodes in the latent structure. 2.2.2 Multi-hop Reasoning Graph neural networks have been widely used in different tasks to perform multi-hop reasoning (Song et al., 2018a; Yang et al., 2019; Tu et al., 2019; Lin et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme. Specifically, our model is based on graph convolutional networks (GCNs) (Kipf and Welling, 2017) to perform reasoning. Formally, given a graph G with n nodes, which can be represented with an n × n adjacency matrix A induced by the previous structure induction module, the convolution computation for the node i at the l-th layer, which takes the representation ul−1 from previous layer as input and outputs the i updated representations ul"
2020.acl-main.141,D17-1188,0,0.117845,"ared by the training and dev/test sets. F1 scores for intra- and intersentence entity pairs are also reported. Evaluation on the test set is done through CodaLab3 . 3.3 Main Results We compare our proposed LSR with the following three types of competitive models on the DocRED dataset, and show the main results in Table 2. • Sequence-based Models. These models leverage different neural architectures to encode sentences in the document, including convolutional neural networks (CNN) (Zeng et al., 2014), LSTM, bidirectional LSTM (BiLSTM) (Cai et al., 2016) and attention-based LSTM (ContextAware) (Sorokin and Gurevych, 2017). • Graph-based Models. These models construct task-specific graphs for inference. GCNN (Sahu et al., 2019) constructs a document-level graph by co-reference links, and then applies relational GCNs for reasoning. EoG (Christopoulou et al., 2019) is the state-of-the-art document-level relation extraction model in biomedical domain. EoG first uses heuristics to construct the graph, then leverages an edge-oriented model to perform inference. GCNN and EoG are based on static structures. GAT (Veliˇckovi´c et al., 2018) is able to learn the weighted graph structure based on a local attention mechani"
2020.acl-main.141,D12-1042,0,0.178239,"Missing"
2020.acl-main.141,P19-1260,0,0.0211726,"ncy edge between the i-th and the j-th node. Then, Aij can be derived based on Equation (8), where δ is the Kronecker delta (Koo et al., 2007). ˆ −1 ]ij Aij = (1 − δ1,j )Pij [L ˆ −1 ]ji −(1 − δi,1 )Pij [L (8) Here, A ∈ Rn×n can be interpreted as a weighted adjacency matrix of the document-level entity graph. Finally, we can feed A ∈ Rn×n into the multi-hop reasoning module to update the representations of nodes in the latent structure. 2.2.2 Multi-hop Reasoning Graph neural networks have been widely used in different tasks to perform multi-hop reasoning (Song et al., 2018a; Yang et al., 2019; Tu et al., 2019; Lin et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme. Specifically, our model is based on graph convolutional networks (GCNs) (Kipf and Welling, 2017) to perform reasoning. Formally, given a graph G with n nodes, which can be represented with an n × n adjacency matrix A induced by the previous structure induction module, the convolution computation for the node i at the l-th layer, which takes the representation ul−1 from previous layer as input and outputs the i updated representations uli , can be defined as: uli n X = σ( A"
2020.acl-main.141,Q17-1008,0,0.650446,"e highlighted with the same color. The solid and dotted lines represent intra- and inter-sentence relations, respectively. Introduction Relation extraction aims to detect relations among entities in the text and plays a significant role in a variety of natural language processing applications. Early research efforts focus on predicting relations between entities within the sentence (Zeng et al., 2014; Xu et al., 2015a,b). However, valuable relational information between entities, such as biomedical findings, is expressed by multiple mentions across sentence boundaries in real-world scenarios (Peng et al., 2017). Therefore, the scope ∗ † Equally Contributed. Work done during internship at SUTD. Object:Ukrainian Relation: country of citizenship of extraction in biomedical domain has recently been expanded to cross-sentence level (Quirk and Poon, 2017; Gupta et al., 2018; Song et al., 2019). A more challenging, yet practical extension, is the document-level relation extraction, where a system needs to comprehend multiple sentences to infer the relations among entities by synthesizing relevant information from the entire document (Jia et al., 2019; Yao et al., 2019). Figure 1 shows an example adapted fr"
2020.acl-main.141,D14-1162,0,0.088872,"Missing"
2020.acl-main.141,E17-1110,0,0.400306,"a variety of natural language processing applications. Early research efforts focus on predicting relations between entities within the sentence (Zeng et al., 2014; Xu et al., 2015a,b). However, valuable relational information between entities, such as biomedical findings, is expressed by multiple mentions across sentence boundaries in real-world scenarios (Peng et al., 2017). Therefore, the scope ∗ † Equally Contributed. Work done during internship at SUTD. Object:Ukrainian Relation: country of citizenship of extraction in biomedical domain has recently been expanded to cross-sentence level (Quirk and Poon, 2017; Gupta et al., 2018; Song et al., 2019). A more challenging, yet practical extension, is the document-level relation extraction, where a system needs to comprehend multiple sentences to infer the relations among entities by synthesizing relevant information from the entire document (Jia et al., 2019; Yao et al., 2019). Figure 1 shows an example adapted from the recently proposed document-level dataset DocRED (Yao et al., 2019). In order to infer the inter-sentence relation (i.e., country of citizenship) between Yulia Tymoshenko and Ukrainian, one first has to identify the fact that Lutsenko w"
2020.acl-main.141,P19-1423,0,0.584413,"Thus, Verga et al. (2018) and Jia et al. (2019) leverage MultiInstance Learning (Riedel et al., 2010; Surdeanu 1546 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1546–1557 c July 5 - 10, 2020. 2020 Association for Computational Linguistics et al., 2012). On the other hand, structural information has been used to perform better reasoning since it models the non-local dependencies that are obscure from the surface form alone. Peng et al. (2017) construct dependency graph to capture interactions among n-ary entities for cross-sentence extraction. Sahu et al. (2019) extend this approach by using co-reference links to connect dependency trees of sentences to construct the document-level graph. Instead, Christopoulou et al. (2019) construct a heterogeneous graph based on a set of heuristics, and then apply an edge-oriented model (Christopoulou et al., 2018) to perform inference. Unlike previous methods, where a documentlevel structure is constructed by co-references and rules, our proposed model treats the graph structure as a latent variable and induces it in an end-to-end fashion. Our model is built based on the structured attention (Kim et al., 2017; Li"
2020.acl-main.141,N18-1080,0,0.522538,"In order to infer the inter-sentence relation (i.e., country of citizenship) between Yulia Tymoshenko and Ukrainian, one first has to identify the fact that Lutsenko works with Yulia Tymoshenko. Next we identify that Lutsenko manages internal affairs, which is a Ukrainian authority. After incrementally connecting the evidence in the document and performing the step-by-step reasoning, we are able to infer that Yulia Tymoshenko is also a Ukrainian. Prior efforts show that interactions between mentions of entities facilitate the reasoning process in the document-level relation extraction. Thus, Verga et al. (2018) and Jia et al. (2019) leverage MultiInstance Learning (Riedel et al., 2010; Surdeanu 1546 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1546–1557 c July 5 - 10, 2020. 2020 Association for Computational Linguistics et al., 2012). On the other hand, structural information has been used to perform better reasoning since it models the non-local dependencies that are obscure from the surface form alone. Peng et al. (2017) construct dependency graph to capture interactions among n-ary entities for cross-sentence extraction. Sahu et al. (2019) extend"
2020.acl-main.141,P16-1123,0,0.276722,"Missing"
2020.acl-main.141,Q18-1021,0,0.115459,"as a latent variable and induces it in an end-to-end fashion. Our model is built based on the structured attention (Kim et al., 2017; Liu and Lapata, 2018). Using a variant of Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007), our model is able to generate task-specific dependency structures for capturing non-local interactions between entities. We further develop an iterative refinement strategy, which enables our model to dynamically build the latent structure based on the last iteration, allowing the model to incrementally capture the complex interactions for better multi-hop reasoning (Welbl et al., 2018). Experiments show that our model significantly outperforms the existing approaches on DocRED, a large-scale document-level relation extraction dataset with a large number of entities and relations, and also yields new state-of-the-art results on two popular document-level relation extraction datasets in the biomedical domain. The code and pretrained model are available at https: //github.com/nanguoshun/LSR 1 . Our contributions are summarized as follows: • We construct a document-level graph for inference in an end-to-end fashion without relying on co-references or rules, which may not always"
2020.acl-main.141,D15-1062,0,0.464759,"et. The example has four entities: Lutsenko, internal affairs, Yulia Tymoshenko and Ukrainian. Here entity Lutsenko has two mentions: Lutsenko and He. Mentions corresponding to the same entity are highlighted with the same color. The solid and dotted lines represent intra- and inter-sentence relations, respectively. Introduction Relation extraction aims to detect relations among entities in the text and plays a significant role in a variety of natural language processing applications. Early research efforts focus on predicting relations between entities within the sentence (Zeng et al., 2014; Xu et al., 2015a,b). However, valuable relational information between entities, such as biomedical findings, is expressed by multiple mentions across sentence boundaries in real-world scenarios (Peng et al., 2017). Therefore, the scope ∗ † Equally Contributed. Work done during internship at SUTD. Object:Ukrainian Relation: country of citizenship of extraction in biomedical domain has recently been expanded to cross-sentence level (Quirk and Poon, 2017; Gupta et al., 2018; Song et al., 2019). A more challenging, yet practical extension, is the document-level relation extraction, where a system needs to compre"
2020.acl-main.141,D15-1206,0,0.512843,"et. The example has four entities: Lutsenko, internal affairs, Yulia Tymoshenko and Ukrainian. Here entity Lutsenko has two mentions: Lutsenko and He. Mentions corresponding to the same entity are highlighted with the same color. The solid and dotted lines represent intra- and inter-sentence relations, respectively. Introduction Relation extraction aims to detect relations among entities in the text and plays a significant role in a variety of natural language processing applications. Early research efforts focus on predicting relations between entities within the sentence (Zeng et al., 2014; Xu et al., 2015a,b). However, valuable relational information between entities, such as biomedical findings, is expressed by multiple mentions across sentence boundaries in real-world scenarios (Peng et al., 2017). Therefore, the scope ∗ † Equally Contributed. Work done during internship at SUTD. Object:Ukrainian Relation: country of citizenship of extraction in biomedical domain has recently been expanded to cross-sentence level (Quirk and Poon, 2017; Gupta et al., 2018; Song et al., 2019). A more challenging, yet practical extension, is the document-level relation extraction, where a system needs to compre"
2020.acl-main.141,D19-1451,1,0.867928,"lity of the dependency edge between the i-th and the j-th node. Then, Aij can be derived based on Equation (8), where δ is the Kronecker delta (Koo et al., 2007). ˆ −1 ]ij Aij = (1 − δ1,j )Pij [L ˆ −1 ]ji −(1 − δi,1 )Pij [L (8) Here, A ∈ Rn×n can be interpreted as a weighted adjacency matrix of the document-level entity graph. Finally, we can feed A ∈ Rn×n into the multi-hop reasoning module to update the representations of nodes in the latent structure. 2.2.2 Multi-hop Reasoning Graph neural networks have been widely used in different tasks to perform multi-hop reasoning (Song et al., 2018a; Yang et al., 2019; Tu et al., 2019; Lin et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme. Specifically, our model is based on graph convolutional networks (GCNs) (Kipf and Welling, 2017) to perform reasoning. Formally, given a graph G with n nodes, which can be represented with an n × n adjacency matrix A induced by the previous structure induction module, the convolution computation for the node i at the l-th layer, which takes the representation ul−1 from previous layer as input and outputs the i updated representations uli , can be defined a"
2020.acl-main.141,P19-1074,0,0.499186,"boundaries in real-world scenarios (Peng et al., 2017). Therefore, the scope ∗ † Equally Contributed. Work done during internship at SUTD. Object:Ukrainian Relation: country of citizenship of extraction in biomedical domain has recently been expanded to cross-sentence level (Quirk and Poon, 2017; Gupta et al., 2018; Song et al., 2019). A more challenging, yet practical extension, is the document-level relation extraction, where a system needs to comprehend multiple sentences to infer the relations among entities by synthesizing relevant information from the entire document (Jia et al., 2019; Yao et al., 2019). Figure 1 shows an example adapted from the recently proposed document-level dataset DocRED (Yao et al., 2019). In order to infer the inter-sentence relation (i.e., country of citizenship) between Yulia Tymoshenko and Ukrainian, one first has to identify the fact that Lutsenko works with Yulia Tymoshenko. Next we identify that Lutsenko manages internal affairs, which is a Ukrainian authority. After incrementally connecting the evidence in the document and performing the step-by-step reasoning, we are able to infer that Yulia Tymoshenko is also a Ukrainian. Prior efforts show that interactions"
2020.acl-main.141,C14-1220,0,0.860517,"om the DocRED dataset. The example has four entities: Lutsenko, internal affairs, Yulia Tymoshenko and Ukrainian. Here entity Lutsenko has two mentions: Lutsenko and He. Mentions corresponding to the same entity are highlighted with the same color. The solid and dotted lines represent intra- and inter-sentence relations, respectively. Introduction Relation extraction aims to detect relations among entities in the text and plays a significant role in a variety of natural language processing applications. Early research efforts focus on predicting relations between entities within the sentence (Zeng et al., 2014; Xu et al., 2015a,b). However, valuable relational information between entities, such as biomedical findings, is expressed by multiple mentions across sentence boundaries in real-world scenarios (Peng et al., 2017). Therefore, the scope ∗ † Equally Contributed. Work done during internship at SUTD. Object:Ukrainian Relation: country of citizenship of extraction in biomedical domain has recently been expanded to cross-sentence level (Quirk and Poon, 2017; Gupta et al., 2018; Song et al., 2019). A more challenging, yet practical extension, is the document-level relation extraction, where a syste"
2020.acl-main.141,D18-1244,0,0.147769,"t requires reasoning across sentences. However, LSR also attends to the mention New Ireland with a high score, thus failing to predict that the entity pair hNew Ireland, World War IIi actually has no relation (NIL type). 4 Related Work Document-level relation extraction. Early efforts focus on predicting relations between entities within a single sentence by modeling interactions in the input sequence (Zeng et al., 2014; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Guo et al., 2020) or the corresponding dependency tree (Xu et al., 2015a,b; Liu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2018). These approaches do not consider interactions across mentions and ignore relations expressed across sentence boundaries. Recent work begins to explore cross-sentence extraction (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2018; Song et al., 2018c, 2019). Instead of using discourse structure understanding techniques (Liu et al., 2019a; Lei et al., 2017, 2018), these approaches leverage the dependency graph to capture inter-sentence interactions, and their scope is still limited to several sentences. More recently, the extraction scope has been expanded to the entire document (Verga"
2020.acl-main.141,D17-1004,0,0.217924,"ion of a relation, such as P607, P17, etc. The LSR model proves capable of filling out the missing relation for hJapan, World War IIi that requires reasoning across sentences. However, LSR also attends to the mention New Ireland with a high score, thus failing to predict that the entity pair hNew Ireland, World War IIi actually has no relation (NIL type). 4 Related Work Document-level relation extraction. Early efforts focus on predicting relations between entities within a single sentence by modeling interactions in the input sequence (Zeng et al., 2014; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Guo et al., 2020) or the corresponding dependency tree (Xu et al., 2015a,b; Liu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2018). These approaches do not consider interactions across mentions and ignore relations expressed across sentence boundaries. Recent work begins to explore cross-sentence extraction (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2018; Song et al., 2018c, 2019). Instead of using discourse structure understanding techniques (Liu et al., 2019a; Lei et al., 2017, 2018), these approaches leverage the dependency graph to capture inter-sentence interactions,"
2020.acl-main.141,P16-2034,0,0.0235911,"19) for the definition of a relation, such as P607, P17, etc. The LSR model proves capable of filling out the missing relation for hJapan, World War IIi that requires reasoning across sentences. However, LSR also attends to the mention New Ireland with a high score, thus failing to predict that the entity pair hNew Ireland, World War IIi actually has no relation (NIL type). 4 Related Work Document-level relation extraction. Early efforts focus on predicting relations between entities within a single sentence by modeling interactions in the input sequence (Zeng et al., 2014; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Guo et al., 2020) or the corresponding dependency tree (Xu et al., 2015a,b; Liu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2018). These approaches do not consider interactions across mentions and ignore relations expressed across sentence boundaries. Recent work begins to explore cross-sentence extraction (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2018; Song et al., 2018c, 2019). Instead of using discourse structure understanding techniques (Liu et al., 2019a; Lei et al., 2017, 2018), these approaches leverage the dependency graph to capture inter-sen"
2020.acl-main.312,W19-4813,0,0.032407,"Missing"
2020.acl-main.312,N19-1357,0,0.307752,"ntities may impact the model performance.1 1 Introduction Attention mechanism (Bahdanau et al., 2015) has been used as an important component across a wide range of NLP models. Typically, an attention layer produces a distribution over input representations to be attended to. Such a distribution is then used for constructing a weighted combination of the inputs, which will then be employed by certain downstream modules. Recently, several research efforts on investigating the interpretability of attention on tasks such as text classification, question answering, and natural language inference (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Arras et al., 2019) have been conducted. One of their important arguments was whether the attention distribution could adequately reflect the significance of inputs. To answer this question, they designed a series of metrics and 1 Supplementary material and code at https:// github.com/richardsun-voyager/UAFTC conducted corresponding experiments. In their approaches, they were mainly observing how the attention may impact the outputs on the pre-trained models by changing some elements in the inputs. While such approaches have resulted in interesting findings, the a"
2020.acl-main.312,N16-1082,0,0.0490454,"interpretable, and show how the polarity scores, another important token-level quantity, will play their roles in the overall model in terms of contributing towards the model performance. 2 Related Work Research on interpretability of neural models has received significant attention recently. One approach was using visualization to explore patterns that exist in the intermediate representations of neural networks. Simonyan et al. (2013) visualized the image-specific class saliency on image classification tasks using learnt ConvNets, and displayed the features captured by the neural networks. Li et al. (2016a,b) proposed visualization methods to look into the neural representations of the embeddings from the local composition, concessive sentences, clause composition, as well as the saliency of phrases and sentences, and illustrated patterns based on the visualizations. An erasure method was also adopted to validate the importance of different dimensions and words. Vig and Belinkov (2019) analyzed the attention structure on the Transformer (Vaswani et al., 2017) language model as well as GPT-2 (Radford et al., 2019) pre-trained model. Another approach to understanding neural approaches is to cond"
2020.acl-main.312,D15-1166,0,0.161898,"Missing"
2020.acl-main.312,P11-1015,0,0.04665,"iments on four text classification datasets7 . The statistics of the datasets are shown in Table 1. We followed the work of Jain and Wallace (2019) for pre-processing of the datasets8 , and lower-cased all the tokens. • Stanford Sentiment Treebank (SST) (Socher et al., 2013). The original dataset that consists of 10,662 instances with labels ranging from 1 (most negative) to 5 (most positive). Similar to the work of Jain and Wallace (2019), we removed neutral instances (with label 3), and regarded instances with label 4 or 5 as positive and instances with the label 1 or 2 as negative. • IMDB (Maas et al., 2011). The original dataset 6 We have further discussions on V > W in the supplementary material. 7 We also conducted analysis on synthetic datasets. The results can be found in the supplementary material. 8 https://github.com/successar/ AttentionExplanation 3423 λ 0.001 1 10 20 50 100 10000 λ 0.001 1 10 20 50 100 10000 SST DP DP-L DP-A AD 55.3 74.4 82.2 81.4 80.8 81.2 79.6 79.8 81.2 81.7 80.9 82.0 81.1 81.4 67.9 73.4 80.8 81.0 81.5 80.7 79.3 62.8 73.4 80.3 81.2 79.9 80.8 80.8 DP DP-L DP-A AD 55.5 79.5 89.2 89.6 89.8 89.3 89.3 87.7 88.2 87.8 88.1 87.2 88.3 88.4 73.3 85.4 89.6 89.6 89.1 89.2 88.9 69"
2020.acl-main.312,N18-1202,0,0.0693555,"Missing"
2020.acl-main.312,P19-1282,0,0.0325919,"a shifted positive PMI (pointwise mutual information) matrix. Recently, several research efforts have focused on the interpretability of the attention mechanism. Jain and Wallace (2019) raised the question on the explainability of feature importance as captured by the attention mechanism. They found the attention weights may not always be consistent with Output Sigmoid s = hT W Linear h = ∑j αj hj Attention h1 h2 ... hj ... hn−1 hn Figure 1: Classification architecture with attention the feature importance from the human perspective in tasks such as text classification and question answering. Serrano and Smith (2019) also carried out an analysis on the interpretability of the attention mechanism, with a focus on the text classification task. They conducted their study in a cautious way with respect to defining interpretability and the research scope. The paper concluded that the attention weights are noisy predictors of importance, but should not be regarded as justification for decisions. Wiegreffe and Pinter (2019) suggested that the notion of explanation needs to be clearly defined, and the study of the explanation requires taking all components of a model into account. Their results indicated that pri"
2020.acl-main.312,D13-1170,0,0.00417589,"res from the model with additive attention, the model with an affine input layer and the model for multi-class classification respectively. There are terms that have similar effects on polarity and attention scores during training. Due to space limitations, we provide such details in the supplementary material. 5 Experiments We conducted experiments on four text classification datasets7 . The statistics of the datasets are shown in Table 1. We followed the work of Jain and Wallace (2019) for pre-processing of the datasets8 , and lower-cased all the tokens. • Stanford Sentiment Treebank (SST) (Socher et al., 2013). The original dataset that consists of 10,662 instances with labels ranging from 1 (most negative) to 5 (most positive). Similar to the work of Jain and Wallace (2019), we removed neutral instances (with label 3), and regarded instances with label 4 or 5 as positive and instances with the label 1 or 2 as negative. • IMDB (Maas et al., 2011). The original dataset 6 We have further discussions on V > W in the supplementary material. 7 We also conducted analysis on synthetic datasets. The results can be found in the supplementary material. 8 https://github.com/successar/ AttentionExplanation 342"
2020.acl-main.312,W19-4808,0,0.0174309,"te representations of neural networks. Simonyan et al. (2013) visualized the image-specific class saliency on image classification tasks using learnt ConvNets, and displayed the features captured by the neural networks. Li et al. (2016a,b) proposed visualization methods to look into the neural representations of the embeddings from the local composition, concessive sentences, clause composition, as well as the saliency of phrases and sentences, and illustrated patterns based on the visualizations. An erasure method was also adopted to validate the importance of different dimensions and words. Vig and Belinkov (2019) analyzed the attention structure on the Transformer (Vaswani et al., 2017) language model as well as GPT-2 (Radford et al., 2019) pre-trained model. Another approach to understanding neural approaches is to conduct theoretical analysis to investigate the underlying explanations of neural models. One example is the work of Levy and Goldberg (2014), which regarded the word embedding learning task as an optimization problem, and found that the training process of the skip-gram model (Mikolov et al., 2013a,b) can be explained as implicit factorization of a shifted positive PMI (pointwise mutual i"
2020.acl-main.312,P19-1580,0,0.027557,"conducted their study in a cautious way with respect to defining interpretability and the research scope. The paper concluded that the attention weights are noisy predictors of importance, but should not be regarded as justification for decisions. Wiegreffe and Pinter (2019) suggested that the notion of explanation needs to be clearly defined, and the study of the explanation requires taking all components of a model into account. Their results indicated that prior work could not disprove the usefulness of attention mechanisms with respect to explainability. Moreover, Michel et al. (2019) and Voita et al. (2019) examined the multi-head self-attention mechanism on Transformer-based models, particularly the roles played by the heads. Our work and findings are largely consistent with such findings reported in the literature. We believe there are many factors involved when understanding the attention mechanism. Inspired by Qian (1999), which investigated the internal mechanism of gradient descent, in this work we focus on understanding attention’s internal mechanism. 3 Classification Model with Attention We consider the task of text classification, with a specific focus on binary classification.2 The arc"
2020.acl-main.312,D19-1002,0,0.191434,"odel performance.1 1 Introduction Attention mechanism (Bahdanau et al., 2015) has been used as an important component across a wide range of NLP models. Typically, an attention layer produces a distribution over input representations to be attended to. Such a distribution is then used for constructing a weighted combination of the inputs, which will then be employed by certain downstream modules. Recently, several research efforts on investigating the interpretability of attention on tasks such as text classification, question answering, and natural language inference (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Arras et al., 2019) have been conducted. One of their important arguments was whether the attention distribution could adequately reflect the significance of inputs. To answer this question, they designed a series of metrics and 1 Supplementary material and code at https:// github.com/richardsun-voyager/UAFTC conducted corresponding experiments. In their approaches, they were mainly observing how the attention may impact the outputs on the pre-trained models by changing some elements in the inputs. While such approaches have resulted in interesting findings, the attention mechanism itself re"
2020.acl-main.312,N16-1174,0,0.125331,"paper, we will mainly investigate a simple neural model where hj = ej . Here ej is the word embedding for the j-th input token. In other words, we assume the word embeddings are used as the inputs to the attention layer. Detailed discussions on other assumptions on hj can be found in the supplementary material. (1) where the hyperparameter λ is the scaling factor √ (typically set to a large value, e.g., d is often used in the literature (Vaswani et al., 2017)), and V ∈ Rd is the context vector that can be viewed as a fixed query asking for the “most informative word” from the input sequence (Yang et al., 2016). The token representation hj can be the word embedding, or the output of an encoder. The corresponding attention weight would be: exp(aj ) . j 0 exp aj 0 αj = P (2) The complete input sequence is represented as: h= X αj hj , (3) j and the output of the linear layer is: s = h> W , (4) which we call instance-level polarity score of the input sequence. Here, W ∈ Rd is the weight vector for the linear layer. When we make predictions, if the resulting polarity score s is positive, the corresponding input sequence will be classified as positive (i.e., y = +1, where y is the output label). Otherwis"
2020.emnlp-main.133,N19-4010,0,0.0503958,"Missing"
2020.emnlp-main.133,doddington-etal-2004-automatic,0,0.265333,"h pre-trained word embeddings and language model are fixed without fine-tuning. In addition, we stack three encoding layers (L = 3) with independent parameters including the GRU cell in each layer. For the table encoder, we use two separate MDRNNs with the directions of “layer+ row+ col+ ” and “layer+ row− col− ” respectively. For the sequence encoder, we use eight attention heads to attend to different representation subspaces. We report the averaged F1 scores of 5 runs for our models. For each run, we keep the model that achieves 5 ACE05 We evaluate our model on four datasets, namely ACE04 (Doddington et al., 2004), ACE05 (Walker et al., 2006), CoNLL04 (Roth and tau Yih, 2004) and ADE (Gurulingappa et al., 2012). More details could be found in Appendix B. Following the established line of work, we use the F1 measure to evaluate the performance of NER and RE. For NER, an entity prediction is correct if and only if its type and boundaries both match with those of a gold entity.5 For RE, a relation prediction is considered correct if its relation type and the boundaries of the two entities match with those in the gold data. We also report the strict relation F1 (denoted RE+), where a relation prediction is"
2020.emnlp-main.133,P06-1060,0,0.746189,"eriments confirm the advantages of having two encoders over one encoder. On several standard datasets, our model shows significant improvements over existing approaches.1 1 Edward Thomas is from Minnesota , United States B-PER ⊥ ⊥ ⊥ live_in ⊥ live_in live_in ⊥ ⊥ live_in ⊥ live_in live_in Thomas ⊥ I-PER is ⊥ ⊥ O ⊥ ⊥ ⊥ ⊥ ⊥ from ⊥ ⊥ ⊥ O ⊥ ⊥ ⊥ ⊥ Minnesota live_in live_in ⊥ ⊥ B-LOC ⊥ loc_in loc_in , ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ United live_in live_in ⊥ ⊥ loc_in ⊥ B-LOC ⊥ States live_in live_in ⊥ ⊥ loc_in ⊥ ⊥ I-LOC O Figure 1: An example of table filling for NER and RE. Introduction Named Entity Recognition (NER, Florian et al. 2006, 2010) and Relation Extraction (RE, Zhao and Grishman 2005; Jiang and Zhai 2007; Sun et al. 2011; Plank and Moschitti 2013) are two fundamental tasks in Information Extraction (IE). Both tasks aim to extract structured information from unstructured texts. One typical approach is to first identify entity mentions, and next perform classification between every two mentions to extract relations, forming a pipeline (Zelenko et al., 2002; Chan and Roth, 2011). An alternative and more recent approach is to perform these two tasks jointly (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 201"
2020.emnlp-main.133,D10-1033,0,0.489728,"Missing"
2020.emnlp-main.133,N19-1308,0,0.240751,"Missing"
2020.emnlp-main.133,P16-1105,0,0.680145,"Florian et al. 2006, 2010) and Relation Extraction (RE, Zhao and Grishman 2005; Jiang and Zhai 2007; Sun et al. 2011; Plank and Moschitti 2013) are two fundamental tasks in Information Extraction (IE). Both tasks aim to extract structured information from unstructured texts. One typical approach is to first identify entity mentions, and next perform classification between every two mentions to extract relations, forming a pipeline (Zelenko et al., 2002; Chan and Roth, 2011). An alternative and more recent approach is to perform these two tasks jointly (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016), which mitigates the error propagation issue associated with the pipeline ap1 Our code is available at https://github.com/ LorrinWWW/two-are-better-than-one. proach and leverages the interaction between tasks, resulting in improved performance. Among several joint approaches, one popular idea is to cast NER and RE as a table filling problem (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017). Typically, a two-dimensional (2D) table is formed where each entry captures the interaction between two individual words within a sentence. NER is then regarded as a sequence labeling problem"
2020.emnlp-main.133,D14-1200,0,0.475506,"ntity Recognition (NER, Florian et al. 2006, 2010) and Relation Extraction (RE, Zhao and Grishman 2005; Jiang and Zhai 2007; Sun et al. 2011; Plank and Moschitti 2013) are two fundamental tasks in Information Extraction (IE). Both tasks aim to extract structured information from unstructured texts. One typical approach is to first identify entity mentions, and next perform classification between every two mentions to extract relations, forming a pipeline (Zelenko et al., 2002; Chan and Roth, 2011). An alternative and more recent approach is to perform these two tasks jointly (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016), which mitigates the error propagation issue associated with the pipeline ap1 Our code is available at https://github.com/ LorrinWWW/two-are-better-than-one. proach and leverages the interaction between tasks, resulting in improved performance. Among several joint approaches, one popular idea is to cast NER and RE as a table filling problem (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017). Typically, a two-dimensional (2D) table is formed where each entry captures the interaction between two individual words within a sentence. NER is then regarded as a s"
2020.emnlp-main.133,2020.acl-main.141,1,0.844168,"o confirm the effectiveness of our proposed approach. 2 Related Work NER and RE can be tackled by using separate models. By assuming gold entity mentions are given as inputs, RE can be regarded as a classification task. Such models include kernel methods (Zelenko et al., 2002), RNNs (Zhang and Wang, 2015), recursive neural networks (Socher et al., 2012), CNNs (Zeng et al., 2014), and Transformer models (Verga et al., 2018; Wang et al., 2019). Another branch is to detect cross-sentence level relations (Peng et al., 2017; Gupta et al., 2019), and even document-level relations (Yao et al., 2019; Nan et al., 2020). However, entities are usually not directly available in practice, so these approaches may require an additional entity recognizer to form a pipeline. Joint learning has been shown effective since it can alleviate the error propagation issue and benefit from exploiting the interrelation between NER and RE. Many studies address the joint problem through a cascade approach, i.e., performing NER first followed by RE. Miwa and Bansal (2016) use bi-LSTM (Graves et al., 2013) and tree-LSTM (Tai et al., 2015) for the joint task. Bekoulis et al. (2018a,b) formulate it as a head selection problem. Ngu"
2020.emnlp-main.133,Q17-1008,0,0.0184592,"n four datasets, namely ACE04, ACE05, CoNLL04, and ADE. We also conduct further experiments to confirm the effectiveness of our proposed approach. 2 Related Work NER and RE can be tackled by using separate models. By assuming gold entity mentions are given as inputs, RE can be regarded as a classification task. Such models include kernel methods (Zelenko et al., 2002), RNNs (Zhang and Wang, 2015), recursive neural networks (Socher et al., 2012), CNNs (Zeng et al., 2014), and Transformer models (Verga et al., 2018; Wang et al., 2019). Another branch is to detect cross-sentence level relations (Peng et al., 2017; Gupta et al., 2019), and even document-level relations (Yao et al., 2019; Nan et al., 2020). However, entities are usually not directly available in practice, so these approaches may require an additional entity recognizer to form a pipeline. Joint learning has been shown effective since it can alleviate the error propagation issue and benefit from exploiting the interrelation between NER and RE. Many studies address the joint problem through a cascade approach, i.e., performing NER first followed by RE. Miwa and Bansal (2016) use bi-LSTM (Graves et al., 2013) and tree-LSTM (Tai et al., 2015"
2020.emnlp-main.133,D14-1162,0,0.0928613,"Missing"
2020.emnlp-main.133,N18-1202,0,0.0197787,"corresponds to the pair of i-th and j-th word of the input sentence. The diagonal of the table is filled with the entity tags and the rest with the relation tags indicating possible relations between word pairs. Similarly, Gupta et al. (2016) employ a bi-RNN structure to label each word pair. Zhang et al. (2017) propose a global optimization method to fill the table. Tran and Kavuluru (2019) investigate CNNs on this task. Recent work (Luan et al., 2019; Dixit and Al, 2019; Wadden et al., 2019; Li et al., 2019; Eberts and Ulges, 2019) usually leverages pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 1707 Figure 3: A layer in the table-sequence encoders. Figure 2: Overview of the table-sequence encoders. Dashed lines are for optional components (T ` ). 2019), and ALBERT (Lan et al., 2019). However, none of them use pre-trained attention weights, which convey rich relational information between words. We believe it can be useful for learning better table representations for RE. 3 Problem Formulation In this section, we formally formulate the NER and RE tasks. We regard NER as a sequence labeling problem, where the gold entity tags y NER are"
2020.emnlp-main.133,P13-1147,0,0.0495711,"significant improvements over existing approaches.1 1 Edward Thomas is from Minnesota , United States B-PER ⊥ ⊥ ⊥ live_in ⊥ live_in live_in ⊥ ⊥ live_in ⊥ live_in live_in Thomas ⊥ I-PER is ⊥ ⊥ O ⊥ ⊥ ⊥ ⊥ ⊥ from ⊥ ⊥ ⊥ O ⊥ ⊥ ⊥ ⊥ Minnesota live_in live_in ⊥ ⊥ B-LOC ⊥ loc_in loc_in , ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ United live_in live_in ⊥ ⊥ loc_in ⊥ B-LOC ⊥ States live_in live_in ⊥ ⊥ loc_in ⊥ ⊥ I-LOC O Figure 1: An example of table filling for NER and RE. Introduction Named Entity Recognition (NER, Florian et al. 2006, 2010) and Relation Extraction (RE, Zhao and Grishman 2005; Jiang and Zhai 2007; Sun et al. 2011; Plank and Moschitti 2013) are two fundamental tasks in Information Extraction (IE). Both tasks aim to extract structured information from unstructured texts. One typical approach is to first identify entity mentions, and next perform classification between every two mentions to extract relations, forming a pipeline (Zelenko et al., 2002; Chan and Roth, 2011). An alternative and more recent approach is to perform these two tasks jointly (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016), which mitigates the error propagation issue associated with the pipeline ap1 Our code is available at https://github.com"
2020.emnlp-main.133,W09-1119,0,0.221207,"e table-sequence encoders. Figure 2: Overview of the table-sequence encoders. Dashed lines are for optional components (T ` ). 2019), and ALBERT (Lan et al., 2019). However, none of them use pre-trained attention weights, which convey rich relational information between words. We believe it can be useful for learning better table representations for RE. 3 Problem Formulation In this section, we formally formulate the NER and RE tasks. We regard NER as a sequence labeling problem, where the gold entity tags y NER are in the standard BIO (Begin, Inside, Outside) scheme (Sang and Veenstra, 1999; Ratinov and Roth, 2009). For the RE task, we mainly follow the work of Miwa and Sasaki (2014) to formulate it as a table filling problem. Formally, given an input sentence x = [xi ]1≤i≤N , we maintain a tag RE ] table y RE = [yi,j 1≤i,j≤N . Suppose there is a relation with type r pointing from mention xib , .., xie − RE = → to mention xj b , .., xj e , we have yi,j r and ← − RE b e b e yj,i = r for all i ∈ [i , i ] ∧ j ∈ [j , j ]. We use ⊥ for word pairs with no relation. An example was given earlier in Figure 1. 4 Model We describe the model in this section. The model consists of two types of interconnected encoder"
2020.emnlp-main.133,W04-2401,0,0.530332,"Missing"
2020.emnlp-main.133,E99-1023,0,0.640977,"7 Figure 3: A layer in the table-sequence encoders. Figure 2: Overview of the table-sequence encoders. Dashed lines are for optional components (T ` ). 2019), and ALBERT (Lan et al., 2019). However, none of them use pre-trained attention weights, which convey rich relational information between words. We believe it can be useful for learning better table representations for RE. 3 Problem Formulation In this section, we formally formulate the NER and RE tasks. We regard NER as a sequence labeling problem, where the gold entity tags y NER are in the standard BIO (Begin, Inside, Outside) scheme (Sang and Veenstra, 1999; Ratinov and Roth, 2009). For the RE task, we mainly follow the work of Miwa and Sasaki (2014) to formulate it as a table filling problem. Formally, given an input sentence x = [xi ]1≤i≤N , we maintain a tag RE ] table y RE = [yi,j 1≤i,j≤N . Suppose there is a relation with type r pointing from mention xib , .., xie − RE = → to mention xj b , .., xj e , we have yi,j r and ← − RE b e b e yj,i = r for all i ∈ [i , i ] ∧ j ∈ [j , j ]. We use ⊥ for word pairs with no relation. An example was given earlier in Figure 1. 4 Model We describe the model in this section. The model consists of two types"
2020.emnlp-main.133,D12-1110,0,0.0487412,"d interaction information carried in the attention weights from BERT, which further improves the performance. Our proposed method achieves the state-of-theart performance on four datasets, namely ACE04, ACE05, CoNLL04, and ADE. We also conduct further experiments to confirm the effectiveness of our proposed approach. 2 Related Work NER and RE can be tackled by using separate models. By assuming gold entity mentions are given as inputs, RE can be regarded as a classification task. Such models include kernel methods (Zelenko et al., 2002), RNNs (Zhang and Wang, 2015), recursive neural networks (Socher et al., 2012), CNNs (Zeng et al., 2014), and Transformer models (Verga et al., 2018; Wang et al., 2019). Another branch is to detect cross-sentence level relations (Peng et al., 2017; Gupta et al., 2019), and even document-level relations (Yao et al., 2019; Nan et al., 2020). However, entities are usually not directly available in practice, so these approaches may require an additional entity recognizer to form a pipeline. Joint learning has been shown effective since it can alleviate the error propagation issue and benefit from exploiting the interrelation between NER and RE. Many studies address the join"
2020.emnlp-main.133,P11-1053,0,0.226239,", our model shows significant improvements over existing approaches.1 1 Edward Thomas is from Minnesota , United States B-PER ⊥ ⊥ ⊥ live_in ⊥ live_in live_in ⊥ ⊥ live_in ⊥ live_in live_in Thomas ⊥ I-PER is ⊥ ⊥ O ⊥ ⊥ ⊥ ⊥ ⊥ from ⊥ ⊥ ⊥ O ⊥ ⊥ ⊥ ⊥ Minnesota live_in live_in ⊥ ⊥ B-LOC ⊥ loc_in loc_in , ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ United live_in live_in ⊥ ⊥ loc_in ⊥ B-LOC ⊥ States live_in live_in ⊥ ⊥ loc_in ⊥ ⊥ I-LOC O Figure 1: An example of table filling for NER and RE. Introduction Named Entity Recognition (NER, Florian et al. 2006, 2010) and Relation Extraction (RE, Zhao and Grishman 2005; Jiang and Zhai 2007; Sun et al. 2011; Plank and Moschitti 2013) are two fundamental tasks in Information Extraction (IE). Both tasks aim to extract structured information from unstructured texts. One typical approach is to first identify entity mentions, and next perform classification between every two mentions to extract relations, forming a pipeline (Zelenko et al., 2002; Chan and Roth, 2011). An alternative and more recent approach is to perform these two tasks jointly (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016), which mitigates the error propagation issue associated with the pipeline ap1 Our code is avai"
2020.emnlp-main.133,D18-1249,0,0.104656,"Missing"
2020.emnlp-main.169,W13-2322,0,0.159262,": The concept (join-01) in vanilla GCNs is that it only captures information from its immediate neighbors (first-order), while in LDGCNs it can integrate information from neighbors of different order (e.g., second-order and third-order). In SANs, the node collects information from all other nodes, while in structured SANs it is aware of its connected nodes in the original graph. Introduction Graph structures play a pivotal role in NLP because they are able to capture particularly rich structural information. For example, Figure 1 shows a directed, labeled Abstract Meaning Representation (AMR; Banarescu et al. 2013) graph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within ∗ Equally Contributed. Work done while Yan Zhang was an intern at DAMO Academy, Alibaba Group and Zhijiang Guo was at the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016"
2020.emnlp-main.169,P18-1026,0,0.626864,"the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlo"
2020.emnlp-main.169,D19-1378,0,0.0186021,"l., 2018; Li et al., 2019a). More recently, SANbased models (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model. Related Work Graph convolutional networks (Kipf and Welling, 2017) have been widely used as the structural encoder in various NLP applications including question answering (De Cao et al., 2019; Lin et al., 2019), semantic parsing (Bogin et al., 2019a,b) and relation extraction (Guo et al., 2019a, 2020). Conclusion In this paper, we propose LDGCNs for AMR-totext generation. Compared with existing GCNs and SANs, LDGCNs maintain a better balance between parameter efficiency and model capacity. LDGCNs outperform state-of-the-art models on AMR-to-text generation. In future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in Natural Language Generation. Acknowledgments We would like to thank the anonymous reviewers for their constructive comments. We would also like t"
2020.emnlp-main.169,P19-1448,0,0.0224765,"l., 2018; Li et al., 2019a). More recently, SANbased models (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model. Related Work Graph convolutional networks (Kipf and Welling, 2017) have been widely used as the structural encoder in various NLP applications including question answering (De Cao et al., 2019; Lin et al., 2019), semantic parsing (Bogin et al., 2019a,b) and relation extraction (Guo et al., 2019a, 2020). Conclusion In this paper, we propose LDGCNs for AMR-totext generation. Compared with existing GCNs and SANs, LDGCNs maintain a better balance between parameter efficiency and model capacity. LDGCNs outperform state-of-the-art models on AMR-to-text generation. In future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in Natural Language Generation. Acknowledgments We would like to thank the anonymous reviewers for their constructive comments. We would also like t"
2020.emnlp-main.169,2020.acl-main.640,0,0.307027,"aph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswani et al. 2017) have been explored as an alternative to capture"
2020.emnlp-main.169,N19-1223,0,0.143753,"al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT Dual (Wang et al., 2020) GT GRU (Cai and Lam, 2020) GT SAN (Zhu et al., 2019) Single Single Single Single LDGCN WT LDGCN GC Single Single C M AMR2017 #P 23.5 23.3 24.4 23.6 25.7 0 54.5‡ 031.5‡ 018.6M‡ 24.3 053.8‡ 30.5 060.3M‡ - - B M #P 26.8 24.9 23.3 50.4 28.3M 2"
2020.emnlp-main.169,W14-3348,0,0.00949318,"and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT Dual (Wang et al., 2020) GT GRU (Cai and Lam, 2020) GT SAN (Zhu et al., 2019)"
2020.emnlp-main.169,E17-3017,0,0.0226855,"max( (hu WQ )(hv WK )T √ ) d (2) where WQ and WK are projection parameters. The adjacency matrix A in GCNs is given by the input AMR graph, while in SANs A is computed based on H, which neglects the structural information of the input AMR. The number of operations required by graph convolutions scales is found linearly in the input length, whereas they are quadratic for SANs. Structured SANs Zhu et al. (2019) and Cai and Lam (2020) extend SAN s by incorporating the relation ruv between node u and node v in the 1 Our implementation is based on MXNET (Chen et al., 2015) and the Sockeye toolkit (Felix et al., 2017). 2163 Hl A1 X Wl Hl+1 X Vanilla GCN Layer Hl A1 Wl X Hl A2 X X Wl X Dynamic Fusion Hl+1 LDGCN Layer Figure 2: Comparison between vanilla GCNs and LDGCNs. Hl denotes the representation of l-th layer. Wl denotes the trainable weights and × denotes matrix multiplication. Vanilla GCNs take the 1st-order adjacency matrix A1 as the input, which only captures information from one-hop neighbors. LDGCNs take k number of k-order adjacency matrix Ak as inputs, Wl is shared for all Ak . k is set to 2 here for simplification. A dynamic fusion mechanism is applied to integrate the information from 1- to k-"
2020.emnlp-main.169,N16-1087,0,0.0142735,"f the sentence, i.e., “rather than let them get even worse”, but it fails to capture the meaning of word “early” in its output, which is a critical part. DeepGCN parses both “early” and “get even worse” in the results. However, the readability of the generated sentence is not satisfactory. Compared to baselines, LDGCN is able to produce the best result, which has a correct starting phrase and captures the semantic meaning of critical words such as “early” and “get even worse” while also attaining good readability. 6 Early efforts for AMR-to-text generation mainly include grammar-based models (Flanigan et al., 2016; Song et al., 2017) and sequence-based models (Pourdamghani et al., 2016; Konstas et al., 2017; Cao and Clark, 2019), discarding crucial structural information when linearising the input AMR graph. To solve that, various GNNs including graph recurrent networks (Song et al., 2018; Ribeiro et al., 2019) and graph convolutional networks (Damonte and Cohen, 2019; Guo et al., 2019b) have been used to encode the AMR structure. Though GNNs are able to operate directly on graphs, the locality nature of them precludes efficient information propagation (Abu-El-Haija et al., 2018, 2019; Luan et al., 201"
2020.emnlp-main.169,P19-1024,1,0.166026,"information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswa"
2020.emnlp-main.169,Q19-1019,1,0.13775,"information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswa"
2020.emnlp-main.169,N19-1366,1,0.811253,"onding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et"
2020.emnlp-main.169,W04-3250,0,0.196269,"Missing"
2020.emnlp-main.169,2020.findings-emnlp.199,1,0.492865,"erly. As shown in Table 6, LDGCN GC has better human rankings in terms of both meaning similarity and readability than the state-of-the art GNN-based (DualGraph) and SAN-based model (GT SAN). DeepGCN without dynamic fusion mechanism obtains lower scores than GT SAN, which further confirms that synthesizing higher order information helps in learning better graph representations. 5.5 0-20 Additional Analysis To further reveal the source of performance gains, we perform additional analysis based on the characteristics of AMR graphs, i.e., graph size and graph reentrancy (Damonte and Cohen, 2019; Damonte et al., 2020). All experiments are conducted on the AMR2.0 test set and CHRF++ scores are reported. CHRF++ 5.4 58 64 58 0-1 2-3 4-5 Graph Reentrancies &gt;5 Figure 6: Performance against graph re-entrancies. Graph Size. As shown in Figure 5, the size of AMR graphs is partitioned into four categories ((0, 20], (20, 30], (30, 40], &gt; 40), Overall, LDGCN GC outperforms the best-reported GT SAN model across all graph sizes, and the performance gap becomes more profound with the increase of graph sizes. Although both models have sharp performance degradation for extremely large graphs (&gt; 40), the performance of LDG"
2020.emnlp-main.169,P17-1014,0,0.450535,"raph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within ∗ Equally Contributed. Work done while Yan Zhang was an intern at DAMO Academy, Alibaba Group and Zhijiang Guo was at the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps captur"
2020.emnlp-main.169,N19-1240,0,0.0608384,"Missing"
2020.emnlp-main.169,D19-1282,0,0.0218573,"mplex non-local interactions (Xu et al., 2018; Li et al., 2019a). More recently, SANbased models (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model. Related Work Graph convolutional networks (Kipf and Welling, 2017) have been widely used as the structural encoder in various NLP applications including question answering (De Cao et al., 2019; Lin et al., 2019), semantic parsing (Bogin et al., 2019a,b) and relation extraction (Guo et al., 2019a, 2020). Conclusion In this paper, we propose LDGCNs for AMR-totext generation. Compared with existing GCNs and SANs, LDGCNs maintain a better balance between parameter efficiency and model capacity. LDGCNs outperform state-of-the-art models on AMR-to-text generation. In future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in Natural Language Generation. Acknowledgments We would like to thank the anonymous reviewers for their const"
2020.emnlp-main.169,D19-1548,0,0.0984317,"g it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswani et al. 2017) have been explored as an alt"
2020.emnlp-main.169,P02-1040,0,0.106582,"dynamic fusion mechanism, N =2 for depthwise graph convolutions and M =6 and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT"
2020.emnlp-main.169,W17-4770,0,0.024584,"for depthwise graph convolutions and M =6 and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT Dual (Wang et al., 2020)"
2020.emnlp-main.169,W16-6603,0,0.247764,"R; Banarescu et al. 2013) graph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within ∗ Equally Contributed. Work done while Yan Zhang was an intern at DAMO Academy, Alibaba Group and Zhijiang Guo was at the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional laye"
2020.emnlp-main.169,D19-1314,0,0.452439,"of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings o"
2020.emnlp-main.169,P16-1162,0,0.0167726,"p convolutional neural networks. Formally, the Hf inal of LDGCNs which have L layer ˆ L , ..., H ˆ 1 ), where is obtained by: Hf inal = F(H F is a linear transformation. 5 5.1 Experiments Setup We evaluate our model on the LDC2015E86 (AMR1.0), LDC2017T10 (AMR2.0) and LDC2020T02 (AMR3.0) datasets, which have 16,833, 36,521 and 55,635 instances for training, respectively. Both AMR1.0 and AMR2.0 have 1,368 instances for development, and 1,371 instances for testing. AMR3.0 has 1,722 instances for development and 1,898 instances for testing. Following Zhu et al. (2019), we use byte pair encodings (Sennrich et al., 2016) to deal with rare words. Following Guo et al. (2019b), we stack 4 LDGCN blocks as the encoder of our model. Each block consists of two sub-blocks where the bottom one contains 6 layers and the top one contains 3 layers. The hidden dimension of LDGCN model is 480. Other model hyperparameters are set as λ=0.7, K=2 for dynamic fusion mechanism, N =2 for depthwise graph convolutions and M =6 and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damo"
2020.emnlp-main.169,P17-2002,0,0.0176185,"rather than let them get even worse”, but it fails to capture the meaning of word “early” in its output, which is a critical part. DeepGCN parses both “early” and “get even worse” in the results. However, the readability of the generated sentence is not satisfactory. Compared to baselines, LDGCN is able to produce the best result, which has a correct starting phrase and captures the semantic meaning of critical words such as “early” and “get even worse” while also attaining good readability. 6 Early efforts for AMR-to-text generation mainly include grammar-based models (Flanigan et al., 2016; Song et al., 2017) and sequence-based models (Pourdamghani et al., 2016; Konstas et al., 2017; Cao and Clark, 2019), discarding crucial structural information when linearising the input AMR graph. To solve that, various GNNs including graph recurrent networks (Song et al., 2018; Ribeiro et al., 2019) and graph convolutional networks (Damonte and Cohen, 2019; Guo et al., 2019b) have been used to encode the AMR structure. Though GNNs are able to operate directly on graphs, the locality nature of them precludes efficient information propagation (Abu-El-Haija et al., 2018, 2019; Luan et al., 2019). Larger and deepe"
2020.emnlp-main.169,P18-1150,0,0.565052,"dinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information pro"
2020.emnlp-main.169,2020.tacl-1.2,0,0.411099,"(GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswani et al. 2017) have been explored as an alternative to capture global dependencies"
2020.emnlp-main.183,D17-1047,1,0.898258,"ets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by app"
2020.emnlp-main.183,S14-2051,0,0.0261129,"worth highlighting that the ensemble models have significant improvements in terms of recall score. Note that the recall score reflects the number of gold triplets extracted. Such improvement confirms our earlier hypothesis that the two models largely complement each other. 5 Related Work ASTE is highly related to another research topic – Aspect Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2016). Such a research topic focuses on identifying aspect categories, recognizing aspect targets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al.,"
2020.emnlp-main.183,P19-1520,0,0.391762,"Representation (BERT) JETt (M = 6)+ BERT 56.00 63.44 54.12 JETo (M = 6)+ BERT 56.89 70.56 55.94 Table 2: Main results on our refined dataset ASTE-Data-V2. The underlined scores indicate the best results on the dev set, and the highlighted scores are the corresponding test results. The experimental results on the previous released dataset ASTE-Data-V1 can be found in the supplementary materials. 3.2 Baselines the same method to obtain all the valid triplets as RINANTE+. Our JET approaches are compared with the following baselines using pipeline. • RINANTE+ (Peng et al., 2019) modifies RINANTE (Dai and Song, 2019) which is designed based on LSTM-CRF (Lample et al., 2016), to co-extract targets with sentiment, and opinion spans. Such an approach also fuses mined rules as weak supervision to capture dependency relations of words in a sentence at the first stage. At the second stage, it generates all the possible triplets and applies a classifier based on MLP on such triplets to determine if each triplet is valid or not. • CMLA+ (Peng et al., 2019) modifies CMLA (Wang et al., 2017) which leverages attention mechanism to capture dependencies among words, to co-extract targets with sentiment, and opinion sp"
2020.emnlp-main.183,N19-1423,0,0.0322748,"ivated by Li-unified-R to co-extract targets with sentiment, and opinion spans simultaneously. Such an approach also fuses GCN to capture dependency information to facilitate the co-extraction. At the second stage, it uses 3.3 Experimental Setup Following the previous work (Peng et al., 2019), we use pre-trained 300d GloVe (Pennington et al., 2014) to initialize the word embeddings. We use 100 as the embedding size of wr (offset embedding). We use the bi-directional LSTM with the hidden size 300. For experiments with contextualised representation, we adopt the pre-trained language model BERT (Devlin et al., 2019). Specifically, we use bert-as-service (Xiao, 2018) to generate the contextualized word embedding without fine-tuning. We use the representation from the last layer of the uncased version of BERT base model for our experiments. Before training, we discard any instance from the training data that contains triplets with offset larger than M . We train our model for a maximal of 20 epochs using Adam (Kingma and Ba, 2014) as the optimizer with batch size 1 and dropout rate 0.513 . We select the best model parameters based on the best F1 score on the development data and apply it to the test data f"
2020.emnlp-main.183,P18-1087,1,0.847518,"recall score. Note that the recall score reflects the number of gold triplets extracted. Such improvement confirms our earlier hypothesis that the two models largely complement each other. 5 Related Work ASTE is highly related to another research topic – Aspect Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2016). Such a research topic focuses on identifying aspect categories, recognizing aspect targets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each ta"
2020.emnlp-main.183,E17-2091,0,0.226331,"CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propose a novel position-aware tagging scheme by enriching label expressiveness to address a limitation associated with exis"
2020.emnlp-main.183,P14-2009,0,0.0446052,"gnizing aspect targets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labe"
2020.emnlp-main.183,P13-1172,0,0.0904361,"20) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propose a novel position-aware tagging scheme by enriching label expressiveness to address a limitation associated with existing works. Such a tagging scheme is able to specify the connection among three elements – a target, the target sentiment as well as an opinion span in an aspect sentiment triplet for the ASTE task. Based on the position-aware tagging scheme, we propose a novel approach JET that is capable of jointly extracting the aspe"
2020.emnlp-main.183,W06-1643,0,0.0587744,"n span as well as the rich interactions among the three elements due to the limited expressiveness. Specifically, BIOES uses the tag B or S to represent the beginning of a target. For example, in the example sentence in Figure 1, “vegan” should be labeled with B, but the tagging scheme does not contain any information to specify the position of its corresponding opinion “excited”. Using such a tagging scheme inevitably leads to an additional step to connect each target with an opinion span as the second stage in the pipeline approach. The skip-chain sequence models (Sutton and McCallum, 2004; Galley, 2006) are able to capture interactions between given input tokens which can be far away from each other. However, they are not suitable for the ASTE task where the positions of targets and opinion spans are not explicitly provided but need to be learned. Motivated by the above observations, we present a novel approach that is capable of predicting the triplets jointly for ASTE. Specifically, we make the following contributions in this work: • We present a novel position-aware tagging scheme that is capable of specifying the structural information for a triplet – the connection among the three eleme"
2020.emnlp-main.183,P14-1030,0,0.0605334,"Missing"
2020.emnlp-main.183,D15-1168,0,0.129729,"Missing"
2020.emnlp-main.183,D18-1504,0,0.0346965,"targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In thi"
2020.emnlp-main.183,P19-1344,0,0.451551,"that the recall score reflects the number of gold triplets extracted. Such improvement confirms our earlier hypothesis that the two models largely complement each other. 5 Related Work ASTE is highly related to another research topic – Aspect Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2016). Such a research topic focuses on identifying aspect categories, recognizing aspect targets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et a"
2020.emnlp-main.183,W04-3250,0,0.0820311,"Missing"
2020.emnlp-main.183,D13-1171,0,0.174703,"Missing"
2020.emnlp-main.183,N16-1030,0,0.774842,"f restaurant domain and 14Lap is of laptop domain. Such datasets were all created based on the datasets originally released by SemEval (Pontiki et al., 2014, 2015, 2016). 8 (x,y)∈D See the supplementary materials for detailed algorithm. https://github.com/xuuuluuu/ SemEval-Triplet-data 10 We also report the results on ASTE-Data-V1 in the supplementary material. 11 We also remove triplets with sentiment originally labeled as “conflict” by SemEval. 12 See the supplementary material for more statistics. 9 The overall model is analogous to that of a neural CRF (Peng et al., 2009; Do et al., 2010; Lample et al., 2016); hence the inference and decod7 We use min (j, k) since we care the offset between the starting positions of an opinion span and a target. Data 2343 Models Dev F1 CMLA+ RINANTE+ Li-unified-R Peng et al. (2019) 14Rest P. R. F1 Dev F1 14Lap P. R. F1 Dev F1 15Rest P. R. F1 Dev F1 16Rest P. R. F1 - 39.18 31.42 41.04 43.24 47.13 39.38 67.35 63.66 42.79 34.95 51.00 51.46 - 30.09 21.71 40.56 37.38 36.92 18.66 44.28 50.38 33.16 20.07 42.34 42.87 - 34.56 29.88 44.72 48.07 39.84 30.06 51.39 57.51 37.01 29.97 47.82 52.32 - 41.34 25.68 37.33 46.96 42.10 22.30 54.51 64.24 41.72 23.87 44.31 54.21 JETt (M J"
2020.emnlp-main.183,D19-1550,1,0.830666,"either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propos"
2020.emnlp-main.183,D14-1162,0,0.0843256,"Missing"
2020.emnlp-main.183,S16-1002,0,0.255302,"Missing"
2020.emnlp-main.183,S15-2082,0,0.585406,"Missing"
2020.emnlp-main.183,S14-2004,0,0.901694,"e the model effectiveness and robustness1 . 1 Figure 1: ASTE with targets in bold in solid squares, their associated sentiment on top, and opinion spans in dashed boxes. The arc indicates connection between a target and the corresponding opinion span. Introduction Designing effective algorithms that are capable of automatically performing sentiment analysis and opinion mining is a challenging and important task in the field of natural language processing (Pang and Lee, 2008; Liu, 2010; Ortigosa et al., 2014; Smailovi´c et al., 2013; Li and Wu, 2010). Recently, Aspect-based Sentiment Analysis (Pontiki et al., 2014) or Targeted Sentiment Analysis (Mitchell et al., 2013) which focuses on extracting target ∗ Equal contribution. Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. The work was done when Hao Li was a PhD student in Singapore University of Technology and Design. 1 We release our code at https://github.com/ xuuuluuu/Position-Aware-Tagging-for-ASTE phrases as well as the sentiment associated with each target, has been receiving much attention. In this work, we focus on a relatively new task – Aspect Sentiment Triplet Extraction (ASTE) proposed"
2020.emnlp-main.183,J11-1002,0,0.69422,"d above, but also the corresponding opinion spans expressing the sentiment for each target. Such three elements: a target, its sentiment and the corresponding opinion span, form a triplet to be extracted. Figure 1 presents an example sentence containing two targets in solid boxes. Each target is associated with a sentiment, where we use + to denote the positive polarity, 0 for neutral, and − for negative. Two opinion spans in dashed boxes are connected to their targets by arcs. Such opinion spans are important, since they largely explain the sentiment polarities for the corresponding targets (Qiu et al., 2011; Yang and Cardie, 2012). This ASTE problem was basically untouched before, and the only existing work that we are aware of (Peng et al., 2019) employs a 2-stage pipeline approach. At the first stage, they employ a unified tagging scheme which fuses the target tag based on the BIOES 2 tagging scheme, and sentiment tag together. Under such a unified tagging scheme, they proposed methods based on Long Short-Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), Conditional Random 2 BIOES is a common tagging scheme for sequence labeling tasks, and BIOES denotes “begin, inside, outside, e"
2020.emnlp-main.183,S15-2127,0,0.0217349,"he ensemble models have significant improvements in terms of recall score. Note that the recall score reflects the number of gold triplets extracted. Such improvement confirms our earlier hypothesis that the two models largely complement each other. 5 Related Work ASTE is highly related to another research topic – Aspect Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2016). Such a research topic focuses on identifying aspect categories, recognizing aspect targets as well as the associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction"
2020.emnlp-main.183,P16-1218,0,0.0133769,"he offsets. We explain such 3 additional factorized scores appearing in Equation 6. We deploy a simple LSTM-based neural architecture for learning features. Given an input token sequence x = {x1 , x2 , · · · , xn } of length n, we first obtain the embedding sequence {e1 , e2 , · · · , en }. As illustrated in Figure 3, we then apply a bidirectional LSTM on the embedding sequence and obtain the hidden state hi for each position i, which could be represented as: → − ← − hi = [hi ; hi ] (3) → − ← − where hi and hi are the hidden states of the forward and backward LSTMs respectively. Motivated by (Wang and Chang, 2016; Stern et al., 2017), we calculate the segment representation ga,b for an opinion span with boundaries of a and b (both inclusive) as follows: → − → − ← − ← − ga,b = [ h b − h a−1 ; h a − h b+1 ] (4) → − ← − where h 0 = 0, h n+1 = 0 and 1 ≤ a ≤ b ≤ n. 2.2.2 Factorized Feature Score We explain how to compute the factorized feature scores (the second part of Equation 2) for the position-aware tagging scheme based on the neural architecture described above. Such factorized feature scores involve 4 types of scores, as illustrated in the solid boxes appearing in Figure 3 (top). Basically, we calcu"
2020.emnlp-main.183,P18-2094,0,0.273339,"timent associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propose a novel position-aware tagging scheme by enriching label expressiveness to address a limitation associated with existing works. Such a tagging scheme is able to specify the connection among three elements – a target, the target sentiment as well as an opinion span in an aspect sentiment triplet for the ASTE task. Based on the position-aware tagging scheme, we propose a novel approach JET that is capable of jointly extracting the aspect sentiment triplets. We also design factorized"
2020.emnlp-main.183,2020.emnlp-main.288,1,0.842257,"vich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu"
2020.emnlp-main.183,P18-1234,0,0.140588,"associated sentiment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based ap"
2020.emnlp-main.183,D12-1122,0,0.0718184,"the corresponding opinion spans expressing the sentiment for each target. Such three elements: a target, its sentiment and the corresponding opinion span, form a triplet to be extracted. Figure 1 presents an example sentence containing two targets in solid boxes. Each target is associated with a sentiment, where we use + to denote the positive polarity, 0 for neutral, and − for negative. Two opinion spans in dashed boxes are connected to their targets by arcs. Such opinion spans are important, since they largely explain the sentiment polarities for the corresponding targets (Qiu et al., 2011; Yang and Cardie, 2012). This ASTE problem was basically untouched before, and the only existing work that we are aware of (Peng et al., 2019) employs a 2-stage pipeline approach. At the first stage, they employ a unified tagging scheme which fuses the target tag based on the BIOES 2 tagging scheme, and sentiment tag together. Under such a unified tagging scheme, they proposed methods based on Long Short-Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), Conditional Random 2 BIOES is a common tagging scheme for sequence labeling tasks, and BIOES denotes “begin, inside, outside, end and single” respectiv"
2020.emnlp-main.183,D15-1073,0,0.0941732,"that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence lab"
2020.emnlp-main.183,P17-1076,0,0.025391,"such 3 additional factorized scores appearing in Equation 6. We deploy a simple LSTM-based neural architecture for learning features. Given an input token sequence x = {x1 , x2 , · · · , xn } of length n, we first obtain the embedding sequence {e1 , e2 , · · · , en }. As illustrated in Figure 3, we then apply a bidirectional LSTM on the embedding sequence and obtain the hidden state hi for each position i, which could be represented as: → − ← − hi = [hi ; hi ] (3) → − ← − where hi and hi are the hidden states of the forward and backward LSTMs respectively. Motivated by (Wang and Chang, 2016; Stern et al., 2017), we calculate the segment representation ga,b for an opinion span with boundaries of a and b (both inclusive) as follows: → − → − ← − ← − ga,b = [ h b − h a−1 ; h a − h b+1 ] (4) → − ← − where h 0 = 0, h n+1 = 0 and 1 ≤ a ≤ b ≤ n. 2.2.2 Factorized Feature Score We explain how to compute the factorized feature scores (the second part of Equation 2) for the position-aware tagging scheme based on the neural architecture described above. Such factorized feature scores involve 4 types of scores, as illustrated in the solid boxes appearing in Figure 3 (top). Basically, we calculate the factorized f"
2020.emnlp-main.183,D16-1021,0,0.0874345,"ysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – target and opinion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propose a novel position-aware tagging scheme by enriching label expressiveness to address a limitation associated with existing works. Such a tagging scheme is ab"
2020.emnlp-main.183,P18-1088,0,0.196648,"ew tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vicente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given target (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classification. The latter mostly regards the task as a sequence labeling problem by applying CRF-based approaches. Another related task – targe"
2020.emnlp-main.288,D17-1047,1,0.961554,"ied based on the extracted opinion features and contextual information. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/"
2020.emnlp-main.288,W14-4012,0,0.0629142,"Missing"
2020.emnlp-main.288,P19-1520,0,0.0555027,"shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” Thi"
2020.emnlp-main.288,S14-2076,0,0.186602,"We set the hidden size of GRU to 32 or 64. The batch size is set to 64 or 96. The dropout rate is selected from 0.3 to 0.8, with a step size of 0.1. The dimension of the aspect indicator is selected from {50, 70, 90}. The value of γ in the position decay function is selected from {1,2,3}. The number of layer of GRU is selected from {1,2,3}. We adopt Adam (Kingma and Ba, 2014) to optimize our model with a learning rate of 0.008. All hyper-parameters are selected based on the best performance on the development set. 3.2 Baselines Our MCRF-SA model is compared with the following methods2 . SVM (Kiritchenko et al., 2014) is a support vector machine based method that integrates surface, lexicon, and parse features. ATAELSTM (Wang et al., 2016) is an LSTM (Hochreiter and Schmidhuber, 1997) based model, which has an extra attention to perform soft-selection over the context words. MemNet (Tang et al., 2016) introduces a deep memory network to implement attention mechanisms to learn the relatedness of context words towards the aspect. IAN (Ma et al., 2017) utilizes two LSTM based attention models to learn both context and aspect representations interactively. SA-LSTM-P (Wang and Lu, 2018) employs structured atten"
2020.emnlp-main.288,N16-1030,0,0.0546551,"pect, L is the maximum length of sentences across all datasets, γ is a hyper-parameter and a larger value enables more influence from the context words that are close to the aspect. Then, the decayed contextual word representation is as follows: rt = f (t) ht (4) 2.4 Multi-CRF Structured Attention We use multiple linear-chain CRFs to intensively incorporate structure dependencies to capture the corresponding opinion spans of an aspect. In particular, we create a latent label (Wang and Lu, 2018) z ∈ {Y es, N o} to indicate whether each context word belongs to part of opinion spans. Similar to (Lample et al., 2016), given the sentence representation x, the CRF is defined as: 3562 P (z|x) = P exp(score(z, x)) 0 z0 exp(score(z , x)) (5) where score(z, x) is a score function that is defined as the summation of transition scores and emission scores from the Bi-GRU: n n X X score(z, x) = Tzt ,zt+1 + Et,zt (6) t=0 Marginal Inference The latent labels introduced in the CRF layer show whether the word influences the given aspect’s sentiment. Intuitively, we can understand that the marginal probabilities on the Y es label indicate the influence of the current context word on the aspect word’s sentiment. By using"
2020.emnlp-main.288,D19-1550,1,0.70703,"a and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) engage structured attention networks (Kim et al., 2017), which extend the"
2020.emnlp-main.288,P18-1087,1,0.827072,"ion. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider th"
2020.emnlp-main.288,P17-1036,0,0.0264457,"tal results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “"
2020.emnlp-main.288,C18-1096,0,0.0811402,"am between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) engage structured attention networks (Kim et al., 2017),"
2020.emnlp-main.288,D19-1466,1,0.829529,"ans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables,"
2020.emnlp-main.288,P19-1048,0,0.0158908,"on Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the o"
2020.emnlp-main.288,K19-1091,0,0.0151803,"niversity of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) engage structured attention networks (Kim et al., 2017), which extend the previous attentio"
2020.emnlp-main.288,E17-2091,0,0.0842539,"/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) en"
2020.emnlp-main.288,P13-1172,0,0.159276,"veness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I wor"
2020.emnlp-main.288,P18-2094,0,0.0301115,"further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables"
2020.emnlp-main.288,2020.emnlp-main.183,1,0.842257,"sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of"
2020.emnlp-main.288,D19-1464,0,0.313505,"lized. The input representation xt is as follows: xt = [wtword ; wtas ] (1) 2.2 Aspect-Specific Contextualized Representation We employ a bi-directional GRU (Cho et al., 2014) to generate the contextualized representation. Since the input representation has already contained the aspect information, the aspect-specific contextualized representation is obtained by concatenating the hidden states from both directions: → − ← − ht = [ht ; ht ] (2) → − where ht is the hidden state from the forward GRU ← − and ht is from the backward. 2.3 Position Decay Following the previous work (Li et al., 2018a; Zhang et al., 2019; Tang et al., 2019), we also use a position decay function to reduce the influence of Figure 1: MCRF-SA Architecture. the context words on the aspect as it goes further away from the aspect. We propose a higher-order decay function, which is more sensitive to distance, and the sensitivity can be tuned by γ on different datasets.  L−i+t γ  t&lt;i ( L ) f (t) = 1 (3) i≤t≤j   L−t+j γ ( L ) j&lt;t where i and j are the starting and ending position of an aspect, L is the maximum length of sentences across all datasets, γ is a hyper-parameter and a larger value enables more influence from the contex"
2020.emnlp-main.288,D14-1162,0,0.0869837,"r, which takes rt as input and returns a vector whose length is label size. 2.4.1 Dataset Experiments Experimental Setup Our proposed MCRF-SA model is evaluated on four benchmark datasets: SemEval 2014 Task4 (Pontiki et al., 2014), SemEval 2015 Task12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016). Following the previous works (Tang et al., 2016; Chen et al., 2017; Wang and Lu, 2018; Table 1: Statistics of datasets. He et al., 2018), we remove a few examples that have conflicting labels. Detailed statistics of the datasets can be found in Table 1. We use the 300d GloVe (Pennington et al., 2014) to initialize our word embeddings. One-sixth of instances are randomly selected from the original training dataset as the development dataset, and the model is only trained with the remaining data. With the development set, we tune our model hyperparameters using an open-source black-box tuner (Alberto and Giacomo, 2018). We set the hidden size of GRU to 32 or 64. The batch size is set to 64 or 96. The dropout rate is selected from 0.3 to 0.8, with a step size of 0.1. The dimension of the aspect indicator is selected from {50, 70, 90}. The value of γ in the position decay function is selected"
2020.emnlp-main.288,S15-2082,0,0.314936,"Missing"
2020.emnlp-main.288,S14-2004,0,0.101049,". #Neg. #Pos. #Neu. #Neg. #Pos. #Neu. #Neg. 1796 539 666 368 94 139 728 196 196 824 383 717 161 72 149 340 167 128 808 29 228 147 5 44 340 28 195 1106 54 406 191 9 60 474 29 127 t=1 where T is a transition matrix and Tzt ,zt+1 denotes the transition score from label zt to zt+1 . Et,zt denotes the emission score of label zt at the t-th position, and the score is obtained from a linear layer, which takes rt as input and returns a vector whose length is label size. 2.4.1 Dataset Experiments Experimental Setup Our proposed MCRF-SA model is evaluated on four benchmark datasets: SemEval 2014 Task4 (Pontiki et al., 2014), SemEval 2015 Task12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016). Following the previous works (Tang et al., 2016; Chen et al., 2017; Wang and Lu, 2018; Table 1: Statistics of datasets. He et al., 2018), we remove a few examples that have conflicting labels. Detailed statistics of the datasets can be found in Table 1. We use the 300d GloVe (Pennington et al., 2014) to initialize our word embeddings. One-sixth of instances are randomly selected from the original training dataset as the development dataset, and the model is only trained with the remaining data. With th"
2020.emnlp-main.288,D16-1021,0,0.584365,"5 1106 54 406 191 9 60 474 29 127 t=1 where T is a transition matrix and Tzt ,zt+1 denotes the transition score from label zt to zt+1 . Et,zt denotes the emission score of label zt at the t-th position, and the score is obtained from a linear layer, which takes rt as input and returns a vector whose length is label size. 2.4.1 Dataset Experiments Experimental Setup Our proposed MCRF-SA model is evaluated on four benchmark datasets: SemEval 2014 Task4 (Pontiki et al., 2014), SemEval 2015 Task12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016). Following the previous works (Tang et al., 2016; Chen et al., 2017; Wang and Lu, 2018; Table 1: Statistics of datasets. He et al., 2018), we remove a few examples that have conflicting labels. Detailed statistics of the datasets can be found in Table 1. We use the 300d GloVe (Pennington et al., 2014) to initialize our word embeddings. One-sixth of instances are randomly selected from the original training dataset as the development dataset, and the model is only trained with the remaining data. With the development set, we tune our model hyperparameters using an open-source black-box tuner (Alberto and Giacomo, 2018). We set the hidden siz"
2020.emnlp-main.288,P19-1053,0,0.124683,"Missing"
2020.emnlp-main.288,D16-1058,0,0.484669,"get is then classified based on the extracted opinion features and contextual information. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at h"
2020.emnlp-main.297,N18-1150,0,0.0196461,"d tuned the minimum summary length on the validation set in the range of [30, 80]. The search range of minimum summary length was empirically set according to the summaries of training split of CNNDM, where the average and medium minimum lengths are both around 55. We used step size of 5 to get quick feedback. Similar 3651 to the pre-training process, the datasets with less instances were fine-tuned with smaller batch sizes (i.e., 64 for NYT and 768 for CNNDM). 5 Model Lead3 BERTExt (Liu and Lapata, 2019) PTGen (See et al., 2017) DRM (Paulus et al., 2018) BottomUp (Gehrmann et al., 2018) DCA (Celikyilmaz et al., 2018) BERTAbs (Liu and Lapata, 2019) UniLM (Dong et al., 2019) T RANSFORMER-S2S RO BERTABASE -S2S RO BERTA-S2S RO BERTACONT -S2S Automatic Evaluation We used ROUGE (Lin, 2004) to measure the quality of different summarization model outputs. We reported full-length F1 based ROUGE1, ROUGE-2 and ROUGE-L scores on CNNDM, while we used the limited-length recall based ROUGE-1, ROUGE-2 and ROUGEL on NYT, following Durrett et al. (2016). The ROUGE scores are computed using the ROUGE-1.5.5.pl script6 . Models in Comparison Lead3 is a baseline which simply takes the first three sentences of a document as its"
2020.emnlp-main.297,P18-1063,0,0.0208635,"rage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement learning (Chen and Bansal, 2018), while our model is simpler. Pre-training Pre-training methods draw a lot of attentions recently. Peters et al. (2018) and Radford et al. (2019) pre-trained LSTM and Transformer using language modeling objectives. To leverage the context in both directions, BERT (Devlin et al., 2019) is trained with the masked language modeling and next sentence prediction objectives. SpanBERT (Joshi et al., 2020) applied only the masked language modeling objective that masks contiguous random spans, rather than random tokens. XLNet (Yang et al., 2019) proposed a permutation language modeling objective that r"
2020.emnlp-main.297,D18-1443,0,0.178386,") that have been extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement learning (Chen and Bansal, 2018), while our model is simpler. Pre-training Pre-training methods draw a lot of attentions recently. Peters et al. (2018) and Radford et al. (2019) pre-trained LSTM and Transformer using language modeling objectives. To leverage the context in both directions, BERT (Devlin et al., 2019) is trained with the masked language modeling and next sentence prediction objectives. SpanBERT (Joshi et al., 2020) applied only the masked language modeling objective that masks contiguous random spans, rather than random tokens. XLNet (Yang e"
2020.emnlp-main.297,P16-1154,0,0.0129739,") that lead to another huge performance gain. However, extractive models have their own limitations. For example, the extracted sentences might be too long and redundant. Besides, manually written summaries in their nature are abstractive. Therefore, we focus on abstractive summarization in this paper. Abstractive Summarization This task aims to generate a summary by rewriting a document, which is a SEQ 2 SEQ learning problem. SEQ 2 SEQ attentive LSTMs (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015) are employed in Nallapati et al. (2016) that have been extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement lear"
2020.emnlp-main.297,P18-1013,0,0.0277906,"ed with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement learning (Chen and Bansal, 2018), while our model is simpler. Pre-training Pre-training methods draw a lot of attentions recently. Peters et al. (2018) and Radford et al. (2019) pre-trained LSTM and Transformer using language modeling objectives. To leverage the context in both directions, BERT (Devlin et al., 2019) is trained with the masked language modeling and next sentence prediction objectives. SpanBERT (Joshi et al., 2020) applied only the masked language modeling objective that masks contiguous random spans, rather than random tokens. XLNet (Yang et al., 2019) propos"
2020.emnlp-main.297,N19-4009,0,0.0673284,"Missing"
2020.emnlp-main.564,D14-1179,0,0.0128654,"Missing"
2020.emnlp-main.564,N19-1423,0,0.038154,"tion, we use M LP to mean multilayer perceptron, ⊕ to represent the concatenation operation, and bold symbols to denote dense representations. Base Model We now detail the base model which consists of 1 an encoder and a decoder. The encoder (part in Figure 2) processes the input (i.e., Q and E) into hidden representation (denoted as h) and the 2 in Figure 2) generates the SQL decoder (part query (i.e, S) accordingly. Encoder: Following (Hwang et al., 2019; Guo et al., 2019; Zhang et al., 2019), we concatenate the input query Q and database schema E to an integrated sequence as input for BERT (Devlin et al., 2019) to generate embeddings for each question token and element in the schema (namely Q = {qi }|Q| i=1 and E = {ei }|E| i=1 ) and the overall representation for the input as h. Here, E consists of embeddings of all the columns/tables and the special token [none]. The embedding of the special token [CLS] in BERT is taken as h. Formally, we have: |Q| |E| {[CLS], Q, E} → h, {qi }i=1 , {ei }i=1 . (1) Note that, in this representation, the schema linking information has also been captured by the multilayer self-attention implicitly. However, we argue the explicit supervisions are required. While a plau"
2020.emnlp-main.564,P16-1004,0,0.0162855,"es and the special token [none]. The embedding of the special token [CLS] in BERT is taken as h. Formally, we have: |Q| |E| {[CLS], Q, E} → h, {qi }i=1 , {ei }i=1 . (1) Note that, in this representation, the schema linking information has also been captured by the multilayer self-attention implicitly. However, we argue the explicit supervisions are required. While a plausible solution is to use the relation-aware encoding proposed by Wang et al. (2020) to do this, we later propose a simpler solution to facilitate our analytical study. Decoder: Inspired by the prior work (Yin and Neubig, 2017; Dong and Lapata, 2016, 2018; Zhang et al., 2019), we adopt a two-step decoder to generate the SQL query from the hidden representation h. We first generate a coarse SQL query S 0 , namely a SQL sequence without aggregate functions, using a GRU network (Cho et al., 2014). We then synthesize the final SQL query S based on S 0 . 2 part in Figure 2 illustrates the generation The of aggregate functions for the column budget during the decoding process. 4.2 Schema Linking Extension To study the role of schema linking, we extend the encoder to explicitly capture the schema linking information. It works in two steps: in s"
2020.emnlp-main.564,P18-1068,0,0.0255061,"research on the areas of problem identification, dataset construction and model evaluation. 2 Related Work Text-to-SQL Parsing: Text-to-SQL parsing has been long studied in past decades (Finegan-Dollak et al., 2018; Yu et al., 2018c). Early text-to-SQL systems rely heavily on complicated rules and handcrafted feature engineering (Zhong et al., 2017; Finegan-Dollak et al., 2018). Fortunately, the research progress has been largely accelerated in recent years thanks to both large-scale text-to-SQL datasets (Zhong et al., 2017; Yu et al., 2018c) and interests in neural modeling (Xu et al., 2017; Dong and Lapata, 2018; Sun et al., 2018; Yu et al., 2018b; Guo et al., 2019; Wang et al., 2020). With years of studies, current research on this task focuses on addressing cross-domain generalizability and generating complex SQL queries. To improve cross-domain generalizability, advanced representations of the schema and the queries are explored, e.g., graph-based schema representations (Bogin et al., 2019b,a), contextualized question representations (Hwang et al., 2019; Guo et al., 2019) and relation-aware self-attention (Wang et al., 2020). As for the complex SQL query generation, approaches are proposed to cons"
2020.emnlp-main.564,2020.emnlp-main.521,0,0.0284301,"proposed to constrain the output with SQL grammar, e.g., modular decoders for separate SQL clauses (Yu et al., 2018b), intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pr"
2020.emnlp-main.564,P19-1444,0,0.0722991,"linking for future developments of text-to-SQL tasks.1 1 Introduction Structured Query Language (SQL), while exact and powerful, suffers from a complex grammar presenting significant challenges for laymen to write queries. Automatically parsing natural language into SQL (text-to-SQL) thus has huge potential, as it would enable lay users to mine the world’s structured data using natural language queries. To achieve practical text-to-SQL workflow, a model needs to correlate natural language queries with the given database. Therefore, schema linking is considered helpful for text-to-SQL parsing (Guo et al., 2019; Bogin et al., 2019b; Dong et al., 2019; Wang et al., 2020). Here, schema linking means identifying references of columns, tables and condition values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the"
2020.emnlp-main.564,D18-1190,0,0.0119239,"e idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string matching between natural language utterances and column/table names (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019). As discussed in Dong et al. (2019), such simple heuristics are difficult to accurately identify columns/tables involved in a natural language utterance and well understand the relation between an utterance and the corresponding database schema. Therefore, they make the first step towards treating sch"
2020.emnlp-main.564,D19-1624,0,0.0124777,"plex SQL queries. To improve cross-domain generalizability, advanced representations of the schema and the queries are explored, e.g., graph-based schema representations (Bogin et al., 2019b,a), contextualized question representations (Hwang et al., 2019; Guo et al., 2019) and relation-aware self-attention (Wang et al., 2020). As for the complex SQL query generation, approaches are proposed to constrain the output with SQL grammar, e.g., modular decoders for separate SQL clauses (Yu et al., 2018b), intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annota"
2020.emnlp-main.564,J82-2003,0,0.69635,"Missing"
2020.emnlp-main.564,P19-1335,0,0.0220677,"modular decoders for separate SQL clauses (Yu et al., 2018b), intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string mat"
2020.emnlp-main.564,D15-1166,0,0.059739,"Missing"
2020.emnlp-main.564,P14-5010,0,0.00440625,"Missing"
2020.emnlp-main.564,D18-1299,0,0.0236196,"Missing"
2020.emnlp-main.564,D17-1127,0,0.016122,"Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string matching between natural language utterances and column/table names (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019). As discussed in Dong et al. (2019), such simple heuristics are difficult to accurately identify columns/tables involved in a natural language utterance and well understand the relation between an utterance and the corresponding database schema. Therefore, they make the first"
2020.emnlp-main.564,P18-1034,0,0.0162153,"f problem identification, dataset construction and model evaluation. 2 Related Work Text-to-SQL Parsing: Text-to-SQL parsing has been long studied in past decades (Finegan-Dollak et al., 2018; Yu et al., 2018c). Early text-to-SQL systems rely heavily on complicated rules and handcrafted feature engineering (Zhong et al., 2017; Finegan-Dollak et al., 2018). Fortunately, the research progress has been largely accelerated in recent years thanks to both large-scale text-to-SQL datasets (Zhong et al., 2017; Yu et al., 2018c) and interests in neural modeling (Xu et al., 2017; Dong and Lapata, 2018; Sun et al., 2018; Yu et al., 2018b; Guo et al., 2019; Wang et al., 2020). With years of studies, current research on this task focuses on addressing cross-domain generalizability and generating complex SQL queries. To improve cross-domain generalizability, advanced representations of the schema and the queries are explored, e.g., graph-based schema representations (Bogin et al., 2019b,a), contextualized question representations (Hwang et al., 2019; Guo et al., 2019) and relation-aware self-attention (Wang et al., 2020). As for the complex SQL query generation, approaches are proposed to constrain the output w"
2020.emnlp-main.564,2020.acl-main.677,0,0.116538,"ntroduction Structured Query Language (SQL), while exact and powerful, suffers from a complex grammar presenting significant challenges for laymen to write queries. Automatically parsing natural language into SQL (text-to-SQL) thus has huge potential, as it would enable lay users to mine the world’s structured data using natural language queries. To achieve practical text-to-SQL workflow, a model needs to correlate natural language queries with the given database. Therefore, schema linking is considered helpful for text-to-SQL parsing (Guo et al., 2019; Bogin et al., 2019b; Dong et al., 2019; Wang et al., 2020). Here, schema linking means identifying references of columns, tables and condition values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the SQL query. Existing solutions largely treat schema linking"
2020.emnlp-main.564,P18-1134,0,0.0208586,"intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string matching between natural language utterances and column/t"
2020.emnlp-main.564,P17-1041,0,0.0399859,"f all the columns/tables and the special token [none]. The embedding of the special token [CLS] in BERT is taken as h. Formally, we have: |Q| |E| {[CLS], Q, E} → h, {qi }i=1 , {ei }i=1 . (1) Note that, in this representation, the schema linking information has also been captured by the multilayer self-attention implicitly. However, we argue the explicit supervisions are required. While a plausible solution is to use the relation-aware encoding proposed by Wang et al. (2020) to do this, we later propose a simpler solution to facilitate our analytical study. Decoder: Inspired by the prior work (Yin and Neubig, 2017; Dong and Lapata, 2016, 2018; Zhang et al., 2019), we adopt a two-step decoder to generate the SQL query from the hidden representation h. We first generate a coarse SQL query S 0 , namely a SQL sequence without aggregate functions, using a GRU network (Cho et al., 2014). We then synthesize the final SQL query S based on S 0 . 2 part in Figure 2 illustrates the generation The of aggregate functions for the column budget during the decoding process. 4.2 Schema Linking Extension To study the role of schema linking, we extend the encoder to explicitly capture the schema linking information. It w"
2020.emnlp-main.564,N18-2093,0,0.272729,"tion values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the SQL query. Existing solutions largely treat schema linking as a minor component implemented with simple string matching (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019) heuristics to support sophisticated textto-SQL models. An exception is Dong et al. (2019), which framed schema linking as a task to be solved by sequential tagging. While they did show the importance of schema linking, how it contribute to text-to-SQL task performance remains unanswered as there is no annotated corpus to analyze. To address these shortcomings, we perform an indepth study on the role of schema linking in text-toSQL parsing. Intuitively, schema linking helps both cross-domain generalizability and complex SQL generation, which have been identified as the curr"
2020.emnlp-main.564,D18-1193,0,0.254085,"tion values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the SQL query. Existing solutions largely treat schema linking as a minor component implemented with simple string matching (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019) heuristics to support sophisticated textto-SQL models. An exception is Dong et al. (2019), which framed schema linking as a task to be solved by sequential tagging. While they did show the importance of schema linking, how it contribute to text-to-SQL task performance remains unanswered as there is no annotated corpus to analyze. To address these shortcomings, we perform an indepth study on the role of schema linking in text-toSQL parsing. Intuitively, schema linking helps both cross-domain generalizability and complex SQL generation, which have been identified as the curr"
2020.emnlp-main.564,D18-1425,0,0.139882,"Missing"
2020.emnlp-main.569,D19-1291,0,0.110337,"Missing"
2020.emnlp-main.569,Q16-1026,0,0.0668326,"Missing"
2020.emnlp-main.569,N19-1423,0,0.0240504,"Missing"
2020.emnlp-main.569,N19-1129,0,0.0324957,"Missing"
2020.emnlp-main.569,W15-4631,0,0.143997,"Missing"
2020.emnlp-main.569,D19-1564,0,0.0469096,"Missing"
2020.emnlp-main.569,P17-2039,0,0.0350856,"Missing"
2020.emnlp-main.569,P11-2088,0,0.0470329,"Missing"
2020.emnlp-main.90,D17-1209,0,0.0151228,"sk is AMR-to-text generation (Konstas et al., 2017). The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts (Li et al., 2019; Fu et al., 2020a). However, they did not explore the knowledge from a KG. Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several tasks (e.g., semi-supervised node classification (Kipf and Welling, 2017), semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017)) and also achieved state-of-the-art performance on graph-to-sequence modeling. In order to capture more graphical information, Velickovic et al. (2017) introduced graph attention networks (GATs) through stacking a graph attentional layer, but only allowed to learn information from adjacent nodes implicitly without considering a more global contextualization. Marcheggiani and Titov (2017) then used GCN as the encoder in order to capture more distant information in graphs. Since there are usually a large amount of labels for edges in KG, such graph-to-sequence models without graph transformatio"
2020.emnlp-main.90,P18-1026,0,0.30088,"specially when using fewer GCN layers. Our main contributions include: Our dataset is not only more practical but also more challenging due to lack of explicit alignment between the input and the output. Therefore, some knowledge is useful for generation, while others might be noise. In such a case that many different relations from the KG are involved, standard graphto-sequence models suffer from the problem of low training speed and parameter explosion, as edges are encoded in the form of parameters. Previous work deals with this problem by transforming the original graphs into Levi graphs (Beck et al., 2018). However, Levi graph transformation only explicitly represents the relations between an original node and its neighbor edges, while the relations between two original nodes are learned implicitly through graph convolutional networks (GCN). Therefore, more GCN layers are required to capture such information (Marcheggiani and Perez-Beltrachini, 2018). As more GCN layers are being stacked, it suffers from information loss from KG (Abu-ElHaija et al., 2018). In order to address these limitations, we present a multi-graph convolutional networks (MGCN) architecture by introducing multigraph transfo"
2020.emnlp-main.90,P11-2031,0,0.0173309,". 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) baseline. We re-implement GRN, GCN and DeepGCN using MXNET. We rearrange the order of input triples following the occurrence of entities in"
2020.emnlp-main.90,W11-2107,0,0.0226219,"7.7 23.4 26.3 24.3 E2S E2S + delex E2S-MEF E2S-MEF + delex The rows below are results of generating from entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) base"
2020.emnlp-main.90,P19-1483,0,0.0182176,"rom entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) baseline. We re-implement GRN, GCN and DeepGCN using MXNET. We rearrange the order of input triples"
2020.emnlp-main.90,2020.emnlp-main.738,1,0.746045,"from the KG. Extensive Dataset and Task. There is an increasing number of new datasets and tasks being proposed in recent years as more attention has been paid to data-to-text generation. Gardent et al. (2017) introduced the WebNLG challenge, which aimed to generate text from a small set of RDF knowledge triples (no more than 7) that are well-aligned with the text. To avoid the high cost of preparing such well-aligned data, researchers also studied how to leverage automatically obtained partially-aligned data in which some portion of the output text cannot be generated from the input triples (Fu et al., 2020b). Koncel-Kedziorski et al. (2019) introduced AGENDA dataset, which aimed to generate paper abstract from a title and a small KG built by information extraction system on the abstracts and has at most 7 relations. In our work, we directly create a knowledge graph for the main entities and topicrelated entities from Wikidata without looking at the relations in our output. Scale-wise, our dataset consists of 110k instances while AGENDA is 40k. Lebret et al. (2016) introduced WIKIBIO dataset that generates the first sentence of biographical articles from the key-value pairs extracted from the ar"
2020.emnlp-main.90,W17-3518,0,0.530368,"nowledge into comprehensive natural language, is an important task in natural language processing (NLP) and user interaction studies (Damljanovic et al., 2010). Specifically, the task takes as input some structured knowledge, such as resource description framework (RDF) triples of ∗ Liying Cheng is under the Joint Ph.D. Program between Alibaba and Singapore University of Technology and Design. † Dekun Wu was a visiting student at SUTD. Yan Zhang and Zhanming Jie were interns at Alibaba. 1 Our code and data are available at https://github.com/LiyingCheng95/ EntityDescriptionGeneration. WebNLG (Gardent et al., 2017), key-value pairs of WIKIBIO (Lebret et al., 2016) and E2E (Novikova et al., 2017), to generate natural text describing the input knowledge. In essence, the task can be formulated as follows: given a main entity, its one-hop attributes/relations (e.g., WIKIBIO and E2E), and/or multi-hop relations (e.g., WebNLG), the goal is to generate a text description of the main entity describing its attributes and relations. Note that these existing datasets basically have a good alignment between an input knowledge set and its output text. Obtaining such data with good alignment could be a laborious and"
2020.emnlp-main.90,Q19-1019,1,0.911671,"amount of labels for edges in KG, such graph-to-sequence models without graph transformation will incur information loss and parameter explosion. Beck et al. (2018) proposed to transform the graph into Levi graph in order to work towards the aforementioned deficiencies, together with gated graph neural network (GGNN) to build graph representation for AMR-to-text problem. However, they face some new limitations brought in by Levi graph transformation: the entityto-entity information is being ignored in Levi transformation, as also mentioned in their paper. Afterwards, deeper GCNs were stacked (Guo et al., 2019) to capture such ignored information implicitly. In contrast, we intend to use fewer GCN layers to capture more global contextualization by explicitly stating all types of graph information with different transformations. 3 Task Description WebNLG AGENDA E2E ENT-DESC # instances Input vocab Output vocab # distinct entities # distinct relations Avg. # triples per input Avg. # words per output 41K 54K 78K 297K 7 4.4 141.3 51K 120 5.2K 77 8 5.6 20.3 110K 420K 248K 691K 957 27.4 31.0 Table 1: Dataset statistics of WebNLG, AGENDA and our prepared ENT-DESC. tice, it is difficult to describe an entit"
2020.emnlp-main.90,N19-1238,0,0.299874,"ive Dataset and Task. There is an increasing number of new datasets and tasks being proposed in recent years as more attention has been paid to data-to-text generation. Gardent et al. (2017) introduced the WebNLG challenge, which aimed to generate text from a small set of RDF knowledge triples (no more than 7) that are well-aligned with the text. To avoid the high cost of preparing such well-aligned data, researchers also studied how to leverage automatically obtained partially-aligned data in which some portion of the output text cannot be generated from the input triples (Fu et al., 2020b). Koncel-Kedziorski et al. (2019) introduced AGENDA dataset, which aimed to generate paper abstract from a title and a small KG built by information extraction system on the abstracts and has at most 7 relations. In our work, we directly create a knowledge graph for the main entities and topicrelated entities from Wikidata without looking at the relations in our output. Scale-wise, our dataset consists of 110k instances while AGENDA is 40k. Lebret et al. (2016) introduced WIKIBIO dataset that generates the first sentence of biographical articles from the key-value pairs extracted from the article’s infobox. Novikova et al. (2"
2020.emnlp-main.90,P17-1014,0,0.023481,"ons associated with Levi graphs. • Experiments and analysis on our new dataset show that our proposed MGCN model incorporated with aggregation methods outperforms strong baselines by effectively capturing and aggregating multi-graph information. 2 1188 Related Work for a single domain, while ours focuses on multiple domains of over 100 categories, including people, event, location, organization, etc. Another difference is that we intend to generate the first paragraph of each Wikipedia article from a more complicated KG, but not key-value pairs. Another popular task is AMR-to-text generation (Konstas et al., 2017). The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts (Li et al., 2019; Fu et al., 2020a). However, they did not explore the knowledge from a KG. Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several tasks (e.g., semi-supervised node classification (Kipf and Welling, 2017), semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017)) and also achieved state-of-"
2020.emnlp-main.90,D16-1128,0,0.439154,"important task in natural language processing (NLP) and user interaction studies (Damljanovic et al., 2010). Specifically, the task takes as input some structured knowledge, such as resource description framework (RDF) triples of ∗ Liying Cheng is under the Joint Ph.D. Program between Alibaba and Singapore University of Technology and Design. † Dekun Wu was a visiting student at SUTD. Yan Zhang and Zhanming Jie were interns at Alibaba. 1 Our code and data are available at https://github.com/LiyingCheng95/ EntityDescriptionGeneration. WebNLG (Gardent et al., 2017), key-value pairs of WIKIBIO (Lebret et al., 2016) and E2E (Novikova et al., 2017), to generate natural text describing the input knowledge. In essence, the task can be formulated as follows: given a main entity, its one-hop attributes/relations (e.g., WIKIBIO and E2E), and/or multi-hop relations (e.g., WebNLG), the goal is to generate a text description of the main entity describing its attributes and relations. Note that these existing datasets basically have a good alignment between an input knowledge set and its output text. Obtaining such data with good alignment could be a laborious and expensive annotation process. More importantly, in"
2020.emnlp-main.90,W18-6501,0,0.36384,"s from the KG are involved, standard graphto-sequence models suffer from the problem of low training speed and parameter explosion, as edges are encoded in the form of parameters. Previous work deals with this problem by transforming the original graphs into Levi graphs (Beck et al., 2018). However, Levi graph transformation only explicitly represents the relations between an original node and its neighbor edges, while the relations between two original nodes are learned implicitly through graph convolutional networks (GCN). Therefore, more GCN layers are required to capture such information (Marcheggiani and Perez-Beltrachini, 2018). As more GCN layers are being stacked, it suffers from information loss from KG (Abu-ElHaija et al., 2018). In order to address these limitations, we present a multi-graph convolutional networks (MGCN) architecture by introducing multigraph transformation incorporated with an aggregation layer. Multi-graph transformation is able to represent the original graph information more accurately, while the aggregation layer learns to extract useful information from the KG. Extensive Dataset and Task. There is an increasing number of new datasets and tasks being proposed in recent years as more attent"
2020.emnlp-main.90,D17-1159,0,0.201683,"re complicated KG, but not key-value pairs. Another popular task is AMR-to-text generation (Konstas et al., 2017). The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts (Li et al., 2019; Fu et al., 2020a). However, they did not explore the knowledge from a KG. Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several tasks (e.g., semi-supervised node classification (Kipf and Welling, 2017), semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017)) and also achieved state-of-the-art performance on graph-to-sequence modeling. In order to capture more graphical information, Velickovic et al. (2017) introduced graph attention networks (GATs) through stacking a graph attentional layer, but only allowed to learn information from adjacent nodes implicitly without considering a more global contextualization. Marcheggiani and Titov (2017) then used GCN as the encoder in order to capture more distant information in graphs. Since there are usually a large amount of labels for edges in KG, su"
2020.emnlp-main.90,W17-5525,0,0.223746,"Missing"
2020.emnlp-main.90,P02-1040,0,0.106802,"31.9 31.5 58.2 59.2 60.0 59.3 27.7 23.4 26.3 24.3 E2S E2S + delex E2S-MEF E2S-MEF + delex The rows below are results of generating from entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well"
2020.emnlp-main.90,E17-2025,0,0.0284364,"described above, we can capture the information of higher-degree neighbors by stacking multiple MGCN layers. Inspired by Xu et al. (2018), we employ a concatenation operation over h(1) , · · · , h(n) to aggregate the graph representations from all MGCN layers (Figure 3 right) to form the final layer h(f inal) , which can be written as follows:   h(f inal) = h(1) , · · · h(n) . Such a mechanism allows weight sharing across graph nodes, which helps to reduce overfitting problems. To further reduce the number of parameters and overfitting problems, we apply the softmax weight tying technique (Press and Wolf, 2017) by tying source embeddings and target embeddings with a target softmax weight matrix. 5.2 Attention-based LSTM Decoder We adopt the commonly-used standard attentionbased LSTM as our decoder, where each next word yt is generated by conditioning on the final graph representation h(f inal) and all words that have been predicted y1 , ..., yt−1 . The training objective is to minimize the negative conditional log-likelihood. Thus, the objective function can be written as: T P L=− log pθ (yt |y1 , ..., yt−1 , h(f inal) ), t=1 where T represents the length of the output sequence, and p is the probabi"
2020.emnlp-main.90,2006.amta-papers.25,0,0.0222088,"x E2S-MEF E2S-MEF + delex The rows below are results of generating from entities only without exploring the KG. 23.3 20.4 68.7 58.8 41.9 21.8 20.5 67.5 59.5 39.5 24.2 21.3 65.8 59.8 43.3 20.6 20.3 66.5 59.1 40.0 Table 2: Main results of models on ENT-DESC dataset. ↓ indicates lower is better. During decoding, we use beam search with a beam size of 10. All models are run with V100 GPU. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE1 , ROUGE2 , ROUGEL (Lin, 2004), PARENT (Dhingra et al., 2019). We adapt MultEval (Clark et al., 2011) and Py-rouge for resampling and significance test. 6.2 Main Experimental Results We present our main experiments on ENT-DESC dataset and compare our proposed MGCN models with various aggregation methods against several strong GNN baselines (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), as well as a sequenceto-sequence (S2S) baseline. We re-implement GRN,"
2020.lrec-1.257,L16-1527,0,0.107847,"considered salient. Two limitations lie in NYT-Salience. First, entities are identified by a multi-step NLP pipeline, which might lead to errors in entity annotations. Second, the dataset is only partially available. The NYT-Salience dataset does not provide the underlying textual content along with the annotations due to copyright restrictions. The Reuters-128 Salience dataset is a corpus built on top of Reuters-128 (R¨oder et al., 2014), an English corpus built for evaluating NER systems, which contains 128 news articles in economy. The entity salience labels are obtained by crowdsourcing (Dojchinovski et al., 2016). The key limitation of the dataset is its small size, which does not allow for the development of supervised learning algorithms. In addition, the entity annotation process used might suffer from errors introduced by entity linking tools. Finally, entities in the dataset are uniquely identified by Wikipedia titles, DBpedia urls and others. Ideally, it is expected that all entities come from the same knowledge base. If entities are identified by entities in different knowledge bases, then many additional processing steps are needed whenever it is necessary to refer to information in knowledge"
2020.lrec-1.257,E14-4040,0,0.27228,"recognized the importance of understanding entity salience (Fetahu et al., 2015; Tran et al., 2015; Xiong et al., 2018; Ponza et al., 2019; Wu et al., 2019). For example, automatically suggesting news pages for populating Wikipedia requires determining whether a news article should be referenced by an entity, considering several aspects of the article, including entity salience, relative authority, and novelty of the article (Fetahu et al., 2015). In general, there is growing interest in understanding entity salience, demonstrated by research on entity salience detection (Gamon et al., 2013; Dunietz and Gillick, 2014). Therefore, it is very important to be able to quantify the salience of an entity. To facilitate research involving entity salience, datasets with both entity annotations and salience labels are necessary. Ideally, one would like to have human annotators labeling salient entities in documents. Unfortunately, this is not scalable due to the high volume of documents that need to be annotated and the cost of human labor. At the same time, with the rise of deep learning algorithms datasets should consist of tens of thousands of annotations to allow learning. A small number of datasets (Gamon et a"
2020.lrec-1.257,roder-etal-2014-n3,0,0.0602852,"Missing"
2020.wosp-1.3,D14-1150,0,0.0250727,"ers are supported by existing works, and readers obtain relevant information beyond the current paper. Citations also form graphs, which provide unique models for ranking, sentimental classification, and plagiarism detection. Therefore, citation analysis plays an important role in helping to understand the deep connection between literature. Accurate citation context recognition is the prerequisite of many downstream applications. Recently, citation context, the text segment that appears around the citation mark in the body text, has been used for enhancing and improving keyphrase extraction (Caragea et al., 2014) and document summarization (Cohan and Goharian, 2015). There are two types of citation context. Explicit citation contexts (ECC) are sentences containing ciJian Wu Old Dominion University Norfolk, VA, USA jwu@cs.odu.edu tation marks. Each citation thus corresponds to one explicit citation context sentence. Implicit citation contexts (ICC) are sentences that are semantically relevant to the cited articles but do not contain citation marks. ICC may appear before or after but may not immediately precede or follow the ECC sentence. One paper could be cited multiple times and each time may have di"
2020.wosp-1.3,D15-1045,0,0.0287422,"btain relevant information beyond the current paper. Citations also form graphs, which provide unique models for ranking, sentimental classification, and plagiarism detection. Therefore, citation analysis plays an important role in helping to understand the deep connection between literature. Accurate citation context recognition is the prerequisite of many downstream applications. Recently, citation context, the text segment that appears around the citation mark in the body text, has been used for enhancing and improving keyphrase extraction (Caragea et al., 2014) and document summarization (Cohan and Goharian, 2015). There are two types of citation context. Explicit citation contexts (ECC) are sentences containing ciJian Wu Old Dominion University Norfolk, VA, USA jwu@cs.odu.edu tation marks. Each citation thus corresponds to one explicit citation context sentence. Implicit citation contexts (ICC) are sentences that are semantically relevant to the cited articles but do not contain citation marks. ICC may appear before or after but may not immediately precede or follow the ECC sentence. One paper could be cited multiple times and each time may have different citation contexts. In the example below, the E"
2020.wosp-1.3,councill-etal-2008-parscit,0,0.149525,"Missing"
2020.wosp-1.3,2020.acl-main.447,0,0.154896,"CC and ICC and other citation-related information. The API was written based on the Springboot framework in Java. The machine learning model was implemented with WEKA. 4.1 File Type Recognition SCC first recognizes the uploaded file type. For a PDF file, SCC invokes GROBID and converts it to an XML file under the TEI schema. If an XML file is uploaded as input, SCC checks whether the schema is in compliance with TEI or PloS ONE schema and passes it to corresponding preprocessors. If a JSON file is uploaded, it checks if it is in compliance with the S2ORC schema, published by Semantic Scholar (Lo et al., 2020). We apply Apache Tika to identify file format. Other format of data files will not be processed. 4.2 Preprocessing The preprocessing step reads files passed from the last step with customized preprocessors depending on the schema and prepares a canonicalized XML for feature extraction. This step includes the following modules. 4.2.1 Tag removal This module involves removing irrelevant tags from the DOM structure in the XML file. For example, in the PloS ONE XML files, the &lt;fig&gt;, &lt;sub&gt;, and &lt;italic&gt; tags used for marking up figures, superscripts, and italic font are all moved. Only the text in"
2020.wosp-1.3,P14-5010,0,0.00437437,"Missing"
2020.wosp-1.3,P10-1057,0,0.0521599,"Missing"
2020.wosp-1.3,2020.acl-demos.14,0,0.0301713,"Missing"
2021.emnlp-main.204,P19-1279,0,0.198524,"a hierarchical attention prototypical network to enhance the representation ability of semantic space. Qu et al. (2020) utilize an external relation graph to study the relationships between different relations. Wang et al. (2020) apply added relative position information and syntactic relation information to enhance prototypical networks. Yang et al. (2020) fuse text descriptions of relations and entities by a collaborative attention mechanism. And Yang et al. (2021) introduce the inherent concepts of entities to provide clues for relation classification. There are also some methods (Baldini Soares et al., 2019; Peng et al., 2020) combining prototypical networks with pre-trained language models, which achieve impressive results. However, the task difficulty of FSRE has not been explored. In this work, we focus on the hard tasks and propose a hybrid contrastive relation-prototype method to better model subtle variations across different relations. 2.2 Contrastive Learning Contrastive learning (Jaiswal et al., 2021) has gained popularity recently in the CV community. • We present HCRP to explore task difficulty as The core idea is to contrast the similarities and disuseful information for FSRE, which"
2021.emnlp-main.204,N19-1423,0,0.0311396,"gnize novel classes. One popular approach is the subtle differences of relations. The relationthe meta-learning paradigm (Vinyals et al., 2016), prototype contrastive learning component is then which mimics the few-shot learning settings at used to leverage the relation label information to 2607 further enhance the discriminative power of the prototype representations. Finally, a task adaptive focal loss is introduced to encourage the model to focus training on hard tasks. of the k-th support instance as: l si ˆsik = k X αns [Sik ]n ∈ Rd (2) n=1 4.1 α Hybrid Prototype Learning We employ BERT (Devlin et al., 2019) as the encoder to obtain contextualized embeddings of query instances {Qj ∈ Rlqj ×d ; j = 1, . . . , R} and supl i ×d port instances {Sik ∈ R sk ; i = 1, . . . , N, k = 1, . . . , K}, where lqj and lsi are the sentence k lengths of the j-th query instance and k-th support instance in class i respectively, and d is the size of the resulting contextualized representations. For each relation, we concatenate the name and description and feed the sequence into the BERT encoder to obtain relation embeddings {Ri ∈ Rlri ×d ; i = 1, . . . , N }, where lri is the length of relation description i. T s l"
2021.emnlp-main.204,D19-1649,0,0.328735,"use the knowledge tions during the test phase. Although impressive to acclimate rapidly to new tasks. A simple yet performance has been achieved, these methods are effective algorithm based on meta-learning is prodifficult to adapt to novel relations that have never totypical network (Snell et al., 2017), aiming to been seen in the training process. In contrast, hu- learn a metric space in which a query instance is mans can identify new relations with very few ex- classified according to its distance to class protoamples. It is thus of great interest to enable the types. Recently, many works (Gao et al., 2019a; model to generalize to new relations with a handful Qu et al., 2020; Yang et al., 2020) for FSRE are of labeled instances. in line with prototypical networks, which achieve 2605 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2605–2616 c November 7–11, 2021. 2021 Association for Computational Linguistics remarkable performance. Nonetheless, the difficulty of distinguishing relations varies in different tasks (Zhou et al., 2020), depending on the similarity between relations. As illustrated in Figure 1, there are easy few-shot task"
2021.emnlp-main.204,D18-1514,0,0.27005,"This is a 3-way-1-shot setup – each task involves three relations, and each relation has one supporting instance. Blue and red colors indicate head and tail entities respectively. For the easy task, the relations are very different, and it is easy to classify the query instance. However, due to the subtle differences among the relations in the hard tasks, it is challenging to correctly predict the true relation. Inspired by the success of few-shot learning in the computer vision (CV) community (Sung et al., Relation extraction aims to detect the relation be- 2018; Satorras and Estrach, 2018), Han et al. (2018) tween two entities contained in a sentence, which first introduce the task of few-shot relation extracis the cornerstone of various natural language pro- tion (FSRE). FSRE requires models to be capable cessing (NLP) applications, including knowledge of handling classification of novel relations with base enrichment (Trisedya et al., 2019), biomed- scarce labeled instances. A popular framework for ical knowledge discovery (Guo et al., 2020), and few-shot learning is meta-learning (Santoro et al., question answering (Han et al., 2020). Conven- 2016; Vinyals et al., 2016), which optimizes the ti"
2021.emnlp-main.204,P16-1105,0,0.0242535,"ontained in a sentence, which first introduce the task of few-shot relation extracis the cornerstone of various natural language pro- tion (FSRE). FSRE requires models to be capable cessing (NLP) applications, including knowledge of handling classification of novel relations with base enrichment (Trisedya et al., 2019), biomed- scarce labeled instances. A popular framework for ical knowledge discovery (Guo et al., 2020), and few-shot learning is meta-learning (Santoro et al., question answering (Han et al., 2020). Conven- 2016; Vinyals et al., 2016), which optimizes the tional neural methods (Miwa and Bansal, 2016; model through collections of few-shot tasks samTran et al., 2019) train a deep network through a pled from the external data containing disjoint relarge amount of labeled data with extensive rela- lations with novel relations, so that the model can tions, so that the model can recognize these rela- learn cross-task knowledge and use the knowledge tions during the test phase. Although impressive to acclimate rapidly to new tasks. A simple yet performance has been achieved, these methods are effective algorithm based on meta-learning is prodifficult to adapt to novel relations that have never"
2021.emnlp-main.204,2020.acl-main.141,1,0.835802,"ules. Our code is available at https://github.com/hanjiale/HCRP . The contributions of this paper are summarized as follows: verse and discriminative representations. • We design a novel task adaptive focal loss to focus training on hard tasks, which enables the model to achieve higher robustness and better performance. • Qualitative and quantitative experiments on two FSRE benchmarks demonstrate the effectiveness of our model. 2 2.1 Related Work Few-shot Relation Extraction Relation extraction is a foundational and important task in NLP and attracts many recent attentions (Chen et al., 2021; Nan et al., 2020, 2021a). Fewshot relation extraction aims to predict novel relations by exploring a few labeled instances. Han et al. (2018) first present a large-scale benchmark FewRel for FSRE. Gao et al. (2019a) design a hybrid attention-based prototypical network to highlight the crucial instances and features. Ye and Ling (2019) propose a prototypical network with multi-level matching and aggregation. Sun et al. (2019) present a hierarchical attention prototypical network to enhance the representation ability of semantic space. Qu et al. (2020) utilize an external relation graph to study the relationshi"
2021.emnlp-main.204,2020.emnlp-main.298,0,0.0613359,"Missing"
2021.emnlp-main.204,D19-1045,0,0.0188982,"veness of our model. 2 2.1 Related Work Few-shot Relation Extraction Relation extraction is a foundational and important task in NLP and attracts many recent attentions (Chen et al., 2021; Nan et al., 2020, 2021a). Fewshot relation extraction aims to predict novel relations by exploring a few labeled instances. Han et al. (2018) first present a large-scale benchmark FewRel for FSRE. Gao et al. (2019a) design a hybrid attention-based prototypical network to highlight the crucial instances and features. Ye and Ling (2019) propose a prototypical network with multi-level matching and aggregation. Sun et al. (2019) present a hierarchical attention prototypical network to enhance the representation ability of semantic space. Qu et al. (2020) utilize an external relation graph to study the relationships between different relations. Wang et al. (2020) apply added relative position information and syntactic relation information to enhance prototypical networks. Yang et al. (2020) fuse text descriptions of relations and entities by a collaborative attention mechanism. And Yang et al. (2021) introduce the inherent concepts of entities to provide clues for relation classification. There are also some methods ("
2021.emnlp-main.204,N19-1286,0,0.028243,"ation extracis the cornerstone of various natural language pro- tion (FSRE). FSRE requires models to be capable cessing (NLP) applications, including knowledge of handling classification of novel relations with base enrichment (Trisedya et al., 2019), biomed- scarce labeled instances. A popular framework for ical knowledge discovery (Guo et al., 2020), and few-shot learning is meta-learning (Santoro et al., question answering (Han et al., 2020). Conven- 2016; Vinyals et al., 2016), which optimizes the tional neural methods (Miwa and Bansal, 2016; model through collections of few-shot tasks samTran et al., 2019) train a deep network through a pled from the external data containing disjoint relarge amount of labeled data with extensive rela- lations with novel relations, so that the model can tions, so that the model can recognize these rela- learn cross-task knowledge and use the knowledge tions during the test phase. Although impressive to acclimate rapidly to new tasks. A simple yet performance has been achieved, these methods are effective algorithm based on meta-learning is prodifficult to adapt to novel relations that have never totypical network (Snell et al., 2017), aiming to been seen in the"
2021.emnlp-main.204,P19-1023,0,0.0239415,"ns in the hard tasks, it is challenging to correctly predict the true relation. Inspired by the success of few-shot learning in the computer vision (CV) community (Sung et al., Relation extraction aims to detect the relation be- 2018; Satorras and Estrach, 2018), Han et al. (2018) tween two entities contained in a sentence, which first introduce the task of few-shot relation extracis the cornerstone of various natural language pro- tion (FSRE). FSRE requires models to be capable cessing (NLP) applications, including knowledge of handling classification of novel relations with base enrichment (Trisedya et al., 2019), biomed- scarce labeled instances. A popular framework for ical knowledge discovery (Guo et al., 2020), and few-shot learning is meta-learning (Santoro et al., question answering (Han et al., 2020). Conven- 2016; Vinyals et al., 2016), which optimizes the tional neural methods (Miwa and Bansal, 2016; model through collections of few-shot tasks samTran et al., 2019) train a deep network through a pled from the external data containing disjoint relarge amount of labeled data with extensive rela- lations with novel relations, so that the model can tions, so that the model can recognize these rel"
2021.emnlp-main.204,2020.clinicalnlp-1.25,0,0.497342,"ction aims to predict novel relations by exploring a few labeled instances. Han et al. (2018) first present a large-scale benchmark FewRel for FSRE. Gao et al. (2019a) design a hybrid attention-based prototypical network to highlight the crucial instances and features. Ye and Ling (2019) propose a prototypical network with multi-level matching and aggregation. Sun et al. (2019) present a hierarchical attention prototypical network to enhance the representation ability of semantic space. Qu et al. (2020) utilize an external relation graph to study the relationships between different relations. Wang et al. (2020) apply added relative position information and syntactic relation information to enhance prototypical networks. Yang et al. (2020) fuse text descriptions of relations and entities by a collaborative attention mechanism. And Yang et al. (2021) introduce the inherent concepts of entities to provide clues for relation classification. There are also some methods (Baldini Soares et al., 2019; Peng et al., 2020) combining prototypical networks with pre-trained language models, which achieve impressive results. However, the task difficulty of FSRE has not been explored. In this work, we focus on the"
2021.emnlp-main.204,2021.acl-short.124,0,0.0227517,"ial instances and features. Ye and Ling (2019) propose a prototypical network with multi-level matching and aggregation. Sun et al. (2019) present a hierarchical attention prototypical network to enhance the representation ability of semantic space. Qu et al. (2020) utilize an external relation graph to study the relationships between different relations. Wang et al. (2020) apply added relative position information and syntactic relation information to enhance prototypical networks. Yang et al. (2020) fuse text descriptions of relations and entities by a collaborative attention mechanism. And Yang et al. (2021) introduce the inherent concepts of entities to provide clues for relation classification. There are also some methods (Baldini Soares et al., 2019; Peng et al., 2020) combining prototypical networks with pre-trained language models, which achieve impressive results. However, the task difficulty of FSRE has not been explored. In this work, we focus on the hard tasks and propose a hybrid contrastive relation-prototype method to better model subtle variations across different relations. 2.2 Contrastive Learning Contrastive learning (Jaiswal et al., 2021) has gained popularity recently in the CV"
2021.emnlp-main.204,P19-1277,0,0.321415,"ce. • Qualitative and quantitative experiments on two FSRE benchmarks demonstrate the effectiveness of our model. 2 2.1 Related Work Few-shot Relation Extraction Relation extraction is a foundational and important task in NLP and attracts many recent attentions (Chen et al., 2021; Nan et al., 2020, 2021a). Fewshot relation extraction aims to predict novel relations by exploring a few labeled instances. Han et al. (2018) first present a large-scale benchmark FewRel for FSRE. Gao et al. (2019a) design a hybrid attention-based prototypical network to highlight the crucial instances and features. Ye and Ling (2019) propose a prototypical network with multi-level matching and aggregation. Sun et al. (2019) present a hierarchical attention prototypical network to enhance the representation ability of semantic space. Qu et al. (2020) utilize an external relation graph to study the relationships between different relations. Wang et al. (2020) apply added relative position information and syntactic relation information to enhance prototypical networks. Yang et al. (2020) fuse text descriptions of relations and entities by a collaborative attention mechanism. And Yang et al. (2021) introduce the inherent conc"
2021.emnlp-main.317,E17-2091,0,0.143942,"in the and opinion words. Second, an inaccurate parse sentence “The battery life of this laptop is very long, but the price is too high”, the sentiment ex- tree could lead to error propagation downstream in the pipeline. Several research groups have explored pressed towards the aspect term “battery life” is positive, whereas the sentiment towards the as- the above issues with a more refined parse tree. pect term “price” is negative. Early research ef- For example, Chen et al. (2020) constructed taskspecific structures by developing a gate mechanism forts (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li and Lu, 2019; Xu et al., 2020a) to dynamically combine the parse tree information and a stochastic graph sampled from the HardKuma * Work done when visiting SUTD. distribution (Bastings et al., 2019). On the other † Corresponding author. 1 hand, Wang et al. (2020) greedily reshaped the Our code is available at https://github.com/ zyxnlp/ACLT. dependency parse tree by using manual rules to 3899 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3899–3909 c November 7–11, 2021. 2021 Association for Computational Linguistics obtain the aspect-relate"
2021.emnlp-main.317,P19-1284,0,0.0124671,"am in the pipeline. Several research groups have explored pressed towards the aspect term “battery life” is positive, whereas the sentiment towards the as- the above issues with a more refined parse tree. pect term “price” is negative. Early research ef- For example, Chen et al. (2020) constructed taskspecific structures by developing a gate mechanism forts (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li and Lu, 2019; Xu et al., 2020a) to dynamically combine the parse tree information and a stochastic graph sampled from the HardKuma * Work done when visiting SUTD. distribution (Bastings et al., 2019). On the other † Corresponding author. 1 hand, Wang et al. (2020) greedily reshaped the Our code is available at https://github.com/ zyxnlp/ACLT. dependency parse tree by using manual rules to 3899 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3899–3909 c November 7–11, 2021. 2021 Association for Computational Linguistics obtain the aspect-related syntactic structures. Despite being able to effectively alleviate the tree representation problem, existing methods still depend on external parse trees, leading to one potential problem. The dependency"
2021.emnlp-main.317,Q18-1005,0,0.206754,"hows an aspect-centric tree where the tree is rooted by the aspect words. The distances between aspect and opinion words are one hop and two hops, which is closer than the distance in the standard dependency parse tree. In this paper, we propose a model that learns Aspect-Centric Latent Trees (we name it as the ACLT model) which are specifically tailored for the ABSA task. We assume that inducing tree structures whose roots are within aspect term enables the model to correlate the aspect and opinion words better. We built our model based on the structure attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) and a variant of the Matrix-Tree Theorem (MTT) (Tutte, 1984; Koo et al., 2007). Additionally, we proposed to impose a soft constraint to encourage the aspect words to serve as the root of the tree structure induced by MTT. As a result, the search space of inferring the root is reduced during the training process. Our contributions are summarized as follows: opinion words in an end-to-end fashion. • Our ACLT model is able to learn an aspectcentric latent tree with a root refinement strategy to better correlate the aspect and opinion words than the standard parse tree. • Experiments show that o"
2021.emnlp-main.317,2020.emnlp-main.451,0,0.273869,"static, and thus cannot adaptively model the complex relationship between multiple aspects lar target in a sentence. For example, in the and opinion words. Second, an inaccurate parse sentence “The battery life of this laptop is very long, but the price is too high”, the sentiment ex- tree could lead to error propagation downstream in the pipeline. Several research groups have explored pressed towards the aspect term “battery life” is positive, whereas the sentiment towards the as- the above issues with a more refined parse tree. pect term “price” is negative. Early research ef- For example, Chen et al. (2020) constructed taskspecific structures by developing a gate mechanism forts (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li and Lu, 2019; Xu et al., 2020a) to dynamically combine the parse tree information and a stochastic graph sampled from the HardKuma * Work done when visiting SUTD. distribution (Bastings et al., 2019). On the other † Corresponding author. 1 hand, Wang et al. (2020) greedily reshaped the Our code is available at https://github.com/ zyxnlp/ACLT. dependency parse tree by using manual rules to 3899 Proceedings of the 2021 Conference on Empirical Methods in Natural"
2021.emnlp-main.317,D17-1047,0,0.139885,"ence. For example, in the and opinion words. Second, an inaccurate parse sentence “The battery life of this laptop is very long, but the price is too high”, the sentiment ex- tree could lead to error propagation downstream in the pipeline. Several research groups have explored pressed towards the aspect term “battery life” is positive, whereas the sentiment towards the as- the above issues with a more refined parse tree. pect term “price” is negative. Early research ef- For example, Chen et al. (2020) constructed taskspecific structures by developing a gate mechanism forts (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li and Lu, 2019; Xu et al., 2020a) to dynamically combine the parse tree information and a stochastic graph sampled from the HardKuma * Work done when visiting SUTD. distribution (Bastings et al., 2019). On the other † Corresponding author. 1 hand, Wang et al. (2020) greedily reshaped the Our code is available at https://github.com/ zyxnlp/ACLT. dependency parse tree by using manual rules to 3899 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3899–3909 c November 7–11, 2021. 2021 Association for Computational Linguistics obt"
2021.emnlp-main.317,N19-1423,0,0.435511,"trees. The underlying tree inducer is a latent-variable model which treats tree structures as the latent variable. Once we have the distribution over the latent trees, we adopt the root refinement procedure to obtain aspect-centric latent trees. Then, we can encode the probabilistic latent trees with a graph or tree encoder. Finally, we use the structured representation from the tree encoder for sentiment classification. 2.1 Sentence Encoder Given a sentence s = [w1 , ..., wn ] and the corresponding aspect term a = [wi , ..., wj ] (1 ≤ i ≤ j ≤ n), we adopt the pre-trained language model BERT (Devlin et al., 2019) to obtain the contextualized representation for each word. We concatenate the words in the sentence and explicitly present the aspect term in the input representation: x = • We propose to use Aspect-Centric Latent ([CLS] w1 , ..., wn [SEP] wi , ..., wj [SEP]). Trees (ACLT) which are specifically tailored The contextualized representation H can be obfor the ABSA task to link up aspects with tained via BERT(x), where H = [h1 , ..., hn ], 2 hi ∈ H represents the contextualized represenThe detail statistic can be found in Appendix A. 3 https://spacy.io/api/dependencyparser tation of the i-th toke"
2021.emnlp-main.317,P14-2009,0,0.0680404,"Missing"
2021.emnlp-main.317,P19-4001,0,0.0443604,"Missing"
2021.emnlp-main.317,2020.acl-main.141,1,0.8998,"ree Inducer awesome menu lunch BiLinear menu is awesome Linear FNN FNN Pre-trained Language Model (BERT) is Inference Root Marginal Probability The lunch menu is an awesome deal Inference Edge Marginal Probability Laplace Transformation Figure 2: ACLT architecture. 2.2 Aspect-Centric Tree Inducer While prior efforts (Wang et al., 2020; Chen et al., 2020) on learning latent (or explicit) trees for the ABSA task exist, one of the major contributions of our work is that we link up aspects and opinion words by addressing the root inconsistency issue. Inspired by recent work (Liu and Lapata, 2018; Nan et al., 2020), we use a variant of Kirchhoff’s MatrixTree Theorem (Tutte, 1984; Koo et al., 2007) to induce the latent dependency structure. Given the contextualized representation h ∈ Rd of each node (token) in the sentence, where d is the dimension of the node representations. We first calculate pair-wise unnormalized edge scores eij between the i-th and the j-th node with the node representation hi and hj by way of a two feed-forward neural network (FNN) and a bilinear function:  eij = tanh(Wp hi ))T Wb (tanh(Wc hj ) , (1) where Wp ∈ Rd×d and Wc ∈ Rd×d are weights for two feedforward neural networks, t"
2021.emnlp-main.317,D18-1108,0,0.0234863,"Missing"
2021.emnlp-main.317,D19-1549,0,0.0162431,"Aspect-based sentiment analysis. Early efforts on aspect-based sentiment focused on predicting polarity by employing attention mechanism (Bahdanau et al., 2015) to model interactions between aspect and context words (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li et al., 2018; Wang et al., 2018). More recently, neural pretrained language models, for instance, BERT (Devlin et al., 2019) enabled ABSA to achieve promising results. For example, Sun et al. (2019a) manually constructed auxiliary sentences using the aspect word to convert ABSA into a sentencepair classification task. Huang and Carley (2019) propagated opinion features from syntax neighborhood words to the aspect words, in a BERT-based model. Another line of work in ABSA focused on leveraging the explicit dependency parse trees to model the relationships between context and aspect words. Zhang et al. (2019) and Sun et al. (2019b) used GCNs to integrate dependency tree information to capture structural and contextual information simultaneously for aspect-based sentiment analysis. Wang et al. (2020) greedily reshaped the dependency parse trees by using manual rules to obtain the task-specific syntactic structures. three main aspect"
2021.emnlp-main.317,S15-2082,0,0.0674944,"Missing"
2021.emnlp-main.317,D19-1399,1,0.833927,"standard parse tree. 3.4 Model Analysis Effect of different tree representations we first use BERT-base as a contextual encoder, then use GCN to encode dependency parse tree information (Parser+GCN), latent Matrix tree information (MTT+GCN), latent Kuma structure (Kuma+GCN7 ) and our aspect-centric tree information (ACLT+GCN). Table 4 summarizes the results. We observe that models incorporated with syntactic information generally outperform the vanilla BERT-SRC, indicating that syntactic information benefits the ABSA task. Such a phenomenon can also be observed in other fundamental NLP tasks (Jie and Lu, 2019; Xu et al., 2021). Moreover, we also found that both our ACLT and ACLT+GCN model consistently outperform models equipped with other dependency trees by a significant margin. These results demonstrate that the aspectcentric tree induced by our model is indeed capable of effectively building relationships between aspect and context words for the ABSA task. Under the same setting, ACLT+GCN outperforms Parser+GCN, MTT+GCN, and Kuma+GCN on all the datasets. In particular, our ACLT+GCN obtains 1.8, 2.6, and 7 points improvement over Parser+GCN, MTT+GCN, and Kuma+GCN on Rest15 in terms of F1 . Moreo"
2021.emnlp-main.317,S16-1002,0,0.0643211,"Missing"
2021.emnlp-main.317,S14-2004,0,0.121397,"Missing"
2021.emnlp-main.317,D18-3004,0,0.0202246,"Missing"
2021.emnlp-main.317,D07-1015,0,0.363263,"nces between aspect and opinion words are one hop and two hops, which is closer than the distance in the standard dependency parse tree. In this paper, we propose a model that learns Aspect-Centric Latent Trees (we name it as the ACLT model) which are specifically tailored for the ABSA task. We assume that inducing tree structures whose roots are within aspect term enables the model to correlate the aspect and opinion words better. We built our model based on the structure attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) and a variant of the Matrix-Tree Theorem (MTT) (Tutte, 1984; Koo et al., 2007). Additionally, we proposed to impose a soft constraint to encourage the aspect words to serve as the root of the tree structure induced by MTT. As a result, the search space of inferring the root is reduced during the training process. Our contributions are summarized as follows: opinion words in an end-to-end fashion. • Our ACLT model is able to learn an aspectcentric latent tree with a root refinement strategy to better correlate the aspect and opinion words than the standard parse tree. • Experiments show that our model outperforms the existing approaches, and also yields new state-of-the-"
2021.emnlp-main.317,D19-1550,1,0.851109,"rds. Second, an inaccurate parse sentence “The battery life of this laptop is very long, but the price is too high”, the sentiment ex- tree could lead to error propagation downstream in the pipeline. Several research groups have explored pressed towards the aspect term “battery life” is positive, whereas the sentiment towards the as- the above issues with a more refined parse tree. pect term “price” is negative. Early research ef- For example, Chen et al. (2020) constructed taskspecific structures by developing a gate mechanism forts (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li and Lu, 2019; Xu et al., 2020a) to dynamically combine the parse tree information and a stochastic graph sampled from the HardKuma * Work done when visiting SUTD. distribution (Bastings et al., 2019). On the other † Corresponding author. 1 hand, Wang et al. (2020) greedily reshaped the Our code is available at https://github.com/ zyxnlp/ACLT. dependency parse tree by using manual rules to 3899 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3899–3909 c November 7–11, 2021. 2021 Association for Computational Linguistics obtain the aspect-related syntactic struc"
2021.emnlp-main.317,P18-1087,0,0.0841771,"ained with the remaining data, the results of R-GAT+BERT (Wang et al., 2020) and KumaGCN+BERT (Chen et al., 2020) are lower than which reported in the original paper. In our experiments, we report the average result and the mean absolute deviations over three runs with the random initialization. We stop training when iterations reached the maximum of 30 epochs. The state-of-the-art baselines selected for comparison fall into three main categories: Syntax information free models, dependency parse tree based models, and latent tree based models. Syntax information free models include: • TNet-AS Li et al. (2018) implements a context-preserving mechanism to get the aspect-specific representations. 3.3 Main Results • BERT-PT Xu et al. (2019) explores a novel As shown in Table 2, dependency tree based modpost-training approach on BERT to enhance els and latent tree based models generally achieve the performance of BERT which has been finebetter results than syntax information free modtuned for ABSA and RRC. els, suggesting that syntactic information indeed • BERT-PAIR Sun et al. (2019a) constructs benefits the ABSA task and enables it to achieve auxiliary sentences from the aspect and conpromising resul"
2021.emnlp-main.317,N19-1035,0,0.278498,"urns the number of hops between two words in the tree. Words marked in red and blue are aspect and opinion, respectively. focus on using an attention mechanism (Bahdanau et al., 2015) to model interactions between aspect and context words. However, such attention-based models may suffer from overly focusing on the frequent words that express sentiment polarity while ignoring low-frequency ones (Tang et al., 2019; Sun and Lu, 2020). Recent efforts show that the syntactic structures of sentences can facilitate the identification of sentiment features related to aspect words (Zhang et al., 2019; Sun et al., 2019b; Huang 1 Introduction and Carley, 2019). Nonetheless, these methods unfortunately suffer from two shortcomings. First, Aspect-based sentiment analysis (ABSA) (Pang the trees obtained from off-the-shelf dependency et al., 2008; Liu, 2012) aims at determining the sentiment polarity expressed towards a particu- parsers are static, and thus cannot adaptively model the complex relationship between multiple aspects lar target in a sentence. For example, in the and opinion words. Second, an inaccurate parse sentence “The battery life of this laptop is very long, but the price is too high”, the sent"
2021.emnlp-main.317,D19-1569,0,0.283903,"urns the number of hops between two words in the tree. Words marked in red and blue are aspect and opinion, respectively. focus on using an attention mechanism (Bahdanau et al., 2015) to model interactions between aspect and context words. However, such attention-based models may suffer from overly focusing on the frequent words that express sentiment polarity while ignoring low-frequency ones (Tang et al., 2019; Sun and Lu, 2020). Recent efforts show that the syntactic structures of sentences can facilitate the identification of sentiment features related to aspect words (Zhang et al., 2019; Sun et al., 2019b; Huang 1 Introduction and Carley, 2019). Nonetheless, these methods unfortunately suffer from two shortcomings. First, Aspect-based sentiment analysis (ABSA) (Pang the trees obtained from off-the-shelf dependency et al., 2008; Liu, 2012) aims at determining the sentiment polarity expressed towards a particu- parsers are static, and thus cannot adaptively model the complex relationship between multiple aspects lar target in a sentence. For example, in the and opinion words. Second, an inaccurate parse sentence “The battery life of this laptop is very long, but the price is too high”, the sent"
2021.emnlp-main.317,2020.acl-main.312,1,0.726185,") = 2 root Loving the harry potter movie marathon... (b) Learned aspect-centric Tree (Ours) Figure 1: An example with different tree representations in the Twitter dataset. “Dist” returns the number of hops between two words in the tree. Words marked in red and blue are aspect and opinion, respectively. focus on using an attention mechanism (Bahdanau et al., 2015) to model interactions between aspect and context words. However, such attention-based models may suffer from overly focusing on the frequent words that express sentiment polarity while ignoring low-frequency ones (Tang et al., 2019; Sun and Lu, 2020). Recent efforts show that the syntactic structures of sentences can facilitate the identification of sentiment features related to aspect words (Zhang et al., 2019; Sun et al., 2019b; Huang 1 Introduction and Carley, 2019). Nonetheless, these methods unfortunately suffer from two shortcomings. First, Aspect-based sentiment analysis (ABSA) (Pang the trees obtained from off-the-shelf dependency et al., 2008; Liu, 2012) aims at determining the sentiment polarity expressed towards a particu- parsers are static, and thus cannot adaptively model the complex relationship between multiple aspects lar"
2021.emnlp-main.317,D16-1021,0,0.0511395,"Missing"
2021.emnlp-main.317,P19-1053,0,0.0266087,"Missing"
2021.emnlp-main.317,2020.acl-main.295,0,0.274473,"ards the aspect term “battery life” is positive, whereas the sentiment towards the as- the above issues with a more refined parse tree. pect term “price” is negative. Early research ef- For example, Chen et al. (2020) constructed taskspecific structures by developing a gate mechanism forts (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li and Lu, 2019; Xu et al., 2020a) to dynamically combine the parse tree information and a stochastic graph sampled from the HardKuma * Work done when visiting SUTD. distribution (Bastings et al., 2019). On the other † Corresponding author. 1 hand, Wang et al. (2020) greedily reshaped the Our code is available at https://github.com/ zyxnlp/ACLT. dependency parse tree by using manual rules to 3899 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3899–3909 c November 7–11, 2021. 2021 Association for Computational Linguistics obtain the aspect-related syntactic structures. Despite being able to effectively alleviate the tree representation problem, existing methods still depend on external parse trees, leading to one potential problem. The dependency parse trees are not designed for the purpose of ABSA but to expr"
2021.emnlp-main.317,P18-1088,0,0.0122099,"whose sentiments can be correctly predicted by our ACLT model. Overall we observe 8 that aspect-centric trees differ from the standard Here “wave” is chosen as the root because it has the dependency trees in the types of dependencies they highest probability (i.e., Pir in Equation 7). 3906 4 Related Work Aspect-based sentiment analysis. Early efforts on aspect-based sentiment focused on predicting polarity by employing attention mechanism (Bahdanau et al., 2015) to model interactions between aspect and context words (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li et al., 2018; Wang et al., 2018). More recently, neural pretrained language models, for instance, BERT (Devlin et al., 2019) enabled ABSA to achieve promising results. For example, Sun et al. (2019a) manually constructed auxiliary sentences using the aspect word to convert ABSA into a sentencepair classification task. Huang and Carley (2019) propagated opinion features from syntax neighborhood words to the aspect words, in a BERT-based model. Another line of work in ABSA focused on leveraging the explicit dependency parse trees to model the relationships between context and aspect words. Zhang et al. (2019) and Sun et al. (2"
2021.emnlp-main.317,D16-1058,0,0.245123,"ar target in a sentence. For example, in the and opinion words. Second, an inaccurate parse sentence “The battery life of this laptop is very long, but the price is too high”, the sentiment ex- tree could lead to error propagation downstream in the pipeline. Several research groups have explored pressed towards the aspect term “battery life” is positive, whereas the sentiment towards the as- the above issues with a more refined parse tree. pect term “price” is negative. Early research ef- For example, Chen et al. (2020) constructed taskspecific structures by developing a gate mechanism forts (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li and Lu, 2019; Xu et al., 2020a) to dynamically combine the parse tree information and a stochastic graph sampled from the HardKuma * Work done when visiting SUTD. distribution (Bastings et al., 2019). On the other † Corresponding author. 1 hand, Wang et al. (2020) greedily reshaped the Our code is available at https://github.com/ zyxnlp/ACLT. dependency parse tree by using manual rules to 3899 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3899–3909 c November 7–11, 2021. 2021 Association for Computatio"
2021.emnlp-main.317,P19-1517,1,0.736907,"Missing"
2021.emnlp-main.317,N19-1242,0,0.116647,"ng aspect-centric graph P , we follow Kim et al. (2017) and Liu and Lapata (2018) to encode the tree information by structure attention mechanism: n X p si = Pki hk + Pir ha k=1 sci = n X Pik hi (9) k=1 si = tanh (Ws [spi , sci , hi ]) , where spi ∈ Rd is the context representation gathered from possible parents of hi , sci ∈ Rd is the context representation gathered from possible children, and ha is the representation for the root node. We concatenate spi , sci with hi and transform with weights Ws ∈ Rd×3d to obtain the structured representation of the i-th word si . 2.4 Classifier Following Xu et al. (2019) and Sun et al. (2019a), we leverage s0 , which is the structured aspectaware representation of each sentence, to compute the probability over the different sentiment polarities as: yp = softmax (Wp s0 + bp ) , (10) Train Dataset Lap14 Rest14 Rest15 Rest16 Twitter Dev Test #Pos. #Neu. #Neg. #Pos. #Neu. #Neg. #Pos. #Neu. #Neg. 0,895 1,948 0,821 1,116 1,405 0,418 0,573 0,032 0,062 2,814 0,783 0,726 0,230 0,395 1,404 099 216 091 124 156 046 064 004 007 313 087 081 026 044 156 341 728 326 469 173 169 196 034 030 346 128 196 182 117 173 Table 1: Statistics of datasets. The objective of the classifi"
2021.emnlp-main.317,2020.emnlp-main.288,1,0.764486,"naccurate parse sentence “The battery life of this laptop is very long, but the price is too high”, the sentiment ex- tree could lead to error propagation downstream in the pipeline. Several research groups have explored pressed towards the aspect term “battery life” is positive, whereas the sentiment towards the as- the above issues with a more refined parse tree. pect term “price” is negative. Early research ef- For example, Chen et al. (2020) constructed taskspecific structures by developing a gate mechanism forts (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li and Lu, 2019; Xu et al., 2020a) to dynamically combine the parse tree information and a stochastic graph sampled from the HardKuma * Work done when visiting SUTD. distribution (Bastings et al., 2019). On the other † Corresponding author. 1 hand, Wang et al. (2020) greedily reshaped the Our code is available at https://github.com/ zyxnlp/ACLT. dependency parse tree by using manual rules to 3899 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3899–3909 c November 7–11, 2021. 2021 Association for Computational Linguistics obtain the aspect-related syntactic structures. Despite be"
2021.emnlp-main.317,2021.naacl-main.271,1,0.708591,"e. 3.4 Model Analysis Effect of different tree representations we first use BERT-base as a contextual encoder, then use GCN to encode dependency parse tree information (Parser+GCN), latent Matrix tree information (MTT+GCN), latent Kuma structure (Kuma+GCN7 ) and our aspect-centric tree information (ACLT+GCN). Table 4 summarizes the results. We observe that models incorporated with syntactic information generally outperform the vanilla BERT-SRC, indicating that syntactic information benefits the ABSA task. Such a phenomenon can also be observed in other fundamental NLP tasks (Jie and Lu, 2019; Xu et al., 2021). Moreover, we also found that both our ACLT and ACLT+GCN model consistently outperform models equipped with other dependency trees by a significant margin. These results demonstrate that the aspectcentric tree induced by our model is indeed capable of effectively building relationships between aspect and context words for the ABSA task. Under the same setting, ACLT+GCN outperforms Parser+GCN, MTT+GCN, and Kuma+GCN on all the datasets. In particular, our ACLT+GCN obtains 1.8, 2.6, and 7 points improvement over Parser+GCN, MTT+GCN, and Kuma+GCN on Rest15 in terms of F1 . Moreover, ACLT+GCN outp"
2021.emnlp-main.317,2020.emnlp-main.183,1,0.715447,"naccurate parse sentence “The battery life of this laptop is very long, but the price is too high”, the sentiment ex- tree could lead to error propagation downstream in the pipeline. Several research groups have explored pressed towards the aspect term “battery life” is positive, whereas the sentiment towards the as- the above issues with a more refined parse tree. pect term “price” is negative. Early research ef- For example, Chen et al. (2020) constructed taskspecific structures by developing a gate mechanism forts (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Li and Lu, 2019; Xu et al., 2020a) to dynamically combine the parse tree information and a stochastic graph sampled from the HardKuma * Work done when visiting SUTD. distribution (Bastings et al., 2019). On the other † Corresponding author. 1 hand, Wang et al. (2020) greedily reshaped the Our code is available at https://github.com/ zyxnlp/ACLT. dependency parse tree by using manual rules to 3899 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3899–3909 c November 7–11, 2021. 2021 Association for Computational Linguistics obtain the aspect-related syntactic structures. Despite be"
2021.emnlp-main.317,D19-1464,0,0.154779,"dataset. “Dist” returns the number of hops between two words in the tree. Words marked in red and blue are aspect and opinion, respectively. focus on using an attention mechanism (Bahdanau et al., 2015) to model interactions between aspect and context words. However, such attention-based models may suffer from overly focusing on the frequent words that express sentiment polarity while ignoring low-frequency ones (Tang et al., 2019; Sun and Lu, 2020). Recent efforts show that the syntactic structures of sentences can facilitate the identification of sentiment features related to aspect words (Zhang et al., 2019; Sun et al., 2019b; Huang 1 Introduction and Carley, 2019). Nonetheless, these methods unfortunately suffer from two shortcomings. First, Aspect-based sentiment analysis (ABSA) (Pang the trees obtained from off-the-shelf dependency et al., 2008; Liu, 2012) aims at determining the sentiment polarity expressed towards a particu- parsers are static, and thus cannot adaptively model the complex relationship between multiple aspects lar target in a sentence. For example, in the and opinion words. Second, an inaccurate parse sentence “The battery life of this laptop is very long, but the price is t"
2021.emnlp-main.317,2020.emnlp-main.286,0,0.0244151,"ective latent tree for ABSA in an endThe dependency parse tree based models to-end fashion. In particular, with the exception are: of R-GAT+BERT on the Rest14 dataset in terms • ASGCN Zhang et al. (2019) uses GCNs to of F1 , our model surpassed all compared models capture the long-range dependencies between by a significant margin. For example, our model words. achieves 72.08 and 78.64 F1 on the Rest15 and • CDT Sun et al. (2019b) uses GCNs to inte- Rest16 datasets, which significantly outperform the grate dependency parse tree information. current state-of-the-art model KumaGCN+BERT, • BiGCN Zhang and Qian (2020) uses syntactic under the same setting. The statistics empirically graph and lexical graph to capture the global show that compared to the models that use syntacword co-occurrence information. tic information, ACLT can induce a more informa• ASGCN+BERT is a baseline that uses BERT tive latent task-specific structure to establish effecinstead of BiLSTM as the context encoder of tive connections between aspect words and context. ASGCN (Zhang et al., 2019). Our ACLT model also shows its superiority over • R-GAT+BERT (Wang et al., 2020) is a de- all baselines in terms of accuracy. pendency tree ba"
2021.emnlp-main.763,2021.acl-long.342,0,0.0148879,"Dep-Guided LSTM, although the predictions are also incorrect. Our CFIE model can obtain correct results for both instances, showing the effectiveness of our novel debiasing approach. Compared to CFIE, the inferior performance of SGG-TDE is due to ignoring the importance of entity (trigger) for the NER and ED tasks. 4 Related Work Long-tailed IE: RE (Zeng et al., 2014; Peng et al., 2017; Quirk and Poon, 2017; Song et al., 2018; Lin et al., 2019; Peng et al., 2020; Nan et al., 2020; Qu et al., 2020; Guo et al., 2020; Zhang et al., 2021c; Zheng et al., 2021; Zhang et al., 2020b; Ye et al., 2021; Bai et al., 2021; Nan et al., 2021a), 3.6 Case Study NER (Lample et al., 2016; Chiu and Nichols, 2016; Figure 11 shows two cases to visualize the predic- Xu et al., 2021b), and ED (Nguyen and Grishman, tions of baseline models and our CFIE model for 2015; Huang et al., 2018) are mainstream IE tasks long-tailed NER and ED, respectively. We use the in NLP. For the long-tailed IE, recent models (Lei “BIOES” tagging scheme for both cases and choose et al., 2018; Zhang et al., 2019) leverage external Dep-Guided LSTM (Jie and Lu, 2019) and SGG- rules or transfer knowledge from data-rich classes TDE (Tang et al., 20"
2021.emnlp-main.763,W13-2322,0,0.0144473,", 2020) that the entity itself may be more important than context in NER. By doing so, the key clues are expected to be wiped off in the representations X ∗ of counterfactuals, strengthening the main effect while reducing the spurious correlations. (4) 2.3 i∈E{X} where fY is the function that computes Y . Compared to the vanilla formula for Yx , we only replace its feature HX with Hx∗ . Counterfactual Generation: There are many language structures such as dependency and constituency trees (Marcus et al., 1993), semantic role labels (Palmer et al., 2005), and abstract meaning representations (Banarescu et al., 2013). We choose the dependency tree in our case as it can capture rich relational information and complex long-distance interactions that have proven effective on IE tasks. Counterfactuals lead us to think about: what are the key clues that determine the relations of two entities for RE, and a certain span of a sentence to be an entity or an event trigger for NER and ED respectively? As demonstrated in Figure 2 (d), we mask entities, or the tokens in the scope of 1 hop on the dependency tree. Then this masked sequence is fed to a BiLSTM or BERT encoder to output new contextualized representations"
2021.emnlp-main.763,2021.findings-acl.196,0,0.0711518,"Missing"
2021.emnlp-main.763,W17-3518,0,0.019565,"ts. 3.3 Task Definitions We show the definition of the IE sub-tasks used in our experiments as follows, including named entity recognition (NER), event detection (ED) and relation extraction (RE). We use five datasets in our experiments including Named Entity Recognition: NER is a sequence OntoNotes5.0 (Pradhan et al., 2013) and ATIS (Tur labeling task that seeks to locate and classify named et al., 2010) for the NER task, ACE2005 (Doddingentities in unstructured text into pre-defined cateton et al., 2004) and MAVEN (Wang et al., 2020b) gories such as person, location, etc. for ED, and NYT24 (Gardent et al., 2017) for the RE task. The labels in the above datasets follow Event Detection: ED aims to detect the occurlong-tailed distributions. We categorize the classes rences of predefined events and categorize them as into three splits based on the number of training triggers from unstructured text. An event trigger instances per class, including Few, Medium, and is defined as the words or phase that most clearly Many, and also report the results on the whole expresses an event occurrence. Taking the sentence dataset with the Overall setting. We focus more on “a cameraman died in the Palestine Hotel” as a"
2021.emnlp-main.763,D18-1247,0,0.0249146,"ity Gardens appears 13 times in the training set of OntoNotes5.0, with the NER tag location LOC, and only 2 times as organization ORG. A classifier trained on this dataset tends to build spurious correlations between Gardens and LOC, although Gardens itself does not indicate a location. Most existing works on addressing spurious correlations focus on images, such as re-balanced training (Lin et al., 2017), transfer learning (Liu et al., 2019) and decoupling (Kang et al., 2019). However, these approaches may not be suitable for natural language inputs. Recent efforts on information extraction (Han et al., 2018; Zhang et al., 2019) incorporate prior knowledge, which requires data-specific designs. Causal inference (Pearl et al., 2016) is promising in tackling the above spurious correlation issues caused by unbalanced data distribution. Along this line, various causal models have been proposed for 1 For the NER task, a model may also build the spurious correlations between the part-of-speech (POS) tags of entities and class labels. For RE and ED tasks, a model may also learn incorrect correlations between features like NER tags and labels. We consider all above issues in our proposed causal diagram."
2021.emnlp-main.763,P18-1201,0,0.0193546,"the importance of entity (trigger) for the NER and ED tasks. 4 Related Work Long-tailed IE: RE (Zeng et al., 2014; Peng et al., 2017; Quirk and Poon, 2017; Song et al., 2018; Lin et al., 2019; Peng et al., 2020; Nan et al., 2020; Qu et al., 2020; Guo et al., 2020; Zhang et al., 2021c; Zheng et al., 2021; Zhang et al., 2020b; Ye et al., 2021; Bai et al., 2021; Nan et al., 2021a), 3.6 Case Study NER (Lample et al., 2016; Chiu and Nichols, 2016; Figure 11 shows two cases to visualize the predic- Xu et al., 2021b), and ED (Nguyen and Grishman, tions of baseline models and our CFIE model for 2015; Huang et al., 2018) are mainstream IE tasks long-tailed NER and ED, respectively. We use the in NLP. For the long-tailed IE, recent models (Lei “BIOES” tagging scheme for both cases and choose et al., 2018; Zhang et al., 2019) leverage external Dep-Guided LSTM (Jie and Lu, 2019) and SGG- rules or transfer knowledge from data-rich classes TDE (Tang et al., 2020b) as baselines. In the first to the tail classes. Plenty of re-balancing models case for NER, the baseline assigns “chinese” with are also proposed, including re-sampling strategies the label S-NORP, which indicates “nationalities or (Mahajan et al., 2018;"
2021.emnlp-main.763,Q16-1026,0,0.250248,"d obtain more robust predictions, we strengthen the impact of entity (trigger) on the final results by WXY Hx∗i as shown in Step 5 of Figure 4(b). Such a design differs from SGG-TDE (Tang et al., 2020a) by providing more flexible adjustment and effect estimation with hyperparameters α and β. We will show that our approach is more suitable for long-tailed IE tasks in experiments. 3 3.1 Experiments Datasets and Settings α on the development sets4 . 3.2 Baselines We categorize the baselines used in our experiments into three groups and outline them as follows. Conventional models include BiLSTM (Chiu and Nichols, 2016), BiLSTM+CRF (Ma and Hovy, 2016), C-GCN (Zhang et al., 2018), Dep-Guided LSTM (Jie and Lu, 2019), and BERT (Devlin et al., 2019). These neural models do not explicitly take the long-tailed issues into consideration. Re-weighting/Decoupling models refer to loss re-weighting approaches including Focal Loss (Lin et al., 2017), and two-stage decoupled learning methods (Kang et al., 2019) that include τ normalization, classifier retraining (cRT) and learnable weight scaling (LWS). Causal model includes SGG-TDE (Tang et al., 2020b). There are also recent studies based on the deconfounded methodology"
2021.emnlp-main.763,N19-1423,0,0.594595,"the best of our knowledge, CFIE is the first study that marries the counterfactual analysis Figure 3 presents our unified SCM Gie for IE tasks and syntactic structure to address the spurious based on our prior knowledge. The variable S indicorrelation issue for long-tailed IE. We build cates the contextualized representations of an input different structural causal models (SCM; Pearl sentence, where the representations are the output et al. 2016) for various IE tasks to better cap- from a BiLSTM (Schuster and Paliwal, 1997) or a ture the underlying main causalities. pre-trained BERT encoder (Devlin et al., 2019). Zj 9684 Number of Training Instances Training Stage Trigger: killed Event: Life:Die 511 training instances Trigger: killed Event: SW:Quit 10 training instances S Training The man was N The program was X killed 1st Inference Inference S was N 2nd Inference:Generate Counterfactuals S* P The Y Y program MASK was S P N killed X* StepP2 Step 2 P X Inference S X (c) Step 1 S N killed Y Structured Causal Model (b) (a) The program P killed S: contextualized word representation N: NER tag P: POS tag X: word representation computed from S Y: prediction logits Y* S*: contextualized word rep after mask"
2021.emnlp-main.763,D19-1399,1,0.774963,"n be easily disentangled with detection or segmentation methods for causal manipulation, texts rely more on the context involving complex syntactic and semantic structures. Hence it is impractical to apply the methods used in images to disentangle tokens’ representations. Recent causal models (Zeng et al., 2020; Wang and Culotta, 2020, 2021) on text classification eliminate biases by replacing target entities or antonyms. These methods do not consider structural information, which has proven effective for various IE tasks as they are able to capture non-local interactions (Zhang et al., 2018; Jie and Lu, 2019). This motivates us to propose a novel framework termed as counterfactual information extraction (CFIE). Different from previous efforts, our CFIE model alleviates the spurious correlations by generating counterfactuals (Bottou et al., 2013; Abbasnejad et al., 2020) based on the syntactic structure (Zhang et al., 2018). From a causal perspective, counterfactuals state the results of the outcome if certain factors had been different. This concept entails a hypothetical scenario where the values in the causal graph can be altered to study the effect of the factor. Intuitively, the factor that yi"
2021.emnlp-main.763,doddington-etal-2004-automatic,0,0.047242,"e available at https: //github.com/HeyyyyyyG/CFIE. Specifically, our major contributions are: • To alleviate spurious corrections, we generate counterfactuals based on syntactic structures. To achieve more robust predictions, we further propose a novel debiasing approach, which maintains a better balance between the direct effect and counterfactual representations. • Extensive quantitative and qualitative experiments on various IE tasks across five datasets show the effectiveness of our approach. 2 Model Figure 2 demonstrates the proposed CFIE method using an example from the ACE2005 dataset (Doddington et al., 2004) on the ED task. As shown in Figure 2 (a), two event types “Life:Die” and “SW:Quit” of the trigger killed have 511 and 19 training instances, respectively. Such an unbalanced distribution may mislead a model to build spurious correlations between the trigger word killed and the type “Life:Die”. The goal of CFIE is to alleviate such incorrect correlations. CFIE employs SCM (Pearl et al., 2016) as causal diagram as it clearly describes relationships among variables. We give the formulation of SCM as follows. SCM: Without loss of generality, we express SCM as a directed acyclic graph (DAG) G = {V"
2021.emnlp-main.763,C18-1036,0,0.0547719,"Missing"
2021.emnlp-main.763,N16-1030,0,0.232966,"les; 2) with our SCM, we then generate counterfactuals based on an explicit language structure to better calculate the direct causal effect during the inference stage; 3) we further propose a novel debiasing approach to yield more robust predictions. Experiments on three IE tasks across five public datasets show the effectiveness of our CFIE model in mitigating the spurious correlation issues. 1 head classes The goal of Information Extraction (IE) is to detect the structured information from unstructured texts. Previous deep learning models for IE tasks, such as named entity recognition (NER; Lample et al. 2016), relation extraction (RE; Peng et al. 2017) and event detection (ED; Nguyen and Grishman 2015), are largely proposed for learning under some reasonably balanced label distributions. However, in practice, these labels usually follow a long-tailed distribution (Doddington et al., 2004). Figure 1 shows such an unbalanced distribution on the ACE2005 (Doddington et al., 2004) dataset. As a result, performance on the instance-scarce (tail) classes may drop significantly. For example, on an existing model for NER (Jie and Lu, 2019), the macro F1 score of instance-rich (head) classes Contributed equa"
2021.emnlp-main.763,J05-1004,0,0.0777453,"his operation also aligns with a recent finding (Zeng et al., 2020) that the entity itself may be more important than context in NER. By doing so, the key clues are expected to be wiped off in the representations X ∗ of counterfactuals, strengthening the main effect while reducing the spurious correlations. (4) 2.3 i∈E{X} where fY is the function that computes Y . Compared to the vanilla formula for Yx , we only replace its feature HX with Hx∗ . Counterfactual Generation: There are many language structures such as dependency and constituency trees (Marcus et al., 1993), semantic role labels (Palmer et al., 2005), and abstract meaning representations (Banarescu et al., 2013). We choose the dependency tree in our case as it can capture rich relational information and complex long-distance interactions that have proven effective on IE tasks. Counterfactuals lead us to think about: what are the key clues that determine the relations of two entities for RE, and a certain span of a sentence to be an entity or an event trigger for NER and ED respectively? As demonstrated in Figure 2 (d), we mask entities, or the tokens in the scope of 1 hop on the dependency tree. Then this masked sequence is fed to a BiLST"
2021.emnlp-main.763,P16-1101,0,0.194532,"strengthen the impact of entity (trigger) on the final results by WXY Hx∗i as shown in Step 5 of Figure 4(b). Such a design differs from SGG-TDE (Tang et al., 2020a) by providing more flexible adjustment and effect estimation with hyperparameters α and β. We will show that our approach is more suitable for long-tailed IE tasks in experiments. 3 3.1 Experiments Datasets and Settings α on the development sets4 . 3.2 Baselines We categorize the baselines used in our experiments into three groups and outline them as follows. Conventional models include BiLSTM (Chiu and Nichols, 2016), BiLSTM+CRF (Ma and Hovy, 2016), C-GCN (Zhang et al., 2018), Dep-Guided LSTM (Jie and Lu, 2019), and BERT (Devlin et al., 2019). These neural models do not explicitly take the long-tailed issues into consideration. Re-weighting/Decoupling models refer to loss re-weighting approaches including Focal Loss (Lin et al., 2017), and two-stage decoupled learning methods (Kang et al., 2019) that include τ normalization, classifier retraining (cRT) and learnable weight scaling (LWS). Causal model includes SGG-TDE (Tang et al., 2020b). There are also recent studies based on the deconfounded methodology (Tang et al., 2020a; Yang et al"
2021.emnlp-main.763,J93-2004,0,0.074067,"he function of the edge S → X to get X ∗ . This operation also aligns with a recent finding (Zeng et al., 2020) that the entity itself may be more important than context in NER. By doing so, the key clues are expected to be wiped off in the representations X ∗ of counterfactuals, strengthening the main effect while reducing the spurious correlations. (4) 2.3 i∈E{X} where fY is the function that computes Y . Compared to the vanilla formula for Yx , we only replace its feature HX with Hx∗ . Counterfactual Generation: There are many language structures such as dependency and constituency trees (Marcus et al., 1993), semantic role labels (Palmer et al., 2005), and abstract meaning representations (Banarescu et al., 2013). We choose the dependency tree in our case as it can capture rich relational information and complex long-distance interactions that have proven effective on IE tasks. Counterfactuals lead us to think about: what are the key clues that determine the relations of two entities for RE, and a certain span of a sentence to be an entity or an event trigger for NER and ED respectively? As demonstrated in Figure 2 (d), we mask entities, or the tokens in the scope of 1 hop on the dependency tree."
2021.emnlp-main.763,2020.acl-main.141,1,0.891771,"Missing"
2021.emnlp-main.763,2020.emnlp-main.298,0,0.0356119,"e trigger word “attack”, and compare it with the two baselines. For both cases, previous SGG-TDE outputs relatively unbiased predictions compared with Dep-Guided LSTM, although the predictions are also incorrect. Our CFIE model can obtain correct results for both instances, showing the effectiveness of our novel debiasing approach. Compared to CFIE, the inferior performance of SGG-TDE is due to ignoring the importance of entity (trigger) for the NER and ED tasks. 4 Related Work Long-tailed IE: RE (Zeng et al., 2014; Peng et al., 2017; Quirk and Poon, 2017; Song et al., 2018; Lin et al., 2019; Peng et al., 2020; Nan et al., 2020; Qu et al., 2020; Guo et al., 2020; Zhang et al., 2021c; Zheng et al., 2021; Zhang et al., 2020b; Ye et al., 2021; Bai et al., 2021; Nan et al., 2021a), 3.6 Case Study NER (Lample et al., 2016; Chiu and Nichols, 2016; Figure 11 shows two cases to visualize the predic- Xu et al., 2021b), and ED (Nguyen and Grishman, tions of baseline models and our CFIE model for 2015; Huang et al., 2018) are mainstream IE tasks long-tailed NER and ED, respectively. We use the in NLP. For the long-tailed IE, recent models (Lei “BIOES” tagging scheme for both cases and choose et al., 2018; Zha"
2021.emnlp-main.763,Q17-1008,0,0.11973,"factuals based on an explicit language structure to better calculate the direct causal effect during the inference stage; 3) we further propose a novel debiasing approach to yield more robust predictions. Experiments on three IE tasks across five public datasets show the effectiveness of our CFIE model in mitigating the spurious correlation issues. 1 head classes The goal of Information Extraction (IE) is to detect the structured information from unstructured texts. Previous deep learning models for IE tasks, such as named entity recognition (NER; Lample et al. 2016), relation extraction (RE; Peng et al. 2017) and event detection (ED; Nguyen and Grishman 2015), are largely proposed for learning under some reasonably balanced label distributions. However, in practice, these labels usually follow a long-tailed distribution (Doddington et al., 2004). Figure 1 shows such an unbalanced distribution on the ACE2005 (Doddington et al., 2004) dataset. As a result, performance on the instance-scarce (tail) classes may drop significantly. For example, on an existing model for NER (Jie and Lu, 2019), the macro F1 score of instance-rich (head) classes Contributed equally. tail classes Figure 1: Class distributi"
2021.emnlp-main.763,E17-1110,0,0.019179,"e second case for ED, we demonstrate a similar issue for the trigger word “attack”, and compare it with the two baselines. For both cases, previous SGG-TDE outputs relatively unbiased predictions compared with Dep-Guided LSTM, although the predictions are also incorrect. Our CFIE model can obtain correct results for both instances, showing the effectiveness of our novel debiasing approach. Compared to CFIE, the inferior performance of SGG-TDE is due to ignoring the importance of entity (trigger) for the NER and ED tasks. 4 Related Work Long-tailed IE: RE (Zeng et al., 2014; Peng et al., 2017; Quirk and Poon, 2017; Song et al., 2018; Lin et al., 2019; Peng et al., 2020; Nan et al., 2020; Qu et al., 2020; Guo et al., 2020; Zhang et al., 2021c; Zheng et al., 2021; Zhang et al., 2020b; Ye et al., 2021; Bai et al., 2021; Nan et al., 2021a), 3.6 Case Study NER (Lample et al., 2016; Chiu and Nichols, 2016; Figure 11 shows two cases to visualize the predic- Xu et al., 2021b), and ED (Nguyen and Grishman, tions of baseline models and our CFIE model for 2015; Huang et al., 2018) are mainstream IE tasks long-tailed NER and ED, respectively. We use the in NLP. For the long-tailed IE, recent models (Lei “BIOES” ta"
2021.emnlp-main.763,D18-1246,0,0.0140112,"we demonstrate a similar issue for the trigger word “attack”, and compare it with the two baselines. For both cases, previous SGG-TDE outputs relatively unbiased predictions compared with Dep-Guided LSTM, although the predictions are also incorrect. Our CFIE model can obtain correct results for both instances, showing the effectiveness of our novel debiasing approach. Compared to CFIE, the inferior performance of SGG-TDE is due to ignoring the importance of entity (trigger) for the NER and ED tasks. 4 Related Work Long-tailed IE: RE (Zeng et al., 2014; Peng et al., 2017; Quirk and Poon, 2017; Song et al., 2018; Lin et al., 2019; Peng et al., 2020; Nan et al., 2020; Qu et al., 2020; Guo et al., 2020; Zhang et al., 2021c; Zheng et al., 2021; Zhang et al., 2020b; Ye et al., 2021; Bai et al., 2021; Nan et al., 2021a), 3.6 Case Study NER (Lample et al., 2016; Chiu and Nichols, 2016; Figure 11 shows two cases to visualize the predic- Xu et al., 2021b), and ED (Nguyen and Grishman, tions of baseline models and our CFIE model for 2015; Huang et al., 2018) are mainstream IE tasks long-tailed NER and ED, respectively. We use the in NLP. For the long-tailed IE, recent models (Lei “BIOES” tagging scheme for bo"
2021.emnlp-main.763,P15-2060,0,0.0686594,"Missing"
2021.emnlp-main.763,C14-1220,0,0.0929909,"Missing"
2021.emnlp-main.763,2020.emnlp-main.129,0,0.0957214,"line methods by ourselves since they may have not been reported on NLP datasets. 3.3 Task Definitions We show the definition of the IE sub-tasks used in our experiments as follows, including named entity recognition (NER), event detection (ED) and relation extraction (RE). We use five datasets in our experiments including Named Entity Recognition: NER is a sequence OntoNotes5.0 (Pradhan et al., 2013) and ATIS (Tur labeling task that seeks to locate and classify named et al., 2010) for the NER task, ACE2005 (Doddingentities in unstructured text into pre-defined cateton et al., 2004) and MAVEN (Wang et al., 2020b) gories such as person, location, etc. for ED, and NYT24 (Gardent et al., 2017) for the RE task. The labels in the above datasets follow Event Detection: ED aims to detect the occurlong-tailed distributions. We categorize the classes rences of predefined events and categorize them as into three splits based on the number of training triggers from unstructured text. An event trigger instances per class, including Few, Medium, and is defined as the words or phase that most clearly Many, and also report the results on the whole expresses an event occurrence. Taking the sentence dataset with the"
2021.emnlp-main.763,2020.emnlp-main.590,0,0.489495,"ence on Empirical Methods in Natural Language Processing, pages 9683–9695 c November 7–11, 2021. 2021 Association for Computational Linguistics visual tasks (Abbasnejad et al., 2020; Tang et al., 2020b). Despite their success, these methods may be unsatisfactory on textual inputs. Unlike images, which can be easily disentangled with detection or segmentation methods for causal manipulation, texts rely more on the context involving complex syntactic and semantic structures. Hence it is impractical to apply the methods used in images to disentangle tokens’ representations. Recent causal models (Zeng et al., 2020; Wang and Culotta, 2020, 2021) on text classification eliminate biases by replacing target entities or antonyms. These methods do not consider structural information, which has proven effective for various IE tasks as they are able to capture non-local interactions (Zhang et al., 2018; Jie and Lu, 2019). This motivates us to propose a novel framework termed as counterfactual information extraction (CFIE). Different from previous efforts, our CFIE model alleviates the spurious correlations by generating counterfactuals (Bottou et al., 2013; Abbasnejad et al., 2020) based on the syntactic struc"
2021.emnlp-main.763,2020.findings-emnlp.308,0,0.0243524,"ethods in Natural Language Processing, pages 9683–9695 c November 7–11, 2021. 2021 Association for Computational Linguistics visual tasks (Abbasnejad et al., 2020; Tang et al., 2020b). Despite their success, these methods may be unsatisfactory on textual inputs. Unlike images, which can be easily disentangled with detection or segmentation methods for causal manipulation, texts rely more on the context involving complex syntactic and semantic structures. Hence it is impractical to apply the methods used in images to disentangle tokens’ representations. Recent causal models (Zeng et al., 2020; Wang and Culotta, 2020, 2021) on text classification eliminate biases by replacing target entities or antonyms. These methods do not consider structural information, which has proven effective for various IE tasks as they are able to capture non-local interactions (Zhang et al., 2018; Jie and Lu, 2019). This motivates us to propose a novel framework termed as counterfactual information extraction (CFIE). Different from previous efforts, our CFIE model alleviates the spurious correlations by generating counterfactuals (Bottou et al., 2013; Abbasnejad et al., 2020) based on the syntactic structure (Zhang et al., 2018"
2021.emnlp-main.763,2021.naacl-main.271,1,0.719808,"ur novel debiasing approach. Compared to CFIE, the inferior performance of SGG-TDE is due to ignoring the importance of entity (trigger) for the NER and ED tasks. 4 Related Work Long-tailed IE: RE (Zeng et al., 2014; Peng et al., 2017; Quirk and Poon, 2017; Song et al., 2018; Lin et al., 2019; Peng et al., 2020; Nan et al., 2020; Qu et al., 2020; Guo et al., 2020; Zhang et al., 2021c; Zheng et al., 2021; Zhang et al., 2020b; Ye et al., 2021; Bai et al., 2021; Nan et al., 2021a), 3.6 Case Study NER (Lample et al., 2016; Chiu and Nichols, 2016; Figure 11 shows two cases to visualize the predic- Xu et al., 2021b), and ED (Nguyen and Grishman, tions of baseline models and our CFIE model for 2015; Huang et al., 2018) are mainstream IE tasks long-tailed NER and ED, respectively. We use the in NLP. For the long-tailed IE, recent models (Lei “BIOES” tagging scheme for both cases and choose et al., 2018; Zhang et al., 2019) leverage external Dep-Guided LSTM (Jie and Lu, 2019) and SGG- rules or transfer knowledge from data-rich classes TDE (Tang et al., 2020b) as baselines. In the first to the tail classes. Plenty of re-balancing models case for NER, the baseline assigns “chinese” with are also proposed, i"
2021.emnlp-main.763,2020.emnlp-demos.1,0,0.199316,"ly unbiased predictions compared with Dep-Guided LSTM, although the predictions are also incorrect. Our CFIE model can obtain correct results for both instances, showing the effectiveness of our novel debiasing approach. Compared to CFIE, the inferior performance of SGG-TDE is due to ignoring the importance of entity (trigger) for the NER and ED tasks. 4 Related Work Long-tailed IE: RE (Zeng et al., 2014; Peng et al., 2017; Quirk and Poon, 2017; Song et al., 2018; Lin et al., 2019; Peng et al., 2020; Nan et al., 2020; Qu et al., 2020; Guo et al., 2020; Zhang et al., 2021c; Zheng et al., 2021; Zhang et al., 2020b; Ye et al., 2021; Bai et al., 2021; Nan et al., 2021a), 3.6 Case Study NER (Lample et al., 2016; Chiu and Nichols, 2016; Figure 11 shows two cases to visualize the predic- Xu et al., 2021b), and ED (Nguyen and Grishman, tions of baseline models and our CFIE model for 2015; Huang et al., 2018) are mainstream IE tasks long-tailed NER and ED, respectively. We use the in NLP. For the long-tailed IE, recent models (Lei “BIOES” tagging scheme for both cases and choose et al., 2018; Zhang et al., 2019) leverage external Dep-Guided LSTM (Jie and Lu, 2019) and SGG- rules or transfer knowledge from da"
2021.emnlp-main.763,N19-1306,0,0.0884926,"s 13 times in the training set of OntoNotes5.0, with the NER tag location LOC, and only 2 times as organization ORG. A classifier trained on this dataset tends to build spurious correlations between Gardens and LOC, although Gardens itself does not indicate a location. Most existing works on addressing spurious correlations focus on images, such as re-balanced training (Lin et al., 2017), transfer learning (Liu et al., 2019) and decoupling (Kang et al., 2019). However, these approaches may not be suitable for natural language inputs. Recent efforts on information extraction (Han et al., 2018; Zhang et al., 2019) incorporate prior knowledge, which requires data-specific designs. Causal inference (Pearl et al., 2016) is promising in tackling the above spurious correlation issues caused by unbalanced data distribution. Along this line, various causal models have been proposed for 1 For the NER task, a model may also build the spurious correlations between the part-of-speech (POS) tags of entities and class labels. For RE and ED tasks, a model may also learn incorrect correlations between features like NER tags and labels. We consider all above issues in our proposed causal diagram. 9683 Proceedings of t"
2021.emnlp-main.763,D18-1244,0,0.373963,"ike images, which can be easily disentangled with detection or segmentation methods for causal manipulation, texts rely more on the context involving complex syntactic and semantic structures. Hence it is impractical to apply the methods used in images to disentangle tokens’ representations. Recent causal models (Zeng et al., 2020; Wang and Culotta, 2020, 2021) on text classification eliminate biases by replacing target entities or antonyms. These methods do not consider structural information, which has proven effective for various IE tasks as they are able to capture non-local interactions (Zhang et al., 2018; Jie and Lu, 2019). This motivates us to propose a novel framework termed as counterfactual information extraction (CFIE). Different from previous efforts, our CFIE model alleviates the spurious correlations by generating counterfactuals (Bottou et al., 2013; Abbasnejad et al., 2020) based on the syntactic structure (Zhang et al., 2018). From a causal perspective, counterfactuals state the results of the outcome if certain factors had been different. This concept entails a hypothetical scenario where the values in the causal graph can be altered to study the effect of the factor. Intuitively,"
2021.emnlp-main.767,Q18-1002,0,0.055032,"Missing"
2021.emnlp-main.767,W15-4611,0,0.0395957,"Missing"
2021.emnlp-main.767,D15-1268,0,0.0540678,"Missing"
2021.emnlp-main.767,P17-2079,0,0.0251169,"e will compensate the freight for you. We will try our best to improve your shopping experience. Thank you for your understanding and patience. Fine, hope I can receive it soon. utter7 Handoff utter5 utter6 Global (dialogue) satisfaction Human Satisfaction Rating: Figure 1: A snippet of a moderately satisfied customer service dialogue. There is a satisfaction rating at the end of the conversation. The utterance with an orange background color denotes a transferable utterance. Chatbot, as one of the recent palpable AI excitements, has been widely adopted to reduce the cost of customer service (Qiu et al., 2017; Ram et al., 2018; Zhou et al., 2020). However, due to the complexity of human conversation, auto-chatbot can hardly meet all users’ needs, while its potential failure perceives skepticism. AI-enabled customer service, for instance, may trigger unexpected business losses because of chatbot failures (Radziwill and Benton, 2017; Rajendran et al., 2019). Moreover, for chatbot adoption in sensitive areas, such as healthcare (Chung and Park, 2019) and criminal justice (Wang et al., 2020a), any subtle statistical miscalculation may trigger serious health and legal Corresponding authors. Chatbot Wha"
2021.emnlp-main.767,N19-1373,0,0.0172444,"rning rate of 1.5 × 10−3 and 2×10−5 for regular baselines and BERT-based models. All the methods run on a server configured with a Tesla V100, 32 CPU, and 32G memory. 4.2 Baselines We compare our model with 14 strong dialogue classification baseline models, which come from MHCH, SSA, and other similar tasks. Generic Baselines: HAN (Yang et al., 2016) and BERT(Devlin et al., 2019)+LSTM. We adopt outputs and the last hidden of RNN to predict handoff labels and the satisfaction rating, respectively. Baselines for the MHCH task: HEC (Kumar et al., 2018), DialogueRNN (Majumder et al., 2019), CASA (Raheja and Tetreault, 2019), https://github.com/songkaisong/ssa https://www.taobao.com 4 9736 https://pypi.org/project/jieba Clothes Makeup MHCH Models F1 SSA MHCH Mac. F1 GT-I GT-II GT-III WS F1 MT F1 US F1 Mac. F1 Acc. HAN 59.8 BERT+LSTM 60.4 78.7 78.9 71.7 73.1 73.4 74.9 74.0 75.9 51.5 42.2 81.7 84.2 70.4 72.9 67.9 66.4 HEC DialogueRNN CASA LSTMLCA CESTa DAMI 59.8 60.8 62.0 62.6 60.6 66.7 78.7 79.2 79.8 80.1 79.1 82.2 71.2 73.1 73.6 72.4 73.4 74.2 72.3 74.6 75.0 73.9 74.8 75.9 73.0 75.6 75.9 74.8 75.6 77.1 - - - - - - - - - - 38.2 44.1 55.4 82.3 83.3 84.4 70.8 69.6 71.5 63.8 65.7 70.4 75.3 76.3 78.3 MILNET HMN CAMIL"
2021.emnlp-main.767,Q19-1024,0,0.048812,"Missing"
2021.emnlp-main.767,schmitt-etal-2012-parameterized,0,0.0156389,"s between two tasks. To the best of our and Følstad, 2018; Jain et al., 2018; Chaves and knowledge, it is the pioneer investigation to leverGerosa, 2020). Besides exploring novel dialogue age the multi-task learning approach for integratmodels, dialogue quality estimation, service satising MHCH and SSA. In practice, we first adopt a 1 shared encoder to obtain the shared representations https://github.com/WeijiaLau/RSSN 9732 faction analysis, and human intervention are vital strategies to enhance chatbot performance. Dialogue Quality and Service Satisfaction Analysis. Interaction Quality (IQ) (Schmitt et al., 2012) and Response Quality (RQ) (Bodigutla et al., 2019b) are dialogue quality evaluation metrics for spoken dialogue systems. Automated models to estimate IQ (Ultes et al., 2014; El Asri et al., 2014) and RQ (Bodigutla et al., 2019a,b, 2020) utilize various features derived from the dialogue content and output from spoken language understanding components. For chat-oriented dialogue system, Higashinaka et al. (2015a,b) introduce Dialogue Breakdown Detection task to detect a system’s inappropriate utterances that lead to dialogue breakdowns. To efficiently analyze dialogue satisfaction, Song et al."
2021.emnlp-main.767,D08-1027,0,0.0751829,"Missing"
2021.emnlp-main.767,W19-5902,0,0.0171805,"timately affect the overall satisfaction. On the one hand, handoff labels of utterances are highly pertinent to local satisfaction, e.g., one can utilize single handoff information to enhance local satisfaction prediction, which ultimately contributes to the overall satisfaction estimation. On the other hand, the overall satisfaction is obtained by combining local satisfactions, which reflects the quality in terms of answer generation, language understanding, and emotion perception, and subsequently helps to facilitate handoff judgment. In recent years, researchers (Bodigutla et al., 2019a,b; Ultes, 2019; Bodigutla et al., 2020) explore joint evaluation of turn and dialogue level qualities in spoken dialogue systems. In terms of general dialogue system, to improve the efficiency of dialogue management, Qin et al. (2020) propose a co-interactive relation layer to explicitly examine the cross-impact and model the interaction between sentiment classification and dialog act recognition, which are relevant tasks at the same level (utterancelevel). However, MHCH (utterance-level) and SSA (dialogue-level) target satisfaction at different levels. More importantly, handoff labels of utterances are mor"
2021.emnlp-main.767,2020.sigdial-1.23,0,0.0308989,"ot, as one of the recent palpable AI excitements, has been widely adopted to reduce the cost of customer service (Qiu et al., 2017; Ram et al., 2018; Zhou et al., 2020). However, due to the complexity of human conversation, auto-chatbot can hardly meet all users’ needs, while its potential failure perceives skepticism. AI-enabled customer service, for instance, may trigger unexpected business losses because of chatbot failures (Radziwill and Benton, 2017; Rajendran et al., 2019). Moreover, for chatbot adoption in sensitive areas, such as healthcare (Chung and Park, 2019) and criminal justice (Wang et al., 2020a), any subtle statistical miscalculation may trigger serious health and legal Corresponding authors. Chatbot What a business! It has been a week! Introduction ∗ Hello, I placed my order about one week ago. When can it be shipped? consequences. To address this problem, recently, scholars proposed new dialog mining tasks to autoassess dialogue satisfaction, a.k.a. Service Satisfaction Analysis (SSA) at dialogue-level (Song et al., 2019), and to predict potential chatbot failure via machine-human chatting handoff (MHCH) at utterance-level (Huang et al., 2018; Liu et al., 2021). In a MHCH context"
2021.emnlp-main.767,N16-1174,0,0.0185935,"of hidden state k, Dense units d, attention units z, and batch size are selected from {32, 64, 128, 256, 512}. The dropout (Srivastava et al., 2014) rate and the loss weight η are selected from (0, 1) by grid search. Finally, we train the models with an initial learning rate of 1.5 × 10−3 and 2×10−5 for regular baselines and BERT-based models. All the methods run on a server configured with a Tesla V100, 32 CPU, and 32G memory. 4.2 Baselines We compare our model with 14 strong dialogue classification baseline models, which come from MHCH, SSA, and other similar tasks. Generic Baselines: HAN (Yang et al., 2016) and BERT(Devlin et al., 2019)+LSTM. We adopt outputs and the last hidden of RNN to predict handoff labels and the satisfaction rating, respectively. Baselines for the MHCH task: HEC (Kumar et al., 2018), DialogueRNN (Majumder et al., 2019), CASA (Raheja and Tetreault, 2019), https://github.com/songkaisong/ssa https://www.taobao.com 4 9736 https://pypi.org/project/jieba Clothes Makeup MHCH Models F1 SSA MHCH Mac. F1 GT-I GT-II GT-III WS F1 MT F1 US F1 Mac. F1 Acc. HAN 59.8 BERT+LSTM 60.4 78.7 78.9 71.7 73.1 73.4 74.9 74.0 75.9 51.5 42.2 81.7 84.2 70.4 72.9 67.9 66.4 HEC DialogueRNN CASA LSTMLC"
2021.emnlp-main.767,2020.cl-1.2,0,0.0617753,"Missing"
2021.findings-acl.30,J05-3002,0,0.0708098,"ment summarization research. Extractive summarization methods produce summaries that are semantically similar to the original documents. Thus, they may be able to achieve relatively high ROUGE scores (Lin, 2004). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as"
2021.findings-acl.30,P15-1153,0,0.0241395,"y similar to the original documents. Thus, they may be able to achieve relatively high ROUGE scores (Lin, 2004). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extrac"
2021.findings-acl.30,N13-1136,0,0.0830578,"Missing"
2021.findings-acl.30,2020.acl-main.457,0,0.0172432,"ons across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. For recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. Fan et al. (2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. Huang et al. (2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. Li et al. (2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, Wang et al. (2020a) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization. Our work is partly similar to theirs, but we construct heterogeneous graphs composed of text unit nodes and entity nodes for abstractive multi-document summariz"
2021.findings-acl.30,N19-1423,0,0.0301625,"input embedding matrix and the matrix Wo to reuse linguistic knowledge (Paulus et al., 2018). We further add a copy mechanism as proposed by See et al. (2017). 3.6 Training Our training process follows that of the traditional sequence-to-sequence modeling, with maximum likelihood estimation that minimizes: 1 X Lseq = − log p(y|x; θ) (17) |D| (y,x)∈D where x and y are document-summary pairs from training set D, and θ are parameters to be learned. 3.7 Pre-trained LMs as Document Encoder Our document encoder illustrated in section 3.3 can be replaced by a pre-trained language model such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). Pre-trained language models can be more effective on short inputs than training stacked transformer layers from scratch. We feed input tokens to a pre-trained language model and take the last layer output as token embeddings. Then a single-layer bidirectional LSTM is employed over token embeddings, producing token features. Finally, we perform the same multi-head pooling strategy to obtain paragraph representations. 4 4.1 Experimental Setup We conduct experiments on two major datasets used in the literature of multi-document summarization, namely WikiSum (Liu e"
2021.findings-acl.30,P19-1102,0,0.0174963,"19). Pre-trained language models can be more effective on short inputs than training stacked transformer layers from scratch. We feed input tokens to a pre-trained language model and take the last layer output as token embeddings. Then a single-layer bidirectional LSTM is employed over token embeddings, producing token features. Finally, we perform the same multi-head pooling strategy to obtain paragraph representations. 4 4.1 Experimental Setup We conduct experiments on two major datasets used in the literature of multi-document summarization, namely WikiSum (Liu et al., 2018) and MultiNews (Fabbri et al., 2019). WikiSum Dataset Liu et al. (2018) treat the generation of Wikipedia section titles as a supervised multi-document summarization task. Liu and Lapata (2019a) crawled Wikipedia articles and source reference documents through the provided urls. They further split the long and messy source documents into multiple paragraphs by line-breaks and select the top-40 paragraphs as input for summarization systems. However, the top-40 dataset is quite heavy for entity extraction and co-reference resolution. Experiment shows that the ROUGE-L recall of top-20 paragraphs against the gold target text is 53.8"
2021.findings-acl.30,D19-1428,0,0.0135799,"ormation and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. For recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. Fan et al. (2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. Huang et al. (2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. Li et al. (2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, Wang et al. (2020a) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extrac"
2021.findings-acl.30,D08-1019,0,0.0553414,"Extractive summarization methods produce summaries that are semantically similar to the original documents. Thus, they may be able to achieve relatively high ROUGE scores (Lin, 2004). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language"
2021.findings-acl.30,W18-2501,0,0.0160934,"e words as semantic units in addition to sentence nodes, acting as the intermediary to enrich the relationships between sentences. However, we argue that word-level semantic units are too fine and will bring huge computational costs. For multi-document summarization, models are usually required to process tens of documents. The total number of words will be vast, which further causes a hindrance for the graph construction and message passing process. Therefore, we use entity clusters as more advanced semantic units. We utilize the co-reference resolution tool (Lee et al., 2017) from AllenNLP (Gardner et al., 2018) to extract entity clusters. Note that we perform extraction globally, which means we concatenate all the documents into one long document. We denote the extracted entity clusters as C = {C1 , C2 , . . . , Cm }, where Ci = {mention1 , mention2 , . . . , mentionl }, and l is 353 Figure 2: Overall architecture of our model. output is xlw . the number of entity mentions in cluster Ci . 3.2 l−1 hlw = LayerNorm(xl−1 w + MHAttn(xw )) (1) Graph Construction Given a source document cluster D, we firstly divide them into smaller semantic units P = {P1 , P2 , . . . , Pn }, such as paragraphs and sentenc"
2021.findings-acl.30,D18-1443,0,0.0124164,"arization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and document understanding, respectively. Moreover, several general purpose sequence-to-sequence pre-trained models are proposed, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2019). T"
2021.findings-acl.30,P17-1147,0,0.0204335,"ous work (Liu and Lapata, 2019a), the evaluation score takes three aspects into account: (1) Informativeness: does the summary include salient 358 Dataset WikiSum MultiNews Model FT T-DMCA HT EMSum FT T-DMCA HS EMSum sults. Experiments on standard datasets show the effectiveness of our model. In the future, we would like to explore other approaches such as reinforcement learning based methods (Sharma et al., 2019) to further improve the summary quality in the context of multidocument summarization. We would also like to apply our method to other tasks such as multidocument question answering (Joshi et al., 2017). Rating -0.517 -0.117 0.250 0.383 -0.650 -0.033 0.317 0.367 Table 5: Human evaluation results on summary quality rating. FT, T-DMCA, HT, HS are baseline models explained in Section 4.2. parts of the input? (2) Fluency: Is the summary fluent and grammatical? (3) Succinctness: does redundancy occur in the summary? We used BestWorst Scaling (Louviere et al., 2015) because it has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2017). Annotators are presented with the gold summary and summaries generated from 3 out of 4 systems and decide which summary is"
2021.findings-acl.30,P17-2074,0,0.021197,"context of multidocument summarization. We would also like to apply our method to other tasks such as multidocument question answering (Joshi et al., 2017). Rating -0.517 -0.117 0.250 0.383 -0.650 -0.033 0.317 0.367 Table 5: Human evaluation results on summary quality rating. FT, T-DMCA, HT, HS are baseline models explained in Section 4.2. parts of the input? (2) Fluency: Is the summary fluent and grammatical? (3) Succinctness: does redundancy occur in the summary? We used BestWorst Scaling (Louviere et al., 2015) because it has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2017). Annotators are presented with the gold summary and summaries generated from 3 out of 4 systems and decide which summary is the best and which is the worst based on the criteria mentioned above. The rating of each system was computed as the percentage of times it was chosen as best minus the times it was selected as worst. Ratings range from -1 (worst) to 1 (best). On the WikiSum dataset, we choose FT, TDMCA, HT, EMSum and conduct human evaluation to compare their performance. On the MultiNews dataset, we choose FT, T-DMCA, HS, together with EMSum. The results are shown in Table 5. These resu"
2021.findings-acl.30,D18-1446,0,0.03189,"Missing"
2021.findings-acl.30,D17-1018,0,0.0278375,"Extraction Wang et al. (2020a) use words as semantic units in addition to sentence nodes, acting as the intermediary to enrich the relationships between sentences. However, we argue that word-level semantic units are too fine and will bring huge computational costs. For multi-document summarization, models are usually required to process tens of documents. The total number of words will be vast, which further causes a hindrance for the graph construction and message passing process. Therefore, we use entity clusters as more advanced semantic units. We utilize the co-reference resolution tool (Lee et al., 2017) from AllenNLP (Gardner et al., 2018) to extract entity clusters. Note that we perform extraction globally, which means we concatenate all the documents into one long document. We denote the extracted entity clusters as C = {C1 , C2 , . . . , Cm }, where Ci = {mention1 , mention2 , . . . , mentionl }, and l is 353 Figure 2: Overall architecture of our model. output is xlw . the number of entity mentions in cluster Ci . 3.2 l−1 hlw = LayerNorm(xl−1 w + MHAttn(xw )) (1) Graph Construction Given a source document cluster D, we firstly divide them into smaller semantic units P = {P1 , P2 , . . . ,"
2021.findings-acl.30,2020.acl-main.703,0,0.0327088,"Missing"
2021.findings-acl.30,D15-1219,0,0.0204091,"ively high ROUGE scores (Lin, 2004). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-l"
2021.findings-acl.30,2020.acl-main.555,0,0.177398,"eam sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models ∗ Corresponding author. that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018). Several previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a; Li et al., 2020). Such relations were shown useful in identifying the salient and redundant information from long documents, and can thus guide the summary generation process. However, while effective empirically, such approaches do not focus on explicitly modeling the underlying semantic information across documents. Entities and their mentions convey rich semantic information, and can be significant in summarization, especially when a specific entity is the topic under discussion for a set of documents. As shown in Figure 1, entity mentions frequently appear in the input article, and are playing unique role"
2021.findings-acl.30,D18-1441,0,0.0162434,"equires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and document understanding, respectively. Moreover, several general purpose sequence-to-sequence pre-trained models are proposed, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2019). They are further fi"
2021.findings-acl.30,W04-1013,0,0.12547,"undancy explicitly. The mechanism can also reduce the computational cost, making it easier to process long inputs. • Our model achieves state-of-the-art results on WikiSum and MultiNews. Extensive analysis including ablation studies show the effectiveness of our model.1 1 2 2.1 Related Work Abstractive Document Summarization Abstractive summarization is often regarded as the ultimate goal of document summarization research. Extractive summarization methods produce summaries that are semantically similar to the original documents. Thus, they may be able to achieve relatively high ROUGE scores (Lin, 2004). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pi"
2021.findings-acl.30,P19-1500,0,0.0892904,"marization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models ∗ Corresponding author. that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018). Several previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a; Li et al., 2020). Such relations were shown useful in identifying the salient and redundant information from long documents, and can thus guide the summary generation process. However, while effective empirically, such approaches do not focus on explicitly modeling the underlying semantic information across documents. Entities and their mentions convey rich semantic information, and can be significant in summarization, especially when a specific entity is the topic under discussion for a set of documents. As shown in Figure 1, entity mentions frequently appear in the input article, and are"
2021.findings-acl.30,D19-1387,0,0.100233,"marization, the mainstream sequence-to-sequence models, which can perform well on single-document summarization, often struggle with extracting salient information and handling redundancy in the presence of multiple, long documents. Thus, simply adopting models ∗ Corresponding author. that were shown effective for single-document summarization to the multi-document setup may not lead to ideal results (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018). Several previous research efforts have shown that modeling cross-document relations is essential in multi-document summarization (Liu and Lapata, 2019a; Li et al., 2020). Such relations were shown useful in identifying the salient and redundant information from long documents, and can thus guide the summary generation process. However, while effective empirically, such approaches do not focus on explicitly modeling the underlying semantic information across documents. Entities and their mentions convey rich semantic information, and can be significant in summarization, especially when a specific entity is the topic under discussion for a set of documents. As shown in Figure 1, entity mentions frequently appear in the input article, and are"
2021.findings-acl.30,2021.ccl-1.108,0,0.0516067,"Missing"
2021.findings-acl.30,D18-1206,0,0.0226576,"on which is composed of an entity-aware content selection module and a summary generation module. By contrast, our EMSum model is an end-to-end method for multi-document summarization. Gunel et al. (2020) inject structural world knowledge from Wikidata to a transformer-based model, enabling the model to be more fact-aware. Zhu et al. (2020) extract factual relations from the source texts to build a local knowledge graph and integrated it into the transformer-based model. Apart from entity or fact information, there are several works that incorporate topic information into summarization model. Narayan et al. (2018) recommend an encoder associating each word with a topic vector capturing whether it is representative of the document’s content, and a decoder where each word prediction is conditioned on a document topic vector. Zheng et al. (2019) propose to mine cross-document subtopics. In their work, sentence salience is estimated in a hierarchical way with subtopic salience and relative sentence salience. Perez-Beltrachini et al. (2019) explicitly model the topic structure of summaries, and utilize it to guide a structured convolutional decoder. Wang et al. (2020b) rearrange and further explore the sema"
2021.findings-acl.30,P19-1504,0,0.0116176,"aph and integrated it into the transformer-based model. Apart from entity or fact information, there are several works that incorporate topic information into summarization model. Narayan et al. (2018) recommend an encoder associating each word with a topic vector capturing whether it is representative of the document’s content, and a decoder where each word prediction is conditioned on a document topic vector. Zheng et al. (2019) propose to mine cross-document subtopics. In their work, sentence salience is estimated in a hierarchical way with subtopic salience and relative sentence salience. Perez-Beltrachini et al. (2019) explicitly model the topic structure of summaries, and utilize it to guide a structured convolutional decoder. Wang et al. (2020b) rearrange and further explore the semantics of the topic model and develope a friendly topic assistant for transfomer-based abstractive summarization models. 3 Model Our model is illustrated in Figure 2, which follows the transformer-based encoder-decoder architecture (Vaswani et al., 2017). We modify the encoder with graph neural networks, so we can incorporate entity information and graph representations at the same time. We design a novel two-level decoding pro"
2021.findings-acl.30,P14-1084,0,0.0171724,"4). However, sentence-level extraction lacks flexibility and tends to produce redundant information. By contrast, the process of abstractive summarization is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and do"
2021.findings-acl.30,P17-1099,0,0.297658,"ation is more similar to the human summarization process and requires more sophisticated natural language understanding and generation techniques. Traditional approaches to abstractive summarization can be divided into sentence fusion-based (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Banerjee et al., 2015), paraphrasing-based (Bing et al., 2015; Cohn and Lapata, 2009) and information extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and document understanding, respectively. Moreover, several general purpose sequence-to-sequence pre-trained models are proposed, such as T5 (Raffel et"
2021.findings-acl.30,D19-1323,0,0.0661926,"ractive summarization. Our work is partly similar to theirs, but we construct heterogeneous graphs composed of text unit nodes and entity nodes for abstractive multi-document summarization. 2.3 Summarization with Additional Features In addition to the direct application of the general sequence-to-sequence framework, researchers attempted to incorporate various features into summarization. Cao et al. (2018) extract actual fact descriptions from the source text and propose a dual-attention mechanism to force the generation conditioned on both the source text and the extracted fact descriptions. Sharma et al. (2019) take a pipeline method for single-document summarization which is composed of an entity-aware content selection module and a summary generation module. By contrast, our EMSum model is an end-to-end method for multi-document summarization. Gunel et al. (2020) inject structural world knowledge from Wikidata to a transformer-based model, enabling the model to be more fact-aware. Zhu et al. (2020) extract factual relations from the source texts to build a local knowledge graph and integrated it into the transformer-based model. Apart from entity or fact information, there are several works that i"
2021.findings-acl.30,P17-1108,0,0.0552391,"Missing"
2021.findings-acl.30,D08-1079,0,0.0497617,"red for abstractive text summarization. Zou et al. (2020) present three sequence-to-sequence pre-training objectives by reinstating source text for abstractive summarization. Our code is at https://github.com/Oceandam/EMSum 352 2.2 Graph-based Document Summarization Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan and Radev, 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Wan (2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. For recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convo"
2021.findings-acl.30,2020.acl-main.553,0,0.0258088,"Missing"
2021.findings-acl.30,P13-1137,0,0.054895,"Missing"
2021.findings-acl.30,2020.emnlp-main.35,0,0.152198,"playing unique roles that contribute towards the coherence and conciseness of the text. We believe that entities can be regarded 351 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 351–362 August 1–6, 2021. ©2021 Association for Computational Linguistics as the indicator of saliency and can be used to reduce redundancy. This motivates us to propose an entity-aware abstractive multi-document summarization model that effectively encodes relations across documents with the help of entities, and explicitly solve the issues of saliency and redundancy. Inspired by Wang et al. (2020a), we build a heterogeneous graph that consists of nodes that represent documents and entities. The entity nodes can serve as bridges that connect different documents – we can model the relations across documents through entity clusters. We apply the graph attention network (GAT) (Veliˇckovi´c et al., 2017) to enable information flow between nodes and iteratively update the node representations. In the decoding process, we design a novel two-level attention mechanism. The decoder first attends to the entities. Next, the attention weights of entities are incorporated with graph edge weights to"
2021.findings-acl.30,K17-1045,0,0.0190012,"tes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Wan (2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. For recent methods based on graph neural networks, Tan et al. (2017) propose a graph-based attention mechanism to identify salient sentences. Yasunaga et al. (2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. Fan et al. (2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. Huang et al. (2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. Li et al. (2020) utilize homogeneous graphs to capture cross-document relations and guide the su"
2021.findings-acl.30,P19-1499,0,0.0191991,"tion extraction-based (Li, 2015; Wang and Cardie, 2013; Pighin et al., 2014). With the development of neural-based methods, abstractive methods achieved promising results on single document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Li et al., 2018). More recently, due to the excellent performance on various text generation tasks, transformer-based methods become the mainstream approach for abstractive multi-document summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and document understanding, respectively. Moreover, several general purpose sequence-to-sequence pre-trained models are proposed, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2019). They are further finetuned for the summarization task. Zhang et al. (2020) propose PEGASUS, in which they design a pre-training objective tailored for abstractive text summarization. Zou et al. (2020) present three sequence-to-sequence pre-training objectives by reinstating source text for abstractive summarization. Our code is at https://github.com/Oce"
2021.findings-acl.30,D19-1311,0,0.0342774,"Missing"
2021.findings-acl.30,2020.emnlp-main.297,1,0.709786,"ocument summarization, as well as pre-trained language models. Liu and Lapata (2019b) propose BertSUM for both extractive and abstractive summarization. Zhang et al. (2019) build low-level and high-level Berts for sentence and document understanding, respectively. Moreover, several general purpose sequence-to-sequence pre-trained models are proposed, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2019). They are further finetuned for the summarization task. Zhang et al. (2020) propose PEGASUS, in which they design a pre-training objective tailored for abstractive text summarization. Zou et al. (2020) present three sequence-to-sequence pre-training objectives by reinstating source text for abstractive summarization. Our code is at https://github.com/Oceandam/EMSum 352 2.2 Graph-based Document Summarization Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan and Radev, 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Wan (2008) further incorporate the document-level informa"
2021.naacl-main.271,C18-1139,0,0.0135403,"P RODUCT entity if a model relies more on the context “begin trading with” but ignores the hidden information that “PCP” is the symbol of “Precision Castparts Corp.”. Previous research works (Li et al., 2017; Jie and 1 Introduction Lu, 2019; Wang et al., 2019) have been using the Named entity recognition (NER) is one of the parse trees (Chomsky, 1956, 1969; Sandra and Taft, most fundamental and important tasks in natu- 2014) to incorporate such structured information. ral language processing (NLP). While the litera- Figure 1 (Dependency Path) shows that the first enture (Peters et al., 2018; Akbik et al., 2018; De- tity can be connected to the second entity following vlin et al., 2019) largely focuses on training deep the dependency tree with 5 hops. Incorporating the language models to improve the contextualized dependency information can be done with graph word representations, previous studies show that neural networks (GNNs) such as graph convolutional networks (GCNs) (Kipf and Welling, 2017). ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. The However, simply stacking the LSTM and GCN work was done when Zhanming Jie was a PhD student in"
2021.naacl-main.271,Q16-1026,0,0.0927949,"r GCN. Without GCN at all, the score drops by 1.13 F1 . The original dependency contributes 0.27 F1 score. Removing the dependency relation embedding also decreases the performance by 0.27 F1 . When we remove the POS tags embedding, the result drops by 0.39 F1 . NER Early work (Sasano and Kurohashi, 2008) uses syntactic dependency features to improve the SVM performance on Japanese NER task. Liu et al. (2010) propose to construct skip-edges to link similar words or words having typed dependencies to capture long-range dependencies. The later works (Collobert et al., 2010; Lample et al., 2016; Chiu and Nichols, 2016b) focus on using neural networks to extract features and achieved the stateof-the-art performance. Jie et al. (2017) find that some relations between the dependency edges and the entities can be used to reduce the search space of their model, which significantly reduces the time complexity. Yu et al. (2020) employ pre-trained language model to encode document-level information to explore all spans with the graph-based dependency graph based ideas. The pre-trained language models (e.g., BERT (Devlin et al., 2019), ELMO (Peters et al., 2018)) further improve neuralbased approaches with a good c"
2021.naacl-main.271,C18-1161,0,0.0375604,"Missing"
2021.naacl-main.271,L18-1550,0,0.014406,"al. Following the work 3460 by Jie and Lu (2019), we transform the parse trees into the Stanford dependency trees (De Marneffe and Manning, 2008) by using Stanford CoreNLP (Manning et al., 2014). Detailed statistics of each dataset can be found in Table 1. Intuitively, longer sentences would require the model to capture more long-distance interactions in the sentences. We present the number of entities in terms of different sentence lengths to show that these datasets have a modest amount of entities in long sentences. Experimental Setup For Catalan, Spanish, and Chinese, we use the FastText (Grave et al., 2018) 300 dimensional embeddings to initialize the word embeddings. For OntoNotes 5.0 English, we adopt the publicly available GloVE (Pennington et al., 2014) 100 dimensional embeddings to initialize the word embeddings. For experiments with the contextualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) to generate the contextualized word representation without fine-tuning. Following Luo et al. (2020), we use the cased version of BERT large model for the experiments on the OntoNotes 5.0 E"
2021.naacl-main.271,D19-1399,1,0.347676,"ne with graph word representations, previous studies show that neural networks (GNNs) such as graph convolutional networks (GCNs) (Kipf and Welling, 2017). ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. The However, simply stacking the LSTM and GCN work was done when Zhanming Jie was a PhD student in architectures for NER can only provide us with Singapore University of Technology and Design. 1 modest improvements; sometimes, it decreases perWe make our code publicly available at https:// github.com/xuuuluuu/SynLSTM-for-NER. formance (Jie and Lu, 2019). Based on the depen3457 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3457–3469 June 6–11, 2021. ©2021 Association for Computational Linguistics dency path in Figure 1, it requires a 5-layer GCN to capture the connections between these two entities. However, deep GCN architectures often face training difficulties, which cause a performance drop (Hamilton et al., 2017b; Kipf and Welling, 2017). Directly stacking GCN and LSTM has difficulties in modeling the interaction between dependency tre"
2021.naacl-main.271,N16-1030,0,0.472356,"he number of state ˜st to represent the cell state that corresponds words. Our model is mainly constructed with three to the graph-encoded representation separately. layers: input representation layer, bi-directional With the proposed Syn-LSTM, the structured Syn-LSTM layer, and CRF layer. The architecture information captured by the dependency trees can of our Syn-LSTM-CRF is shown in Figure 3. be passed to each cell, and the additional gate mt is able to control how much structured information Input Representation Layer Similar to the can be incorporated. The additional gate enables work by Lample et al. (2016), our input representathe model to feed the contextual and structured tion also includes the character embeddings, which information into the LSTM cell separately. Such are the hidden states of character-based BiLSTM. a mechanism allows our model to aggregate the Jie and Lu (2019) highlight that the dependency information from linear sequence and dependency relation helps to enhance the input representation. trees selectively. Furthermore, previous methods (Wang et al., 2018; 3459 Wang and Lu, 2018) use embeddings of part-ofspeech (POS) tags as additional input representation. The input repres"
2021.naacl-main.271,P18-2116,0,0.0181568,"affected by the graph-encoded representation gt . The cell state ct and hidden state ht are computed as follows: ft = σ(W (f ) xt + U (f ) ht−1 + Q(f ) gt + b(f ) ) (1) ot = σ(W (o) xt + U (o) ht−1 + Q(o) gt + b(o) ) (2) it = σ(W (i) xt + U (i) (i) ht−1 + b ) yt−1 yt yt+1 yt+2 Syn-LSTM Syn-LSTM Syn-LSTM Syn-LSTM xt-1 L gt-1 (4) 0 gt-1 gt0 = + t X (ij 0 gt+2 t X t Y (mj t X ht = ot tanh(ct ) (8) j=0 (9) fk ) ˜cj k=j+1 (6) = t Y j=0 ˜st = tanh(W (n) gt + U (n) ht−1 + b(n) ) ct = ft ct−1 + it ˜ct + mt ˜st Q(·) 0 gt+1 ct = ft ct−1 + it ˜ct + mt ˜st j=0 U (·) , L gt+2 Similar to the previous work (Levy et al., 2018), it is also possible to show that the cell state ct implicitly computes the element-wise weighted sum of the previous states by expanding Equation 7: (5) W (·) , xt+2 Figure 3: Syn-LSTM-CRF architecture. ˜ct = tanh(W (u) xt + U (u) ht−1 + b(u) ) (7) L gt+1 xt+1 Graph Convolutional Network (3) mt = σ(W (m) gt + U (m) ht−1 + b(m) ) gtL xt fk ) ˜sj (10) k=j+1 atj ˜cj + t X qtj ˜sj (11) j=0 Note that the two terms, atj and qtj , are the product of gates. The value of the two terms are in the range from 0 to 1. Since the ˜ct and ˜st represent contextual and structured features, the corresponding w"
2021.naacl-main.271,D17-1282,0,0.0187283,"However, sequence models such as bidirectional LSTM (Hochreiter and Schmidhuber, 1997) are not able to fully capture the long-range dependencies (Bengio, 2009). For instance, Figure 1 (top) shows one type of structured information in NER. The words “Precision Castparts Corp.” can be easily inferred as O RGANIZATION by its context (i.e., Corp.). However, the second entity “PCP” could be misclassified as a P RODUCT entity if a model relies more on the context “begin trading with” but ignores the hidden information that “PCP” is the symbol of “Precision Castparts Corp.”. Previous research works (Li et al., 2017; Jie and 1 Introduction Lu, 2019; Wang et al., 2019) have been using the Named entity recognition (NER) is one of the parse trees (Chomsky, 1956, 1969; Sandra and Taft, most fundamental and important tasks in natu- 2014) to incorporate such structured information. ral language processing (NLP). While the litera- Figure 1 (Dependency Path) shows that the first enture (Peters et al., 2018; Akbik et al., 2018; De- tity can be connected to the second entity following vlin et al., 2019) largely focuses on training deep the dependency tree with 5 hops. Incorporating the language models to improve t"
2021.naacl-main.271,N19-1423,0,0.638236,"on helps to enhance the input representation. trees selectively. Furthermore, previous methods (Wang et al., 2018; 3459 Wang and Lu, 2018) use embeddings of part-ofspeech (POS) tags as additional input representation. The input representation xt of our model is the concatenation of the word embedding vt , the character representation et , the dependency relation embedding rt , and the POS embedding pt : xt = [vt ; et ; rt ; pt ] (12) where both rt and pt embeddings are randomly initialized and are fine-tuned during training. For experiments with the contextualized representations (e.g., BERT (Devlin et al., 2019)), we further concatenate the contextual word representation to xt . For our task, we employ the graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018b) to get the graph-encoded representation gt . Given a graph, an adjacency matrix A of size n × n is able to represent the graph structure, where n is the number of nodes; Ai,j = 1 indicates that node i and node j are connected. We transform dependency tree into its corresponding adjacency matrix3 A, and Ai,j = 1 denotes that node i and node j have dependency relation. Note that the purpose of graph-encoded representation gt is"
2021.naacl-main.271,2020.acl-main.519,0,0.0531685,"Missing"
2021.naacl-main.271,2020.acl-main.45,0,0.0243537,"Missing"
2021.naacl-main.271,P05-1045,0,0.11316,"r further analysis demonstrates that our model can capture longer dependencies compared with strong baselines.1 Precision Castparts Corp. , Portlan , will begin trading with the symbol PCP . O RG O RG P RODUCT ? Dependency Path: Corp. begin trading with symbol PCP neighbor context Hybrid Paths: Corp. begin . PCP OR Corp. begin trading with the symbol PCP Figure 1: A sentence annotated with dependency trees and named entities. The paths to connect two entities are shown below the sentence. the structured information such as interactions between non-adjacent words can also be important for NER (Finkel et al., 2005; Jie et al., 2017; Aguilar and Solorio, 2019). However, sequence models such as bidirectional LSTM (Hochreiter and Schmidhuber, 1997) are not able to fully capture the long-range dependencies (Bengio, 2009). For instance, Figure 1 (top) shows one type of structured information in NER. The words “Precision Castparts Corp.” can be easily inferred as O RGANIZATION by its context (i.e., Corp.). However, the second entity “PCP” could be misclassified as a P RODUCT entity if a model relies more on the context “begin trading with” but ignores the hidden information that “PCP” is the symbol of “Preci"
2021.naacl-main.271,W10-1902,0,0.0338542,"on the OntoNotes 5.0 English dataset, and Table 7 presents the detailed results of our model with contextualized representation. We find that the performance drops by 0.24 F1 score when we only use 1-layer GCN. Without GCN at all, the score drops by 1.13 F1 . The original dependency contributes 0.27 F1 score. Removing the dependency relation embedding also decreases the performance by 0.27 F1 . When we remove the POS tags embedding, the result drops by 0.39 F1 . NER Early work (Sasano and Kurohashi, 2008) uses syntactic dependency features to improve the SVM performance on Japanese NER task. Liu et al. (2010) propose to construct skip-edges to link similar words or words having typed dependencies to capture long-range dependencies. The later works (Collobert et al., 2010; Lample et al., 2016; Chiu and Nichols, 2016b) focus on using neural networks to extract features and achieved the stateof-the-art performance. Jie et al. (2017) find that some relations between the dependency edges and the entities can be used to reduce the search space of their model, which significantly reduces the time complexity. Yu et al. (2020) employ pre-trained language model to encode document-level information to explor"
2021.naacl-main.271,P16-1101,0,0.0548691,"textual information well. 7 Conclusion In this paper, we propose a simple and robust SynLSTM model to better integrate the structured information leveraged from the long-range dependencies. Specifically, we introduce an additional graph6 Related Work encoded representation to each recurrent unit. Such LSTM LSTM has demonstrated its great effec- a graph-encoded representation can be obtained via tiveness in many NLP tasks and becomes a stan- GNNs. Through the newly designed gating mechdard module for many state-of-the-art models (Wen anism, the hidden states are enhanced by contexet al., 2015; Ma and Hovy, 2016; Dozat and Man- tual information captured by the linear sequence ning, 2017). However, the sequential nature of the and structured information captured by the depenLSTM makes it challenging to capture long-range dency trees. We present the Syn-LSTM-CRF for dependencies. Zhang et al. (2018a) propose the NER and adopt the GCN on dependency trees to S-LSTM model to include a sentence state to allow obtain the graph-encoded representations. Our exboth local and global information exchange simul- tensive experiments and analysis on the datasets taneously. Mogrifier LSTM (Melis et al., 2020) with f"
2021.naacl-main.271,P14-5010,0,0.0119785,"f our approach when dependency trees of from both directions. We concatenate the hidden different qualities are used. For SemEval 2010 Task → − state ht from forward Syn-LSTM and hidden state 1 datasets, there are 4 entity types: P ER, L OC and 3 O RG and M ISC. For OntoNotes 5.0 datasets, there We treat the dependency edge as undirected and add a self-loop for each node: Ai,j = Aj,i and Ai,i = 1. are 18 entity types in total. Following the work 3460 by Jie and Lu (2019), we transform the parse trees into the Stanford dependency trees (De Marneffe and Manning, 2008) by using Stanford CoreNLP (Manning et al., 2014). Detailed statistics of each dataset can be found in Table 1. Intuitively, longer sentences would require the model to capture more long-distance interactions in the sentences. We present the number of entities in terms of different sentence lengths to show that these datasets have a modest amount of entities in long sentences. Experimental Setup For Catalan, Spanish, and Chinese, we use the FastText (Grave et al., 2018) 300 dimensional embeddings to initialize the word embeddings. For OntoNotes 5.0 English, we adopt the publicly available GloVE (Pennington et al., 2014) 100 dimensional embed"
2021.naacl-main.271,D14-1162,0,0.0968802,"using Stanford CoreNLP (Manning et al., 2014). Detailed statistics of each dataset can be found in Table 1. Intuitively, longer sentences would require the model to capture more long-distance interactions in the sentences. We present the number of entities in terms of different sentence lengths to show that these datasets have a modest amount of entities in long sentences. Experimental Setup For Catalan, Spanish, and Chinese, we use the FastText (Grave et al., 2018) 300 dimensional embeddings to initialize the word embeddings. For OntoNotes 5.0 English, we adopt the publicly available GloVE (Pennington et al., 2014) 100 dimensional embeddings to initialize the word embeddings. For experiments with the contextualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) to generate the contextualized word representation without fine-tuning. Following Luo et al. (2020), we use the cased version of BERT large model for the experiments on the OntoNotes 5.0 English data. We use the cased version of BERT base model for the experiments on the other three datasets. For the character embedding, we randomly initia"
2021.naacl-main.271,N18-1202,0,0.698056,"be misclassified as a P RODUCT entity if a model relies more on the context “begin trading with” but ignores the hidden information that “PCP” is the symbol of “Precision Castparts Corp.”. Previous research works (Li et al., 2017; Jie and 1 Introduction Lu, 2019; Wang et al., 2019) have been using the Named entity recognition (NER) is one of the parse trees (Chomsky, 1956, 1969; Sandra and Taft, most fundamental and important tasks in natu- 2014) to incorporate such structured information. ral language processing (NLP). While the litera- Figure 1 (Dependency Path) shows that the first enture (Peters et al., 2018; Akbik et al., 2018; De- tity can be connected to the second entity following vlin et al., 2019) largely focuses on training deep the dependency tree with 5 hops. Incorporating the language models to improve the contextualized dependency information can be done with graph word representations, previous studies show that neural networks (GNNs) such as graph convolutional networks (GCNs) (Kipf and Welling, 2017). ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. The However, simply stacking the LSTM and GCN work was done when Zhanming Jie"
2021.naacl-main.271,D15-1199,0,0.0401775,"Missing"
2021.naacl-main.271,2020.acl-main.577,0,0.0122708,"tactic dependency features to improve the SVM performance on Japanese NER task. Liu et al. (2010) propose to construct skip-edges to link similar words or words having typed dependencies to capture long-range dependencies. The later works (Collobert et al., 2010; Lample et al., 2016; Chiu and Nichols, 2016b) focus on using neural networks to extract features and achieved the stateof-the-art performance. Jie et al. (2017) find that some relations between the dependency edges and the entities can be used to reduce the search space of their model, which significantly reduces the time complexity. Yu et al. (2020) employ pre-trained language model to encode document-level information to explore all spans with the graph-based dependency graph based ideas. The pre-trained language models (e.g., BERT (Devlin et al., 2019), ELMO (Peters et al., 2018)) further improve neuralbased approaches with a good contextualized representation. However, previous works did not focus on investigating how to effectively integrate structured and contextual information well. 7 Conclusion In this paper, we propose a simple and robust SynLSTM model to better integrate the structured information leveraged from the long-range d"
2021.naacl-main.271,P18-1030,0,0.0790404,"(POS) tags as additional input representation. The input representation xt of our model is the concatenation of the word embedding vt , the character representation et , the dependency relation embedding rt , and the POS embedding pt : xt = [vt ; et ; rt ; pt ] (12) where both rt and pt embeddings are randomly initialized and are fine-tuned during training. For experiments with the contextualized representations (e.g., BERT (Devlin et al., 2019)), we further concatenate the contextual word representation to xt . For our task, we employ the graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018b) to get the graph-encoded representation gt . Given a graph, an adjacency matrix A of size n × n is able to represent the graph structure, where n is the number of nodes; Ai,j = 1 indicates that node i and node j are connected. We transform dependency tree into its corresponding adjacency matrix3 A, and Ai,j = 1 denotes that node i and node j have dependency relation. Note that the purpose of graph-encoded representation gt is to incorporate the dependency information from neighbor nodes. The input and output representations of the l-th layer GCN at t-th position are denoted as gtl−1 and gtl"
2021.naacl-main.271,P18-1144,0,0.038964,"Missing"
2021.naacl-main.271,D17-1283,0,0.0391916,"Missing"
C14-1122,Q13-1005,0,0.0387222,"language and semantics. An example hybrid tree representation is shown in Figure 2. Jones et al. (2012b) recently proposed a framework that performs semantic parsing with tree transducers. The model learns representations that are similar to the hybrid tree structures using a generative process under a Bayesian framework. Besides these approaches, recently there are also several works that take alternative learning approaches for semantic parsing which do not require annotated semantic representations (Poon and Domingos, 2009; Clarke et al., 2010; Goldwasser et al., 2011; Liang et al., 2013; Artzi and Zettlemoyer, 2013). Most of such approaches rely on either weak supervision or certain forms of indirect supervision. Some of these works also focus on optimizing specific downstream tasks rather than the semantic parsing task itself. 1293 ma ma w9 w10 mb mc w3 w4 w5 w1 w2 md mb w1 w2 w3 w4 w5 w7 w8 mc w6 w6 w7 md w10 w8 w9 Figure 3: Two example hybrid trees. Their leaves are natural language words, and the internal nodes are semantic units. Both hybrid trees correspond to the same n-m pair hw1 w2 w3 w4 w5 w6 w7 w8 w9 w10 , ma (mb (mc , md ))i. Thus they can be viewed as two different ways of generating such a"
C14-1122,W09-1206,0,0.134733,"Missing"
C14-1122,D07-1007,0,0.0333349,"nerative process. We note there also exist various multilingual or cross-lingual semantic processing works. Most of such works focus on semantic role labeling(SRL), the task of recovery of shallow meaning. Examples include multilingual semantic role labeling (Bj¨orkelund et al., 2009), multilingual joint syntactic and semantic dependency parsing (Henderson et al., 2013), and cross-lingual transfer of semantic role labeling models (Kozhevnikov and Titov, 2013). Researchers also looked into exploiting semantic information for bilingual processing such as machine translations (Chan et al., 2007; Carpuat and Wu, 2007; Jones et al., 2012a). In this work, we focus on the task of multilingual semantic parsing under the setting where the input consists of semantically equivalent sentences from multiple different languages, and the outputs are formal semantic representations. We specifically focus on the hybrid tree model, a state-of-the-art framework for semantic parsing. We first make an extension to the model, and investigate methods for performing such a multilingual semantic parsing task by aggregating a few variants of the models under such a framework. 3 Approach In this section, we first discuss the hy"
C14-1122,P07-1005,0,0.0199727,"r from the joint generative process. We note there also exist various multilingual or cross-lingual semantic processing works. Most of such works focus on semantic role labeling(SRL), the task of recovery of shallow meaning. Examples include multilingual semantic role labeling (Bj¨orkelund et al., 2009), multilingual joint syntactic and semantic dependency parsing (Henderson et al., 2013), and cross-lingual transfer of semantic role labeling models (Kozhevnikov and Titov, 2013). Researchers also looked into exploiting semantic information for bilingual processing such as machine translations (Chan et al., 2007; Carpuat and Wu, 2007; Jones et al., 2012a). In this work, we focus on the task of multilingual semantic parsing under the setting where the input consists of semantically equivalent sentences from multiple different languages, and the outputs are formal semantic representations. We specifically focus on the hybrid tree model, a state-of-the-art framework for semantic parsing. We first make an extension to the model, and investigate methods for performing such a multilingual semantic parsing task by aggregating a few variants of the models under such a framework. 3 Approach In this section, w"
C14-1122,J07-2003,0,0.223025,"imultaneously parsing semantically equivalent natural language texts in different languages into their underlying semantics. Specifically, in this work, we first introduce a new variant of a semantic parsing model under an existing framework. This new variant can be used together with other models for jointly making semantic parsing predictions, leading to an improved multilingual semantic parsing system. We demonstrate the effectiveness of this new variant through experiments. Although bilingual parsing has been extensively studied in fields such as statistical machine translation (Wu, 1997; Chiang, 2007), to the best of our knowledge, bilingual or multilingual semantic parsing that focuses on parsing sentences from multiple different languages into their formal semantic representations has not yet been studied. We present the very first work on performing multilingual semantic parsing that simultaneously parses semantically equivalent sentences from multiple different languages into their semantics. We believe this line of work can potentially lead to further developments and advancements in areas such as multilingual semantic processing and semantics-based machine translations (Jones et al.,"
C14-1122,W10-2903,0,0.031657,"istribution over the latent hybrid trees which jointly encode both language and semantics. An example hybrid tree representation is shown in Figure 2. Jones et al. (2012b) recently proposed a framework that performs semantic parsing with tree transducers. The model learns representations that are similar to the hybrid tree structures using a generative process under a Bayesian framework. Besides these approaches, recently there are also several works that take alternative learning approaches for semantic parsing which do not require annotated semantic representations (Poon and Domingos, 2009; Clarke et al., 2010; Goldwasser et al., 2011; Liang et al., 2013; Artzi and Zettlemoyer, 2013). Most of such approaches rely on either weak supervision or certain forms of indirect supervision. Some of these works also focus on optimizing specific downstream tasks rather than the semantic parsing task itself. 1293 ma ma w9 w10 mb mc w3 w4 w5 w1 w2 md mb w1 w2 w3 w4 w5 w7 w8 mc w6 w6 w7 md w10 w8 w9 Figure 3: Two example hybrid trees. Their leaves are natural language words, and the internal nodes are semantic units. Both hybrid trees correspond to the same n-m pair hw1 w2 w3 w4 w5 w6 w7 w8 w9 w10 , ma (mb (mc ,"
C14-1122,W05-0602,0,0.0956594,"Substantial research efforts have focused on building monolingual semantic parsing systems. We survey in this section several of them. WASP (Wong and Mooney, 2006) is a model motivated by statistical synchronous parsing-based machine translation (Chiang, 2007), which essentially casts the semantic parsing problem as a phrase-based translation problem (Koehn et al., 2003). K RISP (Kate and Mooney, 2006) makes use of Support Vector Machines with string kernels (Lodhi et al., 2002) to recursively map contiguous word sequences into semantic units to construct a tree structure. The S CISSOR model (Ge and Mooney, 2005) performs integrated semantic and syntactic parsing. The model parses natural language sentences into semantically augmented parse trees whose nodes consist of both semantic and syntactic labels and then builds semantic representations based on such augmented trees. The hybrid tree model (Lu et al., 2008; Lu et al., 2009), whose code is publicly available, makes the assumption that there exists an underlying generative process for jointly producing both the language and semantics. The model employs efficient dynamic programming algorithms for learning a distribution over the latent hybrid tree"
C14-1122,P11-1149,0,0.0346618,"latent hybrid trees which jointly encode both language and semantics. An example hybrid tree representation is shown in Figure 2. Jones et al. (2012b) recently proposed a framework that performs semantic parsing with tree transducers. The model learns representations that are similar to the hybrid tree structures using a generative process under a Bayesian framework. Besides these approaches, recently there are also several works that take alternative learning approaches for semantic parsing which do not require annotated semantic representations (Poon and Domingos, 2009; Clarke et al., 2010; Goldwasser et al., 2011; Liang et al., 2013; Artzi and Zettlemoyer, 2013). Most of such approaches rely on either weak supervision or certain forms of indirect supervision. Some of these works also focus on optimizing specific downstream tasks rather than the semantic parsing task itself. 1293 ma ma w9 w10 mb mc w3 w4 w5 w1 w2 md mb w1 w2 w3 w4 w5 w7 w8 mc w6 w6 w7 md w10 w8 w9 Figure 3: Two example hybrid trees. Their leaves are natural language words, and the internal nodes are semantic units. Both hybrid trees correspond to the same n-m pair hw1 w2 w3 w4 w5 w6 w7 w8 w9 w10 , ma (mb (mc , md ))i. Thus they can be"
C14-1122,J13-4006,0,0.0208243,"nguage words, and the internal nodes are semantic units. Both hybrid trees correspond to the same n-m pair hw1 w2 w3 w4 w5 w6 w7 w8 w9 w10 , ma (mb (mc , md ))i. Thus they can be viewed as two different ways of generating such a pair from the joint generative process. We note there also exist various multilingual or cross-lingual semantic processing works. Most of such works focus on semantic role labeling(SRL), the task of recovery of shallow meaning. Examples include multilingual semantic role labeling (Bj¨orkelund et al., 2009), multilingual joint syntactic and semantic dependency parsing (Henderson et al., 2013), and cross-lingual transfer of semantic role labeling models (Kozhevnikov and Titov, 2013). Researchers also looked into exploiting semantic information for bilingual processing such as machine translations (Chan et al., 2007; Carpuat and Wu, 2007; Jones et al., 2012a). In this work, we focus on the task of multilingual semantic parsing under the setting where the input consists of semantically equivalent sentences from multiple different languages, and the outputs are formal semantic representations. We specifically focus on the hybrid tree model, a state-of-the-art framework for semantic pa"
C14-1122,C12-1083,0,0.159781,"ing the multilingual semantic parsing task. We present our evaluations on a standard dataset annotated with sentences in multiple languages coming from different language families. 1 Introduction Semantic parsing, the task of parsing natural language sentences into their formal semantic representations (Mooney, 2007) is one of the most important tasks in the field of natural language processing and artificial intelligence. This area of research recently has received a significant amount of attention (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012b). Consider these example sentence-semantics pairs: English: Semantics: Which states have points that are higher than the highest point in Texas ? answer(state(loc1 (place(higher2 (highest(place(loc2 (stateid(0 T X 0 ))))))))) English: Semantics: What rivers do not run through Tennessee ? answer(exclude(river(all), traverse2 (stateid(0 T N 0 )))) In the typical setting, the semantic parser learns from a collection of such sentence-semantics pairs a model that can parse novel input sentences into their respective semantic representations. Such semantic representations can then be used to inter"
C14-1122,P12-1051,0,0.381904,"ing the multilingual semantic parsing task. We present our evaluations on a standard dataset annotated with sentences in multiple languages coming from different language families. 1 Introduction Semantic parsing, the task of parsing natural language sentences into their formal semantic representations (Mooney, 2007) is one of the most important tasks in the field of natural language processing and artificial intelligence. This area of research recently has received a significant amount of attention (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012b). Consider these example sentence-semantics pairs: English: Semantics: Which states have points that are higher than the highest point in Texas ? answer(state(loc1 (place(higher2 (highest(place(loc2 (stateid(0 T X 0 ))))))))) English: Semantics: What rivers do not run through Tennessee ? answer(exclude(river(all), traverse2 (stateid(0 T N 0 )))) In the typical setting, the semantic parser learns from a collection of such sentence-semantics pairs a model that can parse novel input sentences into their respective semantic representations. Such semantic representations can then be used to inter"
C14-1122,P06-1115,0,0.745087,"e framework, we then investigate several approaches for performing the multilingual semantic parsing task. We present our evaluations on a standard dataset annotated with sentences in multiple languages coming from different language families. 1 Introduction Semantic parsing, the task of parsing natural language sentences into their formal semantic representations (Mooney, 2007) is one of the most important tasks in the field of natural language processing and artificial intelligence. This area of research recently has received a significant amount of attention (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012b). Consider these example sentence-semantics pairs: English: Semantics: Which states have points that are higher than the highest point in Texas ? answer(state(loc1 (place(higher2 (highest(place(loc2 (stateid(0 T X 0 ))))))))) English: Semantics: What rivers do not run through Tennessee ? answer(exclude(river(all), traverse2 (stateid(0 T N 0 )))) In the typical setting, the semantic parser learns from a collection of such sentence-semantics pairs a model that can parse novel input sentences into their respective semantic representati"
C14-1122,N03-1017,0,0.00948189,"f specific types as arguments, and returns a new semantic representation of a particular type. For example, in Figure 1, the semantic unit at the root has a type Q UERY, a function name answer, and a single argument type R IVER. 2.2 Related Work Substantial research efforts have focused on building monolingual semantic parsing systems. We survey in this section several of them. WASP (Wong and Mooney, 2006) is a model motivated by statistical synchronous parsing-based machine translation (Chiang, 2007), which essentially casts the semantic parsing problem as a phrase-based translation problem (Koehn et al., 2003). K RISP (Kate and Mooney, 2006) makes use of Support Vector Machines with string kernels (Lodhi et al., 2002) to recursively map contiguous word sequences into semantic units to construct a tree structure. The S CISSOR model (Ge and Mooney, 2005) performs integrated semantic and syntactic parsing. The model parses natural language sentences into semantically augmented parse trees whose nodes consist of both semantic and syntactic labels and then builds semantic representations based on such augmented trees. The hybrid tree model (Lu et al., 2008; Lu et al., 2009), whose code is publicly avail"
C14-1122,P13-1117,0,0.0223027,"to the same n-m pair hw1 w2 w3 w4 w5 w6 w7 w8 w9 w10 , ma (mb (mc , md ))i. Thus they can be viewed as two different ways of generating such a pair from the joint generative process. We note there also exist various multilingual or cross-lingual semantic processing works. Most of such works focus on semantic role labeling(SRL), the task of recovery of shallow meaning. Examples include multilingual semantic role labeling (Bj¨orkelund et al., 2009), multilingual joint syntactic and semantic dependency parsing (Henderson et al., 2013), and cross-lingual transfer of semantic role labeling models (Kozhevnikov and Titov, 2013). Researchers also looked into exploiting semantic information for bilingual processing such as machine translations (Chan et al., 2007; Carpuat and Wu, 2007; Jones et al., 2012a). In this work, we focus on the task of multilingual semantic parsing under the setting where the input consists of semantically equivalent sentences from multiple different languages, and the outputs are formal semantic representations. We specifically focus on the hybrid tree model, a state-of-the-art framework for semantic parsing. We first make an extension to the model, and investigate methods for performing such"
C14-1122,D10-1119,0,0.289791,"he original G EO Q UERY dataset (Wong and Mooney, 2006; Kate and Mooney, 2006) contains natural language queries in English only. Additional Chinese annotations were provided by Lu and Ng (2011) when performing a natural language generation task. Jones et al. (2012b) further provided the following three additional language annotations to this dataset: German, Greek and Thai. Thus, this dataset is now fully annotated with five different languages, two of which (Chinese, Thai) are Sino-Tibetan languages, and the rest are all Indo-European languages. Following previous works on semantic parsing (Kwiatkowski et al., 2010; Jones et al., 2012b), we split the dataset into two portions. The training set consists of 600 instances, and we report evaluation results on the portion consisting of the remaining 280 instances. We used the identical split provided by Jones et al. (2012b) for all the experiments. Following previous works, we used the standard approach for 1296 Unigram Bigram Bigram (inv) Mixgram Voting (u,b,m) Voting (u,b,bi) Aggregation EN DE EL TH CN 70.0 75.4 74.3 76.1 76.1 76.4 78.6 59.6 56.1 57.1 62.5 61.1 61.4 60.0 70.0 65.4 65.4 69.3 70.4 71.8 72.1 68.9 70.7 71.1 73.2 73.6 74.3 71.4 68.9 68.9 66.8 7"
C14-1122,J13-2005,0,0.294154,"semantic representations (Wong and Mooney, 2006; Kate and Mooney, 1292 Q UERY : answer(R IVER) What R IVER : exclude(R IVER, R IVER) R IVER : river(all) rivers do not ? R IVER : traverse(S TATE) run through S TATE : stateid(S TATE NAME) S TATE NAME : (0 T N 0 ) Tennessee Figure 2: An example hybrid tree. Such a hybrid tree is generated from the generative process, and captures the correspondences between natural language words and semantic units. 2006), the lambda calculus expressions (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), and dependency-based semantic representations (DCS) (Liang et al., 2013). In this work, we specifically focus on the tree-structured representations for semantics. Each semantic representation consists of semantic units as its tree nodes, where each semantic unit is of the following form: ma ≡ τa : pα (τb ∗) (1) Here ma is used to denote a complete semantic unit, which consists of its semantic type τa , its function symbol pα , as well as a list of types for argument semantic units τb ∗ (here ∗ means 0, 1, or 2; we assume there are at most two arguments for each semantic unit). In other words, each semantic unit can be regarded as a function which takes in other s"
C14-1122,D11-1149,1,0.901648,"ct the output with the highest overall score j s˜(j) as the final output of our system. 4 4.1 Experiments Data and Setup We conducted our experiments on the multilingual G EO Q UERY dataset released by Jones et al. (2012b). This dataset consists of 880 instances of natural language queries related to US geography facts. Each query is coupled with its corresponding semantic representation originally written in Prolog. The original G EO Q UERY dataset (Wong and Mooney, 2006; Kate and Mooney, 2006) contains natural language queries in English only. Additional Chinese annotations were provided by Lu and Ng (2011) when performing a natural language generation task. Jones et al. (2012b) further provided the following three additional language annotations to this dataset: German, Greek and Thai. Thus, this dataset is now fully annotated with five different languages, two of which (Chinese, Thai) are Sino-Tibetan languages, and the rest are all Indo-European languages. Following previous works on semantic parsing (Kwiatkowski et al., 2010; Jones et al., 2012b), we split the dataset into two portions. The training set consists of 600 instances, and we report evaluation results on the portion consisting of"
C14-1122,D08-1082,1,0.854544,"aches for performing the multilingual semantic parsing task. We present our evaluations on a standard dataset annotated with sentences in multiple languages coming from different language families. 1 Introduction Semantic parsing, the task of parsing natural language sentences into their formal semantic representations (Mooney, 2007) is one of the most important tasks in the field of natural language processing and artificial intelligence. This area of research recently has received a significant amount of attention (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012b). Consider these example sentence-semantics pairs: English: Semantics: Which states have points that are higher than the highest point in Texas ? answer(state(loc1 (place(higher2 (highest(place(loc2 (stateid(0 T X 0 ))))))))) English: Semantics: What rivers do not run through Tennessee ? answer(exclude(river(all), traverse2 (stateid(0 T N 0 )))) In the typical setting, the semantic parser learns from a collection of such sentence-semantics pairs a model that can parse novel input sentences into their respective semantic representations. Such semantic representations can t"
C14-1122,D09-1042,1,0.892885,"based translation problem (Koehn et al., 2003). K RISP (Kate and Mooney, 2006) makes use of Support Vector Machines with string kernels (Lodhi et al., 2002) to recursively map contiguous word sequences into semantic units to construct a tree structure. The S CISSOR model (Ge and Mooney, 2005) performs integrated semantic and syntactic parsing. The model parses natural language sentences into semantically augmented parse trees whose nodes consist of both semantic and syntactic labels and then builds semantic representations based on such augmented trees. The hybrid tree model (Lu et al., 2008; Lu et al., 2009), whose code is publicly available, makes the assumption that there exists an underlying generative process for jointly producing both the language and semantics. The model employs efficient dynamic programming algorithms for learning a distribution over the latent hybrid trees which jointly encode both language and semantics. An example hybrid tree representation is shown in Figure 2. Jones et al. (2012b) recently proposed a framework that performs semantic parsing with tree transducers. The model learns representations that are similar to the hybrid tree structures using a generative process"
C14-1122,D09-1001,0,0.0608061,"gorithms for learning a distribution over the latent hybrid trees which jointly encode both language and semantics. An example hybrid tree representation is shown in Figure 2. Jones et al. (2012b) recently proposed a framework that performs semantic parsing with tree transducers. The model learns representations that are similar to the hybrid tree structures using a generative process under a Bayesian framework. Besides these approaches, recently there are also several works that take alternative learning approaches for semantic parsing which do not require annotated semantic representations (Poon and Domingos, 2009; Clarke et al., 2010; Goldwasser et al., 2011; Liang et al., 2013; Artzi and Zettlemoyer, 2013). Most of such approaches rely on either weak supervision or certain forms of indirect supervision. Some of these works also focus on optimizing specific downstream tasks rather than the semantic parsing task itself. 1293 ma ma w9 w10 mb mc w3 w4 w5 w1 w2 md mb w1 w2 w3 w4 w5 w7 w8 mc w6 w6 w7 md w10 w8 w9 Figure 3: Two example hybrid trees. Their leaves are natural language words, and the internal nodes are semantic units. Both hybrid trees correspond to the same n-m pair hw1 w2 w3 w4 w5 w6 w7 w8 w"
C14-1122,N06-1056,0,0.766334,"vestigate several approaches for performing the multilingual semantic parsing task. We present our evaluations on a standard dataset annotated with sentences in multiple languages coming from different language families. 1 Introduction Semantic parsing, the task of parsing natural language sentences into their formal semantic representations (Mooney, 2007) is one of the most important tasks in the field of natural language processing and artificial intelligence. This area of research recently has received a significant amount of attention (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012b). Consider these example sentence-semantics pairs: English: Semantics: Which states have points that are higher than the highest point in Texas ? answer(state(loc1 (place(higher2 (highest(place(loc2 (stateid(0 T X 0 ))))))))) English: Semantics: What rivers do not run through Tennessee ? answer(exclude(river(all), traverse2 (stateid(0 T N 0 )))) In the typical setting, the semantic parser learns from a collection of such sentence-semantics pairs a model that can parse novel input sentences into their respective semantic representations. Such semantic repr"
C14-1122,P07-1121,0,0.0765431,"formalisms for semantic parsing. Popular examples include the tree-structured semantic representations (Wong and Mooney, 2006; Kate and Mooney, 1292 Q UERY : answer(R IVER) What R IVER : exclude(R IVER, R IVER) R IVER : river(all) rivers do not ? R IVER : traverse(S TATE) run through S TATE : stateid(S TATE NAME) S TATE NAME : (0 T N 0 ) Tennessee Figure 2: An example hybrid tree. Such a hybrid tree is generated from the generative process, and captures the correspondences between natural language words and semantic units. 2006), the lambda calculus expressions (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), and dependency-based semantic representations (DCS) (Liang et al., 2013). In this work, we specifically focus on the tree-structured representations for semantics. Each semantic representation consists of semantic units as its tree nodes, where each semantic unit is of the following form: ma ≡ τa : pα (τb ∗) (1) Here ma is used to denote a complete semantic unit, which consists of its semantic type τa , its function symbol pα , as well as a list of types for argument semantic units τb ∗ (here ∗ means 0, 1, or 2; we assume there are at most two arguments for each semantic unit). In other word"
C14-1122,J97-3002,0,0.196614,"pable of simultaneously parsing semantically equivalent natural language texts in different languages into their underlying semantics. Specifically, in this work, we first introduce a new variant of a semantic parsing model under an existing framework. This new variant can be used together with other models for jointly making semantic parsing predictions, leading to an improved multilingual semantic parsing system. We demonstrate the effectiveness of this new variant through experiments. Although bilingual parsing has been extensively studied in fields such as statistical machine translation (Wu, 1997; Chiang, 2007), to the best of our knowledge, bilingual or multilingual semantic parsing that focuses on parsing sentences from multiple different languages into their formal semantic representations has not yet been studied. We present the very first work on performing multilingual semantic parsing that simultaneously parses semantically equivalent sentences from multiple different languages into their semantics. We believe this line of work can potentially lead to further developments and advancements in areas such as multilingual semantic processing and semantics-based machine translations"
C14-1122,P09-1110,0,0.0389284,"nical Papers, pages 1291–1301, Dublin, Ireland, August 23-29 2014. Q UERY : answer(R IVER) R IVER : exclude(R IVER, R IVER) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : (0 tn0 ) What rivers do not run through Tennessee ? 什么 河流 不 贯穿 田纳西 州 ？ Welche Fl¨usse fließen nicht durch Tennessee ? Figure 1: An example tree-structured semantic representation (above) and its corresponding natural language sentences (in English, Chinese and German). challenging task. Researchers resorted to performing context-dependent semantic parsing to alleviate such an issue (Zettlemoyer and Collins, 2009). On the other hand, researchers have successfully exploited parallel texts for improved word-level semantic processing (Chan and Ng, 2005). This is because words from different languages that convey the same semantics can be used to disambiguate each other’s semantics. In fact, texts from different languages that convey the same semantic information becomes increasingly available nowadays. Web crawlers such as Google and Yahoo! are able to rapidly aggregate a large volume of news stories every day. One crucial fact is that many such news articles written in different languages are actually al"
C14-1122,P11-1060,0,\N,Missing
D08-1082,J93-2003,0,0.0329288,"d. 2 S TATE : exclude (S TATE S TATE) S TATE : state (all) S TATE : loc 1 (R IVER) R IVER : river (all) Figure 1: An example MR structure Related Work In Section 9, we will compare performance with the three existing systems that were evaluated on the same data sets we consider. S ILT (Kate et al., 2005) learns deterministic rules to transform either sentences or their syntactic parse trees to meaning structures. WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. K RISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work ha"
D08-1082,J05-1003,0,0.0283535,"erceptron algorithms usually optimize the accuracy measure, we extend it to allow optimization of the F-measure by introducing an explicit separating plane on the feature space that rejects certain predictions even when they score highest. The idea is to find a threshold b after w is learned, such that a prediction with score below b gets rejected. We pick the threshold that leads to the optimal F-measure when applied to the training set. 8.2 Features We list in Table 2 the set of features we used. Examples are given based on the hybrid tree in Figure 789 3. Some of the them are adapted from (Collins and Koo, 2005) for a natural language parsing task. Features 1-5 are indicator functions (i.e., it takes value 1 if a certain combination as the ones listed in Table 2 is present, 0 otherwise), while feature 6 is real valued. Features that do not appear more than once in the training set are discarded. 9 Evaluation Our evaluations were performed on two corpora, G EOQUERY and ROBOCUP. The G EOQUERY corpus contains MR defined by a Prolog-based language used in querying a database on U.S. geography. The ROBOCUP corpus contains MR defined by a coaching language used in a robot coaching competition. There are in"
D08-1082,W02-1001,0,0.00973314,"el’s joint probability Example : S TATE : loc 1(R IVER ) → have R IVER : S TATE : loc 1(R IVER ) → hhave, R IVER : river(all)i : S TATE : exclude(S TATE S TATE ) → rivers : S TATE : loc 1(R IVER ) → rivers : hR IVER : river(all), S TATE : loc 1(R IVER )i → rivers   log b P(w, m, T ) . f1 f2 f3 f4 f5 Table 2: All the features used. There is one feature for each possible combination, under feature type 1-5. It takes value 1 if the combination is present, and 0 otherwise. Feature 6 takes real values. 8.1 The Averaged Perceptron Algorithm with Separating Plane The averaged perceptron algorithm (Collins, 2002) has previously been applied to various NLP tasks (Collins, 2002; Collins, 2001) for discriminative reranking. The detailed algorithm can be found in (Collins, 2002). In this section, we extend the conventional averaged perceptron by introducing an explicit separating plane on the feature space. Our reranking approach requires three components during training: a GEN function that defines for each NL sentence a set of candidate hybrid trees; a single correct reference hybrid tree for each training instance; and a feature function Φ that defines a mapping from a hybrid tree to a feature vector."
D08-1082,J03-4003,0,0.0881966,"well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not make use of an explicit grammar but, instead, models the correspondence between sentences and their meanings with a generative process. This model is defined over hybrid trees whose nodes include both natural language words and meaning representation tokens. Inspired by the work of Collins (2003), the generative model builds trees by recursively creating nodes at each level according to a Markov process. This implicit grammar representation leads to flexible learned models that generalize well. In practice, we observe that it can correctly parse a wider range of test examples than previous approaches. The generative model is learned from data that consists of sentences paired with their meaning representations. However, there is no explicit labeling of the correspondence between words and meaning tokens that is necessary for building the hybrid trees. This creates a challenging, hidde"
D08-1082,W05-0602,0,0.281678,"e translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. K RISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR s"
D08-1082,P06-2034,0,0.375014,"ues. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. K RISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider th"
D08-1082,P06-1115,0,0.830701,"ranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. 1 Introduction To enable computers to understand natural human language is one of the classic goals of research in natural language processing. Recently, researchers have developed techniques for learning to map sentences to hierarchical representations of their underlying meaning (Wong and Mooney, 2006; Kate and Mooney, 2006). One common approach is to learn some form of probabilistic grammar which includes a list of lexical items that models the meanings of input words and also includes rules for combining lexical meanings to analyze complete sentences. This approach performs well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not make use of an explicit gr"
D08-1082,N06-1056,0,0.872814,"ith a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. 1 Introduction To enable computers to understand natural human language is one of the classic goals of research in natural language processing. Recently, researchers have developed techniques for learning to map sentences to hierarchical representations of their underlying meaning (Wong and Mooney, 2006; Kate and Mooney, 2006). One common approach is to learn some form of probabilistic grammar which includes a list of lexical items that models the meanings of input words and also includes rules for combining lexical meanings to analyze complete sentences. This approach performs well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not ma"
D08-1082,P07-1121,0,0.687661,"s a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the G EO QUERY corpus (Kate et al., 2005): The NL sentence “How many states do not have rivers ?” consists of 8 words, including punctuation. The MR is a hiera"
D08-1082,P01-1067,0,0.0121173,"accessible. In other words, there is a single deterministic derivation associated with each training instance. Therefore model parameters can be directly estimated from the training corpus by counting. However, in our task, the correct correspondence between NL words and MR structures is unknown. Many possible derivations could reach the same NL-MR pair, where each such derivation forms a hybrid tree. The hybrid tree is constructed using hidden variables and estimated from the training set. An efficient inside-outside style algorithm can be used for model estimation, similar to that used in (Yamada and Knight, 2001), as discussed next. 5.2.1 The Inside-Outside Algorithm with EM In this section, we discuss how to estimate the emission and pattern parameters with the Expectation Maximization (EM) algorithm (Dempster et al., 1977), by using an inside-outside (Baker, 1979) dynamic programming approach. Denote ni ≡ hmi , wi i as the i-th training instance, where mi and wi are the MR structure and the NL sentence of the i-th instance respectively. We also denote nv ≡ hmv , wv i as an aligned pair of MR substructure and contiguous NL substring, where the MR substructure rooted by MR production mv will correspon"
D08-1082,D07-1071,1,0.4397,"on structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the G EO QUERY corpus (Kate et al., 2005): The NL sentence “How many states do not have rivers ?” consists of 8 words, including punctuation. The MR is a hierarchical tree 784 structure, as shown in Figure 1. Following an"
D08-1082,P02-1062,0,\N,Missing
D09-1042,D08-1082,1,0.787788,"of Computer Science National University of Singapore luwei@nus.edu.sg {nght,leews}@comp.nus.edu.sg Abstract methods for constructing a natural language generation system. Given a set of pairs, where each pair consists of a natural language (NL) sentence and its formal meaning representation (MR), a learning method induces an algorithm that can be used for performing language generation from other previously unseen meaning representations. A crucial question in any natural language processing system is the representation used. Meaning representations can be in the form of a tree structure. In Lu et al. (2008), we introduced a hybrid tree framework together with a probabilistic generative model to tackle semantic parsing, where tree structured meaning representations are used. The hybrid tree gives a natural joint tree representation of a natural language sentence and its meaning representation. A joint generative model for natural language and its meaning representation, such as that used in Lu et al. (2008) has several advantages over various previous approaches designed for semantic parsing. First, unlike most previous approaches, the generative approach models a simultaneous generation process"
D09-1042,I05-1015,0,0.0166827,"amework and the LNLZ08 System Q UERY : answer(R IVER) R IVER : longest(R IVER) R IVER : exclude(R IVER1 R IVER2 ) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : texas Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 what is the longest river that does not run through texas Figure 1: An example MR paired with its NL"
D09-1042,J08-1002,0,0.0111796,"vertex features, and the last are edge features. Examples are given based on Figure 3. All the features are indicator functions, i.e., a feature takes value 1 if a certain combination is present, and 0 otherwise. The last three features explicitly encode information from the tree structure of MR. Adjacent hybrid sequence features : two adjacent hybrid sequences, together with their associated MR productions. For example: f1 : hrun through S TATE1 , R IVER1 that does not R IVER2 , R IVER : traverse(S TATE), R IVER : exclude(R IVER1 , R IVER2 )i . For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. The model enables efficient training over packed trees that potentially represent exponential number of trees. The tree conditional random fields model can be effectively represented using the feature forest model. The model has also been successfully applied to the HPSG parsing task. To train the model, we run the Viterbi algorithm on the trained LNLZ08 model and perform convex optimization using the feature forest model. The LNLZ08 model is trained using an EM algori"
D09-1042,W05-1510,0,0.0211552,"stem Q UERY : answer(R IVER) R IVER : longest(R IVER) R IVER : exclude(R IVER1 R IVER2 ) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : texas Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most"
D09-1042,P03-1021,0,0.0178115,"Missing"
D09-1042,P02-1040,0,0.0946926,"Missing"
D09-1042,W05-0602,0,0.0529684,"ammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008), we consider MRs in the form of tree structures. An example MR and its corresponding natural language sentence are shown in Figure 1. The MR is a tree consisting of nodes called MR productions. For example, the node “Q UERY : answer(R IVER)” is one MR production. Each MR production consists of a semantic category (“Q UERY”), a function symbol (“answer”) which can be optionally omitted, as well as an argument list which possibly contains 401 Q UERY : answer(R IVER) R IVER : longest(R IVER) what is R IVER : exclude(R IVER1 R IVER2"
D09-1042,W03-2316,0,0.0225286,"2 ) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : texas Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney,"
D09-1042,P06-1115,0,0.080526,"d Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008), we consider MRs in the form of tree structures. An example MR and its corresponding natural language sentence are shown in Figure 1. The MR is a tree consisting of nodes called MR productions. For example, the node “Q UERY : answer(R IVER)” is one MR production. Each MR production consists of a semantic category (“Q UERY”), a function symbol (“answer”) which can be optionally omitted, as well as an argument list which possibly contains 401 Q UERY : answer(R IVER) R IVER : longest(R IVER) what is R IVER : exclude(R IVER1 R IVER2 ) the longest R IVER :"
D09-1042,N06-1056,0,0.0477113,"Missing"
D09-1042,P96-1027,0,0.181319,"OH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1 ++ in Section 5. 3 The Hybrid Tree Framework and the LNLZ08 System Q UERY : answer(R IVER) R IVER : longest(R IVER) R IVER : exclude(R IVER1 R IVER2 ) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : texas Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semanti"
D09-1042,N07-1022,0,0.783615,"eneration at the phrase level. Motivated by conditional random fields (CRF) (Lafferty et al., 2001), a different parameterization of the conditional probability of the hybrid tree that enables the model to encode some longer range dependencies amongst phrases and MRs is used. This novel model is referred to as the tree CRF-based model. Evaluation results for both models are presented, through which we demonstrate that the tree CRF -based model performs better than the direct inversion model. We also compare the tree CRFbased model against the previous state-of-the-art model of Wong and Mooney (2007). Furthermore, we evaluate our model on a dataset annotated with several natural languages other than English (Japanese, Spanish, and Turkish). Evaluation results show that our proposed tree CRF-based model outperforms the previous model. 2 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, 1972). The system took in a linearized MR tree as input, and translated it into a natural language sentence as output. Unlike most previous systems, their system integrated both lexical selection and surface realization in a single fr"
D09-1042,koen-2004-pharaoh,0,0.0100966,"n English (Japanese, Spanish, and Turkish). Evaluation results show that our proposed tree CRF-based model outperforms the previous model. 2 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, 1972). The system took in a linearized MR tree as input, and translated it into a natural language sentence as output. Unlike most previous systems, their system integrated both lexical selection and surface realization in a single framework. The performance of the system was enhanced by incorporating models borrowed from P HARAOH (Koehn, 2004). Experiments show that this new hybrid system named WASP−1 ++ gives state-of-the-art accuracies and outperforms the direct translation model obtained from P HARAOH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1 ++ in Section 5. 3 The Hybrid Tree Framework and the LNLZ08 System Q UERY : answer(R IVER) R IVER : longest(R IVER) R IVER : exclude(R IVER1 R IVER2 ) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : texas Related Work There have been substantial earlier research efforts on investigating methods f"
D10-1018,P96-1041,0,0.0614142,"sequentially from left to right. In the cascaded approach, we format the training sentences by replacing all sentence-ending punctuation symbols with special sentence boundary symbols first. A model for sentence boundary prediction is learned based on such training data. This step is then followed by predicting the actual punctuation symbols. Both trigram and 5-gram language models are tried for all combinations of the above settings. This gives us a total of 8 possible combinations based on the hidden event language model. When training all the language models, modified Kneser-Ney smoothing (Chen and Goodman, 1996) for n-grams is used. To assess the performance of the punctuation prediction task, we compute precision (prec.), recall (rec.), and F1-measure (F1 ), as defined by the following equations: http://www.cis.upenn.edu/∼treebank/tokenization.html 182 = # Correctly predicted punctuation symbols # predicted punctuation symbols # Correctly predicted punctuation symbols # expected punctuation symbols 2 1/prec. + 1/rec. Performance on Correctly Recognized Texts The performance of punctuation prediction on both Chinese (C N) and English (E N) texts in the correctly recognized output of the BTEC and CT d"
D10-1018,1993.eamt-1.1,0,0.156259,"w many/much . . . ). These pose difficulties for the simple hidden event language model, which only encodes simple dependencies over surrounding words by means of n-gram language modeling. By adopting a discriminative model which exploits non-independent, overlapping features, the LC RF model generally outperforms the hidden event language model. By introducing an additional layer of tags for performing sentence segmentation and sentence type prediction, the F-C RF model further boosts the performance over the L-C RF model. We perform statistical significance tests using bootstrap resampling (Efron et al., 1993). The improvements of F-C RF over L-C RF are statistically significant (p < 0.01) on Chinese and English texts in the CT BTEC LM ORDER Prec. CN Rec. F1 Prec. EN Rec. F1 N O D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 85.96 84.80 86.48 85.12 81.87 82.78 83.15 82.78 83.86 83.78 84.78 83.94 62.38 59.29 56.86 54.22 64.17 60.99 58.76 56.21 63.27 60.13 57.79 55.20 U SE D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 66.86 68.76 68.00 68.75 63.92 66.12 65.38 66.48 65.36 67.41 66.67 67.60 85.23 87.29 84.49 81.32 88.22 89.65 87.58 84.55 86.70 88.45 86.00 82.90 L-C RF F-C RF 92.81 85.16 88.83 90.67 88.22 89"
D10-1018,2007.iwslt-1.13,0,0.0167097,"ze the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated boundary detection and punctuation insertion as an interword hidden event detection task. Their proposed method was implemented in the handy utility hidden-ngram as 178 part of the SRILM toolkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts consider the boundary detection and punctuation"
D10-1018,2005.iwslt-1.8,0,0.0189298,"ll the default settings of Moses, except with the lexicalized reordering model enabled. This is because 3 http://code.google.com/p/berkeleyaligner/ LM ORDER CN → EN EN → CN N O D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 30.77 30.71 30.98 30.64 21.21 21.00 21.16 20.76 U SE D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 30.16 30.26 30.33 30.42 23.03 24.04 23.61 23.34 L-C RF F-C RF 31.27 23.44 31.30 24.18 Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of B LEU) lexicalized reordering gives better performance than simple distance-based reordering (Koehn et al., 2005). Specifically, the default lexicalized reordering model (msd-bidirectional-fe) is used. For tuning the parameters of Moses, we use the official IWSLT05 evaluation set where the correct punctuation symbols are present. Evaluations are performed on the ASR outputs of the IWSLT08 BTEC evaluation dataset, with punctuation symbols inserted by each punctuation prediction method. The tuning set and evaluation set include 7 reference translations. Following a common practice in statistical machine translation, we report B LEU-4 scores (Papineni et al., 2002), which were shown to have good correlation"
D10-1018,P07-2045,0,0.0509435,"feeding the punctuated ASR texts to a state-of-the-art machine translation system, and evaluate the resulting translation performance. The translation performance is in turn measured by an automatic evaluation metric which correlates well with human judgments. We believe that such a task-oriented approach for evaluating the quality of punctuation prediction for ASR output texts is useful, since it tells us how well the punctuated ASR output texts from each punctuation prediction system can be used for further processing, such as in statistical machine translation. In this paper, we use Moses (Koehn et al., 2007), a state-of-the-art phrase-based statistical machine translation toolkit, as our translation engine. We use the entire IWSLT09 BTEC training set for training the translation system. The state-of-the-art unsupervised Berkeley aligner3 (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexicalized reordering model enabled. This is because 3 http://code.google.com/p/berkeleyaligner/ LM ORDER CN → EN EN → CN N O D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 30.77 30.71 30.98 30.64 21.21 21.00 21.16 20.76 U SE D UPLICATION S INGLE"
D10-1018,N06-1014,0,0.0268783,"ents. We believe that such a task-oriented approach for evaluating the quality of punctuation prediction for ASR output texts is useful, since it tells us how well the punctuated ASR output texts from each punctuation prediction system can be used for further processing, such as in statistical machine translation. In this paper, we use Moses (Koehn et al., 2007), a state-of-the-art phrase-based statistical machine translation toolkit, as our translation engine. We use the entire IWSLT09 BTEC training set for training the translation system. The state-of-the-art unsupervised Berkeley aligner3 (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexicalized reordering model enabled. This is because 3 http://code.google.com/p/berkeleyaligner/ LM ORDER CN → EN EN → CN N O D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 30.77 30.71 30.98 30.64 21.21 21.00 21.16 20.76 U SE D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 30.16 30.26 30.33 30.42 23.03 24.04 23.61 23.34 L-C RF F-C RF 31.27 23.44 31.30 24.18 Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of B LEU) lexicalized reordering gives better"
D10-1018,P05-1056,0,0.0417853,"task. Kim and Woodland (2001) performed punctuation insertion during speech recognition. Prosodic features together with language model probabilities were used within a decision tree framework. Christensen et al. (2001) focused on the broadcast news domain and investigated both finite state and multi-layer perceptron methods for the task, where prosodic and lexical information was incorporated. Huang and Zweig (2002) presented a maximum entropy-based tagging approach to punctuation insertion in spontaneous English conversational speech, where both lexical and prosodic features were exploited. Liu et al. (2005) focused on the sentence boundary detection task, by making use of conditional random fields (CRF) (Lafferty et al., 2001). Their method was shown to improve over a previous method based on hidden Markov model (HMM). There is relatively less work that exploited lexical features only. Beeferman et al. (1998) focused on comma prediction with a trigram language model. A joint language model was learned from punctuated texts, and commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted p"
D10-1018,2006.iwslt-papers.1,0,0.0189916,"olkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts consider the boundary detection and punctuation insertion task as a hidden event detection task. One such well-known approach was introduced by Stolcke et al. (1998). They adopted a HMM to describe a joint distribution over words and interword events, where the observations are the words, and the word/event pairs are encoded as hidden states. Specifically, in this task word boundaries and punctuation symbols are encoded as interword events. The training phase involves training an n-g"
D10-1018,P03-1021,0,0.0395812,"the official IWSLT05 evaluation set where the correct punctuation symbols are present. Evaluations are performed on the ASR outputs of the IWSLT08 BTEC evaluation dataset, with punctuation symbols inserted by each punctuation prediction method. The tuning set and evaluation set include 7 reference translations. Following a common practice in statistical machine translation, we report B LEU-4 scores (Papineni et al., 2002), which were shown to have good correlation with human judgments, with the closest reference length as the effective reference length. The minimum error rate training (MERT) (Och, 2003) procedure is used for tuning the model parameters of the translation system. Due to the unstable nature of MERT, we perform 10 runs for each translation task, with a different random initialization of parameters in each run, and report the B LEU4 scores averaged over 10 runs. The results are reported in Table 7. The best translation performances for both translation directions are achieved by applying F-C RF as the punctuation prediction model to the ASR texts. Such improvements are observed to be consistent over different runs. The improvement of F-C RF over LC RF in translation quality is s"
D10-1018,P02-1040,0,0.103814,"rmance than simple distance-based reordering (Koehn et al., 2005). Specifically, the default lexicalized reordering model (msd-bidirectional-fe) is used. For tuning the parameters of Moses, we use the official IWSLT05 evaluation set where the correct punctuation symbols are present. Evaluations are performed on the ASR outputs of the IWSLT08 BTEC evaluation dataset, with punctuation symbols inserted by each punctuation prediction method. The tuning set and evaluation set include 7 reference translations. Following a common practice in statistical machine translation, we report B LEU-4 scores (Papineni et al., 2002), which were shown to have good correlation with human judgments, with the closest reference length as the effective reference length. The minimum error rate training (MERT) (Och, 2003) procedure is used for tuning the model parameters of the translation system. Due to the unstable nature of MERT, we perform 10 runs for each translation task, with a different random initialization of parameters in each run, and report the B LEU4 scores averaged over 10 runs. The results are reported in Table 7. The best translation performances for both translation directions are achieved by applying F-C RF as"
D10-1018,N03-1028,0,0.0295433,"Missing"
D10-1018,I05-3027,0,0.0104152,"hniques based on conditional random fields to tackle the difficulties due to long range dependencies. 4 Linear-Chain Conditional Random Fields One natural approach to relax the strong dependency assumptions encoded by the hidden event language model is to adopt an undirected graphical model, where arbitrary overlapping features can be exploited. Conditional random fields (CRF) (Lafferty et al., 2001) have been widely used in various sequence labeling and segmentation tasks (Sha and Pereira, 1 http://mastarpj.nict.go.jp/IWSLT2008/downloads/ case+punc tool using SRILM.instructions.txt 179 2003; Tseng et al., 2005). Unlike a HMM which models the joint distribution of both the label sequence and the observation, a CRF is a discriminative model of the conditional distribution of the complete label sequence given the observation. Specifically, a first-order linear-chain CRF which assumes first-order Markov property is defined by the following equation: ! XX 1 pλ (y|x) = exp λk fk (x, yt−1 , yt , t) Z(x) t k (1) where x is the observation and y is the label sequence. Feature functions fk with time step t are defined over the entire observation x and two adjacent hidden labels. Z(x) is a normalization factor"
D10-1018,2008.iwslt-evaluation.18,0,0.0118497,"d commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated boundary detection and punctuation insertion as an interword hidden event detection task. Their proposed method was implemented in the handy utility hidden-ngram as 178 part of the SRILM toolkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts con"
D10-1018,2008.iwslt-evaluation.1,0,\N,Missing
D10-1018,2009.iwslt-evaluation.1,0,\N,Missing
D11-1149,J93-2003,0,0.0591676,"metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alignment information can be estimated from alignment models, such as the IBM alignment models (Brown et al., 1993) and HMM-based alignment models (Vogel et al., 1996; Liang et al., 2006). However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. It is not obvious how to establish alignments between logical terms and texts with such alignment models. Fortunately, the generative model for λ-hybrid tree introduced in Section 3 explicitly models the mappings from λ-sub-expressions to (possibly discontiguous) word sequences with a joint generative process. This motivates us to extract grammar rules from the λ-hybrid trees. Thus, we first find the Vi"
D11-1149,P96-1041,0,0.0386177,"automatic lexical acquisition. We thus compare our system against two stateof-the-art machine translation systems: a phrasebased translation system, implemented in the Moses toolkit (Koehn et al., 2007)6 , and a hierarchical phrase-based translation system, implemented in the Joshua toolkit (Li et al., 2009), which is a reimplementation of the original Hiero system (Chiang, 2005; Chiang, 2007). The state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems when performing M ERT. 5.1 Automatic Evaluation For automatic evaluation, we measure the original IBM B LEU score (Papineni et al., 2002) (4-gram precision with brevity penalty) and the T ER score (Snover et al., 2006) (the amount of edits required to change a system output into the reference)7 . Note that T ER measures the translation error rate, thus a 6 We used the default settings, and enabled the default lexicali"
D11-1149,P05-1033,0,0.373602,"pLM (ˆ s)wLM (2) where r ∈ D refers to a rule r that appears in the derivation D, sˆ is the target side (sentence) associated with the derivation D, and fi is a rulespecific feature (one of features 1–3 above) which 1615 is weighted with wi . The language model feature is weighted with wLM . Once the feature values are computed, our goal is to find the optimal weight vector w ¯ ∗ that maximizes a certain evaluation metric when used for decoding, as we will discuss in Section 4.4. Following popular approaches to learning feature weights in the machine translation community (Och and Ney, 2004; Chiang, 2005), we use the minimum error rate training (M ERT) (Och, 2003) algorithm to learn the feature weights that directly optimize certain automatic evaluation metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-al"
D11-1149,J07-2003,0,0.391248,"n, which has been extensively studied in the machine translation community, translating from logical forms into text presents additional challenges. Specifically, logical forms such as λ-expressions may have complex internal structures and variable dependencies across subexpressions. Problems arise when performing automatic acquisition of a translation lexicon, as well as performing lexical selection and surface realization during generation. In this work, we tackle these challenges by making the following contributions: • A novel forest-to-string generation algorithm: Inspired by the work of Chiang (2007), we introduce a novel reduction-based weighted binary synchronous context-free grammar formalism for generation from logical forms (λexpressions), which can then be integrated with a probabilistic forest-to-string generation algo1611 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1611–1622, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics rithm. • A novel grammar induction algorithm: To automatically induce such synchronous grammar rules, we propose a novel generative model that establishes phrasal corre"
D11-1149,N10-1140,0,0.0318778,"translation community (Och and Ney, 2004; Chiang, 2005), we use the minimum error rate training (M ERT) (Och, 2003) algorithm to learn the feature weights that directly optimize certain automatic evaluation metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alignment information can be estimated from alignment models, such as the IBM alignment models (Brown et al., 1993) and HMM-based alignment models (Vogel et al., 1996; Liang et al., 2006). However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. It is not obvious how to establish alignments between logical terms and texts with such alignment models. Fortunately, the generative model for λ-hybrid tree introduced in Section 3 explicitly models the"
D11-1149,P09-1069,0,0.0114433,"guage generation model using the same meaning representation based on tree conditional random fields. Angeli et al. (2010) presented a domain-independent probabilistic approach for generation from database entries. All these models are probabilistic models. Recently there are also substantial research efforts on the task of mapping natural language to meaning 1612 representations in various formalisms – the inverse task of language generation called semantic parsing. Examples include Zettlemoyer and Collins (2005; 2007; 2009), Kate and Mooney (2006), Wong and Mooney (2007b), Lu et al. (2008), Ge and Mooney (2009), as well as Kwiatkowski et al. (2010). Of particular interest is our prior work Lu et al. (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. One important property of the model in our prior work is that it induces a hybrid tree structure automatically in an unsupervised manner, which reveals the correspondences between natural language word sequences and semantic elements. We extend our prior model in th"
D11-1149,P06-1115,0,0.0443551,"ing representation into sentences. Lu et al. (2009) presented a language generation model using the same meaning representation based on tree conditional random fields. Angeli et al. (2010) presented a domain-independent probabilistic approach for generation from database entries. All these models are probabilistic models. Recently there are also substantial research efforts on the task of mapping natural language to meaning 1612 representations in various formalisms – the inverse task of language generation called semantic parsing. Examples include Zettlemoyer and Collins (2005; 2007; 2009), Kate and Mooney (2006), Wong and Mooney (2007b), Lu et al. (2008), Ge and Mooney (2009), as well as Kwiatkowski et al. (2010). Of particular interest is our prior work Lu et al. (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. One important property of the model in our prior work is that it induces a hybrid tree structure automatically in an unsupervised manner, which reveals the correspondences between natural language word"
D11-1149,N03-1017,0,0.0774625,"ture values. For generality, we only consider the following four simple features in this work: 1. p˜(hw |pλ ): the relative frequency estimate of a hybrid sequence hw given the λ-production pλ ; 2. p˜(pλ |hw , τ ): the relative frequency estimate of a λ-production pλ given the phrase hw and the type τ ; 3. exp(−wc(hw )): the number of words generated, where wc(hw ) refers to the number of words in hw (i.e., word penalty); and 4. pLM (ˆ s): the language model score of the generated sentence sˆ. The first three features, which are also widely used in state-of-the-art machine translation models (Koehn et al., 2003; Chiang, 2007), are rule-specific and thus can be computed before decoding. The last feature is computed during the decoding phase in combination with the sibling rules used. We score a derivation D with a log-linear model: w(D) = YY r∈D i fi (r) wi ! × pLM (ˆ s)wLM (2) where r ∈ D refers to a rule r that appears in the derivation D, sˆ is the target side (sentence) associated with the derivation D, and fi is a rulespecific feature (one of features 1–3 above) which 1615 is weighted with wi . The language model feature is weighted with wLM . Once the feature values are computed, our goal is to"
D11-1149,P07-2045,0,0.00288058,"abase records with an additional focus on content selection (selection of records and their subfields for generation). It is not obvious how to adopt their algorithm in our context where content selection is not required but the more complex logical semantic representation is used as input. Other earlier approaches such as the work of Wang (1980) and Shieber et al. (1990) made use of rule-based approaches without automatic lexical acquisition. We thus compare our system against two stateof-the-art machine translation systems: a phrasebased translation system, implemented in the Moses toolkit (Koehn et al., 2007)6 , and a hierarchical phrase-based translation system, implemented in the Joshua toolkit (Li et al., 2009), which is a reimplementation of the original Hiero system (Chiang, 2005; Chiang, 2007). The state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems w"
D11-1149,W04-3250,0,0.016236,"presents a very different syntactic structure and word ordering from English. 8 We assume word unigrams are generated from free variables, quantifiers, and logical connectives in IBM model 1. 1618 Our system, on the other hand, employs a packed forest representation for λ-expressions. Therefore, it eliminates the ordering constraint by encompassing exponentially many possible tree structures during both the alignment and decoding stage. As a result, our system obtains significant improvements in both B LEU and 1−T ER using the significance test under the paired bootstrap resampling method of Koehn (2004). We obtain p &lt; 0.01 for all cases, except when comparing against Joshua-preorder for English, where we obtain p &lt; 0.05 for both metrics. text preorder inorder postorder text preorder Joshua inorder postorder This work (t) (t) w/o type 2 rules (t) w/o type 3 rules Moses English B LEU 1−T ER 48.93 61.08 51.13 63.73 46.72 57.59 44.30 55.05 37.40 48.97 51.40 64.69 40.31 50.47 31.10 42.44 54.58 67.65 53.77 66.43 53.68 66.17 Chinese B LEU 1−T ER 43.23 51.71 42.08 50.43 48.03 55.29 46.36 54.59 36.60 46.20 40.05 49.70 48.32 54.64 41.31 49.71 55.11 63.77 54.30 62.49 50.96 60.13 Table 1: Performance on"
D11-1149,D10-1119,0,0.361659,"same meaning representation based on tree conditional random fields. Angeli et al. (2010) presented a domain-independent probabilistic approach for generation from database entries. All these models are probabilistic models. Recently there are also substantial research efforts on the task of mapping natural language to meaning 1612 representations in various formalisms – the inverse task of language generation called semantic parsing. Examples include Zettlemoyer and Collins (2005; 2007; 2009), Kate and Mooney (2006), Wong and Mooney (2007b), Lu et al. (2008), Ge and Mooney (2009), as well as Kwiatkowski et al. (2010). Of particular interest is our prior work Lu et al. (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. One important property of the model in our prior work is that it induces a hybrid tree structure automatically in an unsupervised manner, which reveals the correspondences between natural language word sequences and semantic elements. We extend our prior model in the next section, so as to support λ-exp"
D11-1149,A00-2023,0,0.0616365,"rface realization and lexical acquisition. We demonstrate the effectiveness of our model in Section 5. 2 Related Work The task of language generation from logical forms has a long history. Many early works do not rely on probabilistic approaches. Wang (1980) presented an approach for generation from an extended predicate logic formalism using hand-written rules. Shieber et al. (1990) presented a semantic head-driven approach for generation from logical forms based on rules written in Prolog. Shemtov (1996) presented a system for generation of multiple paraphrases from ambiguous logical forms. Langkilde (2000) presented a probabilistic model for generation from a packed forest meaning representation, without concerning lexical acquisition. Specifically, we are not aware of any prior work that handles both automatic unsupervised lexical acquisition and surface realization for generation from logical forms in a single framework. Another line of research efforts focused on the task of language generation from other meaning representation formalisms. Wong and Mooney (2007a) as well as Chen and Mooney (2008) made use of synchronous grammars to transform a variablefree tree-structured meaning representat"
D11-1149,W09-0424,0,0.027091,"ration). It is not obvious how to adopt their algorithm in our context where content selection is not required but the more complex logical semantic representation is used as input. Other earlier approaches such as the work of Wang (1980) and Shieber et al. (1990) made use of rule-based approaches without automatic lexical acquisition. We thus compare our system against two stateof-the-art machine translation systems: a phrasebased translation system, implemented in the Moses toolkit (Koehn et al., 2007)6 , and a hierarchical phrase-based translation system, implemented in the Joshua toolkit (Li et al., 2009), which is a reimplementation of the original Hiero system (Chiang, 2005; Chiang, 2007). The state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems when performing M ERT. 5.1 Automatic Evaluation For automatic evaluation, we measure the original IBM B LEU"
D11-1149,N06-1014,0,0.189371,"gorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alignment information can be estimated from alignment models, such as the IBM alignment models (Brown et al., 1993) and HMM-based alignment models (Vogel et al., 1996; Liang et al., 2006). However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. It is not obvious how to establish alignments between logical terms and texts with such alignment models. Fortunately, the generative model for λ-hybrid tree introduced in Section 3 explicitly models the mappings from λ-sub-expressions to (possibly discontiguous) word sequences with a joint generative process. This motivates us to extract grammar rules from the λ-hybrid trees. Thus, we first find the Viterbi λ-hybrid trees for all training instances, he, ti 2 : λg.λf.λx.g(x"
D11-1149,D08-1082,1,0.859768,"ith a probabilistic forest-to-string generation algo1611 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1611–1622, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics rithm. • A novel grammar induction algorithm: To automatically induce such synchronous grammar rules, we propose a novel generative model that establishes phrasal correspondences between logical sub-expressions and natural language word sequences, by extending a previous model proposed for parsing natural language into meaning representations (Lu et al., 2008). To our best knowledge, this is the first probabilistic model for generating sentences from the lambda calculus encodings of their underlying formal meaning representations, that concerns both surface realization and lexical acquisition. We demonstrate the effectiveness of our model in Section 5. 2 Related Work The task of language generation from logical forms has a long history. Many early works do not rely on probabilistic approaches. Wang (1980) presented an approach for generation from an extended predicate logic formalism using hand-written rules. Shieber et al. (1990) presented a seman"
D11-1149,D09-1042,1,0.882966,"ral language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980; Shieber et al., 1990), which concern surface realization (ordering and inflecting of words) but largely ignore lexical acquisition. Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of specific forms such as variable-free tree-structured representations (Wong and Mooney, 2007a; Lu et al., 2009) or database entries (Angeli et al., 2010). While these algorithms usually work well on specific semantic formalisms, it is unclear how well they could be applied to a different semantic formalism. In this work, we propose a general probabilistic model that performs generation from underlying formal semantics in the form of typed lambda calculus expressions (we refer to them as λ-expressions throughout this paper), where both lexical acquisition and surface realization are integrated in a single framework. One natural proposal is to adopt a state-of-the-art statistical machine translation appr"
D11-1149,J04-4002,0,0.0807474,"r∈D i fi (r) wi ! × pLM (ˆ s)wLM (2) where r ∈ D refers to a rule r that appears in the derivation D, sˆ is the target side (sentence) associated with the derivation D, and fi is a rulespecific feature (one of features 1–3 above) which 1615 is weighted with wi . The language model feature is weighted with wLM . Once the feature values are computed, our goal is to find the optimal weight vector w ¯ ∗ that maximizes a certain evaluation metric when used for decoding, as we will discuss in Section 4.4. Following popular approaches to learning feature weights in the machine translation community (Och and Ney, 2004; Chiang, 2005), we use the minimum error rate training (M ERT) (Och, 2003) algorithm to learn the feature weights that directly optimize certain automatic evaluation metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin"
D11-1149,P03-1021,0,0.0124825,"n the derivation D, sˆ is the target side (sentence) associated with the derivation D, and fi is a rulespecific feature (one of features 1–3 above) which 1615 is weighted with wi . The language model feature is weighted with wLM . Once the feature values are computed, our goal is to find the optimal weight vector w ¯ ∗ that maximizes a certain evaluation metric when used for decoding, as we will discuss in Section 4.4. Following popular approaches to learning feature weights in the machine translation community (Och and Ney, 2004; Chiang, 2005), we use the minimum error rate training (M ERT) (Och, 2003) algorithm to learn the feature weights that directly optimize certain automatic evaluation metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alig"
D11-1149,P02-1040,0,0.091148,"is a reimplementation of the original Hiero system (Chiang, 2005; Chiang, 2007). The state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems when performing M ERT. 5.1 Automatic Evaluation For automatic evaluation, we measure the original IBM B LEU score (Papineni et al., 2002) (4-gram precision with brevity penalty) and the T ER score (Snover et al., 2006) (the amount of edits required to change a system output into the reference)7 . Note that T ER measures the translation error rate, thus a 6 We used the default settings, and enabled the default lexicalized reordering model, which yielded better performance. 7 We used tercom version 0.7.25 with the default settings. smaller score indicates a better result. For clarity, we report 1−T ER scores. Following the tuning procedure as conducted in Galley and Manning (2010), we perform M ERT using B LEU as the metric. We c"
D11-1149,C96-2155,0,0.305681,"the lambda calculus encodings of their underlying formal meaning representations, that concerns both surface realization and lexical acquisition. We demonstrate the effectiveness of our model in Section 5. 2 Related Work The task of language generation from logical forms has a long history. Many early works do not rely on probabilistic approaches. Wang (1980) presented an approach for generation from an extended predicate logic formalism using hand-written rules. Shieber et al. (1990) presented a semantic head-driven approach for generation from logical forms based on rules written in Prolog. Shemtov (1996) presented a system for generation of multiple paraphrases from ambiguous logical forms. Langkilde (2000) presented a probabilistic model for generation from a packed forest meaning representation, without concerning lexical acquisition. Specifically, we are not aware of any prior work that handles both automatic unsupervised lexical acquisition and surface realization for generation from logical forms in a single framework. Another line of research efforts focused on the task of language generation from other meaning representation formalisms. Wong and Mooney (2007a) as well as Chen and Moone"
D11-1149,J90-1004,0,0.638979,"Missing"
D11-1149,2006.amta-papers.25,0,0.0302407,"he state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems when performing M ERT. 5.1 Automatic Evaluation For automatic evaluation, we measure the original IBM B LEU score (Papineni et al., 2002) (4-gram precision with brevity penalty) and the T ER score (Snover et al., 2006) (the amount of edits required to change a system output into the reference)7 . Note that T ER measures the translation error rate, thus a 6 We used the default settings, and enabled the default lexicalized reordering model, which yielded better performance. 7 We used tercom version 0.7.25 with the default settings. smaller score indicates a better result. For clarity, we report 1−T ER scores. Following the tuning procedure as conducted in Galley and Manning (2010), we perform M ERT using B LEU as the metric. We compare our model against state-of-the-art statistical machine translation systems"
D11-1149,C96-2141,0,0.0907415,"ementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alignment information can be estimated from alignment models, such as the IBM alignment models (Brown et al., 1993) and HMM-based alignment models (Vogel et al., 1996; Liang et al., 2006). However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. It is not obvious how to establish alignments between logical terms and texts with such alignment models. Fortunately, the generative model for λ-hybrid tree introduced in Section 3 explicitly models the mappings from λ-sub-expressions to (possibly discontiguous) word sequences with a joint generative process. This motivates us to extract grammar rules from the λ-hybrid trees. Thus, we first find the Viterbi λ-hybrid trees for all training instances, he"
D11-1149,C80-1061,0,0.75627,"sions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation. 1 Introduction This work focuses on the task of generating natural language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980; Shieber et al., 1990), which concern surface realization (ordering and inflecting of words) but largely ignore lexical acquisition. Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of specific forms such as variable-free tree-structured representations (Wong and Mooney, 2007a; Lu et al., 2009) or database entries (Angeli et al., 2010). While these algorithms usually work well on specific semantic formalisms, it is unclear how well they could be applied to a different semantic formalism. In this work, we pr"
D11-1149,N06-1056,0,0.103481,"each (fi , si ) ∈ (f, s), find the most probable λ-hybrid tree hi , and then extract the grammar rules from it: hi = F IND H YBRID T REE(fi , si , θ¯∗ ) Γ = Γ ∪ E XTRACT RULES(hi ) 3. Output the learned grammar rule set Γ. Figure 5: The algorithm for learning the grammar rules 5 Experiments For experiments, we evaluated on the G EOQUERY dataset, which consists of 880 queries on U.S. geography. The dataset was manually labeled with λexpressions as their semantics in Zettlemoyer and Collins (2005). It was used in many previous research efforts on semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). The original dataset was annotated with English sentences only. In order to assess the generation performance across different languages, in our work the entire dataset was also manually annotated with Chinese by a native Chinese speaker with linguistics background5 . For all the experiments we present in this section, we use the same split as that of Kwiatkowski 5 The annotator created annotations with both λ-expressions and corresponding English sentences available as references. 1617 et al. (2010), where 280 instances are used for"
D11-1149,N07-1022,0,0.427542,"task of generating natural language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980; Shieber et al., 1990), which concern surface realization (ordering and inflecting of words) but largely ignore lexical acquisition. Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of specific forms such as variable-free tree-structured representations (Wong and Mooney, 2007a; Lu et al., 2009) or database entries (Angeli et al., 2010). While these algorithms usually work well on specific semantic formalisms, it is unclear how well they could be applied to a different semantic formalism. In this work, we propose a general probabilistic model that performs generation from underlying formal semantics in the form of typed lambda calculus expressions (we refer to them as λ-expressions throughout this paper), where both lexical acquisition and surface realization are integrated in a single framework. One natural proposal is to adopt a state-of-the-art statistical machi"
D11-1149,P07-1121,0,0.330828,"task of generating natural language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980; Shieber et al., 1990), which concern surface realization (ordering and inflecting of words) but largely ignore lexical acquisition. Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of specific forms such as variable-free tree-structured representations (Wong and Mooney, 2007a; Lu et al., 2009) or database entries (Angeli et al., 2010). While these algorithms usually work well on specific semantic formalisms, it is unclear how well they could be applied to a different semantic formalism. In this work, we propose a general probabilistic model that performs generation from underlying formal semantics in the form of typed lambda calculus expressions (we refer to them as λ-expressions throughout this paper), where both lexical acquisition and surface realization are integrated in a single framework. One natural proposal is to adopt a state-of-the-art statistical machi"
D11-1149,D07-1071,0,0.0271805,"), find the most probable λ-hybrid tree hi , and then extract the grammar rules from it: hi = F IND H YBRID T REE(fi , si , θ¯∗ ) Γ = Γ ∪ E XTRACT RULES(hi ) 3. Output the learned grammar rule set Γ. Figure 5: The algorithm for learning the grammar rules 5 Experiments For experiments, we evaluated on the G EOQUERY dataset, which consists of 880 queries on U.S. geography. The dataset was manually labeled with λexpressions as their semantics in Zettlemoyer and Collins (2005). It was used in many previous research efforts on semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). The original dataset was annotated with English sentences only. In order to assess the generation performance across different languages, in our work the entire dataset was also manually annotated with Chinese by a native Chinese speaker with linguistics background5 . For all the experiments we present in this section, we use the same split as that of Kwiatkowski 5 The annotator created annotations with both λ-expressions and corresponding English sentences available as references. 1617 et al. (2010), where 280 instances are used for testing, and the remaining inst"
D11-1149,P09-1110,0,0.097247,"Missing"
D11-1149,D10-1049,0,\N,Missing
D12-1062,W06-1623,0,0.176951,"d as follows: X xheI,1i = 1, ∀e ∈ E (7) I∈I Our model also enforces reflexivity and transitivity constraints on the relations among event mentions as follows: yhei ej ,ri − yhej ei ,ˆri = 0, ∀ei ej = (ei , ej ) ∈ EE, i 6= j (8) yhei ej ,r1 i + yhej ek ,r2 i − yhei ek ,r3 i ≤ 1, ∀ei ej , ej ek , ei ek ∈ EE, i 6= j 6= k (9) The equality constraints in (8) encode reflexive property of event-event relations, where the relation rˆ denotes the inversion of the relation r. The set  of possible (r, rˆ) pairs is defined as follows: (¯b, a ¯), (¯ a, ¯b), (¯ o, o¯), (¯ n, n ¯ ) . Following the work of (Bramsen et al., 2006; Chambers and Jurafsky, 2008), we encode transitive closure of relations between event mentions with inequality constraints in (9), which states that if the pair (ei , ej ) has a certain relation r1 , and the pair (ej , ek ) has the relation r2 , then the relation r3 must be satisfied between ei and ek . Examples of such triple (r1 , r2 , r3 ) include (¯b, ¯b, ¯b) and (¯ a, a ¯, a ¯). Finally, to capture the interactions between our local pairwise classifiers we add the following constraints: xhei Ik ,1i + xhej Il ,1i − yhei ej ,¯bi ≤ 1, ∀ei Ik , ej Il ∈ EI, ∀ei ej ∈ EE, Ik precedes Il , i 6="
D12-1062,D08-1073,0,0.184215,"e1 | I1 e2 t2 • t3 • | | I2 t4 • e7 − e6 I3 • +∞ | | Figure 1: A graphical illustration of our timeline representation. The e’s, t’s and I’s are events, time points and time intervals, respectively. Introduction Inferring temporal relations amongst a collection of events in a text is a significant step towards various important tasks such as automatic information extraction and document comprehension. Over the past few years, with the development of the TimeBank corpus (Pustejovsky et al., 2003) , there have been several works on building automatic systems for such a task (Mani et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011). Most previous works devoted much efforts to the task of identifying relative temporal relations (such as before, or overlap) amongst events (Chambers and Jurafsky, 2008; Denis and Muller, 2011), without addressing the task of identifying correct associations between events and their absolute time of occurrence. Even if this issue is addressed, certain restrictions are often imposed for efficiency reasons (Yoshikawa et al., 2009; Verhagen et al., 2010). In practice, however, being able to automatically infer the correct time of occurrence assoc"
D12-1062,W09-4303,0,0.0549401,"29 Prec. 20.86 CE−E Rec. 32.81 F1 25.03 Prec. 27.06 62.70 47.88 34.50 47.88 43.29 47.88 40.46 41.42 42.42 48.04 40.96 44.14 51.58 44.65 38.46 47.96 42.13 46.01 50.88 50.88 50.88 50.88 50.88 50.88 43.86 48.04 52.65 62.45 47.46 54.05 47.37 49.46 51.77 56.67 49.17 52.47 46.37 46.37 46.37 46.37 46.37 46.37 40.83 42.09 45.28 52.50 42.60 46.47 43.60 44.23 45.83 49.44 44.49 46.42 Overall Rec. F1 33.05 29.16 Table 2: Performance under various evaluation settings. All figures are averaged scores from 5-fold cross-validation experiments. periment, we re-trained the event coreference system described in Chen et al. (2009) on all articles in the ACE 2005 corpus, excluding the 20 articles used in our data set. The performance of these systems are shown in the fourth group of the results in Table 2. The results show that by using a learned event coreference system, we achieved the same improvement trends as with gold event coreference. However, we did not obtain significant improvement when comparing with global inference without event coreference information. This result shows that the performance of an event coreference system can have a significant impact on the overall performance. While this suggests that a"
D12-1062,W04-3250,0,0.0500438,"Missing"
D12-1062,P06-1095,0,0.107888,"stem. 1 • −∞ | t1 • e1 | I1 e2 t2 • t3 • | | I2 t4 • e7 − e6 I3 • +∞ | | Figure 1: A graphical illustration of our timeline representation. The e’s, t’s and I’s are events, time points and time intervals, respectively. Introduction Inferring temporal relations amongst a collection of events in a text is a significant step towards various important tasks such as automatic information extraction and document comprehension. Over the past few years, with the development of the TimeBank corpus (Pustejovsky et al., 2003) , there have been several works on building automatic systems for such a task (Mani et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011). Most previous works devoted much efforts to the task of identifying relative temporal relations (such as before, or overlap) amongst events (Chambers and Jurafsky, 2008; Denis and Muller, 2011), without addressing the task of identifying correct associations between events and their absolute time of occurrence. Even if this issue is addressed, certain restrictions are often imposed for efficiency reasons (Yoshikawa et al., 2009; Verhagen et al., 2010). In practice, however, being able to automatically infer the cor"
D12-1062,C08-1108,0,0.0253528,"s (Verhagen et al., 2007; Verhagen et al., 2010). In these challenges, several temporal-related tasks were defined including the tasks of identifying the temporal relation between an event mention and a temporal expression in the same sentence, and recognizing temporal relations of pairs of event mentions in adjacent sentences. However, with several restrictions imposed to these tasks, the developed systems were not practical. Recently, there has been much work attempting to leverage Allen’s interval algebra of temporal relations to enforce global constraints on local predictions. The work of Tatu and Srikanth (2008) used global relational constraints to not only expand the training data but also identifies temporal inconsistencies to improve local classifiers. They used greedy search to select the most appropriate configuration of temporal relations among events and temporal expressions. For exact inferences, Bramsen et al. (2006), Chambers and Jurafsky (2008), Denis and Muller (2011), and Talukdar et al. (2012) formulated the temporal reasoning problem in an ILP. However, the inference models in their work were not a joint model involving multiple local classifiers but only one local classifier was invo"
D12-1062,S07-1014,0,0.118715,"esult shows that the performance of an event coreference system can have a significant impact on the overall performance. While this suggests that a better event coreference system could potentially help the task more, it also opens the question whether event coreference can be benefited from our local classifiers through the use of a joint inference framework. We would like to leave this for future investigations. 5.4 Previous Work-Related Experiments We also performed experiments using the same setting as in (Yoshikawa et al., 2009), which followed the guidelines of the TempEval challenges (Verhagen et al., 2007; Verhagen et al., 2010), on our saturated data. Several assumptions were made to simplify the task. For example, only main events in adjacent sentences are considered when identifying event-event relations. See (Yoshikawa et al., 2009) for more details. We performed 5-fold cross validation without event coreference. Overall, the system achieved 29.99 F1 for the local classifiers and 34.69 when the global inference is used. These results are better than the baseline but underperform our full models where those simplification assumptions are 685 not imposed, as shown in Table 2, indicating the"
D12-1062,P09-1046,0,0.357113,"t4 • e7 − e6 I3 • +∞ | | Figure 1: A graphical illustration of our timeline representation. The e’s, t’s and I’s are events, time points and time intervals, respectively. Introduction Inferring temporal relations amongst a collection of events in a text is a significant step towards various important tasks such as automatic information extraction and document comprehension. Over the past few years, with the development of the TimeBank corpus (Pustejovsky et al., 2003) , there have been several works on building automatic systems for such a task (Mani et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011). Most previous works devoted much efforts to the task of identifying relative temporal relations (such as before, or overlap) amongst events (Chambers and Jurafsky, 2008; Denis and Muller, 2011), without addressing the task of identifying correct associations between events and their absolute time of occurrence. Even if this issue is addressed, certain restrictions are often imposed for efficiency reasons (Yoshikawa et al., 2009; Verhagen et al., 2010). In practice, however, being able to automatically infer the correct time of occurrence associated with each event is"
D12-1062,N12-3008,1,0.89997,"and normalize two time intervals which are explicitly written, including January 1, 1993 → [199301-01 00:00:00, 1993-01-01 23:59:59] and February 7, 2000 → [2000-02-07 00:00:00, 2000-02-07 23:59:59]. Moreover, an explicit interval can also be formed by one or more separate explicit temporal expressions. In the example above, the connective term between relates the two expressions to form a single time interval: between January 1, 1993 and February 7, 2000 → [1993-01-01 00:00:00, 200002-07 23:59:59]. To extract explicit time intervals from text, we use the time interval extractor described in Zhao et al. (2012). Implicit intervals are time intervals that are not explicitly mentioned in the text. We observed that there are events that cannot be assigned to any precise time interval but are roughly known to occur in the past or in the future relative to the Document Creation Time (DCT) of the article. We introduce two implicit time intervals to represent the past and the future events as (−∞, t− DCT ] and [t+ , +∞), respectively. In addition, we also alDCT low an event mention to be assigned into the entire timeline, which is denoted by (−∞, +∞) if we cannot identify its time of occurrence. We also co"
D12-1062,S10-1010,0,\N,Missing
D14-1137,Q13-1005,0,0.0240817,"actly A contiguous sequence of words A natural language word A semantic unit A node in the relaxed hybrid tree The feature vector The k-th feature The weight vector (model parameters) The weight for the k-th feature φk Table 1: Notation Table for answering questions without relying on semantic annotations. Goldwasser et al. (2011) took an unsupervised approach for semantic parsing based on self-training driven by confidence estimation. Liang et al. (2013) proposed a model for learning the dependency-based compositional semantics (DCS) which can be used for optimizing the end-task performance. Artzi and Zettlemoyer (2013) proposed a model for mapping instructions to actions with weak supervision. 3 Approach We discuss our approach to semantic parsing in this section. The notation that we use in this paper is summarized in Table 1. 3.1 Model In standard supervised syntactic parsing, one typically has access to a complete syntactic parse tree for each sentence in the training phase, which exactly tells the correct associations between words and syntactic labels. In our problem, however, each sentence is only paired with a complete semantic representation where the correct associations between words and semantic"
D14-1137,J07-2003,0,0.0347048,"entations for language and semantics that capture how individual words and atomic semantic units connect to each other. Typically, different existing models employ different assumptions for establishing such connections, leading to very different definitions of joint representations. We survey in this section various representations proposed by previous works. The WASP semantic parser (Wong and Mooney, 2006) essentially casts the semantic parsing problem as a string-to-string transformation problem by employing a statistical phrase-based machine translation approach with synchronous grammars (Chiang, 2007). Therefore, one can think of the joint representation for both language and semantics as a synchronous derivation tree consisting of those derivation steps for transforming sentences into target semantic representation strings. While this joint representation is flexible, allowing blocks of semantic structures to map to word sequences, it does not fully exploit the structural information (tree) as conveyed by the semantics. The K RISP semantic parser (Kate and Mooney, 2006) makes use of Support Vector Machines with string kernels (Lodhi et al., 2002) to recursively map contiguous word sequenc"
D14-1137,W10-2903,0,0.587006,"ree transducers. The model learns representations that are similar to the hybrid tree structures using a generative process under a Bayesian setting. Thus, their representations also potentially present similar issues as the ones mentioned above. Besides these supervised approaches, recently there are also several works that take alternative learning approaches to (mostly task-dependent) semantic parsing. Poon and Domingos (2009) proposed a model for unsupervised semantic parsing that transforms dependency trees into semantic representations using Markov logic (Richardson and Domingos, 2006). Clarke et al. (2010) proposed a model that learns a semantic parser Symbol n m h H(n, m) n, na w, wk m, ma h, ha Φ φk Λ λk Description A complete natural language sentence A complete semantic representation A complete latent joint representation (or, a relaxed hybrid tree for our work) A complete set of latent joint representations that contain the (n, m) pair exactly A contiguous sequence of words A natural language word A semantic unit A node in the relaxed hybrid tree The feature vector The k-th feature The weight vector (model parameters) The weight for the k-th feature φk Table 1: Notation Table for answerin"
D14-1137,P08-1109,0,0.0908045,"Missing"
D14-1137,W05-0602,0,0.081442,"nformation (tree) as conveyed by the semantics. The K RISP semantic parser (Kate and Mooney, 2006) makes use of Support Vector Machines with string kernels (Lodhi et al., 2002) to recursively map contiguous word sequences into semantic units to construct a tree structure. Our relaxed hybrid tree structures also allow input word sequences to map to semantic units in a recursive manner. One key distinction, as we will see, is that our structure distinguishes words which are imme1309 diately associated with a particular semantic unit, from words which are remotely associated. The S CISSOR model (Ge and Mooney, 2005) performs integrated semantic and syntactic parsing. The model parses natural language sentences into semantically augmented parse trees whose nodes consist of both semantic and syntactic labels and then builds semantic representations based on such augmented trees. Such a joint representation conveys more information, but requires languagespecific syntactic analysis. The hybrid tree model (Lu et al., 2008) is based on the assumption that there exists an underlying generative process which jointly produces both the sentence and the semantic tree in a top-down recursive manner. The generative p"
D14-1137,P11-1149,0,0.0373374,"l n m h H(n, m) n, na w, wk m, ma h, ha Φ φk Λ λk Description A complete natural language sentence A complete semantic representation A complete latent joint representation (or, a relaxed hybrid tree for our work) A complete set of latent joint representations that contain the (n, m) pair exactly A contiguous sequence of words A natural language word A semantic unit A node in the relaxed hybrid tree The feature vector The k-th feature The weight vector (model parameters) The weight for the k-th feature φk Table 1: Notation Table for answering questions without relying on semantic annotations. Goldwasser et al. (2011) took an unsupervised approach for semantic parsing based on self-training driven by confidence estimation. Liang et al. (2013) proposed a model for learning the dependency-based compositional semantics (DCS) which can be used for optimizing the end-task performance. Artzi and Zettlemoyer (2013) proposed a model for mapping instructions to actions with weak supervision. 3 Approach We discuss our approach to semantic parsing in this section. The notation that we use in this paper is summarized in Table 1. 3.1 Model In standard supervised syntactic parsing, one typically has access to a complete"
D14-1137,D11-1149,1,0.957911,"milar to (Lu et al., 2008), we can also define and compute the outside scores for (n, m) (the combined score of such incomplete relaxed hybrid trees that contain (n, m) as one of its leave nodes) in an analogous manner, where the computation of the gradient functions can be efficiently integrated in this process. Computation of the second part of the objective function (6) involves dynamic programming over a packed forest representation rather than a single tree, which requires an extension to the algorithm described in (Lu et al., 2008). The resulting algorithm is similar to the one used in (Lu and Ng, 2011), which has been used for language generation from packed forest representations of typed λ-calculus expressions. 4.2 Decoding The decoding phase involves finding the optimal semantic tree m∗ given a new input sentence n: m∗ = arg max P (m|n) m (7) This in fact is equivalent to finding the following optimal semantic tree m∗ : X m∗ = arg max eΛ·Φ(n,m,h) (8) m h∈H(n,m) Unfortunately, the summation operation inside the arg max prevents us from employing a similar version of the dynamic programming algorithm we developed for learning in Section 4.1. To overcome this difficulty, we instead find the"
D14-1137,D08-1082,1,0.0846622,"large collection of simple features, our model is shown to be competitive to previous works and achieves state-of-theart performance on standard benchmark data across four different languages. The system and code can be downloaded from http://statnlp.org/research/sp/. R IVER : exclude(R IVER, R IVER) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : (0 tn0 ) What rivers do not run through Tennessee ? Figure 1: An example tree-structured semantic representation (above) and its corresponding natural language sentence. Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012). Following previous research efforts, we perform semantic parsing under a setting where the semantics for complete sentences are provided as training data, but detailed word-level semantic information is not explicitly given during the training phase. As one example, consider the following natural language sentence paired with its corresponding semantic representation: What rivers do not run through Tennessee ? answer(exclude(river(all), traverse(stateid(0 tn0 )))) Introduction Semantic parsing, the task of transforming natural language sentences into formal representatio"
D14-1137,D09-1042,1,0.841761,"complex due to latent structures. Developed on top of our novel relaxed hybrid tree representations, our model allows certain long-distance dependencies to be captured. We also present efficient algorithms for learning and decoding. Experiments on benchmark data show that our model is competitive to previous works and achieves the state-of-the-art performance across several different languages. Future works include development of efficient algorithms for feature-based semantic parsing with alternative loss functions (Zhou et al., 2013), development of feature-based language generation models (Lu et al., 2009; Lu and Ng, 2011) and multilingual semantic parsers (Jie and Lu, 2014), as well as the development of efficient semantic parsing algorithms for optimizing the performance of certain downstream NLP tasks with less supervision (Clarke et al., 2010; Liang et al., 2013). Being able to efficiently exploit features defined over individual words, our model also opens up the possibility for us to exploit alternative representations of words for learning (Turian et al., 2010), or to perform joint learning of both distributional and logical semantics (Lewis and Steedman, 2013). Furthermore, as a genera"
D14-1137,D09-1001,0,0.317415,"s associated with a contiguous word sequence where overlappings amongst word sequences are not allowed. Jones et al. (2012) recently proposed a framework that performs semantic parsing with tree transducers. The model learns representations that are similar to the hybrid tree structures using a generative process under a Bayesian setting. Thus, their representations also potentially present similar issues as the ones mentioned above. Besides these supervised approaches, recently there are also several works that take alternative learning approaches to (mostly task-dependent) semantic parsing. Poon and Domingos (2009) proposed a model for unsupervised semantic parsing that transforms dependency trees into semantic representations using Markov logic (Richardson and Domingos, 2006). Clarke et al. (2010) proposed a model that learns a semantic parser Symbol n m h H(n, m) n, na w, wk m, ma h, ha Φ φk Λ λk Description A complete natural language sentence A complete semantic representation A complete latent joint representation (or, a relaxed hybrid tree for our work) A complete set of latent joint representations that contain the (n, m) pair exactly A contiguous sequence of words A natural language word A seman"
D14-1137,C14-1122,1,0.481267,"ed hybrid tree representations, our model allows certain long-distance dependencies to be captured. We also present efficient algorithms for learning and decoding. Experiments on benchmark data show that our model is competitive to previous works and achieves the state-of-the-art performance across several different languages. Future works include development of efficient algorithms for feature-based semantic parsing with alternative loss functions (Zhou et al., 2013), development of feature-based language generation models (Lu et al., 2009; Lu and Ng, 2011) and multilingual semantic parsers (Jie and Lu, 2014), as well as the development of efficient semantic parsing algorithms for optimizing the performance of certain downstream NLP tasks with less supervision (Clarke et al., 2010; Liang et al., 2013). Being able to efficiently exploit features defined over individual words, our model also opens up the possibility for us to exploit alternative representations of words for learning (Turian et al., 2010), or to perform joint learning of both distributional and logical semantics (Lewis and Steedman, 2013). Furthermore, as a general string-to-tree structured prediction model, this work may find applic"
D14-1137,P10-1040,0,0.0804851,"Missing"
D14-1137,P12-1051,0,0.421585,"of simple features, our model is shown to be competitive to previous works and achieves state-of-theart performance on standard benchmark data across four different languages. The system and code can be downloaded from http://statnlp.org/research/sp/. R IVER : exclude(R IVER, R IVER) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : (0 tn0 ) What rivers do not run through Tennessee ? Figure 1: An example tree-structured semantic representation (above) and its corresponding natural language sentence. Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012). Following previous research efforts, we perform semantic parsing under a setting where the semantics for complete sentences are provided as training data, but detailed word-level semantic information is not explicitly given during the training phase. As one example, consider the following natural language sentence paired with its corresponding semantic representation: What rivers do not run through Tennessee ? answer(exclude(river(all), traverse(stateid(0 tn0 )))) Introduction Semantic parsing, the task of transforming natural language sentences into formal representations of their underlyin"
D14-1137,N06-1056,0,0.487501,"ts that by exploiting a large collection of simple features, our model is shown to be competitive to previous works and achieves state-of-theart performance on standard benchmark data across four different languages. The system and code can be downloaded from http://statnlp.org/research/sp/. R IVER : exclude(R IVER, R IVER) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : (0 tn0 ) What rivers do not run through Tennessee ? Figure 1: An example tree-structured semantic representation (above) and its corresponding natural language sentence. Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012). Following previous research efforts, we perform semantic parsing under a setting where the semantics for complete sentences are provided as training data, but detailed word-level semantic information is not explicitly given during the training phase. As one example, consider the following natural language sentence paired with its corresponding semantic representation: What rivers do not run through Tennessee ? answer(exclude(river(all), traverse(stateid(0 tn0 )))) Introduction Semantic parsing, the task of transforming natural language sentences into for"
D14-1137,P06-1115,0,0.806979,"-to-string transformation problem by employing a statistical phrase-based machine translation approach with synchronous grammars (Chiang, 2007). Therefore, one can think of the joint representation for both language and semantics as a synchronous derivation tree consisting of those derivation steps for transforming sentences into target semantic representation strings. While this joint representation is flexible, allowing blocks of semantic structures to map to word sequences, it does not fully exploit the structural information (tree) as conveyed by the semantics. The K RISP semantic parser (Kate and Mooney, 2006) makes use of Support Vector Machines with string kernels (Lodhi et al., 2002) to recursively map contiguous word sequences into semantic units to construct a tree structure. Our relaxed hybrid tree structures also allow input word sequences to map to semantic units in a recursive manner. One key distinction, as we will see, is that our structure distinguishes words which are imme1309 diately associated with a particular semantic unit, from words which are remotely associated. The S CISSOR model (Ge and Mooney, 2005) performs integrated semantic and syntactic parsing. The model parses natural"
D14-1137,D10-1119,0,0.724795,"which jointly produces both the sentence and the semantic tree in a top-down recursive manner. The generative process results in a hybrid tree structure which consists of words as leaves and semantic units as nodes. An example hybrid tree structure is shown in Figure 2 (a). Such a representation allows each semantic unit to map to a possibly discontiguous sequence of words. The model was shown to be effective empirically, but it implicitly assumes that both the sentence and semantics exhibit certain degree of structural similarity that allows the hybrid tree structures to be constructed. UBL (Kwiatkowski et al., 2010) is a semantic parser based on restricted higher-order unification with CCG (Steedman, 1996). The model can be used to handle both tree structured semantic representations and lambda calculus expressions, and assumes there exist CCG derivations as joint representations in which each semantic unit is associated with a contiguous word sequence where overlappings amongst word sequences are not allowed. Jones et al. (2012) recently proposed a framework that performs semantic parsing with tree transducers. The model learns representations that are similar to the hybrid tree structures using a gener"
D14-1137,Q13-1015,0,0.0823002,"Missing"
D14-1137,P07-1121,0,0.682582,"del is a discriminative string-to-tree model which recursively maps overlapping contiguous word sequences to tree nodes at different levels, where efficient dynamic programming algorithms can be used. Such a model may find applications in other areas of natural language processing, such as statistical machine translation and information extraction. 2 2.1 Background Semantics Various semantic formalisms have been considered for semantic parsing. Examples include the tree-structured semantic representations (Wong and Mooney, 2006), the lambda calculus expressions (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), and dependency-based compositional semantic representations (Liang et al., 2013). In this work, we specifically focus on the tree-structured representations for semantics. Each semantic representation consists of semantic units as its tree nodes, where each semantic unit is of the following form: ma ≡ τa : pα (τb ∗) (1) Here ma is used to denote a complete semantic unit, which consists of its semantic type τa , its function symbol pα , as well as an argument list τb ∗ (we assume there are at most two arguments for each semantic unit). In other words, each semantic unit can be regarded as a f"
D14-1137,J13-2005,0,0.336101,"guous word sequences to tree nodes at different levels, where efficient dynamic programming algorithms can be used. Such a model may find applications in other areas of natural language processing, such as statistical machine translation and information extraction. 2 2.1 Background Semantics Various semantic formalisms have been considered for semantic parsing. Examples include the tree-structured semantic representations (Wong and Mooney, 2006), the lambda calculus expressions (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), and dependency-based compositional semantic representations (Liang et al., 2013). In this work, we specifically focus on the tree-structured representations for semantics. Each semantic representation consists of semantic units as its tree nodes, where each semantic unit is of the following form: ma ≡ τa : pα (τb ∗) (1) Here ma is used to denote a complete semantic unit, which consists of its semantic type τa , its function symbol pα , as well as an argument list τb ∗ (we assume there are at most two arguments for each semantic unit). In other words, each semantic unit can be regarded as a function which takes in other semantics of specific types as arguments, and returns"
D14-1137,N04-3012,0,\N,Missing
D14-1137,P11-1060,0,\N,Missing
D15-1102,N13-1122,0,0.0171977,"Missing"
D15-1102,W07-1009,0,0.487584,"task (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005). Additional efforts on addressing the NERC problem under a multilingual or cross lingual setting also exist (Florian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entities are often nested. This fact was often ignored by the community largely due to technical reasons. They therefore proposed to use a constituency parser with a O(n3 ) time complexity (n is the number of words in the input sentence) to handle nested entities, and showed its effectiveness across several datasets. Alex et al. (2007) also presented several approaches by building models on top of linear-chain conditional random fields for recognizing nested entities in biomedical texts. Hoffmann et al. (2011) looked into a separate 3 3.1 Approach Mentions and Their Combinations Typically, a mention that appears in a natural language sentence consists of a contiguous sequence of natural language words. Consider a sentence that consists of n words where each word is indexed with its position in the sentence. A mention m can be uniquely represented with a tuple hbm , em , τ i, where bm and em are the indices of the first and"
D15-1102,P08-1024,0,0.0328942,"∂wk i X − fk (xi , yi ) + 2λwk 3.5 The features that we use are inspired by the work of (Carreras et al., 2002). Specifically, we consider the following features defined over the inputs: • Words (and POS tags, if available) that appear around the current word (with position information), with a window of size 3. (3) • Word n-grams (and POS n-grams, if available) that contain the current word (with position information), for n = 2, 3, 4. i where wk is the weight of the k-th feature fk . We note that unlike many recent latent-variable approaches to structured prediction (Petrov and Klein, 2007; Blunsom et al., 2008), we are able to represent each of our outputs y with a single fullyobserved structure. Thus, our objective function essentially defines a standard regularized softmax regression model, and is therefore convex (Boyd and Vandenberghe, 2004), where a global optimum can be found. The objective function defined in Equation 2 can be optimized with standard gradient-based methods. We used L-BFGS (Liu and Nocedal, 1989) as our optimization method. 3.4 Features • Bag of words around the current word, with a window of size 5. • Word pattern features 2 . Note that these are the indicator functions defin"
D15-1102,P11-1055,0,0.00896767,"ian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entities are often nested. This fact was often ignored by the community largely due to technical reasons. They therefore proposed to use a constituency parser with a O(n3 ) time complexity (n is the number of words in the input sentence) to handle nested entities, and showed its effectiveness across several datasets. Alex et al. (2007) also presented several approaches by building models on top of linear-chain conditional random fields for recognizing nested entities in biomedical texts. Hoffmann et al. (2011) looked into a separate 3 3.1 Approach Mentions and Their Combinations Typically, a mention that appears in a natural language sentence consists of a contiguous sequence of natural language words. Consider a sentence that consists of n words where each word is indexed with its position in the sentence. A mention m can be uniquely represented with a tuple hbm , em , τ i, where bm and em are the indices of the first and last word of the mention, respectively, and τ is its semantic class (type). We can see that for a given sentence consisting of n words, there are altogether tn(n + 1)/2 possible"
D15-1102,W01-1812,0,0.0540666,"ingle hyperedge, while the two brown links that connect two I nodes and one X node form a separate single hyperedge. compactly represent exponentially many possible combinations of potentially overlapping, lengthunbounded mentions of different types. A hypergraph is a generalization of a conventional graph, whose edges (a.k.a. hyperedges) can connect two or more nodes. In this work, we consider a special class of hypergraphs, where each hyperedge consists of a designated parent node and an ordered list of child nodes. Hypergraphs have also been used in other fields, such as syntactic parsing (Klein and Manning, 2001), semantic parsing (Lu, 2015) and machine translation (Cmejrek et al., 2013). Our mention hypergraphs consist of five types of nodes which are used to compactly represent many mentions of different semantic types and boundaries, namely, A nodes, E nodes, T nodes, I nodes, and X nodes. A partial mention hypergraph is depicted in Figure 1. We describe the definition of each type of nodes next. Proof For each mention, there exists a unique path in the mention hypergraph to represent it. For any combination of mentions, there exist unique paths in the mention hypergraph to represent such a combina"
D15-1102,W02-2004,0,0.0186569,"Missing"
D15-1102,D13-1057,1,0.777883,"Missing"
D15-1102,P14-1038,0,0.108717,"s amongst entities. Named entity recognition and classification still remains a popular topic in the field of statistical natural language processing. Ritter et al. (2011) looked into recognizing entities from social media data that involves informal and potentially noisy texts. Pasupat and Liang (2014) looked into the issue of zero-shot entity extraction from Web pages with natural language queries where minimal supervision was used. Neelakantan and Collins (2014) looked into the problem of automatically constructing dictionaries with minimal supervision for improved named entity extraction. Li and Ji (2014) presented an approach to perform the task of extraction of mentions and their relations in a joint and incremental manner. handle overlapping mentions with unbounded lengths. • The learning and inference algorithms of our proposed model have a time complexity that is linear in the number of words in the input sentence and also linear in the number of possible semantic classes/types, making our model scalable to extremely large datasets. • Our model can additionally capture mentions’ head information in a joint manner under the same time complexity. Our system and code are available for downlo"
D15-1102,N13-1006,0,0.0152784,"m fields for extracting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-supervised and unsupervised approaches for such a task (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005). Additional efforts on addressing the NERC problem under a multilingual or cross lingual setting also exist (Florian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entities are often nested. This fact was often ignored by the community largely due to technical reasons. They therefore proposed to use a constituency parser with a O(n3 ) time complexity (n is the number of words in the input sentence) to handle nested entities, and showed its effectiveness across several datasets. Alex et al. (2007) also presented several approaches by building models on top of linear-chain conditional random fields for recognizing nested entities in biomedical texts. Hoffmann et al. (2011) looked into"
D15-1102,P13-1008,0,0.0296153,"ctly representing exponentially many possible mentions enables a mention’s boundaries, type and head information to be jointly learned in a single framework. The model scales linearly with respect to the number of words in the input sentence, and performs exact learning where a unique global optimum can be found. Empirically, we have demonstrated the effectiveness of such a model across several standard datasets. Future work include explorations of efficient algorithms for other information extraction tasks, such as joint mention and relation extraction (Li and Ji, 2014) and event extraction (Li et al., 2013). Our system and code can be downloaded from http://statnlp.org/research/ie/. Results on C ONLL2003 To understand how well our model works on datasets where mentions or entities do not overlap with one another, we conducted additional experiments on the standard dataset used in the C O NLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003), where the named entities strictly do not overlap with one another. We compared our system’s performance against that of a baseline version of the state-of-the-art Illinois NER system (Ratinov and Roth, 2009). Their system performed sequential prediction"
D15-1102,D13-1052,0,0.0269047,"ode form a separate single hyperedge. compactly represent exponentially many possible combinations of potentially overlapping, lengthunbounded mentions of different types. A hypergraph is a generalization of a conventional graph, whose edges (a.k.a. hyperedges) can connect two or more nodes. In this work, we consider a special class of hypergraphs, where each hyperedge consists of a designated parent node and an ordered list of child nodes. Hypergraphs have also been used in other fields, such as syntactic parsing (Klein and Manning, 2001), semantic parsing (Lu, 2015) and machine translation (Cmejrek et al., 2013). Our mention hypergraphs consist of five types of nodes which are used to compactly represent many mentions of different semantic types and boundaries, namely, A nodes, E nodes, T nodes, I nodes, and X nodes. A partial mention hypergraph is depicted in Figure 1. We describe the definition of each type of nodes next. Proof For each mention, there exists a unique path in the mention hypergraph to represent it. For any combination of mentions, there exist unique paths in the mention hypergraph to represent such a combination. These paths altogether form a unique sub-hypergraph of the original hy"
D15-1102,P15-2121,1,0.761848,"hat connect two I nodes and one X node form a separate single hyperedge. compactly represent exponentially many possible combinations of potentially overlapping, lengthunbounded mentions of different types. A hypergraph is a generalization of a conventional graph, whose edges (a.k.a. hyperedges) can connect two or more nodes. In this work, we consider a special class of hypergraphs, where each hyperedge consists of a designated parent node and an ordered list of child nodes. Hypergraphs have also been used in other fields, such as syntactic parsing (Klein and Manning, 2001), semantic parsing (Lu, 2015) and machine translation (Cmejrek et al., 2013). Our mention hypergraphs consist of five types of nodes which are used to compactly represent many mentions of different semantic types and boundaries, namely, A nodes, E nodes, T nodes, I nodes, and X nodes. A partial mention hypergraph is depicted in Figure 1. We describe the definition of each type of nodes next. Proof For each mention, there exists a unique path in the mention hypergraph to represent it. For any combination of mentions, there exist unique paths in the mention hypergraph to represent such a combination. These paths altogether"
D15-1102,J01-1005,0,0.0110782,"using an HMMbased approach. Florian et al. (2003) presented a system for named entity recognition by combining different classifiers. McDonald and Pereira (2005) used conditional random fields for extracting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-supervised and unsupervised approaches for such a task (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005). Additional efforts on addressing the NERC problem under a multilingual or cross lingual setting also exist (Florian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entities are often nested. This fact was often ignored by the community largely due to technical reasons. They therefore proposed to use a constituency parser with a O(n3 ) time complexity (n is the number of words in the input sentence) to handle nested entities, and showed its effectiveness across several datasets. Alex et al. (2007) also presented sev"
D15-1102,N04-4028,0,0.0372638,"mean of the precision (P ) and recall (R) scores, where precision is the ratio between the number of correctly predicted mentions and the total number of predicted mentions, and recall is the ratio between the number of correctly predicted mentions and the total number of gold mentions. We will also adopt these metrics in our evaluations later. Unfortunately, the model only optimizes its objective function defined in Equation 2, which is the negative (regularized) joint log-likelihood. Previous work showed it was possible to optimize the F measure in a log-linear model (Suzuki et al., 2006). Culotta and McCallum (2004) also proposed a method for optimizing information extraction performance based on confidence estimation. Their work is based on linear-chain CRF and estimate the confidence of extracted fields based on marginal probabilities. The technique is not directly applicable to our task where a hypergraph representation is used to encode overlapping mentions. In this work, we used a very simple and intuitive technique for optimizing the F measure. The idea is to further tune the weight of a single parameter – mention penalty based on the development set, after the training process completes. 4 Experim"
D15-1102,P09-1113,0,0.0284313,"Missing"
D15-1102,E14-1048,0,0.0154488,"essing, pages 857–867, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. issue, which is to identify overlapping relations amongst entities. Named entity recognition and classification still remains a popular topic in the field of statistical natural language processing. Ritter et al. (2011) looked into recognizing entities from social media data that involves informal and potentially noisy texts. Pasupat and Liang (2014) looked into the issue of zero-shot entity extraction from Web pages with natural language queries where minimal supervision was used. Neelakantan and Collins (2014) looked into the problem of automatically constructing dictionaries with minimal supervision for improved named entity extraction. Li and Ji (2014) presented an approach to perform the task of extraction of mentions and their relations in a joint and incremental manner. handle overlapping mentions with unbounded lengths. • The learning and inference algorithms of our proposed model have a time complexity that is linear in the number of words in the input sentence and also linear in the number of possible semantic classes/types, making our model scalable to extremely large datasets. • Our model"
D15-1102,D09-1015,0,0.776009,"vely studied in the past few decades by the community, such a semantic tagging task presents several additional new challenges. First, a mention can consist of multiple words, so its length can be arbitrarily long. Second, the mentions can overlap with one another. Popular models used for POS tagging, such as linear-chain conditional random fields (Lafferty et al., 2001) or semi-Markov conditional random fields (Sarawagi and Cohen, 2004) have difficulties coping with these issues. While approaches on addressing these issues exist, current algorithms typically suffer from high time complexity (Finkel and Manning, 2009) and are therefore difficult to scale to large datasets. On the other hand, the problem of designing efficient and scalable models for mention extraction and classification from natural language texts becomes increasingly important in this era where a large volume of textual data is becoming available on the Web every day – users need systems which are able to scale to extremely large datasets to support efficient semantic analysis for timely decisonmaking. In this paper, we tackle the above-mentioned issue by introducing a novel model for joint mention extraction and classification. We make t"
D15-1102,P14-1037,0,0.021832,"al attention, largely due • We propose a model that is able to effectively 857 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 857–867, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. issue, which is to identify overlapping relations amongst entities. Named entity recognition and classification still remains a popular topic in the field of statistical natural language processing. Ritter et al. (2011) looked into recognizing entities from social media data that involves informal and potentially noisy texts. Pasupat and Liang (2014) looked into the issue of zero-shot entity extraction from Web pages with natural language queries where minimal supervision was used. Neelakantan and Collins (2014) looked into the problem of automatically constructing dictionaries with minimal supervision for improved named entity extraction. Li and Ji (2014) presented an approach to perform the task of extraction of mentions and their relations in a joint and incremental manner. handle overlapping mentions with unbounded lengths. • The learning and inference algorithms of our proposed model have a time complexity that is linear in the numbe"
D15-1102,W03-0425,0,0.0127339,"g our model scalable to extremely large datasets. • Our model can additionally capture mentions’ head information in a joint manner under the same time complexity. Our system and code are available for download from http://statnlp.org/research/ie/. 2 Related Work Existing work has been largely focused on the task of named entity recognition and classification (NERC). The survey of (Nadeau and Sekine, 2007) is a comprehensive study of this topic. Most prior work took a supervised learning approach. Zhou and Su (2002) presented a system for recognizing named entities using an HMMbased approach. Florian et al. (2003) presented a system for named entity recognition by combining different classifiers. McDonald and Pereira (2005) used conditional random fields for extracting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-supervised and unsupervised approaches for such a task (Cucchiarelli and Velardi, 2001; Etzioni et al., 20"
D15-1102,N04-1001,0,0.134894,"t the same time identify their semantic classes (e.g., person, organization, etc). Such a task is often known as named entity recognition and classification (NERC), one of the standard tasks in information extraction (IE). While such a task focuses on the extraction and classification of entities in the texts which are named, recently researchers also showed interest in a closely related task – mention extraction and classification/typing. Unlike a named entity, a mention is typically defined as a reference to an entity in natural language text that can be either named, nominal or pronominal (Florian et al., 2004). The task of mention detection and tracking has received substantial attention, largely due • We propose a model that is able to effectively 857 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 857–867, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. issue, which is to identify overlapping relations amongst entities. Named entity recognition and classification still remains a popular topic in the field of statistical natural language processing. Ritter et al. (2011) looked into recognizing entities from soc"
D15-1102,W09-1119,1,0.942365,"p.org/research/ie/. 2 Related Work Existing work has been largely focused on the task of named entity recognition and classification (NERC). The survey of (Nadeau and Sekine, 2007) is a comprehensive study of this topic. Most prior work took a supervised learning approach. Zhou and Su (2002) presented a system for recognizing named entities using an HMMbased approach. Florian et al. (2003) presented a system for named entity recognition by combining different classifiers. McDonald and Pereira (2005) used conditional random fields for extracting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-supervised and unsupervised approaches for such a task (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005). Additional efforts on addressing the NERC problem under a multilingual or cross lingual setting also exist (Florian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entitie"
D15-1102,D11-1141,0,0.0137711,"be either named, nominal or pronominal (Florian et al., 2004). The task of mention detection and tracking has received substantial attention, largely due • We propose a model that is able to effectively 857 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 857–867, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. issue, which is to identify overlapping relations amongst entities. Named entity recognition and classification still remains a popular topic in the field of statistical natural language processing. Ritter et al. (2011) looked into recognizing entities from social media data that involves informal and potentially noisy texts. Pasupat and Liang (2014) looked into the issue of zero-shot entity extraction from Web pages with natural language queries where minimal supervision was used. Neelakantan and Collins (2014) looked into the problem of automatically constructing dictionaries with minimal supervision for improved named entity extraction. Li and Ji (2014) presented an approach to perform the task of extraction of mentions and their relations in a joint and incremental manner. handle overlapping mentions wit"
D15-1102,P06-1028,0,0.00967452,"efined as the harmonic mean of the precision (P ) and recall (R) scores, where precision is the ratio between the number of correctly predicted mentions and the total number of predicted mentions, and recall is the ratio between the number of correctly predicted mentions and the total number of gold mentions. We will also adopt these metrics in our evaluations later. Unfortunately, the model only optimizes its objective function defined in Equation 2, which is the negative (regularized) joint log-likelihood. Previous work showed it was possible to optimize the F measure in a log-linear model (Suzuki et al., 2006). Culotta and McCallum (2004) also proposed a method for optimizing information extraction performance based on confidence estimation. Their work is based on linear-chain CRF and estimate the confidence of extracted fields based on marginal probabilities. The technique is not directly applicable to our task where a hypergraph representation is used to encode overlapping mentions. In this work, we used a very simple and intuitive technique for optimizing the F measure. The idea is to further tune the weight of a single parameter – mention penalty based on the development set, after the training"
D15-1102,W03-0419,0,0.0555241,"Missing"
D15-1102,P13-1106,0,0.0117956,"cting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-supervised and unsupervised approaches for such a task (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005). Additional efforts on addressing the NERC problem under a multilingual or cross lingual setting also exist (Florian et al., 2004; Che et al., 2013; Wang et al., 2013). As pointed out by Finkel and Manning (2009), named entities are often nested. This fact was often ignored by the community largely due to technical reasons. They therefore proposed to use a constituency parser with a O(n3 ) time complexity (n is the number of words in the input sentence) to handle nested entities, and showed its effectiveness across several datasets. Alex et al. (2007) also presented several approaches by building models on top of linear-chain conditional random fields for recognizing nested entities in biomedical texts. Hoffmann et al. (2011) looked into a separate 3 3.1 Ap"
D15-1102,P02-1060,0,0.279921,"rds in the input sentence and also linear in the number of possible semantic classes/types, making our model scalable to extremely large datasets. • Our model can additionally capture mentions’ head information in a joint manner under the same time complexity. Our system and code are available for download from http://statnlp.org/research/ie/. 2 Related Work Existing work has been largely focused on the task of named entity recognition and classification (NERC). The survey of (Nadeau and Sekine, 2007) is a comprehensive study of this topic. Most prior work took a supervised learning approach. Zhou and Su (2002) presented a system for recognizing named entities using an HMMbased approach. Florian et al. (2003) presented a system for named entity recognition by combining different classifiers. McDonald and Pereira (2005) used conditional random fields for extracting gene and protein mentions from biomedical texts. Ratinov and Roth (2009) presented a systematic analysis over several issues related to the design of a named entity recognition and classification system where issues such as chunk representations and the choice of inference algorithms were discussed. Researchers also looked into semi-superv"
D15-1170,P13-2009,0,0.382508,"before pre-‐processing NL’: what be the area of sea0le MRL’: answer@1 area_1@1 cityid@2 sea0le@s _@0 (b) aGer pre-‐processing Figure 1: Example of a sentence pair in NL and MRL. naturally viewed as a statistical machine translation (SMT) task, which translates a sentence in NL (i.e., the source language in SMT) into its meaning representation in MRL (i.e., the target language in SMT). Indeed, many attempts have been made to directly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013). However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art. For details, please see performance comparison in Andreas et al. (2013) and Lu (2014). Introduction Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently received a significant amount"
D15-1170,D10-1119,0,0.187631,"the hybrid tree structures, to learn a generative process under a Bayesian framework. RHT (Lu, 2014) defined distributions over relaxed hybrid tree structures that jointly represented both sentences and semantics. Most recently, f-RHT (Lu, 2015) introduced constrained semantic forests to improve RHT model. SCISSOR (Ge and Mooney, 2005) augmented syntactic parse tree with semantic information and then performed integrated semantic and syntactic parsing to NL sentences. KRISP (Mooney, 2006) used string classifiers to label substrings of an NL with entities from the meaning representation. UBL (Kwiatkowski et al., 2010) performed semantic parsing with an automatically-induced CCG lexicon. Table 7 shows the evaluation results of our system as well as those of several other comparable related works which share the same experiment setup as ours. We can observe from Table 7 that semantic parsing with SMT components gives 1462 System non-enriched + gdfa non-enriched + all enriched + gdfa enriched + all enriched + all + unknown word translation English Acc. F1 77.5 83.5 81.5 85.2 78.9 83.9 82.9 86.1 86.3 87.1 German Acc. F1 66.0 74.9 72.1 76.8 66.7 74.6 75.4 79.5 79.1 80.3 Greek Acc. F1 65.6 74.1 75.2 80.5 67.8 76"
D15-1170,P14-1133,0,0.0598895,"a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data. We also noticed recent advance in tree-based SMT. Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree labels on the target side. However, due to the constraint that each targ"
D15-1170,D11-1140,0,0.299131,"to obtain improvement on Thai, the performance is still lower than those of RHT and TREETRANS. This is probably because of the low quality of word alignment output between this Asian language and MRL. 6 Conclusion and Future Work In this paper, we have presented an enriched SCFG approach for semantic parsing which realizes the potential of the SMT approach. The performance improvement is contributed from the extension of translation rules with informative symbols and increased coverage. Such an extension share a similar spirit as generalization of a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to exten"
D15-1170,D13-1160,0,0.109238,"as generalization of a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data. We also noticed recent advance in tree-based SMT. Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree labels on the target side. However, due to the"
D15-1170,D13-1161,0,0.120885,"ry dataset, which is publicly available. 4.1 Experimental Settings Data GeoQuery dataset consists of 880 questions paired with their corresponding tree structured semantic representations. Following the experimental setup in Jones et al. (2012), we use the 600 question pairs to train and tune our SMT de1459 SCFG coder, and evaluated on the remaining 280. Note that there is another version of GeoQuery dataset where the semantic representation is annotated with lambda calculus expressions and which is extensively studied (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). Performance on the version of lambda calculus is higher than that on the tree structured version, however, the results obtained over the two versions are not directly comparable. SMT Setting We use cdec (Dyer et al., 2010) as our HPB decoder. As mentioned above, 600 instances are used to train and tune our decoder. To get fair results, we split the 600 instances into 10 folds, each having 60 instances. Then for each fold, we use it as the tuning data while the other 540 instances and the NP list are used as training data.7 We use IRSTLM toolkit (Federico et al., 2008) to train a 5-gram LM on"
D15-1170,J93-2003,0,0.0296164,"glue rules can be applied to any two neighboring translation nodes if the non-terminal symbols are matched. 3.2 Word Alignment for Semantic Parsing Word alignment is an essential step for rule extraction in SMT, where recognizing that wo shi in Chinese is a good translation for I am in English requires establishing a correspondence between wo and I, and between shi and am. In the SMT community, researchers have developed standard, proven alignment tools such as GIZA++ (Och and Ney, 2003), which can be used to train IBM Models 1-5. However, there is one fundamental problem with the IBM models (Brown et al., 1993): each word on one side can be traced back to exactly one particular on the other word (or the null token which indicates the word aligns to no word on the other side). Figure 2(a) shows an example of GIZA++ alignment output from source side to target side, from which we can see that each source word aligns to exactly one target word. While alignment of multiple target words to one source word is common in SMT, a trick is then to run IBM model training in both directions. Then two resulting word alignments can be symmetrized, for instance, taking the intersection or the union of alignment poin"
D15-1170,W12-3128,1,0.907527,"Missing"
D15-1170,D08-1024,0,0.0333419,"Missing"
D15-1170,J07-2003,0,0.823312,"ch translates a sentence in NL (i.e., the source language in SMT) into its meaning representation in MRL (i.e., the target language in SMT). Indeed, many attempts have been made to directly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013). However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art. For details, please see performance comparison in Andreas et al. (2013) and Lu (2014). Introduction Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently received a significant amount of attention with various models proposed over the past few years. Consider the NL sentence paired with its corresponding MRL in Figure 1(a). Semantic parsing can be The key issues behind the limited success of applying SMT systems directly"
D15-1170,W10-2903,0,0.116255,"are a similar spirit as generalization of a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data. We also noticed recent advance in tree-based SMT. Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree labels on the target side."
D15-1170,P10-4002,0,0.0148194,"et al. (2012), we use the 600 question pairs to train and tune our SMT de1459 SCFG coder, and evaluated on the remaining 280. Note that there is another version of GeoQuery dataset where the semantic representation is annotated with lambda calculus expressions and which is extensively studied (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). Performance on the version of lambda calculus is higher than that on the tree structured version, however, the results obtained over the two versions are not directly comparable. SMT Setting We use cdec (Dyer et al., 2010) as our HPB decoder. As mentioned above, 600 instances are used to train and tune our decoder. To get fair results, we split the 600 instances into 10 folds, each having 60 instances. Then for each fold, we use it as the tuning data while the other 540 instances and the NP list are used as training data.7 We use IRSTLM toolkit (Federico et al., 2008) to train a 5-gram LM on the MRL0 side of the training data, using modified Kneser-Ney smoothing. We use Mira (Chiang et al., 2008) to tune the parameters of the system to maximize BLEU (Papineni et al., 2002). When extracting translation rules fro"
D15-1170,P11-1060,0,0.333497,"Missing"
D15-1170,D08-1082,1,0.929001,"to avoid the issues caused by word alignment between NL and MRL, we triple training data with each sentence pair having multiple alignments. However, WASP used a sequence of productions to represent MRL before running GIZA++. Third, we use typical features in HPB SMT (e.g., phrase translation probabilities, lexical translation probabilities, language model feature, etc.) while WASP used rule identity features. SMT-SemParse (Andreas et al., 2013) adapted standard SMT components for semantic parsing. The present work is based on theirs with all the extensions detailed in Section 3. HYBRIDTREE+ (Lu et al., 2008) learned a synchronous generative model which simultaneously generated a NL sentence and an MRL tree. tsVB (Jones et al., 2012) used tree transducers, which were similar to the hybrid tree structures, to learn a generative process under a Bayesian framework. RHT (Lu, 2014) defined distributions over relaxed hybrid tree structures that jointly represented both sentences and semantics. Most recently, f-RHT (Lu, 2015) introduced constrained semantic forests to improve RHT model. SCISSOR (Ge and Mooney, 2005) augmented syntactic parse tree with semantic information and then performed integrated se"
D15-1170,D14-1137,1,0.858388,"rectly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013). However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art. For details, please see performance comparison in Andreas et al. (2013) and Lu (2014). Introduction Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently received a significant amount of attention with various models proposed over the past few years. Consider the NL sentence paired with its corresponding MRL in Figure 1(a). Semantic parsing can be The key issues behind the limited success of applying SMT systems directly to semantic parsing lie in the difference between semantic parsing and SMT: MRL is not a real natural language with different properties from natural language. First, MRL is mach"
D15-1170,P15-2121,1,0.776128,"e (Andreas et al., 2013) adapted standard SMT components for semantic parsing. The present work is based on theirs with all the extensions detailed in Section 3. HYBRIDTREE+ (Lu et al., 2008) learned a synchronous generative model which simultaneously generated a NL sentence and an MRL tree. tsVB (Jones et al., 2012) used tree transducers, which were similar to the hybrid tree structures, to learn a generative process under a Bayesian framework. RHT (Lu, 2014) defined distributions over relaxed hybrid tree structures that jointly represented both sentences and semantics. Most recently, f-RHT (Lu, 2015) introduced constrained semantic forests to improve RHT model. SCISSOR (Ge and Mooney, 2005) augmented syntactic parse tree with semantic information and then performed integrated semantic and syntactic parsing to NL sentences. KRISP (Mooney, 2006) used string classifiers to label substrings of an NL with entities from the meaning representation. UBL (Kwiatkowski et al., 2010) performed semantic parsing with an automatically-induced CCG lexicon. Table 7 shows the evaluation results of our system as well as those of several other comparable related works which share the same experiment setup as"
D15-1170,P06-1115,0,0.552136,"Missing"
D15-1170,J03-1002,0,0.012028,"bility p (α |γ), and a glue rule penalty. Table 1(b) shows examples of a straight and an inverted glue rules. Moreover, these glue rules can be applied to any two neighboring translation nodes if the non-terminal symbols are matched. 3.2 Word Alignment for Semantic Parsing Word alignment is an essential step for rule extraction in SMT, where recognizing that wo shi in Chinese is a good translation for I am in English requires establishing a correspondence between wo and I, and between shi and am. In the SMT community, researchers have developed standard, proven alignment tools such as GIZA++ (Och and Ney, 2003), which can be used to train IBM Models 1-5. However, there is one fundamental problem with the IBM models (Brown et al., 1993): each word on one side can be traced back to exactly one particular on the other word (or the null token which indicates the word aligns to no word on the other side). Figure 2(a) shows an example of GIZA++ alignment output from source side to target side, from which we can see that each source word aligns to exactly one target word. While alignment of multiple target words to one source word is common in SMT, a trick is then to run IBM model training in both directio"
D15-1170,W05-0602,0,0.0486525,"e present work is based on theirs with all the extensions detailed in Section 3. HYBRIDTREE+ (Lu et al., 2008) learned a synchronous generative model which simultaneously generated a NL sentence and an MRL tree. tsVB (Jones et al., 2012) used tree transducers, which were similar to the hybrid tree structures, to learn a generative process under a Bayesian framework. RHT (Lu, 2014) defined distributions over relaxed hybrid tree structures that jointly represented both sentences and semantics. Most recently, f-RHT (Lu, 2015) introduced constrained semantic forests to improve RHT model. SCISSOR (Ge and Mooney, 2005) augmented syntactic parse tree with semantic information and then performed integrated semantic and syntactic parsing to NL sentences. KRISP (Mooney, 2006) used string classifiers to label substrings of an NL with entities from the meaning representation. UBL (Kwiatkowski et al., 2010) performed semantic parsing with an automatically-induced CCG lexicon. Table 7 shows the evaluation results of our system as well as those of several other comparable related works which share the same experiment setup as ours. We can observe from Table 7 that semantic parsing with SMT components gives 1462 Syst"
D15-1170,P12-1051,0,0.209181,"nd. words. Note that it may generate a synthetic rule with null at the target side since the lexical translation table derived from aligned training data contains translation to null. Each synthetic translation rule for unknown words is associated with three features returned from function generate rule. 4 Experimentation In this section, we test our approach on the GeoQuery dataset, which is publicly available. 4.1 Experimental Settings Data GeoQuery dataset consists of 880 questions paired with their corresponding tree structured semantic representations. Following the experimental setup in Jones et al. (2012), we use the 600 question pairs to train and tune our SMT de1459 SCFG coder, and evaluated on the remaining 280. Note that there is another version of GeoQuery dataset where the semantic representation is annotated with lambda calculus expressions and which is extensively studied (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). Performance on the version of lambda calculus is higher than that on the tree structured version, however, the results obtained over the two versions are not directly comparable. SMT Setting We use cdec (Dyer et al.,"
D15-1170,P02-1040,0,0.0958993,"ectly comparable. SMT Setting We use cdec (Dyer et al., 2010) as our HPB decoder. As mentioned above, 600 instances are used to train and tune our decoder. To get fair results, we split the 600 instances into 10 folds, each having 60 instances. Then for each fold, we use it as the tuning data while the other 540 instances and the NP list are used as training data.7 We use IRSTLM toolkit (Federico et al., 2008) to train a 5-gram LM on the MRL0 side of the training data, using modified Kneser-Ney smoothing. We use Mira (Chiang et al., 2008) to tune the parameters of the system to maximize BLEU (Papineni et al., 2002). When extracting translation rules from aligned training data, we include both tight and untight phrases. Evaluation We use the standard evaluation criteria for evaluation by executing both the predicted MRL and the gold standard against the database and obtaining their respective answer. Specifically, we convert a translation from MRL0 into MRL (if exists). The translation then is considered correct if and only if its MRL retrieves the same answers as the gold standard MRL (Jones et al., 2012), allowing for a fair comparison between our systems and previous works. As in Jones et al. (2012),"
D15-1170,P07-2045,0,0.00630803,"Missing"
D15-1170,D09-1001,0,0.0817093,"age. Such an extension share a similar spirit as generalization of a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data. We also noticed recent advance in tree-based SMT. Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree label"
D15-1170,P08-1066,0,0.0136646,"goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data. We also noticed recent advance in tree-based SMT. Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree labels on the target side. However, due to the constraint that each target phrase needs to map to a syntactic constituent, phrase tables in tree-based translation models usually suffer from the low coverage issue, especially if the training data size is small. Therefore, another direction of our future work is to explore specific problems that will emerge when employing tree-based SMT systems to semantic parsing, and provide solutions to them. Ackno"
D15-1170,C12-2122,0,0.0498778,"Missing"
D15-1170,D14-1135,0,0.10458,"Thai, the performance is still lower than those of RHT and TREETRANS. This is probably because of the low quality of word alignment output between this Asian language and MRL. 6 Conclusion and Future Work In this paper, we have presented an enriched SCFG approach for semantic parsing which realizes the potential of the SMT approach. The performance improvement is contributed from the extension of translation rules with informative symbols and increased coverage. Such an extension share a similar spirit as generalization of a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framew"
D15-1170,N06-1056,0,0.475908,"erman and Greek. 1 (a) before pre-‐processing NL’: what be the area of sea0le MRL’: answer@1 area_1@1 cityid@2 sea0le@s _@0 (b) aGer pre-‐processing Figure 1: Example of a sentence pair in NL and MRL. naturally viewed as a statistical machine translation (SMT) task, which translates a sentence in NL (i.e., the source language in SMT) into its meaning representation in MRL (i.e., the target language in SMT). Indeed, many attempts have been made to directly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013). However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art. For details, please see performance comparison in Andreas et al. (2013) and Lu (2014). Introduction Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently receiv"
D15-1170,P07-1121,0,0.64142,"section, we test our approach on the GeoQuery dataset, which is publicly available. 4.1 Experimental Settings Data GeoQuery dataset consists of 880 questions paired with their corresponding tree structured semantic representations. Following the experimental setup in Jones et al. (2012), we use the 600 question pairs to train and tune our SMT de1459 SCFG coder, and evaluated on the remaining 280. Note that there is another version of GeoQuery dataset where the semantic representation is annotated with lambda calculus expressions and which is extensively studied (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). Performance on the version of lambda calculus is higher than that on the tree structured version, however, the results obtained over the two versions are not directly comparable. SMT Setting We use cdec (Dyer et al., 2010) as our HPB decoder. As mentioned above, 600 instances are used to train and tune our decoder. To get fair results, we split the 600 instances into 10 folds, each having 60 instances. Then for each fold, we use it as the tuning data while the other 540 instances and the NP list are used as training data.7 We use IRSTLM toolkit"
D15-1170,P01-1067,0,0.654953,"anslation (SMT) task, which translates a sentence in NL (i.e., the source language in SMT) into its meaning representation in MRL (i.e., the target language in SMT). Indeed, many attempts have been made to directly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013). However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art. For details, please see performance comparison in Andreas et al. (2013) and Lu (2014). Introduction Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently received a significant amount of attention with various models proposed over the past few years. Consider the NL sentence paired with its corresponding MRL in Figure 1(a). Semantic parsing can be The key issues behind the limited success of applying SMT s"
D15-1170,W06-3119,0,0.0500807,"te which nonterminal occurrences are linked by ∼. The fact that SCFGs in HPB models contain only one type of non-terminal symbol2 is responsible for ill-formed translation (e.g., answer@1 state@1). To this end, we enrich the nonterminals to capture the tree structure information, guiding the translation in favor of well-formed translations. The enrichment of non-terminals is two-fold: first, it can handle MRL with a nested structure to guarantee the well-formed translations; second, related studies in SMT have shown that introducing multiple non-terminal symbols in SCFGs benefits translation (Zollmann and Venugopal, 2006; Li et al., 2012). Given a word sequence eij from position i to position j in MRL0 , we enrich the non-terminal symbol X to reflect the internal structure of the word sequence of eij . A correct translation rule selection therefore not only maps source terminals into target terminals, but is both constrained and guided by structure information in the nonterminals. As mentioned earlier, we regard the nested structure in MRL0 as function-argument structure, where each function takes one or more arguments as input while its return serves as an argument to the outside function. As in Figure 1, fu"
D16-1008,C12-1004,0,0.0172381,"iguous entities in a medical domain. Words annotated with the same index are part of the same entity. Note that entity 3 and entity 4 overlap with one another. Introduction Building effective automatic named entity recognition (NER) systems that is capable of extracting useful semantic shallow information from texts has been one of the most important tasks in the field of natural language processing. An effective NER system can typically play an important role in certain downstream NLP tasks such as relation extraction, event extraction, and knowledge base construction (Hasegawa et al., 2004; Al-Rfou and Skiena, 2012). Most traditional NER systems are capable of extracting entities1 as short spans of texts. Two basic assumptions are typically made when extract1 Or sometimes mentions are considered, which can be named, nominal or pronominal references to entities (Florian et al., 2004). In this paper we use “mentions” and “entities” interchangeably. ing entities: 1) entities do not overlap with one another, and 2) each entity consists of a contiguous sequence of words. These assumptions allow the task to be modeled as a sequence labeling task, for which many existing models are readily available, such as li"
D16-1008,N03-1002,0,0.047906,"nt models’ ambiguity under this theoretical framework. • Empirically, we demonstrate that our model can significantly outperform conventional approaches designed for handling discontiguous entities on data which contains many discontiguous entities. 2 Related Work Learning to recognize named entities is a popular task in the field of natural language processing. A survey by Nadeau (2007) lists several approaches in NER, including Hidden Markov Models (HMM) (Bikel et al., 1997), Decision Trees (Sekine, 1998), Maximum Entropy Models (Borthwick and Sterling, 1998), Support Vector Machines (SVM) (Asahara and Matsumoto, 2003), and also semi-supervised and unsupervised approaches. Ratinov (2009) utilizes averaged perceptron to solve this problem and also focused on four key design decisions, achieving state-of-the-art in MUC-7 dataset. These ap76 proaches work on standard texts, such as news articles, and the entities to be recognized are defined to be contiguous and non-overlapping. Noticing that many named entities contain other named entities inside them, Finkel and Manning (2009) proposed a model that is capable of extracting nested named entities by representing the sentence as a constituency parse tree, with"
D16-1008,A97-1029,0,0.375946,"ifying the ambiguity of different NER models that can handle discontiguous entities. We present a study and make comparisons about different models’ ambiguity under this theoretical framework. • Empirically, we demonstrate that our model can significantly outperform conventional approaches designed for handling discontiguous entities on data which contains many discontiguous entities. 2 Related Work Learning to recognize named entities is a popular task in the field of natural language processing. A survey by Nadeau (2007) lists several approaches in NER, including Hidden Markov Models (HMM) (Bikel et al., 1997), Decision Trees (Sekine, 1998), Maximum Entropy Models (Borthwick and Sterling, 1998), Support Vector Machines (SVM) (Asahara and Matsumoto, 2003), and also semi-supervised and unsupervised approaches. Ratinov (2009) utilizes averaged perceptron to solve this problem and also focused on four key design decisions, achieving state-of-the-art in MUC-7 dataset. These ap76 proaches work on standard texts, such as news articles, and the entities to be recognized are defined to be contiguous and non-overlapping. Noticing that many named entities contain other named entities inside them, Finkel and M"
D16-1008,M98-1018,0,0.26375,"ntities. We present a study and make comparisons about different models’ ambiguity under this theoretical framework. • Empirically, we demonstrate that our model can significantly outperform conventional approaches designed for handling discontiguous entities on data which contains many discontiguous entities. 2 Related Work Learning to recognize named entities is a popular task in the field of natural language processing. A survey by Nadeau (2007) lists several approaches in NER, including Hidden Markov Models (HMM) (Bikel et al., 1997), Decision Trees (Sekine, 1998), Maximum Entropy Models (Borthwick and Sterling, 1998), Support Vector Machines (SVM) (Asahara and Matsumoto, 2003), and also semi-supervised and unsupervised approaches. Ratinov (2009) utilizes averaged perceptron to solve this problem and also focused on four key design decisions, achieving state-of-the-art in MUC-7 dataset. These ap76 proaches work on standard texts, such as news articles, and the entities to be recognized are defined to be contiguous and non-overlapping. Noticing that many named entities contain other named entities inside them, Finkel and Manning (2009) proposed a model that is capable of extracting nested named entities by"
D16-1008,D09-1015,0,0.263911,"Missing"
D16-1008,N04-1001,0,0.446319,"l semantic shallow information from texts has been one of the most important tasks in the field of natural language processing. An effective NER system can typically play an important role in certain downstream NLP tasks such as relation extraction, event extraction, and knowledge base construction (Hasegawa et al., 2004; Al-Rfou and Skiena, 2012). Most traditional NER systems are capable of extracting entities1 as short spans of texts. Two basic assumptions are typically made when extract1 Or sometimes mentions are considered, which can be named, nominal or pronominal references to entities (Florian et al., 2004). In this paper we use “mentions” and “entities” interchangeably. ing entities: 1) entities do not overlap with one another, and 2) each entity consists of a contiguous sequence of words. These assumptions allow the task to be modeled as a sequence labeling task, for which many existing models are readily available, such as linear-chain CRFs (McCallum and Li, 2003). While the above two assumptions are valid for most cases, they are not always true. For example, in the entity University of New Hampshire of type ORG there exists another entity New Hampshire of type LOC. This violates the first a"
D16-1008,P04-1053,0,0.013862,"es. 1 Figure 1: Discontiguous entities in a medical domain. Words annotated with the same index are part of the same entity. Note that entity 3 and entity 4 overlap with one another. Introduction Building effective automatic named entity recognition (NER) systems that is capable of extracting useful semantic shallow information from texts has been one of the most important tasks in the field of natural language processing. An effective NER system can typically play an important role in certain downstream NLP tasks such as relation extraction, event extraction, and knowledge base construction (Hasegawa et al., 2004; Al-Rfou and Skiena, 2012). Most traditional NER systems are capable of extracting entities1 as short spans of texts. Two basic assumptions are typically made when extract1 Or sometimes mentions are considered, which can be named, nominal or pronominal references to entities (Florian et al., 2004). In this paper we use “mentions” and “entities” interchangeably. ing entities: 1) entities do not overlap with one another, and 2) each entity consists of a contiguous sequence of words. These assumptions allow the task to be modeled as a sequence labeling task, for which many existing models are re"
D16-1008,D15-1102,1,0.800122,"ch many existing models are readily available, such as linear-chain CRFs (McCallum and Li, 2003). While the above two assumptions are valid for most cases, they are not always true. For example, in the entity University of New Hampshire of type ORG there exists another entity New Hampshire of type LOC. This violates the first assumption above, yet it is crucial to extract both entities for subsequent tasks such as relation extraction and knowledge base construction. Researchers therefore have proposed to tackle the above issues in NER using more sophisticated models (Finkel and Manning, 2009; Lu and Roth, 2015). Such efforts still largely rely on the second assumption. Unfortunately, the second assumption is also not always true in practice. There are also cases where the entities are composed of multiple discontiguous sequences of words, such as in disorder mention recognition in clinical texts (Pradhan et al., 2014b), where the entities (disorder mentions in this case) may be discontiguous. Consider the example shown in Figure 1. In this example there are four enti75 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 75–84, c Austin, Texas, November 1-5,"
D16-1008,W03-0430,0,0.0242628,"l NER systems are capable of extracting entities1 as short spans of texts. Two basic assumptions are typically made when extract1 Or sometimes mentions are considered, which can be named, nominal or pronominal references to entities (Florian et al., 2004). In this paper we use “mentions” and “entities” interchangeably. ing entities: 1) entities do not overlap with one another, and 2) each entity consists of a contiguous sequence of words. These assumptions allow the task to be modeled as a sequence labeling task, for which many existing models are readily available, such as linear-chain CRFs (McCallum and Li, 2003). While the above two assumptions are valid for most cases, they are not always true. For example, in the entity University of New Hampshire of type ORG there exists another entity New Hampshire of type LOC. This violates the first assumption above, yet it is crucial to extract both entities for subsequent tasks such as relation extraction and knowledge base construction. Researchers therefore have proposed to tackle the above issues in NER using more sophisticated models (Finkel and Manning, 2009; Lu and Roth, 2015). Such efforts still largely rely on the second assumption. Unfortunately, the"
D16-1008,N16-1085,1,0.86792,"Missing"
D16-1008,S14-2007,0,0.0734479,". water shed - Infarctions . . . embolic You see blood or dark/black material when you vomit or have a bowel movement. - blood . . . vomit - blood . . . bowel movement - dark . . . material . . . vomit - dark . . . bowel movement - black material . . . vomit - black material . . . bowel movement and discontiguous mentions, as each of the components in {blood, dark, black material} is paired with each of the word in {vomit, bowel movement}, resulting in six mentions in total, with one having three components (dark . . . material . . . vomit). 5.2 Motivated by the features used by Zhang et al. (2014), for both the linear-chain CRF model and our models we use the following features: neighbouring words with relative position information (we consider previous and next k words, where k=1, 2, 3), neighbouring words with relative position information paired with the current word, word n-grams containing the current word (n=2,3), POS tag for the current word, POS tag n-grams containing the current word (n=2,3), orthographic features (prefix, suffix, capitalization, lemma), note type (discharge summary, echo report, radiology, and ECG report), section name (e.g. Medications, Past Medical History)"
D16-1008,W09-1119,0,0.554302,"Missing"
D16-1008,M98-1019,0,0.12303,"models that can handle discontiguous entities. We present a study and make comparisons about different models’ ambiguity under this theoretical framework. • Empirically, we demonstrate that our model can significantly outperform conventional approaches designed for handling discontiguous entities on data which contains many discontiguous entities. 2 Related Work Learning to recognize named entities is a popular task in the field of natural language processing. A survey by Nadeau (2007) lists several approaches in NER, including Hidden Markov Models (HMM) (Bikel et al., 1997), Decision Trees (Sekine, 1998), Maximum Entropy Models (Borthwick and Sterling, 1998), Support Vector Machines (SVM) (Asahara and Matsumoto, 2003), and also semi-supervised and unsupervised approaches. Ratinov (2009) utilizes averaged perceptron to solve this problem and also focused on four key design decisions, achieving state-of-the-art in MUC-7 dataset. These ap76 proaches work on standard texts, such as news articles, and the entities to be recognized are defined to be contiguous and non-overlapping. Noticing that many named entities contain other named entities inside them, Finkel and Manning (2009) proposed a model"
D16-1008,N03-1033,0,0.0969528,"g words with relative position information (we consider previous and next k words, where k=1, 2, 3), neighbouring words with relative position information paired with the current word, word n-grams containing the current word (n=2,3), POS tag for the current word, POS tag n-grams containing the current word (n=2,3), orthographic features (prefix, suffix, capitalization, lemma), note type (discharge summary, echo report, radiology, and ECG report), section name (e.g. Medications, Past Medical History)3 , Brown cluster, and wordlevel semantic category information4 . We used Stanford POS tagger (Toutanova et al., 2003) for POS tagging, and NLP4J package5 for lemmatization. For Brown cluster features, following Tang et al. (2013), we used 1,000 clusters from the combination of training, development, and test set, and used all the subpaths of the cluster IDs as features. 5.3 Figure 4: Examples of discontiguous and overlapping mentions, taken from the dataset. Figure 4 shows some examples of the mentions. The first example shows two discontiguous mentions that do not overlap. The second example shows a typical discontiguous and overlapping case. The last example shows a very hard case of overlapping 2 It is te"
D16-1095,W06-1615,0,0.176921,"d allows practitioners to tune hyper-parameters to encourage transfer between close domains and avoid negative transfer between distant ones. 1 Domain Adaptation Framework O(Di ; wi ) = Li (Di ; wi ) − λi ||wi ||2 , (1) and we maximize Li by tuning the parameter vector wi . For example, Li can be the log-likelihood or the negative hinge loss. The term λi ||wi ||2 is the L2 regularization term where λi is a positive scalar. In our framework, we propose to maximize Introduction Domain adaptation (DA) is an important problem that has received substantial attention in natural language processing (Blitzer et al., 2006; Daum´e III, 2007; Finkel and Manning, 2009; Daum´e III et al., 2010). In this paper, we propose a novel regularization framework which allows DA practitioners to tune hyper-parameters to encourage transfer between close domains, and avoid negative transfer (Rosenstein et al., 2005) between distant ones. In our framework, model parameters in multiple domains are learned jointly and constrained to remain close to one another. In the transfer learning taxonomy (Pan and Yang, 2010), our framework falls under the parameter-transfer category for multi-task inductive learning. We show that our fram"
D16-1095,P07-1056,0,0.139067,"he 6 domains is also shown in the table. Model LR Experimental Results In this section we apply our framework to both structured and un-structured tasks. For structured prediction, we use the named-entity recognition (NER) ACE-2005 dataset with 7 classes and 6 domains. We apply the linear chain CRF (Lafferty et al., 2001), and show results using standard and softmaxmargin CRF (SM-CRF) (Gimpel and Smith, 2010), with features consisting of word shape features, neighboring words, previous prediction and prefixes/suffixes. The second task is sentiment classification on the Amazon review data set (Blitzer et al., 2007) from 4 domains, labeled positive or negative. We apply logistic regression (LR) and SVM using unigram and bigram features. All the models used in this section are implemented on top of a common framework, which was also used to implement various structured prediction models previously (Lu, 2015; Lu and Roth, 2015; Muis and Lu, 2016). For each task we compare: TGT Trained only on the specific domain data, ALL Trained on the data from all domains, Dom. bc bn cts nw un wl avg SVM Dom. TGT ALL AUG RF book dvd elec. kit. avg 75.83 82.17 84.67 83.83 81.63 79.33 82.83 84.67 86.33 83.29 79.00 83.83 8"
D16-1095,W10-2608,0,0.0464465,"Missing"
D16-1095,P07-1033,0,0.676952,"Missing"
D16-1095,N09-1068,0,0.374053,"ameters to encourage transfer between close domains and avoid negative transfer between distant ones. 1 Domain Adaptation Framework O(Di ; wi ) = Li (Di ; wi ) − λi ||wi ||2 , (1) and we maximize Li by tuning the parameter vector wi . For example, Li can be the log-likelihood or the negative hinge loss. The term λi ||wi ||2 is the L2 regularization term where λi is a positive scalar. In our framework, we propose to maximize Introduction Domain adaptation (DA) is an important problem that has received substantial attention in natural language processing (Blitzer et al., 2006; Daum´e III, 2007; Finkel and Manning, 2009; Daum´e III et al., 2010). In this paper, we propose a novel regularization framework which allows DA practitioners to tune hyper-parameters to encourage transfer between close domains, and avoid negative transfer (Rosenstein et al., 2005) between distant ones. In our framework, model parameters in multiple domains are learned jointly and constrained to remain close to one another. In the transfer learning taxonomy (Pan and Yang, 2010), our framework falls under the parameter-transfer category for multi-task inductive learning. We show that our framework generalizes the frustratingly easy dom"
D16-1095,N10-1112,0,0.020477,": F-score on the ACE NER task. The domains are broadcast conversations (bc), broadcast news (bn), conversational telephone speech (cts), newswire (nw), usenet (un) and weblog (wl). The macro-average (avg) over the 6 domains is also shown in the table. Model LR Experimental Results In this section we apply our framework to both structured and un-structured tasks. For structured prediction, we use the named-entity recognition (NER) ACE-2005 dataset with 7 classes and 6 domains. We apply the linear chain CRF (Lafferty et al., 2001), and show results using standard and softmaxmargin CRF (SM-CRF) (Gimpel and Smith, 2010), with features consisting of word shape features, neighboring words, previous prediction and prefixes/suffixes. The second task is sentiment classification on the Amazon review data set (Blitzer et al., 2007) from 4 domains, labeled positive or negative. We apply logistic regression (LR) and SVM using unigram and bigram features. All the models used in this section are implemented on top of a common framework, which was also used to implement various structured prediction models previously (Lu, 2015; Lu and Roth, 2015; Muis and Lu, 2016). For each task we compare: TGT Trained only on the spec"
D16-1095,D15-1102,1,0.842491,"001), and show results using standard and softmaxmargin CRF (SM-CRF) (Gimpel and Smith, 2010), with features consisting of word shape features, neighboring words, previous prediction and prefixes/suffixes. The second task is sentiment classification on the Amazon review data set (Blitzer et al., 2007) from 4 domains, labeled positive or negative. We apply logistic regression (LR) and SVM using unigram and bigram features. All the models used in this section are implemented on top of a common framework, which was also used to implement various structured prediction models previously (Lu, 2015; Lu and Roth, 2015; Muis and Lu, 2016). For each task we compare: TGT Trained only on the specific domain data, ALL Trained on the data from all domains, Dom. bc bn cts nw un wl avg SVM Dom. TGT ALL AUG RF book dvd elec. kit. avg 75.83 82.17 84.67 83.83 81.63 79.33 82.83 84.67 86.33 83.29 79.00 83.83 84.83 86.17 83.46 80.67 83.83 84.83 87.33 84.17 book dvd elec. kit. avg 76.83 83.17 85.00 86.33 82.83 80.67 83.17 86.50 85.83 84.04 80.33 82.50 85.83 88.33 84.25 81.00 84.00 85.67 87.83 84.63 Table 2: Accuracies on the sentiment classification task. The domains are books (book), dvds (dvd), electronics (elec.) and"
D16-1095,P15-2121,1,0.835672,"et al., 2001), and show results using standard and softmaxmargin CRF (SM-CRF) (Gimpel and Smith, 2010), with features consisting of word shape features, neighboring words, previous prediction and prefixes/suffixes. The second task is sentiment classification on the Amazon review data set (Blitzer et al., 2007) from 4 domains, labeled positive or negative. We apply logistic regression (LR) and SVM using unigram and bigram features. All the models used in this section are implemented on top of a common framework, which was also used to implement various structured prediction models previously (Lu, 2015; Lu and Roth, 2015; Muis and Lu, 2016). For each task we compare: TGT Trained only on the specific domain data, ALL Trained on the data from all domains, Dom. bc bn cts nw un wl avg SVM Dom. TGT ALL AUG RF book dvd elec. kit. avg 75.83 82.17 84.67 83.83 81.63 79.33 82.83 84.67 86.33 83.29 79.00 83.83 84.83 86.17 83.46 80.67 83.83 84.83 87.33 84.17 book dvd elec. kit. avg 76.83 83.17 85.00 86.33 82.83 80.67 83.17 86.50 85.83 84.04 80.33 82.50 85.83 88.33 84.25 81.00 84.00 85.67 87.83 84.63 Table 2: Accuracies on the sentiment classification task. The domains are books (book), dvds (dvd), elect"
D16-1095,N16-1085,1,0.825106,"lts using standard and softmaxmargin CRF (SM-CRF) (Gimpel and Smith, 2010), with features consisting of word shape features, neighboring words, previous prediction and prefixes/suffixes. The second task is sentiment classification on the Amazon review data set (Blitzer et al., 2007) from 4 domains, labeled positive or negative. We apply logistic regression (LR) and SVM using unigram and bigram features. All the models used in this section are implemented on top of a common framework, which was also used to implement various structured prediction models previously (Lu, 2015; Lu and Roth, 2015; Muis and Lu, 2016). For each task we compare: TGT Trained only on the specific domain data, ALL Trained on the data from all domains, Dom. bc bn cts nw un wl avg SVM Dom. TGT ALL AUG RF book dvd elec. kit. avg 75.83 82.17 84.67 83.83 81.63 79.33 82.83 84.67 86.33 83.29 79.00 83.83 84.83 86.17 83.46 80.67 83.83 84.83 87.33 84.17 book dvd elec. kit. avg 76.83 83.17 85.00 86.33 82.83 80.67 83.17 86.50 85.83 84.04 80.33 82.50 85.83 88.33 84.25 81.00 84.00 85.67 87.83 84.63 Table 2: Accuracies on the sentiment classification task. The domains are books (book), dvds (dvd), electronics (elec.) and kitchen (kit.). The"
D16-1225,P02-1061,1,0.818197,"tive and qualitative analysis on how RNN helps improve truecasing. 1 Introduction Natural language texts (e.g., automatic speech transcripts or social media data) often come in nonstandard forms, and normalization would typically improve the performance of downstream natural language processing (NLP) applications. This paper investigates a particular sub-task in text normalization: case restoration or truecasing. Truecasing refers to the task of restoring case information (uppercase or lowercase) of characters in a text corpus. Case information is important for certain NLP tasks. For example, Chieu and Ng (2002) used unlabeled mixed case text to improve named entity recognition (NER) on uppercase text. The task often presents ambiguity: consider the word “apple” in the sentences “he bought an apple” and “he works at apple”. While the former refers to a fruit (hence, it should be in lowercase), the latter refers to a company name (hence, it should be capitalized). Moreover, we often need to recover the case information for words that are previously unseen by the system. In this paper, we propose the use of characterlevel recurrent neural networks for truecasing. Previous approaches for truecasing are"
D16-1225,P11-2117,0,0.0324002,"1 ) = P|V | j=1 exp(wj ht ) EN-Wiki EN-WSJ (11) EN-Reuters where wk is the k-th row vector of a weight matrix W . The probability of a sequence of characters x1:T is defined as: P (x1:T ) = T Y t=1 P (xt |x1:t−1 ) (12) Similar to the N-gram language modeling approach we described previously, we need to maximize Equation 12 in order to decode the most probable cased sequence. Instead of Viterbi decoding, we approximate this using a beam search. 4 Experiments and Results 4.1 Datasets and Tools Our approach is evaluated on English and German datasets. For English, we use a Wikipedia corpus from (Coster and Kauchak, 2011), WSJ corpus (Paul and Baker, 1992), and the Reuters corpus from the CoNLL-2003 shared task on named entity recognition (Tjong Kim Sang and De Meulder, 2003). For German, we use the ECI Multilingual Text Corpus from the same shared task. Each corpus is tokenized.1 The input test data is lowercased. Table 1 shows the statistics of each corpus split into training, development, and test sets. We use SRILM (Stolcke, 2002) for N-gram language model training (N ∈ {3, 5}) and HMM decoding. The word-based CRF models are trained using the CRF implementation in Stanford’s CoreNLP 1 DE-ECI News headlines"
D16-1225,P05-1045,0,0.176972,"language model trained on a cased corpus is used for scoring candidate sequences. For decoding, the Viterbi algorithm (Rabiner, 1989) computes the highest scoring sequence. The second approach is a discriminative classifier based on linear chain CRF (Lafferty et al., 2001). In this approach, truecasing is treated as a sequence labeling task, labelling each word with one of the following labels: all lowercase, all uppercase, initial capital, and mixed case. For our experiments, we used the truecaser in Stanford’s NLP pipeline (Manning et al., 2014). Their model includes a rich set of features (Finkel et al., 2005), such as surrounding words, character N-grams, word shape, etc. Dealing with mixed case Both approaches require a separate treatment for mixed case words. In particular, we need a gazetteer that maps each word to its mixed case form – either manually created or statistically collected from training data. The character-level approach is motivated by this: Instead of treating them as a special case, we train our model to capitalize a word character by character. 2091 Character-Level Approach it = σ(Wi ht−1 + Ui xt ) (1) ot = σ(Wo ht−1 + Uo xt ) (2) ft = σ(Wf ht−1 + Uf xt ) (3) gt = tanh(Wg ht−1"
D16-1225,P03-1020,0,0.0391292,"words. Our main contributions are: (i) we show that character-level approaches are viable compared to word-level approaches, (ii) we show that characterlevel RNN has a competitive performance compared to character-level CRF, and (iii) we provide our quantitative and qualitative analysis on how RNN helps improve truecasing. 2 Related Work Word-based truecasing The most widely used approach works at the word level. The simplest approach converts each word to its most frequently seen form in the training data. One popular approach uses HMM-based tagging with an N-gram language model, such as in (Lita et al., 2003; Nebhi et al., 2015). Others used a discriminative tagger, such as MEMM (Chelba and Acero, 2006) or CRF (Wang et al., 2006). Another approach uses statistical machine translation to translate uncased text into a cased one. Interestingly, no previous work operated at the character level. Nebhi et al. (2015) investigated truecasing in tweets, where truecased cor2090 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2090–2095, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics pora are less available. 3.2 Recurrent neur"
D16-1225,D15-1166,0,0.0957624,"Missing"
D16-1225,P14-5010,0,0.00660483,"cased sequence of words to a corresponding cased sequence. An Ngram language model trained on a cased corpus is used for scoring candidate sequences. For decoding, the Viterbi algorithm (Rabiner, 1989) computes the highest scoring sequence. The second approach is a discriminative classifier based on linear chain CRF (Lafferty et al., 2001). In this approach, truecasing is treated as a sequence labeling task, labelling each word with one of the following labels: all lowercase, all uppercase, initial capital, and mixed case. For our experiments, we used the truecaser in Stanford’s NLP pipeline (Manning et al., 2014). Their model includes a rich set of features (Finkel et al., 2005), such as surrounding words, character N-grams, word shape, etc. Dealing with mixed case Both approaches require a separate treatment for mixed case words. In particular, we need a gazetteer that maps each word to its mixed case form – either manually created or statistically collected from training data. The character-level approach is motivated by this: Instead of treating them as a special case, we train our model to capitalize a word character by character. 2091 Character-Level Approach it = σ(Wi ht−1 + Ui xt ) (1) ot = σ(W"
D16-1225,N16-1085,1,0.880781,"Missing"
D16-1225,H92-1073,0,0.543058,"WSJ (11) EN-Reuters where wk is the k-th row vector of a weight matrix W . The probability of a sequence of characters x1:T is defined as: P (x1:T ) = T Y t=1 P (xt |x1:t−1 ) (12) Similar to the N-gram language modeling approach we described previously, we need to maximize Equation 12 in order to decode the most probable cased sequence. Instead of Viterbi decoding, we approximate this using a beam search. 4 Experiments and Results 4.1 Datasets and Tools Our approach is evaluated on English and German datasets. For English, we use a Wikipedia corpus from (Coster and Kauchak, 2011), WSJ corpus (Paul and Baker, 1992), and the Reuters corpus from the CoNLL-2003 shared task on named entity recognition (Tjong Kim Sang and De Meulder, 2003). For German, we use the ECI Multilingual Text Corpus from the same shared task. Each corpus is tokenized.1 The input test data is lowercased. Table 1 shows the statistics of each corpus split into training, development, and test sets. We use SRILM (Stolcke, 2002) for N-gram language model training (N ∈ {3, 5}) and HMM decoding. The word-based CRF models are trained using the CRF implementation in Stanford’s CoreNLP 1 DE-ECI News headlines, which are all in uppercase, are d"
D16-1225,W03-0419,0,0.518852,"Missing"
D16-1225,N06-1001,0,0.706231,", (ii) we show that characterlevel RNN has a competitive performance compared to character-level CRF, and (iii) we provide our quantitative and qualitative analysis on how RNN helps improve truecasing. 2 Related Work Word-based truecasing The most widely used approach works at the word level. The simplest approach converts each word to its most frequently seen form in the training data. One popular approach uses HMM-based tagging with an N-gram language model, such as in (Lita et al., 2003; Nebhi et al., 2015). Others used a discriminative tagger, such as MEMM (Chelba and Acero, 2006) or CRF (Wang et al., 2006). Another approach uses statistical machine translation to translate uncased text into a cased one. Interestingly, no previous work operated at the character level. Nebhi et al. (2015) investigated truecasing in tweets, where truecased cor2090 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2090–2095, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics pora are less available. 3.2 Recurrent neural networks Recent years have shown a resurgence of interest in RNN, particularly variants with long short-term memory (Hoch"
D17-1276,W07-1009,0,0.857553,"rformance of many downstream tasks such as relation extraction (Mintz et al., 2009; Gupta and Andrassy, 2016), event extraction (Lu and Roth, 2012; Li et al., 2013; Nguyen et al., 2016), coreference resolution (Chang et al., 2013; Lu et al., 2016), question answering (Mollá et al., 2007), and equation parsing (Roy et al., 2016). Overlapping mention recognition is non-trivial, as existing methods that model mention recognition as a sequence prediction problem – e.g., using linear-chain conditional random fields (CRF) (Lafferty et al., 2001) – have difficulties in handling overlapping mentions (Alex et al., 2007). Finkel and Manning (2009) proposed to use a treebased constituency parsing model to handle nested entities.2 Due to the tree structured representation used, the resulting algorithm has a time complexity that is cubic in n for its inference procedure with n being the number of words in the sentence. This effectively makes the algorithm less scalable compared to models such as linearchain CRF where the complexity is linear in n. Lu and Roth (2015) proposed an alternative approach which shows a time complexity that is linear in n. Their method differs from the conven2 We note that nested entiti"
D17-1276,D13-1057,0,0.1729,"cross different domains (Doddington et al., 2004; Kim et al., 2003; Suominen et al., 2013). Developing algorithms that can effectively and efficiently extract overlapping mentions can be crucial for the 1 ORG under the federal wiretap statute. As noted in (Florian et al., 2004), mention recognition is more general than NER, where a mention can be either named, nominal, or pronominal. performance of many downstream tasks such as relation extraction (Mintz et al., 2009; Gupta and Andrassy, 2016), event extraction (Lu and Roth, 2012; Li et al., 2013; Nguyen et al., 2016), coreference resolution (Chang et al., 2013; Lu et al., 2016), question answering (Mollá et al., 2007), and equation parsing (Roy et al., 2016). Overlapping mention recognition is non-trivial, as existing methods that model mention recognition as a sequence prediction problem – e.g., using linear-chain conditional random fields (CRF) (Lafferty et al., 2001) – have difficulties in handling overlapping mentions (Alex et al., 2007). Finkel and Manning (2009) proposed to use a treebased constituency parsing model to handle nested entities.2 Due to the tree structured representation used, the resulting algorithm has a time complexity that i"
D17-1276,D09-1015,0,0.298688,"extensive empirical analysis on standard datasets, we demonstrate the effectiveness of our approach. 1 :::::::::::::::::::: CAT expression directed by the IL2 regulatory region :: DNA ::::::::::::::: DNA or by a multimer of the NF-AT -binding site was lower. :::: PROT ::::::::::::: DNA Figure 1: Examples of overlapping mentions. Introduction Named entity recognition (NER), or in general the task of recognizing entity mentions1 in a text, has been a research topic for many years (McCallum and Li, 2003; Nadeau and Sekine, 2007; Ratinov and Roth, 2009; Ling and Weld, 2012). However, as noted by Finkel and Manning (2009), many previous works ignored overlapping mentions, although they are quite common. Figure 1 illustrates some examples of overlapping mentions adapted from existing datasets. For example, the location mention Pennsylvania appears within the mention of type organization a Pennsylvania radio station. In practice, overlapping mentions have been found in many existing datasets across different domains (Doddington et al., 2004; Kim et al., 2003; Suominen et al., 2013). Developing algorithms that can effectively and efficiently extract overlapping mentions can be crucial for the 1 ORG under the fede"
D17-1276,N04-1001,0,0.0868869,"lapping mentions, although they are quite common. Figure 1 illustrates some examples of overlapping mentions adapted from existing datasets. For example, the location mention Pennsylvania appears within the mention of type organization a Pennsylvania radio station. In practice, overlapping mentions have been found in many existing datasets across different domains (Doddington et al., 2004; Kim et al., 2003; Suominen et al., 2013). Developing algorithms that can effectively and efficiently extract overlapping mentions can be crucial for the 1 ORG under the federal wiretap statute. As noted in (Florian et al., 2004), mention recognition is more general than NER, where a mention can be either named, nominal, or pronominal. performance of many downstream tasks such as relation extraction (Mintz et al., 2009; Gupta and Andrassy, 2016), event extraction (Lu and Roth, 2012; Li et al., 2013; Nguyen et al., 2016), coreference resolution (Chang et al., 2013; Lu et al., 2016), question answering (Mollá et al., 2007), and equation parsing (Roy et al., 2016). Overlapping mention recognition is non-trivial, as existing methods that model mention recognition as a sequence prediction problem – e.g., using linear-chain"
D17-1276,C16-1239,0,0.0108799,"f type organization a Pennsylvania radio station. In practice, overlapping mentions have been found in many existing datasets across different domains (Doddington et al., 2004; Kim et al., 2003; Suominen et al., 2013). Developing algorithms that can effectively and efficiently extract overlapping mentions can be crucial for the 1 ORG under the federal wiretap statute. As noted in (Florian et al., 2004), mention recognition is more general than NER, where a mention can be either named, nominal, or pronominal. performance of many downstream tasks such as relation extraction (Mintz et al., 2009; Gupta and Andrassy, 2016), event extraction (Lu and Roth, 2012; Li et al., 2013; Nguyen et al., 2016), coreference resolution (Chang et al., 2013; Lu et al., 2016), question answering (Mollá et al., 2007), and equation parsing (Roy et al., 2016). Overlapping mention recognition is non-trivial, as existing methods that model mention recognition as a sequence prediction problem – e.g., using linear-chain conditional random fields (CRF) (Lafferty et al., 2001) – have difficulties in handling overlapping mentions (Alex et al., 2007). Finkel and Manning (2009) proposed to use a treebased constituency parsing model to handl"
D17-1276,W04-3250,0,0.0376644,"additional comparisons on running time by running them under the same machine. In addition, we also analyzed the convergence rate for different models. 6 6.1 Results and Discussion Results on ACE Table 2 shows the results on the ACE datasets, and these are our main results. Following previous works (Finkel and Manning, 2009; Lu and Roth, 2015), we report standard precision (P ), recall (R) and F1 -score percentage scores. The highest results (F1 -score) and those results that are not significantly different from the highest results are highlighted in bold (based on bootstrap resampling test (Koehn, 2004), where p &gt; 0.01). For ACE datasets, we make comparisons with the two versions of the linear-chain CRF baseline: LCRF (single) which does not support overlapping mentions at all and LCRF (multiple) which does not support overlapping mentions of the same type, as well as our implementation of the mention hypergraph baseline (Lu and Roth, 2015). From such empirical results we can see that our proposed model using mention separators consistently yields significantly better results (p &lt; 0.01) than the mention hypergraph model across these two datasets, under two setups (whether to optimize F1 -sco"
D17-1276,N01-1025,0,0.0224914,"es issue in its learning procedure. On the other hand, it still maintains the same inference time complexity as the previous model. • Empirically, we show that our model is able to achieve higher F1 -scores compared to previous models in multiple datasets. We believe our proposed approach and the novel representations can be applied in other research problems involving predicting overlapping structures, and we hope this work can inspire further research along such a direction. 2 Related Work NER or mention detection is normally regarded as a chunking task similar to base noun phrase chunking (Kudo and Matsumoto, 2001; Shen and Sarkar, 2005), and hence the entities or mentions are usually represented in a similar way, using BILOU (Beginning, Inside, Last, Outside, Unitlength mention) or the simpler BIO annotation scheme (Ratinov and Roth, 2009). As a chunking task, it is commonly modeled using sequence labeling models, such as the linear-chain CRF (Lafferty et al., 2001), which has time complexity O(nT 2 ) with n being the number of words in the sentence and T the number of mention types. On the task of recognizing mentions that may overlap with one another, one of the earliest works that attempted to rega"
D17-1276,P13-1008,0,0.0549215,"verlapping mentions have been found in many existing datasets across different domains (Doddington et al., 2004; Kim et al., 2003; Suominen et al., 2013). Developing algorithms that can effectively and efficiently extract overlapping mentions can be crucial for the 1 ORG under the federal wiretap statute. As noted in (Florian et al., 2004), mention recognition is more general than NER, where a mention can be either named, nominal, or pronominal. performance of many downstream tasks such as relation extraction (Mintz et al., 2009; Gupta and Andrassy, 2016), event extraction (Lu and Roth, 2012; Li et al., 2013; Nguyen et al., 2016), coreference resolution (Chang et al., 2013; Lu et al., 2016), question answering (Mollá et al., 2007), and equation parsing (Roy et al., 2016). Overlapping mention recognition is non-trivial, as existing methods that model mention recognition as a sequence prediction problem – e.g., using linear-chain conditional random fields (CRF) (Lafferty et al., 2001) – have difficulties in handling overlapping mentions (Alex et al., 2007). Finkel and Manning (2009) proposed to use a treebased constituency parsing model to handle nested entities.2 Due to the tree structured represe"
D17-1276,C16-1308,0,0.0238356,"ins (Doddington et al., 2004; Kim et al., 2003; Suominen et al., 2013). Developing algorithms that can effectively and efficiently extract overlapping mentions can be crucial for the 1 ORG under the federal wiretap statute. As noted in (Florian et al., 2004), mention recognition is more general than NER, where a mention can be either named, nominal, or pronominal. performance of many downstream tasks such as relation extraction (Mintz et al., 2009; Gupta and Andrassy, 2016), event extraction (Lu and Roth, 2012; Li et al., 2013; Nguyen et al., 2016), coreference resolution (Chang et al., 2013; Lu et al., 2016), question answering (Mollá et al., 2007), and equation parsing (Roy et al., 2016). Overlapping mention recognition is non-trivial, as existing methods that model mention recognition as a sequence prediction problem – e.g., using linear-chain conditional random fields (CRF) (Lafferty et al., 2001) – have difficulties in handling overlapping mentions (Alex et al., 2007). Finkel and Manning (2009) proposed to use a treebased constituency parsing model to handle nested entities.2 Due to the tree structured representation used, the resulting algorithm has a time complexity that is cubic in n for i"
D17-1276,P12-1088,1,0.841514,"ion. In practice, overlapping mentions have been found in many existing datasets across different domains (Doddington et al., 2004; Kim et al., 2003; Suominen et al., 2013). Developing algorithms that can effectively and efficiently extract overlapping mentions can be crucial for the 1 ORG under the federal wiretap statute. As noted in (Florian et al., 2004), mention recognition is more general than NER, where a mention can be either named, nominal, or pronominal. performance of many downstream tasks such as relation extraction (Mintz et al., 2009; Gupta and Andrassy, 2016), event extraction (Lu and Roth, 2012; Li et al., 2013; Nguyen et al., 2016), coreference resolution (Chang et al., 2013; Lu et al., 2016), question answering (Mollá et al., 2007), and equation parsing (Roy et al., 2016). Overlapping mention recognition is non-trivial, as existing methods that model mention recognition as a sequence prediction problem – e.g., using linear-chain conditional random fields (CRF) (Lafferty et al., 2001) – have difficulties in handling overlapping mentions (Alex et al., 2007). Finkel and Manning (2009) proposed to use a treebased constituency parsing model to handle nested entities.2 Due to the tree s"
D17-1276,D15-1102,1,0.243084,"ion problem – e.g., using linear-chain conditional random fields (CRF) (Lafferty et al., 2001) – have difficulties in handling overlapping mentions (Alex et al., 2007). Finkel and Manning (2009) proposed to use a treebased constituency parsing model to handle nested entities.2 Due to the tree structured representation used, the resulting algorithm has a time complexity that is cubic in n for its inference procedure with n being the number of words in the sentence. This effectively makes the algorithm less scalable compared to models such as linearchain CRF where the complexity is linear in n. Lu and Roth (2015) proposed an alternative approach which shows a time complexity that is linear in n. Their method differs from the conven2 We note that nested entities are only one of the two kinds of overlapping entities, the other kind being crossing entities, where two entities overlap but neither is contained in another. However, it is extremely rare, and there is only one occurrence of crossing entity in our datasets. 2608 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2608–2618 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational"
D17-1276,W03-0430,0,0.312636,"Missing"
D17-1276,H05-1124,0,0.02623,"mentions are usually represented in a similar way, using BILOU (Beginning, Inside, Last, Outside, Unitlength mention) or the simpler BIO annotation scheme (Ratinov and Roth, 2009). As a chunking task, it is commonly modeled using sequence labeling models, such as the linear-chain CRF (Lafferty et al., 2001), which has time complexity O(nT 2 ) with n being the number of words in the sentence and T the number of mention types. On the task of recognizing mentions that may overlap with one another, one of the earliest works that attempted to regard this task as a structured prediction task was by McDonald et al. (2005). They represented entity mentions as top-k predictions with positive score from a structured multilabel classification model. Their model has a time complexity of O(n3 T ). Alex et al. (2007) proposed a cascading approach using multiple linear-chain CRF models, each handling a subset of all the possible mention types, where the models which come later in the pipeline have access to the predictions of the models earlier in the pipeline. This results in the time complexity of roughly O(nT ) depending on how the pipeline was designed. Finkel and Manning (2009) later proposed a constituency parse"
D17-1276,P09-1113,0,0.0191039,"within the mention of type organization a Pennsylvania radio station. In practice, overlapping mentions have been found in many existing datasets across different domains (Doddington et al., 2004; Kim et al., 2003; Suominen et al., 2013). Developing algorithms that can effectively and efficiently extract overlapping mentions can be crucial for the 1 ORG under the federal wiretap statute. As noted in (Florian et al., 2004), mention recognition is more general than NER, where a mention can be either named, nominal, or pronominal. performance of many downstream tasks such as relation extraction (Mintz et al., 2009; Gupta and Andrassy, 2016), event extraction (Lu and Roth, 2012; Li et al., 2013; Nguyen et al., 2016), coreference resolution (Chang et al., 2013; Lu et al., 2016), question answering (Mollá et al., 2007), and equation parsing (Roy et al., 2016). Overlapping mention recognition is non-trivial, as existing methods that model mention recognition as a sequence prediction problem – e.g., using linear-chain conditional random fields (CRF) (Lafferty et al., 2001) – have difficulties in handling overlapping mentions (Alex et al., 2007). Finkel and Manning (2009) proposed to use a treebased constitu"
D17-1276,U07-1010,0,0.0202734,"Missing"
D17-1276,D16-1008,1,0.913443,"hain CRFs, with additional semantics attached to the edges. Also note that this graph encodes only one mention type. To support multiple types, similar to the S TATE-based approach we can use multiple chains, one for each type. Note that the edges in our E DGE-based representations are directed, with nodes on the left serving as parents to the nodes on the right. Such directed edges will be helpful when performing inference, to be discussed in the next section. We remark that the way we utilize multigraph in the E DGE-based model can also be applied to the discontiguous mention model (DMM) by Muis and Lu (2016). In fact, it can be shown that the number of canonical structures as calculated in the supplementary material of DMM paper matches the number of possible paths in our multigraphbased model, as the transition matrix in DMM corresponds to the number of possible transitions from one position to the next position, which is regarded as a lattice where edges are associated with labels. X Figure 6: A linear-chain CRF model encoding a mention in BIO scheme. encoded in our multigraph-based model as edges between adjacent positions. See the supplemental material for more discussion on this. 4.1 Trainin"
D17-1276,N16-1034,0,0.00462688,"ns have been found in many existing datasets across different domains (Doddington et al., 2004; Kim et al., 2003; Suominen et al., 2013). Developing algorithms that can effectively and efficiently extract overlapping mentions can be crucial for the 1 ORG under the federal wiretap statute. As noted in (Florian et al., 2004), mention recognition is more general than NER, where a mention can be either named, nominal, or pronominal. performance of many downstream tasks such as relation extraction (Mintz et al., 2009; Gupta and Andrassy, 2016), event extraction (Lu and Roth, 2012; Li et al., 2013; Nguyen et al., 2016), coreference resolution (Chang et al., 2013; Lu et al., 2016), question answering (Mollá et al., 2007), and equation parsing (Roy et al., 2016). Overlapping mention recognition is non-trivial, as existing methods that model mention recognition as a sequence prediction problem – e.g., using linear-chain conditional random fields (CRF) (Lafferty et al., 2001) – have difficulties in handling overlapping mentions (Alex et al., 2007). Finkel and Manning (2009) proposed to use a treebased constituency parsing model to handle nested entities.2 Due to the tree structured representation used, the resu"
D17-1276,W09-1119,0,0.851537,", and discuss the possible implications of the differences. Through extensive empirical analysis on standard datasets, we demonstrate the effectiveness of our approach. 1 :::::::::::::::::::: CAT expression directed by the IL2 regulatory region :: DNA ::::::::::::::: DNA or by a multimer of the NF-AT -binding site was lower. :::: PROT ::::::::::::: DNA Figure 1: Examples of overlapping mentions. Introduction Named entity recognition (NER), or in general the task of recognizing entity mentions1 in a text, has been a research topic for many years (McCallum and Li, 2003; Nadeau and Sekine, 2007; Ratinov and Roth, 2009; Ling and Weld, 2012). However, as noted by Finkel and Manning (2009), many previous works ignored overlapping mentions, although they are quite common. Figure 1 illustrates some examples of overlapping mentions adapted from existing datasets. For example, the location mention Pennsylvania appears within the mention of type organization a Pennsylvania radio station. In practice, overlapping mentions have been found in many existing datasets across different domains (Doddington et al., 2004; Kim et al., 2003; Suominen et al., 2013). Developing algorithms that can effectively and efficiently ex"
D17-1276,D16-1117,0,0.0363039,"Missing"
D17-1276,tateisi-tsujii-2004-part,0,0.416259,"Missing"
D17-1276,W03-0419,0,0.0428813,"Missing"
D17-1312,P15-1071,0,0.364288,"is very 2898 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2898–2904 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics little work on domain adaptation for word embedding learning. One major reason preventing people from using text corpora from different domains for word embedding learning is the lack of guidance on which kind of information is worth learning from the source domain(s) for the target domain. In order to address this problem, some pioneering work has looked into this problem. For example, Bollegala et al. (2015) considered those frequent words in the source domain and the target domain as the “pivots”. Then it tried to use the pivots to predict the surrounding “non-pivots”, meanwhile ensuring the pivots to have the same embedding across two domains. Embeddings learned from such an approach were shown to be able to improve the performance on a cross-domain sentiment classification task. However, this model fails to learn embeddings for many words which are neither pivots nor non-pivots, which could be crucial for some downstream tasks such as named entity recognition. 3 Our Approach Let us first state"
D17-1312,N10-1004,0,0.0143903,"arning word representations, namely the skip-gram model (Mikolov et al., 2013a,b) was proposed and implemented in the widely used word2vec toolkit. It tries to use the current word to predict the surrounding context words, where the prediction is defined over the embeddings of these words. As a result, it learns the word embeddings by maximizing the likelihood of predictions. Domain adaptation is an important research topic (Pan et al., 2013), and it has been considered in many NLP tasks. For example, domain adaptation is studied for sentiment classification (Glorot et al., 2011) and parsing (McClosky et al., 2010), just to name a few. However, there is very 2898 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2898–2904 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics little work on domain adaptation for word embedding learning. One major reason preventing people from using text corpora from different domains for word embedding learning is the lack of guidance on which kind of information is worth learning from the source domain(s) for the target domain. In order to address this problem, some pioneering work has loo"
D17-1312,N13-1090,0,0.657401,"utd.edu.sg @adsc.com.sg Abstract learn better word embeddings, so as to improve the downstream NLP applications in a target domain like bioinformatics? Our answer is yes, because despite the domain differences, these additional domains do introduce more text data converying useful information (i.e., more words, more word co-occurrences), which can be helpful for consolidating the word embeddings in the target bioinformatics domain. In this paper, we propose a simple and easyto-implement approach for learning cross-domain word embeddings. Our model can be seen as a regularized skip-gram model (Mikolov et al., 2013a,b), where the source domain information is selectively incorporated for learning the target domain word embeddings in a principled manner. Learning word embeddings has received a significant amount of attention recently. Often, word embeddings are learned in an unsupervised manner from a large collection of text. The genre of the text typically plays an important role in the effectiveness of the resulting embeddings. How to effectively train word embedding models using data from different domains remains a problem that is underexplored. In this paper, we present a simple yet effective method"
D17-1312,D13-1171,0,0.0601698,"Missing"
D17-1312,W16-2506,0,0.0127693,"te the learned embeddings from both source and target domains as additional features. GENIA R 63.9 62.3 64.5 61.8 64.1 61.5 65.4 F1 67.3 66.4 67.9 66.1 67.6 66.1 68.7 P 64.5 63.5 63.3 64.6 63.5 62.4 64.5 ACE R 52.3 57.3 57.1 57.2 57.7 54.5 58.9 F1 57.7 60.3 60.0 60.7 60.5 58.2 61.6 Table 2: Results on entity recognition. • DARep: we use the previous approach of Bollegala et al. (2015) for learning crosslingual word representations as additional features. Experiments We present extensive evaluations to assess the effectiveness of our approach. Following recent advice by Nayak et al. (2016) and Faruqui et al. (2016), to assess the quality of the learned word embeddings, we considered employing the learned word embeddings as continuous features in several down-stream NLP tasks, including entity recognition, sentiment classification, and targeted sentiment analysis. We have used various datasets from different domains for learning cross-domain word embeddings under different tasks. We list the data statistics in Table 1. P 71.1 71.1 71.6 71.2 71.5 71.4 72.4 4.2 Entity Recognition Our first experiment was conducted on entity recognition (Tjong Kim Sang and De Meulder, 2003; Florian et al., 2004), where the"
D17-1312,N04-1001,0,0.0202021,"Missing"
D17-1312,W16-2504,0,0.0255996,"CAT: we simply concatenate the learned embeddings from both source and target domains as additional features. GENIA R 63.9 62.3 64.5 61.8 64.1 61.5 65.4 F1 67.3 66.4 67.9 66.1 67.6 66.1 68.7 P 64.5 63.5 63.3 64.6 63.5 62.4 64.5 ACE R 52.3 57.3 57.1 57.2 57.7 54.5 58.9 F1 57.7 60.3 60.0 60.7 60.5 58.2 61.6 Table 2: Results on entity recognition. • DARep: we use the previous approach of Bollegala et al. (2015) for learning crosslingual word representations as additional features. Experiments We present extensive evaluations to assess the effectiveness of our approach. Following recent advice by Nayak et al. (2016) and Faruqui et al. (2016), to assess the quality of the learned word embeddings, we considered employing the learned word embeddings as continuous features in several down-stream NLP tasks, including entity recognition, sentiment classification, and targeted sentiment analysis. We have used various datasets from different domains for learning cross-domain word embeddings under different tasks. We list the data statistics in Table 1. P 71.1 71.1 71.6 71.2 71.5 71.4 72.4 4.2 Entity Recognition Our first experiment was conducted on entity recognition (Tjong Kim Sang and De Meulder, 2003; Florian"
D17-1312,W04-3250,0,0.0160866,"Missing"
D17-1312,W02-1011,0,0.0401806,"of domain-specific terms, therefore learning good representations for such words can be crucial. As we have discussed earlier, our regularization method enables our model to differentiate domain-specific words from words which are more general in the learning process. We believe this mechanism can lead to improved learning of representations for both types of words. 4.3 Sentiment Classification The second task we consider is sentiment classification, which is essentially a text classification task, where the goal is to assign each text document a class label indicating its sentiment polarity (Pang et al., 2002; Liu, 2012). This is also the only task presented in the previous DARep work by Bollegala et al. (2015). As such, we largely followed Bollegala et al. (2015) for experiments. Instead of using the dataset they used which only consists of 2,000 reviews, we considered two much larger datasets – IMDB and Yelp 2014 – for such a task, which was previously used in a sentiment classification task (Tang et al., 2015). IMDB dataset (Diao et al., 2014) is crawled from the movie review site IMDB3 which consists of 84,919 reviews. Yelp 2014 dataset consists 2 We selected the optimal value for the hyper-pa"
D17-1312,D16-1095,1,0.821584,"Missing"
D17-1312,D14-1162,0,0.116387,"gs. How to effectively train word embedding models using data from different domains remains a problem that is underexplored. In this paper, we present a simple yet effective method for learning word embeddings based on text from different domains. We demonstrate the effectiveness of our approach through extensive experiments on various down-stream NLP tasks. 1 2 Introduction Recently, the learning of distributed representations for natural language words (or word embeddings) has received a significant amount of attention (Mnih and Hinton, 2007; Turian et al., 2010; Mikolov et al., 2013a,b,c; Pennington et al., 2014). Such representations were shown to be able to capture syntactic and semantic level information associated with words (Mikolov et al., 2013a). Word embeddings were shown effective in tasks such as named entity recognition (Sienˇcnik, 2015), sentiment analysis (Li and Lu, 2017) and syntactic parsing (Durrett and Klein, 2015). One common assumption made by most of the embedding methods is that, the text corpus is from one single domain; e.g., articles from bioinformatics. However, in practice, there are often text corpora from multiple domains; e.g., we may have text collections from broadcast"
D17-1312,W15-1830,0,0.0342824,"Missing"
D17-1312,P15-1098,0,0.0389052,"sk we consider is sentiment classification, which is essentially a text classification task, where the goal is to assign each text document a class label indicating its sentiment polarity (Pang et al., 2002; Liu, 2012). This is also the only task presented in the previous DARep work by Bollegala et al. (2015). As such, we largely followed Bollegala et al. (2015) for experiments. Instead of using the dataset they used which only consists of 2,000 reviews, we considered two much larger datasets – IMDB and Yelp 2014 – for such a task, which was previously used in a sentiment classification task (Tang et al., 2015). IMDB dataset (Diao et al., 2014) is crawled from the movie review site IMDB3 which consists of 84,919 reviews. Yelp 2014 dataset consists 2 We selected the optimal value for the hyper-parameter λ from the set λ ∈ {0.1, 1, 5, 10, 20, 30, 50} for all experiments in this paper. 3 http://www.imdb.com of 231,163 online reviews provided by the Yelp Dataset Challenge4 . Following Bollegala et al. (2015), for this task we simply learned the word embeddings from the training portion of the review datasets themselves only. No external data was used for learning word embeddings. As Bollegala et al. (20"
D17-1312,W03-0419,0,0.165627,"Missing"
D17-1312,P15-1030,0,\N,Missing
D17-1312,P10-1040,0,\N,Missing
D17-3006,P14-1134,0,0.0609975,"Missing"
D17-3006,D15-1102,1,0.930547,"locks. Based on such a framework, we show how some seemingly complicated structured prediction models such as a semantic parsing model (Lu et al., 2008; Lu, 2014) can be implemented conveniently and quickly. Furthermore, we also show that the framework can be used to solve certain structured prediction problems that otherwise cannot be easily handled by conventional structured 1 http://statnlp.org/statnlp-framework prediction models. Specifically, we show how to use such a framework to construct models that are capable of predicting non-conventional structures, such as overlapping structures (Lu and Roth, 2015; Muis and Lu, 2016a). We will also discuss how to make use of the framework to build other related models such as topic models and highlight its potential applications in some recent popular tasks (e.g., AMR parsing (Flanigan et al., 2014)). This tutorial consists of the following 3 main sections.2 Foundations of structured prediction models Duration: 45 minutes In this section, we introduce the basics of structured prediction models. We will review all the above-mentioned structured prediction models. We then provide a global picture that shows the underlying connections between different mo"
D17-3006,D08-1082,1,0.755364,"at the structured support vector machines (Tsochantaridis et al., 2004) and its latent variable variant (Yu and Joachims, 2009) as well. Furthermore, new models that integrate neural networks and graphical models, such as neural CRFs (Do et al., 2010) were also proposed. In this tutorial, we will be discussing how such a wide spectrum of existing structured prediction models can all be implemented under a unified framework1 that involves some basic building blocks. Based on such a framework, we show how some seemingly complicated structured prediction models such as a semantic parsing model (Lu et al., 2008; Lu, 2014) can be implemented conveniently and quickly. Furthermore, we also show that the framework can be used to solve certain structured prediction problems that otherwise cannot be easily handled by conventional structured 1 http://statnlp.org/statnlp-framework prediction models. Specifically, we show how to use such a framework to construct models that are capable of predicting non-conventional structures, such as overlapping structures (Lu and Roth, 2015; Muis and Lu, 2016a). We will also discuss how to make use of the framework to build other related models such as topic models and hi"
D17-3006,D14-1137,1,0.841237,"d support vector machines (Tsochantaridis et al., 2004) and its latent variable variant (Yu and Joachims, 2009) as well. Furthermore, new models that integrate neural networks and graphical models, such as neural CRFs (Do et al., 2010) were also proposed. In this tutorial, we will be discussing how such a wide spectrum of existing structured prediction models can all be implemented under a unified framework1 that involves some basic building blocks. Based on such a framework, we show how some seemingly complicated structured prediction models such as a semantic parsing model (Lu et al., 2008; Lu, 2014) can be implemented conveniently and quickly. Furthermore, we also show that the framework can be used to solve certain structured prediction problems that otherwise cannot be easily handled by conventional structured 1 http://statnlp.org/statnlp-framework prediction models. Specifically, we show how to use such a framework to construct models that are capable of predicting non-conventional structures, such as overlapping structures (Lu and Roth, 2015; Muis and Lu, 2016a). We will also discuss how to make use of the framework to build other related models such as topic models and highlight its"
D17-3006,P15-2121,1,0.85044,"ment seemingly very different types of structured prediction models using our unified framework. We will show 2 The material associated with this tutorial will be available at http://statnlp.org/tutorials/. how to conveniently implement several structured prediction models within our framework using real code examples. The framework has been extensively used by our research group for developing various structured prediction models, including models for information extraction (Lu and Roth, 2015; Muis and Lu, 2016a; Jie et al., 2017), noun phrase chunking (Muis and Lu, 2016b), semantic parsing (Lu, 2015; Susanto and Lu, 2017), and sentiment analysis (Li and Lu, 2017). It is our hope that this tutorial will be helpful for many natural language processing researchers who are interested in designing their own structured prediction models rapidly. We also hope this tutorial allows researchers to strengthen their understandings on the connections between various structured prediction models, and that the open release of the framework will bring value to the NLP research community and enhance its overall productivity. About the Instructor Wei Lu is an Assistant Professor at the Singapore Universit"
D17-3006,D16-1008,1,0.924234,"h a framework, we show how some seemingly complicated structured prediction models such as a semantic parsing model (Lu et al., 2008; Lu, 2014) can be implemented conveniently and quickly. Furthermore, we also show that the framework can be used to solve certain structured prediction problems that otherwise cannot be easily handled by conventional structured 1 http://statnlp.org/statnlp-framework prediction models. Specifically, we show how to use such a framework to construct models that are capable of predicting non-conventional structures, such as overlapping structures (Lu and Roth, 2015; Muis and Lu, 2016a). We will also discuss how to make use of the framework to build other related models such as topic models and highlight its potential applications in some recent popular tasks (e.g., AMR parsing (Flanigan et al., 2014)). This tutorial consists of the following 3 main sections.2 Foundations of structured prediction models Duration: 45 minutes In this section, we introduce the basics of structured prediction models. We will review all the above-mentioned structured prediction models. We then provide a global picture that shows the underlying connections between different models. Unified frame"
D17-3006,N16-1085,1,0.925643,"h a framework, we show how some seemingly complicated structured prediction models such as a semantic parsing model (Lu et al., 2008; Lu, 2014) can be implemented conveniently and quickly. Furthermore, we also show that the framework can be used to solve certain structured prediction problems that otherwise cannot be easily handled by conventional structured 1 http://statnlp.org/statnlp-framework prediction models. Specifically, we show how to use such a framework to construct models that are capable of predicting non-conventional structures, such as overlapping structures (Lu and Roth, 2015; Muis and Lu, 2016a). We will also discuss how to make use of the framework to build other related models such as topic models and highlight its potential applications in some recent popular tasks (e.g., AMR parsing (Flanigan et al., 2014)). This tutorial consists of the following 3 main sections.2 Foundations of structured prediction models Duration: 45 minutes In this section, we introduce the basics of structured prediction models. We will review all the above-mentioned structured prediction models. We then provide a global picture that shows the underlying connections between different models. Unified frame"
D17-3006,W05-0622,0,0.060517,"tructured prediction models. The hidden Markov model (HMM) and the probabilistic context-free grammars (PCFGs) are two classic generative models used for predicting outputs with linear-chain and tree structures, respectively. As HMM’s discriminative counterpart, the linear-chain conditional random fields (CRFs) (Lafferty et al., 2001) model was later proposed. Such a model was shown to yield good performance on standard NLP tasks such as information extraction. Several extensions to such a model were then proposed afterward, including the semiMarkov CRFs (Sarawagi and Cohen, 2004), tree CRFs (Cohn and Blunsom, 2005), as well as discriminative parsing models and their latent variable variants (Petrov and Klein, 2007). On the other hand, utilizing a slightly different loss function, one could arrive at the structured support vector machines (Tsochantaridis et al., 2004) and its latent variable variant (Yu and Joachims, 2009) as well. Furthermore, new models that integrate neural networks and graphical models, such as neural CRFs (Do et al., 2010) were also proposed. In this tutorial, we will be discussing how such a wide spectrum of existing structured prediction models can all be implemented under a unifi"
D18-1019,A00-1041,0,0.0258957,"the senIntroduction One of the most crucial steps towards building a natural language understanding system is the identification of basic semantic chunks in text. Such a task is typically characterized by the named entity recognition task (Grishman, 1997; Tjong Kim Sang and De Meulder, 2003), or the more general mention recognition task, where mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). The extracted mentions can be used in various downstream tasks for performing further semantic related tasks, including question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). One popular approach to the task of mention extraction is to regard it as a sequence labeling prob1 We make our system and code available at: http:// statnlp.org/research/ie 204 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 204–214 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Alex et"
D18-1019,N18-1131,0,0.195789,"overlap. While such an assumption largely holds in practice, it comes with a cost – the chart-based parser suffers from its cubic time complexity, making it not scalable to large datasets involving long sentences. Based on the same idea, Wang et al. (2018) proposed a scalable transition-based approach to construct a constituency forest (a collection of constituency trees). Instead of relying on structured models, Xu et al. (2017) proposed a local classifier for each possible span. However, this local approach is unable to capture the interactions between spans. Similar to (Alex et al., 2007), Ju et al. (2018) dynamically stacked multiple flat layers which recognize mentions sequentially from innermost mentions to outermost mentions. Our work is inspired by the model of Lu and Roth (2015), who introduced a mention hypergraph representation for capturing overlapping mentions. Their model was shown fast and effective, and was improved by the mention separator model (Muis and Lu, 2017). However, we note that (as also highlighted in their papers) both models suffer from the structural ambiguity issue during inference, which we will discuss later. Our new representation does not have this limitation.2 R"
D18-1019,W07-1009,0,0.395568,"better than alternative representations reported in the literature in terms of representational power. Coupled with neural networks for feature learning, our model achieves the state-of-the-art performance in three benchmark datasets annotated with overlapping mentions.1 1 . . . pPEBP2 alpha A1 , alpha B1, and . . . :::::: :::::: P ROTEIN P ROTEIN ::::::::::::: :::::: P ROTEIN P ROTEIN Figure 1: Examples of overlapping mentions. lem, with the underlying primary assumption being that the mentions are non-overlapping spans in the text. However, as highlighted in several prior research efforts (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015), mentions may overlap with one another in practice. Thus, models based on such a simplified assumption may result in sub-optimal performance for a down-stream task when they are deployed in practice. For example, consider a phrase “At the Seattle zoo, . . . ” shown in Figure 1, the relation L O CATED I N between the mentions “the Seattle zoo” (of type FACILITY) and “Seattle” (of type G PE: Geo-political entities) will not be extracted unless both of these two overlapping mentions could be extracted. Similarly, there are 4 mentions of the same type"
D18-1019,N18-1079,0,0.518704,"lly stacked multiple flat layers which recognize mentions sequentially from innermost mentions to outermost mentions. Our work is inspired by the model of Lu and Roth (2015), who introduced a mention hypergraph representation for capturing overlapping mentions. Their model was shown fast and effective, and was improved by the mention separator model (Muis and Lu, 2017). However, we note that (as also highlighted in their papers) both models suffer from the structural ambiguity issue during inference, which we will discuss later. Our new representation does not have this limitation.2 Recently, Katiyar and Cardie (2018) also proposed a hypergraph-based representation based on the BILOU tagging scheme. Their model is trained greedily using neural networks by viewing the hypergraph construction procedure as a multi-label assignment process. tence. A recent approach by Lu and Roth (2015) introduced a hypergraph representation for capturing overlapping mentions, which was shown fast and effective. The work was improved by Muis and Lu (2017), who proposed a sequence labeling approach that assigns tags to gaps between words. However, both approaches suffer from the structural ambiguity issue during inference, as w"
D18-1019,D13-1057,0,0.0237545,"ed entity recognition task (Grishman, 1997; Tjong Kim Sang and De Meulder, 2003), or the more general mention recognition task, where mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). The extracted mentions can be used in various downstream tasks for performing further semantic related tasks, including question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). One popular approach to the task of mention extraction is to regard it as a sequence labeling prob1 We make our system and code available at: http:// statnlp.org/research/ie 204 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 204–214 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Alex et al. (2007) proposed several ways to combine multiple conditional random fields (CRF) (Lafferty et al., 2001) for such tasks. Their best results were obtained by cascading several CRF models in a specific order w"
D18-1019,Q16-1026,0,0.615529,"pes in different colors). cant attention. They have been proven effective, even in the absence of handcrafted features. Collobert et al. (2011) used convolutional neural networks (CNN) over word sequences, paired with a CRF output layer. Huang et al. (2015) replaced the CNN with a bidirectional long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997). Strubell et al. (2017) proposed an iterated dilated CNN to improve computational efficiency. Beyond word-level compositions, several methods incorporated character-level compositions with character embeddings, either through CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) or LSTM (Lample et al., 2016). 3 Segmental Hypergraph A segmental hypergraph is a representation that aims at representing all possible combinations of (potentially overlapping) mentions in a given sentence. It belongs to a class of directed hypergraphs (Gallo et al., 1993), where each hyperedge e consists of a single designated parent node (head of e) and an ordered list of child nodes (tail of e). Specifically, our segmental hypergraph consists of the following 5 types of nodes: • Ai encodes all such mentions that start with the i-th or a later word • Ei encodes all ment"
D18-1019,doddington-etal-2004-automatic,0,0.055268,"Missing"
D18-1019,N16-1030,0,0.910044,"been proven effective, even in the absence of handcrafted features. Collobert et al. (2011) used convolutional neural networks (CNN) over word sequences, paired with a CRF output layer. Huang et al. (2015) replaced the CNN with a bidirectional long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997). Strubell et al. (2017) proposed an iterated dilated CNN to improve computational efficiency. Beyond word-level compositions, several methods incorporated character-level compositions with character embeddings, either through CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) or LSTM (Lample et al., 2016). 3 Segmental Hypergraph A segmental hypergraph is a representation that aims at representing all possible combinations of (potentially overlapping) mentions in a given sentence. It belongs to a class of directed hypergraphs (Gallo et al., 1993), where each hyperedge e consists of a single designated parent node (head of e) and an ordered list of child nodes (tail of e). Specifically, our segmental hypergraph consists of the following 5 types of nodes: • Ai encodes all such mentions that start with the i-th or a later word • Ei encodes all mentions that start exactly with the i-th word • Tki r"
D18-1019,D09-1015,0,0.443414,"ative representations reported in the literature in terms of representational power. Coupled with neural networks for feature learning, our model achieves the state-of-the-art performance in three benchmark datasets annotated with overlapping mentions.1 1 . . . pPEBP2 alpha A1 , alpha B1, and . . . :::::: :::::: P ROTEIN P ROTEIN ::::::::::::: :::::: P ROTEIN P ROTEIN Figure 1: Examples of overlapping mentions. lem, with the underlying primary assumption being that the mentions are non-overlapping spans in the text. However, as highlighted in several prior research efforts (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015), mentions may overlap with one another in practice. Thus, models based on such a simplified assumption may result in sub-optimal performance for a down-stream task when they are deployed in practice. For example, consider a phrase “At the Seattle zoo, . . . ” shown in Figure 1, the relation L O CATED I N between the mentions “the Seattle zoo” (of type FACILITY) and “Seattle” (of type G PE: Geo-political entities) will not be extracted unless both of these two overlapping mentions could be extracted. Similarly, there are 4 mentions of the same type (P ROTEIN) in the text sp"
D18-1019,P13-1008,0,0.0265375,"on of basic semantic chunks in text. Such a task is typically characterized by the named entity recognition task (Grishman, 1997; Tjong Kim Sang and De Meulder, 2003), or the more general mention recognition task, where mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). The extracted mentions can be used in various downstream tasks for performing further semantic related tasks, including question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). One popular approach to the task of mention extraction is to regard it as a sequence labeling prob1 We make our system and code available at: http:// statnlp.org/research/ie 204 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 204–214 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Alex et al. (2007) proposed several ways to combine multiple conditional random fields (CRF) (Lafferty et al., 2001) for such task"
D18-1019,N04-1001,0,0.0933946,"ng based approach to nested mention extraction. Due to the chart-based parsing algorithm involved, the model has a cubic time complexity in the number of words in the senIntroduction One of the most crucial steps towards building a natural language understanding system is the identification of basic semantic chunks in text. Such a task is typically characterized by the named entity recognition task (Grishman, 1997; Tjong Kim Sang and De Meulder, 2003), or the more general mention recognition task, where mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). The extracted mentions can be used in various downstream tasks for performing further semantic related tasks, including question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). One popular approach to the task of mention extraction is to regard it as a sequence labeling prob1 We make our system and code available at: http:// statnlp.org/research/ie 204 Proceedings of the 2018 Conference on Empirical"
D18-1019,D17-1005,0,0.0221118,"ding a natural language understanding system is the identification of basic semantic chunks in text. Such a task is typically characterized by the named entity recognition task (Grishman, 1997; Tjong Kim Sang and De Meulder, 2003), or the more general mention recognition task, where mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). The extracted mentions can be used in various downstream tasks for performing further semantic related tasks, including question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). One popular approach to the task of mention extraction is to regard it as a sequence labeling prob1 We make our system and code available at: http:// statnlp.org/research/ie 204 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 204–214 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Alex et al. (2007) proposed several ways to combine multiple condit"
D18-1019,D15-1102,1,0.238588,"rted in the literature in terms of representational power. Coupled with neural networks for feature learning, our model achieves the state-of-the-art performance in three benchmark datasets annotated with overlapping mentions.1 1 . . . pPEBP2 alpha A1 , alpha B1, and . . . :::::: :::::: P ROTEIN P ROTEIN ::::::::::::: :::::: P ROTEIN P ROTEIN Figure 1: Examples of overlapping mentions. lem, with the underlying primary assumption being that the mentions are non-overlapping spans in the text. However, as highlighted in several prior research efforts (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015), mentions may overlap with one another in practice. Thus, models based on such a simplified assumption may result in sub-optimal performance for a down-stream task when they are deployed in practice. For example, consider a phrase “At the Seattle zoo, . . . ” shown in Figure 1, the relation L O CATED I N between the mentions “the Seattle zoo” (of type FACILITY) and “Seattle” (of type G PE: Geo-political entities) will not be extracted unless both of these two overlapping mentions could be extracted. Similarly, there are 4 mentions of the same type (P ROTEIN) in the text span “. . . PEBP2 alph"
D18-1019,N10-1112,0,0.0842863,"Missing"
D18-1019,P16-1101,0,0.074113,". cant attention. They have been proven effective, even in the absence of handcrafted features. Collobert et al. (2011) used convolutional neural networks (CNN) over word sequences, paired with a CRF output layer. Huang et al. (2015) replaced the CNN with a bidirectional long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997). Strubell et al. (2017) proposed an iterated dilated CNN to improve computational efficiency. Beyond word-level compositions, several methods incorporated character-level compositions with character embeddings, either through CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) or LSTM (Lample et al., 2016). 3 Segmental Hypergraph A segmental hypergraph is a representation that aims at representing all possible combinations of (potentially overlapping) mentions in a given sentence. It belongs to a class of directed hypergraphs (Gallo et al., 1993), where each hyperedge e consists of a single designated parent node (head of e) and an ordered list of child nodes (tail of e). Specifically, our segmental hypergraph consists of the following 5 types of nodes: • Ai encodes all such mentions that start with the i-th or a later word • Ei encodes all mentions that start exac"
D18-1019,H05-1124,0,0.0598683,"en evaluated on data that does not have overlapping mentions annotated when comparing against other recently proposed state-ofthe-art neural models that are capable of extracting non-overlapping mentions only. 2 Related Work Overlapping Mention Recognition One of the earliest research efforts on handling overlapping mentions is a rule-based approach (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006) that is evaluated on the GENIA dataset (Kim et al., 2003). The authors first detected the innermost mentions and then relied on rule-based postprocessing methods to identify overlapping mentions. McDonald et al. (2005) presented a multilabel classification algorithm to model overlapping segments in a sentence systematically. Neural Models for Mention Recognition Recently, neural network based approaches to entity or mention recognition have received signifi2 205 A model comparison can be found later in Table 1. i i+1 i+2 i+3 Ai Ai+1 Ai+2 Ai+3 Ei Ei+1 Ei+2 Ei+3 .. T1i .. . Ai+4 .. . signed to indicate how the semantics of a parent node can be re-expressed in terms of its child nodes. Figure 2 gives a partial segmental hypergraph representing all combinations of mentions within the span [i, i + 3] consisting"
D18-1019,P09-1113,0,0.157201,"l steps towards building a natural language understanding system is the identification of basic semantic chunks in text. Such a task is typically characterized by the named entity recognition task (Grishman, 1997; Tjong Kim Sang and De Meulder, 2003), or the more general mention recognition task, where mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). The extracted mentions can be used in various downstream tasks for performing further semantic related tasks, including question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). One popular approach to the task of mention extraction is to regard it as a sequence labeling prob1 We make our system and code available at: http:// statnlp.org/research/ie 204 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 204–214 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Alex et al. (2007) proposed several ways to comb"
D18-1019,E12-1017,0,0.0158954,"mar G. 4.1 Softmax-Margin Training Recall that there are 4 types of hyperedges in our hypergraph, over which we can define the score functions. Since every valid mention hyperpath contains the first and second kind of hyperedges, defining scores over such hyperedges are unnecessary as their scores would serve as a constant factor that can be eliminated in the overall loss function of the log-linear model. Thus we only need to define the score functions on the latter two types of hyperedges. For hyperedges that only involve two nodes, we use a linear layer to compute their scores: Inspired by (Mohit et al., 2012), we consider the softmax-margin (Gimpel and Smith, 2010) in our model. The function ψ(e, x) is defined as follows: ψ(e, x) = φ(e, x) + ∆(e, Gy∗ ) (4) where φ(e, x) is a feature function, and ∆(e, Gy∗ ) is the cost function that defines the margin: ( ∆(e, Gy∗ ) = β TX[e] ∧ e ∈ / Gy ∗ 1 TI[e] ∧ e ∈ / Gy∗ 0 otherwise (5) Here, y ∗ is the gold mention combination, and TX[e] and TI[e] are indicator functions that return true if e is between T and X and between T and I respectively, and false otherwise. We set β ≥ 1 such that the cost function will assign more penalty to false negatives than to fal"
D18-1019,D16-1008,1,0.872607,"ure overlapping mentions. We show that our model has some theoretical advantages over previous state-of-theart approaches for recognizing overlapping mentions. Through extensive experiments, we show that our model is general and robust in handling both overlapping and non-overlapping mentions. The model achieves the state-of-the-art results in three standard datasets for recognizing overlapping mentions. We anticipate this model could be leveraged in other similar sequence modeling tasks that involve predicting overlapping structures such as recognizing overlapping and discontinuous entities (Muis and Lu, 2016) which frequently exist in the biomedical domain. What if the data has no overlapping mentions? To assess the robustness of our model and understand whether it could serve as a general mention extraction model, we additionally evaluate our model on CoNLL 2003 dataset which is annotated with non-overlapping mentions only. We compared our model with recent state-of-the-art neural network based models. For a fair comparison, we used the Collobert et al. (2011) embeddings widely used by previous models, and ignored POS tag features even though they are available. Results are in Table 6. Only neura"
D18-1019,D17-1276,1,0.437404,"stead of relying on structured models, Xu et al. (2017) proposed a local classifier for each possible span. However, this local approach is unable to capture the interactions between spans. Similar to (Alex et al., 2007), Ju et al. (2018) dynamically stacked multiple flat layers which recognize mentions sequentially from innermost mentions to outermost mentions. Our work is inspired by the model of Lu and Roth (2015), who introduced a mention hypergraph representation for capturing overlapping mentions. Their model was shown fast and effective, and was improved by the mention separator model (Muis and Lu, 2017). However, we note that (as also highlighted in their papers) both models suffer from the structural ambiguity issue during inference, which we will discuss later. Our new representation does not have this limitation.2 Recently, Katiyar and Cardie (2018) also proposed a hypergraph-based representation based on the BILOU tagging scheme. Their model is trained greedily using neural networks by viewing the hypergraph construction procedure as a multi-label assignment process. tence. A recent approach by Lu and Roth (2015) introduced a hypergraph representation for capturing overlapping mentions,"
D18-1019,P02-1014,0,0.0996879,"racterized by the named entity recognition task (Grishman, 1997; Tjong Kim Sang and De Meulder, 2003), or the more general mention recognition task, where mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). The extracted mentions can be used in various downstream tasks for performing further semantic related tasks, including question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). One popular approach to the task of mention extraction is to regard it as a sequence labeling prob1 We make our system and code available at: http:// statnlp.org/research/ie 204 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 204–214 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Alex et al. (2007) proposed several ways to combine multiple conditional random fields (CRF) (Lafferty et al., 2001) for such tasks. Their best results were obtained by cascading several CRF models"
D18-1019,D14-1162,0,0.101084,"sequence of actions. • SH (-NN): a non-neural version of our segmental hypergraph model that excludes the LSTMs but employs handcrafted features. 8 As discussed earlier, we also evaluate the variSH -D -SM -P ACE-2004 (c=6) (c=n) 72.7 74.5 71.5 73.1 72.0 73.3 71.5 72.7 ACE-2005 (c=6) (c=n) 72.5 74.2 71.3 72.9 71.8 73.5 71.2 73.0 GENIA (c=6) (c=n) 73.7 74.5 72.1 72.8 72.4 73.3 72.0 73.2 Table 4: Results of various ablations. D: dropout, SM: softmax-margin, P: pre-trained embeddings. ants of our model that takes character-level representations (+char). 6.3 Training Pre-trained embeddings GloVe (Pennington et al., 2014) of dimension 100 are used to initialize the trainable word vectors for experiments in ACE and GENIA datasets.9 The embeddings for POS tags are initialized randomly with dimension 32. Early stopping is used based on the performance of development set. The value β used in softmaxmargin is chosen from [1, 3] with step size 0.5. 6.4 Experimental Results Main results can be found in Table 3. Using the same set of handcrafted features, our unrestricted non-neural model SH (-NN, c=n) achieves the best performance compared with other nonneural models, revealing the effectiveness of our newly proposed"
D18-1019,W09-1119,0,0.501404,"Missing"
D18-1019,D11-1001,0,0.0306055,"system is the identification of basic semantic chunks in text. Such a task is typically characterized by the named entity recognition task (Grishman, 1997; Tjong Kim Sang and De Meulder, 2003), or the more general mention recognition task, where mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). The extracted mentions can be used in various downstream tasks for performing further semantic related tasks, including question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). One popular approach to the task of mention extraction is to regard it as a sequence labeling prob1 We make our system and code available at: http:// statnlp.org/research/ie 204 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 204–214 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Alex et al. (2007) proposed several ways to combine multiple conditional random fields (CRF) (Lafferty et al., 2"
D18-1019,J01-4004,0,0.369492,"sk is typically characterized by the named entity recognition task (Grishman, 1997; Tjong Kim Sang and De Meulder, 2003), or the more general mention recognition task, where mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). The extracted mentions can be used in various downstream tasks for performing further semantic related tasks, including question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). One popular approach to the task of mention extraction is to regard it as a sequence labeling prob1 We make our system and code available at: http:// statnlp.org/research/ie 204 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 204–214 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Alex et al. (2007) proposed several ways to combine multiple conditional random fields (CRF) (Lafferty et al., 2001) for such tasks. Their best results were obtained by cascadin"
D18-1019,D17-1283,0,0.0597786,"erpath that encodes this specific combination of mentions that overlap with one another. . .. . Tki .. . X Iki,i Iki,i+1 Iki,i+2 Iki,i+3 X X X X Tm i Figure 2: An example of partial segmental hypergraph (hyperedges of different types in different colors). cant attention. They have been proven effective, even in the absence of handcrafted features. Collobert et al. (2011) used convolutional neural networks (CNN) over word sequences, paired with a CRF output layer. Huang et al. (2015) replaced the CNN with a bidirectional long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997). Strubell et al. (2017) proposed an iterated dilated CNN to improve computational efficiency. Beyond word-level compositions, several methods incorporated character-level compositions with character embeddings, either through CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) or LSTM (Lample et al., 2016). 3 Segmental Hypergraph A segmental hypergraph is a representation that aims at representing all possible combinations of (potentially overlapping) mentions in a given sentence. It belongs to a class of directed hypergraphs (Gallo et al., 1993), where each hyperedge e consists of a single designated parent node (head"
D18-1019,tateisi-tsujii-2004-part,0,0.0897171,"de corresponds to a sum/max computation. Since one node is incident to 3 hyperedges maximally, the time complexity of inference algorithm can be implied by the number of nodes in the graph, which is O(cmn), where c is the maximal length for any mention. This complexity is the same as that of a zero-th order semiMarkov CRF model (Sarawagi and Cohen, 2005). Please refer to the supplementary material for a detailed explanation of the inference algorithm. 5 See supplementary material for complete data statistics. Following previous works, we used version 3.02p which comes with annotated POS tags (Tateisi, 2004) . Following (Finkel and Manning, 2009), we collapse DNA, RNA and protein subtypes into DNA, RNA and protein respectively, keep cell line and cell type and remove mentions of other types. 6 209 Non-Neural Neural CRF (LINEAR) CRF (CASCADED) Semi-CRF (c=6) Semi-CRF (c=n) Finkel and Manning (2009) Lu and Roth (2015) Muis and Lu (2017) SH (-NN, c=6) SH (-NN, c=n) FOFE (Xu et al., 2017) (c=6) FOFE (Xu et al., 2017) (c=n) Katiyar and Cardie (2018) Ju et al. (2018) 7 Wang et al. (2018) SH (c=6) SH (c=6) + char SH (c=n) SH (c=n) + char ACE-2004 P R F1 71.8 40.8 52.1 78.4 46.4 58.3 76.1 41.4 53.6 66.7"
D18-1019,W03-0419,0,0.580409,"Missing"
D18-1019,D18-1124,1,0.201461,"es to the next model. However, such an approach cannot model overlapping mentions of the same type, which frequently appear in practice. Finkel and Manning (2009) approached this task from a parsing perspective by constructing a constituency tree, mapping each mention to a node in the tree. This approach assumes one mention is contained by the other when they overlap. While such an assumption largely holds in practice, it comes with a cost – the chart-based parser suffers from its cubic time complexity, making it not scalable to large datasets involving long sentences. Based on the same idea, Wang et al. (2018) proposed a scalable transition-based approach to construct a constituency forest (a collection of constituency trees). Instead of relying on structured models, Xu et al. (2017) proposed a local classifier for each possible span. However, this local approach is unable to capture the interactions between spans. Similar to (Alex et al., 2007), Ju et al. (2018) dynamically stacked multiple flat layers which recognize mentions sequentially from innermost mentions to outermost mentions. Our work is inspired by the model of Lu and Roth (2015), who introduced a mention hypergraph representation for c"
D18-1019,P17-1114,0,0.403134,"task from a parsing perspective by constructing a constituency tree, mapping each mention to a node in the tree. This approach assumes one mention is contained by the other when they overlap. While such an assumption largely holds in practice, it comes with a cost – the chart-based parser suffers from its cubic time complexity, making it not scalable to large datasets involving long sentences. Based on the same idea, Wang et al. (2018) proposed a scalable transition-based approach to construct a constituency forest (a collection of constituency trees). Instead of relying on structured models, Xu et al. (2017) proposed a local classifier for each possible span. However, this local approach is unable to capture the interactions between spans. Similar to (Alex et al., 2007), Ju et al. (2018) dynamically stacked multiple flat layers which recognize mentions sequentially from innermost mentions to outermost mentions. Our work is inspired by the model of Lu and Roth (2015), who introduced a mention hypergraph representation for capturing overlapping mentions. Their model was shown fast and effective, and was improved by the mention separator model (Muis and Lu, 2017). However, we note that (as also high"
D18-1124,A00-1041,0,0.36257,"of-the-art results on ACE datasets, showing its effectiveness in detecting nested mentions.1 1 Figure 1: An example sentence of nested mentions represented in the structure of forest. PER:Person, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// github.com/berlino/nest-trans-em18. 2 Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). Figure 1, “UN Secretary"
D18-1124,W07-1009,0,0.886538,"ch is employed to efficiently and effectively represent the states of the system in a continuous space, our system is further incorporated with a character-based component to capture letterlevel patterns. Our model achieves the stateof-the-art results on ACE datasets, showing its effectiveness in detecting nested mentions.1 1 Figure 1: An example sentence of nested mentions represented in the structure of forest. PER:Person, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For ex"
D18-1124,D13-1057,0,0.054954,", GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// github.com/berlino/nest-trans-em18. 2 Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). Figure 1, “UN Secretary General” of type Person also contains “UN” of type Organization. Traditional sequence labeling models such as conditional random fields (CRF) (Lafferty et al., 2001) do not allow hierarchical structures between"
D18-1124,Q16-1026,0,0.137935,"e 2: Deduction rules. [S, i, A] denotes stack, buffer front index and action history respectively. explicitly, which may limit their performance. In contrast, the chart-based parsing method (Finkel and Manning, 2009) can capture the dependencies between nested mentions with composition rules which allow an outer entity to be influenced by its contained entities. However, their cubic time complexity makes them not scalable to large datasets. As neural network based approaches are proven effective in entity or mention recognition (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016), recent efforts focus on incorporating neural components for recognizing nested mentions. Ju et al. (2018) dynamically stacked multiple LSTM-CRF layers (Lample et al., 2016), detecting mentions in an inside-out manner until no outer entities are extracted. Katiyar and Cardie (2018) used recurrent neural networks to extract features for a hypergraph which encodes all nested mentions based on the BILOU tagging scheme. 3 Model Specifically, given a sequence of words {x0 , x1 , . . . , xn }, the goal of our system is to output a set of mentions where nested structures are allo"
D18-1124,doddington-etal-2004-automatic,0,0.0703334,"ecognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// github.com/berlino/nest-trans-em18. 2 Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). Figure 1, “UN Secretary General” of type Person also contains “UN” of type Organization. Traditional sequence labeling models such as conditional random fields (CRF) (Lafferty et al., 2001) do not allow hierarchical structures between segments, making them incapable to handle such problems. Finkel and Manning (2009) presented a chart-"
D18-1124,N16-1030,0,0.359163,"[S|t0 , i, A] [S|X, i, A|U NARY-X] Figure 2: Deduction rules. [S, i, A] denotes stack, buffer front index and action history respectively. explicitly, which may limit their performance. In contrast, the chart-based parsing method (Finkel and Manning, 2009) can capture the dependencies between nested mentions with composition rules which allow an outer entity to be influenced by its contained entities. However, their cubic time complexity makes them not scalable to large datasets. As neural network based approaches are proven effective in entity or mention recognition (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016), recent efforts focus on incorporating neural components for recognizing nested mentions. Ju et al. (2018) dynamically stacked multiple LSTM-CRF layers (Lample et al., 2016), detecting mentions in an inside-out manner until no outer entities are extracted. Katiyar and Cardie (2018) used recurrent neural networks to extract features for a hypergraph which encodes all nested mentions based on the BILOU tagging scheme. 3 Model Specifically, given a sequence of words {x0 , x1 , . . . , xn }, the goal of our system is to output a set"
D18-1124,P13-1008,0,0.0584785,"f nested mentions represented in the structure of forest. PER:Person, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// github.com/berlino/nest-trans-em18. 2 Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). Figure 1, “UN Secretary General” of type Person also contains “UN” of type Organization. Traditional sequence labeling models such as conditional"
D18-1124,D17-1005,0,0.0668819,"in detecting nested mentions.1 1 Figure 1: An example sentence of nested mentions represented in the structure of forest. PER:Person, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// github.com/berlino/nest-trans-em18. 2 Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). Figure 1, “UN Secretary General” of type Person also contains “UN” of type Organiza"
D18-1124,P15-1033,0,0.209401,"d system learns to construct this forest through a sequence of shift-reduce actions. Figure 1 shows an example of such a forest. In contrast, the tree structure by Finkel and Manning (2009) further uses a root node to connect all tree elements. Our forest representation eliminates the root node so that the number of actions required to 1011 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1011–1017 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics construct it can be reduced significantly. Following (Dyer et al., 2015), we employ StackLSTM to represent the system’s state, which consists of the states of input, stack and action history, in a continuous space incrementally. The (partially) processed nested mentions in the stack are encoded with recursive neural networks (Socher et al., 2013) where composition functions are used to capture dependencies between nested mentions. Based on the observation that letter-level patterns such as capitalization and prefix can be beneficial in detecting mentions, we incorporate a characterlevel LSTM to capture such morphological information. Meanwhile, this character-leve"
D18-1124,D15-1102,1,0.720951,"represent the states of the system in a continuous space, our system is further incorporated with a character-based component to capture letterlevel patterns. Our model achieves the stateof-the-art results on ACE datasets, showing its effectiveness in detecting nested mentions.1 1 Figure 1: An example sentence of nested mentions represented in the structure of forest. PER:Person, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation availab"
D18-1124,D09-1015,0,0.629366,"fficiently and effectively represent the states of the system in a continuous space, our system is further incorporated with a character-based component to capture letterlevel patterns. Our model achieves the stateof-the-art results on ACE datasets, showing its effectiveness in detecting nested mentions.1 1 Figure 1: An example sentence of nested mentions represented in the structure of forest. PER:Person, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our imp"
D18-1124,P16-1101,0,0.086824,", i, A] denotes stack, buffer front index and action history respectively. explicitly, which may limit their performance. In contrast, the chart-based parsing method (Finkel and Manning, 2009) can capture the dependencies between nested mentions with composition rules which allow an outer entity to be influenced by its contained entities. However, their cubic time complexity makes them not scalable to large datasets. As neural network based approaches are proven effective in entity or mention recognition (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016), recent efforts focus on incorporating neural components for recognizing nested mentions. Ju et al. (2018) dynamically stacked multiple LSTM-CRF layers (Lample et al., 2016), detecting mentions in an inside-out manner until no outer entities are extracted. Katiyar and Cardie (2018) used recurrent neural networks to extract features for a hypergraph which encodes all nested mentions based on the BILOU tagging scheme. 3 Model Specifically, given a sequence of words {x0 , x1 , . . . , xn }, the goal of our system is to output a set of mentions where nested structures are allowed. We use the fore"
D18-1124,N04-1001,0,0.247351,"s like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// github.com/berlino/nest-trans-em18. 2 Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). Figure 1, “UN Secretary General” of type Person also contains “UN” of type Organization. Traditional sequence labeling models such as conditional random fields (CRF) (Lafferty et al., 2001) do not allow hierarchical structures between segments, making them incapable to handle such problems. Finkel and Manning (2009) presented a chart-based parsing approach where each sentence with nested mentions is mapped to a rooted constituent tree. The issue of using a chart-based parser is its cubic time complexity in the number of words in the sentence. To achieve a scalable and effective solution for"
D18-1124,H05-1124,0,0.163974,"al information. Meanwhile, this character-level component can also help deal with the out-of-vocabulary problem of neural models. We conduct experiments in three standard datasets. Our system achieves the state-of-the-art performance on ACE datasets and comparable performance in GENIA dataset. 2 Related Work Entity mention recognition with nested structures has been explored first with rule-based approaches (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006) where the authors first detected the innermost mentions and then relied on rule-based postprocessing methods to identify outer mentions. McDonald et al. (2005) proposed a structured multi-label model to represent overlapping segments in a sentence. but it came with a cubic time complexity in the number of words. Alex et al. (2007) proposed several ways to combine multiple conditional random fields (CRF) (Lafferty et al., 2001) for such tasks. Their best results were obtained by cascading several CRF models in a specific order while each model is responsible for detecting mentions of a particular type. However, such an approach cannot model nested mentions of the same type, which frequently appear. Lu and Roth (2015) and Muis and Lu (2017) proposed n"
D18-1124,C12-1059,0,0.0316539,"datasets. 5 Conclusion and Future Work In this paper, we present a transition-based model for nested mention recognition using a forest representation. Coupled with Stack-LSTM for representing the system’s state, our neural model can capture dependencies between nested mentions efficiently. Moreover, the character-based component helps capture letter-level patterns in words. The system achieves the state-of-the-art performance in ACE datasets. One potential drawback of the system is the greedy training and decoding. We believe that alternatives like beam search and training with exploration (Goldberg and Nivre, 2012) could further boost the performance. Another direction that we plan to work on is to apply this model to recognizing overlapping and entities that involve discontinuous spans (Muis and Lu, 2016) which frequently exist in the biomedical domain. Decoding Speed Note that Lu and Roth (2015) and Muis and Lu (2017) also feature linear-time complexity, but with a greater constant factor. To compare the decoding speed, we re-implemented their model with the same platform (PyTorch) and run them on the same machine (CPU: Intel i5 2.7GHz). Our model turns out to be around 3-5 times faster than theirs, s"
D18-1124,P09-1113,0,0.11891,"g its effectiveness in detecting nested mentions.1 1 Figure 1: An example sentence of nested mentions represented in the structure of forest. PER:Person, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// github.com/berlino/nest-trans-em18. 2 Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). Figure 1, “UN Secretary General” of type Person also contains “U"
D18-1124,D16-1008,1,0.852678,"em’s state, our neural model can capture dependencies between nested mentions efficiently. Moreover, the character-based component helps capture letter-level patterns in words. The system achieves the state-of-the-art performance in ACE datasets. One potential drawback of the system is the greedy training and decoding. We believe that alternatives like beam search and training with exploration (Goldberg and Nivre, 2012) could further boost the performance. Another direction that we plan to work on is to apply this model to recognizing overlapping and entities that involve discontinuous spans (Muis and Lu, 2016) which frequently exist in the biomedical domain. Decoding Speed Note that Lu and Roth (2015) and Muis and Lu (2017) also feature linear-time complexity, but with a greater constant factor. To compare the decoding speed, we re-implemented their model with the same platform (PyTorch) and run them on the same machine (CPU: Intel i5 2.7GHz). Our model turns out to be around 3-5 times faster than theirs, showing its scalability. 9 We also additionally tried using embeddings trained on PubMed for GENIA but the performance was comparable. Acknowledgements We would like to thank the anonymous reviewe"
D18-1124,D17-1276,1,0.757163,"es of the system in a continuous space, our system is further incorporated with a character-based component to capture letterlevel patterns. Our model achieves the stateof-the-art results on ACE datasets, showing its effectiveness in detecting nested mentions.1 1 Figure 1: An example sentence of nested mentions represented in the structure of forest. PER:Person, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// githu"
D18-1124,N18-1131,0,0.107386,"performance. In contrast, the chart-based parsing method (Finkel and Manning, 2009) can capture the dependencies between nested mentions with composition rules which allow an outer entity to be influenced by its contained entities. However, their cubic time complexity makes them not scalable to large datasets. As neural network based approaches are proven effective in entity or mention recognition (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016), recent efforts focus on incorporating neural components for recognizing nested mentions. Ju et al. (2018) dynamically stacked multiple LSTM-CRF layers (Lample et al., 2016), detecting mentions in an inside-out manner until no outer entities are extracted. Katiyar and Cardie (2018) used recurrent neural networks to extract features for a hypergraph which encodes all nested mentions based on the BILOU tagging scheme. 3 Model Specifically, given a sequence of words {x0 , x1 , . . . , xn }, the goal of our system is to output a set of mentions where nested structures are allowed. We use the forest structure to model the nested mentions scattered in a sentence, as shown in Figure 1. The mapping is str"
D18-1124,P02-1014,0,0.0477929,"son, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// github.com/berlino/nest-trans-em18. 2 Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). Figure 1, “UN Secretary General” of type Person also contains “UN” of type Organization. Traditional sequence labeling models such as conditional random fields (CRF) (Lafferty et al., 2001) do not allow hierarchica"
D18-1124,N18-1079,0,0.491558,"llow an outer entity to be influenced by its contained entities. However, their cubic time complexity makes them not scalable to large datasets. As neural network based approaches are proven effective in entity or mention recognition (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016), recent efforts focus on incorporating neural components for recognizing nested mentions. Ju et al. (2018) dynamically stacked multiple LSTM-CRF layers (Lample et al., 2016), detecting mentions in an inside-out manner until no outer entities are extracted. Katiyar and Cardie (2018) used recurrent neural networks to extract features for a hypergraph which encodes all nested mentions based on the BILOU tagging scheme. 3 Model Specifically, given a sequence of words {x0 , x1 , . . . , xn }, the goal of our system is to output a set of mentions where nested structures are allowed. We use the forest structure to model the nested mentions scattered in a sentence, as shown in Figure 1. The mapping is straightforward: each outermost mention forms a tree where the mention is the root and its contained mentions correspond to constituents of the tree.4 3.1 Shift-Reduce System Our"
D18-1124,D14-1162,0,0.0822004,"Missing"
D18-1124,D11-1001,0,0.0552839,"re 1: An example sentence of nested mentions represented in the structure of forest. PER:Person, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// github.com/berlino/nest-trans-em18. 2 Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). Figure 1, “UN Secretary General” of type Person also contains “UN” of type Organization. Traditional sequence labeling models su"
D18-1124,W05-1513,0,0.0571976,"a set of mentions where nested structures are allowed. We use the forest structure to model the nested mentions scattered in a sentence, as shown in Figure 1. The mapping is straightforward: each outermost mention forms a tree where the mention is the root and its contained mentions correspond to constituents of the tree.4 3.1 Shift-Reduce System Our transition-based model is based on the shiftreduce parser for constituency parsing (Watan4 Note that words that are not contained in any mention each forms a single-node tree. 1012 Stack abe and Sumita, 2015), which adopts (Zhang and Clark, 2009; Sagae and Lavie, 2005). Generally, our system employs a stack to store (partially) processed nested elements. The system’s state is defined as [S, i, A] which denotes stack, buffer front index and action history respectively. In each step. an action is applied to change the system’s state. Our system consists of three types of transition actions, which are also summarized in Figure 2: Shift Indonesian To make sure that each action sequence is valid, we need to make some hard constraints on the ac5 In this case, each word is shifted (n) and involved in a unary action (n). Then all elements are reduced to a single no"
D18-1124,D13-1170,0,0.017909,"minates the root node so that the number of actions required to 1011 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1011–1017 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics construct it can be reduced significantly. Following (Dyer et al., 2015), we employ StackLSTM to represent the system’s state, which consists of the states of input, stack and action history, in a continuous space incrementally. The (partially) processed nested mentions in the stack are encoded with recursive neural networks (Socher et al., 2013) where composition functions are used to capture dependencies between nested mentions. Based on the observation that letter-level patterns such as capitalization and prefix can be beneficial in detecting mentions, we incorporate a characterlevel LSTM to capture such morphological information. Meanwhile, this character-level component can also help deal with the out-of-vocabulary problem of neural models. We conduct experiments in three standard datasets. Our system achieves the state-of-the-art performance on ACE datasets and comparable performance in GENIA dataset. 2 Related Work Entity menti"
D18-1124,J01-4004,0,0.279516,"of forest. PER:Person, ORG:Organization, GPE:Geo-Political Entity. Introduction There has been an increasing interest in named entity recognition or more generally recognizing entity mentions2 (Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017) that the nested hierarchical structure of entity mentions should be taken into account to better facilitate downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). Practically, the mentions with nested structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). For example in 1 We make our implementation available at https:// github.com/berlino/nest-trans-em18. 2 Mentions are defined as references to entities that could be named, nominal or pronominal (Florian et al., 2004). Figure 1, “UN Secretary General” of type Person also contains “UN” of type Organization. Traditional sequence labeling models such as conditional random fields (CRF) (Lafferty et al., 2001) do"
D18-1124,P15-1113,0,0.0233439,"Finkel and Manning (2009) presented a chart-based parsing approach where each sentence with nested mentions is mapped to a rooted constituent tree. The issue of using a chart-based parser is its cubic time complexity in the number of words in the sentence. To achieve a scalable and effective solution for recognizing nested mentions, we design a transition-based system which is inspired by the recent success of employing transition-based methods for constituent parsing (Zhang and Clark, 2009) and named entity recognition (Lou et al., 2017), especially when they are paired with neural networks (Watanabe and Sumita, 2015). Generally, each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions. Then our transition-based system learns to construct this forest through a sequence of shift-reduce actions. Figure 1 shows an example of such a forest. In contrast, the tree structure by Finkel and Manning (2009) further uses a root node to connect all tree elements. Our forest representation eliminates the root node so that the number of actions required to 1011 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,"
D18-1124,W09-3825,0,0.224087,"(Lafferty et al., 2001) do not allow hierarchical structures between segments, making them incapable to handle such problems. Finkel and Manning (2009) presented a chart-based parsing approach where each sentence with nested mentions is mapped to a rooted constituent tree. The issue of using a chart-based parser is its cubic time complexity in the number of words in the sentence. To achieve a scalable and effective solution for recognizing nested mentions, we design a transition-based system which is inspired by the recent success of employing transition-based methods for constituent parsing (Zhang and Clark, 2009) and named entity recognition (Lou et al., 2017), especially when they are paired with neural networks (Watanabe and Sumita, 2015). Generally, each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions. Then our transition-based system learns to construct this forest through a sequence of shift-reduce actions. Figure 1 shows an example of such a forest. In contrast, the tree structure by Finkel and Manning (2009) further uses a root node to connect all tree elements. Our forest representation eliminates the root node so t"
D18-1198,D15-1198,0,0.45317,"tute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutskever et al., 2014) methods (Peng et al., 2017; Konstas et al., 2017); 4) Transition: the transition-based methods, whose input is the plain text sentence and the output is the corresponding graph (Zhou et al., 2016; Damonte et al., 2016; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018). Apart from these models, Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar (Steedman and Baldridge, 2011) for it. Foland and Martin (2017) decompose the parsing task into many subtasks. Then they use multiple bidirectional LSTMs for identifying different types of concepts and relations and iteratively combine these components to form the AMR graph. Because the mapping between AMR concepts and tokens in the input sentence is latent, external aligners have been developed for training purpose. The most popular aligner is JAMR (Flanigan et al., 2016), which greedily aligns input tokens to graph fragments by using a static template."
D18-1198,D17-1130,0,0.758817,"igure 1. AMR parsing, the task of transforming a sentence into its AMR graph, is a challenging task as it requires the parser to learn to predict not only concepts, which consist of predicates, lemmas, named entities, wiki-links and co-references, but also a large number of relation types based on relatively sparse training data (Peng et al., 2017). Given the challenges, many state-of-the-art AMR parsers employ various external resources and adopt a pipeline approach (Flanigan et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017; van Noord and Bos, 2017). Recently, Damonte et al. (2016); Ballesteros and Al-Onaizan (2017); Peng et al. (2018) have successfully developed AMR parsers in an end-to-end fashion using a transition-based approach. The transition-based approach (Yamada and Matsumoto, 2003; Nivre, 2003, 2004) has been 1 ARG1 Dynet (Neubig et al., 2017) is used to implement our parser. We make the supplementary material and code available at http://statnlp.org/research/sp "" Ki ngdom"" It seems that he knows under the population changes ( if it continues ) that United Kingdom will not unite. Figure 1: An example AMR graph for a sentence. popular among many NLP tasks, including syntactic parsing (Zhang and"
D18-1198,W13-2322,0,0.534085,"Missing"
D18-1198,S16-1176,0,0.303542,"he popular newswire section of LDC2014T12 dataset, our parser outperforms the previous best system by around 3 points in terms of F1 measure. 2 Related Work Since the AMR graph encodes rich information, it has been explored for many downstream applications such as language generation (Song et al., 2016), information extraction (Huang et al., 2016) and machine comprehension (Sachan and Xing, 2016). Currently, most parsers can be categorized into 4 classes: 1) Tree: such approaches incrementally convert a dependency tree into its corresponding AMR graph (Wang et al., 2015; Goodman et al., 2016; Barzdins and Gosko, 2016); 2) Graph: the graph-based models calculate scores of edges and then use a maximum spanning connected subgraph algorithm to select edges that will constitute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutskever et al., 2014) methods (Peng et al., 2017; Konstas et al., 2017); 4) Transition: the transition-based methods, whose input is the plain text sentence and the output is the corresponding graph (Zhou et al., 2016; Damonte et al., 2016; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018). Apart from these models, Peng"
D18-1198,J93-2003,0,0.0540757,"el to allow it to capture locality beyond linearity (Wang and Xue, 2017). This property of the unsupervised aligner can alleviate the problem of non-projectivity caused by alignment error. Similar to what is done in the JAMR aligner, we also design rules based on properties of AMR graphs to improve alignments of non-lexical concepts. In the preprocessing stage of the hybrid aligner, we remove all relations. As non-lexical concepts can be aligned to their child concepts in the compact graph, we then remove all non-lexical concepts. Our unsupervised method is based on IBM word alignment models (Brown et al., 1993). In the postprocessing stage, we align nonlexical concepts iteratively to the same span that its child concepts are aligned to. For example, non-lexical concepts country and name shown in Figure 1 are removed in preprocessing. During postprocessing, their alignments come from child concepts “United” and “Kingdom”, they are aligned to the 16th token “United” and the 17th token “Kingdom” respectively. Therefore, nonlexical concepts country and name can be aligned to the span 16-17, which is “United Kingdom”. 3.3 Transition System The transition system consists of a stack S containing words that"
D18-1198,P17-1112,0,0.0997737,"Missing"
D18-1198,P13-2131,0,0.373535,"Missing"
D18-1198,D14-1082,0,0.00966831,"(2018) have successfully developed AMR parsers in an end-to-end fashion using a transition-based approach. The transition-based approach (Yamada and Matsumoto, 2003; Nivre, 2003, 2004) has been 1 ARG1 Dynet (Neubig et al., 2017) is used to implement our parser. We make the supplementary material and code available at http://statnlp.org/research/sp "" Ki ngdom"" It seems that he knows under the population changes ( if it continues ) that United Kingdom will not unite. Figure 1: An example AMR graph for a sentence. popular among many NLP tasks, including syntactic parsing (Zhang and Clark, 2011; Chen and Manning, 2014), named entity recognition (Lample et al., 2016), and semantic parsing (Cheng et al., 2017). Different from the graph-based approach for structured prediction (e.g., conditional random fields (Lafferty et al., 2001)), such an approach is able to maintain a good balance between efficiency and accuracy (Nivre and McDonald, 2008), and has achieved state-of-the-art results on a number of tasks (Swayamdipta et al., 2016; Shi et al., 2017; Cheng et al., 2017). While the transition-based approach is promising for AMR parsing, existing transition-based AMR parsers still cannot attain the state-of-thea"
D18-1198,P17-1005,0,0.0213136,"ed approach. The transition-based approach (Yamada and Matsumoto, 2003; Nivre, 2003, 2004) has been 1 ARG1 Dynet (Neubig et al., 2017) is used to implement our parser. We make the supplementary material and code available at http://statnlp.org/research/sp "" Ki ngdom"" It seems that he knows under the population changes ( if it continues ) that United Kingdom will not unite. Figure 1: An example AMR graph for a sentence. popular among many NLP tasks, including syntactic parsing (Zhang and Clark, 2011; Chen and Manning, 2014), named entity recognition (Lample et al., 2016), and semantic parsing (Cheng et al., 2017). Different from the graph-based approach for structured prediction (e.g., conditional random fields (Lafferty et al., 2001)), such an approach is able to maintain a good balance between efficiency and accuracy (Nivre and McDonald, 2008), and has achieved state-of-the-art results on a number of tasks (Swayamdipta et al., 2016; Shi et al., 2017; Cheng et al., 2017). While the transition-based approach is promising for AMR parsing, existing transition-based AMR parsers still cannot attain the state-of-theart results on such a task. We observe that the key to the development of an effective trans"
D18-1198,P15-1033,0,0.14905,"): creates a non-lexical concept invoked by the item at the front of the buffer, which can be any type of concept or composed representation after MERGE transition. Then the non-lexical concept is pushed back to the buffer right after the item. This action can be applied recursively. An example is that concept have-org-role-91 is invoked by official. If token official is at the front of the buffer, GEN(have-org-role-91) will be applied. 4 Stack LSTMs A classifier is required to decide which action to take at each time step, given the current state. 4.1 Stack LSTMs for AMR parsing Stack LSTMs (Dyer et al., 2015) are LSTMs (Hochreiter and Schmidhuber, 1997) that allow stack operations. Using stack LSTMs, each state can be represented using the contents of the stack, buffer and a list with the history of actions. Let st , bt and at denote the summaries of stack, buffer and the history of actions at time step t respectively. The parser state yt is given by: 1716 yt = max{0, W[st ; bt ; at ] + d} (1) Action Stack ENperson ENperson, is ENperson, is ENperson ENperson, ENcountry ENperson, ENcountry ENperson, ENcountry ENperson, ENcountry ENperson, ENcountry, official Buffer Iftik, Ahmed, is, Pakistani, offi"
D18-1198,S16-1186,0,0.418947,"es represent the relations between concepts. An example AMR graph together with its corresponding sentence are illustrated in Figure 1. AMR parsing, the task of transforming a sentence into its AMR graph, is a challenging task as it requires the parser to learn to predict not only concepts, which consist of predicates, lemmas, named entities, wiki-links and co-references, but also a large number of relation types based on relatively sparse training data (Peng et al., 2017). Given the challenges, many state-of-the-art AMR parsers employ various external resources and adopt a pipeline approach (Flanigan et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017; van Noord and Bos, 2017). Recently, Damonte et al. (2016); Ballesteros and Al-Onaizan (2017); Peng et al. (2018) have successfully developed AMR parsers in an end-to-end fashion using a transition-based approach. The transition-based approach (Yamada and Matsumoto, 2003; Nivre, 2003, 2004) has been 1 ARG1 Dynet (Neubig et al., 2017) is used to implement our parser. We make the supplementary material and code available at http://statnlp.org/research/sp "" Ki ngdom"" It seems that he knows under the population changes ( if it continues ) that United K"
D18-1198,P14-1134,0,0.425813,"Missing"
D18-1198,P17-1043,0,0.470351,". An example AMR graph together with its corresponding sentence are illustrated in Figure 1. AMR parsing, the task of transforming a sentence into its AMR graph, is a challenging task as it requires the parser to learn to predict not only concepts, which consist of predicates, lemmas, named entities, wiki-links and co-references, but also a large number of relation types based on relatively sparse training data (Peng et al., 2017). Given the challenges, many state-of-the-art AMR parsers employ various external resources and adopt a pipeline approach (Flanigan et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017; van Noord and Bos, 2017). Recently, Damonte et al. (2016); Ballesteros and Al-Onaizan (2017); Peng et al. (2018) have successfully developed AMR parsers in an end-to-end fashion using a transition-based approach. The transition-based approach (Yamada and Matsumoto, 2003; Nivre, 2003, 2004) has been 1 ARG1 Dynet (Neubig et al., 2017) is used to implement our parser. We make the supplementary material and code available at http://statnlp.org/research/sp "" Ki ngdom"" It seems that he knows under the population changes ( if it continues ) that United Kingdom will not unite. Figure 1: An example A"
D18-1198,S16-1180,0,0.066994,"C2017T10 dataset. On the popular newswire section of LDC2014T12 dataset, our parser outperforms the previous best system by around 3 points in terms of F1 measure. 2 Related Work Since the AMR graph encodes rich information, it has been explored for many downstream applications such as language generation (Song et al., 2016), information extraction (Huang et al., 2016) and machine comprehension (Sachan and Xing, 2016). Currently, most parsers can be categorized into 4 classes: 1) Tree: such approaches incrementally convert a dependency tree into its corresponding AMR graph (Wang et al., 2015; Goodman et al., 2016; Barzdins and Gosko, 2016); 2) Graph: the graph-based models calculate scores of edges and then use a maximum spanning connected subgraph algorithm to select edges that will constitute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutskever et al., 2014) methods (Peng et al., 2017; Konstas et al., 2017); 4) Transition: the transition-based methods, whose input is the plain text sentence and the output is the corresponding graph (Zhou et al., 2016; Damonte et al., 2016; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018). Ap"
D18-1198,W17-6810,0,0.0255521,"OS tags; DEP: dependency trees; NER: named entities; SRL: semantic role labels. Other features include: Word (WordNet for concept identification), GIGA (20M unlabeled Gigaword), and Silver (100k additional training pairs created by using CAMR and JAMR). “JAMR aligner” indicates that the parser is trained by action sequence generated by the JAMR aligner. “no UNK strategy” shows results without our UNK strategy. “no compact AMR graph” shows results without constraints on the compact AMR graph. Following several previous work, we also report standard deviation for the full model. bra defined in (Groschwitz et al., 2017). Since AM algebra can be viewed as dependency trees over the sentence, they can train a dependency parser to map the sentence into this structure. Different from the structure used in CAMR (Wang et al., 2015), this structure can be directly transformed to AMR graph by using postprocessing rather than relying on another transition-based system. The complexity of their projective decoder is O(n5 ), where n is the length of the sentence. Zhou et al. (2016) is also a transition-based parser, which adopts and improves beam search strategy. In contrast, our system does not use a beam, but we antici"
D18-1198,J13-4006,0,0.0225783,")|B) (u, v|S, B) ! (v, u|S, B) (u|S, v|B) ! (S, gm (u, v)|B) (S, u|B) ! (S, n|B) (S, u|B) ! (S, g` (u, `)|B) (S, u|B) ! (S, u, n|B) ROOT r oot per son ARG0- of domai n have- or g- r ol e- 91 Table 1: Definition of actions. (u|S)/(u, v|S): item u (or u, v) is at the top of the stack. gr , gm and g` represent composition functions described in Section 4.3. B containing words to be processed. Initially, S1 is empty and B1 contains the whole input sentence and a end-of-sentence symbol at the end. Execution ends on time step t such that Bt is empty and St contains a single structure. Motivated by (Henderson et al., 2013; Ballesteros and Al-Onaizan, 2017), we design 9 types of actions summarized in Table 1. An example for parsing a sentence into its compact AMR graph shown in Figure 4 is provided in Table 2. • SHIFT: removes an item from the front of the buffer and pushes it to the stack. • REDUCE: pops the item on top of the stack. • RIGHTLABEL(r): creates a relation arc from the item on top of the stack to the item at the front of the buffer. Since these two items are not removed, they can be attached by another relation arc in the future. Therefore, reentrancy is allowed. • LEFTLABEL(r): creates a relation"
D18-1198,P16-1025,0,0.0258282,"esource performs competitively on benchmark datasets. To the best of our knowledge, the parser achieves the highest score on the standard LDC2014T12 dataset. Our parser also yields competitive scores on the LDC2015E86 dataset and the more recent LDC2017T10 dataset. On the popular newswire section of LDC2014T12 dataset, our parser outperforms the previous best system by around 3 points in terms of F1 measure. 2 Related Work Since the AMR graph encodes rich information, it has been explored for many downstream applications such as language generation (Song et al., 2016), information extraction (Huang et al., 2016) and machine comprehension (Sachan and Xing, 2016). Currently, most parsers can be categorized into 4 classes: 1) Tree: such approaches incrementally convert a dependency tree into its corresponding AMR graph (Wang et al., 2015; Goodman et al., 2016; Barzdins and Gosko, 2016); 2) Graph: the graph-based models calculate scores of edges and then use a maximum spanning connected subgraph algorithm to select edges that will constitute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutskever et al., 2014) methods (Peng et al., 2017"
D18-1198,P18-1170,0,0.292514,"select the correct sense when PRED action is applied. Comparison with other parsers: Currently, all state-of-the-art models either have a relatively high complexity or adopt a pipeline approach. Wang and Xue (2017) incorporate a module called Factor Concept Labels consisting of Bi-LSTMs and CNN based on CAMR (Wang et al., 2016). Another popular parser is JAMR (Flanigan et al., 2016). The relation identification stage of JAMR has the complexity of O(|V |2 log |V |), where |V | is the number of concepts. RIGA (Barzdins and Gosko, 2016) is an ensemble system that combines CAMR and seq2seq model. Johnson et al. (2018) view AMR graph as the structure AM alge1718 Parser Flanigan et al. (2014) Flanigan et al. (2016) Werling et al. (2015) Artzi et al. (2015) Pust et al. (2015) Zhou et al. (2016) Damonte et al. (2016) Goodman et al. (2016) Barzdins and Gosko (2016) Wang et al. (2015) Wang et al. (2016) Wang and Xue (2017) Peng et al. (2017) Konstas et al. (2017) Ballesteros and Al-Onaizan (2017) Foland and Martin (2017) Buys and Blunsom (2017) van Noord and Bos (2017) Peng et al. (2018) Vilares and G´omez-Rodr´ıguez (2018) Lyu and Titov (2018) Johnson et al. (2018) This work (full model) 0no compact AMR graph 0"
D18-1198,P17-1014,0,0.681779,"nd machine comprehension (Sachan and Xing, 2016). Currently, most parsers can be categorized into 4 classes: 1) Tree: such approaches incrementally convert a dependency tree into its corresponding AMR graph (Wang et al., 2015; Goodman et al., 2016; Barzdins and Gosko, 2016); 2) Graph: the graph-based models calculate scores of edges and then use a maximum spanning connected subgraph algorithm to select edges that will constitute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutskever et al., 2014) methods (Peng et al., 2017; Konstas et al., 2017); 4) Transition: the transition-based methods, whose input is the plain text sentence and the output is the corresponding graph (Zhou et al., 2016; Damonte et al., 2016; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018). Apart from these models, Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar (Steedman and Baldridge, 2011) for it. Foland and Martin (2017) decompose the parsing task into many subtasks. Then they use multiple b"
D18-1198,N16-1030,0,0.0131793,"an end-to-end fashion using a transition-based approach. The transition-based approach (Yamada and Matsumoto, 2003; Nivre, 2003, 2004) has been 1 ARG1 Dynet (Neubig et al., 2017) is used to implement our parser. We make the supplementary material and code available at http://statnlp.org/research/sp "" Ki ngdom"" It seems that he knows under the population changes ( if it continues ) that United Kingdom will not unite. Figure 1: An example AMR graph for a sentence. popular among many NLP tasks, including syntactic parsing (Zhang and Clark, 2011; Chen and Manning, 2014), named entity recognition (Lample et al., 2016), and semantic parsing (Cheng et al., 2017). Different from the graph-based approach for structured prediction (e.g., conditional random fields (Lafferty et al., 2001)), such an approach is able to maintain a good balance between efficiency and accuracy (Nivre and McDonald, 2008), and has achieved state-of-the-art results on a number of tasks (Swayamdipta et al., 2016; Shi et al., 2017; Cheng et al., 2017). While the transition-based approach is promising for AMR parsing, existing transition-based AMR parsers still cannot attain the state-of-theart results on such a task. We observe that the k"
D18-1198,N15-1142,0,0.0153818,"u et al., 2014) to the words the buffer contains. After getting the representation, we can use it to compute the probability of the next action: p(z|yt ) = P exp(gz>t yt + qzt ) > z 0 2A exp(gz 0 yt + qz 0 ) (2) where gz is a vector representing the embedding of the action z and qz is a bias term. The set A represents the valid action set in each time step. This set varies for different parsing states due to the constraints we made for the compact AMR graph. 4.2 Representations and UNK strategy Following (Dyer et al., 2015), we concatenate three vector representations: pretrained word vector (Ling et al., 2015), learned word vector and learned vector for the POS tag, followed by a linear map to get the representation of each input token. For relation label representations and generated concepts, we simply use the learned embedding of the parser action that was applied to construct the relation. Dyer et al. (2015) show that these representations can deal flexibly with out-of-vocabulary (OOV) words7 . We extend this UNK strategy to 7 Both OOV words in the parsing data and OOV words in the pretraining language model can be represented. AMR parsing. Apart from stochastically replacing (with p = 0.2) eac"
D18-1198,W02-0109,0,0.0972212,". From Table 3, we can see that if we follow the action sequence generated by the hybrid aligner, our parser will achieve a higher Smatch score consistently. Improvement gain gets larger as the corpus gets larger. 5.2 Parser Evaluation Then we evaluate our parser on the same datasets. As illustrated in Table 4, most models incorporate additional features such as dependency trees, named entities, non-lexical role labels and external corpora. As stack LSTMs use POS tags to get the representation of the input, our parser does not use external resources other than POS tags obtained by using NLTK (Loper and Bird, 2002). We also try to evaluate our parser by removing the POS tags. The performance drops around 1.2 F1 points on average. POS tags help the parser to select the correct sense when PRED action is applied. Comparison with other parsers: Currently, all state-of-the-art models either have a relatively high complexity or adopt a pipeline approach. Wang and Xue (2017) incorporate a module called Factor Concept Labels consisting of Bi-LSTMs and CNN based on CAMR (Wang et al., 2016). Another popular parser is JAMR (Flanigan et al., 2016). The relation identification stage of JAMR has the complexity of O(|"
D18-1198,D08-1082,1,0.83172,"Missing"
D18-1198,P18-1037,0,0.662407,", 2016) is an ensemble system that combines CAMR and seq2seq model. Johnson et al. (2018) view AMR graph as the structure AM alge1718 Parser Flanigan et al. (2014) Flanigan et al. (2016) Werling et al. (2015) Artzi et al. (2015) Pust et al. (2015) Zhou et al. (2016) Damonte et al. (2016) Goodman et al. (2016) Barzdins and Gosko (2016) Wang et al. (2015) Wang et al. (2016) Wang and Xue (2017) Peng et al. (2017) Konstas et al. (2017) Ballesteros and Al-Onaizan (2017) Foland and Martin (2017) Buys and Blunsom (2017) van Noord and Bos (2017) Peng et al. (2018) Vilares and G´omez-Rodr´ıguez (2018) Lyu and Titov (2018) Johnson et al. (2018) This work (full model) 0no compact AMR graph 0JAMR aligner 0no compact AMR graph, JAMR aligner 0no UNK strategy 0no POS Type Graph Graph Graph Others Others Transition Transition Tree Tree Tree Tree Tree Seq2seq Seq2seq Transition Others Seq2seq Seq2seq Transition Transition Others Tree Transition Transition Transition Transition Transition Transition POS p p p p × p p p p p p p DEP p p p × × p p × p p p p × × p × × p × p p p p p p p p p p p × × × p p × × × × × × × × × Features NER × p p SRL × × × × × p p × p p p p p p p p × × × p p × p × × × × × × × × × × × × × × × × ×"
D18-1198,D16-1224,0,0.0289025,"makes use of POS tags as the only external resource performs competitively on benchmark datasets. To the best of our knowledge, the parser achieves the highest score on the standard LDC2014T12 dataset. Our parser also yields competitive scores on the LDC2015E86 dataset and the more recent LDC2017T10 dataset. On the popular newswire section of LDC2014T12 dataset, our parser outperforms the previous best system by around 3 points in terms of F1 measure. 2 Related Work Since the AMR graph encodes rich information, it has been explored for many downstream applications such as language generation (Song et al., 2016), information extraction (Huang et al., 2016) and machine comprehension (Sachan and Xing, 2016). Currently, most parsers can be categorized into 4 classes: 1) Tree: such approaches incrementally convert a dependency tree into its corresponding AMR graph (Wang et al., 2015; Goodman et al., 2016; Barzdins and Gosko, 2016); 2) Graph: the graph-based models calculate scores of edges and then use a maximum spanning connected subgraph algorithm to select edges that will constitute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutsk"
D18-1198,W03-3017,0,0.412109,"Missing"
D18-1198,W04-0308,0,0.159946,"Missing"
D18-1198,K16-1019,0,0.0163955,"if it continues ) that United Kingdom will not unite. Figure 1: An example AMR graph for a sentence. popular among many NLP tasks, including syntactic parsing (Zhang and Clark, 2011; Chen and Manning, 2014), named entity recognition (Lample et al., 2016), and semantic parsing (Cheng et al., 2017). Different from the graph-based approach for structured prediction (e.g., conditional random fields (Lafferty et al., 2001)), such an approach is able to maintain a good balance between efficiency and accuracy (Nivre and McDonald, 2008), and has achieved state-of-the-art results on a number of tasks (Swayamdipta et al., 2016; Shi et al., 2017; Cheng et al., 2017). While the transition-based approach is promising for AMR parsing, existing transition-based AMR parsers still cannot attain the state-of-theart results on such a task. We observe that the key to the development of an effective transitionbased system is a properly defined search space, and argue that the search space used in existing transition systems needs to be refined. Inspired by (Wang et al., 2015), we design a new compact AMR graph representation. Transition actions designed based on such a compact graph enable our parser to generate the target st"
D18-1198,P09-1040,0,0.0536954,"compact AMR graph shown in Figure 4 is provided in Table 2. • SHIFT: removes an item from the front of the buffer and pushes it to the stack. • REDUCE: pops the item on top of the stack. • RIGHTLABEL(r): creates a relation arc from the item on top of the stack to the item at the front of the buffer. Since these two items are not removed, they can be attached by another relation arc in the future. Therefore, reentrancy is allowed. • LEFTLABEL(r): creates a relation in the reverse direction as RIGHTLABEL. • SWAP: swaps the top two items on the stack. This action allows non-projective relations (Nivre, 2009) and helps to introduce reentrancy. Repetitive SWAP actions are disallowed to avoid infinite swapping. • PRED(n): predicts the predicate and lemma concepts corresponding to the item at the front of the buffer. This action requires a look-up table M generated during the training phase. For example, meet is mapped to concepts such as meet, meet-01 and meet-02 in M . If the token meet is at the front of the buffer, then we add all its corresponding concepts based on M to the valid action sets. • MERGE: removes the item at the front of the buffer and the item on top of the stack, then a compositio"
D18-1198,N18-2023,0,0.061635,"Missing"
D18-1198,P08-1108,0,0.0196619,"/statnlp.org/research/sp "" Ki ngdom"" It seems that he knows under the population changes ( if it continues ) that United Kingdom will not unite. Figure 1: An example AMR graph for a sentence. popular among many NLP tasks, including syntactic parsing (Zhang and Clark, 2011; Chen and Manning, 2014), named entity recognition (Lample et al., 2016), and semantic parsing (Cheng et al., 2017). Different from the graph-based approach for structured prediction (e.g., conditional random fields (Lafferty et al., 2001)), such an approach is able to maintain a good balance between efficiency and accuracy (Nivre and McDonald, 2008), and has achieved state-of-the-art results on a number of tasks (Swayamdipta et al., 2016; Shi et al., 2017; Cheng et al., 2017). While the transition-based approach is promising for AMR parsing, existing transition-based AMR parsers still cannot attain the state-of-theart results on such a task. We observe that the key to the development of an effective transitionbased system is a properly defined search space, and argue that the search space used in existing transition systems needs to be refined. Inspired by (Wang et al., 2015), we design a new compact AMR graph representation. Transition"
D18-1198,K15-1004,0,0.156616,"016); 2) Graph: the graph-based models calculate scores of edges and then use a maximum spanning connected subgraph algorithm to select edges that will constitute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutskever et al., 2014) methods (Peng et al., 2017; Konstas et al., 2017); 4) Transition: the transition-based methods, whose input is the plain text sentence and the output is the corresponding graph (Zhou et al., 2016; Damonte et al., 2016; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018). Apart from these models, Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar (Steedman and Baldridge, 2011) for it. Foland and Martin (2017) decompose the parsing task into many subtasks. Then they use multiple bidirectional LSTMs for identifying different types of concepts and relations and iteratively combine these components to form the AMR graph. Because the mapping between AMR concepts and tokens in the input sentence is latent, external aligners have been developed for"
D18-1198,C96-2141,0,0.124156,"ry the ISI aligner (Pourdamghani et al., 2014). According to preliminary results (Table 3), we find that the performance of such an unsupervised model is not as good as the JAMR aligner when aligning relation and non-lexical concepts. Therefore, we propose a hybrid aligner, which combines unsupervised learning and rule-based strategy6 . JAMR does not consider information about the structure whereas the unsupervised models can capture locality (Wang and Xue, 2017) – the assumption that words that are adjacent in the source sentence tend to align to words that are closer in the target sentence (Vogel et al., 1996). Structural information can also be incorporated into the model to allow it to capture locality beyond linearity (Wang and Xue, 2017). This property of the unsupervised aligner can alleviate the problem of non-projectivity caused by alignment error. Similar to what is done in the JAMR aligner, we also design rules based on properties of AMR graphs to improve alignments of non-lexical concepts. In the preprocessing stage of the hybrid aligner, we remove all relations. As non-lexical concepts can be aligned to their child concepts in the compact graph, we then remove all non-lexical concepts. O"
D18-1198,S16-1181,0,0.104409,"t the representation of the input, our parser does not use external resources other than POS tags obtained by using NLTK (Loper and Bird, 2002). We also try to evaluate our parser by removing the POS tags. The performance drops around 1.2 F1 points on average. POS tags help the parser to select the correct sense when PRED action is applied. Comparison with other parsers: Currently, all state-of-the-art models either have a relatively high complexity or adopt a pipeline approach. Wang and Xue (2017) incorporate a module called Factor Concept Labels consisting of Bi-LSTMs and CNN based on CAMR (Wang et al., 2016). Another popular parser is JAMR (Flanigan et al., 2016). The relation identification stage of JAMR has the complexity of O(|V |2 log |V |), where |V | is the number of concepts. RIGA (Barzdins and Gosko, 2016) is an ensemble system that combines CAMR and seq2seq model. Johnson et al. (2018) view AMR graph as the structure AM alge1718 Parser Flanigan et al. (2014) Flanigan et al. (2016) Werling et al. (2015) Artzi et al. (2015) Pust et al. (2015) Zhou et al. (2016) Damonte et al. (2016) Goodman et al. (2016) Barzdins and Gosko (2016) Wang et al. (2015) Wang et al. (2016) Wang and Xue (2017) Pe"
D18-1198,D17-1129,0,0.851864,"ons between concepts. An example AMR graph together with its corresponding sentence are illustrated in Figure 1. AMR parsing, the task of transforming a sentence into its AMR graph, is a challenging task as it requires the parser to learn to predict not only concepts, which consist of predicates, lemmas, named entities, wiki-links and co-references, but also a large number of relation types based on relatively sparse training data (Peng et al., 2017). Given the challenges, many state-of-the-art AMR parsers employ various external resources and adopt a pipeline approach (Flanigan et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017; van Noord and Bos, 2017). Recently, Damonte et al. (2016); Ballesteros and Al-Onaizan (2017); Peng et al. (2018) have successfully developed AMR parsers in an end-to-end fashion using a transition-based approach. The transition-based approach (Yamada and Matsumoto, 2003; Nivre, 2003, 2004) has been 1 ARG1 Dynet (Neubig et al., 2017) is used to implement our parser. We make the supplementary material and code available at http://statnlp.org/research/sp "" Ki ngdom"" It seems that he knows under the population changes ( if it continues ) that United Kingdom will not unit"
D18-1198,N15-1040,0,0.607306,"aintain a good balance between efficiency and accuracy (Nivre and McDonald, 2008), and has achieved state-of-the-art results on a number of tasks (Swayamdipta et al., 2016; Shi et al., 2017; Cheng et al., 2017). While the transition-based approach is promising for AMR parsing, existing transition-based AMR parsers still cannot attain the state-of-theart results on such a task. We observe that the key to the development of an effective transitionbased system is a properly defined search space, and argue that the search space used in existing transition systems needs to be refined. Inspired by (Wang et al., 2015), we design a new compact AMR graph representation. Transition actions designed based on such a compact graph enable our parser to generate the target structure with fewer actions and to better capture the correspondence 1712 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1712–1722 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics between concepts and tokens in the sentence. Oracle, the algorithm used at the training time for specifying the action sequence that can recover the gold AMR graph, is als"
D18-1198,E17-1035,0,0.695149,"., 2013) is a formalism that captures the semantics of a sentence with a rooted directed graph, in which nodes represent the concepts and edges represent the relations between concepts. An example AMR graph together with its corresponding sentence are illustrated in Figure 1. AMR parsing, the task of transforming a sentence into its AMR graph, is a challenging task as it requires the parser to learn to predict not only concepts, which consist of predicates, lemmas, named entities, wiki-links and co-references, but also a large number of relation types based on relatively sparse training data (Peng et al., 2017). Given the challenges, many state-of-the-art AMR parsers employ various external resources and adopt a pipeline approach (Flanigan et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017; van Noord and Bos, 2017). Recently, Damonte et al. (2016); Ballesteros and Al-Onaizan (2017); Peng et al. (2018) have successfully developed AMR parsers in an end-to-end fashion using a transition-based approach. The transition-based approach (Yamada and Matsumoto, 2003; Nivre, 2003, 2004) has been 1 ARG1 Dynet (Neubig et al., 2017) is used to implement our parser. We make the supplementary material and co"
D18-1198,P15-1095,0,0.131689,"mation, it has been explored for many downstream applications such as language generation (Song et al., 2016), information extraction (Huang et al., 2016) and machine comprehension (Sachan and Xing, 2016). Currently, most parsers can be categorized into 4 classes: 1) Tree: such approaches incrementally convert a dependency tree into its corresponding AMR graph (Wang et al., 2015; Goodman et al., 2016; Barzdins and Gosko, 2016); 2) Graph: the graph-based models calculate scores of edges and then use a maximum spanning connected subgraph algorithm to select edges that will constitute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutskever et al., 2014) methods (Peng et al., 2017; Konstas et al., 2017); 4) Transition: the transition-based methods, whose input is the plain text sentence and the output is the corresponding graph (Zhou et al., 2016; Damonte et al., 2016; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018). Apart from these models, Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinator"
D18-1198,D14-1048,0,0.481957,"orical grammar (Steedman and Baldridge, 2011) for it. Foland and Martin (2017) decompose the parsing task into many subtasks. Then they use multiple bidirectional LSTMs for identifying different types of concepts and relations and iteratively combine these components to form the AMR graph. Because the mapping between AMR concepts and tokens in the input sentence is latent, external aligners have been developed for training purpose. The most popular aligner is JAMR (Flanigan et al., 2016), which greedily aligns input tokens to graph fragments by using a static template. Another aligner is ISI (Pourdamghani et al., 2014), which is a statistical approach that borrows techniques from statistical machine translation. 3 Approach We adopt a transition-based approach for AMR parsing. We first propose a new compact representation for AMR graph. Based on our new representation, we further present a novel technique for constructing the action sequence used for training our model. As we will see later, both newly introduced techniques are crucial for building an improved transition-based system within our refined search space. 3.1 Compact AMR Graph Inspired by (Wang et al., 2015), we design a representation called comp"
D18-1198,W03-3023,0,0.322417,"Missing"
D18-1198,D15-1136,0,0.646281,"imum spanning connected subgraph algorithm to select edges that will constitute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutskever et al., 2014) methods (Peng et al., 2017; Konstas et al., 2017); 4) Transition: the transition-based methods, whose input is the plain text sentence and the output is the corresponding graph (Zhou et al., 2016; Damonte et al., 2016; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018). Apart from these models, Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar (Steedman and Baldridge, 2011) for it. Foland and Martin (2017) decompose the parsing task into many subtasks. Then they use multiple bidirectional LSTMs for identifying different types of concepts and relations and iteratively combine these components to form the AMR graph. Because the mapping between AMR concepts and tokens in the input sentence is latent, external aligners have been developed for training purpose. The most popular aligner is JAMR (Flanigan et al., 2016), which"
D18-1198,P16-2079,0,0.0130993,"asets. To the best of our knowledge, the parser achieves the highest score on the standard LDC2014T12 dataset. Our parser also yields competitive scores on the LDC2015E86 dataset and the more recent LDC2017T10 dataset. On the popular newswire section of LDC2014T12 dataset, our parser outperforms the previous best system by around 3 points in terms of F1 measure. 2 Related Work Since the AMR graph encodes rich information, it has been explored for many downstream applications such as language generation (Song et al., 2016), information extraction (Huang et al., 2016) and machine comprehension (Sachan and Xing, 2016). Currently, most parsers can be categorized into 4 classes: 1) Tree: such approaches incrementally convert a dependency tree into its corresponding AMR graph (Wang et al., 2015; Goodman et al., 2016; Barzdins and Gosko, 2016); 2) Graph: the graph-based models calculate scores of edges and then use a maximum spanning connected subgraph algorithm to select edges that will constitute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutskever et al., 2014) methods (Peng et al., 2017; Konstas et al., 2017); 4) Transition: the transi"
D18-1198,J11-1005,0,0.0187513,"zan (2017); Peng et al. (2018) have successfully developed AMR parsers in an end-to-end fashion using a transition-based approach. The transition-based approach (Yamada and Matsumoto, 2003; Nivre, 2003, 2004) has been 1 ARG1 Dynet (Neubig et al., 2017) is used to implement our parser. We make the supplementary material and code available at http://statnlp.org/research/sp "" Ki ngdom"" It seems that he knows under the population changes ( if it continues ) that United Kingdom will not unite. Figure 1: An example AMR graph for a sentence. popular among many NLP tasks, including syntactic parsing (Zhang and Clark, 2011; Chen and Manning, 2014), named entity recognition (Lample et al., 2016), and semantic parsing (Cheng et al., 2017). Different from the graph-based approach for structured prediction (e.g., conditional random fields (Lafferty et al., 2001)), such an approach is able to maintain a good balance between efficiency and accuracy (Nivre and McDonald, 2008), and has achieved state-of-the-art results on a number of tasks (Swayamdipta et al., 2016; Shi et al., 2017; Cheng et al., 2017). While the transition-based approach is promising for AMR parsing, existing transition-based AMR parsers still cannot"
D18-1198,D16-1065,0,0.608757,"nvert a dependency tree into its corresponding AMR graph (Wang et al., 2015; Goodman et al., 2016; Barzdins and Gosko, 2016); 2) Graph: the graph-based models calculate scores of edges and then use a maximum spanning connected subgraph algorithm to select edges that will constitute the graph (Werling et al., 2015; Flanigan et al., 2016); 3) Seq2seq: the models adapted from sequence-to-sequence (Sutskever et al., 2014) methods (Peng et al., 2017; Konstas et al., 2017); 4) Transition: the transition-based methods, whose input is the plain text sentence and the output is the corresponding graph (Zhou et al., 2016; Damonte et al., 2016; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018). Apart from these models, Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar (Steedman and Baldridge, 2011) for it. Foland and Martin (2017) decompose the parsing task into many subtasks. Then they use multiple bidirectional LSTMs for identifying different types of concepts and relations and iteratively combine these components to form the AMR graph. Becau"
D18-1198,D17-1002,0,0.0204734,"ted Kingdom will not unite. Figure 1: An example AMR graph for a sentence. popular among many NLP tasks, including syntactic parsing (Zhang and Clark, 2011; Chen and Manning, 2014), named entity recognition (Lample et al., 2016), and semantic parsing (Cheng et al., 2017). Different from the graph-based approach for structured prediction (e.g., conditional random fields (Lafferty et al., 2001)), such an approach is able to maintain a good balance between efficiency and accuracy (Nivre and McDonald, 2008), and has achieved state-of-the-art results on a number of tasks (Swayamdipta et al., 2016; Shi et al., 2017; Cheng et al., 2017). While the transition-based approach is promising for AMR parsing, existing transition-based AMR parsers still cannot attain the state-of-theart results on such a task. We observe that the key to the development of an effective transitionbased system is a properly defined search space, and argue that the search space used in existing transition systems needs to be refined. Inspired by (Wang et al., 2015), we design a new compact AMR graph representation. Transition actions designed based on such a compact graph enable our parser to generate the target structure with fewer"
D18-1198,P13-1045,0,0.0349575,"ochastically replacing (with p = 0.2) each singleton in the training data, we also replace the original PRED(n) as PRED(UNK) if the token should be selected as a concept. At the postprocessing stage, if an AMR concept is generated by PRED(UNK) we will use its lemma form for nouns or the most frequent sense for verbs to replace the “UNK” placeholder. This strategy makes the classifier trained not only to predict whether an OOV word should be selected as an AMR concept but also to predict the correct concept for the in-vocabulary word. 4.3 Composition Functions We use recursive neural networks (Socher et al., 2013) to compute the representations of partially constructed structures. For relation attachments, a composition function gr is used to combine representations of AMR parent concept (h), child concept (d) and the corresponding relation label (r) as: gr (h, d, r) = tanh(Ur [h; d; r] + br ) (3) where Ur and br are parameters in the model, as Um , bm , U` and b` in equation (4) and (5). For generated non-lexical concepts, if terminal concepts are lexical concepts as shown in Figure 4 we will use a composition function gm to get their (c1 and c2 ) merged representation: 1717 gm (c1 , c2 ) = tanh(Um [c"
D18-1226,D16-1250,0,0.0140624,"Fang and Cohn (2017) use the cross-lingual word embeddings to obtain distant supervision for target languages. Yang et al. (2017a) propose to re-train word embeddings on target domain by using regularization terms based on the sourcedomain embeddings, where some hyper-parameter tuning based on down-stream tasks is required. Our word adaptation layer serves as a lineartransformation (Mikolov et al., 2013), which is learned based on corpus level statistics. Although there are alternative methods that also learn a mapping between embeddings learned from different domains (Faruqui and Dyer, 2014; Artetxe et al., 2016; Smith et al., 2017), such methods usually involve modifying source domain embeddings, and thus re-training of the source model based on the modified source embeddings would be required for the subsequent transfer process. All these existing works do not use domainspecific embeddings for different domains and they use the same neural model for source and target models. However, with our word adaptation layer, it opens the opportunity to use domainspecific embeddings. Our approach also addresses the domain shift problem at both input and output level by re-constructing target models with our s"
D18-1226,W06-1615,0,0.157717,"for the two source domains, whereas when the target data set is larger (WNUT16), the best becomes 0.7 and 0.8. The results suggest that the optimal tends to be relatively larger when the target data set is larger. 6 Related Work Domain adaptation and transfer learning has been a popular topic that has been extensively studied in the past few years (Pan and Yang, 2010). For well-studied conventional feature-based models in NLP, there are various classic transfer approaches, such as EasyAdapt (Daum´e, 2007), instance weighting (Jiang and Zhai, 2007) and 2019 structural correspondence learning (Blitzer et al., 2006). Fewer works have been focused on transfer approaches for neural models in NLP. Mou et al. (2016) use intuitive transfer methods (INIT and MULT) to study the transferability of neural network models for the sentence (pair) classification problem; Lee et al. (2017) utilize the INIT method on highly related datasets of electronic health records to study their specific deidentification problem. Yang et al. (2017b) use the MULT approach in sequence tagging tasks including named entity recognition. Following the MULT scheme, Wang et al. (2018) introduce a label-aware mechanism into maximum mean di"
D18-1226,P15-1071,0,0.0202487,"word adaptation layer, which takes in the target domain word embeddings and returns the transformed embeddings in the new space. We learn Z with stochastic gradient descent. After learning, the projected new embeddings would be VT Z, which would be used in the subsequent steps as illustrated in Figure 2 and Figure 3. With such a word-level input-space transformation, the parameters of the pre-trained source models based on VS can still be relevant, which can be used in subsequent steps. We would like to highlight that, unlike many previous approaches to learning cross-domain word embeddings (Bollegala et al., 2015; Yang et al., 2017a), the learning of our word adaptation layer involves no modifications to the sourcedomain embedding spaces. It also requires no retraining of the embeddings based on the targetdomain data. Such a distinctive advantage of our approach comes with some important practical implications: it essentially enables the transfer learning process to work directly on top of a welltrained model by performing adaptation without involving significant re-training efforts. For example, the existing model could be one that has already gone through extensive training, tuning and testing for m"
D18-1226,P07-1033,0,0.513953,"Missing"
D18-1226,P17-2093,0,0.186929,"fication problem. Yang et al. (2017b) use the MULT approach in sequence tagging tasks including named entity recognition. Following the MULT scheme, Wang et al. (2018) introduce a label-aware mechanism into maximum mean discrepancy (MMD) to explicitly reduce domain shift between the same labels across domains in medical data. Their approach requires the output space to be the same in both source and target domains. Note that the scenario in our paper is that the output spaces are different in two domains. We propose to learn the word adaptation layer in our task inspired by two prior studies. Fang and Cohn (2017) use the cross-lingual word embeddings to obtain distant supervision for target languages. Yang et al. (2017a) propose to re-train word embeddings on target domain by using regularization terms based on the sourcedomain embeddings, where some hyper-parameter tuning based on down-stream tasks is required. Our word adaptation layer serves as a lineartransformation (Mikolov et al., 2013), which is learned based on corpus level statistics. Although there are alternative methods that also learn a mapping between embeddings learned from different domains (Faruqui and Dyer, 2014; Artetxe et al., 2016"
D18-1226,E14-1049,0,0.0158271,"d by two prior studies. Fang and Cohn (2017) use the cross-lingual word embeddings to obtain distant supervision for target languages. Yang et al. (2017a) propose to re-train word embeddings on target domain by using regularization terms based on the sourcedomain embeddings, where some hyper-parameter tuning based on down-stream tasks is required. Our word adaptation layer serves as a lineartransformation (Mikolov et al., 2013), which is learned based on corpus level statistics. Although there are alternative methods that also learn a mapping between embeddings learned from different domains (Faruqui and Dyer, 2014; Artetxe et al., 2016; Smith et al., 2017), such methods usually involve modifying source domain embeddings, and thus re-training of the source model based on the modified source embeddings would be required for the subsequent transfer process. All these existing works do not use domainspecific embeddings for different domains and they use the same neural model for source and target models. However, with our word adaptation layer, it opens the opportunity to use domainspecific embeddings. Our approach also addresses the domain shift problem at both input and output level by re-constructing ta"
D18-1226,W10-0713,0,0.0431028,"well (around 90.0 F1-score (Ma and Hovy, 2016)). However, the performance drop dramatically in social media data (around 60.0 F-score (Strauss et al., 2016)). • Important: Social media is a rich soil for text mining (Petrovic et al., 2010; Rosenthal and McKeown, 2015; Wang and Yang, 2015), and NER is of significant importance for other information extraction tasks in social media (Ritter et al., 2011a; Peng and Dredze, 2016; Chou et al., 2016). • Representative: The noisy nature of user generated content as well as emerging entities with novel surface forms make the domain shift very salient (Finin et al., 2010; Han et al., 2016). Nevertheless, the techniques developed in this paper are domain independent and thus can be used for other learning tasks across any two domains so long as we have the necessary resources. 4.2 Resources for Cross-domain Embeddings We utilizes GloVe (Pennington et al., 2014) to train domain-specific and domain-general word embeddings from different corpora, denoted as follows: 1) source emb, which is trained on the newswire domain corpus (NewYorkTimes and DailyMail articles); 2) target emb, which is trained on a social media corpus (Archive Team’s Twitter stream grab3 ); 3)"
D18-1226,P11-1038,0,0.0122178,"t lexicon P is a set of such word pairs. To construct a pivot lexicon, first, motivated by Tan et al. (2015), we define P1 , which consists of the ordinary words that have high relative frequency in both source and target domain corpora: P1 = {(ws , wt )|ws = wt , f (ws ) s , f (wt ) t }, where f (w) is the frequency function that returns the number of occorrence of the word w in the dataset, and s and t are word frequency thresholds2 . Optionally, we can utilize a customized word-pair list P2 , which gives mappings between domain-specific words across domains, such as normalization lexicons (Han and Baldwin, 2011; Liu et al., 2012). The final lexicon is thus defined as P = P1 [ P2 . 3.1.2 Projection Learning Mathematically, given the pre-trained domainspecific word embeddings VS and VT as well as a pivot lexicon P, we would like to learn a linear projection transforming word vectors from VT into VS . This idea is based on a bilingual word embedding model (Mikolov et al., 2013), but we adapt it to this domain adaptation task. ⇤, We first construct two matrices VS⇤ and VT where the i-th rows of these two matrices are the vector representations for the words in the i-th entry of P: P (i) = (wsi , wti )."
D18-1226,W16-3919,0,0.0684036,"-score (Ma and Hovy, 2016)). However, the performance drop dramatically in social media data (around 60.0 F-score (Strauss et al., 2016)). • Important: Social media is a rich soil for text mining (Petrovic et al., 2010; Rosenthal and McKeown, 2015; Wang and Yang, 2015), and NER is of significant importance for other information extraction tasks in social media (Ritter et al., 2011a; Peng and Dredze, 2016; Chou et al., 2016). • Representative: The noisy nature of user generated content as well as emerging entities with novel surface forms make the domain shift very salient (Finin et al., 2010; Han et al., 2016). Nevertheless, the techniques developed in this paper are domain independent and thus can be used for other learning tasks across any two domains so long as we have the necessary resources. 4.2 Resources for Cross-domain Embeddings We utilizes GloVe (Pennington et al., 2014) to train domain-specific and domain-general word embeddings from different corpora, denoted as follows: 1) source emb, which is trained on the newswire domain corpus (NewYorkTimes and DailyMail articles); 2) target emb, which is trained on a social media corpus (Archive Team’s Twitter stream grab3 ); 3) general emb, which"
D18-1226,P07-1034,0,0.0753067,"data set is small (Ritter11), the best are 0.5 and 0.6 respectively for the two source domains, whereas when the target data set is larger (WNUT16), the best becomes 0.7 and 0.8. The results suggest that the optimal tends to be relatively larger when the target data set is larger. 6 Related Work Domain adaptation and transfer learning has been a popular topic that has been extensively studied in the past few years (Pan and Yang, 2010). For well-studied conventional feature-based models in NLP, there are various classic transfer approaches, such as EasyAdapt (Daum´e, 2007), instance weighting (Jiang and Zhai, 2007) and 2019 structural correspondence learning (Blitzer et al., 2006). Fewer works have been focused on transfer approaches for neural models in NLP. Mou et al. (2016) use intuitive transfer methods (INIT and MULT) to study the transferability of neural network models for the sentence (pair) classification problem; Lee et al. (2017) utilize the INIT method on highly related datasets of electronic health records to study their specific deidentification problem. Yang et al. (2017b) use the MULT approach in sequence tagging tasks including named entity recognition. Following the MULT scheme, Wang e"
D18-1226,N16-1030,0,0.701796,"Introduction Named entity recognition (NER) focuses on extracting named entities in a given text while identifying their underlying semantic types. Most earlier approaches to NER are based on conventional structured prediction models such as conditional random fields (CRF) (Lafferty et al., 2001; Sarawagi and Cohen, 2004), relying on handcrafted features which can be designed based on domain-specific knowledge (Yang and Cardie, 2012; Passos et al., 2014; Luo et al., 2015). Recently, neural architectures have been shown effective in such a task, whereby minimal feature engineering is required (Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018). Domain adaptation, as a special case for transfer learning, aims to exploit the abundant data of wellstudied source domains to improve the performance in target domains of interest (Pan and Yang, 2010; Weiss et al., 2016). There is a growing interest in investigating the transferability of neural models for NLP. Two notable approaches, namely INIT (parameter initialization) and MULT (multitask learning), have been proposed for studying the transferrability of neural networks under tasks such as sentence (pair) classification (Mou et"
D18-1226,P12-1109,0,0.0161733,"such word pairs. To construct a pivot lexicon, first, motivated by Tan et al. (2015), we define P1 , which consists of the ordinary words that have high relative frequency in both source and target domain corpora: P1 = {(ws , wt )|ws = wt , f (ws ) s , f (wt ) t }, where f (w) is the frequency function that returns the number of occorrence of the word w in the dataset, and s and t are word frequency thresholds2 . Optionally, we can utilize a customized word-pair list P2 , which gives mappings between domain-specific words across domains, such as normalization lexicons (Han and Baldwin, 2011; Liu et al., 2012). The final lexicon is thus defined as P = P1 [ P2 . 3.1.2 Projection Learning Mathematically, given the pre-trained domainspecific word embeddings VS and VT as well as a pivot lexicon P, we would like to learn a linear projection transforming word vectors from VT into VS . This idea is based on a bilingual word embedding model (Mikolov et al., 2013), but we adapt it to this domain adaptation task. ⇤, We first construct two matrices VS⇤ and VT where the i-th rows of these two matrices are the vector representations for the words in the i-th entry of P: P (i) = (wsi , wti ). We use VS⇤i to deno"
D18-1226,D15-1104,0,0.017262,"hared Word Embeddings Training with Random Samples from Source or Target Data (b) MULT Figure 1: Two existing adaptation approaches for NER. Introduction Named entity recognition (NER) focuses on extracting named entities in a given text while identifying their underlying semantic types. Most earlier approaches to NER are based on conventional structured prediction models such as conditional random fields (CRF) (Lafferty et al., 2001; Sarawagi and Cohen, 2004), relying on handcrafted features which can be designed based on domain-specific knowledge (Yang and Cardie, 2012; Passos et al., 2014; Luo et al., 2015). Recently, neural architectures have been shown effective in such a task, whereby minimal feature engineering is required (Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018). Domain adaptation, as a special case for transfer learning, aims to exploit the abundant data of wellstudied source domains to improve the performance in target domains of interest (Pan and Yang, 2010; Weiss et al., 2016). There is a growing interest in investigating the transferability of neural models for NLP. Two notable approaches, namely INIT (parameter initialization) and MULT (multitask"
D18-1226,P16-1101,0,0.645623,"tity recognition (NER) focuses on extracting named entities in a given text while identifying their underlying semantic types. Most earlier approaches to NER are based on conventional structured prediction models such as conditional random fields (CRF) (Lafferty et al., 2001; Sarawagi and Cohen, 2004), relying on handcrafted features which can be designed based on domain-specific knowledge (Yang and Cardie, 2012; Passos et al., 2014; Luo et al., 2015). Recently, neural architectures have been shown effective in such a task, whereby minimal feature engineering is required (Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018). Domain adaptation, as a special case for transfer learning, aims to exploit the abundant data of wellstudied source domains to improve the performance in target domains of interest (Pan and Yang, 2010; Weiss et al., 2016). There is a growing interest in investigating the transferability of neural models for NLP. Two notable approaches, namely INIT (parameter initialization) and MULT (multitask learning), have been proposed for studying the transferrability of neural networks under tasks such as sentence (pair) classification (Mou et al., 2016) and sequ"
D18-1226,D16-1046,0,0.375137,"., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018). Domain adaptation, as a special case for transfer learning, aims to exploit the abundant data of wellstudied source domains to improve the performance in target domains of interest (Pan and Yang, 2010; Weiss et al., 2016). There is a growing interest in investigating the transferability of neural models for NLP. Two notable approaches, namely INIT (parameter initialization) and MULT (multitask learning), have been proposed for studying the transferrability of neural networks under tasks such as sentence (pair) classification (Mou et al., 2016) and sequence labeling (Yang et al., 2017b). The INIT method first trains a model using labeled data from the source domain; next, it initializes a target model with the learned parameters; finally, it fine-tunes the initialized target model using labeled data from the target domain. The MULT method, on the other hand, simultaneously trains two models using both source and target data respectively, where some parameters are shared across the two models during the learning process. Figure 1 illustrates the two approaches based on the BLSTM-CRF (bidirectional LSTM augmented with a CRF layer) arc"
D18-1226,W14-1609,0,0.0321713,"INIT Shared BLSTMs Shared Word Embeddings Training with Random Samples from Source or Target Data (b) MULT Figure 1: Two existing adaptation approaches for NER. Introduction Named entity recognition (NER) focuses on extracting named entities in a given text while identifying their underlying semantic types. Most earlier approaches to NER are based on conventional structured prediction models such as conditional random fields (CRF) (Lafferty et al., 2001; Sarawagi and Cohen, 2004), relying on handcrafted features which can be designed based on domain-specific knowledge (Yang and Cardie, 2012; Passos et al., 2014; Luo et al., 2015). Recently, neural architectures have been shown effective in such a task, whereby minimal feature engineering is required (Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018). Domain adaptation, as a special case for transfer learning, aims to exploit the abundant data of wellstudied source domains to improve the performance in target domains of interest (Pan and Yang, 2010; Weiss et al., 2016). There is a growing interest in investigating the transferability of neural models for NLP. Two notable approaches, namely INIT (parameter initialization)"
D18-1226,P16-2025,0,0.0238392,"main is social media. We designed this experimental setup based on the following considerations: • Challenging: Newswire is a well-studied domain for NER and existing neural models perform very well (around 90.0 F1-score (Ma and Hovy, 2016)). However, the performance drop dramatically in social media data (around 60.0 F-score (Strauss et al., 2016)). • Important: Social media is a rich soil for text mining (Petrovic et al., 2010; Rosenthal and McKeown, 2015; Wang and Yang, 2015), and NER is of significant importance for other information extraction tasks in social media (Ritter et al., 2011a; Peng and Dredze, 2016; Chou et al., 2016). • Representative: The noisy nature of user generated content as well as emerging entities with novel surface forms make the domain shift very salient (Finin et al., 2010; Han et al., 2016). Nevertheless, the techniques developed in this paper are domain independent and thus can be used for other learning tasks across any two domains so long as we have the necessary resources. 4.2 Resources for Cross-domain Embeddings We utilizes GloVe (Pennington et al., 2014) to train domain-specific and domain-general word embeddings from different corpora, denoted as follows: 1) source"
D18-1226,D14-1162,0,0.0829122,", and NER is of significant importance for other information extraction tasks in social media (Ritter et al., 2011a; Peng and Dredze, 2016; Chou et al., 2016). • Representative: The noisy nature of user generated content as well as emerging entities with novel surface forms make the domain shift very salient (Finin et al., 2010; Han et al., 2016). Nevertheless, the techniques developed in this paper are domain independent and thus can be used for other learning tasks across any two domains so long as we have the necessary resources. 4.2 Resources for Cross-domain Embeddings We utilizes GloVe (Pennington et al., 2014) to train domain-specific and domain-general word embeddings from different corpora, denoted as follows: 1) source emb, which is trained on the newswire domain corpus (NewYorkTimes and DailyMail articles); 2) target emb, which is trained on a social media corpus (Archive Team’s Twitter stream grab3 ); 3) general emb, which is pretrained on CommonCrawl containing both formal 2016 3 https://archive.org/details/twitterstream #train token #dev token #test token #train sent. #dev sent. #test sent. #entity type CO 204,567 51,578 46,666 14,987 3,466 3,684 4 ON 848,220 144,319 49,235 33,908 5,771 1,89"
D18-1226,P17-1161,0,0.112917,"ER) focuses on extracting named entities in a given text while identifying their underlying semantic types. Most earlier approaches to NER are based on conventional structured prediction models such as conditional random fields (CRF) (Lafferty et al., 2001; Sarawagi and Cohen, 2004), relying on handcrafted features which can be designed based on domain-specific knowledge (Yang and Cardie, 2012; Passos et al., 2014; Luo et al., 2015). Recently, neural architectures have been shown effective in such a task, whereby minimal feature engineering is required (Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018). Domain adaptation, as a special case for transfer learning, aims to exploit the abundant data of wellstudied source domains to improve the performance in target domains of interest (Pan and Yang, 2010; Weiss et al., 2016). There is a growing interest in investigating the transferability of neural models for NLP. Two notable approaches, namely INIT (parameter initialization) and MULT (multitask learning), have been proposed for studying the transferrability of neural networks under tasks such as sentence (pair) classification (Mou et al., 2016) and sequence labeling (Yang e"
D18-1226,N10-1021,0,0.0204669,"he base model (↵base ) and the learning rate used for the adaptation Source and Target Domains We evaluate our approach with the setting that the source domain is newswire and the target domain is social media. We designed this experimental setup based on the following considerations: • Challenging: Newswire is a well-studied domain for NER and existing neural models perform very well (around 90.0 F1-score (Ma and Hovy, 2016)). However, the performance drop dramatically in social media data (around 60.0 F-score (Strauss et al., 2016)). • Important: Social media is a rich soil for text mining (Petrovic et al., 2010; Rosenthal and McKeown, 2015; Wang and Yang, 2015), and NER is of significant importance for other information extraction tasks in social media (Ritter et al., 2011a; Peng and Dredze, 2016; Chou et al., 2016). • Representative: The noisy nature of user generated content as well as emerging entities with novel surface forms make the domain shift very salient (Finin et al., 2010; Han et al., 2016). Nevertheless, the techniques developed in this paper are domain independent and thus can be used for other learning tasks across any two domains so long as we have the necessary resources. 4.2 Resour"
D18-1226,D11-1141,0,0.38573,"wire and the target domain is social media. We designed this experimental setup based on the following considerations: • Challenging: Newswire is a well-studied domain for NER and existing neural models perform very well (around 90.0 F1-score (Ma and Hovy, 2016)). However, the performance drop dramatically in social media data (around 60.0 F-score (Strauss et al., 2016)). • Important: Social media is a rich soil for text mining (Petrovic et al., 2010; Rosenthal and McKeown, 2015; Wang and Yang, 2015), and NER is of significant importance for other information extraction tasks in social media (Ritter et al., 2011a; Peng and Dredze, 2016; Chou et al., 2016). • Representative: The noisy nature of user generated content as well as emerging entities with novel surface forms make the domain shift very salient (Finin et al., 2010; Han et al., 2016). Nevertheless, the techniques developed in this paper are domain independent and thus can be used for other learning tasks across any two domains so long as we have the necessary resources. 4.2 Resources for Cross-domain Embeddings We utilizes GloVe (Pennington et al., 2014) to train domain-specific and domain-general word embeddings from different corpora, denot"
D18-1226,W15-4625,0,0.0210838,"and the learning rate used for the adaptation Source and Target Domains We evaluate our approach with the setting that the source domain is newswire and the target domain is social media. We designed this experimental setup based on the following considerations: • Challenging: Newswire is a well-studied domain for NER and existing neural models perform very well (around 90.0 F1-score (Ma and Hovy, 2016)). However, the performance drop dramatically in social media data (around 60.0 F-score (Strauss et al., 2016)). • Important: Social media is a rich soil for text mining (Petrovic et al., 2010; Rosenthal and McKeown, 2015; Wang and Yang, 2015), and NER is of significant importance for other information extraction tasks in social media (Ritter et al., 2011a; Peng and Dredze, 2016; Chou et al., 2016). • Representative: The noisy nature of user generated content as well as emerging entities with novel surface forms make the domain shift very salient (Finin et al., 2010; Han et al., 2016). Nevertheless, the techniques developed in this paper are domain independent and thus can be used for other learning tasks across any two domains so long as we have the necessary resources. 4.2 Resources for Cross-domain Embeddin"
D18-1226,P15-2108,0,0.0124591,"w can we address such challenges while maintaining the improvement by using domain-specific embeddings? We address this issue by developing a word adaptation layer, bridging the gap between the source and target embedding spaces, so that both input features and model parameters be1 We also confirm this claim with experiments presented in supplementary materials. Pivot Lexicon A pivot word pair is denoted as (ws , wt ), where ws 2 XS and wt 2 XT . Here, XS and XT are source and target vocabularies. A pivot lexicon P is a set of such word pairs. To construct a pivot lexicon, first, motivated by Tan et al. (2015), we define P1 , which consists of the ordinary words that have high relative frequency in both source and target domain corpora: P1 = {(ws , wt )|ws = wt , f (ws ) s , f (wt ) t }, where f (w) is the frequency function that returns the number of occorrence of the word w in the dataset, and s and t are word frequency thresholds2 . Optionally, we can utilize a customized word-pair list P2 , which gives mappings between domain-specific words across domains, such as normalization lexicons (Han and Baldwin, 2011; Liu et al., 2012). The final lexicon is thus defined as P = P1 [ P2 . 3.1.2 Projectio"
D18-1226,D15-1306,0,0.0132666,"r the adaptation Source and Target Domains We evaluate our approach with the setting that the source domain is newswire and the target domain is social media. We designed this experimental setup based on the following considerations: • Challenging: Newswire is a well-studied domain for NER and existing neural models perform very well (around 90.0 F1-score (Ma and Hovy, 2016)). However, the performance drop dramatically in social media data (around 60.0 F-score (Strauss et al., 2016)). • Important: Social media is a rich soil for text mining (Petrovic et al., 2010; Rosenthal and McKeown, 2015; Wang and Yang, 2015), and NER is of significant importance for other information extraction tasks in social media (Ritter et al., 2011a; Peng and Dredze, 2016; Chou et al., 2016). • Representative: The noisy nature of user generated content as well as emerging entities with novel surface forms make the domain shift very salient (Finin et al., 2010; Han et al., 2016). Nevertheless, the techniques developed in this paper are domain independent and thus can be used for other learning tasks across any two domains so long as we have the necessary resources. 4.2 Resources for Cross-domain Embeddings We utilizes GloVe ("
D18-1226,N18-1001,0,0.182949,"2007) and 2019 structural correspondence learning (Blitzer et al., 2006). Fewer works have been focused on transfer approaches for neural models in NLP. Mou et al. (2016) use intuitive transfer methods (INIT and MULT) to study the transferability of neural network models for the sentence (pair) classification problem; Lee et al. (2017) utilize the INIT method on highly related datasets of electronic health records to study their specific deidentification problem. Yang et al. (2017b) use the MULT approach in sequence tagging tasks including named entity recognition. Following the MULT scheme, Wang et al. (2018) introduce a label-aware mechanism into maximum mean discrepancy (MMD) to explicitly reduce domain shift between the same labels across domains in medical data. Their approach requires the output space to be the same in both source and target domains. Note that the scenario in our paper is that the output spaces are different in two domains. We propose to learn the word adaptation layer in our task inspired by two prior studies. Fang and Cohn (2017) use the cross-lingual word embeddings to obtain distant supervision for target languages. Yang et al. (2017a) propose to re-train word embeddings"
D18-1226,D12-1122,0,0.0303996,"ng with Target Data (a) INIT Shared BLSTMs Shared Word Embeddings Training with Random Samples from Source or Target Data (b) MULT Figure 1: Two existing adaptation approaches for NER. Introduction Named entity recognition (NER) focuses on extracting named entities in a given text while identifying their underlying semantic types. Most earlier approaches to NER are based on conventional structured prediction models such as conditional random fields (CRF) (Lafferty et al., 2001; Sarawagi and Cohen, 2004), relying on handcrafted features which can be designed based on domain-specific knowledge (Yang and Cardie, 2012; Passos et al., 2014; Luo et al., 2015). Recently, neural architectures have been shown effective in such a task, whereby minimal feature engineering is required (Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018). Domain adaptation, as a special case for transfer learning, aims to exploit the abundant data of wellstudied source domains to improve the performance in target domains of interest (Pan and Yang, 2010; Weiss et al., 2016). There is a growing interest in investigating the transferability of neural models for NLP. Two notable approaches, namely INIT (param"
D18-1226,D17-1312,1,0.64268,", 2017; Liu et al., 2018). Domain adaptation, as a special case for transfer learning, aims to exploit the abundant data of wellstudied source domains to improve the performance in target domains of interest (Pan and Yang, 2010; Weiss et al., 2016). There is a growing interest in investigating the transferability of neural models for NLP. Two notable approaches, namely INIT (parameter initialization) and MULT (multitask learning), have been proposed for studying the transferrability of neural networks under tasks such as sentence (pair) classification (Mou et al., 2016) and sequence labeling (Yang et al., 2017b). The INIT method first trains a model using labeled data from the source domain; next, it initializes a target model with the learned parameters; finally, it fine-tunes the initialized target model using labeled data from the target domain. The MULT method, on the other hand, simultaneously trains two models using both source and target data respectively, where some parameters are shared across the two models during the learning process. Figure 1 illustrates the two approaches based on the BLSTM-CRF (bidirectional LSTM augmented with a CRF layer) architecture for NER. While such approaches"
D18-1226,C00-2137,0,0.112634,"s As shown in Table 5, we conduct some additional experiments to investigate the significance of our improvements on different source-target domains, and whether the improvement is simply because of the increased model expressiveness due to a larger number of parameters.7 We first set the hidden dimension to be the same as the dimension of source-domain word embeddings for the sentence adaptation layer, which is 200 (I/M-200). The dimension used for the output adaptation layer is just half of that of the base BLSTM model. Overall, our model roughly 6 We use the approximate randomization test (Yeh, 2000) for statistical significance of the difference between “Ours” and “MULT+INIT”. Our improvements are statistically significant with a p-value of 0.0033. 7 In this table, I-200/300 and M-200/300 represent the best performance from {INIT-Frozen, INIT-FineTune} and {MULT, MULT+INIT} respectively after tuning; “in-do.” here is the best score of our base model without transfer. 5.2 Ablation Test 2018 Settings Ours-Frozen Ours-FineTune w/o transfer w/o using confidence ci w/o using P2 w/o word adapt. layer w/o sentence adapt. layer w/o output adapt. layer F 63.95 63.40 63.26 66.04 66.11 64.51 65.25"
D18-1265,W13-3520,0,0.0248219,"(Dong and Lapata, 2016), the neural hybrid tree (N EURAL HT) model (Susanto and Lu, 2017b), and the multilingual semantic 11 The embeddings are fixed to avoid overfitting. https://gitlab.com/sutd nlp/statnlp-core 13 (Lu, 2015) is an extension of the original relaxed hybrid tree (Lu, 2014), which reports improved results. 12 ? negara bagian apa Dependency-based Hybrid Tree m1 m5 Hyperparameters We set the maximum depth c of the semantic tree to 20, following Lu (2015). The L2 regularization coefficient is tuned from 0.01 to 0.05 using 5-fold cross-validation on the training set. The Polyglot (Al-Rfou et al., 2013) multilingual word embeddings11 (with 64 dimensions) are used for all languages. We use LBFGS (Liu and Nocedal, 1989) to optimize the D EP HT model until convergence and stochastic gradient descent (SGD) with a learning rate of 0.05 to optimize the neural D EP HT model. We implemented our neural component with the Torch7 library (Collobert et al., 2011). Our complete implementation is based on the StatNLP12 structured prediction framework (Lu, 2017). m1 : QUERY : answer (STATE) m2 : STATE : loc (CITY) m3 : CITY : cityid (CITYNAME) m4 : STATE : state (all) m5 : CITYNAME : (0 san antonio0 ) m1 r"
D18-1265,D13-1160,0,0.0677848,"semantics as latent dependencies between words for semantic parsing. 2 Related Work The literature on semantic parsing has focused on various types of semantic formalisms. The λ-calculus expressions (Zettlemoyer and Collins, 2005) have been popular and widely used in semantic parsing tasks over recent years (Dong and Lapata, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2016, 2017; Susanto and Lu, 2017a; Cheng et al., 2017). Dependency-based compositional semantics (DCS)2 was introduced by Liang et al. (2011), whose extension, λ-DCS, was later proposed by Liang (2013). Various models (Berant et al., 2013; Wang et al., 2015; Jia and Liang, 2016) on semantic parsing with the λ-DCS formalism were proposed. In this work, we focus on the tree-structured semantic formalism which has been examined by various research efforts (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2010; Jones et al., 2012; Lu, 2014; Zou and Lu, 2018). Wong and Mooney (2006) proposed the WASP semantic parser that regards the task as a phrasebased machine translation problem. Lu et al. (2008) proposed a generative process to generate natural language words and semantic units in a joint model"
D18-1265,P16-1223,0,0.0128247,"feature representations within our graphical model. We employ a neural architecture to calculate the score associated with each dependency arc (wp , wc , m) (here wp and wc are the parent and child words in the dependency and m is the semantic unit over the arc), where the input to the neural network consists of words (i.e., (wp , wc )) associated with this dependency and the neural network will calculate a score for each possible semantic unit, including m. The two words are first mapped to word embeddings ep and ec (both of dimension d). Next, we use a bilinear layer10 (Socher et al., 2013; Chen et al., 2016) to capture the interaction between the parent and the child in a dependency: ri = eT p Ui ec where ri represents the score for the i-th semantic unit and Ui ∈ Rd×d . The scores are then incorporated into the probability expression in Equation 1 during learning and decoding. As a comparison, we also implemented a variant where our model directly takes in the average embedding of ep and ec as additional features, without using our neural component. 4 Experiments Data and evaluation methodology We conduct experiments on the publicly available variablefree version of the GeoQuery dataset, which h"
D18-1265,D14-1082,0,0.0470459,"Missing"
D18-1265,P17-1005,0,0.179467,"ning representations automatically. The task has been popular for decades and keeps receiving significant attention from the NLP community. Various systems (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011) were proposed over the years to deal with different types of semantic representations. Such models include structure-based models (Wong and Mooney, 2006; Lu et al., 2008; 1 We make our system and code available at http:// statnlp.org/research/sp. Kwiatkowski et al., 2010; Jones et al., 2012) and neural network based models (Dong and Lapata, 2016; Cheng et al., 2017). Following various previous research efforts (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), in this work, we adopt a popular class of semantic formalism – logical forms that can be equivalently represented as tree structures. The tree representation of an example MR is shown in the middle of Figure 1. One challenge associated with building a semantic parser is that the exact correspondence between the words and atomic semantic units are not explicitly given during the training phase. The key to the building of a successful semantic parsing model lies in the identification of a"
D18-1265,W09-3726,0,0.0326989,"ons proposed in the literature include a chart used in phrase-based translation (Wong and Mooney, 2006), a constituency tree-like representation known as hybrid tree (Lu et al., 2008), and a CCG-based derivation tree (Kwiatkowski et al., 2010). Previous research efforts have shown the effec2431 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2431–2441 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics tiveness of using dependency structures to extract semantic representations (Debusmann et al., 2004; Cimiano, 2009; B´edaride and Gardent, 2011; Stanovsky et al., 2016). Recently, Reddy et al. (2016, 2017) proposed a model to construct logical representations from sentences that are parsed into dependency structures. Their work demonstrates the connection between the dependency structures of a sentence and its underlying semantics. Although their setup and objectives are different from ours where externally trained dependency parsers are assumed available and their system was trained to use the semantics for a specific down-stream task, the success of their work motivates us to propose a novel joint repre"
D18-1265,C04-1026,0,0.0475149,"ample joint representations proposed in the literature include a chart used in phrase-based translation (Wong and Mooney, 2006), a constituency tree-like representation known as hybrid tree (Lu et al., 2008), and a CCG-based derivation tree (Kwiatkowski et al., 2010). Previous research efforts have shown the effec2431 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2431–2441 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics tiveness of using dependency structures to extract semantic representations (Debusmann et al., 2004; Cimiano, 2009; B´edaride and Gardent, 2011; Stanovsky et al., 2016). Recently, Reddy et al. (2016, 2017) proposed a model to construct logical representations from sentences that are parsed into dependency structures. Their work demonstrates the connection between the dependency structures of a sentence and its underlying semantics. Although their setup and objectives are different from ours where externally trained dependency parsers are assumed available and their system was trained to use the semantics for a specific down-stream task, the success of their work motivates us to propose a no"
D18-1265,P16-1004,0,0.488336,"chine interpretable meaning representations automatically. The task has been popular for decades and keeps receiving significant attention from the NLP community. Various systems (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011) were proposed over the years to deal with different types of semantic representations. Such models include structure-based models (Wong and Mooney, 2006; Lu et al., 2008; 1 We make our system and code available at http:// statnlp.org/research/sp. Kwiatkowski et al., 2010; Jones et al., 2012) and neural network based models (Dong and Lapata, 2016; Cheng et al., 2017). Following various previous research efforts (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), in this work, we adopt a popular class of semantic formalism – logical forms that can be equivalently represented as tree structures. The tree representation of an example MR is shown in the middle of Figure 1. One challenge associated with building a semantic parser is that the exact correspondence between the words and atomic semantic units are not explicitly given during the training phase. The key to the building of a successful semantic parsing model lies in the"
D18-1265,P15-1030,0,0.115831,"d with patterns. Feature Type Word Pattern Transition Head word Modifier word Bag of words Examples “m4 & run”, “m4 & through” “m2 & XY”, “m4 & WX” “m2 & m3 ”, “m2 & m4 ” “m2 & What”, “m4 & not” “m2 & not”, “m4 & through” “m4 & not”, “m4 & run”, “m4 & through” Table 2: Features for the example in Figure 2. input sentence n by the Viterbi algorithm. This step can also be done efficiently with our dynamicprogramming approach, where we switch from marginal inference to MAP inference: m∗ , t∗ = arg max ew·f (n,m,t) m,t∈T (n,m) A similar decoding procedure has been used in previous work (Lu, 2014; Durrett and Klein, 2015) with CKY-based parsing algorithm. 3.6 Features As shown in Equation 1, the features are defined on the tuple (n, m, t). With the dynamicprogramming procedure, we can define the features over the structures in Figure 2. Our feature design is inspired by the hybrid tree model (Lu, 2015) and graph-based dependency parsing (McDonald et al., 2005). Table 2 shows the feature templates for the example in Figure 2. Specifically, we define simple unigram features (concatenation of a semantic unit and a word that directly appears under the unit), pattern features (concatenation of the semantic unit and"
D18-1265,C96-1058,0,0.695481,"act inference for learning and decoding in the next section. 3.5 Learning and Decoding We propose dynamic-programming algorithms to perform efficient and exact inference, which will be used for calculating the objective and gradients discussed in the previous section. The algorithms are inspired by the inside-outside style algorithm (Baker, 1979), graph-based dependency parsing (Eisner, 2000; Koo and Collins, 2010; Shi et al., 2017), and the relaxed hybrid tree model (Lu, 2014, 2015). As discussed in Section 3.3, our latent dependency trees are projective as in traditional dependency parsing (Eisner, 1996; Nivre and Scholz, 2004; McDonald et al., 2005) – the dependencies are non-crossing with respect to the word order (see bottom of Figure 1). 2435 7 We ignore the L2 regularization term for brevity. The objective function in Equation 2 can be further decomposed into the following form8 : (a) = m1 ,B i X L(w) = − log (n,m)∈D + X log (n,m)∈D X ew·f (n,m,t) (b) t∈T (n,m) X w·f (n,m0 ,t0 ) e m0 ,t0 ∈T (n,m0 ) (c) j k i m1 ,W + k = i j m1 ,W k i+1 m1 ,X (d) i = m1 ,WX i We can see the first term is essentially the combined score of all the possible latent structures containing the pair (n, m). The"
D18-1265,P18-1033,0,0.0267708,"Missing"
D18-1265,P16-1002,0,0.056482,"n words for semantic parsing. 2 Related Work The literature on semantic parsing has focused on various types of semantic formalisms. The λ-calculus expressions (Zettlemoyer and Collins, 2005) have been popular and widely used in semantic parsing tasks over recent years (Dong and Lapata, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2016, 2017; Susanto and Lu, 2017a; Cheng et al., 2017). Dependency-based compositional semantics (DCS)2 was introduced by Liang et al. (2011), whose extension, λ-DCS, was later proposed by Liang (2013). Various models (Berant et al., 2013; Wang et al., 2015; Jia and Liang, 2016) on semantic parsing with the λ-DCS formalism were proposed. In this work, we focus on the tree-structured semantic formalism which has been examined by various research efforts (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2010; Jones et al., 2012; Lu, 2014; Zou and Lu, 2018). Wong and Mooney (2006) proposed the WASP semantic parser that regards the task as a phrasebased machine translation problem. Lu et al. (2008) proposed a generative process to generate natural language words and semantic units in a joint model. The resulting representation is called"
D18-1265,C14-1122,1,0.882688,"Missing"
D18-1265,P12-1051,0,0.157235,"s to transform the natural language sentences into machine interpretable meaning representations automatically. The task has been popular for decades and keeps receiving significant attention from the NLP community. Various systems (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011) were proposed over the years to deal with different types of semantic representations. Such models include structure-based models (Wong and Mooney, 2006; Lu et al., 2008; 1 We make our system and code available at http:// statnlp.org/research/sp. Kwiatkowski et al., 2010; Jones et al., 2012) and neural network based models (Dong and Lapata, 2016; Cheng et al., 2017). Following various previous research efforts (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), in this work, we adopt a popular class of semantic formalism – logical forms that can be equivalently represented as tree structures. The tree representation of an example MR is shown in the middle of Figure 1. One challenge associated with building a semantic parser is that the exact correspondence between the words and atomic semantic units are not explicitly given during the training phase. The key to the buil"
D18-1265,P06-1115,0,0.0584038,"sed in semantic parsing tasks over recent years (Dong and Lapata, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2016, 2017; Susanto and Lu, 2017a; Cheng et al., 2017). Dependency-based compositional semantics (DCS)2 was introduced by Liang et al. (2011), whose extension, λ-DCS, was later proposed by Liang (2013). Various models (Berant et al., 2013; Wang et al., 2015; Jia and Liang, 2016) on semantic parsing with the λ-DCS formalism were proposed. In this work, we focus on the tree-structured semantic formalism which has been examined by various research efforts (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2010; Jones et al., 2012; Lu, 2014; Zou and Lu, 2018). Wong and Mooney (2006) proposed the WASP semantic parser that regards the task as a phrasebased machine translation problem. Lu et al. (2008) proposed a generative process to generate natural language words and semantic units in a joint model. The resulting representation is called hybrid tree where both natural language words and semantics are encoded into a joint representation. The UBL-s (Kwiatkowski et al., 2010) parser applied the CCG grammar (Steedman, 1996) to model the joint representation of"
D18-1265,P10-1001,0,0.30566,"tations involves all possible dependency-based hybrid trees. As there are exponentially many such trees, an efficient inference procedure is required. We will present our efficient algorithm to perform exact inference for learning and decoding in the next section. 3.5 Learning and Decoding We propose dynamic-programming algorithms to perform efficient and exact inference, which will be used for calculating the objective and gradients discussed in the previous section. The algorithms are inspired by the inside-outside style algorithm (Baker, 1979), graph-based dependency parsing (Eisner, 2000; Koo and Collins, 2010; Shi et al., 2017), and the relaxed hybrid tree model (Lu, 2014, 2015). As discussed in Section 3.3, our latent dependency trees are projective as in traditional dependency parsing (Eisner, 1996; Nivre and Scholz, 2004; McDonald et al., 2005) – the dependencies are non-crossing with respect to the word order (see bottom of Figure 1). 2435 7 We ignore the L2 regularization term for brevity. The objective function in Equation 2 can be further decomposed into the following form8 : (a) = m1 ,B i X L(w) = − log (n,m)∈D + X log (n,m)∈D X ew·f (n,m,t) (b) t∈T (n,m) X w·f (n,m0 ,t0 ) e m0 ,t0 ∈T (n,m"
D18-1265,D10-1119,0,0.37268,"re 1. Semantic parsing aims to transform the natural language sentences into machine interpretable meaning representations automatically. The task has been popular for decades and keeps receiving significant attention from the NLP community. Various systems (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011) were proposed over the years to deal with different types of semantic representations. Such models include structure-based models (Wong and Mooney, 2006; Lu et al., 2008; 1 We make our system and code available at http:// statnlp.org/research/sp. Kwiatkowski et al., 2010; Jones et al., 2012) and neural network based models (Dong and Lapata, 2016; Cheng et al., 2017). Following various previous research efforts (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), in this work, we adopt a popular class of semantic formalism – logical forms that can be equivalently represented as tree structures. The tree representation of an example MR is shown in the middle of Figure 1. One challenge associated with building a semantic parser is that the exact correspondence between the words and atomic semantic units are not explicitly given during the training phase"
D18-1265,N16-1030,0,0.0236515,"s (McDonald et al., 2005). We use the parent (head) and child (modifier) words of the dependency as features. We also use the bag-of-words covered under a dependency as features. The dependency features are useful in helping improve the performance as we can see in the experiments section. 3.7 Neural Component Following the approach used in Susanto and Lu (2017b), we could further incorporate neural networks into our latent-variable graphical model. The integration is analogous to the approaches described in the neural CRF models (Do and Artieres, 2010; Durrett and Klein, 2015; Gormley, 2015; Lample et al., 2016), where we use neural networks to learn distributed feature representations within our graphical model. We employ a neural architecture to calculate the score associated with each dependency arc (wp , wc , m) (here wp and wc are the parent and child words in the dependency and m is the semantic unit over the arc), where the input to the neural network consists of words (i.e., (wp , wc )) associated with this dependency and the neural network will calculate a score for each possible semantic unit, including m. The two words are first mapped to word embeddings ep and ec (both of dimension d). Ne"
D18-1265,J13-2005,0,0.017709,"the first work that models the semantics as latent dependencies between words for semantic parsing. 2 Related Work The literature on semantic parsing has focused on various types of semantic formalisms. The λ-calculus expressions (Zettlemoyer and Collins, 2005) have been popular and widely used in semantic parsing tasks over recent years (Dong and Lapata, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2016, 2017; Susanto and Lu, 2017a; Cheng et al., 2017). Dependency-based compositional semantics (DCS)2 was introduced by Liang et al. (2011), whose extension, λ-DCS, was later proposed by Liang (2013). Various models (Berant et al., 2013; Wang et al., 2015; Jia and Liang, 2016) on semantic parsing with the λ-DCS formalism were proposed. In this work, we focus on the tree-structured semantic formalism which has been examined by various research efforts (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2010; Jones et al., 2012; Lu, 2014; Zou and Lu, 2018). Wong and Mooney (2006) proposed the WASP semantic parser that regards the task as a phrasebased machine translation problem. Lu et al. (2008) proposed a generative process to generate natural language word"
D18-1265,P11-1060,0,0.241589,"dependency-based hybrid tree representation. Introduction Semantic parsing is a fundamental task within the field of natural language processing (NLP). Consider a natural language (NL) sentence and its corresponding meaning representation (MR) as illustrated in Figure 1. Semantic parsing aims to transform the natural language sentences into machine interpretable meaning representations automatically. The task has been popular for decades and keeps receiving significant attention from the NLP community. Various systems (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011) were proposed over the years to deal with different types of semantic representations. Such models include structure-based models (Wong and Mooney, 2006; Lu et al., 2008; 1 We make our system and code available at http:// statnlp.org/research/sp. Kwiatkowski et al., 2010; Jones et al., 2012) and neural network based models (Dong and Lapata, 2016; Cheng et al., 2017). Following various previous research efforts (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), in this work, we adopt a popular class of semantic formalism – logical forms that can be equivalently represented as tree s"
D18-1265,D14-1137,1,0.344176,"ishnamurthy, 2017; Reddy et al., 2016, 2017; Susanto and Lu, 2017a; Cheng et al., 2017). Dependency-based compositional semantics (DCS)2 was introduced by Liang et al. (2011), whose extension, λ-DCS, was later proposed by Liang (2013). Various models (Berant et al., 2013; Wang et al., 2015; Jia and Liang, 2016) on semantic parsing with the λ-DCS formalism were proposed. In this work, we focus on the tree-structured semantic formalism which has been examined by various research efforts (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2010; Jones et al., 2012; Lu, 2014; Zou and Lu, 2018). Wong and Mooney (2006) proposed the WASP semantic parser that regards the task as a phrasebased machine translation problem. Lu et al. (2008) proposed a generative process to generate natural language words and semantic units in a joint model. The resulting representation is called hybrid tree where both natural language words and semantics are encoded into a joint representation. The UBL-s (Kwiatkowski et al., 2010) parser applied the CCG grammar (Steedman, 1996) to model the joint representation of both semantic units and contiguous word sequences which do not overlap wi"
D18-1265,D08-1082,1,0.871455,"nguage (NL) sentence and its corresponding meaning representation (MR) as illustrated in Figure 1. Semantic parsing aims to transform the natural language sentences into machine interpretable meaning representations automatically. The task has been popular for decades and keeps receiving significant attention from the NLP community. Various systems (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011) were proposed over the years to deal with different types of semantic representations. Such models include structure-based models (Wong and Mooney, 2006; Lu et al., 2008; 1 We make our system and code available at http:// statnlp.org/research/sp. Kwiatkowski et al., 2010; Jones et al., 2012) and neural network based models (Dong and Lapata, 2016; Cheng et al., 2017). Following various previous research efforts (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), in this work, we adopt a popular class of semantic formalism – logical forms that can be equivalently represented as tree structures. The tree representation of an example MR is shown in the middle of Figure 1. One challenge associated with building a semantic parser is that the exact corresp"
D18-1265,P05-1012,0,0.48072,"g in the next section. 3.5 Learning and Decoding We propose dynamic-programming algorithms to perform efficient and exact inference, which will be used for calculating the objective and gradients discussed in the previous section. The algorithms are inspired by the inside-outside style algorithm (Baker, 1979), graph-based dependency parsing (Eisner, 2000; Koo and Collins, 2010; Shi et al., 2017), and the relaxed hybrid tree model (Lu, 2014, 2015). As discussed in Section 3.3, our latent dependency trees are projective as in traditional dependency parsing (Eisner, 1996; Nivre and Scholz, 2004; McDonald et al., 2005) – the dependencies are non-crossing with respect to the word order (see bottom of Figure 1). 2435 7 We ignore the L2 regularization term for brevity. The objective function in Equation 2 can be further decomposed into the following form8 : (a) = m1 ,B i X L(w) = − log (n,m)∈D + X log (n,m)∈D X ew·f (n,m,t) (b) t∈T (n,m) X w·f (n,m0 ,t0 ) e m0 ,t0 ∈T (n,m0 ) (c) j k i m1 ,W + k = i j m1 ,W k i+1 m1 ,X (d) i = m1 ,WX i We can see the first term is essentially the combined score of all the possible latent structures containing the pair (n, m). The second term is the combined score for all the po"
D18-1265,nivre-etal-2006-maltparser,0,0.194856,"Missing"
D18-1265,C04-1010,0,0.0263942,"for learning and decoding in the next section. 3.5 Learning and Decoding We propose dynamic-programming algorithms to perform efficient and exact inference, which will be used for calculating the objective and gradients discussed in the previous section. The algorithms are inspired by the inside-outside style algorithm (Baker, 1979), graph-based dependency parsing (Eisner, 2000; Koo and Collins, 2010; Shi et al., 2017), and the relaxed hybrid tree model (Lu, 2014, 2015). As discussed in Section 3.3, our latent dependency trees are projective as in traditional dependency parsing (Eisner, 1996; Nivre and Scholz, 2004; McDonald et al., 2005) – the dependencies are non-crossing with respect to the word order (see bottom of Figure 1). 2435 7 We ignore the L2 regularization term for brevity. The objective function in Equation 2 can be further decomposed into the following form8 : (a) = m1 ,B i X L(w) = − log (n,m)∈D + X log (n,m)∈D X ew·f (n,m,t) (b) t∈T (n,m) X w·f (n,m0 ,t0 ) e m0 ,t0 ∈T (n,m0 ) (c) j k i m1 ,W + k = i j m1 ,W k i+1 m1 ,X (d) i = m1 ,WX i We can see the first term is essentially the combined score of all the possible latent structures containing the pair (n, m). The second term is the combi"
D18-1265,D13-1170,0,0.00575662,"to learn distributed feature representations within our graphical model. We employ a neural architecture to calculate the score associated with each dependency arc (wp , wc , m) (here wp and wc are the parent and child words in the dependency and m is the semantic unit over the arc), where the input to the neural network consists of words (i.e., (wp , wc )) associated with this dependency and the neural network will calculate a score for each possible semantic unit, including m. The two words are first mapped to word embeddings ep and ec (both of dimension d). Next, we use a bilinear layer10 (Socher et al., 2013; Chen et al., 2016) to capture the interaction between the parent and the child in a dependency: ri = eT p Ui ec where ri represents the score for the i-th semantic unit and Ui ∈ Rd×d . The scores are then incorporated into the probability expression in Equation 1 during learning and decoding. As a comparison, we also implemented a variant where our model directly takes in the average embedding of ep and ec as additional features, without using our neural component. 4 Experiments Data and evaluation methodology We conduct experiments on the publicly available variablefree version of the GeoQu"
D18-1265,P17-2007,1,0.34512,"ms the state-of-theart models on 7 out of 8 languages. Further analysis confirms the effectiveness of our dependency-based representation. To the best of our knowledge, this is the first work that models the semantics as latent dependencies between words for semantic parsing. 2 Related Work The literature on semantic parsing has focused on various types of semantic formalisms. The λ-calculus expressions (Zettlemoyer and Collins, 2005) have been popular and widely used in semantic parsing tasks over recent years (Dong and Lapata, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2016, 2017; Susanto and Lu, 2017a; Cheng et al., 2017). Dependency-based compositional semantics (DCS)2 was introduced by Liang et al. (2011), whose extension, λ-DCS, was later proposed by Liang (2013). Various models (Berant et al., 2013; Wang et al., 2015; Jia and Liang, 2016) on semantic parsing with the λ-DCS formalism were proposed. In this work, we focus on the tree-structured semantic formalism which has been examined by various research efforts (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2010; Jones et al., 2012; Lu, 2014; Zou and Lu, 2018). Wong and Mooney (2006) proposed the"
D18-1265,P11-1145,0,0.0348768,"ly generates the meaning representations and natural language words. Lu (2014, 2015) proposed a discriminative version of the hybrid tree model of (Lu et al., 2008) where richer features can be captured. Dong and Lapata (2016) proposed a sequence-totree model using recurrent neural networks where the decoder can branch out to produce tree structures. Susanto and Lu (2017b) augmented the discriminative hybrid tree model with multilayer perceptron and achieved state-of-the-art performance. There exists another line of work that applies given syntactic dependency information to semantic parsing. Titov and Klementiev (2011) decomposed a syntactic dependency tree into fragments and modeled the semantics as relations between the fragments. Poon (2013) learned to derive semantic structures based on syntactic dependency trees predicted by the Stanford dependency parser. Reddy et al. (2016, 2017) proposed a linguistically motivated procedure to transform syntactic dependencies into logical forms. Their semantic parsing performance relies on the quality of the syntactic dependencies. Unlike such efforts, we do not re2 Unlike ours, their work captures dependencies between semantic units but not natural language words."
D18-1265,P13-1092,0,0.0274074,"of (Lu et al., 2008) where richer features can be captured. Dong and Lapata (2016) proposed a sequence-totree model using recurrent neural networks where the decoder can branch out to produce tree structures. Susanto and Lu (2017b) augmented the discriminative hybrid tree model with multilayer perceptron and achieved state-of-the-art performance. There exists another line of work that applies given syntactic dependency information to semantic parsing. Titov and Klementiev (2011) decomposed a syntactic dependency tree into fragments and modeled the semantics as relations between the fragments. Poon (2013) learned to derive semantic structures based on syntactic dependency trees predicted by the Stanford dependency parser. Reddy et al. (2016, 2017) proposed a linguistically motivated procedure to transform syntactic dependencies into logical forms. Their semantic parsing performance relies on the quality of the syntactic dependencies. Unlike such efforts, we do not re2 Unlike ours, their work captures dependencies between semantic units but not natural language words. 2432 Sentence: What rivers do not run through Tennessee ? Relaxed Hybrid Tree Dependency-based Hybrid Tree m1 m1 : QUERY : answe"
D18-1265,P15-1129,0,0.0960921,"Missing"
D18-1265,N06-1056,0,0.496684,". Consider a natural language (NL) sentence and its corresponding meaning representation (MR) as illustrated in Figure 1. Semantic parsing aims to transform the natural language sentences into machine interpretable meaning representations automatically. The task has been popular for decades and keeps receiving significant attention from the NLP community. Various systems (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011) were proposed over the years to deal with different types of semantic representations. Such models include structure-based models (Wong and Mooney, 2006; Lu et al., 2008; 1 We make our system and code available at http:// statnlp.org/research/sp. Kwiatkowski et al., 2010; Jones et al., 2012) and neural network based models (Dong and Lapata, 2016; Cheng et al., 2017). Following various previous research efforts (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), in this work, we adopt a popular class of semantic formalism – logical forms that can be equivalently represented as tree structures. The tree representation of an example MR is shown in the middle of Figure 1. One challenge associated with building a semantic parser is that"
D18-1265,Q16-1010,0,0.131776,"Missing"
D18-1265,D17-1009,0,0.0474918,"Missing"
D18-1265,P18-2107,1,0.810334,"y, 2017; Reddy et al., 2016, 2017; Susanto and Lu, 2017a; Cheng et al., 2017). Dependency-based compositional semantics (DCS)2 was introduced by Liang et al. (2011), whose extension, λ-DCS, was later proposed by Liang (2013). Various models (Berant et al., 2013; Wang et al., 2015; Jia and Liang, 2016) on semantic parsing with the λ-DCS formalism were proposed. In this work, we focus on the tree-structured semantic formalism which has been examined by various research efforts (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2010; Jones et al., 2012; Lu, 2014; Zou and Lu, 2018). Wong and Mooney (2006) proposed the WASP semantic parser that regards the task as a phrasebased machine translation problem. Lu et al. (2008) proposed a generative process to generate natural language words and semantic units in a joint model. The resulting representation is called hybrid tree where both natural language words and semantics are encoded into a joint representation. The UBL-s (Kwiatkowski et al., 2010) parser applied the CCG grammar (Steedman, 1996) to model the joint representation of both semantic units and contiguous word sequences which do not overlap with one another. Jon"
D18-1265,D11-1149,1,\N,Missing
D18-1265,P15-2121,1,\N,Missing
D18-1265,D17-1002,0,\N,Missing
D19-1451,D18-1269,0,0.0277625,"ultilingual entity and description embeddings. Wang et al. (2018) applied GCNs with the connectivity matrix defined on relations to embed entities from multilingual KGs into a unified low-dimensional space. In this work, we also employ GCNs. However, in contrast to Wang et al. (2018), we regard relation features as input to our models. In addition, we investigate two different ways to capture relation and attribute features. Multilingual Sentence Representations. Another line of research related to this work is aligning sentences in multiple languages. Recent works (Hermann and Blunsom, 2014; Conneau et al., 2018; Eriguchi et al., 2018) studied crosslingual sentence classification via zero-shot learning. Johnson et al. (2017) proposed a sequenceto-sequence multilingual machine translation system where the encoder can be used to produce cross-lingual sentence embeddings (Artetxe and Schwenk, 2018). Recently, BERT (Devlin et al., 2019) has advanced the state-of-the-art on multiple natural language understanding tasks. Specifically, multilingual BERT enables learning representations of sentences under multilingual settings. We adopt BERT to produce cross-lingual representations of entity literal descript"
D19-1451,P13-1153,0,0.0780832,"Missing"
D19-1451,N19-1423,0,0.480151,"entities (i.e., topological connections, relations and attributes, as well as literal descriptions) remains under-explored. In this work, we propose a novel approach to learn cross-lingual entity embeddings by using all aforementioned aspects of information in KGs. To be specific, we propose two variants of GCNbased models, namely M AN and H MAN, that incorporate multi-aspect features, including topological features, relation types, and attributes into cross-lingual entity embeddings. To capture semantic relatedness of literal descriptions, we finetune the pretrained multilingual BERT model (Devlin et al., 2019) to bridge cross-lingual gaps. We design two strategies to combine GCN-based and BERT-based modules to make alignment decisions. Experiments show that our method achieves new state-of-the-art results on two benchmark datasets. Source code for our models is publicly available at https://github.com/ h324yang/HMAN. 2 Problem Definition In a multilingual knowledge graph G, we use L to denote the set of languages that G contains and Gi = {Ei , Ri , Ai , Vi , Di } to represent the language-specific knowledge graph in language Li ∈ L. Ei , Ri , Ai , Vi and Di are sets of entities, relations, attribut"
D19-1451,P19-1024,1,0.845187,"duce two GCN-based models, namely M AN and H MAN, that learn entity embeddings from the graph structures. Second, we discuss two uses of a multilingual pretrained BERT model to learn cross-lingual embeddings of entity descriptions: P OINTWISE B ERT and PAIRWISE B ERT. Finally, we investigate two strategies to integrate the GCN-based and the BERT-based modules. 3.1 Cross-Lingual Graph Embeddings Graph convolutional networks (GCNs) (Kipf and Welling, 2017) are variants of convolutional networks that have proven effective in capturing information from graph structures, such as dependency graphs (Guo et al., 2019b), abstract meaning representation graphs (Guo et al., 2019a), and knowledge graphs (Wang et al., 2018). In practice, multi-layer GCNs are stacked to collect evidence from multi-hop neighbors. Formally, the l-th GCN layer takes as input feature representations H (l−1) and outputs H (l) :   (l) − 12 ˜ ˜ − 12 (l−1) (l) ˜ H = φ D AD H W (1) where A˜ = A + I is the adjacency matrix, I is ˜ is the diagonal node degree the identity matrix, D ˜ matrix of A, φ(·) is ReLU function, and W (l) represents learnable parameters in the l-th layer. H (0) is the initial input. GCNs can iteratively update th"
D19-1451,Q19-1019,1,0.822025,"duce two GCN-based models, namely M AN and H MAN, that learn entity embeddings from the graph structures. Second, we discuss two uses of a multilingual pretrained BERT model to learn cross-lingual embeddings of entity descriptions: P OINTWISE B ERT and PAIRWISE B ERT. Finally, we investigate two strategies to integrate the GCN-based and the BERT-based modules. 3.1 Cross-Lingual Graph Embeddings Graph convolutional networks (GCNs) (Kipf and Welling, 2017) are variants of convolutional networks that have proven effective in capturing information from graph structures, such as dependency graphs (Guo et al., 2019b), abstract meaning representation graphs (Guo et al., 2019a), and knowledge graphs (Wang et al., 2018). In practice, multi-layer GCNs are stacked to collect evidence from multi-hop neighbors. Formally, the l-th GCN layer takes as input feature representations H (l−1) and outputs H (l) :   (l) − 12 ˜ ˜ − 12 (l−1) (l) ˜ H = φ D AD H W (1) where A˜ = A + I is the adjacency matrix, I is ˜ is the diagonal node degree the identity matrix, D ˜ matrix of A, φ(·) is ReLU function, and W (l) represents learnable parameters in the l-th layer. H (0) is the initial input. GCNs can iteratively update th"
D19-1451,P14-1006,0,0.0259027,"ithm to alternately learn multilingual entity and description embeddings. Wang et al. (2018) applied GCNs with the connectivity matrix defined on relations to embed entities from multilingual KGs into a unified low-dimensional space. In this work, we also employ GCNs. However, in contrast to Wang et al. (2018), we regard relation features as input to our models. In addition, we investigate two different ways to capture relation and attribute features. Multilingual Sentence Representations. Another line of research related to this work is aligning sentences in multiple languages. Recent works (Hermann and Blunsom, 2014; Conneau et al., 2018; Eriguchi et al., 2018) studied crosslingual sentence classification via zero-shot learning. Johnson et al. (2017) proposed a sequenceto-sequence multilingual machine translation system where the encoder can be used to produce cross-lingual sentence embeddings (Artetxe and Schwenk, 2018). Recently, BERT (Devlin et al., 2019) has advanced the state-of-the-art on multiple natural language understanding tasks. Specifically, multilingual BERT enables learning representations of sentences under multilingual settings. We adopt BERT to produce cross-lingual representations of e"
D19-1451,D14-1005,0,0.0346512,"t Network (M AN) to capture the three aspects of entity features. Specifically, three l-layer GCNs take as inputs the tripleaspect features (i.e., Xt , Xr , and Xa ) and produce (l) (l) (l) the representations Ht , Hr , and Ha according to Equation 1, respectively. Finally, the multiaspect entity embedding is: (l) Hm = [Ht ⊕ Ha(l) ⊕ Hr(l) ] (2) where ⊕ denotes vector concatenation. Hm can then feed into alignment decisions. Such fusion through concatenation is also known as Scoring Level Fusion, which has been proven simple but effective for capturing multimodal semantics (Bruni et al., 2014; Kiela and Bottou, 2014; Collell et al., 2017). It is worth noting that the main differences between M AN and the work of Wang et al. (2018) are two fold: First, we use the same approach as in Kipf and Welling (2017) to construct the adjacency matrix, while Wang et al. (2018) designed a new connectivity matrix as the adjacency matrix for the GCNs. Second, M AN explicitly regards the relation type features as model input, while Wang et al. (2018) incorporated such relation information into the connectivity matrix. H MAN. Note that M AN propagates relation and attribute information through the graph structure. However"
D19-1451,N19-1229,1,0.801615,"rview of P OINTWISE B ERT (left) and PAIRWISE B ERT (right). from which the final hidden state is used as the sequence representation, and [SEP] is the special token for separating token sequences, and produces the probability of classifying the pair as equivalent entities. The probability is then used to rank all candidate entity pairs, i.e., ranking score. We denote this model as P OINTWISE B ERT, shown in Figure 3 (left). This approach is computationally expensive, since for each entity we need to consider all candidate entities in the target language. One solution, inspired by the work of Shi et al. (2019), is to reduce the search space for each entity with a reranking strategy (see Section 3.3). PAIRWISE B ERT. Due to the heavy computational cost of P OINTWISE B ERT, semantic matching between all entity pairs is very expensive. Instead of producing ranking scores for description pairs, we propose PAIRWISE B ERT to encode the entity literal descriptions as cross-lingual textual embeddings, where distances between entity pairs can be directly measured using these embeddings. The PAIRWISE B ERT model consists of two components, each of which takes as input the description of one entity (from the"
D19-1451,D18-1032,0,0.160471,"essing, pages 4431–4441, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics including topological connections, relation types, attributes, and literal descriptions expressed in different languages (Bizer et al., 2009; Xie et al., 2016), as shown in Figure 1 (bottom). The key challenge of addressing such a task thus is how to better model and use provided multi-aspect information of entities to bridge cross-lingual gaps and find more equivalent entities (i.e., ILLs). Recently, embedding-based solutions (Chen et al., 2017b; Sun et al., 2017; Zhu et al., 2017; Wang et al., 2018; Chen et al., 2018) have been proposed to unify multilingual KGs into the same low-dimensional vector space where equivalent entities are close to each other. Such methods only make use of one or two aspects of the aforementioned information. For example, Zhu et al. (2017) relied only on topological features while Sun et al. (2017) and Wang et al. (2018) exploited both topological and attribute features. Chen et al. (2018) proposed a co-training algorithm to combine topological features and literal descriptions of entities. However, combining these multi-aspect information of entities (i.e.,"
D19-1451,D18-1244,0,0.0428449,"ice, multi-layer GCNs are stacked to collect evidence from multi-hop neighbors. Formally, the l-th GCN layer takes as input feature representations H (l−1) and outputs H (l) :   (l) − 12 ˜ ˜ − 12 (l−1) (l) ˜ H = φ D AD H W (1) where A˜ = A + I is the adjacency matrix, I is ˜ is the diagonal node degree the identity matrix, D ˜ matrix of A, φ(·) is ReLU function, and W (l) represents learnable parameters in the l-th layer. H (0) is the initial input. GCNs can iteratively update the representation of each entity node via a propagation mechanism through the graph. Inspired by previous studies (Zhang et al., 2018; Wang et al., 2018), we also 4432 adopt GCNs in this work to collect evidence from multilingual KG structures and to learn crosslingual embeddings of entities. The primary assumptions are: (1) equivalent entities tend to be neighbored by equivalent entities via the same types of relations; (2) equivalent entities tend to share similar or even the same attributes. Multi-Aspect Entity Features. Existing KGs (Bizer et al., 2009; Suchanek et al., 2008; Rebele et al., 2016) provide multi-aspect information of entities. In this section, we mainly focus on the following three aspects: topological co"
D19-1536,Q13-1005,0,0.209347,"ssions in Figure 1 can be equivalently represented by expression trees consisting of such nodes, as illustrated in Figure 2. 2.2 Latent Text-Math Tree With the specifically designed expression trees for representing the math expressions, we will now be able to design a model for parsing the text into the expression tree. This is essentially a semantic parsing task. One of the key assumptions made by the various semantic parsing algorithms is the intermediate joint representation used for connecting the words and semantics (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Lu et al., 2008; Artzi and Zettlemoyer, 2013b). In this work, we adopt an approach that is inspired by (Lu et al., 2008; Lu, 2014), which learns a latent joint representation for words and semantics in the form of hybrid trees where word-semantics correspondence information is captured. Specifically, we introduce a text-math tree representation that jointly encodes both text and the math expression tree. 5328 Mike picked 7 apples. Nancy picked 3 apples and Keith picked 6 apples at the farm. In total, how many apples were picked? A DD(+) (wAwBw) {Mike picked} A {. Nancy picked} B {apples were picked?} A DD(+) (wAwBw) {Mike picked} A {.}"
D19-1536,P14-1026,0,0.299383,"which may involve multiple sentences) into a target math expression, from which an answer can be calculated. The latter task focuses on mapping a description (usually a single sentence) into a math equation that typically involves one or more unknowns. As we can observe from Figure 1, in both cases, the output can be represented as a tree structure. Earlier approaches to solving arithmetic word problems focused on rule-based methods where hand-crafted rules have been used (Mukherjee and Garain, 2008; Hosseini et al., 2014). Recently, learning-based approaches based on statistical classifiers (Kushman et al., 2014; Roy and Roth, 2015; Roy et al., 2016; Liang et al., 2018) or Figure 1: An example arithmetic word problem (top) and an example equation parsing problem (bottom) where the outputs can be represented as trees. neural networks (Wang et al., 2017, 2018b) have been used for making decisions in the expression1 construction process. However, these models do not focus on predicting the target tree as a complete structure at once, but locally trained classifiers are often used and local decisions are then combined. Such local classifiers often make predictions on the choice of the underlying operator"
D19-1536,D10-1119,0,0.0368038,"y problems (Dries et al., 2017), logic puzzle problems (Mitra and Baral, 2015; Chesani et al., 2017) and geometry problems (Seo et al., 2014, 2015). Besides the benchmark datasets used in this work, other popular datasets include Dolphin18K (Shi et al., 2015) and AQuA (Ling et al., 2017) for algebra word problems which are not the focus in this work. Roy et al. (2016) first proposed the Equation Parsing task and designed a pipeline method with three structured predictors. Semantic Parsing. Another line of related works is semantic parsing (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2018; Zou and Lu, 2018), which aims to map sentences into logic forms, including CCGbased lambda calculus expressions (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013b; Dong and Lapata, 2016), FunQL (Kate et al., 2005; Wong and Mooney, 2006; Jones et al., 2012), lambda-DCS (Liang et al., 2011; Berant et al., 2013; Jia and Liang, 2016), graph queries (Harris et al., 2013; Holzschuher and Peinl, 2013) and SQL (Yin et al., 2015; Sun et al., 2018). In this work, we adopt a text-math semantic representation encoding words a"
D19-1536,D13-1160,0,0.017309,"ion Parsing task and designed a pipeline method with three structured predictors. Semantic Parsing. Another line of related works is semantic parsing (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2018; Zou and Lu, 2018), which aims to map sentences into logic forms, including CCGbased lambda calculus expressions (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013b; Dong and Lapata, 2016), FunQL (Kate et al., 2005; Wong and Mooney, 2006; Jones et al., 2012), lambda-DCS (Liang et al., 2011; Berant et al., 2013; Jia and Liang, 2016), graph queries (Harris et al., 2013; Holzschuher and Peinl, 2013) and SQL (Yin et al., 2015; Sun et al., 2018). In this work, we adopt a text-math semantic representation encoding words and the expression tree. 5 Conclusion In this work, we propose a unified structured prediction approach, Text2Math, to solving both arithmetic word problems and equation parsing tasks. We leverage a novel joint representation to automatically learn the correspondence between words and math expressions which reflects semantic closeness. Different from many existing models, Text2Math is agn"
D19-1536,N16-1030,0,0.0266621,"ath representation should precisely contain the exact information associated with the text and its corresponding math expression and nothing else. We will defer the discussion on how to exactly construct such joint representations until Sec. 2.3. The training corpus provides both the problem text x and its math expression, which we represent with an expression tree y. The joint representation t is not available in the training data, which we model as a latent variable. The conditional random fields (CRF) (Lafferty et al., 2001) has been successfully applied to many tasks in the NLP community (Lample et al., 2016; Zou and Lu, 2018, 2019a). In this work, we also apply CRF to model the conditional probability of the latent variable t and output expression y, conditioned on the input x. The objective is defined as follows: PΛ,Θ (y|x) = X PΛ,Θ (y, t|x) t∈T (x,y) P =P t∈T (x,y) e[Λ·Φ(x,y,t)+GΘ (x,y,t)] [Λ·Φ(x,y 0 ,t0 )+GΘ (x,y 0 ,t0 )] y 0 ,t0 ∈T (x,y 0 ) e (1) where Φ(x, y, t) returns a list of discrete features defined over the tuple (x, y, t), Λ is the feature weight vector, GΘ is a neural scoring function parameterized by Θ and T (x, y) is a set of possible joint representations (i.e., text-math trees)"
D19-1536,N18-1060,0,0.11382,"ression, from which an answer can be calculated. The latter task focuses on mapping a description (usually a single sentence) into a math equation that typically involves one or more unknowns. As we can observe from Figure 1, in both cases, the output can be represented as a tree structure. Earlier approaches to solving arithmetic word problems focused on rule-based methods where hand-crafted rules have been used (Mukherjee and Garain, 2008; Hosseini et al., 2014). Recently, learning-based approaches based on statistical classifiers (Kushman et al., 2014; Roy and Roth, 2015; Roy et al., 2016; Liang et al., 2018) or Figure 1: An example arithmetic word problem (top) and an example equation parsing problem (bottom) where the outputs can be represented as trees. neural networks (Wang et al., 2017, 2018b) have been used for making decisions in the expression1 construction process. However, these models do not focus on predicting the target tree as a complete structure at once, but locally trained classifiers are often used and local decisions are then combined. Such local classifiers often make predictions on the choice of the underlying operator between two operands (e.g., numbers) appearing in the text"
D19-1536,P16-1004,0,0.0295626,"(Ling et al., 2017) for algebra word problems which are not the focus in this work. Roy et al. (2016) first proposed the Equation Parsing task and designed a pipeline method with three structured predictors. Semantic Parsing. Another line of related works is semantic parsing (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2018; Zou and Lu, 2018), which aims to map sentences into logic forms, including CCGbased lambda calculus expressions (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013b; Dong and Lapata, 2016), FunQL (Kate et al., 2005; Wong and Mooney, 2006; Jones et al., 2012), lambda-DCS (Liang et al., 2011; Berant et al., 2013; Jia and Liang, 2016), graph queries (Harris et al., 2013; Holzschuher and Peinl, 2013) and SQL (Yin et al., 2015; Sun et al., 2018). In this work, we adopt a text-math semantic representation encoding words and the expression tree. 5 Conclusion In this work, we propose a unified structured prediction approach, Text2Math, to solving both arithmetic word problems and equation parsing tasks. We leverage a novel joint representation to automatically learn the correspondence"
D19-1536,P18-1068,0,0.0221935,"problems (Mitra and Baral, 2015; Chesani et al., 2017) and geometry problems (Seo et al., 2014, 2015). Besides the benchmark datasets used in this work, other popular datasets include Dolphin18K (Shi et al., 2015) and AQuA (Ling et al., 2017) for algebra word problems which are not the focus in this work. Roy et al. (2016) first proposed the Equation Parsing task and designed a pipeline method with three structured predictors. Semantic Parsing. Another line of related works is semantic parsing (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2018; Zou and Lu, 2018), which aims to map sentences into logic forms, including CCGbased lambda calculus expressions (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013b; Dong and Lapata, 2016), FunQL (Kate et al., 2005; Wong and Mooney, 2006; Jones et al., 2012), lambda-DCS (Liang et al., 2011; Berant et al., 2013; Jia and Liang, 2016), graph queries (Harris et al., 2013; Holzschuher and Peinl, 2013) and SQL (Yin et al., 2015; Sun et al., 2018). In this work, we adopt a text-math semantic representation encoding words and the expression tree. 5 Conclusion In thi"
D19-1536,D14-1058,0,0.381575,"., 2016), as illustrated in Figure 1. The former task focuses on mapping the input paragraph (which may involve multiple sentences) into a target math expression, from which an answer can be calculated. The latter task focuses on mapping a description (usually a single sentence) into a math equation that typically involves one or more unknowns. As we can observe from Figure 1, in both cases, the output can be represented as a tree structure. Earlier approaches to solving arithmetic word problems focused on rule-based methods where hand-crafted rules have been used (Mukherjee and Garain, 2008; Hosseini et al., 2014). Recently, learning-based approaches based on statistical classifiers (Kushman et al., 2014; Roy and Roth, 2015; Roy et al., 2016; Liang et al., 2018) or Figure 1: An example arithmetic word problem (top) and an example equation parsing problem (bottom) where the outputs can be represented as trees. neural networks (Wang et al., 2017, 2018b) have been used for making decisions in the expression1 construction process. However, these models do not focus on predicting the target tree as a complete structure at once, but locally trained classifiers are often used and local decisions are then comb"
D19-1536,P16-1002,0,0.0220184,"designed a pipeline method with three structured predictors. Semantic Parsing. Another line of related works is semantic parsing (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2018; Zou and Lu, 2018), which aims to map sentences into logic forms, including CCGbased lambda calculus expressions (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013b; Dong and Lapata, 2016), FunQL (Kate et al., 2005; Wong and Mooney, 2006; Jones et al., 2012), lambda-DCS (Liang et al., 2011; Berant et al., 2013; Jia and Liang, 2016), graph queries (Harris et al., 2013; Holzschuher and Peinl, 2013) and SQL (Yin et al., 2015; Sun et al., 2018). In this work, we adopt a text-math semantic representation encoding words and the expression tree. 5 Conclusion In this work, we propose a unified structured prediction approach, Text2Math, to solving both arithmetic word problems and equation parsing tasks. We leverage a novel joint representation to automatically learn the correspondence between words and math expressions which reflects semantic closeness. Different from many existing models, Text2Math is agnostic of the semantics"
D19-1536,P12-1051,0,0.0190838,"this work. Roy et al. (2016) first proposed the Equation Parsing task and designed a pipeline method with three structured predictors. Semantic Parsing. Another line of related works is semantic parsing (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2018; Zou and Lu, 2018), which aims to map sentences into logic forms, including CCGbased lambda calculus expressions (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013b; Dong and Lapata, 2016), FunQL (Kate et al., 2005; Wong and Mooney, 2006; Jones et al., 2012), lambda-DCS (Liang et al., 2011; Berant et al., 2013; Jia and Liang, 2016), graph queries (Harris et al., 2013; Holzschuher and Peinl, 2013) and SQL (Yin et al., 2015; Sun et al., 2018). In this work, we adopt a text-math semantic representation encoding words and the expression tree. 5 Conclusion In this work, we propose a unified structured prediction approach, Text2Math, to solving both arithmetic word problems and equation parsing tasks. We leverage a novel joint representation to automatically learn the correspondence between words and math expressions which reflects semantic closeness."
D19-1536,Q15-1042,0,0.0358354,"Missing"
D19-1536,P11-1060,0,0.0140441,"2017), logic puzzle problems (Mitra and Baral, 2015; Chesani et al., 2017) and geometry problems (Seo et al., 2014, 2015). Besides the benchmark datasets used in this work, other popular datasets include Dolphin18K (Shi et al., 2015) and AQuA (Ling et al., 2017) for algebra word problems which are not the focus in this work. Roy et al. (2016) first proposed the Equation Parsing task and designed a pipeline method with three structured predictors. Semantic Parsing. Another line of related works is semantic parsing (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2018; Zou and Lu, 2018), which aims to map sentences into logic forms, including CCGbased lambda calculus expressions (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013b; Dong and Lapata, 2016), FunQL (Kate et al., 2005; Wong and Mooney, 2006; Jones et al., 2012), lambda-DCS (Liang et al., 2011; Berant et al., 2013; Jia and Liang, 2016), graph queries (Harris et al., 2013; Holzschuher and Peinl, 2013) and SQL (Yin et al., 2015; Sun et al., 2018). In this work, we adopt a text-math semantic representation encoding words and the expression tr"
D19-1536,P17-1015,0,0.0301913,"clude semantic parsing based approaches (Liang et al., 2018) and neural methods (Wang et al., 2017, 2018a,b). Unlike arithmetic word problems, the goal of algebra word problems is to map the text to an equation set (Kushman et al., 2014; Shi et al., 2015). Other types of problems have also been investigated, including probability problems (Dries et al., 2017), logic puzzle problems (Mitra and Baral, 2015; Chesani et al., 2017) and geometry problems (Seo et al., 2014, 2015). Besides the benchmark datasets used in this work, other popular datasets include Dolphin18K (Shi et al., 2015) and AQuA (Ling et al., 2017) for algebra word problems which are not the focus in this work. Roy et al. (2016) first proposed the Equation Parsing task and designed a pipeline method with three structured predictors. Semantic Parsing. Another line of related works is semantic parsing (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2018; Zou and Lu, 2018), which aims to map sentences into logic forms, including CCGbased lambda calculus expressions (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013b; Dong and Lapata, 20"
D19-1536,D14-1137,1,0.930921,"strated in Figure 2. 2.2 Latent Text-Math Tree With the specifically designed expression trees for representing the math expressions, we will now be able to design a model for parsing the text into the expression tree. This is essentially a semantic parsing task. One of the key assumptions made by the various semantic parsing algorithms is the intermediate joint representation used for connecting the words and semantics (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Lu et al., 2008; Artzi and Zettlemoyer, 2013b). In this work, we adopt an approach that is inspired by (Lu et al., 2008; Lu, 2014), which learns a latent joint representation for words and semantics in the form of hybrid trees where word-semantics correspondence information is captured. Specifically, we introduce a text-math tree representation that jointly encodes both text and the math expression tree. 5328 Mike picked 7 apples. Nancy picked 3 apples and Keith picked 6 apples at the farm. In total, how many apples were picked? A DD(+) (wAwBw) {Mike picked} A {. Nancy picked} B {apples were picked?} A DD(+) (wAwBw) {Mike picked} A {.} B {In total, how many apples were picked?} A DD(+) C ON(7) (wAwBw) (w) {7 apples} {Nan"
D19-1536,P15-2121,1,0.920351,"nsists of the valid trees: Definition 2.1 For a given text x and an expression tree y, a valid text-math tree satisfies the following two properties: 1) the semantics portion of the tree gives exactly y, and 2) the text obtained through the recursive rewriting procedure discussed above gives exactly x3 . Given the definition of the valid text-math trees, we will be able to use a bottom-up procedure to construct the set T (x, y). Similarly, we will be able to construct the set T (x) by considering a forest-structured semantic representation that encodes all possible expression trees following (Lu, 2015). One nice property associated with considering only such joint representations is that there are known algorithms that can be used for performing efficient inference. Indeed, the resulting text-math trees are similar to the hybrid tree representations used in (Lu et al., 2008; Lu, 3 We regard words that appear at different positions in x as distinct words, regardless of their string forms. 5330 2014)4 , where dynamic programming based inference algorithms have been developed. Such algorithms allow O(n3 m) time complexity for inference where n is the text length and m is the number of grammar"
D19-1536,D08-1082,1,0.87271,"he two math expressions in Figure 1 can be equivalently represented by expression trees consisting of such nodes, as illustrated in Figure 2. 2.2 Latent Text-Math Tree With the specifically designed expression trees for representing the math expressions, we will now be able to design a model for parsing the text into the expression tree. This is essentially a semantic parsing task. One of the key assumptions made by the various semantic parsing algorithms is the intermediate joint representation used for connecting the words and semantics (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Lu et al., 2008; Artzi and Zettlemoyer, 2013b). In this work, we adopt an approach that is inspired by (Lu et al., 2008; Lu, 2014), which learns a latent joint representation for words and semantics in the form of hybrid trees where word-semantics correspondence information is captured. Specifically, we introduce a text-math tree representation that jointly encodes both text and the math expression tree. 5328 Mike picked 7 apples. Nancy picked 3 apples and Keith picked 6 apples at the farm. In total, how many apples were picked? A DD(+) (wAwBw) {Mike picked} A {. Nancy picked} B {apples were picked?} A DD(+)"
D19-1536,P14-5010,0,0.00532807,"first example, but not for the second, though their texts only differ slightly. Unlike the second example, instead of using the pattern AwB, the first joint representation adopts the pattern BwA for the S UB expression node. Thus, our model is able to work without the underlying knowledge on whether an operator is commutative or not. 2.4 Features Discrete Features. The feature function Φ(x, y, t) is defined over each node hx, y, pi in the joint tree as well as the complete expression tree y. For each node hx, y, pi, we extract word ngram, the word association pattern, and POS tags for words (Manning et al., 2014). The knowledge that whether a number is relevant to the question (if available in the annotated data) is also taken as a binary feature. To assess the quality of the structure associated with the expression tree (i.e., features defined over y), we extract parent-child relational information (ya , yb ) from y, where ya is 4 They need to handle semantic nodes with arity 1 (which requires special constraints for properly defining T (x) (Lu, 2015)), and their semantic nodes are also assumed to convey semantic type information for guiding the expression tree construction process, while we do not n"
D19-1536,D15-1118,0,0.0200376,"y and Roth, 2017, 2018). Zou and Lu (2019b,c) is the first work that proposed a sequence labelling approach to solving arithmetic word problems, which focuses on addition-subtraction word problems. Other systems include semantic parsing based approaches (Liang et al., 2018) and neural methods (Wang et al., 2017, 2018a,b). Unlike arithmetic word problems, the goal of algebra word problems is to map the text to an equation set (Kushman et al., 2014; Shi et al., 2015). Other types of problems have also been investigated, including probability problems (Dries et al., 2017), logic puzzle problems (Mitra and Baral, 2015; Chesani et al., 2017) and geometry problems (Seo et al., 2014, 2015). Besides the benchmark datasets used in this work, other popular datasets include Dolphin18K (Shi et al., 2015) and AQuA (Ling et al., 2017) for algebra word problems which are not the focus in this work. Roy et al. (2016) first proposed the Equation Parsing task and designed a pipeline method with three structured predictors. Semantic Parsing. Another line of related works is semantic parsing (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2018; Zou and"
D19-1536,P16-1202,0,0.0459395,", a bird was able to make 150 kilometers per hour. X1 + X2 = 150 X1 = 150 Table 6: Examples with wrong predictions. Gold denotes the annotated correct equations and Text2Math refers to output equations generated by our method. ing variables. Consider Example 6. Without world knowledge, it might be difficult for the algorithm to recognize that “Flying with the wind” implies the speed of the wind which should be considered as a variable of the equation. 4 Related Work Math Word Problems. Mukherjee and Garain (2008) surveyed related approaches to this task in literature. Hosseini et al. (2014); Mitra and Baral (2016) solved the task by categorizing verbs or problems. The first method that can handle general arithmetic problems with multiple steps was proposed by Roy and Roth (2015), which was further extended by introducing (Roy and Roth, 2017, 2018). Zou and Lu (2019b,c) is the first work that proposed a sequence labelling approach to solving arithmetic word problems, which focuses on addition-subtraction word problems. Other systems include semantic parsing based approaches (Liang et al., 2018) and neural methods (Wang et al., 2017, 2018a,b). Unlike arithmetic word problems, the goal of algebra word pro"
D19-1536,D15-1202,0,0.230459,"ilar to the work of Lu (2014) the effectiveness of different components, such was leveraged to find the optimal latent structure as POS tags, the lexicon and number relevance, t∗ . We then obtain the optimal expression tree y ∗ ∗ as indicated by “- POS”, “- LEX”, “- ID” in Table from t , which is the output of our system for the 2. By eliminating POS tag features, we achieve input problem text x. ∂L(Λ, Θ) X X = E PΛ,Θ (t|xi ,yi ) [φk (xi , y i , t)] ∂λk t i XX E PΛ,Θ (y,t|xi ) [φk (xi , y, t)] (3) − 5332 System ∗Liang et al. (2018) (Statistical) ∗Liang et al. (2018) (DNN) ∗Roy and Roth (2017) Roy and Roth (2015) Koncel-Kedziorski et al. (2015) Roy et al. (2015) Kushman et al. (2014) Hosseini et al. (2014) Text2Math 12- POS N ON -N EURAL 12- LEX 12- ID L=0 L=1 L=2 L=3 N EURAL L=4 L=5 L=6 AI2 81.5 69.8 76.2 78.0 52.4 64.0 77.7 85.8 86.0 76.8 75.4 84.8 84.5 85.5 86.2 85.5 85.2 86.5 IL 81.0 70.6 71.0 73.9 72.9 52.7 73.7 80.4 81.0 69.4 78.1 79.7 80.3 80.3 80.9 80.0 81.4 81.0 Average 81.25 70.20 73.60 75.95 62.65 68.85 83.10 83.50 73.10 76.75 82.25 82.40 82.90 83.55 82.75 83.30 83.75 System ∗Roy et al. (2016) (Pipeline) ∗Roy et al. (2016) (Joint) ∗ SPF (Artzi and Zettlemoyer, 2013a) Text2Math 12- POS N ON"
D19-1536,D07-1071,0,0.347314,"s supposed to be a leaf node. The two math expressions in Figure 1 can be equivalently represented by expression trees consisting of such nodes, as illustrated in Figure 2. 2.2 Latent Text-Math Tree With the specifically designed expression trees for representing the math expressions, we will now be able to design a model for parsing the text into the expression tree. This is essentially a semantic parsing task. One of the key assumptions made by the various semantic parsing algorithms is the intermediate joint representation used for connecting the words and semantics (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Lu et al., 2008; Artzi and Zettlemoyer, 2013b). In this work, we adopt an approach that is inspired by (Lu et al., 2008; Lu, 2014), which learns a latent joint representation for words and semantics in the form of hybrid trees where word-semantics correspondence information is captured. Specifically, we introduce a text-math tree representation that jointly encodes both text and the math expression tree. 5328 Mike picked 7 apples. Nancy picked 3 apples and Keith picked 6 apples at the farm. In total, how many apples were picked? A DD(+) (wAwBw) {Mike picked} A {. Nancy picked} B {apples were"
D19-1536,P18-2107,1,0.899564,"ould precisely contain the exact information associated with the text and its corresponding math expression and nothing else. We will defer the discussion on how to exactly construct such joint representations until Sec. 2.3. The training corpus provides both the problem text x and its math expression, which we represent with an expression tree y. The joint representation t is not available in the training data, which we model as a latent variable. The conditional random fields (CRF) (Lafferty et al., 2001) has been successfully applied to many tasks in the NLP community (Lample et al., 2016; Zou and Lu, 2018, 2019a). In this work, we also apply CRF to model the conditional probability of the latent variable t and output expression y, conditioned on the input x. The objective is defined as follows: PΛ,Θ (y|x) = X PΛ,Θ (y, t|x) t∈T (x,y) P =P t∈T (x,y) e[Λ·Φ(x,y,t)+GΘ (x,y,t)] [Λ·Φ(x,y 0 ,t0 )+GΘ (x,y 0 ,t0 )] y 0 ,t0 ∈T (x,y 0 ) e (1) where Φ(x, y, t) returns a list of discrete features defined over the tuple (x, y, t), Λ is the feature weight vector, GΘ is a neural scoring function parameterized by Θ and T (x, y) is a set of possible joint representations (i.e., text-math trees) for the pair (x,"
D19-1536,Q18-1012,0,0.151856,"e as a tree structure, where minimal manual efforts are involved in the process. Empirical results on benchmark datasets demonstrate the efficacy of our approach. 1 + + 7 3 6 Answerioniion 16 Problem 2 3 times one of the numbers is 11 less than 5 times the other. Expressionionio (3 × X1 ) = (5 × X2 ) − 11 = × 3 − × X1 5 11 X2 Introduction Designing computer algorithms that can automatically solve math word problems is a challenge for the AI research community (Bobrow, 1964). Two representative tasks have been proposed and studied recently – solving arithmetic word problems (Wang et al., 2017; Roy and Roth, 2018; Zou and Lu, 2019b) and equation parsing (Roy et al., 2016), as illustrated in Figure 1. The former task focuses on mapping the input paragraph (which may involve multiple sentences) into a target math expression, from which an answer can be calculated. The latter task focuses on mapping a description (usually a single sentence) into a math equation that typically involves one or more unknowns. As we can observe from Figure 1, in both cases, the output can be represented as a tree structure. Earlier approaches to solving arithmetic word problems focused on rule-based methods where hand-crafte"
D19-1536,N19-1217,1,0.856749,"e, where minimal manual efforts are involved in the process. Empirical results on benchmark datasets demonstrate the efficacy of our approach. 1 + + 7 3 6 Answerioniion 16 Problem 2 3 times one of the numbers is 11 less than 5 times the other. Expressionionio (3 × X1 ) = (5 × X2 ) − 11 = × 3 − × X1 5 11 X2 Introduction Designing computer algorithms that can automatically solve math word problems is a challenge for the AI research community (Bobrow, 1964). Two representative tasks have been proposed and studied recently – solving arithmetic word problems (Wang et al., 2017; Roy and Roth, 2018; Zou and Lu, 2019b) and equation parsing (Roy et al., 2016), as illustrated in Figure 1. The former task focuses on mapping the input paragraph (which may involve multiple sentences) into a target math expression, from which an answer can be calculated. The latter task focuses on mapping a description (usually a single sentence) into a math equation that typically involves one or more unknowns. As we can observe from Figure 1, in both cases, the output can be represented as a tree structure. Earlier approaches to solving arithmetic word problems focused on rule-based methods where hand-crafted rules have been"
D19-1536,D16-1117,0,0.177309,"nd Parsing Text into Math Expressions Yanyan Zou and Wei Lu StatNLP Research Group Singapore University of Technology and Design yanyan zou@mymail.sutd.edu.sg, luwei@sutd.edu.sg Problem 1 Mike picked 7 apples. Nancy picked 3 apples and Keith picked 6 apples at the farm. In total, how many apples were picked? Expressionion (7 + (3 + 6)) Abstract We propose Text2Math, a model for semantically parsing text into math expressions. The model can be used to solve different math related problems including arithmetic word problems (Roy and Roth, 2017; Liang et al., 2018) and equation parsing problems (Roy et al., 2016). Unlike previous approaches, we tackle the problem from an end-to-end structured prediction perspective where our algorithm aims to predict the complete math expression at once as a tree structure, where minimal manual efforts are involved in the process. Empirical results on benchmark datasets demonstrate the efficacy of our approach. 1 + + 7 3 6 Answerioniion 16 Problem 2 3 times one of the numbers is 11 less than 5 times the other. Expressionionio (3 × X1 ) = (5 × X2 ) − 11 = × 3 − × X1 5 11 X2 Introduction Designing computer algorithms that can automatically solve math word problems is a"
D19-1536,Q15-1001,0,0.0278721,"Missing"
D19-1536,P19-1517,1,0.861567,"e, where minimal manual efforts are involved in the process. Empirical results on benchmark datasets demonstrate the efficacy of our approach. 1 + + 7 3 6 Answerioniion 16 Problem 2 3 times one of the numbers is 11 less than 5 times the other. Expressionionio (3 × X1 ) = (5 × X2 ) − 11 = × 3 − × X1 5 11 X2 Introduction Designing computer algorithms that can automatically solve math word problems is a challenge for the AI research community (Bobrow, 1964). Two representative tasks have been proposed and studied recently – solving arithmetic word problems (Wang et al., 2017; Roy and Roth, 2018; Zou and Lu, 2019b) and equation parsing (Roy et al., 2016), as illustrated in Figure 1. The former task focuses on mapping the input paragraph (which may involve multiple sentences) into a target math expression, from which an answer can be calculated. The latter task focuses on mapping a description (usually a single sentence) into a math equation that typically involves one or more unknowns. As we can observe from Figure 1, in both cases, the output can be represented as a tree structure. Earlier approaches to solving arithmetic word problems focused on rule-based methods where hand-crafted rules have been"
D19-1536,D15-1171,0,0.0468203,"Missing"
D19-1536,D15-1135,0,0.248394,"decisions in the expression1 construction process. However, these models do not focus on predicting the target tree as a complete structure at once, but locally trained classifiers are often used and local decisions are then combined. Such local classifiers often make predictions on the choice of the underlying operator between two operands (e.g., numbers) appearing in the text in a particular order. As a result, special treatments of the non-commutative operators such as subtraction (−) and division (÷) are often involved, where the introduction of inverse operators is typically required2 . Shi et al. (2015) tackled the problem from a structured prediction perspective, where a semantic parsing algorithm us1 In this work, we use the term expression to refer to a math expression (for arithmetic word problem) or an equation (for equation parsing). 2 For example, the inverse operator −i applied to two operands a followed by b is used to denote b − a. 5327 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5327–5337, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computati"
D19-1536,P18-1034,0,0.0125419,"mantic parsing (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2018; Zou and Lu, 2018), which aims to map sentences into logic forms, including CCGbased lambda calculus expressions (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013b; Dong and Lapata, 2016), FunQL (Kate et al., 2005; Wong and Mooney, 2006; Jones et al., 2012), lambda-DCS (Liang et al., 2011; Berant et al., 2013; Jia and Liang, 2016), graph queries (Harris et al., 2013; Holzschuher and Peinl, 2013) and SQL (Yin et al., 2015; Sun et al., 2018). In this work, we adopt a text-math semantic representation encoding words and the expression tree. 5 Conclusion In this work, we propose a unified structured prediction approach, Text2Math, to solving both arithmetic word problems and equation parsing tasks. We leverage a novel joint representation to automatically learn the correspondence between words and math expressions which reflects semantic closeness. Different from many existing models, Text2Math is agnostic of the semantics of operands and learns to map from text to math expressions in an end-to-end manner based on a data-driven app"
D19-1536,D18-1132,0,0.20727,"Missing"
D19-1536,D17-1088,0,0.522026,"h expression at once as a tree structure, where minimal manual efforts are involved in the process. Empirical results on benchmark datasets demonstrate the efficacy of our approach. 1 + + 7 3 6 Answerioniion 16 Problem 2 3 times one of the numbers is 11 less than 5 times the other. Expressionionio (3 × X1 ) = (5 × X2 ) − 11 = × 3 − × X1 5 11 X2 Introduction Designing computer algorithms that can automatically solve math word problems is a challenge for the AI research community (Bobrow, 1964). Two representative tasks have been proposed and studied recently – solving arithmetic word problems (Wang et al., 2017; Roy and Roth, 2018; Zou and Lu, 2019b) and equation parsing (Roy et al., 2016), as illustrated in Figure 1. The former task focuses on mapping the input paragraph (which may involve multiple sentences) into a target math expression, from which an answer can be calculated. The latter task focuses on mapping a description (usually a single sentence) into a math equation that typically involves one or more unknowns. As we can observe from Figure 1, in both cases, the output can be represented as a tree structure. Earlier approaches to solving arithmetic word problems focused on rule-based metho"
D19-1536,N06-1056,0,0.375806,"ile C ON with arity 0 is supposed to be a leaf node. The two math expressions in Figure 1 can be equivalently represented by expression trees consisting of such nodes, as illustrated in Figure 2. 2.2 Latent Text-Math Tree With the specifically designed expression trees for representing the math expressions, we will now be able to design a model for parsing the text into the expression tree. This is essentially a semantic parsing task. One of the key assumptions made by the various semantic parsing algorithms is the intermediate joint representation used for connecting the words and semantics (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Lu et al., 2008; Artzi and Zettlemoyer, 2013b). In this work, we adopt an approach that is inspired by (Lu et al., 2008; Lu, 2014), which learns a latent joint representation for words and semantics in the form of hybrid trees where word-semantics correspondence information is captured. Specifically, we introduce a text-math tree representation that jointly encodes both text and the math expression tree. 5328 Mike picked 7 apples. Nancy picked 3 apples and Keith picked 6 apples at the farm. In total, how many apples were picked? A DD(+) (wAwBw) {Mike picked} A"
D19-1550,W17-1106,0,0.0207737,"us Statistics of Main Dataset contain 2,350 English tweets and 7,105 Spanish tweets, with target and targeted sentiment annotated. See Table 1 for corpus statistics. Evaluation Metrics Following the previous works, we report the precision (P.), recall (R.) and F1 scores for target recognition and targeted sentiment. Note that a correct target prediction requires the boundary of the target to be correct, and a correct targeted sentiment prediction requires both target boundary and sentiment polarity to be correct. Hyperparameters We adopt pretrained embeddings from Pennington et al. (2014) and Cieliebak et al. (2017) for English data and Spanish data respectively. We use a 2-layer LSTM (for both directions) with a hidden dimension of 500 and 6003 for English data and Spanish data respectively. The dimension of the attention weight U is 300. As for optimization, we use the Adam (Kingma and Ba, 2014) optimizer to optimize the model with batch size 1 and dropout rate 0.5. All the neural weights are initialized by Xavier (Glorot and Bengio, 2010). Training and Implementation We train our model for a maximal of 6 epochs. We select the best model parameters based on the best F1 score on the development data aft"
D19-1550,P14-2009,0,0.33935,". Boundaries for the sentiment scope are highlighted in dashed boxes. Introduction Targeted sentiment analysis (TSA) is an important task useful for public opinion mining (Pang and Lee, 2008; Liu, 2010; Ortigosa et al., 2014; Smailovi´c et al., 2013; Li and Wu, 2010). The task focuses on predicting the sentiment information towards a specific target phrase, which is usually a named entity, in a given input sentence. Currently, TSA in the literature may refer to either of the two possible tasks under two different setups: 1) predicting the sentiment polarity for a given specific target phrase (Dong et al., 2014; Wang et al., 2016; Zhang et al., 2016; Xue and Li, 2018); 2) jointly predicting the targets together with the sentiment polarity assigned to each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018). In this paper, we focus on the latter setup which was originally proposed by Mitchell 1 We release our code at http://www.statnlp. org/research/st. et al. (2013). Figure 1 presents an example sentence containing three targets. Each target is associated with a sentiment, where we use + for denoting positive polarity, 0 for neutral and − for negative. Existing resea"
D19-1550,D18-1380,0,0.0469804,"ample, in Figure 1, their model will not be able to capture the interaction between the target word “OZ” in the first sentiment span and the keyword “amazing” due to the assumptions made on the explicit structures in the output space. One idea to resolve this issue is to design an alternative mechanism to capture such useful structural information that resides in the input space. On the other hand, recent literature shows that feature learning mechanisms such as self-attention have been successful for the task of sentiment prediction when targets are given (Wang and Lu, 2018; He et al., 2018; Fan et al., 2018) (i.e., under the first setup mentioned above). Such approaches essentially attempt to learn rich implicit structural information in the input space that captures the interactions between a given target and all other word tokens within the sentence. Such implicit structures are then used to generate sentiment summary representation towards the given target, leading to the performance boost. However, to date capturing rich implicit structures in the joint prediction task that we focus on (i.e., the second setup) remains largely unexplored. Unlike the first setup, in our setup the targets are no"
D19-1550,C18-1096,0,0.0368463,"e another. For example, in Figure 1, their model will not be able to capture the interaction between the target word “OZ” in the first sentiment span and the keyword “amazing” due to the assumptions made on the explicit structures in the output space. One idea to resolve this issue is to design an alternative mechanism to capture such useful structural information that resides in the input space. On the other hand, recent literature shows that feature learning mechanisms such as self-attention have been successful for the task of sentiment prediction when targets are given (Wang and Lu, 2018; He et al., 2018; Fan et al., 2018) (i.e., under the first setup mentioned above). Such approaches essentially attempt to learn rich implicit structural information in the input space that captures the interactions between a given target and all other word tokens within the sentence. Such implicit structures are then used to generate sentiment summary representation towards the given target, leading to the performance boost. However, to date capturing rich implicit structures in the joint prediction task that we focus on (i.e., the second setup) remains largely unexplored. Unlike the first setup, in our setup"
D19-1550,W04-3250,0,0.0125946,"pare our model EI with the work proposed by Zhang et al. (2015). The Pipeline model (based on CRF) as well as Joint and Collapse models (based on SSVM) in their work capture fixed explicit structures. Such two models rely on multilayer perceptron (MLP) to obtain the local context features for implicit structures. These two models do not put much effort to capture better explicit structures and implicit structures. Our model EI (and even EI-) outperforms these two models significantly. We also compare our work with mod5 We have conducted significance test using the bootstrap resampling method (Koehn, 2004). els in Ma et al. (2018), which also capture fixed explicit structures. Such models leverage different GRUs (single-layer or multi-layer) and different input features (word embeddings and character representations) to learn better contextual features. Their best result by HMBi-GRU is obtained with multi-layer GRU with word embeddings and character embeddings. As we can see, our model EI outperforms HMBi-GRU under all evaluation metrics. On the English data, EI obtains 6.50 higher F1 score and 2.50 higher F1 score on target recognition and targeted sentiment respectively. On Spanish, EI obtain"
D19-1550,N16-1030,0,0.107223,"lustrated in Figure 4, we calculate ak , the output of self-attention at position k: ak = n X αk,j ej j=1 αk,j = softmax(βk,j ) j + 0 − + 0 − ft (hk ) fs (hk ) gs (ak ) hk = BiLST M (e1 , e2 , · · · , en ) ak = Self AT T (e1 , e2 , · · · , en ) ek = [wk ; ck ] Figure 4: Neural Architecture representation at position k and contextual representation at position j. In addition, W and b as well as the attention matrix U are the weights to be learned. Such a vector ak encodes the implicit structures between the word xk and each word in the remaining sentence. Motivated by the character embeddings (Lample et al., 2016) which are generated based on hidden states at two ends of a subsequence, we encode such implicit structures for a target similarly. For any target starting at the position k1 and ending at the position k2 , we could use ak1 and ak2 at two ends to represent the implicit structures of such a target. We encode such information on the 1 and Ek2 Ak2 which appear at the edges Bkp1 Ek,p ,p p beginning and the end of a target phrase respectively with sentiment polarity p. To do so, we assign the scores calculated from the self-attention to such two edges: 1 φx (Bkp1 Ek,p ) = gs (ak1 )p 2 φx (Ek,p"
D19-1550,E17-2091,0,0.25293,"fforts on two types of TSA tasks mentioned in the introduction. Note that TSA is related to aspect sentiment analysis which is to determine the sentiment polarity given a target and an aspect describing a property of related topics. Predicting sentiment for a given target Such a task is typically solved by leveraging sentence structural information, such as syntactic trees (Dong et al., 2014), dependency trees (Wang et al., 2016) as well as surrounding context based on LSTM (Tang et al., 2016a), GRU (Zhang et al., 2016) or CNN (Xue and Li, 2018). Another line of works leverage self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016b) to encode rich global context information. Wang and Lu (2018) adopted the segmental attention (Kong et al., 2016) to model the important text segments to compute the targeted sentiment. Wang et al. (2018) studied the issue that the different combinations of target and aspect may result in different sentiment polarity. They proposed a model to distinguish such different combinations based on memory networks to produce the representation for aspect sentiment classification. Jointly predicting targets and their associated sentiment Such a joint task is usu"
D19-1550,D18-1504,0,0.736432,"i´c et al., 2013; Li and Wu, 2010). The task focuses on predicting the sentiment information towards a specific target phrase, which is usually a named entity, in a given input sentence. Currently, TSA in the literature may refer to either of the two possible tasks under two different setups: 1) predicting the sentiment polarity for a given specific target phrase (Dong et al., 2014; Wang et al., 2016; Zhang et al., 2016; Xue and Li, 2018); 2) jointly predicting the targets together with the sentiment polarity assigned to each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018). In this paper, we focus on the latter setup which was originally proposed by Mitchell 1 We release our code at http://www.statnlp. org/research/st. et al. (2013). Figure 1 presents an example sentence containing three targets. Each target is associated with a sentiment, where we use + for denoting positive polarity, 0 for neutral and − for negative. Existing research efforts mostly regard this task as a sequence labeling problem by assigning a tag to each word token, where the tags are typically designed in a way that capture both the target boundary as well as the targeted sentiment polarit"
D19-1550,D13-1171,0,0.262865,"Missing"
D19-1550,D14-1162,0,0.0999707,"ts per sentence Table 1: Corpus Statistics of Main Dataset contain 2,350 English tweets and 7,105 Spanish tweets, with target and targeted sentiment annotated. See Table 1 for corpus statistics. Evaluation Metrics Following the previous works, we report the precision (P.), recall (R.) and F1 scores for target recognition and targeted sentiment. Note that a correct target prediction requires the boundary of the target to be correct, and a correct targeted sentiment prediction requires both target boundary and sentiment polarity to be correct. Hyperparameters We adopt pretrained embeddings from Pennington et al. (2014) and Cieliebak et al. (2017) for English data and Spanish data respectively. We use a 2-layer LSTM (for both directions) with a hidden dimension of 500 and 6003 for English data and Spanish data respectively. The dimension of the attention weight U is 300. As for optimization, we use the Adam (Kingma and Ba, 2014) optimizer to optimize the model with batch size 1 and dropout rate 0.5. All the neural weights are initialized by Xavier (Glorot and Bengio, 2010). Training and Implementation We train our model for a maximal of 6 epochs. We select the best model parameters based on the best F1 score"
D19-1550,S16-1002,0,0.219547,"Missing"
D19-1550,C16-1311,0,0.0595248,"Missing"
D19-1550,D16-1021,0,0.158345,"ssian. Model SS (Li and Lu, 2017) SS + emb (Li and Lu, 2017) SA-CRF EI5 EI Related Work We briefly survey the research efforts on two types of TSA tasks mentioned in the introduction. Note that TSA is related to aspect sentiment analysis which is to determine the sentiment polarity given a target and an aspect describing a property of related topics. Predicting sentiment for a given target Such a task is typically solved by leveraging sentence structural information, such as syntactic trees (Dong et al., 2014), dependency trees (Wang et al., 2016) as well as surrounding context based on LSTM (Tang et al., 2016a), GRU (Zhang et al., 2016) or CNN (Xue and Li, 2018). Another line of works leverage self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016b) to encode rich global context information. Wang and Lu (2018) adopted the segmental attention (Kong et al., 2016) to model the important text segments to compute the targeted sentiment. Wang et al. (2018) studied the issue that the different combinations of target and aspect may result in different sentiment polarity. They proposed a model to distinguish such different combinations based on memory networks to produce the representat"
D19-1550,P18-1088,0,0.204368,"timent for a given target Such a task is typically solved by leveraging sentence structural information, such as syntactic trees (Dong et al., 2014), dependency trees (Wang et al., 2016) as well as surrounding context based on LSTM (Tang et al., 2016a), GRU (Zhang et al., 2016) or CNN (Xue and Li, 2018). Another line of works leverage self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016b) to encode rich global context information. Wang and Lu (2018) adopted the segmental attention (Kong et al., 2016) to model the important text segments to compute the targeted sentiment. Wang et al. (2018) studied the issue that the different combinations of target and aspect may result in different sentiment polarity. They proposed a model to distinguish such different combinations based on memory networks to produce the representation for aspect sentiment classification. Jointly predicting targets and their associated sentiment Such a joint task is usually regarded as sequence labeling problem. Mitchell et al. (2013) introduced the task of open domain targeted sentiment analysis. They proposed several models based on CRF such as the pipeline model, the collapsed model as well as the joint mod"
D19-1550,D16-1059,0,0.14849,"e sentiment scope are highlighted in dashed boxes. Introduction Targeted sentiment analysis (TSA) is an important task useful for public opinion mining (Pang and Lee, 2008; Liu, 2010; Ortigosa et al., 2014; Smailovi´c et al., 2013; Li and Wu, 2010). The task focuses on predicting the sentiment information towards a specific target phrase, which is usually a named entity, in a given input sentence. Currently, TSA in the literature may refer to either of the two possible tasks under two different setups: 1) predicting the sentiment polarity for a given specific target phrase (Dong et al., 2014; Wang et al., 2016; Zhang et al., 2016; Xue and Li, 2018); 2) jointly predicting the targets together with the sentiment polarity assigned to each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018). In this paper, we focus on the latter setup which was originally proposed by Mitchell 1 We release our code at http://www.statnlp. org/research/st. et al. (2013). Figure 1 presents an example sentence containing three targets. Each target is associated with a sentiment, where we use + for denoting positive polarity, 0 for neutral and − for negative. Existing research efforts mostly"
D19-1550,P18-1234,0,0.391123,"shed boxes. Introduction Targeted sentiment analysis (TSA) is an important task useful for public opinion mining (Pang and Lee, 2008; Liu, 2010; Ortigosa et al., 2014; Smailovi´c et al., 2013; Li and Wu, 2010). The task focuses on predicting the sentiment information towards a specific target phrase, which is usually a named entity, in a given input sentence. Currently, TSA in the literature may refer to either of the two possible tasks under two different setups: 1) predicting the sentiment polarity for a given specific target phrase (Dong et al., 2014; Wang et al., 2016; Zhang et al., 2016; Xue and Li, 2018); 2) jointly predicting the targets together with the sentiment polarity assigned to each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018). In this paper, we focus on the latter setup which was originally proposed by Mitchell 1 We release our code at http://www.statnlp. org/research/st. et al. (2013). Figure 1 presents an example sentence containing three targets. Each target is associated with a sentiment, where we use + for denoting positive polarity, 0 for neutral and − for negative. Existing research efforts mostly regard this task as a sequence labeling"
D19-1550,D15-1073,0,0.158217,"2010; Ortigosa et al., 2014; Smailovi´c et al., 2013; Li and Wu, 2010). The task focuses on predicting the sentiment information towards a specific target phrase, which is usually a named entity, in a given input sentence. Currently, TSA in the literature may refer to either of the two possible tasks under two different setups: 1) predicting the sentiment polarity for a given specific target phrase (Dong et al., 2014; Wang et al., 2016; Zhang et al., 2016; Xue and Li, 2018); 2) jointly predicting the targets together with the sentiment polarity assigned to each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018). In this paper, we focus on the latter setup which was originally proposed by Mitchell 1 We release our code at http://www.statnlp. org/research/st. et al. (2013). Figure 1 presents an example sentence containing three targets. Each target is associated with a sentiment, where we use + for denoting positive polarity, 0 for neutral and − for negative. Existing research efforts mostly regard this task as a sequence labeling problem by assigning a tag to each word token, where the tags are typically designed in a way that capture both the target boundary as wel"
D19-1644,A00-1041,0,0.241038,"Missing"
D19-1644,W07-1009,0,0.0502739,"structured model, our work tries to decompose the task into two sub-tasks to resolve the ambiguity issue. This task is also related to joint entity and relation extraction (Kate and Mooney, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014) where the discontiguous entities can be viewed as relation links between segments. The major difference is that discontiguous entities require explicitly modeling overlapping entities and linking multiple segments. 3 Model The task of extracting overlapping entities has long been studied (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006; McDonald et al., 2005; Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017). As neural models (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016) are proven effective for NER, there have been several neural systems recently proposed to handle entities of overlapping structures (Ju et al., 2018; Katiyar and Cardie, 2018; Wang et al., 2018; Sohrab and Miwa, 2018; Wang and Lu, 2018; Strakov´a et al., 2019; Lin et al., 2019; Fisher and Vlachos, 2019). Our system is based on the model of neural segmental hypergraphs (Wang and Lu, 2018) which encodes al"
D19-1644,D13-1057,0,0.0197915,"ce of words and the latter also overlaps with another entity “blood in stomach”. Introduction Named entity recognition (NER) aims at identifying shallow semantic elements in text and has been a crucial step towards natural language understanding (Tjong Kim Sang and De Meulder, 2003). Extracted entities can facilitate various downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Lu and Roth, 2012; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). The underlying assumptions behind most NER systems are that an entity should contain a contiguous sequence of words and should not overlap with each other. However, such assumptions do not always hold in practice. First, entities or mentions1 with overlapping structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). Second, entities can be discontiguous, especially in clinical texts (Pradhan et al., 2014b). For example, Figure 3 shows three entities where two of them are discontiguous (“laceration . . . esophagus” and “stomach . . . lac”), and"
D19-1644,W16-2922,0,0.0233175,"Missing"
D19-1644,Q16-1026,0,0.0354441,"tion (Kate and Mooney, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014) where the discontiguous entities can be viewed as relation links between segments. The major difference is that discontiguous entities require explicitly modeling overlapping entities and linking multiple segments. 3 Model The task of extracting overlapping entities has long been studied (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006; McDonald et al., 2005; Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017). As neural models (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016) are proven effective for NER, there have been several neural systems recently proposed to handle entities of overlapping structures (Ju et al., 2018; Katiyar and Cardie, 2018; Wang et al., 2018; Sohrab and Miwa, 2018; Wang and Lu, 2018; Strakov´a et al., 2019; Lin et al., 2019; Fisher and Vlachos, 2019). Our system is based on the model of neural segmental hypergraphs (Wang and Lu, 2018) which encodes all the Our goal is to extract a set of entities that may have overlapping and discontiguous structures given a natural language sentence. We use x = x1 . . . x|x |to denote"
D19-1644,N16-1030,0,0.087895,"lated to joint entity and relation extraction (Kate and Mooney, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014) where the discontiguous entities can be viewed as relation links between segments. The major difference is that discontiguous entities require explicitly modeling overlapping entities and linking multiple segments. 3 Model The task of extracting overlapping entities has long been studied (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006; McDonald et al., 2005; Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017). As neural models (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016) are proven effective for NER, there have been several neural systems recently proposed to handle entities of overlapping structures (Ju et al., 2018; Katiyar and Cardie, 2018; Wang et al., 2018; Sohrab and Miwa, 2018; Wang and Lu, 2018; Strakov´a et al., 2019; Lin et al., 2019; Fisher and Vlachos, 2019). Our system is based on the model of neural segmental hypergraphs (Wang and Lu, 2018) which encodes all the Our goal is to extract a set of entities that may have overlapping and discontiguous structures given a natural language s"
D19-1644,P19-1511,0,0.184839,"Missing"
D19-1644,D17-1005,0,0.018341,"s that previous methods used. 1 Figure 1: Entities are highlighted with colored underlines. “laceration ... esophagus” and “stomach ... lac” contain discontiguous sequence of words and the latter also overlaps with another entity “blood in stomach”. Introduction Named entity recognition (NER) aims at identifying shallow semantic elements in text and has been a crucial step towards natural language understanding (Tjong Kim Sang and De Meulder, 2003). Extracted entities can facilitate various downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Lu and Roth, 2012; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). The underlying assumptions behind most NER systems are that an entity should contain a contiguous sequence of words and should not overlap with each other. However, such assumptions do not always hold in practice. First, entities or mentions1 with overlapping structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). Second, entities can be discontiguous, especially in clinica"
D19-1644,P12-1088,1,0.725354,"ed with colored underlines. “laceration ... esophagus” and “stomach ... lac” contain discontiguous sequence of words and the latter also overlaps with another entity “blood in stomach”. Introduction Named entity recognition (NER) aims at identifying shallow semantic elements in text and has been a crucial step towards natural language understanding (Tjong Kim Sang and De Meulder, 2003). Extracted entities can facilitate various downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Lu and Roth, 2012; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). The underlying assumptions behind most NER systems are that an entity should contain a contiguous sequence of words and should not overlap with each other. However, such assumptions do not always hold in practice. First, entities or mentions1 with overlapping structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). Second, entities can be discontiguous, especially in clinical texts (Pradhan et al., 2014b). For example, Figure 3 shows thr"
D19-1644,D15-1102,1,0.902343,"the task into two sub-tasks to resolve the ambiguity issue. This task is also related to joint entity and relation extraction (Kate and Mooney, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014) where the discontiguous entities can be viewed as relation links between segments. The major difference is that discontiguous entities require explicitly modeling overlapping entities and linking multiple segments. 3 Model The task of extracting overlapping entities has long been studied (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006; McDonald et al., 2005; Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017). As neural models (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016) are proven effective for NER, there have been several neural systems recently proposed to handle entities of overlapping structures (Ju et al., 2018; Katiyar and Cardie, 2018; Wang et al., 2018; Sohrab and Miwa, 2018; Wang and Lu, 2018; Strakov´a et al., 2019; Lin et al., 2019; Fisher and Vlachos, 2019). Our system is based on the model of neural segmental hypergraphs (Wang and Lu, 2018) which encodes all the Our goal is to extract a set of entitie"
D19-1644,P16-1101,0,0.0514532,"010; Li and Ji, 2014; Miwa and Sasaki, 2014) where the discontiguous entities can be viewed as relation links between segments. The major difference is that discontiguous entities require explicitly modeling overlapping entities and linking multiple segments. 3 Model The task of extracting overlapping entities has long been studied (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006; McDonald et al., 2005; Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017). As neural models (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016) are proven effective for NER, there have been several neural systems recently proposed to handle entities of overlapping structures (Ju et al., 2018; Katiyar and Cardie, 2018; Wang et al., 2018; Sohrab and Miwa, 2018; Wang and Lu, 2018; Strakov´a et al., 2019; Lin et al., 2019; Fisher and Vlachos, 2019). Our system is based on the model of neural segmental hypergraphs (Wang and Lu, 2018) which encodes all the Our goal is to extract a set of entities that may have overlapping and discontiguous structures given a natural language sentence. We use x = x1 . . . x|x |to denote a sentence, and use"
D19-1644,H05-1124,0,0.359005,"s in one stage using a structured model, our work tries to decompose the task into two sub-tasks to resolve the ambiguity issue. This task is also related to joint entity and relation extraction (Kate and Mooney, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014) where the discontiguous entities can be viewed as relation links between segments. The major difference is that discontiguous entities require explicitly modeling overlapping entities and linking multiple segments. 3 Model The task of extracting overlapping entities has long been studied (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006; McDonald et al., 2005; Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017). As neural models (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016) are proven effective for NER, there have been several neural systems recently proposed to handle entities of overlapping structures (Ju et al., 2018; Katiyar and Cardie, 2018; Wang et al., 2018; Sohrab and Miwa, 2018; Wang and Lu, 2018; Strakov´a et al., 2019; Lin et al., 2019; Fisher and Vlachos, 2019). Our system is based on the model of neural segmental hypergraphs (Wang and Lu, 201"
D19-1644,N18-1131,0,0.024911,"that discontiguous entities require explicitly modeling overlapping entities and linking multiple segments. 3 Model The task of extracting overlapping entities has long been studied (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006; McDonald et al., 2005; Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017). As neural models (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016) are proven effective for NER, there have been several neural systems recently proposed to handle entities of overlapping structures (Ju et al., 2018; Katiyar and Cardie, 2018; Wang et al., 2018; Sohrab and Miwa, 2018; Wang and Lu, 2018; Strakov´a et al., 2019; Lin et al., 2019; Fisher and Vlachos, 2019). Our system is based on the model of neural segmental hypergraphs (Wang and Lu, 2018) which encodes all the Our goal is to extract a set of entities that may have overlapping and discontiguous structures given a natural language sentence. We use x = x1 . . . x|x |to denote a sentence, and use y = {[bi:j . . . bm:n ]k } to denote a set of discontiguous entities where each entity of type k contains a list of spans, e.g., bi:j and bm:n , with"
D19-1644,P09-1113,0,0.0290788,"of external features that previous methods used. 1 Figure 1: Entities are highlighted with colored underlines. “laceration ... esophagus” and “stomach ... lac” contain discontiguous sequence of words and the latter also overlaps with another entity “blood in stomach”. Introduction Named entity recognition (NER) aims at identifying shallow semantic elements in text and has been a crucial step towards natural language understanding (Tjong Kim Sang and De Meulder, 2003). Extracted entities can facilitate various downstream tasks like question answering (Abney et al., 2000), relation extraction (Mintz et al., 2009; Liu et al., 2017), event extraction (Riedel and McCallum, 2011; Lu and Roth, 2012; Li et al., 2013), and coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Chang et al., 2013). The underlying assumptions behind most NER systems are that an entity should contain a contiguous sequence of words and should not overlap with each other. However, such assumptions do not always hold in practice. First, entities or mentions1 with overlapping structures frequently exist in news (Doddington et al., 2004) and biomedical documents (Kim et al., 2003). Second, entities can be discontiguous, es"
D19-1644,W10-2924,0,0.0399117,"d Lu (2016a) proposed a hypergraph-based representation to reduce the level of ambiguity. Essentially, these systems trade expressiveness for efficiency: they inexactly encoded the whole space of discontiguous entities with ambiguity for training, and then relied on some heuristics to handle the ambiguity during decoding. 3 Considering it is intrinsically hard to exactly identify discontiguous entities in one stage using a structured model, our work tries to decompose the task into two sub-tasks to resolve the ambiguity issue. This task is also related to joint entity and relation extraction (Kate and Mooney, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014) where the discontiguous entities can be viewed as relation links between segments. The major difference is that discontiguous entities require explicitly modeling overlapping entities and linking multiple segments. 3 Model The task of extracting overlapping entities has long been studied (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006; McDonald et al., 2005; Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017). As neural models (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma"
D19-1644,D14-1200,0,0.0288461,"representation to reduce the level of ambiguity. Essentially, these systems trade expressiveness for efficiency: they inexactly encoded the whole space of discontiguous entities with ambiguity for training, and then relied on some heuristics to handle the ambiguity during decoding. 3 Considering it is intrinsically hard to exactly identify discontiguous entities in one stage using a structured model, our work tries to decompose the task into two sub-tasks to resolve the ambiguity issue. This task is also related to joint entity and relation extraction (Kate and Mooney, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014) where the discontiguous entities can be viewed as relation links between segments. The major difference is that discontiguous entities require explicitly modeling overlapping entities and linking multiple segments. 3 Model The task of extracting overlapping entities has long been studied (Zhang et al., 2004; Zhou et al., 2004; Zhou, 2006; McDonald et al., 2005; Alex et al., 2007; Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017). As neural models (Collobert et al., 2011; Lample et al., 2016; Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016) are proven effective for"
D19-1644,S15-2052,0,0.0530673,"Missing"
D19-1644,S14-2142,0,0.0769058,"Missing"
D19-3041,Q15-1016,0,0.0295171,"ve tens of hyper-parameters and various tricks, and some of which exert large impacts on final performance. Sometimes it is unlikely to report all details and their effects in research paper. This may lead to a huge gap between research papers and code implementations. To solve the above problem, some works are proposed to implement a class of models in a framework. This type of work includes OpenNMT (Klein et al., 2017), fairseq (Ott et al., 2019) for neural machine translation; glyph (Zhang and LeCun, 2017) for classification; NCRF++ (Yang and Zhang, 2018) for sequence labeling; Hyperwords (Levy et al., 2015), ngram2vec (Zhao et al., 2017) for word embedding, to name a few. Recently, we witness many influential pretraining works such as GPT, ULMFiT, and BERT. We think it could be useful to develop a framework to facilitate reproducing and refining those models. UER provides the flexibility of building pre-training models of different properties. 3 3.3 Target (objective) Using suitable target is the key to the success of pre-training. Many papers in this field propose their targets and show their advantages over other ones. UER consists of a range of targets. Users can choose one of them, or use mu"
D19-3041,N19-4009,0,0.0326817,"LP tasks. We implement Transformer module and integrate it into UER. With Transformer module, we can implement models such as GPT and BERT easily. Many NLP models have tens of hyper-parameters and various tricks, and some of which exert large impacts on final performance. Sometimes it is unlikely to report all details and their effects in research paper. This may lead to a huge gap between research papers and code implementations. To solve the above problem, some works are proposed to implement a class of models in a framework. This type of work includes OpenNMT (Klein et al., 2017), fairseq (Ott et al., 2019) for neural machine translation; glyph (Zhang and LeCun, 2017) for classification; NCRF++ (Yang and Zhang, 2018) for sequence labeling; Hyperwords (Levy et al., 2015), ngram2vec (Zhao et al., 2017) for word embedding, to name a few. Recently, we witness many influential pretraining works such as GPT, ULMFiT, and BERT. We think it could be useful to develop a framework to facilitate reproducing and refining those models. UER provides the flexibility of building pre-training models of different properties. 3 3.3 Target (objective) Using suitable target is the key to the success of pre-training."
D19-3041,N18-1202,0,0.0561689,"d encapsulated with rich modules. By assembling modules on demand, users can either reproduce a state-of-the-art pre-training model or develop a pre-training model that remains unexplored. With UER, we have built a model zoo, which contains pre-trained models based on different corpora, encoders, and targets (objectives). With proper pre-trained models, we could achieve new state-of-the-art results on a range of downstream datasets. 1 Table 1 lists 8 popular pre-training models and their main differences (Kiros et al., 2015; Logeswaran and Lee, 2018; McCann et al., 2017; Conneau et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). In additional to encoder, target, and fine-tuning strategy, corpus is also listed in Table 1 as an important factor for pre-training models. There are many open-source implementations of pre-training models, such as Google BERT1 , ELMO from AllenAI2 , GPT and BERT from HuggingFace3 . However, these works usually focus on the designs of either one or a few pre-training models. Due to the diversity of the downstream tasks and the computational resources constraint, there does not exist a single pre-training model that works be"
D19-3041,D17-1070,0,0.416106,"is loosely coupled, and encapsulated with rich modules. By assembling modules on demand, users can either reproduce a state-of-the-art pre-training model or develop a pre-training model that remains unexplored. With UER, we have built a model zoo, which contains pre-trained models based on different corpora, encoders, and targets (objectives). With proper pre-trained models, we could achieve new state-of-the-art results on a range of downstream datasets. 1 Table 1 lists 8 popular pre-training models and their main differences (Kiros et al., 2015; Logeswaran and Lee, 2018; McCann et al., 2017; Conneau et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). In additional to encoder, target, and fine-tuning strategy, corpus is also listed in Table 1 as an important factor for pre-training models. There are many open-source implementations of pre-training models, such as Google BERT1 , ELMO from AllenAI2 , GPT and BERT from HuggingFace3 . However, these works usually focus on the designs of either one or a few pre-training models. Due to the diversity of the downstream tasks and the computational resources constraint, there does not exist a single pre-trainin"
D19-3041,P18-4013,0,0.0325348,"nt models such as GPT and BERT easily. Many NLP models have tens of hyper-parameters and various tricks, and some of which exert large impacts on final performance. Sometimes it is unlikely to report all details and their effects in research paper. This may lead to a huge gap between research papers and code implementations. To solve the above problem, some works are proposed to implement a class of models in a framework. This type of work includes OpenNMT (Klein et al., 2017), fairseq (Ott et al., 2019) for neural machine translation; glyph (Zhang and LeCun, 2017) for classification; NCRF++ (Yang and Zhang, 2018) for sequence labeling; Hyperwords (Levy et al., 2015), ngram2vec (Zhao et al., 2017) for word embedding, to name a few. Recently, we witness many influential pretraining works such as GPT, ULMFiT, and BERT. We think it could be useful to develop a framework to facilitate reproducing and refining those models. UER provides the flexibility of building pre-training models of different properties. 3 3.3 Target (objective) Using suitable target is the key to the success of pre-training. Many papers in this field propose their targets and show their advantages over other ones. UER consists of a ran"
D19-3041,D17-1023,1,0.861133,"various tricks, and some of which exert large impacts on final performance. Sometimes it is unlikely to report all details and their effects in research paper. This may lead to a huge gap between research papers and code implementations. To solve the above problem, some works are proposed to implement a class of models in a framework. This type of work includes OpenNMT (Klein et al., 2017), fairseq (Ott et al., 2019) for neural machine translation; glyph (Zhang and LeCun, 2017) for classification; NCRF++ (Yang and Zhang, 2018) for sequence labeling; Hyperwords (Levy et al., 2015), ngram2vec (Zhao et al., 2017) for word embedding, to name a few. Recently, we witness many influential pretraining works such as GPT, ULMFiT, and BERT. We think it could be useful to develop a framework to facilitate reproducing and refining those models. UER provides the flexibility of building pre-training models of different properties. 3 3.3 Target (objective) Using suitable target is the key to the success of pre-training. Many papers in this field propose their targets and show their advantages over other ones. UER consists of a range of targets. Users can choose one of them, or use multiple targets and give them di"
D19-3041,P16-2034,0,0.0318405,"esults in remarkable improvements on a range of downstream datasets (Devlin et al., 2018). Instead of training models on a specific task from scratch, pretraining models are firstly trained on generaldomain corpora, then followed by fine-tuning on downstream tasks. Thus far, a large number of works have been proposed for finding better pretraining models. Existing pre-training models mainly differ in the following three aspects: 1) Model encoder. Commonly-used encoders include RNN (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), AttentionNN (Bahdanau et al., 2014), and their combinations (Zhou et al., 2016). Recently, ∗ 1 https://github.com/google-research/bert https://github.com/allenai/bilm-tf 3 https://github.com/huggingface 2 Corresponding author. 241 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 241–246 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics Model Skip-thoughts Quick-thoughts CoVe Infersent ELMO ULMFiT GPT BERT Corpus Bookcorpus Bookcorpus+UMBCcorpus English-German Natural language inference 1billion benchmark Wikipedia Bookcorpus; 1billion benchmark Wikipedia+bookcorpus Encoder GRU GRU Bi-LSTM LSTM;GRU;CNN;"
D19-3041,D14-1181,0,0.0289578,"ng has been well recognized as an essential step for NLP tasks since it results in remarkable improvements on a range of downstream datasets (Devlin et al., 2018). Instead of training models on a specific task from scratch, pretraining models are firstly trained on generaldomain corpora, then followed by fine-tuning on downstream tasks. Thus far, a large number of works have been proposed for finding better pretraining models. Existing pre-training models mainly differ in the following three aspects: 1) Model encoder. Commonly-used encoders include RNN (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), AttentionNN (Bahdanau et al., 2014), and their combinations (Zhou et al., 2016). Recently, ∗ 1 https://github.com/google-research/bert https://github.com/allenai/bilm-tf 3 https://github.com/huggingface 2 Corresponding author. 241 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 241–246 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics Model Skip-thoughts Quick-thoughts CoVe Infersent ELMO ULMFiT GPT BERT Corpus Bookcorpus Bookcorpus+UMBCcorpus English-German Natural language inference 1billion benchmark Wikipedia Bookcorp"
N16-1085,W06-1655,0,0.0639778,"Missing"
N16-1085,J92-4003,0,0.196116,"ed nodes in Weak Semi-CRF are the Begin nodes. indexed words inside current segment, running from the start and from the end of the segment, (2) the word before and after current segment, and (3) the labels of previous segment and current segment. In weak semi-CRF we use the same feature set as semi-CRF, adjusting the features accordingly where segment-specific features (1) are defined only in the Begin-End edges, and transition features (3) are defined only in the End-Begin edges. For each model we then add the character prefixes and suffixes up to length 3 for each word (+a), Brown cluster (Brown et al., 1992) for current word (+b), and word shapes (+s). For Brown cluster features we used 100 clusters trained on the whole NUS SMS Corpus. The cluster information is then used directly as a feature. Word shapes can be considered a generic representation of words that retains only the “shape” information, such as whether it starts with capital letter or whether it contains digits. The Brown clusters and word shapes features are applied to each of the word features described in each model. 5 Experiments All models were built by us using Java, and were optimized with L-BFGS. Models are all tuned in the d"
N16-1085,P14-1016,0,0.0140209,"hen explored several graphical models, including a novel variant of the semi-Markov conditional random fields (semi-CRF) for the task of noun phrase chunking. We demonstrated through empirical evaluations on the new dataset that the new variant yielded similar accuracy but ran in significantly lower running time compared to the conventional semi-CRF. 1 Introduction Processing user-generated text data is getting more popular recently as a way to gather information, such as collecting facts about certain events (Ritter et al., 2015), gathering and identifying user profiles (Layton et al., 2010; Li et al., 2014; Spitters et al., 2015), or extracting information in open domain (Ritter et al., 2012; Mitchell et al., 2015). Most recent work focus on the texts generated through Twitter, which, due to the design of Twitter, contain a lot of announcement-like messages mostly intended for general public. In contrast, SMS was designed as a way to communicate short personal messages to a known person, and hence SMS messages tend to be more conversational and more informal compared to tweets. As conversational texts, SMS data often contains references to named entities such as people and locations relevant to"
N16-1085,D15-1102,1,0.732739,"the following objective function: L(T ) =   X X  wT f (e) − log Zw (x)− λ||w||2 (1) (x,y)∈T e∈E(x,y) where T is the training set, (x, y) is a training instance consisting of the sentence x and the label sequence y ∈ Y n for a label set Y, w is the feature weight vector, E(x, y) is the set of edges which defines the input x labeled with the label sequence y, f (e) is the feature vector of the edge e, Zw (x) is the normalization term which sums over all possible paths from the root to the leaf node, and λ is the regularization parameter. 3 Extension to directed hypergraphs is possible. See (Lu and Roth, 2015). The set of edges and features defined in each model affects the feature expectation and the normalization term. Computation of the normalization term, being the highest in time complexity, will determine the overall complexity of training the model. The set of edges and the normalization term in each model will be described in the following sections. 3.1 Linear CRF A linear-chain CRF, or linear CRF is a standard version of CRF which was introduced in Lafferty et al. (2001), where each word in the sentence is given a set of nodes representing the possible labels, and edges are present between"
N16-1085,D11-1141,0,0.150012,"Missing"
N19-1079,W09-1119,0,0.182651,"Most existing approaches to NER focused on a supervised setup, where fully annotated named entity information is assumed to be available during the training phase. However, in practice, obtaining high-quality annotations can be a very laborious and expensive process (Snow et al., 2008). One of the common issues with data annotations is there may be incomplete annotations. Figure 1 shows an example sentence with two named entities “John Lloyd Jones” and “BBC radio” of type PER (person) and ORG (organization), respectively. Following the standard BIOES tagging scheme (Ramshaw and Marcus, 1999; Ratinov and Roth, 2009), the corresponding gold label sequence is shown below the sentence. When the data annotations are incomplete, certain labels 2 Why should the O labels be assumed unavailable? This is because the annotators typically do not actively specify the O labels when working on annotations. If the annotator chooses not to annotate a word, it could either mean it is not part of any entity, or the word is actually part of an entity but the annotator neglected it in the annotation process (therefore we have incomplete annotations). However, we note that assigning the O label to a word would precisely indi"
N19-1079,D17-1035,0,0.0260153,"Missing"
N19-1079,W02-1001,0,0.743519,"2 3 Related Work Approach Given the input word sequence x, the NER task is to predict a label sequence y that encodes the NER information (e.g., in a form following the BIOES tagging scheme). Given a training set that consists of completely labeled data D, one can tackle this problem using a standard linear-chain conditional random field (CRF) (Lafferty et al., 2001) whose loss function is as follows:5 X L(w) = − log pw (y(i) |x(i) ) (1) Previous research efforts on partially annotated data are mostly based on the conditional random fields (CRF) (Lafferty et al., 2001), structured perceptron (Collins, 2002) and maxmargin (Tsochantaridis et al., 2005) (e.g. structural support vector machine) models. Bellare and McCallum (2007) proposed a missing label linear-chain CRF4 which is essentially a latent3 The task is to tag the BibTex records with different labels (i.e., “title”, “author”, “affiliation” and so on). 4 This model was also named as Partial CRF (Carlson et al., 2009) and EM Marginal CRF (Greenberg et al., 2018). i 5 In practice, we also have an L2 regularization term, which we exclude from the formula for brevity. 730 where (x(i) , y(i) ) is the i-th instance from D. Now, assume we have an"
N19-1079,D08-1027,0,0.186369,"Missing"
N19-1079,D18-1306,0,0.0173192,"refers to one possible complete label sequence, and the density of the color indicates probability (we excluded B and E tags for brevity). variable CRF (Quattoni et al., 2005) on citation parsing (McCallum et al., 2000). This model had also been used in part-of-speeching tagging and segmentation task with incomplete annotations (Tsuboi et al., 2008; Liu et al., 2014; Yang and Vozila, 2014). Yang et al. (2018) showed the effectiveness of such a model on Chinese NER with incomplete annotations due to the fact that they required a certain number of fully annotated data to perform joint training. Greenberg et al. (2018) applied this model on a biomedical NER task and achieved promising performance with incomplete annotations. However, in their assumption for the incomplete annotations, the O labels are still considered, which we believe is not realistic. Carlson et al. (2009) modified the structured perceptron algorithm and defined features only on the tokens with annotated labels in partially labeled sequences. Fernandes and Brefeld (2011) and Lou et al. (2012) proposed to use a largemargin learning framework similar to structured support vector machines with latent variables (Yu and Joachims, 2009). able ("
N19-1079,W02-2024,0,0.0599522,"Missing"
N19-1079,N16-1030,0,0.34866,"Missing"
N19-1079,W03-0419,0,0.164319,"Missing"
N19-1079,D14-1093,0,0.0216085,"aid on BBC radio O O O O PER PER PER PER ORG ORG ORG ORG Figure 3: Graphical illustrations on different assumptions on unavailable labels, where the entity “John Lloyd Jones” of type PER is labeled but “BBC radio” of type ORG is missing. Each path refers to one possible complete label sequence, and the density of the color indicates probability (we excluded B and E tags for brevity). variable CRF (Quattoni et al., 2005) on citation parsing (McCallum et al., 2000). This model had also been used in part-of-speeching tagging and segmentation task with incomplete annotations (Tsuboi et al., 2008; Liu et al., 2014; Yang and Vozila, 2014). Yang et al. (2018) showed the effectiveness of such a model on Chinese NER with incomplete annotations due to the fact that they required a certain number of fully annotated data to perform joint training. Greenberg et al. (2018) applied this model on a biomedical NER task and achieved promising performance with incomplete annotations. However, in their assumption for the incomplete annotations, the O labels are still considered, which we believe is not realistic. Carlson et al. (2009) modified the structured perceptron algorithm and defined features only on the token"
N19-1079,C08-1113,0,0.056481,"n BBC radio O O O O said on BBC radio O O O O PER PER PER PER ORG ORG ORG ORG Figure 3: Graphical illustrations on different assumptions on unavailable labels, where the entity “John Lloyd Jones” of type PER is labeled but “BBC radio” of type ORG is missing. Each path refers to one possible complete label sequence, and the density of the color indicates probability (we excluded B and E tags for brevity). variable CRF (Quattoni et al., 2005) on citation parsing (McCallum et al., 2000). This model had also been used in part-of-speeching tagging and segmentation task with incomplete annotations (Tsuboi et al., 2008; Liu et al., 2014; Yang and Vozila, 2014). Yang et al. (2018) showed the effectiveness of such a model on Chinese NER with incomplete annotations due to the fact that they required a certain number of fully annotated data to perform joint training. Greenberg et al. (2018) applied this model on a biomedical NER task and achieved promising performance with incomplete annotations. However, in their assumption for the incomplete annotations, the O labels are still considered, which we believe is not realistic. Carlson et al. (2009) modified the structured perceptron algorithm and defined features"
N19-1079,P16-1101,0,0.382917,"Missing"
N19-1079,D14-1010,0,0.0306641,"O O O PER PER PER PER ORG ORG ORG ORG Figure 3: Graphical illustrations on different assumptions on unavailable labels, where the entity “John Lloyd Jones” of type PER is labeled but “BBC radio” of type ORG is missing. Each path refers to one possible complete label sequence, and the density of the color indicates probability (we excluded B and E tags for brevity). variable CRF (Quattoni et al., 2005) on citation parsing (McCallum et al., 2000). This model had also been used in part-of-speeching tagging and segmentation task with incomplete annotations (Tsuboi et al., 2008; Liu et al., 2014; Yang and Vozila, 2014). Yang et al. (2018) showed the effectiveness of such a model on Chinese NER with incomplete annotations due to the fact that they required a certain number of fully annotated data to perform joint training. Greenberg et al. (2018) applied this model on a biomedical NER task and achieved promising performance with incomplete annotations. However, in their assumption for the incomplete annotations, the O labels are still considered, which we believe is not realistic. Carlson et al. (2009) modified the structured perceptron algorithm and defined features only on the tokens with annotated labels"
N19-1079,P08-1108,0,0.291717,"sequence. Figure 3(b) illustrates a naive approach, where we regard all the missing labels as O labels. This essentially assumes that the q distribution in the above equation puts all probability mass to this single label sequence, which is an incorrect assumption. Now let us look at what assumptions on q have been made by the existing approach of Bellare and McCallum (2007). The model regards the missing labels as latent variables and learns a latent variable CRF using the following loss: X X − log pw (y|x(i) ) (2) i Dataset Estimating q Inspired by the classifier stacking technique used in Nivre and McDonald (2008), we empirically found that a reasonable q distribution can be acquired in a k-fold cross-validation fashion. We first start with an initialization step where we assign specific labels to words without labels, forming complete label sequences (we will discuss our initialization strategy in experiments). Next, we perform k-fold cross-validation on the training set. Specifically, each time we train a model with (k-1) folds of the data and based on the learned model we define our q distribution. We describe two different ways of defining the q distribution, namely the hard approach, and the soft"
N19-1079,C18-1183,0,0.181978,"RG ORG ORG ORG Figure 3: Graphical illustrations on different assumptions on unavailable labels, where the entity “John Lloyd Jones” of type PER is labeled but “BBC radio” of type ORG is missing. Each path refers to one possible complete label sequence, and the density of the color indicates probability (we excluded B and E tags for brevity). variable CRF (Quattoni et al., 2005) on citation parsing (McCallum et al., 2000). This model had also been used in part-of-speeching tagging and segmentation task with incomplete annotations (Tsuboi et al., 2008; Liu et al., 2014; Yang and Vozila, 2014). Yang et al. (2018) showed the effectiveness of such a model on Chinese NER with incomplete annotations due to the fact that they required a certain number of fully annotated data to perform joint training. Greenberg et al. (2018) applied this model on a biomedical NER task and achieved promising performance with incomplete annotations. However, in their assumption for the incomplete annotations, the O labels are still considered, which we believe is not realistic. Carlson et al. (2009) modified the structured perceptron algorithm and defined features only on the tokens with annotated labels in partially labeled"
N19-1217,P18-2087,0,0.791077,"e-art results. 1 Introduction There exists a class of language construction known as pun in natural language texts and utterances, where a certain word or other lexical items are used to exploit two or more separate meanings. It has been shown that understanding of puns is an important research question with various real-world applications, such as human-computer interaction (Morkes et al., 1999; Hempelmann, 2008) and machine translation (Schr¨oter, 2005). Recently, many researchers show their interests in studying puns, like detecting pun sentences (Vadehra, 2017), locating puns in the text (Cai et al., 2018), interpreting pun sentences (Sevgili et al., 2017) and generating sentences containing puns (Ritchie, 2005; Hong and Ong, 2009; Yu et al., 2018). A pun is a wordplay in which a certain word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect. Puns can be generally categorized into two groups, namely hetero(2) Some diets cause a gut reaction. The first punning joke exploits the sound similarity between the word “propane” and the latent target “profane”, which can be categorized into the group"
N19-1217,Q16-1026,0,0.0946378,"with N or B, respectively. Exemplified by the second sentence “Some diets cause a gut reaction,” the pun is given as “gut.” Thus, under the BPA scheme, it should be tagged with P, while the words before it are assigned with the tag B and words after it are with A, as illustrated in Figure 1. Likewise, the NP scheme tags the word “gut” with P, while other words are tagged with N. Therefore, we can combine the pun detection and location tasks into one problem which can be solved by the sequence labeling approach. 2.2 Model Neural models have shown their effectiveness on sequence labeling tasks (Chiu and Nichols, 2016; Ma and Hovy, 2016; Liu et al., 2018). In this work, we adopt the bidirectional Long Short Term Memory (BiLSTM) (Graves and Schmidhuber, 2005) networks on top of the Conditional Random Fields (Lafferty et al., 2001) (CRF) architecture to make labeling decisions, which is one of the classical models for sequence labeling. Our model architecture is illustrated in Figure 1 with a running example. Given a context/sentence x = (x1 , x2 , . . . , xn ) where n is the length of the context, we generate the corresponding tag sequence y = (y1 , y2 , . . . , yn ) based on our designed tagging schemes an"
N19-1217,S17-2011,0,0.0879948,"ns when the given text is short. Consider the example “Superglue! Tom rejoined,” here the word rejoined is the corresponding pun. However, it would be challenging to figure out the pun with such limited contextual information. 4 Related Work Most existing systems address pun detection and location separately. Pedersen (2017) applied word sense knowledge to conduct pun detection. Indurthi and Oota (2017) trained a bidirectional RNN classifier for detecting homographic puns. Next, a knowledge-based approach is adopted to find the exact pun. Such a system is not applicable to heterographic puns. Doogan et al. (2017) applied Google n-gram and word2vec to make decisions. The phonetic distance via the CMU Pronouncing Dictionary is computed to detect heterographic puns. Pramanick and Das (2017) used the hidden Markov model and a cyclic dependency network with rich features to detect and locate puns. Mikhalkova and Karyakin (2017) used a supervised approach to pun detection and a weakly supervised approach to pun location based on the position within the context and part of speech features. Vechtomova (2017) proposed a rulebased system for pun location that scores candidate words according to eleven simple he"
N19-1217,P17-1044,0,0.0639788,"Missing"
N19-1217,P16-1101,0,0.186639,"y. Exemplified by the second sentence “Some diets cause a gut reaction,” the pun is given as “gut.” Thus, under the BPA scheme, it should be tagged with P, while the words before it are assigned with the tag B and words after it are with A, as illustrated in Figure 1. Likewise, the NP scheme tags the word “gut” with P, while other words are tagged with N. Therefore, we can combine the pun detection and location tasks into one problem which can be solved by the sequence labeling approach. 2.2 Model Neural models have shown their effectiveness on sequence labeling tasks (Chiu and Nichols, 2016; Ma and Hovy, 2016; Liu et al., 2018). In this work, we adopt the bidirectional Long Short Term Memory (BiLSTM) (Graves and Schmidhuber, 2005) networks on top of the Conditional Random Fields (Lafferty et al., 2001) (CRF) architecture to make labeling decisions, which is one of the classical models for sequence labeling. Our model architecture is illustrated in Figure 1 with a running example. Given a context/sentence x = (x1 , x2 , . . . , xn ) where n is the length of the context, we generate the corresponding tag sequence y = (y1 , y2 , . . . , yn ) based on our designed tagging schemes and the original anno"
N19-1217,S17-2072,0,0.360968,"it is labeled as the gold pun in the dataset. As to pun location, to make fair comparisons with prior studies, we only consider the instances that are labeled as the ones containing a pun. We report precision, recall and F1 score in Table 1. A list of prior works that did not employ joint learning are also shown in the first block of Table 1. 3.2 Results We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger (Toutanova et al., 2003), n-grams, label tran2119 System Pedersen (2017) Pramanick and Das (2017) Mikhalkova and Karyakin (2017) Vadehra (2017) Indurthi and Oota (2017) Vechtomova (2017) Cai et al. (2018) CRF Ours – NP Ours – BPA Ours – BPA-p Pipeline Homographic Detection Location P. R. F1 P. R. F1 78.32 87.24 82.54 44.00 44.00 44.00 72.51 90.79 68.84 33.48 33.48 33.48 79.93 73.37 67.82 32.79 32.79 32.79 68.38 47.23 46.71 34.10 34.10 34.10 90.24 89.70 85.33 52.15 52.15 52.15 65.26 65.21 65.23 81.50 74.70 78.00 87.21 64.09 73.89 86.31 55.32 67.43 89.19 86.25 87.69 82.11 70.82 76.04 89.24 92.28 91.04 83.55 77.10 80.19 91.25 93.28 92.19 82.06 76.54 79.20 67.70 67.70 67.70 Heterographic Detection Location P. R. F1 P. R. F"
N19-1217,S17-2005,0,0.232524,"edings of NAACL-HLT 2019, pages 2117–2123 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics duce error propagation and allows information exchange between tasks which is potentially beneficial to all the tasks. In this work, we demonstrate that the detection and location of puns can be jointly addressed by a single model. The pun detection and location tasks can be combined as a sequence labeling problem, which allows us to jointly detect and locate a pun in a sentence by assigning each word a tag. Since each context contains a maximum of one pun (Miller et al., 2017), we design a novel tagging scheme to capture this structural constraint. Statistics on the corpora also show that a pun tends to appear in the second half of a context. To capture such a structural property, we also incorporate word position knowledge into our structured prediction model. Experiments on the benchmark datasets show that detection and location tasks can reinforce each other, leading to new state-of-the-art performance on these two tasks. To the best of our knowledge, this is the first work that performs joint detection and location of English puns by using a sequence labeling a"
N19-1217,S17-2070,0,0.194816,"pun is regarded as correct if and only if it is labeled as the gold pun in the dataset. As to pun location, to make fair comparisons with prior studies, we only consider the instances that are labeled as the ones containing a pun. We report precision, recall and F1 score in Table 1. A list of prior works that did not employ joint learning are also shown in the first block of Table 1. 3.2 Results We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger (Toutanova et al., 2003), n-grams, label tran2119 System Pedersen (2017) Pramanick and Das (2017) Mikhalkova and Karyakin (2017) Vadehra (2017) Indurthi and Oota (2017) Vechtomova (2017) Cai et al. (2018) CRF Ours – NP Ours – BPA Ours – BPA-p Pipeline Homographic Detection Location P. R. F1 P. R. F1 78.32 87.24 82.54 44.00 44.00 44.00 72.51 90.79 68.84 33.48 33.48 33.48 79.93 73.37 67.82 32.79 32.79 32.79 68.38 47.23 46.71 34.10 34.10 34.10 90.24 89.70 85.33 52.15 52.15 52.15 65.26 65.21 65.23 81.50 74.70 78.00 87.21 64.09 73.89 86.31 55.32 67.43 89.19 86.25 87.69 82.11 70.82 76.04 89.24 92.28 91.04 83.55 77.10 80.19 91.25 93.28 92.19 82.06 76.54 79.20 67.70 67.70"
N19-1217,N18-1135,0,0.0189795,"tances (with or without puns) are taken into account during training. For the task of pun location, a separate system is used to make a single prediction as to which word in the given sentence in the text that trigger more than one semantic interpretations of the text, where the training data involves only sentences that contain a pun. Therefore, if one is interested in solving both problems at the same time, a pipeline approach that performs pun detection followed by pun location can be used. Compared to the pipeline methods, joint learning has been shown effective (Katiyar and Cardie, 2016; Peng et al., 2018) since it is able to re2117 Proceedings of NAACL-HLT 2019, pages 2117–2123 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics duce error propagation and allows information exchange between tasks which is potentially beneficial to all the tasks. In this work, we demonstrate that the detection and location of puns can be jointly addressed by a single model. The pun detection and location tasks can be combined as a sequence labeling problem, which allows us to jointly detect and locate a pun in a sentence by assigning each word a tag. Since each contex"
N19-1217,W09-2004,0,0.0340218,"ces, where a certain word or other lexical items are used to exploit two or more separate meanings. It has been shown that understanding of puns is an important research question with various real-world applications, such as human-computer interaction (Morkes et al., 1999; Hempelmann, 2008) and machine translation (Schr¨oter, 2005). Recently, many researchers show their interests in studying puns, like detecting pun sentences (Vadehra, 2017), locating puns in the text (Cai et al., 2018), interpreting pun sentences (Sevgili et al., 2017) and generating sentences containing puns (Ritchie, 2005; Hong and Ong, 2009; Yu et al., 2018). A pun is a wordplay in which a certain word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect. Puns can be generally categorized into two groups, namely hetero(2) Some diets cause a gut reaction. The first punning joke exploits the sound similarity between the word “propane” and the latent target “profane”, which can be categorized into the group of heterographic puns. Another categorization of English puns is homographic pun, exemplified by the second instance leveraging"
N19-1217,D14-1162,0,0.0858892,"Missing"
N19-1217,S17-2079,0,0.125613,"Missing"
N19-1217,S17-2073,0,0.474885,"he first punning joke exploits the sound similarity between the word “propane” and the latent target “profane”, which can be categorized into the group of heterographic puns. Another categorization of English puns is homographic pun, exemplified by the second instance leveraging distinct senses of the word “gut”. Pun detection is the task of detecting whether there is a pun residing in the given text. The goal of pun location is to find the exact word appearing in the text that implies more than one meanings. Most previous work addresses such two tasks separately and develop separate systems (Pramanick and Das, 2017; Sevgili et al., 2017). Typically, a system for pun detection is built to make a binary prediction on whether a sentence contains a pun or not, where all instances (with or without puns) are taken into account during training. For the task of pun location, a separate system is used to make a single prediction as to which word in the given sentence in the text that trigger more than one semantic interpretations of the text, where the training data involves only sentences that contain a pun. Therefore, if one is interested in solving both problems at the same time, a pipeline approach that perf"
N19-1217,P16-1087,0,0.0291397,"pun or not, where all instances (with or without puns) are taken into account during training. For the task of pun location, a separate system is used to make a single prediction as to which word in the given sentence in the text that trigger more than one semantic interpretations of the text, where the training data involves only sentences that contain a pun. Therefore, if one is interested in solving both problems at the same time, a pipeline approach that performs pun detection followed by pun location can be used. Compared to the pipeline methods, joint learning has been shown effective (Katiyar and Cardie, 2016; Peng et al., 2018) since it is able to re2117 Proceedings of NAACL-HLT 2019, pages 2117–2123 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics duce error propagation and allows information exchange between tasks which is potentially beneficial to all the tasks. In this work, we demonstrate that the detection and location of puns can be jointly addressed by a single model. The pun detection and location tasks can be combined as a sequence labeling problem, which allows us to jointly detect and locate a pun in a sentence by assigning each word a ta"
N19-1217,W05-1614,0,0.0565743,"xts and utterances, where a certain word or other lexical items are used to exploit two or more separate meanings. It has been shown that understanding of puns is an important research question with various real-world applications, such as human-computer interaction (Morkes et al., 1999; Hempelmann, 2008) and machine translation (Schr¨oter, 2005). Recently, many researchers show their interests in studying puns, like detecting pun sentences (Vadehra, 2017), locating puns in the text (Cai et al., 2018), interpreting pun sentences (Sevgili et al., 2017) and generating sentences containing puns (Ritchie, 2005; Hong and Ong, 2009; Yu et al., 2018). A pun is a wordplay in which a certain word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect. Puns can be generally categorized into two groups, namely hetero(2) Some diets cause a gut reaction. The first punning joke exploits the sound similarity between the word “propane” and the latent target “profane”, which can be categorized into the group of heterographic puns. Another categorization of English puns is homographic pun, exemplified by the second"
N19-1217,N16-1030,0,0.0551123,"ilarity between every pair of words in the context and position to pinpoint the pun. The state-of-the-art system for homographic pun location is a neural method (Cai et al., 2018), where the word senses are incorporated into a bidirectional LSTM model. This method only supports the pun location task on homographic puns. Another line of research efforts related to this work is sequence labeling, such as POS tagging, chunking, word segmentation and NER. The neural methods have shown their effectiveness in this task, such as BiLSTM-CNN (Chiu and Nichols, 2016), GRNN (Xu and Sun, 2016), LSTM-CRF (Lample et al., 2016), LSTM-CNNCRF (Ma and Hovy, 2016), LM-LSTM-CRF (Liu et al., 2018). In this work, we combine pun detection and location tasks as a single sequence labeling problem. Inspired by the work of (Liu et al., 2018), we also adopt a LSTM-CRF with character embeddings to make labeling decisions. 5 Conclusion In this paper, we propose to perform pun detection and location tasks in a joint manner from a sequence labeling perspective. We observe that each text in our corpora contains a maximum of one pun. Hence, we design a novel tagging scheme to incorporate such a constraint. Such a scheme guarantees tha"
N19-1217,D18-1170,0,0.0635239,"Missing"
N19-1217,S17-2074,0,0.0234846,"ass of language construction known as pun in natural language texts and utterances, where a certain word or other lexical items are used to exploit two or more separate meanings. It has been shown that understanding of puns is an important research question with various real-world applications, such as human-computer interaction (Morkes et al., 1999; Hempelmann, 2008) and machine translation (Schr¨oter, 2005). Recently, many researchers show their interests in studying puns, like detecting pun sentences (Vadehra, 2017), locating puns in the text (Cai et al., 2018), interpreting pun sentences (Sevgili et al., 2017) and generating sentences containing puns (Ritchie, 2005; Hong and Ong, 2009; Yu et al., 2018). A pun is a wordplay in which a certain word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect. Puns can be generally categorized into two groups, namely hetero(2) Some diets cause a gut reaction. The first punning joke exploits the sound similarity between the word “propane” and the latent target “profane”, which can be categorized into the group of heterographic puns. Another categorization of En"
N19-1217,N03-1033,0,0.22875,"otherwise false. For the pun location task, a predicted pun is regarded as correct if and only if it is labeled as the gold pun in the dataset. As to pun location, to make fair comparisons with prior studies, we only consider the instances that are labeled as the ones containing a pun. We report precision, recall and F1 score in Table 1. A list of prior works that did not employ joint learning are also shown in the first block of Table 1. 3.2 Results We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger (Toutanova et al., 2003), n-grams, label tran2119 System Pedersen (2017) Pramanick and Das (2017) Mikhalkova and Karyakin (2017) Vadehra (2017) Indurthi and Oota (2017) Vechtomova (2017) Cai et al. (2018) CRF Ours – NP Ours – BPA Ours – BPA-p Pipeline Homographic Detection Location P. R. F1 P. R. F1 78.32 87.24 82.54 44.00 44.00 44.00 72.51 90.79 68.84 33.48 33.48 33.48 79.93 73.37 67.82 32.79 32.79 32.79 68.38 47.23 46.71 34.10 34.10 34.10 90.24 89.70 85.33 52.15 52.15 52.15 65.26 65.21 65.23 81.50 74.70 78.00 87.21 64.09 73.89 86.31 55.32 67.43 89.19 86.25 87.69 82.11 70.82 76.04 89.24 92.28 91.04 83.55 77.10 80.19"
N19-1217,S17-2077,0,0.402793,"hat our approach can achieve new state-ofthe-art results. 1 Introduction There exists a class of language construction known as pun in natural language texts and utterances, where a certain word or other lexical items are used to exploit two or more separate meanings. It has been shown that understanding of puns is an important research question with various real-world applications, such as human-computer interaction (Morkes et al., 1999; Hempelmann, 2008) and machine translation (Schr¨oter, 2005). Recently, many researchers show their interests in studying puns, like detecting pun sentences (Vadehra, 2017), locating puns in the text (Cai et al., 2018), interpreting pun sentences (Sevgili et al., 2017) and generating sentences containing puns (Ritchie, 2005; Hong and Ong, 2009; Yu et al., 2018). A pun is a wordplay in which a certain word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect. Puns can be generally categorized into two groups, namely hetero(2) Some diets cause a gut reaction. The first punning joke exploits the sound similarity between the word “propane” and the latent target “prof"
N19-1217,S17-2071,0,0.452702,"ake fair comparisons with prior studies, we only consider the instances that are labeled as the ones containing a pun. We report precision, recall and F1 score in Table 1. A list of prior works that did not employ joint learning are also shown in the first block of Table 1. 3.2 Results We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger (Toutanova et al., 2003), n-grams, label tran2119 System Pedersen (2017) Pramanick and Das (2017) Mikhalkova and Karyakin (2017) Vadehra (2017) Indurthi and Oota (2017) Vechtomova (2017) Cai et al. (2018) CRF Ours – NP Ours – BPA Ours – BPA-p Pipeline Homographic Detection Location P. R. F1 P. R. F1 78.32 87.24 82.54 44.00 44.00 44.00 72.51 90.79 68.84 33.48 33.48 33.48 79.93 73.37 67.82 32.79 32.79 32.79 68.38 47.23 46.71 34.10 34.10 34.10 90.24 89.70 85.33 52.15 52.15 52.15 65.26 65.21 65.23 81.50 74.70 78.00 87.21 64.09 73.89 86.31 55.32 67.43 89.19 86.25 87.69 82.11 70.82 76.04 89.24 92.28 91.04 83.55 77.10 80.19 91.25 93.28 92.19 82.06 76.54 79.20 67.70 67.70 67.70 Heterographic Detection Location P. R. F1 P. R. F1 73.99 86.62 68.71 73.67 94.02 71.74 37.92 37.92 37.92 75"
N19-1217,P16-2092,0,0.0257298,"ocator considers word2vec similarity between every pair of words in the context and position to pinpoint the pun. The state-of-the-art system for homographic pun location is a neural method (Cai et al., 2018), where the word senses are incorporated into a bidirectional LSTM model. This method only supports the pun location task on homographic puns. Another line of research efforts related to this work is sequence labeling, such as POS tagging, chunking, word segmentation and NER. The neural methods have shown their effectiveness in this task, such as BiLSTM-CNN (Chiu and Nichols, 2016), GRNN (Xu and Sun, 2016), LSTM-CRF (Lample et al., 2016), LSTM-CNNCRF (Ma and Hovy, 2016), LM-LSTM-CRF (Liu et al., 2018). In this work, we combine pun detection and location tasks as a single sequence labeling problem. Inspired by the work of (Liu et al., 2018), we also adopt a LSTM-CRF with character embeddings to make labeling decisions. 5 Conclusion In this paper, we propose to perform pun detection and location tasks in a joint manner from a sequence labeling perspective. We observe that each text in our corpora contains a maximum of one pun. Hence, we design a novel tagging scheme to incorporate such a constrai"
N19-1217,P18-1153,0,0.107342,"word or other lexical items are used to exploit two or more separate meanings. It has been shown that understanding of puns is an important research question with various real-world applications, such as human-computer interaction (Morkes et al., 1999; Hempelmann, 2008) and machine translation (Schr¨oter, 2005). Recently, many researchers show their interests in studying puns, like detecting pun sentences (Vadehra, 2017), locating puns in the text (Cai et al., 2018), interpreting pun sentences (Sevgili et al., 2017) and generating sentences containing puns (Ritchie, 2005; Hong and Ong, 2009; Yu et al., 2018). A pun is a wordplay in which a certain word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect. Puns can be generally categorized into two groups, namely hetero(2) Some diets cause a gut reaction. The first punning joke exploits the sound similarity between the word “propane” and the latent target “profane”, which can be categorized into the group of heterographic puns. Another categorization of English puns is homographic pun, exemplified by the second instance leveraging distinct senses o"
N19-1346,P06-2013,0,0.0569518,"ed, which are capable of capturing complex dependencies among chunks. 6 Related Work While the Chinese address parsing task is new, it is related to the following traditional tasks within the field of natural language processing (NLP) – chunking, named entity recognition, word segmentation and parsing. We briefly survey research efforts which are most related to our task below. Chunking as a fundamental task in NLP has been investigated for decades (Abney, 1991). Chunking for Chinese can typically be regarded as a sequence labeling problem solvable by models such as conditional random fields (Chen et al., 2006; Tan et al., 2005; Zhou et al., 2012), hidden Markov models (Li et al., 2003), support vector machines (Tan et al., 2004) and the maximum entropy model (Wu et al., 2005). Our task can also be regarded as a chunking task where we need to assign an address-specific label to each chunk. Named entity recognition (NER) is another fundamental task close to chunking within the field of NLP, which focuses on the extraction of semantically meaningful entities from the text. The stateof-the-art approach by Lample et al. (2016) employs a LSTM-CRF model. Ma and Hovy (2016) proposed a LSTM-CNNs-CRF model"
N19-1346,P15-1033,0,0.0232049,"e the supplementary material for details on the features for `CRF and sCRF. For sCRF (and LSTM-sCRF), maximal chunk length is set to 36, which is the length of the longest chunk appearing in the training set. 3425 • LSTM-`CRF is proposed by Lample et al. (2016) which is the state-of-the-art for many sequence labeling tasks • LSTM-sCRF is based on segmental recurrent neural network (Kong et al., 2016) which is the neural network version of semi-Markov CRF (Sarawagi and Cohen, 2004). • TP is a transition-based parser for chunking based on Lample et al. (2016), which makes use of the stack LSTM (Dyer et al., 2015) to encode the representation of the stack. Hyperparameters We conducted all the experiments based on our Chinese Address corpus. We pre-trained Chinese character embeddings based on the Chinese Gigaword corpus (Graff and Chen, 2005), using the skip-gram model with hierarchical softmax implemented within the word2vec toolkit (Mikolov et al., 2013) where we set the sample rate to 10−5 and embedding size to 100. We use a 2-layer LSTM (for both directions) with a hidden dimension of 200. For optimization, we adopt the Adam (Kingma and Ba, 2014) optimizer to optimize the model with batch size 1 an"
N19-1346,N16-1024,0,0.030051,"be solved by CRF based models (Peng et al., 2004; Zhao et al., 2006), their latent-variable variants (Sun et al., 2009), or maxmargin based models (Zhang and Clark, 2007). Recently, Zhang et al. (2016) proposed a neural transition-based segmentation approach by encoding both words and characters as well as the history action sequence. Yang et al. (2017) suggested to perform segmentation with a neural transitionbased method with rich pre-training. Constituent parsing is another line of work that is related to our task. The state-of-the-art approaches to parsing include transition-based models (Dyer et al., 2016) and chart-based models (Stern et al., 2017; Kitaev and Klein, 2018).Our model is motivated by the latter approaches, where we additionally introduce latent variables for capturing complex dependencies among chunks. 7 Conclusion In this work, we introduce a new task – Chinese address parsing, which is to segment a given Chinese address text into chunks while assigning each chunk a semantically meaningful label. We create and publish a Chinese address corpus that consists of 15K fully labeled Chinese addresses. We identify interesting characteristics associated with the task and design a novel"
N19-1346,P18-1249,0,0.0145155,"2006), their latent-variable variants (Sun et al., 2009), or maxmargin based models (Zhang and Clark, 2007). Recently, Zhang et al. (2016) proposed a neural transition-based segmentation approach by encoding both words and characters as well as the history action sequence. Yang et al. (2017) suggested to perform segmentation with a neural transitionbased method with rich pre-training. Constituent parsing is another line of work that is related to our task. The state-of-the-art approaches to parsing include transition-based models (Dyer et al., 2016) and chart-based models (Stern et al., 2017; Kitaev and Klein, 2018).Our model is motivated by the latter approaches, where we additionally introduce latent variables for capturing complex dependencies among chunks. 7 Conclusion In this work, we introduce a new task – Chinese address parsing, which is to segment a given Chinese address text into chunks while assigning each chunk a semantically meaningful label. We create and publish a Chinese address corpus that consists of 15K fully labeled Chinese addresses. We identify interesting characteristics associated with the task and design a novel neural parsing model with latent variables for this task, which is a"
N19-1346,N16-1030,0,0.703456,"nd Cohen, 2004) with discrete features5 . • LSTM is the standard bi-directional LSTM model for sequence labeling tasks. 4 In some cases, it is possible to predict a tree with one or more leaf chunks labeled with auxiliary labels (e.g., ROAD). We have a post-processing step that converts such labels into their corresponding original labels (e.g., ROAD). 5 See the supplementary material for details on the features for `CRF and sCRF. For sCRF (and LSTM-sCRF), maximal chunk length is set to 36, which is the length of the longest chunk appearing in the training set. 3425 • LSTM-`CRF is proposed by Lample et al. (2016) which is the state-of-the-art for many sequence labeling tasks • LSTM-sCRF is based on segmental recurrent neural network (Kong et al., 2016) which is the neural network version of semi-Markov CRF (Sarawagi and Cohen, 2004). • TP is a transition-based parser for chunking based on Lample et al. (2016), which makes use of the stack LSTM (Dyer et al., 2015) to encode the representation of the stack. Hyperparameters We conducted all the experiments based on our Chinese Address corpus. We pre-trained Chinese character embeddings based on the Chinese Gigaword corpus (Graff and Chen, 2005), using th"
N19-1346,P16-1101,0,0.0270875,"ch as conditional random fields (Chen et al., 2006; Tan et al., 2005; Zhou et al., 2012), hidden Markov models (Li et al., 2003), support vector machines (Tan et al., 2004) and the maximum entropy model (Wu et al., 2005). Our task can also be regarded as a chunking task where we need to assign an address-specific label to each chunk. Named entity recognition (NER) is another fundamental task close to chunking within the field of NLP, which focuses on the extraction of semantically meaningful entities from the text. The stateof-the-art approach by Lample et al. (2016) employs a LSTM-CRF model. Ma and Hovy (2016) proposed a LSTM-CNNs-CRF model that utilizes convolutional neural networks (CNNs) to extract character-level features besides word-level features. Zhai et al. (2017) suggested a neural chunking model based on pointer networks (Vinyals et al., 2015) to resolve the issue of being difficult to use chunk-level features such as the length of the chunk for segmentation. Zhang and Yang (2018) tackled the problem of Chinese NER by deploying a lattice LSTM leveraging lexicons. Another task closely related to our task is the Chinese word segmentation task which at least dates back to the 1990s (Sproat"
N19-1346,C04-1081,0,0.446087,"ict) 下沙开发区(Xiasha Development Zone) 乔司镇(Qiaosi Street) 荆山社区(Jingshan Community) 中山路(Zhongshan Road) 丹心巷(Danxin Road) 4-5号(#4-#5) 8号(#8) 萧山医院(Xiaoshan Hospital) 西三苑(Xisan Sub-residence) 3幢(Block #3) 1单元(Unit #1) 5层(Level 5) 402室(Room 402) 大厅(the hall) 对面(opposite) -,! 32,133 08,957 50,934 02,985 17,024 02,985 16,704 Table 1: Statistics of different labels in our Chinese Address corpus. Parsing a Chinese address into semantically meaningful structures can be regarded as a special type of chunking task (Abney, 1991), where we need to perform address-specific Chinese word segmentation (Xue, 2003; Peng et al., 2004; Zhao et al., 2006) while assigning a semantic label to each chunk. However, existing models designed for chunking may not be readily applicable in this task. Our observations show that there are a few characteristics associated with the task. We found that while generally there exists certain ordering information among the chunks of different labels in the addresses, such ordering information is better preserved among the chunks that appear at the beginning of the addresses. For the chunks appearing towards the end of the addresses, chunks of different types often appear in more flexible ord"
N19-1346,P94-1010,0,0.61226,"(2016) proposed a LSTM-CNNs-CRF model that utilizes convolutional neural networks (CNNs) to extract character-level features besides word-level features. Zhai et al. (2017) suggested a neural chunking model based on pointer networks (Vinyals et al., 2015) to resolve the issue of being difficult to use chunk-level features such as the length of the chunk for segmentation. Zhang and Yang (2018) tackled the problem of Chinese NER by deploying a lattice LSTM leveraging lexicons. Another task closely related to our task is the Chinese word segmentation task which at least dates back to the 1990s (Sproat et al., 1994). The segmentation task is typically casted as a character-based sequence labeling problem (Xue, 2003) which can be solved by CRF based models (Peng et al., 2004; Zhao et al., 2006), their latent-variable variants (Sun et al., 2009), or maxmargin based models (Zhang and Clark, 2007). Recently, Zhang et al. (2016) proposed a neural transition-based segmentation approach by encoding both words and characters as well as the history action sequence. Yang et al. (2017) suggested to perform segmentation with a neural transitionbased method with rich pre-training. Constituent parsing is another line"
N19-1346,P17-1076,0,0.146538,"discuss the effect of different sp values in the experiments section. 3.3 Chunk Representation A parse tree corresponds to a collection of labeled chunks as leaves. We adopt a bi-directional LSTM over a given input to compute the spanlevel representation. At each position i in the orig3424 inal input consisting of a sequence of characters, we use fi and bi to denote the outputs of forward LSTM and backward LSTM respectively. We use ci,j = [fj − fi ; bi − bj ] to denote the vector representation of the span covering characters from position i to position j (Wang and Chang, 2016). Motivated by Stern et al. (2017), we define the label score as follows: where F is a 2-layer feed-forward neural network with output dimension being the number of chunk labels. In addition, we denote the score of the span with a specific label l as the value of the l-th element in the vector s(i, j): 3.4 (1) Model Inspired by Stern et al. (2017), we build a chartbased parsing model. Unlike that work, however, our model involves latent structures as mentioned in Section 3.1. For a given sequence of labeled chunks, our model considers all possible constituent trees whose yield are exactly the labeled chunks. Consider a tree t"
N19-1346,N09-1007,0,0.0265811,"Vinyals et al., 2015) to resolve the issue of being difficult to use chunk-level features such as the length of the chunk for segmentation. Zhang and Yang (2018) tackled the problem of Chinese NER by deploying a lattice LSTM leveraging lexicons. Another task closely related to our task is the Chinese word segmentation task which at least dates back to the 1990s (Sproat et al., 1994). The segmentation task is typically casted as a character-based sequence labeling problem (Xue, 2003) which can be solved by CRF based models (Peng et al., 2004; Zhao et al., 2006), their latent-variable variants (Sun et al., 2009), or maxmargin based models (Zhang and Clark, 2007). Recently, Zhang et al. (2016) proposed a neural transition-based segmentation approach by encoding both words and characters as well as the history action sequence. Yang et al. (2017) suggested to perform segmentation with a neural transitionbased method with rich pre-training. Constituent parsing is another line of work that is related to our task. The state-of-the-art approaches to parsing include transition-based models (Dyer et al., 2016) and chart-based models (Stern et al., 2017; Kitaev and Klein, 2018).Our model is motivated by the la"
N19-1346,W00-0726,0,0.594536,"Missing"
N19-1346,P16-1218,0,0.0190107,"rent variants of our models. We will discuss the effect of different sp values in the experiments section. 3.3 Chunk Representation A parse tree corresponds to a collection of labeled chunks as leaves. We adopt a bi-directional LSTM over a given input to compute the spanlevel representation. At each position i in the orig3424 inal input consisting of a sequence of characters, we use fi and bi to denote the outputs of forward LSTM and backward LSTM respectively. We use ci,j = [fj − fi ; bi − bj ] to denote the vector representation of the span covering characters from position i to position j (Wang and Chang, 2016). Motivated by Stern et al. (2017), we define the label score as follows: where F is a 2-layer feed-forward neural network with output dimension being the number of chunk labels. In addition, we denote the score of the span with a specific label l as the value of the l-th element in the vector s(i, j): 3.4 (1) Model Inspired by Stern et al. (2017), we build a chartbased parsing model. Unlike that work, however, our model involves latent structures as mentioned in Section 3.1. For a given sequence of labeled chunks, our model considers all possible constituent trees whose yield are exactly the"
N19-1346,O05-1017,0,0.077069,"nal tasks within the field of natural language processing (NLP) – chunking, named entity recognition, word segmentation and parsing. We briefly survey research efforts which are most related to our task below. Chunking as a fundamental task in NLP has been investigated for decades (Abney, 1991). Chunking for Chinese can typically be regarded as a sequence labeling problem solvable by models such as conditional random fields (Chen et al., 2006; Tan et al., 2005; Zhou et al., 2012), hidden Markov models (Li et al., 2003), support vector machines (Tan et al., 2004) and the maximum entropy model (Wu et al., 2005). Our task can also be regarded as a chunking task where we need to assign an address-specific label to each chunk. Named entity recognition (NER) is another fundamental task close to chunking within the field of NLP, which focuses on the extraction of semantically meaningful entities from the text. The stateof-the-art approach by Lample et al. (2016) employs a LSTM-CRF model. Ma and Hovy (2016) proposed a LSTM-CNNs-CRF model that utilizes convolutional neural networks (CNNs) to extract character-level features besides word-level features. Zhai et al. (2017) suggested a neural chunking model b"
N19-1346,O03-4002,0,0.529045,"oyang District) 下沙开发区(Xiasha Development Zone) 乔司镇(Qiaosi Street) 荆山社区(Jingshan Community) 中山路(Zhongshan Road) 丹心巷(Danxin Road) 4-5号(#4-#5) 8号(#8) 萧山医院(Xiaoshan Hospital) 西三苑(Xisan Sub-residence) 3幢(Block #3) 1单元(Unit #1) 5层(Level 5) 402室(Room 402) 大厅(the hall) 对面(opposite) -,! 32,133 08,957 50,934 02,985 17,024 02,985 16,704 Table 1: Statistics of different labels in our Chinese Address corpus. Parsing a Chinese address into semantically meaningful structures can be regarded as a special type of chunking task (Abney, 1991), where we need to perform address-specific Chinese word segmentation (Xue, 2003; Peng et al., 2004; Zhao et al., 2006) while assigning a semantic label to each chunk. However, existing models designed for chunking may not be readily applicable in this task. Our observations show that there are a few characteristics associated with the task. We found that while generally there exists certain ordering information among the chunks of different labels in the addresses, such ordering information is better preserved among the chunks that appear at the beginning of the addresses. For the chunks appearing towards the end of the addresses, chunks of different types often appear i"
N19-1346,P17-1078,0,0.0176059,"lexicons. Another task closely related to our task is the Chinese word segmentation task which at least dates back to the 1990s (Sproat et al., 1994). The segmentation task is typically casted as a character-based sequence labeling problem (Xue, 2003) which can be solved by CRF based models (Peng et al., 2004; Zhao et al., 2006), their latent-variable variants (Sun et al., 2009), or maxmargin based models (Zhang and Clark, 2007). Recently, Zhang et al. (2016) proposed a neural transition-based segmentation approach by encoding both words and characters as well as the history action sequence. Yang et al. (2017) suggested to perform segmentation with a neural transitionbased method with rich pre-training. Constituent parsing is another line of work that is related to our task. The state-of-the-art approaches to parsing include transition-based models (Dyer et al., 2016) and chart-based models (Stern et al., 2017; Kitaev and Klein, 2018).Our model is motivated by the latter approaches, where we additionally introduce latent variables for capturing complex dependencies among chunks. 7 Conclusion In this work, we introduce a new task – Chinese address parsing, which is to segment a given Chinese address"
N19-1346,P16-1040,0,0.0193399,"features such as the length of the chunk for segmentation. Zhang and Yang (2018) tackled the problem of Chinese NER by deploying a lattice LSTM leveraging lexicons. Another task closely related to our task is the Chinese word segmentation task which at least dates back to the 1990s (Sproat et al., 1994). The segmentation task is typically casted as a character-based sequence labeling problem (Xue, 2003) which can be solved by CRF based models (Peng et al., 2004; Zhao et al., 2006), their latent-variable variants (Sun et al., 2009), or maxmargin based models (Zhang and Clark, 2007). Recently, Zhang et al. (2016) proposed a neural transition-based segmentation approach by encoding both words and characters as well as the history action sequence. Yang et al. (2017) suggested to perform segmentation with a neural transitionbased method with rich pre-training. Constituent parsing is another line of work that is related to our task. The state-of-the-art approaches to parsing include transition-based models (Dyer et al., 2016) and chart-based models (Stern et al., 2017; Kitaev and Klein, 2018).Our model is motivated by the latter approaches, where we additionally introduce latent variables for capturing co"
N19-1346,P07-1106,0,0.053438,"being difficult to use chunk-level features such as the length of the chunk for segmentation. Zhang and Yang (2018) tackled the problem of Chinese NER by deploying a lattice LSTM leveraging lexicons. Another task closely related to our task is the Chinese word segmentation task which at least dates back to the 1990s (Sproat et al., 1994). The segmentation task is typically casted as a character-based sequence labeling problem (Xue, 2003) which can be solved by CRF based models (Peng et al., 2004; Zhao et al., 2006), their latent-variable variants (Sun et al., 2009), or maxmargin based models (Zhang and Clark, 2007). Recently, Zhang et al. (2016) proposed a neural transition-based segmentation approach by encoding both words and characters as well as the history action sequence. Yang et al. (2017) suggested to perform segmentation with a neural transitionbased method with rich pre-training. Constituent parsing is another line of work that is related to our task. The state-of-the-art approaches to parsing include transition-based models (Dyer et al., 2016) and chart-based models (Stern et al., 2017; Kitaev and Klein, 2018).Our model is motivated by the latter approaches, where we additionally introduce la"
N19-1346,P18-1144,0,0.0160987,"tal task close to chunking within the field of NLP, which focuses on the extraction of semantically meaningful entities from the text. The stateof-the-art approach by Lample et al. (2016) employs a LSTM-CRF model. Ma and Hovy (2016) proposed a LSTM-CNNs-CRF model that utilizes convolutional neural networks (CNNs) to extract character-level features besides word-level features. Zhai et al. (2017) suggested a neural chunking model based on pointer networks (Vinyals et al., 2015) to resolve the issue of being difficult to use chunk-level features such as the length of the chunk for segmentation. Zhang and Yang (2018) tackled the problem of Chinese NER by deploying a lattice LSTM leveraging lexicons. Another task closely related to our task is the Chinese word segmentation task which at least dates back to the 1990s (Sproat et al., 1994). The segmentation task is typically casted as a character-based sequence labeling problem (Xue, 2003) which can be solved by CRF based models (Peng et al., 2004; Zhao et al., 2006), their latent-variable variants (Sun et al., 2009), or maxmargin based models (Zhang and Clark, 2007). Recently, Zhang et al. (2016) proposed a neural transition-based segmentation approach by e"
N19-1346,Y06-1012,0,0.234857,"evelopment Zone) 乔司镇(Qiaosi Street) 荆山社区(Jingshan Community) 中山路(Zhongshan Road) 丹心巷(Danxin Road) 4-5号(#4-#5) 8号(#8) 萧山医院(Xiaoshan Hospital) 西三苑(Xisan Sub-residence) 3幢(Block #3) 1单元(Unit #1) 5层(Level 5) 402室(Room 402) 大厅(the hall) 对面(opposite) -,! 32,133 08,957 50,934 02,985 17,024 02,985 16,704 Table 1: Statistics of different labels in our Chinese Address corpus. Parsing a Chinese address into semantically meaningful structures can be regarded as a special type of chunking task (Abney, 1991), where we need to perform address-specific Chinese word segmentation (Xue, 2003; Peng et al., 2004; Zhao et al., 2006) while assigning a semantic label to each chunk. However, existing models designed for chunking may not be readily applicable in this task. Our observations show that there are a few characteristics associated with the task. We found that while generally there exists certain ordering information among the chunks of different labels in the addresses, such ordering information is better preserved among the chunks that appear at the beginning of the addresses. For the chunks appearing towards the end of the addresses, chunks of different types often appear in more flexible order. On top of the ab"
N19-1346,D12-1051,0,0.0199246,"mplex dependencies among chunks. 6 Related Work While the Chinese address parsing task is new, it is related to the following traditional tasks within the field of natural language processing (NLP) – chunking, named entity recognition, word segmentation and parsing. We briefly survey research efforts which are most related to our task below. Chunking as a fundamental task in NLP has been investigated for decades (Abney, 1991). Chunking for Chinese can typically be regarded as a sequence labeling problem solvable by models such as conditional random fields (Chen et al., 2006; Tan et al., 2005; Zhou et al., 2012), hidden Markov models (Li et al., 2003), support vector machines (Tan et al., 2004) and the maximum entropy model (Wu et al., 2005). Our task can also be regarded as a chunking task where we need to assign an address-specific label to each chunk. Named entity recognition (NER) is another fundamental task close to chunking within the field of NLP, which focuses on the extraction of semantically meaningful entities from the text. The stateof-the-art approach by Lample et al. (2016) employs a LSTM-CRF model. Ma and Hovy (2016) proposed a LSTM-CNNs-CRF model that utilizes convolutional neural net"
P12-1088,N10-1083,0,0.0315726,"Missing"
P12-1088,P07-1036,1,0.931812,"i)[fk (si, h, t)] (13) i 3.2 Approximate Learning Computation of the denominator terms of Equation 10 (and the second term of Equation 11) can be done 839 pΘ (t, h|si ; κ) ∝ ef(si ,h,t)·Θ × κ(si , h, t) pΘ (t, h|si ) ∝ ef(si ,h,t)·Θ Recall that s is an event span, t is a specfic realization of the event template, and h is the hidden mention information for the event span. 4 Discussion: Preferences v.s. Constraints Note that the objective function in Equation 5, if written in the additive form, leads to a cost function reminiscent of the one used in constraint-driven learning algorithm (CoDL) (Chang et al., 2007) (and similarly, posterior regularization (Ganchev et al., 2010), which we will discuss later at Section 6). Specifically, in CoDL, the following cost function is involved in its EM-like inference procedure: X d(y, Yc ) (14) arg max Θ · f(x, y) − ρ y c where Yc defines the set of y’s that all satisfy a certain constraint c, and d defines a distance function from y to that set. The parameter ρ controls the degree of the penalty when constraints are violated. There are some important distinctions between structured preference modeling (PM) and CoDL. CoDL primarily concerns constraints, which pen"
P12-1088,N10-1066,1,0.371947,"design a priori an effective neighborhood (which also needs to be designed in certain forms to allow efficient computation of the normalization terms) in order to obtain optimal performance. The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al., 2009), where good neighborhoods can be identified. However, it is less intuitive what constitutes a good neighborhood in this task. The neighborhood assumption of CE is relaxed in another latent structure approach (Chang et al., 2010a; Chang et al., 2010b) that focuses on semisupervised learning with indirect supervisions, inspired by the CoDL model described above. The locally normalized logistic regression (Berg843 Kirkpatrick et al., 2010) is another recently proposed framework for unsupervised structured prediction. Their model can be regarded as a generative model whose component multinomial is replaced with a miniature logistic regression where a rich set of local features can be incorporated. Empirically the model is effective in various unsupervised structured prediction tasks, and outperforms the globally normali"
P12-1088,N06-1041,0,0.0152495,"popular topic in natural language processing. 4 For each event, we only performed 1 run with all the initial feature weights set to zeros. Event Attack Meet Die Transport Random 14.26 26.65 19.17 15.78 Unsup 26.19 14.08 9.09 10.14 PM 32.89 45.28 44.44 49.73 semi-CRF 46.92 58.18 48.57 52.34 Table 3: Event extraction performance with automatic mention identifier and typer. We report F1 percentage scores for preference modeling (PM) as well as two baseline approaches. We also report performance of the supervised approach trained with the semi-CRF model for comparison. Prototype driven learning (Haghighi and Klein, 2006) tackled the sequence labeling problem in a primarily unsupervised setting. In their work, a Markov random fields model was used, where some local constraints are specified via their prototype list. Constraint-driven learning (CoDL) (Chang et al., 2007) and posterior regularization (PR) (Ganchev et al., 2010) are both primarily semi-supervised models. They define a constrained EM framework that regularizes posterior distribution at the E-step of each EM iteration, by pushing posterior distributions towards a constrained posterior set. We have already discussed CoDL in Section 4 and gave a comp"
P12-1088,P06-1059,0,0.00539906,"itional probability of observing the template realization for the observed event span s: X L(Θ) = log PΘ (ti |si ) i = X i P f(si ,h,ti )·Θ e log P h f(s ,h,t)·Θ i t,h e (2) This function is not convex due to the summation over the hidden variable h. To optimize it, we take its partial derivative with respect to θj : X ∂L(Θ) = EpΘ (h|si ,ti ) [fj (si , h, ti )] ∂θj i X − EpΘ (t,h|si ) [fj (si , h, t)] (3) i which requires computation of expectations terms under two different distributions. Such statistics can be collected efficiently with a forward-backward style algorithm in polynomial time (Okanohara et al., 2006). We will discuss the time complexity for our case in the next section. Given its partial derivatives in Equation 3, one could optimize the objective function of Equation 2 with stochastic gradient ascent (LeCun et al., 1998) or L-BFGS (Liu and Nocedal, 1989). We choose to use L-BFGS for all our experiments in this paper. Inference involves computing the most probable template realization t for a given event span: X arg max PΘ (t|s) = arg max PΘ (t, h|s) (4) t t h where the possible hidden assignments h need to be marginalized out. In this task, a particular realization t already uniquely defi"
P12-1088,N09-1024,0,0.0132905,", 2005a) is another log-linear framework for primarily unsupervised structured prediction. Their objective function is related to the pseudolikelihood estimator proposed by Besag (1975). One challenge is that it requires one to design a priori an effective neighborhood (which also needs to be designed in certain forms to allow efficient computation of the normalization terms) in order to obtain optimal performance. The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al., 2009), where good neighborhoods can be identified. However, it is less intuitive what constitutes a good neighborhood in this task. The neighborhood assumption of CE is relaxed in another latent structure approach (Chang et al., 2010a; Chang et al., 2010b) that focuses on semisupervised learning with indirect supervisions, inspired by the CoDL model described above. The locally normalized logistic regression (Berg843 Kirkpatrick et al., 2010) is another recently proposed framework for unsupervised structured prediction. Their model can be regarded as a generative model whose component multinomial i"
P12-1088,W09-1119,1,0.171038,"Consider the following text span that describes an “Attack” event: . . . North Korea’s military may have fired a laser at a U.S. helicopter in March, a U.S. official said Tuesday, as the communist state ditched its last legal obligation to keep itself free of nuclear weapons . . . A partial event template for the “Attack” event is shown on the left of Figure 1. Each row shows an Such mention type information as shown on the left of Figure 1 can be obtained from various sources such as dictionaries, gazetteers, rule-based systems (Str¨otgen and Gertz, 2010), statistically trained classifiers (Ratinov and Roth, 2009), or some web resources such as Wikipedia (Ratinov et al., 2011). However, in practice, outputs from existing mention identification and typing systems can be far from ideal. Instead of obtaining the above ideal annotation, one might observe the following noisy and ambiguous annotation for the given event span: . . . [[North Korea’s]G PE|L OC military]O RG may have fired a laser at [a [U.S.]G PE|L OC helicopter]V EH in [March]T ME , [a [U.S.]G PE|L OC official]P ER said [Tuesday]T ME , as [the communist state]O RG|FAC|L OC ditched its last legal obligation to keep [itself ]O RG free of [nuclea"
P12-1088,P11-1138,1,0.165136,": . . . North Korea’s military may have fired a laser at a U.S. helicopter in March, a U.S. official said Tuesday, as the communist state ditched its last legal obligation to keep itself free of nuclear weapons . . . A partial event template for the “Attack” event is shown on the left of Figure 1. Each row shows an Such mention type information as shown on the left of Figure 1 can be obtained from various sources such as dictionaries, gazetteers, rule-based systems (Str¨otgen and Gertz, 2010), statistically trained classifiers (Ratinov and Roth, 2009), or some web resources such as Wikipedia (Ratinov et al., 2011). However, in practice, outputs from existing mention identification and typing systems can be far from ideal. Instead of obtaining the above ideal annotation, one might observe the following noisy and ambiguous annotation for the given event span: . . . [[North Korea’s]G PE|L OC military]O RG may have fired a laser at [a [U.S.]G PE|L OC helicopter]V EH in [March]T ME , [a [U.S.]G PE|L OC official]P ER said [Tuesday]T ME , as [the communist state]O RG|FAC|L OC ditched its last legal obligation to keep [itself ]O RG free of [nuclear weapons]W EA . . . Our task is to design a model to effectivel"
P12-1088,N12-1087,1,0.801652,"pecified via their prototype list. Constraint-driven learning (CoDL) (Chang et al., 2007) and posterior regularization (PR) (Ganchev et al., 2010) are both primarily semi-supervised models. They define a constrained EM framework that regularizes posterior distribution at the E-step of each EM iteration, by pushing posterior distributions towards a constrained posterior set. We have already discussed CoDL in Section 4 and gave a comparison to our model. Unlike CoDL, in the PR framework constraints are relaxed to expectation constraints, in order to allow tractable dynamic programming. See also Samdani et al. (2012) for more discussions. Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. Their objective function is related to the pseudolikelihood estimator proposed by Besag (1975). One challenge is that it requires one to design a priori an effective neighborhood (which also needs to be designed in certain forms to allow efficient computation of the normalization terms) in order to obtain optimal performance. The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar i"
P12-1088,P05-1044,0,0.0423737,"ng et al., 2007) and posterior regularization (PR) (Ganchev et al., 2010) are both primarily semi-supervised models. They define a constrained EM framework that regularizes posterior distribution at the E-step of each EM iteration, by pushing posterior distributions towards a constrained posterior set. We have already discussed CoDL in Section 4 and gave a comparison to our model. Unlike CoDL, in the PR framework constraints are relaxed to expectation constraints, in order to allow tractable dynamic programming. See also Samdani et al. (2012) for more discussions. Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. Their objective function is related to the pseudolikelihood estimator proposed by Besag (1975). One challenge is that it requires one to design a priori an effective neighborhood (which also needs to be designed in certain forms to allow efficient computation of the normalization terms) in order to obtain optimal performance. The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon e"
P12-1088,S10-1071,0,0.0244013,"Missing"
P15-2121,W10-2903,0,0.0291135,"Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 737–742, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics ma ma ma (w1 w2 ) w3 w4 w5 w6 w7 w8 w9 (w10 ) w1 w2 mb mc mb w3 w4 w5 md w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 (a) mc w10 w7 w8 w6 md mb (w3 w4 w5 ) w6 (w7 w8 ) w9 w9 (b) mc md (w6 ) (w9 ) (c) Figure 1: The semantics-sentence pair (a), an example hybrid tree (b), and an example relaxed hybrid tree (c). Q UERY : answer(R IVER) of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations. Goldwasser et al. (2011) presented a confidence-driven approach to semantic parsing based on self-training. Liang et al. (2013) introduced semantic parsers based on dependency based semantics (DCS) that map sentences into their denotations. In this work, we focus on parsing sentences into their formal semantic representations. 3 R IVER : exclude(R IVER, R IVER) R IVER : river(all) S TATE : stateid(S TATE NAME) S TATE NAME : (0 tn0 ) What rivers do not run through Tennessee ? Figure"
P15-2121,P11-1149,0,0.0191538,"Short Papers), pages 737–742, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics ma ma ma (w1 w2 ) w3 w4 w5 w6 w7 w8 w9 (w10 ) w1 w2 mb mc mb w3 w4 w5 md w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 (a) mc w10 w7 w8 w6 md mb (w3 w4 w5 ) w6 (w7 w8 ) w9 w9 (b) mc md (w6 ) (w9 ) (c) Figure 1: The semantics-sentence pair (a), an example hybrid tree (b), and an example relaxed hybrid tree (c). Q UERY : answer(R IVER) of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations. Goldwasser et al. (2011) presented a confidence-driven approach to semantic parsing based on self-training. Liang et al. (2013) introduced semantic parsers based on dependency based semantics (DCS) that map sentences into their denotations. In this work, we focus on parsing sentences into their formal semantic representations. 3 R IVER : exclude(R IVER, R IVER) R IVER : river(all) S TATE : stateid(S TATE NAME) S TATE NAME : (0 tn0 ) What rivers do not run through Tennessee ? Figure 2: An example tree-structured semantic representation (above) and its corresponding natural language sentence (below). Relaxed Hybrid Tre"
P15-2121,P12-1051,0,0.883698,"discriminative semantic parsing model – the relaxed hybrid tree model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/. 1 2 Related Work Semantic parsing has recently attracted a significant amount of attention in the community. In this section, we provide a relatively brief discussion of prior work in semantic parsing. The hybrid tree model (Lu et al., 2008) and the Bayesian tree transducer based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which per"
P15-2121,P06-1115,0,0.466266,"(Lu, 2014) which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The relaxed hybrid tree model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms Introduction This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). One state-of-the-art model for semantic parsing is our recently introduced relaxed hybrid tree model (Lu, 2014), which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. The model allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. It relies on representations called relaxed hybrid trees that can jointly represent both the sentences and semantics. The"
P15-2121,D10-1119,0,0.65948,"er based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the relaxed hybrid tree model (Lu, 2014) which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The relaxed hybrid tree model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms Introduction This paper addresses the problem of parsing natural language sentences into their corresponding semantic r"
P15-2121,J13-2005,0,0.0263387,"tics ma ma ma (w1 w2 ) w3 w4 w5 w6 w7 w8 w9 (w10 ) w1 w2 mb mc mb w3 w4 w5 md w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 (a) mc w10 w7 w8 w6 md mb (w3 w4 w5 ) w6 (w7 w8 ) w9 w9 (b) mc md (w6 ) (w9 ) (c) Figure 1: The semantics-sentence pair (a), an example hybrid tree (b), and an example relaxed hybrid tree (c). Q UERY : answer(R IVER) of supervision is also possible. Clarke et al. (2010) proposed a model that learns a semantic parser for answering questions without relying on semantic annotations. Goldwasser et al. (2011) presented a confidence-driven approach to semantic parsing based on self-training. Liang et al. (2013) introduced semantic parsers based on dependency based semantics (DCS) that map sentences into their denotations. In this work, we focus on parsing sentences into their formal semantic representations. 3 R IVER : exclude(R IVER, R IVER) R IVER : river(all) S TATE : stateid(S TATE NAME) S TATE NAME : (0 tn0 ) What rivers do not run through Tennessee ? Figure 2: An example tree-structured semantic representation (above) and its corresponding natural language sentence (below). Relaxed Hybrid Trees We briefly discuss our previously proposed relaxed hybrid tree model (Lu, 2014) in this section. The"
P15-2121,D08-1082,1,0.718353,"ortant limitation associated with our previous stateof-the-art discriminative semantic parsing model – the relaxed hybrid tree model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/. 1 2 Related Work Semantic parsing has recently attracted a significant amount of attention in the community. In this section, we provide a relatively brief discussion of prior work in semantic parsing. The hybrid tree model (Lu et al., 2008) and the Bayesian tree transducer based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine transl"
P15-2121,D14-1137,1,0.539366,"ated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the relaxed hybrid tree model (Lu, 2014) which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The relaxed hybrid tree model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms Introduction This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mo"
P15-2121,N06-1056,0,0.86746,"ively brief discussion of prior work in semantic parsing. The hybrid tree model (Lu et al., 2008) and the Bayesian tree transducer based model (Jones et al., 2012) are generative frameworks, which essentially assume natural language and semantics are jointly generated from an underlying generative process. Such models are efficient, but are limited in their predictive power due to the simple independence assumptions made. On the other hand, discriminative models are able to exploit arbitrary features and are usually able to give better results. Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the relaxed hybrid tree model (Lu, 2014) which extends the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The relaxed hybrid tree model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under"
P15-2121,P07-1121,0,0.0418851,"ds the generative hybrid tree model. This extension results in a discriminative model that incorporates rich features and allows long-distance dependencies to be captured. The relaxed hybrid tree model has achieved the state-of-the-art results on standard benchmark datasets across different languages. Performing semantic parsing under other forms Introduction This paper addresses the problem of parsing natural language sentences into their corresponding semantic representations in the form of formal logical representations. Such a task is also known as semantic parsing (Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010). One state-of-the-art model for semantic parsing is our recently introduced relaxed hybrid tree model (Lu, 2014), which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference. The model allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured. It relies on representations called relaxed hybrid trees that can jointly represent both the sentences and semantics. The model is essentially di"
P15-2121,Q13-1015,0,\N,Missing
P15-2121,P11-1060,0,\N,Missing
P17-1143,J92-4003,0,0.567085,"ed to train a model for directly predicting token labels, b) A pipeline approach where the NB model from Task 1 is used to filter relevant sentences. A CRF model is then trained to predict token labels for relevant sentences. The CRF model in Approach 1 is trained on the entire training set, whereas the CRF model in Approach 2 is trained only on the gold relevant sentences in the training set. For features in both approaches, we use unigrams and bigrams, part-of-Speech labels from the Stanford POStagger (Toutanova et al., 2003), and Brown clustering features after optimizing the cluster size (Brown et al., 1992). A C++ implementation of the Brown clustering algorithm is 1562 Token Label Entity Action Modifier Average Approach 1 P R F1 48.8 25.1 32.9 55.2 30.3 38.9 55.7 28.4 37.3 51.7 27.0 35.2 Approach 2 P R F1 42.8 33.8 37.6 50.8 41.1 45.2 48.9 37.4 42.1 45.9 36.3 40.3 Token Label Entity Action Modifier Average Approach 1 P R F1 63.6 32.1 42.3 60.2 31.4 41.0 56.4 28.1 37.1 62.7 31.8 41.9 Approach 2 P R F1 56.5 46.3 50.6 54.6 42.8 47.7 50.1 37.1 42.3 55.9 45.3 49.8 Table 4: Task 2 relaxed/token-level scores. Table 3: Task 2 scores: predicting token labels. used (Liang, 2005). The Brown cluster was tr"
P17-1143,S10-1004,0,0.0592651,"Missing"
P17-1143,E99-1023,0,0.280945,"ty. 5.2 Task 2 - Predict token labels Task 2 concerns automating Stage 1 for the annotation process described in Section 3.3. Within the annotated database, we find several cases where a single word-phrase may be annotated with both Subject and Object labels (see Figure 5). In order to simplify the model for prediction, we redefine Task 2 as predicting Entity, Action and Modifier labels for word-phrases. The single Entity label is used to replace both Subject and Object labels. Since the labels may extend beyond a single word token, we use the BIO format for indicating the span of the labels (Sang and Veenstra, 1999). We use two approaches for tackling this task: a) CRF is used to train a model for directly predicting token labels, b) A pipeline approach where the NB model from Task 1 is used to filter relevant sentences. A CRF model is then trained to predict token labels for relevant sentences. The CRF model in Approach 1 is trained on the entire training set, whereas the CRF model in Approach 2 is trained only on the gold relevant sentences in the training set. For features in both approaches, we use unigrams and bigrams, part-of-Speech labels from the Stanford POStagger (Toutanova et al., 2003), and B"
P17-1143,E12-2021,0,0.114051,"Missing"
P17-1143,N03-1033,0,0.067355,"Missing"
P17-1165,C16-1166,1,0.746617,"Missing"
P17-1165,D11-1024,0,0.103245,"Missing"
P17-1165,N13-1019,0,0.0560453,"Missing"
P17-1165,N16-1057,0,0.0262075,"Missing"
P17-1165,N10-1012,0,0.130576,"Missing"
P17-1165,P06-1003,0,0.0268391,"Missing"
P17-1165,P16-1120,0,0.0352724,"Missing"
P17-1165,P06-1124,0,0.182084,"Missing"
P17-1165,P09-2075,0,0.0835396,"Missing"
P17-2007,N06-1056,0,\N,Missing
P17-2007,D07-1071,0,\N,Missing
P17-2007,D08-1082,1,\N,Missing
P17-2007,C14-1122,1,\N,Missing
P18-2085,P11-2049,0,0.0263172,"he negation scope detection task has mostly been regarded as a boundary detection task. Morante et al. (2008) and Morante and Daelemans (2009) tackled the task by building classifiers based on k-nearest neighbors algorithm (Cover and Hart, 1967), SVM (Cortes and Vapnik, 1995) as well as CRF (Lafferty et al., 2001) on each token to determine if it is inside the scope. Li et al. (2010) incorporated more syntactic features such as parse tree information by adopting shallow semantic parsing (Gildea and Palmer, 2002; Punyakanok et al., 2005) for building an SVM classifier. With similar motivation, Apostolova et al. (2011) proposed a rule-based method to extract lexico-syntactic patterns to identify the scope boundaries. To further investigate the syntactic features, Zou et al. (2013) extracted more syntactic information from constituency and dependency trees obtained from parsers to feed into the SVM classifier. Qian et al. (2016) adopted a convolutional neural network based approach (LeCun et al., 1989) to extract position features and syntactic path features encoding the path from the cue to the candidate token along the constituency trees. They also captured relative position information between the words i"
P18-2085,C10-1076,0,0.75274,"egation cues, each of them is annotated separately. The corpus statistics is listed in Table 1. During training and testing, following prior works (Fancellu et al., 2016), only instances with at least one negation cue will be selected. For the sentence containing multiple negation cues, we create as many copies as the number of instances, 1 The official evaluation contains both two versions. We explain the differences between two versions of evaluation in the supplementary material. 535 System This person is alone and can not be approached by letter without a breach of that absolute secrecy . Li et al. (2010) Velldal et al. (2012) Zou et al. (2013) He has been there for ten days, and neither Mr. Warren , nor I , nor the girl has once set eyes upon him. Qian et al. (2016) Linear Semi io Latent io Figure 3: Examples showing incorrect instances in the Linear model but correct in Semi o model (The incorrect predictions by Linear model are underlined). Full Paper F1T F1A P CS 64.0 70.2 61.2 83.5 55.3 80.8 74.0 58.8 83.1 75.1 60.1 79.5 71.0 55.1 F1T 94.4 96.4 97.5 97.3 Clinical F1A P CS 89.8 90.7 85.3 89.7 96.6 93.3 97.1 94.4 97.0 94.1 Table 3: Results on BioScope datasets. rors for the Linear model wit"
P18-2085,P04-3031,0,0.144256,"predictions that are incorrect in Linear model that are correct in the Latent models, we observe that Latent models tend to make more accurate predictions. We found that there is only 1 incorrect prediction from the Latent io that is corrected by the Linear model. This indicates that the Latent io model is able to fix er4.3 Experiments on Model Robustness To understand the robustness of our model, we additionally conducted two sets of experiments. BioScope The BioScope corpus (Szarvas et al., 2008) contains three data collections from medical domains: Abstract, Full Paper and Clinical. NLTK (Bird and Loper, 2004) is used to perform tokenization and POS tagging for preprocessing. Following (Morante and Daelemans, 2009; Qian et al., 2016), we perform 10-fold cross validation on Ab536 System (Zou et al., 2015) Linear Semi io Latent io F1T 89.60 90.78 90.60 Product Review F1A F1B 81.86 69.39 83.49 71.69 83.95 72.43 5 P CS 60.93 69.39 71.69 72.43 Related Work The negation scope extraction task has been studied within the NLP community through the BioScope corpus (Szarvas et al., 2008) in biomedical domain, usually together with the negation cue detection task. The negation scope detection task has mostly b"
P18-2085,S12-1045,0,0.139268,"n existing approaches reported in the literature. 1 There is neither money nor credit in it , and yet one would wish to tidy it up . Figure 1: Two examples with negation cues in bold blue and negation scope in red. tion perspective, aiming to identify whether each word token in the sentence belongs to the negation scope or not. To perform sequence labeling, various approaches have been proposed based on models such as support vector machines (SVMs) (with heuristic rules) (Read et al., 2012; de Albornoz et al., 2012; Packard et al., 2014), conditional random fields (CRF) (Lapponi et al., 2012; Chowdhury and Mahbub, 2012; White, 2012; Zou et al., 2015) and neural networks (Fancellu et al., 2016; Qian et al., 2016). These models typically either make use of external resources for extracting complex syntax and grammar features, or are based on neural architectures such as long shortterm memory networks (LSTM) and convolutional neural networks (CNNs) to extract automatic features. We observe that there are some useful features that can be explicitly and implicitly captured and modelled in the learning process for negation scope extraction. We use the term partial scope to refer to a continuous text span that is"
P18-2085,S12-1035,0,0.59107,"given an input sentence x is defined as: P (wT f (x, y, h)) h exp P P p(y|x) = 0 T 0 y0 ∈Y(x) h0 exp(w f (x, y , h )) each of which has only one negation cue and its corresponding negation scope. The L2 regularization hyper-parameter λ is set to 0.1 based on the development set. We conduct evaluations of negation scope extraction based on metrics at token-level evaluations and scope-level evaluations. There are two versions of evaluation metrics, referred to as version A and version B1 , defined at the scope-level that can be used to measure the performance according to *SEM2012 shared task (Morante and Blanco, 2012). Moreover, to understand the model robustness, we also conducted additional experiments on BioScope (Szarvas et al., 2008) and CNeSP (Zou et al., 2015). 4 Train 847 983 Dev 144 173 Results and Discussion 4.1 Main Results The main results on the CDS-CO corpus are shown in Table 2. PA ., RA . and F1A . are precision, recall and F1 measure under version A, while PB ., RB ., F1B . are for version B. Note that none of the prior works reported results under version B. Moreover, c refers to the cue type features, r refers to relative position of partial scope with respect to the cue. We focus on Lin"
P18-2085,W09-1105,0,0.0454564,"at Latent models tend to make more accurate predictions. We found that there is only 1 incorrect prediction from the Latent io that is corrected by the Linear model. This indicates that the Latent io model is able to fix er4.3 Experiments on Model Robustness To understand the robustness of our model, we additionally conducted two sets of experiments. BioScope The BioScope corpus (Szarvas et al., 2008) contains three data collections from medical domains: Abstract, Full Paper and Clinical. NLTK (Bird and Loper, 2004) is used to perform tokenization and POS tagging for preprocessing. Following (Morante and Daelemans, 2009; Qian et al., 2016), we perform 10-fold cross validation on Ab536 System (Zou et al., 2015) Linear Semi io Latent io F1T 89.60 90.78 90.60 Product Review F1A F1B 81.86 69.39 83.49 71.69 83.95 72.43 5 P CS 60.93 69.39 71.69 72.43 Related Work The negation scope extraction task has been studied within the NLP community through the BioScope corpus (Szarvas et al., 2008) in biomedical domain, usually together with the negation cue detection task. The negation scope detection task has mostly been regarded as a boundary detection task. Morante et al. (2008) and Morante and Daelemans (2009) tackled"
P18-2085,W10-3110,0,0.536497,"e of the negation cue differ in terms of composition of words and their associated syntactic roles in the sentence. Furthermore, the type of cue Introduction Negation is an important linguistic phenomenon (Morante and Sporleder, 2012), which reverts the assertion associated with a proposition. Broadly speaking, the part of the sentence being negated is called negation scope (Huddleston et al., 2002). Automatic negation scope detection is a vital but challenging task that has various applications in areas such as text mining (Szarvas et al., 2008), and sentiment analysis (Wiegand et al., 2010; Councill et al., 2010). Negation scope detection task commonly involves a negation cue which can be one of the following 3 types – either a single word (e.g., not), affixes (e.g., im-, -less) or multiple words (e.g., no longer) expressing negation. Figure 1 presents two real examples for such a task, where the first example involves discontinuous negation scope of an affix cue. The second example shows a discontinuous negation cue and its corresponding discontinuous negation scope. Most existing approaches tackled the negation scope detection problem from a boundary detec533 Proceedings of the 56th Annual Meeting o"
P18-2085,D08-1075,0,0.724742,"gging for preprocessing. Following (Morante and Daelemans, 2009; Qian et al., 2016), we perform 10-fold cross validation on Ab536 System (Zou et al., 2015) Linear Semi io Latent io F1T 89.60 90.78 90.60 Product Review F1A F1B 81.86 69.39 83.49 71.69 83.95 72.43 5 P CS 60.93 69.39 71.69 72.43 Related Work The negation scope extraction task has been studied within the NLP community through the BioScope corpus (Szarvas et al., 2008) in biomedical domain, usually together with the negation cue detection task. The negation scope detection task has mostly been regarded as a boundary detection task. Morante et al. (2008) and Morante and Daelemans (2009) tackled the task by building classifiers based on k-nearest neighbors algorithm (Cover and Hart, 1967), SVM (Cortes and Vapnik, 1995) as well as CRF (Lafferty et al., 2001) on each token to determine if it is inside the scope. Li et al. (2010) incorporated more syntactic features such as parse tree information by adopting shallow semantic parsing (Gildea and Palmer, 2002; Punyakanok et al., 2005) for building an SVM classifier. With similar motivation, Apostolova et al. (2011) proposed a rule-based method to extract lexico-syntactic patterns to identify the sc"
P18-2085,J12-2001,0,0.0662549,"led in the learning process for negation scope extraction. We use the term partial scope to refer to a continuous text span that is part of discontinuous scope, and use the term gap to refer to the text span between two pieces of partial scope. From the first example in Figure 1 we can observe that, with the negation cue as a prefix in a word, the partial scope before, after and in the middle of the negation cue differ in terms of composition of words and their associated syntactic roles in the sentence. Furthermore, the type of cue Introduction Negation is an important linguistic phenomenon (Morante and Sporleder, 2012), which reverts the assertion associated with a proposition. Broadly speaking, the part of the sentence being negated is called negation scope (Huddleston et al., 2002). Automatic negation scope detection is a vital but challenging task that has various applications in areas such as text mining (Szarvas et al., 2008), and sentiment analysis (Wiegand et al., 2010; Councill et al., 2010). Negation scope detection task commonly involves a negation cue which can be one of the following 3 types – either a single word (e.g., not), affixes (e.g., im-, -less) or multiple words (e.g., no longer) expres"
P18-2085,P14-1007,0,0.222451,"atasets demonstrate that our approaches are able to achieve better results than existing approaches reported in the literature. 1 There is neither money nor credit in it , and yet one would wish to tidy it up . Figure 1: Two examples with negation cues in bold blue and negation scope in red. tion perspective, aiming to identify whether each word token in the sentence belongs to the negation scope or not. To perform sequence labeling, various approaches have been proposed based on models such as support vector machines (SVMs) (with heuristic rules) (Read et al., 2012; de Albornoz et al., 2012; Packard et al., 2014), conditional random fields (CRF) (Lapponi et al., 2012; Chowdhury and Mahbub, 2012; White, 2012; Zou et al., 2015) and neural networks (Fancellu et al., 2016; Qian et al., 2016). These models typically either make use of external resources for extracting complex syntax and grammar features, or are based on neural architectures such as long shortterm memory networks (LSTM) and convolutional neural networks (CNNs) to extract automatic features. We observe that there are some useful features that can be explicitly and implicitly captured and modelled in the learning process for negation scope ex"
P18-2085,S12-1037,0,0.517833,"Missing"
P18-2085,W05-0639,0,0.04495,"08) in biomedical domain, usually together with the negation cue detection task. The negation scope detection task has mostly been regarded as a boundary detection task. Morante et al. (2008) and Morante and Daelemans (2009) tackled the task by building classifiers based on k-nearest neighbors algorithm (Cover and Hart, 1967), SVM (Cortes and Vapnik, 1995) as well as CRF (Lafferty et al., 2001) on each token to determine if it is inside the scope. Li et al. (2010) incorporated more syntactic features such as parse tree information by adopting shallow semantic parsing (Gildea and Palmer, 2002; Punyakanok et al., 2005) for building an SVM classifier. With similar motivation, Apostolova et al. (2011) proposed a rule-based method to extract lexico-syntactic patterns to identify the scope boundaries. To further investigate the syntactic features, Zou et al. (2013) extracted more syntactic information from constituency and dependency trees obtained from parsers to feed into the SVM classifier. Qian et al. (2016) adopted a convolutional neural network based approach (LeCun et al., 1989) to extract position features and syntactic path features encoding the path from the cue to the candidate token along the consti"
P18-2085,P16-1047,0,0.594223,"redit in it , and yet one would wish to tidy it up . Figure 1: Two examples with negation cues in bold blue and negation scope in red. tion perspective, aiming to identify whether each word token in the sentence belongs to the negation scope or not. To perform sequence labeling, various approaches have been proposed based on models such as support vector machines (SVMs) (with heuristic rules) (Read et al., 2012; de Albornoz et al., 2012; Packard et al., 2014), conditional random fields (CRF) (Lapponi et al., 2012; Chowdhury and Mahbub, 2012; White, 2012; Zou et al., 2015) and neural networks (Fancellu et al., 2016; Qian et al., 2016). These models typically either make use of external resources for extracting complex syntax and grammar features, or are based on neural architectures such as long shortterm memory networks (LSTM) and convolutional neural networks (CNNs) to extract automatic features. We observe that there are some useful features that can be explicitly and implicitly captured and modelled in the learning process for negation scope extraction. We use the term partial scope to refer to a continuous text span that is part of discontinuous scope, and use the term gap to refer to the text span"
P18-2085,P02-1031,0,0.0694082,"orpus (Szarvas et al., 2008) in biomedical domain, usually together with the negation cue detection task. The negation scope detection task has mostly been regarded as a boundary detection task. Morante et al. (2008) and Morante and Daelemans (2009) tackled the task by building classifiers based on k-nearest neighbors algorithm (Cover and Hart, 1967), SVM (Cortes and Vapnik, 1995) as well as CRF (Lafferty et al., 2001) on each token to determine if it is inside the scope. Li et al. (2010) incorporated more syntactic features such as parse tree information by adopting shallow semantic parsing (Gildea and Palmer, 2002; Punyakanok et al., 2005) for building an SVM classifier. With similar motivation, Apostolova et al. (2011) proposed a rule-based method to extract lexico-syntactic patterns to identify the scope boundaries. To further investigate the syntactic features, Zou et al. (2013) extracted more syntactic information from constituency and dependency trees obtained from parsers to feed into the SVM classifier. Qian et al. (2016) adopted a convolutional neural network based approach (LeCun et al., 1989) to extract position features and syntactic path features encoding the path from the cue to the candid"
P18-2085,D16-1078,0,0.448473,"ne would wish to tidy it up . Figure 1: Two examples with negation cues in bold blue and negation scope in red. tion perspective, aiming to identify whether each word token in the sentence belongs to the negation scope or not. To perform sequence labeling, various approaches have been proposed based on models such as support vector machines (SVMs) (with heuristic rules) (Read et al., 2012; de Albornoz et al., 2012; Packard et al., 2014), conditional random fields (CRF) (Lapponi et al., 2012; Chowdhury and Mahbub, 2012; White, 2012; Zou et al., 2015) and neural networks (Fancellu et al., 2016; Qian et al., 2016). These models typically either make use of external resources for extracting complex syntax and grammar features, or are based on neural architectures such as long shortterm memory networks (LSTM) and convolutional neural networks (CNNs) to extract automatic features. We observe that there are some useful features that can be explicitly and implicitly captured and modelled in the learning process for negation scope extraction. We use the term partial scope to refer to a continuous text span that is part of discontinuous scope, and use the term gap to refer to the text span between two pieces"
P18-2085,S12-1041,0,0.387821,". Extensive experiments on several standard datasets demonstrate that our approaches are able to achieve better results than existing approaches reported in the literature. 1 There is neither money nor credit in it , and yet one would wish to tidy it up . Figure 1: Two examples with negation cues in bold blue and negation scope in red. tion perspective, aiming to identify whether each word token in the sentence belongs to the negation scope or not. To perform sequence labeling, various approaches have been proposed based on models such as support vector machines (SVMs) (with heuristic rules) (Read et al., 2012; de Albornoz et al., 2012; Packard et al., 2014), conditional random fields (CRF) (Lapponi et al., 2012; Chowdhury and Mahbub, 2012; White, 2012; Zou et al., 2015) and neural networks (Fancellu et al., 2016; Qian et al., 2016). These models typically either make use of external resources for extracting complex syntax and grammar features, or are based on neural architectures such as long shortterm memory networks (LSTM) and convolutional neural networks (CNNs) to extract automatic features. We observe that there are some useful features that can be explicitly and implicitly captured and model"
P18-2085,S12-1044,0,0.161267,"ed in the literature. 1 There is neither money nor credit in it , and yet one would wish to tidy it up . Figure 1: Two examples with negation cues in bold blue and negation scope in red. tion perspective, aiming to identify whether each word token in the sentence belongs to the negation scope or not. To perform sequence labeling, various approaches have been proposed based on models such as support vector machines (SVMs) (with heuristic rules) (Read et al., 2012; de Albornoz et al., 2012; Packard et al., 2014), conditional random fields (CRF) (Lapponi et al., 2012; Chowdhury and Mahbub, 2012; White, 2012; Zou et al., 2015) and neural networks (Fancellu et al., 2016; Qian et al., 2016). These models typically either make use of external resources for extracting complex syntax and grammar features, or are based on neural architectures such as long shortterm memory networks (LSTM) and convolutional neural networks (CNNs) to extract automatic features. We observe that there are some useful features that can be explicitly and implicitly captured and modelled in the learning process for negation scope extraction. We use the term partial scope to refer to a continuous text span that is part of disco"
P18-2085,W10-3111,0,0.46024,"after and in the middle of the negation cue differ in terms of composition of words and their associated syntactic roles in the sentence. Furthermore, the type of cue Introduction Negation is an important linguistic phenomenon (Morante and Sporleder, 2012), which reverts the assertion associated with a proposition. Broadly speaking, the part of the sentence being negated is called negation scope (Huddleston et al., 2002). Automatic negation scope detection is a vital but challenging task that has various applications in areas such as text mining (Szarvas et al., 2008), and sentiment analysis (Wiegand et al., 2010; Councill et al., 2010). Negation scope detection task commonly involves a negation cue which can be one of the following 3 types – either a single word (e.g., not), affixes (e.g., im-, -less) or multiple words (e.g., no longer) expressing negation. Figure 1 presents two real examples for such a task, where the first example involves discontinuous negation scope of an affix cue. The second example shows a discontinuous negation cue and its corresponding discontinuous negation scope. Most existing approaches tackled the negation scope detection problem from a boundary detec533 Proceedings of t"
P18-2085,W08-0606,0,0.879916,"a prefix in a word, the partial scope before, after and in the middle of the negation cue differ in terms of composition of words and their associated syntactic roles in the sentence. Furthermore, the type of cue Introduction Negation is an important linguistic phenomenon (Morante and Sporleder, 2012), which reverts the assertion associated with a proposition. Broadly speaking, the part of the sentence being negated is called negation scope (Huddleston et al., 2002). Automatic negation scope detection is a vital but challenging task that has various applications in areas such as text mining (Szarvas et al., 2008), and sentiment analysis (Wiegand et al., 2010; Councill et al., 2010). Negation scope detection task commonly involves a negation cue which can be one of the following 3 types – either a single word (e.g., not), affixes (e.g., im-, -less) or multiple words (e.g., no longer) expressing negation. Figure 1 presents two real examples for such a task, where the first example involves discontinuous negation scope of an affix cue. The second example shows a discontinuous negation cue and its corresponding discontinuous negation scope. Most existing approaches tackled the negation scope detection pro"
P18-2085,D13-1099,0,0.0821313,"separately. The corpus statistics is listed in Table 1. During training and testing, following prior works (Fancellu et al., 2016), only instances with at least one negation cue will be selected. For the sentence containing multiple negation cues, we create as many copies as the number of instances, 1 The official evaluation contains both two versions. We explain the differences between two versions of evaluation in the supplementary material. 535 System This person is alone and can not be approached by letter without a breach of that absolute secrecy . Li et al. (2010) Velldal et al. (2012) Zou et al. (2013) He has been there for ten days, and neither Mr. Warren , nor I , nor the girl has once set eyes upon him. Qian et al. (2016) Linear Semi io Latent io Figure 3: Examples showing incorrect instances in the Linear model but correct in Semi o model (The incorrect predictions by Linear model are underlined). Full Paper F1T F1A P CS 64.0 70.2 61.2 83.5 55.3 80.8 74.0 58.8 83.1 75.1 60.1 79.5 71.0 55.1 F1T 94.4 96.4 97.5 97.3 Clinical F1A P CS 89.8 90.7 85.3 89.7 96.6 93.3 97.1 94.4 97.0 94.1 Table 3: Results on BioScope datasets. rors for the Linear model without producing other wrong predictions."
P18-2085,W00-1308,0,0.321789,"is trained on Abstract, but Full Paper contains much longer sentences with longer negation scope, which presents a challenge for our model as discussed in the previous sections. On the other hand, the baseline systems (Li et al., 2010; Velldal et al., 2012) adopt features from syntactic trees, which allow them to capture long-distance syntactic dependencies. CNeSP To understand how well our model works on another language other than English, we also conducted an experiment on the Product Review collection from the CNeSP corpus (Zou et al., 2015). We used Jieba (Sun, 2012) and Stanford tagger (Toutanova and Manning, 2000) to perform Chinese word segmentation and POS tagging. Following the data splitting scheme described in (Zou et al., 2015), we performed 10fold cross-validation and the results are shown in Table 4. Our model obtains a significantly higher P CS score than the model reported in (Zou et al., 2015). The results further confirm the robustness of our model, showing it is language independent. 6 Conclusion We explored several approaches based on CRF to capture some useful features for solving the task of extracting negation scope based on a given negation cue in a sentence. We conducted extensive ex"
P18-2085,P15-1064,0,0.278884,"erature. 1 There is neither money nor credit in it , and yet one would wish to tidy it up . Figure 1: Two examples with negation cues in bold blue and negation scope in red. tion perspective, aiming to identify whether each word token in the sentence belongs to the negation scope or not. To perform sequence labeling, various approaches have been proposed based on models such as support vector machines (SVMs) (with heuristic rules) (Read et al., 2012; de Albornoz et al., 2012; Packard et al., 2014), conditional random fields (CRF) (Lapponi et al., 2012; Chowdhury and Mahbub, 2012; White, 2012; Zou et al., 2015) and neural networks (Fancellu et al., 2016; Qian et al., 2016). These models typically either make use of external resources for extracting complex syntax and grammar features, or are based on neural architectures such as long shortterm memory networks (LSTM) and convolutional neural networks (CNNs) to extract automatic features. We observe that there are some useful features that can be explicitly and implicitly captured and modelled in the learning process for negation scope extraction. We use the term partial scope to refer to a continuous text span that is part of discontinuous scope, and"
P18-2107,D14-1134,0,0.0153215,"ensional embeddings of such statements with the help of a given logical knowledge base consisting of first-order rules so that the learned representations are consistent with these rules. They adopt stochastic gradient descent (SGD) to conduct optimizations. This work learns distributed representations of logical forms from cross-lingual data based on co-occurrence information without relying on external knowledge bases. 2 3 Related Work 3.1 Many research efforts on semantic parsing have been made, such as mapping sentences into lambda calculus forms based on CCG (Artzi and Zettlemoyer, 2011; Artzi et al., 2014; Kwiatkowski et al., 2011), modeling dependencybased compositional semantics (Liang et al., 2011; Zhang et al., 2017), or transforming sentences into tree structured semantic representations (Lu, 2015; Susanto and Lu, 2017b). With the development of multilingual datasets, systems for multilingual semantic parsing are also developed. Jie and Lu (2014) employed majority voting to combine outputs from different parsers for certain languages to perform multilingual semantic parsing. Susanto and Lu (2017a) presented an extension of one existing neural parser, S EQ 2T REE (Dong and Lapata, 2016), b"
P18-2107,P15-2121,1,0.951021,"semantic parsing – using data annotated for one language to help improve the performance of another lan673 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 673–679 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics resentations for individual semantic unit based on multilingual datasets where semantic representations are annotated with different languages. Such distributed representations capture shared information cross different languages. We extend two existing monolingual semantic parsers (Lu, 2015; Susanto and Lu, 2017b) to incorporate such crosslingual features. To the best of our knowledge, this is the first work that exploits cross-lingual embeddings for logical representations for semantic parsing. Our system is publicly available at http://statnlp.org/research/sp/. produced by a semantic parser or an information extraction system into expressions in tensor calculus. They then learn low-dimensional embeddings of such statements with the help of a given logical knowledge base consisting of first-order rules so that the learned representations are consistent with these rules. They ad"
P18-2107,D15-1198,0,0.0143536,"uted representation useful for semantic parsing, based on multilingual datasets. Figure 1 depicts an instance of such tree-shaped semantic representations, which correspond to the two semantically equivalent sentences in English and German below it. For such structured semantics, we consider each semantic unit separately. We learn distributed repIntroduction Semantic parsing, one of the classic tasks in natural language processing (NLP), has been extensively studied in the past few years (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006, 2007; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi et al., 2015). With the development of datasets annotated in different languages, learning semantic parsers from such multilingual datasets also attracted attention of researchers (Susanto and Lu, 2017a). However, how to make use of such cross-lingual data to perform cross-lingual semantic parsing – using data annotated for one language to help improve the performance of another lan673 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 673–679 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics resentations fo"
P18-2107,D08-1082,1,0.896796,"work involves exploiting distributed output representations for improved structured predictions, which is in line with works of (Srikumar and Manning, 2014; Rockt¨aschel et al., 2014; Xiao and Guo, 2015). The work of (Rockt¨aschel et al., 2014) is perhaps the most related to this research. The authors first map first-order logical statements Approach Semantic Parser In this work, we build our model and conduct experiments on top of the discriminative hybrid tree semantic parser (Lu, 2014, 2015). The parser was designed based on the hybrid tree representation (HT- G) originally introduced in (Lu et al., 2008). The hybrid tree is a joint representation encoding both sentence and semantics that aims to capture the interactions between words and semantic units. A discriminative hybrid tree (HT- D) (Lu, 2014, 2015) learns the optimal latent wordsemantics correspondence where every word in the input sentence is associated with a semantic unit. Such a model allows us to incorporate rich features and long-range dependencies. Recently, Susanto and Lu (2017b) extended HT- D by attaching neural architectures, resulting in their neural hybrid tree (HT- D ( NN )). Since the correct correspondence between word"
P18-2107,D11-1039,0,0.0245479,"ulus. They then learn low-dimensional embeddings of such statements with the help of a given logical knowledge base consisting of first-order rules so that the learned representations are consistent with these rules. They adopt stochastic gradient descent (SGD) to conduct optimizations. This work learns distributed representations of logical forms from cross-lingual data based on co-occurrence information without relying on external knowledge bases. 2 3 Related Work 3.1 Many research efforts on semantic parsing have been made, such as mapping sentences into lambda calculus forms based on CCG (Artzi and Zettlemoyer, 2011; Artzi et al., 2014; Kwiatkowski et al., 2011), modeling dependencybased compositional semantics (Liang et al., 2011; Zhang et al., 2017), or transforming sentences into tree structured semantic representations (Lu, 2015; Susanto and Lu, 2017b). With the development of multilingual datasets, systems for multilingual semantic parsing are also developed. Jie and Lu (2014) employed majority voting to combine outputs from different parsers for certain languages to perform multilingual semantic parsing. Susanto and Lu (2017a) presented an extension of one existing neural parser, S EQ 2T REE (Dong"
P18-2107,P07-1005,0,0.0277783,"presentations as features. Experiments show that our proposed approach is able to yield improved semantic parsing results on the standard multilingual GeoQuery dataset. RIVER: exclude (RIVER, RIVER) RIVER : state (all) RIVER : traverse (STATE) STATE : stateid (STATENAME) STATENAME : (0 texas0 ) English: which rivers do not run through texas ? German: welche fl¨usse fliessen nicht durch texas ? Figure 1: An example of two semantically equivalent sentences (below) and their tree-shaped semantic representation (above). guage remains a research question that is largely under-explored. Prior work (Chan et al., 2007) shows that semantically equivalent words coming from different languages may contain shared semantic level information, which will be helpful for certain semantic processing tasks. In this work, we propose a simple method to learn the distributed representations for output structured semantic representations which allow us to capture cross-lingual features. Specifically, following previous work (Wong and Mooney, 2006; Jones et al., 2012; Susanto and Lu, 2017b), we adopt a commonly used tree-shaped form as the underlying meaning representation where each tree node is a semantic unit. Our objec"
P18-2107,P17-1135,0,0.0184807,"ne existing neural parser, S EQ 2T REE (Dong and Lapata, 2016), by developing a shared attention mechanism for different languages to conduct multilingual semantic parsing. Such a model allows two types of input signals: single source SL-S INGLE and multi-source SL-M ULTI. However, semantic parsing with cross-lingual features has not been explored, while many recent works in various NLP tasks show the effectiveness of shared information cross different languages. Examples include semantic role labeling (Kozhevnikov and Titov, 2013), information extraction (Wang et al., 2013; Pan et al., 2017; Ni et al., 2017), and question answering (Joty et al., 2017), which motivate this work. Our work involves exploiting distributed output representations for improved structured predictions, which is in line with works of (Srikumar and Manning, 2014; Rockt¨aschel et al., 2014; Xiao and Guo, 2015). The work of (Rockt¨aschel et al., 2014) is perhaps the most related to this research. The authors first map first-order logical statements Approach Semantic Parser In this work, we build our model and conduct experiments on top of the discriminative hybrid tree semantic parser (Lu, 2014, 2015). The parser was designed"
P18-2107,P16-1004,0,0.216411,"2011; Artzi et al., 2014; Kwiatkowski et al., 2011), modeling dependencybased compositional semantics (Liang et al., 2011; Zhang et al., 2017), or transforming sentences into tree structured semantic representations (Lu, 2015; Susanto and Lu, 2017b). With the development of multilingual datasets, systems for multilingual semantic parsing are also developed. Jie and Lu (2014) employed majority voting to combine outputs from different parsers for certain languages to perform multilingual semantic parsing. Susanto and Lu (2017a) presented an extension of one existing neural parser, S EQ 2T REE (Dong and Lapata, 2016), by developing a shared attention mechanism for different languages to conduct multilingual semantic parsing. Such a model allows two types of input signals: single source SL-S INGLE and multi-source SL-M ULTI. However, semantic parsing with cross-lingual features has not been explored, while many recent works in various NLP tasks show the effectiveness of shared information cross different languages. Examples include semantic role labeling (Kozhevnikov and Titov, 2013), information extraction (Wang et al., 2013; Pan et al., 2017; Ni et al., 2017), and question answering (Joty et al., 2017),"
P18-2107,P17-1178,0,0.0143534,"an extension of one existing neural parser, S EQ 2T REE (Dong and Lapata, 2016), by developing a shared attention mechanism for different languages to conduct multilingual semantic parsing. Such a model allows two types of input signals: single source SL-S INGLE and multi-source SL-M ULTI. However, semantic parsing with cross-lingual features has not been explored, while many recent works in various NLP tasks show the effectiveness of shared information cross different languages. Examples include semantic role labeling (Kozhevnikov and Titov, 2013), information extraction (Wang et al., 2013; Pan et al., 2017; Ni et al., 2017), and question answering (Joty et al., 2017), which motivate this work. Our work involves exploiting distributed output representations for improved structured predictions, which is in line with works of (Srikumar and Manning, 2014; Rockt¨aschel et al., 2014; Xiao and Guo, 2015). The work of (Rockt¨aschel et al., 2014) is perhaps the most related to this research. The authors first map first-order logical statements Approach Semantic Parser In this work, we build our model and conduct experiments on top of the discriminative hybrid tree semantic parser (Lu, 2014, 2015). The p"
P18-2107,K17-1038,0,0.0951715,"Missing"
P18-2107,W14-2409,0,0.0695587,"Missing"
P18-2107,C14-1122,1,0.811688,"co-occurrence information without relying on external knowledge bases. 2 3 Related Work 3.1 Many research efforts on semantic parsing have been made, such as mapping sentences into lambda calculus forms based on CCG (Artzi and Zettlemoyer, 2011; Artzi et al., 2014; Kwiatkowski et al., 2011), modeling dependencybased compositional semantics (Liang et al., 2011; Zhang et al., 2017), or transforming sentences into tree structured semantic representations (Lu, 2015; Susanto and Lu, 2017b). With the development of multilingual datasets, systems for multilingual semantic parsing are also developed. Jie and Lu (2014) employed majority voting to combine outputs from different parsers for certain languages to perform multilingual semantic parsing. Susanto and Lu (2017a) presented an extension of one existing neural parser, S EQ 2T REE (Dong and Lapata, 2016), by developing a shared attention mechanism for different languages to conduct multilingual semantic parsing. Such a model allows two types of input signals: single source SL-S INGLE and multi-source SL-M ULTI. However, semantic parsing with cross-lingual features has not been explored, while many recent works in various NLP tasks show the effectiveness"
P18-2107,P12-1051,0,0.870058,"ivalent sentences (below) and their tree-shaped semantic representation (above). guage remains a research question that is largely under-explored. Prior work (Chan et al., 2007) shows that semantically equivalent words coming from different languages may contain shared semantic level information, which will be helpful for certain semantic processing tasks. In this work, we propose a simple method to learn the distributed representations for output structured semantic representations which allow us to capture cross-lingual features. Specifically, following previous work (Wong and Mooney, 2006; Jones et al., 2012; Susanto and Lu, 2017b), we adopt a commonly used tree-shaped form as the underlying meaning representation where each tree node is a semantic unit. Our objective is to learn for each semantic unit a distributed representation useful for semantic parsing, based on multilingual datasets. Figure 1 depicts an instance of such tree-shaped semantic representations, which correspond to the two semantically equivalent sentences in English and German below it. For such structured semantics, we consider each semantic unit separately. We learn distributed repIntroduction Semantic parsing, one of the cl"
P18-2107,K17-1024,0,0.0662118,"Missing"
P18-2107,P17-2007,1,0.539771,"elow) and their tree-shaped semantic representation (above). guage remains a research question that is largely under-explored. Prior work (Chan et al., 2007) shows that semantically equivalent words coming from different languages may contain shared semantic level information, which will be helpful for certain semantic processing tasks. In this work, we propose a simple method to learn the distributed representations for output structured semantic representations which allow us to capture cross-lingual features. Specifically, following previous work (Wong and Mooney, 2006; Jones et al., 2012; Susanto and Lu, 2017b), we adopt a commonly used tree-shaped form as the underlying meaning representation where each tree node is a semantic unit. Our objective is to learn for each semantic unit a distributed representation useful for semantic parsing, based on multilingual datasets. Figure 1 depicts an instance of such tree-shaped semantic representations, which correspond to the two semantically equivalent sentences in English and German below it. For such structured semantics, we consider each semantic unit separately. We learn distributed repIntroduction Semantic parsing, one of the classic tasks in natural"
P18-2107,D11-1140,0,0.530608,"ch semantic unit a distributed representation useful for semantic parsing, based on multilingual datasets. Figure 1 depicts an instance of such tree-shaped semantic representations, which correspond to the two semantically equivalent sentences in English and German below it. For such structured semantics, we consider each semantic unit separately. We learn distributed repIntroduction Semantic parsing, one of the classic tasks in natural language processing (NLP), has been extensively studied in the past few years (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006, 2007; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi et al., 2015). With the development of datasets annotated in different languages, learning semantic parsers from such multilingual datasets also attracted attention of researchers (Susanto and Lu, 2017a). However, how to make use of such cross-lingual data to perform cross-lingual semantic parsing – using data annotated for one language to help improve the performance of another lan673 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 673–679 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Lingui"
P18-2107,P13-1063,0,0.0227538,"u (2017a) presented an extension of one existing neural parser, S EQ 2T REE (Dong and Lapata, 2016), by developing a shared attention mechanism for different languages to conduct multilingual semantic parsing. Such a model allows two types of input signals: single source SL-S INGLE and multi-source SL-M ULTI. However, semantic parsing with cross-lingual features has not been explored, while many recent works in various NLP tasks show the effectiveness of shared information cross different languages. Examples include semantic role labeling (Kozhevnikov and Titov, 2013), information extraction (Wang et al., 2013; Pan et al., 2017; Ni et al., 2017), and question answering (Joty et al., 2017), which motivate this work. Our work involves exploiting distributed output representations for improved structured predictions, which is in line with works of (Srikumar and Manning, 2014; Rockt¨aschel et al., 2014; Xiao and Guo, 2015). The work of (Rockt¨aschel et al., 2014) is perhaps the most related to this research. The authors first map first-order logical statements Approach Semantic Parser In this work, we build our model and conduct experiments on top of the discriminative hybrid tree semantic parser (Lu,"
P18-2107,D10-1119,0,0.183106,"that the shared information cross different languages could guide the model so that it can make more accurate predictions, eliminating certain semantic level ambiguities associated with the semantic units. This is exemplified by a real instance from the English portion of the dataset: Results We compare our models against different existing systems, especially the two baselines HT- D (Lu, 2015) and HT- D ( NN ) (Susanto and Lu, 2017b) with different word window sizes J ∈ {0, 1, 2}. WASP (Wong and Mooney, 2006) is a semantic parser based on statistical phrase-based machine translation. UBL- S (Kwiatkowski et al., 2010) induced probabilistic CCG grammars with higherorder unification that allowed to construct general logical forms for input sentences. TREETRANS (Jones et al., 2012) is built based on a Bayesian inference framework. We run WASP, UBL- S, HT- G, UBL- S, S EQ 2T REE and SL-S INGLE 1 for comparisons. Note that there exist multiple versions of logical representations used in the G EO Q UERY dataset. Specifically, one version is based on lambda calculus expression, and the other is based on the variable free tree-shaped represenInput: Gold: Output: Output (+ O ): 1 Note that in Dong and Lapata (2016)"
P18-2107,N06-1056,0,0.380735,"of two semantically equivalent sentences (below) and their tree-shaped semantic representation (above). guage remains a research question that is largely under-explored. Prior work (Chan et al., 2007) shows that semantically equivalent words coming from different languages may contain shared semantic level information, which will be helpful for certain semantic processing tasks. In this work, we propose a simple method to learn the distributed representations for output structured semantic representations which allow us to capture cross-lingual features. Specifically, following previous work (Wong and Mooney, 2006; Jones et al., 2012; Susanto and Lu, 2017b), we adopt a commonly used tree-shaped form as the underlying meaning representation where each tree node is a semantic unit. Our objective is to learn for each semantic unit a distributed representation useful for semantic parsing, based on multilingual datasets. Figure 1 depicts an instance of such tree-shaped semantic representations, which correspond to the two semantically equivalent sentences in English and German below it. For such structured semantics, we consider each semantic unit separately. We learn distributed repIntroduction Semantic pa"
P18-2107,P07-1121,0,0.243085,"Missing"
P18-2107,P11-1060,0,0.429386,"e is to learn for each semantic unit a distributed representation useful for semantic parsing, based on multilingual datasets. Figure 1 depicts an instance of such tree-shaped semantic representations, which correspond to the two semantically equivalent sentences in English and German below it. For such structured semantics, we consider each semantic unit separately. We learn distributed repIntroduction Semantic parsing, one of the classic tasks in natural language processing (NLP), has been extensively studied in the past few years (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006, 2007; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi et al., 2015). With the development of datasets annotated in different languages, learning semantic parsers from such multilingual datasets also attracted attention of researchers (Susanto and Lu, 2017a). However, how to make use of such cross-lingual data to perform cross-lingual semantic parsing – using data annotated for one language to help improve the performance of another lan673 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 673–679 c Melbourne, Australia, July 15 - 20, 2018. 2018 Associatio"
P18-2107,C69-0101,0,0.406782,"Missing"
P18-2107,P15-2086,0,0.0219984,"wever, semantic parsing with cross-lingual features has not been explored, while many recent works in various NLP tasks show the effectiveness of shared information cross different languages. Examples include semantic role labeling (Kozhevnikov and Titov, 2013), information extraction (Wang et al., 2013; Pan et al., 2017; Ni et al., 2017), and question answering (Joty et al., 2017), which motivate this work. Our work involves exploiting distributed output representations for improved structured predictions, which is in line with works of (Srikumar and Manning, 2014; Rockt¨aschel et al., 2014; Xiao and Guo, 2015). The work of (Rockt¨aschel et al., 2014) is perhaps the most related to this research. The authors first map first-order logical statements Approach Semantic Parser In this work, we build our model and conduct experiments on top of the discriminative hybrid tree semantic parser (Lu, 2014, 2015). The parser was designed based on the hybrid tree representation (HT- G) originally introduced in (Lu et al., 2008). The hybrid tree is a joint representation encoding both sentence and semantics that aims to capture the interactions between words and semantic units. A discriminative hybrid tree (HT- D"
P18-2107,D17-1125,0,0.0127406,"s so that the learned representations are consistent with these rules. They adopt stochastic gradient descent (SGD) to conduct optimizations. This work learns distributed representations of logical forms from cross-lingual data based on co-occurrence information without relying on external knowledge bases. 2 3 Related Work 3.1 Many research efforts on semantic parsing have been made, such as mapping sentences into lambda calculus forms based on CCG (Artzi and Zettlemoyer, 2011; Artzi et al., 2014; Kwiatkowski et al., 2011), modeling dependencybased compositional semantics (Liang et al., 2011; Zhang et al., 2017), or transforming sentences into tree structured semantic representations (Lu, 2015; Susanto and Lu, 2017b). With the development of multilingual datasets, systems for multilingual semantic parsing are also developed. Jie and Lu (2014) employed majority voting to combine outputs from different parsers for certain languages to perform multilingual semantic parsing. Susanto and Lu (2017a) presented an extension of one existing neural parser, S EQ 2T REE (Dong and Lapata, 2016), by developing a shared attention mechanism for different languages to conduct multilingual semantic parsing. Such a mod"
P18-2107,D14-1137,1,\N,Missing
P19-1024,P05-1061,0,0.0852399,"ular, each node only attends to its neighbors in GATs whereas AGGCNs measure the relatedness among all nodes. The network topology in GATs remains the same, while fully connected graphs will be built in AGGCNs to capture long-range semantic interactions. Related Work Our work builds on a rich line of recent efforts on relation extraction models and graph convolutional networks. Relation Extraction. Early research efforts are based on statistical methods. Tree-based kernels (Zelenko et al., 2002) and dependency path-based kernels (Bunescu and Mooney, 2005) are explored to extract the relation. McDonald et al. (2005) construct maximal cliques of entities to predict relations. Mintz et al. (2009) include syntactic features to a statistical classifier. Recently, sequencebased models leverages different neural networks to extract relations, including convolutional neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016), recurrent neural networks (Zhou et al., 2016; Zhang et al., 2017) the combination of both (Vu et al., 2016) and transformer (Verga et al., 2018). Dependency-based approaches also try to incorporate structural information into the neural models. Peng et al. (2017) fir"
P19-1024,D17-1209,0,0.0666208,"Missing"
P19-1024,P09-1113,0,0.104828,"latedness among all nodes. The network topology in GATs remains the same, while fully connected graphs will be built in AGGCNs to capture long-range semantic interactions. Related Work Our work builds on a rich line of recent efforts on relation extraction models and graph convolutional networks. Relation Extraction. Early research efforts are based on statistical methods. Tree-based kernels (Zelenko et al., 2002) and dependency path-based kernels (Bunescu and Mooney, 2005) are explored to extract the relation. McDonald et al. (2005) construct maximal cliques of entities to predict relations. Mintz et al. (2009) include syntactic features to a statistical classifier. Recently, sequencebased models leverages different neural networks to extract relations, including convolutional neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016), recurrent neural networks (Zhou et al., 2016; Zhang et al., 2017) the combination of both (Vu et al., 2016) and transformer (Verga et al., 2018). Dependency-based approaches also try to incorporate structural information into the neural models. Peng et al. (2017) first split the dependency graph into two DAGs, then extend the tree LSTM model (Ta"
P19-1024,P16-1105,0,0.397154,", luwei@sutd.edu.sg Abstract 2014; Wang et al., 2016), whereas dependencybased models incorporate dependency trees into the models (Bunescu and Mooney, 2005; Peng et al., 2017). Compared to sequence-based models, dependency-based models are able to capture non-local syntactic relations that are obscure from the surface form alone (Zhang et al., 2018). Various pruning strategies are also proposed to distill the dependency information in order to further improve the performance. Xu et al. (2015b,c) apply neural networks only on the shortest dependency path between the entities in the full tree. Miwa and Bansal (2016) reduce the full tree to the subtree below the lowest common ancestor (LCA) of the entities. Zhang et al. (2018) apply graph convolutional networks (GCNs) (Kipf and Welling, 2017) model over a pruned tree. This tree includes tokens that are up to distance K away from the dependency path in the LCA subtree. However, rule-based pruning strategies might eliminate some important information in the full tree. Figure 1 shows an example in cross-sentence n-ary relation extraction that the key tokens partial response would be excluded if the model only takes the pruned tree into consideration. Ideally"
P19-1024,H05-1091,0,0.563517,"heir motivations and network structures are different. In particular, each node only attends to its neighbors in GATs whereas AGGCNs measure the relatedness among all nodes. The network topology in GATs remains the same, while fully connected graphs will be built in AGGCNs to capture long-range semantic interactions. Related Work Our work builds on a rich line of recent efforts on relation extraction models and graph convolutional networks. Relation Extraction. Early research efforts are based on statistical methods. Tree-based kernels (Zelenko et al., 2002) and dependency path-based kernels (Bunescu and Mooney, 2005) are explored to extract the relation. McDonald et al. (2005) construct maximal cliques of entities to predict relations. Mintz et al. (2009) include syntactic features to a statistical classifier. Recently, sequencebased models leverages different neural networks to extract relations, including convolutional neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016), recurrent neural networks (Zhou et al., 2016; Zhang et al., 2017) the combination of both (Vu et al., 2016) and transformer (Verga et al., 2018). Dependency-based approaches also try to incorporate structur"
P19-1024,W15-1506,0,0.0377479,"efforts on relation extraction models and graph convolutional networks. Relation Extraction. Early research efforts are based on statistical methods. Tree-based kernels (Zelenko et al., 2002) and dependency path-based kernels (Bunescu and Mooney, 2005) are explored to extract the relation. McDonald et al. (2005) construct maximal cliques of entities to predict relations. Mintz et al. (2009) include syntactic features to a statistical classifier. Recently, sequencebased models leverages different neural networks to extract relations, including convolutional neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016), recurrent neural networks (Zhou et al., 2016; Zhang et al., 2017) the combination of both (Vu et al., 2016) and transformer (Verga et al., 2018). Dependency-based approaches also try to incorporate structural information into the neural models. Peng et al. (2017) first split the dependency graph into two DAGs, then extend the tree LSTM model (Tai et al., 2015) over these two graphs for n-ary relation extraction. Closest to our work, Song et al. (2018b) use graph recurrent networks (Song et al., 2018a) to directly encode the whole dependency graph without breaking it. The"
P19-1024,D16-1053,0,0.03674,"oops are omitted for simplification). Numbers near the edges represent the weights in the matrix. Resulting matrices are fed into N separate densely connected layers, generating new representations. Top left shows an example of the densely connected layer, where the number (L) of sub-layers is 3 (L is a hyper-parameter). Each sub-layer concatenates all preceding outputs as the input. Eventually, a linear combination is applied to combine outputs of N densely connected layers into hidden representations. 2.2 ˜ can be constructed fully connected graph G(1) . A by using self-attention mechanism (Cheng et al., 2016), which is an attention mechanism (Bahdanau et al., 2015) that captures the interactions between two arbitrary positions of a single se˜ we can use it as the inquence. Once we get A, put for the computation of the later graph convolu˜ is the same as tional layer. Note that the size of A the original adjacency matrix A (n × n). Therefore, no additional computational overhead is involved. The key idea behind the attention guided layer is to use attention for inducing relations between nodes, especially for those connected by indirect, multi-hop paths. These soft relations can be captured by diff"
P19-1024,Q17-1008,0,0.611973,"cross-sentence n-ary relation extraction task. For the sentence-level relation extraction task, we report the micro-averaged F1 scores for the TACRED dataset and the macro-averaged F1 scores for the SemEval dataset (Zhang et al., 2018). (8) where hf inal will be taken as inputs to a logistic regression classifier to make a prediction. 3 3.1 Setup Experiments Data We evaluate the performance of our model on two tasks, namely, cross-sentence n-ary relation extraction and sentence-level relation extraction. For the cross-sentence n-ary relation extraction task, we use the dataset introduced in (Peng et al., 2017), which contains 6,987 ternary relation instances and 6,087 binary relation instances extracted from PubMed.4 Most instances contain multiple sentences and each instance is assigned with one of the five labels, including “resistance or nonresponse”, “sensitivity”, “response”, “resistance” and “none”. We consider two specific tasks for evaluation, i,e., binary-class n-ary relation extraction and multi-class n-ary relation extraction. For binary-class n-ary relation extraction, we follow (Peng et al., 2017) to binarize multi-class labels by grouping the four relation classes as “yes” and treatin"
P19-1024,D14-1162,0,0.0910581,"018b)4 , while for the sentence-level relation extraction task, we use the same development set from (Zhang et al., 2018)5 . We choose the number of heads N for attention guided layer from {1, 2, 3, 4}, the block number M from {1, 2, 3}, the number of sublayers L in each densely connected layer from {2, 3, 4, 5, 6}. Through preliminary experiments on the development sets, we find that the combinations (N =2, M =2, L=5, dhidden =340) and (N =3, M =2, L=5, dhidden =300) give the best results on cross-sentence n-ary relation extraction and sentence-level relation extraction, respectively. GloVe (Pennington et al., 2014)6 vectors are used as the initialization for word embeddings. Models are evaluated using the same metrics as previous work (Song et al., 2018b; Zhang et al., 2018). We report the test accuracy averaged over five cross validation folds (Song et al., 2018b) for the cross-sentence n-ary relation extraction task. For the sentence-level relation extraction task, we report the micro-averaged F1 scores for the TACRED dataset and the macro-averaged F1 scores for the SemEval dataset (Zhang et al., 2018). (8) where hf inal will be taken as inputs to a logistic regression classifier to make a prediction."
P19-1024,Q19-1019,1,0.81783,"ngly, we modify the computation of each layer as follows (for the t-th ˜ (t) ): matrix A 2.3 2.4 (l) hti = ρ (1) (l−1) ].  (4) where t = 1, ..., N and t selects the weight matrix and bias term associated with the attention ˜ (t) . The column diguided adjacency matrix A mension of the weight matrix increases by dhidden (l) (l) per sub-layer, i.e., Wt ∈ Rdhidden ×d , where d(l) = d + dhidden × (l − 1). Unlike previous pruning strategies, which lead to a resulting structure that is smaller than the original structure, our attention guided layer outputs a larger fully connected graph. Following (Guo et al., 2019), we introduce dense connections (Huang et al., 2017) into the AGGCN model in order to capture more structural information on large graphs. With the help of dense connections, we are able to train a deeper model, allowing rich local and non-local information to be captured for learning a better graph representation. Dense connectivity is shown in Figure 2. Direct connections are introduced from any layer to all its preceding layers. Mathematically, we first define (l) gj as the concatenation of the initial node representation and the node representations produced in layers 1, · · · , l − 1: (l"
P19-1024,E17-1110,0,0.148055,"ly attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches. 1 Introduction Relation extraction aims to detect relations among entities in the text. It plays a significant role in a variety of natural language processing applications including biomedical knowledge discovery (Quirk and Poon, 2017), knowledge base population (Zhang et al., 2017) and question answering (Yu et al., 2017). Figure 1 shows an example about expressing a relation sensitivity among three entities L858E, EGFR and gefitinib in two sentences. Most existing relation extraction models can be categorized into two classes: sequence-based and dependency-based. Sequence-based models operate only on the word sequences (Zeng et al., ∗ Equally Contributed. 241 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 241–251 c Florence, Italy, July 28 - August 2, 2019. 2019 Association"
P19-1024,S10-1057,0,0.210389,"Missing"
P19-1024,S10-1006,0,0.0767778,"Missing"
P19-1024,P18-1150,0,0.397944,") 244 (7) where hei indicates the hidden representation corresponding to the i-th entity.3 Entity representations will be concatenated with sentence representation to form a new representation. Following (Zhang et al., 2018), we apply a feed-forward neural network (FFNN) over the concatenated representations inspired by relational reasoning works (Santoro et al., 2017; Lee et al., 2017): hf inal = FFNN([hsent ; he1 ; ...hei ]) 3.2 We tune the hyper-parameters according to results on the development sets. For the cross-sentence nary relation extraction task, we use the same data split used in (Song et al., 2018b)4 , while for the sentence-level relation extraction task, we use the same development set from (Zhang et al., 2018)5 . We choose the number of heads N for attention guided layer from {1, 2, 3, 4}, the block number M from {1, 2, 3}, the number of sublayers L in each densely connected layer from {2, 3, 4, 5, 6}. Through preliminary experiments on the development sets, we find that the combinations (N =2, M =2, L=5, dhidden =340) and (N =3, M =2, L=5, dhidden =300) give the best results on cross-sentence n-ary relation extraction and sentence-level relation extraction, respectively. GloVe (Pen"
P19-1024,D18-1246,0,0.239827,") 244 (7) where hei indicates the hidden representation corresponding to the i-th entity.3 Entity representations will be concatenated with sentence representation to form a new representation. Following (Zhang et al., 2018), we apply a feed-forward neural network (FFNN) over the concatenated representations inspired by relational reasoning works (Santoro et al., 2017; Lee et al., 2017): hf inal = FFNN([hsent ; he1 ; ...hei ]) 3.2 We tune the hyper-parameters according to results on the development sets. For the cross-sentence nary relation extraction task, we use the same data split used in (Song et al., 2018b)4 , while for the sentence-level relation extraction task, we use the same development set from (Zhang et al., 2018)5 . We choose the number of heads N for attention guided layer from {1, 2, 3, 4}, the block number M from {1, 2, 3}, the number of sublayers L in each densely connected layer from {2, 3, 4, 5, 6}. Through preliminary experiments on the development sets, we find that the combinations (N =2, M =2, L=5, dhidden =340) and (N =3, M =2, L=5, dhidden =300) give the best results on cross-sentence n-ary relation extraction and sentence-level relation extraction, respectively. GloVe (Pen"
P19-1024,D17-1018,0,0.0261306,". f : Rd×n → Rd×1 is a max pooling function that maps from n output vectors to 1 sentence vector. Similarly, we can obtain the entity representations. For the i-th entity, its representation hei can be computed as: hei = f (hei ) 244 (7) where hei indicates the hidden representation corresponding to the i-th entity.3 Entity representations will be concatenated with sentence representation to form a new representation. Following (Zhang et al., 2018), we apply a feed-forward neural network (FFNN) over the concatenated representations inspired by relational reasoning works (Santoro et al., 2017; Lee et al., 2017): hf inal = FFNN([hsent ; he1 ; ...hei ]) 3.2 We tune the hyper-parameters according to results on the development sets. For the cross-sentence nary relation extraction task, we use the same data split used in (Song et al., 2018b)4 , while for the sentence-level relation extraction task, we use the same development set from (Zhang et al., 2018)5 . We choose the number of heads N for attention guided layer from {1, 2, 3, 4}, the block number M from {1, 2, 3}, the number of sublayers L in each densely connected layer from {2, 3, 4, 5, 6}. Through preliminary experiments on the development sets,"
P19-1024,P15-1150,0,0.201004,"Missing"
P19-1024,P15-2047,0,0.0886337,"5) over these two graphs for n-ary relation extraction. Closest to our work, Song et al. (2018b) use graph recurrent networks (Song et al., 2018a) to directly encode the whole dependency graph without breaking it. The contrast between our model and theirs is reminiscent of the contrast between CNN and RNN. Various pruning strategies have also been proposed to distill the dependency information in order to further improve the performance. Xu et al. (2015b,c) adapt neural models to encode the shortest dependency path. Miwa and Bansal (2016) apply LSTM model over the LCA subtree of two entities. Liu et al. (2015) combine the shortest dependency path and the dependency subtree. Zhang et al. (2018) adopt a path-centric pruning strategy. Unlike these strategies that remove edges in preprocessing, our model learns to assign each edge a different weight 5 Conclusion We introduce the novel Attention Guided Graph Convolutional Networks (AGGCNs). Experimental results show that AGGCNs achieve state-ofthe-art results on various relation extraction tasks. Unlike previous approaches, AGGCNs operate directly on the full tree and learn to distill the useful information from it in an end-to-end fashion. There are mu"
P19-1024,D17-1159,0,0.0765289,"select and discard information. Combining with dense connections, our AGGCN model is able to learn a better graph representation. • Our model achieves new state-of-the-art results without additional computational over1 2 Attention Guided GCNs In this section, we will present the basic components used for constructing our AGGCN model. 2.1 GCNs GCNs are neural networks that operate directly on graph structures (Kipf and Welling, 2017). Here we mathematically illustrate how multi-layer GCNs work on a graph. Given a graph with n nodes, we can represent the graph with an n × n adjacency matrix A. Marcheggiani and Titov (2017) extend GCNs for encoding dependency trees by incorporating directionality of edges into the model. They add a self-loop for each node in the tree. Opposite direction of a dependency arc is also included, which means Aij = 1 and Aji = 1 if there is an edge going from node i to node j, otherwise Aij = 0 and Aji = 0. The convolution computation for node i at the l-th layer, which takes the input feature representation h(l−1) as in(l) put and outputs the induced representation hi , can be defined as: n X  (l) (l−1) hi = ρ Aij W(l) hj + b(l) (1) j=1 where W(l) is the weight matrix, b(l) is the b"
P19-1024,N18-1080,0,0.109441,"elenko et al., 2002) and dependency path-based kernels (Bunescu and Mooney, 2005) are explored to extract the relation. McDonald et al. (2005) construct maximal cliques of entities to predict relations. Mintz et al. (2009) include syntactic features to a statistical classifier. Recently, sequencebased models leverages different neural networks to extract relations, including convolutional neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016), recurrent neural networks (Zhou et al., 2016; Zhang et al., 2017) the combination of both (Vu et al., 2016) and transformer (Verga et al., 2018). Dependency-based approaches also try to incorporate structural information into the neural models. Peng et al. (2017) first split the dependency graph into two DAGs, then extend the tree LSTM model (Tai et al., 2015) over these two graphs for n-ary relation extraction. Closest to our work, Song et al. (2018b) use graph recurrent networks (Song et al., 2018a) to directly encode the whole dependency graph without breaking it. The contrast between our model and theirs is reminiscent of the contrast between CNN and RNN. Various pruning strategies have also been proposed to distill the dependency"
P19-1024,N16-1065,0,0.0481246,"Missing"
P19-1024,P16-1123,0,0.254015,"Missing"
P19-1024,D15-1062,0,0.435265,"ei Lu StatNLP Research Group Singapore University of Technology and Design {zhijiang guo,yan zhang}@mymail.sutd.edu.sg, luwei@sutd.edu.sg Abstract 2014; Wang et al., 2016), whereas dependencybased models incorporate dependency trees into the models (Bunescu and Mooney, 2005; Peng et al., 2017). Compared to sequence-based models, dependency-based models are able to capture non-local syntactic relations that are obscure from the surface form alone (Zhang et al., 2018). Various pruning strategies are also proposed to distill the dependency information in order to further improve the performance. Xu et al. (2015b,c) apply neural networks only on the shortest dependency path between the entities in the full tree. Miwa and Bansal (2016) reduce the full tree to the subtree below the lowest common ancestor (LCA) of the entities. Zhang et al. (2018) apply graph convolutional networks (GCNs) (Kipf and Welling, 2017) model over a pruned tree. This tree includes tokens that are up to distance K away from the dependency path in the LCA subtree. However, rule-based pruning strategies might eliminate some important information in the full tree. Figure 1 shows an example in cross-sentence n-ary relation extracti"
P19-1024,D15-1206,0,0.412865,"ei Lu StatNLP Research Group Singapore University of Technology and Design {zhijiang guo,yan zhang}@mymail.sutd.edu.sg, luwei@sutd.edu.sg Abstract 2014; Wang et al., 2016), whereas dependencybased models incorporate dependency trees into the models (Bunescu and Mooney, 2005; Peng et al., 2017). Compared to sequence-based models, dependency-based models are able to capture non-local syntactic relations that are obscure from the surface form alone (Zhang et al., 2018). Various pruning strategies are also proposed to distill the dependency information in order to further improve the performance. Xu et al. (2015b,c) apply neural networks only on the shortest dependency path between the entities in the full tree. Miwa and Bansal (2016) reduce the full tree to the subtree below the lowest common ancestor (LCA) of the entities. Zhang et al. (2018) apply graph convolutional networks (GCNs) (Kipf and Welling, 2017) model over a pruned tree. This tree includes tokens that are up to distance K away from the dependency path in the LCA subtree. However, rule-based pruning strategies might eliminate some important information in the full tree. Figure 1 shows an example in cross-sentence n-ary relation extracti"
P19-1024,W02-1010,0,0.101262,"al layers (Vaswani et al., 2017). Compared to our work, their motivations and network structures are different. In particular, each node only attends to its neighbors in GATs whereas AGGCNs measure the relatedness among all nodes. The network topology in GATs remains the same, while fully connected graphs will be built in AGGCNs to capture long-range semantic interactions. Related Work Our work builds on a rich line of recent efforts on relation extraction models and graph convolutional networks. Relation Extraction. Early research efforts are based on statistical methods. Tree-based kernels (Zelenko et al., 2002) and dependency path-based kernels (Bunescu and Mooney, 2005) are explored to extract the relation. McDonald et al. (2005) construct maximal cliques of entities to predict relations. Mintz et al. (2009) include syntactic features to a statistical classifier. Recently, sequencebased models leverages different neural networks to extract relations, including convolutional neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016), recurrent neural networks (Zhou et al., 2016; Zhang et al., 2017) the combination of both (Vu et al., 2016) and transformer (Verga et al., 2018)."
P19-1024,C14-1220,0,0.227478,"rich line of recent efforts on relation extraction models and graph convolutional networks. Relation Extraction. Early research efforts are based on statistical methods. Tree-based kernels (Zelenko et al., 2002) and dependency path-based kernels (Bunescu and Mooney, 2005) are explored to extract the relation. McDonald et al. (2005) construct maximal cliques of entities to predict relations. Mintz et al. (2009) include syntactic features to a statistical classifier. Recently, sequencebased models leverages different neural networks to extract relations, including convolutional neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016), recurrent neural networks (Zhou et al., 2016; Zhang et al., 2017) the combination of both (Vu et al., 2016) and transformer (Verga et al., 2018). Dependency-based approaches also try to incorporate structural information into the neural models. Peng et al. (2017) first split the dependency graph into two DAGs, then extend the tree LSTM model (Tai et al., 2015) over these two graphs for n-ary relation extraction. Closest to our work, Song et al. (2018b) use graph recurrent networks (Song et al., 2018a) to directly encode the whole dependency grap"
P19-1024,D18-1244,0,0.100199,"into the models (Bunescu and Mooney, 2005; Peng et al., 2017). Compared to sequence-based models, dependency-based models are able to capture non-local syntactic relations that are obscure from the surface form alone (Zhang et al., 2018). Various pruning strategies are also proposed to distill the dependency information in order to further improve the performance. Xu et al. (2015b,c) apply neural networks only on the shortest dependency path between the entities in the full tree. Miwa and Bansal (2016) reduce the full tree to the subtree below the lowest common ancestor (LCA) of the entities. Zhang et al. (2018) apply graph convolutional networks (GCNs) (Kipf and Welling, 2017) model over a pruned tree. This tree includes tokens that are up to distance K away from the dependency path in the LCA subtree. However, rule-based pruning strategies might eliminate some important information in the full tree. Figure 1 shows an example in cross-sentence n-ary relation extraction that the key tokens partial response would be excluded if the model only takes the pruned tree into consideration. Ideally, the model should be able to learn how to maintain a balance between including and excluding information in the"
P19-1024,D17-1004,0,0.294763,"r the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches. 1 Introduction Relation extraction aims to detect relations among entities in the text. It plays a significant role in a variety of natural language processing applications including biomedical knowledge discovery (Quirk and Poon, 2017), knowledge base population (Zhang et al., 2017) and question answering (Yu et al., 2017). Figure 1 shows an example about expressing a relation sensitivity among three entities L858E, EGFR and gefitinib in two sentences. Most existing relation extraction models can be categorized into two classes: sequence-based and dependency-based. Sequence-based models operate only on the word sequences (Zeng et al., ∗ Equally Contributed. 241 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 241–251 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ADVCL ROOT NSUBJ D"
P19-1024,P16-2034,0,0.0812939,"elation Extraction. Early research efforts are based on statistical methods. Tree-based kernels (Zelenko et al., 2002) and dependency path-based kernels (Bunescu and Mooney, 2005) are explored to extract the relation. McDonald et al. (2005) construct maximal cliques of entities to predict relations. Mintz et al. (2009) include syntactic features to a statistical classifier. Recently, sequencebased models leverages different neural networks to extract relations, including convolutional neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016), recurrent neural networks (Zhou et al., 2016; Zhang et al., 2017) the combination of both (Vu et al., 2016) and transformer (Verga et al., 2018). Dependency-based approaches also try to incorporate structural information into the neural models. Peng et al. (2017) first split the dependency graph into two DAGs, then extend the tree LSTM model (Tai et al., 2015) over these two graphs for n-ary relation extraction. Closest to our work, Song et al. (2018b) use graph recurrent networks (Song et al., 2018a) to directly encode the whole dependency graph without breaking it. The contrast between our model and theirs is reminiscent of the contra"
P19-1024,W09-2415,0,\N,Missing
P19-1024,P17-1053,0,\N,Missing
P19-1141,N13-1006,0,0.176139,"Missing"
P19-1141,D15-1141,0,0.026907,"hese coefficients are used to define the amount of contribution from each type of structural information (the gazetteers and the character sequence) for our task. In our model, an adapted GGNN architecture is utilized to learn the node representations. The (0) initial state hv of a node v is defined as follows:  W g (v) v ∈ Vs ∪ Ve (0) hv = c > bi > > [W (v) , W (v) ] v ∈ Vc (1) where W c and W g are lookup tables for the character or the gazetteer the node represents. In the case of character nodes, a bigram embedding table W bi is used since it has been shown to be useful for the NER task (Chen et al., 2015). The structural information of the graph is stored in the adjacency matrix A which serves to retrieve the states of neighboring nodes at each step. To adapt to the multi-digraph structure, A is extended to include edges of different labels, A = [A1 , ..., A|L |]. The contribution coefficients are transformed into weights of edges in A: [wc , wg1 , . . . , wgm ] = σ([αc , αg1 , . . . , αgm ]) (2) Edges of the same label share the same weight. Next, the hidden states are updated by GRU. The basic recurrence for this propagation network is: H= (t−1) [h1 (t−1) , . . . , h|V |]> (3) > > > a(t) v ="
P19-1141,Q16-1026,0,0.117445,"ed of manually labeling the data and can handle rare and unseen cases (Wang et al., 2018). On the other hand, resources of gazetteers are abundant. Many gazetteers have been manually created by previous studies (Zamin and Oxley, 2011). Besides, gazetteers can also be easily constructed from knowledge bases (e.g., Freebase (Bollacker et al., 2008)) or commercial data sources (e.g., product catalogues of e-commence websites). While such background knowledge can be helpful, in practice the gazetteers may also contain irrelevant and even erroneous information which harms the system’s performance (Chiu and Nichols, 2016). This is especially the case for Chinese NER, where enormous errors can be introduced due to wrongly matched entities. Chinese language is inherently ambiguous since the granularity of words is less well defined than other languages (such as English). Thus massive wrongly matched entities can be generated with the use of gazetteers. As we can see from the example shown in Figure 1, matching a simple 9-character sentence with 4 gazetteers may result in 6 matched entities, among which 2 are incorrect. To effectively eliminate the errors, we need a way to resolve the conflicting matches. Existin"
P19-1141,N16-1030,0,0.165921,"s maximizing the total number of matched tokens in a sentence results in wrongly matched entity 张 三在 (Zhang Sanzai) instead of 张三 (Zhang San). While such solutions either rely on manual efforts for rules, templates or heuristics, we believe it is possible to take a data-driven approach here to learn how to combine gazetteer knowledge. To this end, we propose a novel multi-digraph structure which can explicitly model the interaction of the characters and the gazetteers. Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) (Li et al., 2016) and a standard bidirectional LSTM-CRF (Lample et al., 2016) (BiLSTM-CRF), our model learns a weighted combination of the information from different gazetteers and resolves matching conflicts based on contextual information. We summarize our contributions as follows: 1) we propose a novel multi-digraph model to learn how to combine the gazetteer information and to resolve conflicting matches in learning with contexts. To the best of our knowledge, we are the first neural approach to NER that models the gazetteer information with a graph structure; 2) experimental results show that our model significantly outperforms previous methods of using gazetteers"
P19-1141,W06-0115,0,0.622162,"through adjacent nodes. Equations 5, 6, 7, and 8 combine the information from adjacent nodes and the current hidden state of the nodes to compute the new hidden state at time step t. After T steps, we have (T ) our final state hv for the node v. BiLSTM-CRF. The learned feature representa(T ) tions of characters {hv |v ∈ Vc } are then fed to a standard BiLSTM-CRF following the character order in the original sentence, to produce the output sequence. 3 3.1 Experiments Experimental Setup Dataset. The three public datasets used in our experiments are OntoNotes 4.0 (Weischedel et al., 2010), MSRA (Levow, 2006), and Weibo-NER (Peng and Dredze, 2016). OntoNotes and MSRA are two datasets consisting of newswire text. Weibo-NER is in the domain of social media. We use the same split as Che et al. (2013) and Peng and Dredze (2016) on OntoNotes and on WeiboNER. To demonstrate the effectiveness of our model in the e-commerce domain, we further constructed a new dataset by crawling and manually annotating the NEs of two types, namely PROD (“products”) and BRAN (“brands”). We name our dataset as “E-commerce-NER”. The NER task in the e-commerce domain is more challenging. The NEs of interest are usually the n"
P19-1141,W09-1119,0,0.126111,"proach based on graph neural networks with a multidigraph structure that captures the information that the gazetteers offer. Experiments on various datasets show that our model is effective in incorporating rich gazetteer information while resolving ambiguities, outperforming previous approaches. 1 PER2 PER2 Three At Zhang San PER1 LOC1 North Capital Human Beijing Zhang Sanzai The actual translation: Zhang San is at the Beijing People’s Park LOC1 People Public LOC2 Park People’s Park Beijing citizen Wrong matches Correct matches Figure 1: Example of Entity Matching Introduction Previous work (Ratinov and Roth, 2009) shows that NER is a knowledge intensive task. Background knowledge is often incorporated into an NER system in the form of named entity (NE) gazetteers (Seyler et al., 2018). Each gazetteer is typically a list containing NEs of the same type. Many earlier research efforts show that an NER model can benefit from the use of gazetteers (Li et al., 2005). On the one hand, the use of NE gazetteers alleviates the need of manually labeling the data and can handle rare and unseen cases (Wang et al., 2018). On the other hand, resources of gazetteers are abundant. Many gazetteers have been manually cre"
P19-1141,E14-4016,0,0.0265063,"are incorrect. To effectively eliminate the errors, we need a way to resolve the conflicting matches. Existing methods often rely on hand-crafted templates or predefined selection strategies. For example, Qi et al. (2019) defined several n-gram templates to construct features for each character based on dictionaries and contexts. These templates are taskspecific and the lengths of the matched entities are constrained by templates. Several selection strategies are proposed, such as maximizing the total number of matched tokens in a sentence (Shang et al., 2018), or maximum matching with rules (Sassano, 2014). Though general, these strategies are unable to effectively utilize the contextual information. For example, as shown in Figure 1, 1462 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1462–1467 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics maximizing the total number of matched tokens in a sentence results in wrongly matched entity 张 三在 (Zhang Sanzai) instead of 张三 (Zhang San). While such solutions either rely on manual efforts for rules, templates or heuristics, we believe it is possible to take a da"
P19-1141,P18-2039,0,0.0478745,"is effective in incorporating rich gazetteer information while resolving ambiguities, outperforming previous approaches. 1 PER2 PER2 Three At Zhang San PER1 LOC1 North Capital Human Beijing Zhang Sanzai The actual translation: Zhang San is at the Beijing People’s Park LOC1 People Public LOC2 Park People’s Park Beijing citizen Wrong matches Correct matches Figure 1: Example of Entity Matching Introduction Previous work (Ratinov and Roth, 2009) shows that NER is a knowledge intensive task. Background knowledge is often incorporated into an NER system in the form of named entity (NE) gazetteers (Seyler et al., 2018). Each gazetteer is typically a list containing NEs of the same type. Many earlier research efforts show that an NER model can benefit from the use of gazetteers (Li et al., 2005). On the one hand, the use of NE gazetteers alleviates the need of manually labeling the data and can handle rare and unseen cases (Wang et al., 2018). On the other hand, resources of gazetteers are abundant. Many gazetteers have been manually created by previous studies (Zamin and Oxley, 2011). Besides, gazetteers can also be easily constructed from knowledge bases (e.g., Freebase (Bollacker et al., 2008)) or commerc"
P19-1141,D18-1230,0,0.0260449,"eers may result in 6 matched entities, among which 2 are incorrect. To effectively eliminate the errors, we need a way to resolve the conflicting matches. Existing methods often rely on hand-crafted templates or predefined selection strategies. For example, Qi et al. (2019) defined several n-gram templates to construct features for each character based on dictionaries and contexts. These templates are taskspecific and the lengths of the matched entities are constrained by templates. Several selection strategies are proposed, such as maximizing the total number of matched tokens in a sentence (Shang et al., 2018), or maximum matching with rules (Sassano, 2014). Though general, these strategies are unable to effectively utilize the contextual information. For example, as shown in Figure 1, 1462 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1462–1467 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics maximizing the total number of matched tokens in a sentence results in wrongly matched entity 张 三在 (Zhang Sanzai) instead of 张三 (Zhang San). While such solutions either rely on manual efforts for rules, templates or he"
P19-1141,P18-4013,0,0.0528297,"Missing"
P19-1141,W06-0126,0,0.378098,"Missing"
P19-1141,P18-1144,0,0.424648,"Missing"
P19-1141,P16-2025,0,0.276512,"ions 5, 6, 7, and 8 combine the information from adjacent nodes and the current hidden state of the nodes to compute the new hidden state at time step t. After T steps, we have (T ) our final state hv for the node v. BiLSTM-CRF. The learned feature representa(T ) tions of characters {hv |v ∈ Vc } are then fed to a standard BiLSTM-CRF following the character order in the original sentence, to produce the output sequence. 3 3.1 Experiments Experimental Setup Dataset. The three public datasets used in our experiments are OntoNotes 4.0 (Weischedel et al., 2010), MSRA (Levow, 2006), and Weibo-NER (Peng and Dredze, 2016). OntoNotes and MSRA are two datasets consisting of newswire text. Weibo-NER is in the domain of social media. We use the same split as Che et al. (2013) and Peng and Dredze (2016) on OntoNotes and on WeiboNER. To demonstrate the effectiveness of our model in the e-commerce domain, we further constructed a new dataset by crawling and manually annotating the NEs of two types, namely PROD (“products”) and BRAN (“brands”). We name our dataset as “E-commerce-NER”. The NER task in the e-commerce domain is more challenging. The NEs of interest are usually the names of products 1464 OntoNotes Models"
P19-1252,D11-1120,0,0.250718,"approaches to predicting Twitter users’ demographic attributes explore, select, and combine various features generated from text and network to achieve the best predictive performances in respective classification tasks (Han et al., 2013; Miller et al., 2012; Preot¸iuc-Pietro et al., 2015; Huang et al., 2015; Aletras and Chamberlain, 2018). The three categories of features are: account level features, tweet text features, and network based features. Past research have shown the distinctive usage of language across gender, age, location, etc. in tweets (Sloan et al., 2015; Cheng et al., 2010; Burger et al., 2011; Rao et al., 2010), which makes content based prediction effective. As for user occupational class prediction, Preot¸iuc-Pietro et al. (2015) built a dataset where 2633 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2633–2638 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics users are assigned to hierarchical job categories. They used word cluster distribution features of content information to predict a user’s occupational group. Aletras and Chamberlain (2018) constructed a user’s followings connections"
P19-1252,P13-4002,0,0.0202598,"itter. By looking at their Bio and tweets, it can be inferred that these users share the same occupational interest. Profiling users can enhance service quality and improve product recommendation, and hence is a widely studied problem. User occupational class prediction is an important component of user profiling and a sub-task of user demographic feature prediction. Existing approaches to predicting Twitter users’ demographic attributes explore, select, and combine various features generated from text and network to achieve the best predictive performances in respective classification tasks (Han et al., 2013; Miller et al., 2012; Preot¸iuc-Pietro et al., 2015; Huang et al., 2015; Aletras and Chamberlain, 2018). The three categories of features are: account level features, tweet text features, and network based features. Past research have shown the distinctive usage of language across gender, age, location, etc. in tweets (Sloan et al., 2015; Cheng et al., 2010; Burger et al., 2011; Rao et al., 2010), which makes content based prediction effective. As for user occupational class prediction, Preot¸iuc-Pietro et al. (2015) built a dataset where 2633 Proceedings of the 57th Annual Meeting of the Ass"
P19-1252,P15-1169,0,0.138486,"Missing"
P19-1517,D14-1058,0,0.53926,"nt, variable-sized quantity span surrounding the quantity token in the text, which conveys information useful for determining its sign. Empirical results show that our method achieves 5 and 8 points of accuracy gains on two datasets respectively, compared to prior approaches. 1 Figure 1: Two examples of arithmetic word problems described in English with answers. Introduction Teaching machines to automatically solve arithmetic word problems, exemplified by two problems in Figure 1, is a long-standing Artificial Intelligence (AI) task (Bobrow, 1964; Mukherjee and Garain, 2008). Recent research (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015; Wang et al., 2017, 2018b,a) focused on designing algorithms to automatically solve arithmetic word problems. One line of prior works designed rules (Mukherjee and Garain, 2008; Hosseini et al., 2014) or templates (Kushman et al., 2014; Zhou et al., 2015; Mitra and Baral, 2016) to map problems to expressions, where rules or templates are collected from training data. However, it would be non-trivial and expensive to acquire a general set of rules or templates. Furthermore, such approaches typically require additional annotations. The addition-subtract"
P19-1517,P14-1026,0,0.615789,"tity span surrounding the quantity token in the text, which conveys information useful for determining its sign. Empirical results show that our method achieves 5 and 8 points of accuracy gains on two datasets respectively, compared to prior approaches. 1 Figure 1: Two examples of arithmetic word problems described in English with answers. Introduction Teaching machines to automatically solve arithmetic word problems, exemplified by two problems in Figure 1, is a long-standing Artificial Intelligence (AI) task (Bobrow, 1964; Mukherjee and Garain, 2008). Recent research (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015; Wang et al., 2017, 2018b,a) focused on designing algorithms to automatically solve arithmetic word problems. One line of prior works designed rules (Mukherjee and Garain, 2008; Hosseini et al., 2014) or templates (Kushman et al., 2014; Zhou et al., 2015; Mitra and Baral, 2016) to map problems to expressions, where rules or templates are collected from training data. However, it would be non-trivial and expensive to acquire a general set of rules or templates. Furthermore, such approaches typically require additional annotations. The addition-subtraction problems, which co"
P19-1517,D15-1202,0,0.386282,"the quantity token in the text, which conveys information useful for determining its sign. Empirical results show that our method achieves 5 and 8 points of accuracy gains on two datasets respectively, compared to prior approaches. 1 Figure 1: Two examples of arithmetic word problems described in English with answers. Introduction Teaching machines to automatically solve arithmetic word problems, exemplified by two problems in Figure 1, is a long-standing Artificial Intelligence (AI) task (Bobrow, 1964; Mukherjee and Garain, 2008). Recent research (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015; Wang et al., 2017, 2018b,a) focused on designing algorithms to automatically solve arithmetic word problems. One line of prior works designed rules (Mukherjee and Garain, 2008; Hosseini et al., 2014) or templates (Kushman et al., 2014; Zhou et al., 2015; Mitra and Baral, 2016) to map problems to expressions, where rules or templates are collected from training data. However, it would be non-trivial and expensive to acquire a general set of rules or templates. Furthermore, such approaches typically require additional annotations. The addition-subtraction problems, which constitute the most fu"
P19-1517,D18-1132,0,0.665105,"Missing"
P19-1517,D17-1088,0,0.362757,"n the text, which conveys information useful for determining its sign. Empirical results show that our method achieves 5 and 8 points of accuracy gains on two datasets respectively, compared to prior approaches. 1 Figure 1: Two examples of arithmetic word problems described in English with answers. Introduction Teaching machines to automatically solve arithmetic word problems, exemplified by two problems in Figure 1, is a long-standing Artificial Intelligence (AI) task (Bobrow, 1964; Mukherjee and Garain, 2008). Recent research (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015; Wang et al., 2017, 2018b,a) focused on designing algorithms to automatically solve arithmetic word problems. One line of prior works designed rules (Mukherjee and Garain, 2008; Hosseini et al., 2014) or templates (Kushman et al., 2014; Zhou et al., 2015; Mitra and Baral, 2016) to map problems to expressions, where rules or templates are collected from training data. However, it would be non-trivial and expensive to acquire a general set of rules or templates. Furthermore, such approaches typically require additional annotations. The addition-subtraction problems, which constitute the most fundamental class of"
P19-1517,D15-1096,0,0.0144884,"of arithmetic word problems described in English with answers. Introduction Teaching machines to automatically solve arithmetic word problems, exemplified by two problems in Figure 1, is a long-standing Artificial Intelligence (AI) task (Bobrow, 1964; Mukherjee and Garain, 2008). Recent research (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015; Wang et al., 2017, 2018b,a) focused on designing algorithms to automatically solve arithmetic word problems. One line of prior works designed rules (Mukherjee and Garain, 2008; Hosseini et al., 2014) or templates (Kushman et al., 2014; Zhou et al., 2015; Mitra and Baral, 2016) to map problems to expressions, where rules or templates are collected from training data. However, it would be non-trivial and expensive to acquire a general set of rules or templates. Furthermore, such approaches typically require additional annotations. The addition-subtraction problems, which constitute the most fundamental class of arithmetic word problems, have been the focus for many previous works (Hosseini et al., 2014; Mitra and Baral, 2016). We also focus on this important task in this work. Our key observation is that essentially solving such a class of pro"
P19-1517,P18-2107,1,0.592816,"h|t(i) ) [fk (t(i) , y 0 , h)] ∂wk i X − Ep(h|t(i) ,y(i) ) [fk (t(i) , y (i) , h)] (4) i where Ep [·] is the expectation under distribution p. We can construct a lattice representation on top of the nodes shown in Figure 2. The representation compactly encodes exponentially many paths, where each path corresponds to one possible label sequence. Note that there exists a topological ordering amongst all nodes. This allows us to apply a generalized forward-backward algorithm to perform exact marginal inference so as to calculate both objective and expectation values efficiently (Li and Lu, 2017; Zou and Lu, 2018). The MAP inference procedure can be done analogously, which is called during the decoding time. 2.3 Model Variants We further consider two variants of our model. Semi-Markov Variant: Our first variant, namely QT( S ), employs the semi-Markov assumption (Sarawagi and Cohen, 2005), where N nodes are removed. Different from QT which makes the first-order Markov assumption, QT( S ) assumes L 5248 Model Hosseini et al. (2014) Kushman et al. (2014) Koncel-Kedziorski et al. (2015) Roy and Roth (2015) Zhou et al. (2015) Mitra and Baral (2016) Roy and Roth (2017) Wang et al. (2017) Wang et al. (2018b)"
P19-1517,N19-1217,1,0.58003,". 2.2 A Tagging Problem si qi + sx x = 0 Quantity Tagger Our primary assumption is that, for each quantity, there exists an implicit quantity span that resides in the problem text and can convey relevant information useful for determining the signs of the quantities. The quantity span of a quantity is essentially a contiguous token sequence from the problem text that consists of the quantity itself and some surrounding word tokens. Formally, our model needs to learn how to sequentially assign each quantity q ∈ Q its optimal sign s ∈ S. This is a sequence labeling problem (Lample et al., 2016; Zou and Lu, 2019). Common sequence labeling tasks, such as NER and POS tagging, mainly consider one sentence at a 5247 time, and tag each token in the sentence. However, our tagging problem typically involves multiple sentences where relatively unimportant information may be potentially included. For instance, the second sentence of Problem 2 in Figure 1, “Park workers will plant walnut trees today” describes background knowledge of the problem, but such information may not be useful for solving problems, yet even obstructive. For each quantity q ∈ Q, we first consider a token window consisting of q and J − 1"
P19-1517,N16-1030,0,0.0243847,"an be easily obtained. 2.2 A Tagging Problem si qi + sx x = 0 Quantity Tagger Our primary assumption is that, for each quantity, there exists an implicit quantity span that resides in the problem text and can convey relevant information useful for determining the signs of the quantities. The quantity span of a quantity is essentially a contiguous token sequence from the problem text that consists of the quantity itself and some surrounding word tokens. Formally, our model needs to learn how to sequentially assign each quantity q ∈ Q its optimal sign s ∈ S. This is a sequence labeling problem (Lample et al., 2016; Zou and Lu, 2019). Common sequence labeling tasks, such as NER and POS tagging, mainly consider one sentence at a 5247 time, and tag each token in the sentence. However, our tagging problem typically involves multiple sentences where relatively unimportant information may be potentially included. For instance, the second sentence of Problem 2 in Figure 1, “Park workers will plant walnut trees today” describes background knowledge of the problem, but such information may not be useful for solving problems, yet even obstructive. For each quantity q ∈ Q, we first consider a token window consist"
P19-1517,P14-5010,0,0.0244687,"uantity span for each quantity is a fixed-size token window. All of our proposed models consistently outperform previous research efforts. These figures confirm the capability of our approach to provide more promising solutions to addition-subtraction problems. We do not require any additional annotations which can be expensive, while annotations like variable-word alignments and formulas are necessary for works of (Kushman et al., 2014; Mitra and Baral, 2016). To investigate the power of features extracted by external tools, such as ConceptNet (Liu and Singh, 2004) and Stanford CoreNLP tool (Manning et al., 2014), we conduct additional experiments on the afore-mentioned datasets, where we call such features external features (see supplementary material), indicated as “- EF”. It is expected that the performance drops because such features are necessary for capturing evidence across sentences. Especially, for the AddSub dataset, it affects a lot. As discussed before (Hosseini et al., 2014; Mitra and Baral, 2016), there exists lots of irrelevant information and information gaps in AddSub. We thus can infer the external features support our approach to be capable of bridging information gaps and recognizi"
P19-1517,P16-1202,0,0.902697,"problems described in English with answers. Introduction Teaching machines to automatically solve arithmetic word problems, exemplified by two problems in Figure 1, is a long-standing Artificial Intelligence (AI) task (Bobrow, 1964; Mukherjee and Garain, 2008). Recent research (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015; Wang et al., 2017, 2018b,a) focused on designing algorithms to automatically solve arithmetic word problems. One line of prior works designed rules (Mukherjee and Garain, 2008; Hosseini et al., 2014) or templates (Kushman et al., 2014; Zhou et al., 2015; Mitra and Baral, 2016) to map problems to expressions, where rules or templates are collected from training data. However, it would be non-trivial and expensive to acquire a general set of rules or templates. Furthermore, such approaches typically require additional annotations. The addition-subtraction problems, which constitute the most fundamental class of arithmetic word problems, have been the focus for many previous works (Hosseini et al., 2014; Mitra and Baral, 2016). We also focus on this important task in this work. Our key observation is that essentially solving such a class of problems can be tackled fro"
Q19-1019,P18-1026,0,0.203904,"y applied to many NLP tasks (Bastings et al., 2017; Zhang et al., 2018b). Interestingly, although deeper GCNs with more layers will be able to capture richer neighborhood information of a graph, empirically it has been observed that the best performance is achieved with a 2-layer model (Li et al., 2018). Therefore, recent efforts that leverage recurrencebased graph neural networks have been explored as the alternatives to encode the structural information of graphs. Examples include graph-state long short-term memory (LSTM) networks (Song et al., 2018) and gated graph neural networks (GGNNs) (Beck et al., 2018). Deep architectures based on such recurrence-based models have been successfully built for tasks such as language generation, where rich neighborhood information captured was shown useful. Compared with recurrent neural networks, convolutional architectures are highly parallelizable and are more amenable to hardware acceleration We focus on graph-to-sequence learning, which can be framed as transducing graph structures to sequences for text generation. To capture structural information associated with graphs, we investigate the problem of encoding graphs using graph convolutional networks (GC"
Q19-1019,E17-3017,0,0.0802216,"ion hv , can be defined as h(vl) = ρ   W (l) h(ul−1) + b(l)  (1) u∈N (v ) where W (l) is the weight matrix, b(l) is the bias vector, N (v ) is the set of one-hop neighbors of node v , and ρ is an activation function (e.g., RELU (0) [Nair and Hinton, 2010]). hv is the initial input xv , where xv ∈ Rd and d is the input feature dimension. GCNs with Residual Connections. Bastings et al. (2017) integrate residual connections (He et al., 2016) into GCNs to help information propagation. Specifically, each node is updated 1 Our implementation is based on MXNET (Chen et al., 2015) and the Sockeye (Felix et al., 2017) toolkit. 298 according to Equation (1) first and then the resulting representation is combined with the node’s representation from the last iteration:    W (l) h(ul−1) + b(l) + h(vl−1) (2) h(vl) = ρ u∈N (v ) GCNs with Layer Aggregations. Xu et al. (2018) propose layer aggregations for GCNs, in which the final representation of each node is computed by combining the node’s representations from all GCN layers: hfv inal = LA(h(vl) , h(vl−1) , . . . . , h(1) v ) (3) where the LA function can be concatenation, maxpooling, or LSTM-attention operations as defined in Xu et al. (2018). 2.2 Dense Co"
Q19-1019,N16-1087,0,0.355797,"Missing"
Q19-1019,P07-2045,0,0.0143398,"that our model is more effective in terms of using automatically generated AMR graphs. Using 0.3M additional data, our ensemble model achieves the new state-of-the-art result of 35.3 BLEU points. 4.3 Main Results on Syntax-based NMT Table 4 shows the results for the English-German (En-De) and English-Czech (En-Cs) translation tasks. BoW+GCN, CNN+GCN, and BiRNN+GCN refer to utilizing the following encoders with a GCN layer on top respectively: 1) a bag-ofwords encoder, 2) a one-layer CNN, and 3) a bidirectional RNN. PB-SMT is the phrase-based statistical machine translation model using Moses (Koehn et al., 2007). Our single model achieves 19.0 and 12.1 BLEU points on the En-De and En-Cs tasks, respectively, significantly outperforming all the single models. For example, compared 304 English-German Model Type English-Czech #P B C #P B C BoW+GCN (Bastings et al., 2017) CNN+GCN (Bastings et al., 2017) BiRNN+GCN (Bastings et al., 2017) Single Single Single − − − 12.2 13.7 16.1 − − − − − − 7.5 8.7 9.6 − − − PB-SMT (Beck et al., 2018) Seq2SeqB (Beck et al., 2018) GGNN2Seq (Beck et al., 2018) Single Single Single − 41.4M 41.2M 12.8 15.5 16.7 43.2 40.8 42.4 − 39.1M 38.8M 8.6 8.9 9.8 36.4 33.8 33.3 DCGCN (our"
Q19-1019,N03-1017,0,0.0466757,"l methods. Lu et al. (2009) present a language generation model using the tree-structured meaning representation based on tree conditional random fields. Lu and Ng (2011) propose a model for language generation from lambda calculus expressions that can be represented as forest structures. Konstas and Lapata (2012, 2013) leverage hypergraphs for concept-to-text generation. Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it into a sentence using a tree-to-string transducer. Pourdamghani et al. (2016) adopt a phrase-based model for machine translation (Koehn et al., 2003) based on a linearized AMR graph. Song et al. (2017) leverage a synchronous node replacement grammar. Konstas et al. (2017) also linearize the input graph and feed it to the Seq2Seq model (Sutskever et al., 2014). Sequence-based neural networks may lose structural information from the original graph because they require linearization of the input graph. Recent research efforts consider developing encoders with graph neural networks. Beck et al. (2018) use GGNNs (Li et al., 2016) as the encoder and introduce the Levi graph that allows nodes and edges to have their own hidden representations. So"
Q19-1019,P17-1014,0,0.104723,"s the effectiveness of our models on two typical graph-to-sequence learning tasks, (9) 302 Dataset Train AMR15 (LDC2015E86) AMR17 (LDC2017T10) 16,833 1,368 1,371 36,521 1,368 1,371 English-Czech English-German Dev Test 181,112 2,656 2,999 226,822 2,169 2,999 Table 1: The number of sentences in four datasets. including AMR-to-text generation and syntaxbased neural machine translation (NMT). For the AMR-to-text generation task, we use two benchmarks—the LDC2015E86 dataset (AMR15) and the LDC2017T10 dataset (AMR17). In these datasets, each instance contains a sentence and an AMR graph. We follow Konstas et al. (2017) to apply entity simplification in the preprocessing steps. We then transform each preprocessed AMR graph into its extended Levi graph as described in Section 3.2. For the syntax-based NMT task, we evaluate our model on both the En-De and the En-Cs News Commentary v11 dataset from the WMT16 translation task.2 We parse English sentences after tokenization to generate the dependency trees on the source side using SyntaxNet (Alberti et al., 2017).3 We tokenize Czech and German using the Moses tokenizer.4 On the target side, we use byte-pair encodings (Sennrich et al., 2016) with 8,000 merge opera"
Q19-1019,N12-1093,0,0.0557704,"earning better graph representations. Related Work Our work builds on a rich line of recent efforts on graph-to-sequence models, graph convolutional networks, and densely connected convolutional networks. Graph-to-Sequence Learning. Early research efforts for graph-to-sequence learning are based on statistical methods. Lu et al. (2009) present a language generation model using the tree-structured meaning representation based on tree conditional random fields. Lu and Ng (2011) propose a model for language generation from lambda calculus expressions that can be represented as forest structures. Konstas and Lapata (2012, 2013) leverage hypergraphs for concept-to-text generation. Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it into a sentence using a tree-to-string transducer. Pourdamghani et al. (2016) adopt a phrase-based model for machine translation (Koehn et al., 2003) based on a linearized AMR graph. Song et al. (2017) leverage a synchronous node replacement grammar. Konstas et al. (2017) also linearize the input graph and feed it to the Seq2Seq model (Sutskever et al., 2014). Sequence-based neural networks may lose structural information from the original"
Q19-1019,D18-1198,1,0.89429,"important role in natural language processing (NLP) as they are able to capture richer structural information than sequences and trees. Generally, semantics of sentences can be encoded as graphs. For example, the abstract meaning representation (AMR) (Banarescu et al., 2013) is a directed, labeled graph as shown in Figure 1, where nodes in the graph denote semantic concepts and edges denote relations between concepts. Such graph representations can capture rich semanticlevel structural information, and are attractive representations useful for semantics-related tasks such as semantic parsing (Guo and Lu, 2018) and natural language generation (Beck et al., 2018). In this paper, we focus on the graph-to-sequence ∗ Contributed equally. 297 Transactions of the Association for Computational Linguistics, vol. 7, pp. 297–312, 2019. Action Editor: Stefan Reizler. Submission batch: 11/2018; Revision batch: 2/2019; Published 6/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  a better graph representation than those learned from the shallower GCN models. Experiments show that our model is able to achieve better performance for graph-to-sequence learning tasks. F"
Q19-1019,D13-1157,0,0.105322,"Missing"
Q19-1019,D11-1149,1,0.877608,"e. Our graph encoder solely relies on the DCGCN model, whose deep network structure encodes richer local and non-local information for learning better graph representations. Related Work Our work builds on a rich line of recent efforts on graph-to-sequence models, graph convolutional networks, and densely connected convolutional networks. Graph-to-Sequence Learning. Early research efforts for graph-to-sequence learning are based on statistical methods. Lu et al. (2009) present a language generation model using the tree-structured meaning representation based on tree conditional random fields. Lu and Ng (2011) propose a model for language generation from lambda calculus expressions that can be represented as forest structures. Konstas and Lapata (2012, 2013) leverage hypergraphs for concept-to-text generation. Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it into a sentence using a tree-to-string transducer. Pourdamghani et al. (2016) adopt a phrase-based model for machine translation (Koehn et al., 2003) based on a linearized AMR graph. Song et al. (2017) leverage a synchronous node replacement grammar. Konstas et al. (2017) also linearize the input gr"
Q19-1019,P82-1020,0,0.821017,"Missing"
Q19-1019,D09-1042,1,0.829221,"7) stack GCNs upon a RNN or CNN encoder because 2-layer GCNs may not be able to capture nonlocal information, especially when the graph is large. Our graph encoder solely relies on the DCGCN model, whose deep network structure encodes richer local and non-local information for learning better graph representations. Related Work Our work builds on a rich line of recent efforts on graph-to-sequence models, graph convolutional networks, and densely connected convolutional networks. Graph-to-Sequence Learning. Early research efforts for graph-to-sequence learning are based on statistical methods. Lu et al. (2009) present a language generation model using the tree-structured meaning representation based on tree conditional random fields. Lu and Ng (2011) propose a model for language generation from lambda calculus expressions that can be represented as forest structures. Konstas and Lapata (2012, 2013) leverage hypergraphs for concept-to-text generation. Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it into a sentence using a tree-to-string transducer. Pourdamghani et al. (2016) adopt a phrase-based model for machine translation (Koehn et al., 2003) based o"
Q19-1019,D17-1159,0,0.0899509,"L) ] and hout ∈ Rd . xv is the input of the DCGCN layer. hout and xv share the same dimension d. Wcomb ∈ Rd×d is a weight matrix and bcomb is a bias vector for the linear transformation. Both Wcomb and bcomb are different according to different DCGCN layers. In addition, another linear combination layer is added to obtain the final representations as shown in Figure 3. 3.2 Extended Levi Graph In order to improve the information propagation process in graph structures such as AMR graphs and dependency trees, previous researchers enrich the original input graphs with additional transformations. Marcheggiani and Titov (2017) add reverse edges as well as self-loop edges for each node to the original graph. This strategy is similar to the bidirectional recurrent neural networks (RNNs) (Elman, 1990), which can enjoy the information propagation from two directions. Beck et al. (2018) adapt this approach and additionally transform the directed input graphs into Levi graphs (Gross et al., 2013). Basically, edges in the original graphs are turned into additional nodes in Levi graphs. With this approach, we can encode the original edge labels and node inputs in the same way. Specifically, Beck et al. (2018) define three"
Q19-1019,P02-1040,0,0.105491,"2: Main results on AMR17. #P shows the model size in terms of parameters; ‘‘S’’ and ‘‘E’’ denote single and ensemble models, respectively. batch size (Batch) candidates are {16, 20, 24}. We determine when to stop training based on the perplexity change in the development set. For decoding, we use beam search with beam size 10. Through preliminary experiments, we find that the combinations (Block = 4, d = 360, Batch = 16) and (Block = 2, d = 360, Batch = 24) give best results on AMR and NMT tasks, respectively. Following previous work, we evaluate the results in terms of both BLEU (B) scores (Papineni et al., 2002) and sentence-level CHRF++ (C) scores (Popovic, 2017; Beck et al., 2018). Particularly, we use case-insensitive BLEU scores for AMR and case sensitive BLEU scores for NMT. For ensemble models, we train five models with different random seeds and then use Sockeye (Felix et al., 2017) to perform default ensemble decoding. 4.2 Main Results on AMR-to-text Generation We compare the performance of DCGCNs with the other three kinds of models: (1) sequence-tosequence (Seq2Seq) models, which use linearized graphs as inputs; (2) recurrent graph encoders (GGNN2Seq, GraphLSTM); (3) models trained with ext"
Q19-1019,P18-1150,0,0.178538,"information that is L hops away. GCNs have been successfully applied to many NLP tasks (Bastings et al., 2017; Zhang et al., 2018b). Interestingly, although deeper GCNs with more layers will be able to capture richer neighborhood information of a graph, empirically it has been observed that the best performance is achieved with a 2-layer model (Li et al., 2018). Therefore, recent efforts that leverage recurrencebased graph neural networks have been explored as the alternatives to encode the structural information of graphs. Examples include graph-state long short-term memory (LSTM) networks (Song et al., 2018) and gated graph neural networks (GGNNs) (Beck et al., 2018). Deep architectures based on such recurrence-based models have been successfully built for tasks such as language generation, where rich neighborhood information captured was shown useful. Compared with recurrent neural networks, convolutional architectures are highly parallelizable and are more amenable to hardware acceleration We focus on graph-to-sequence learning, which can be framed as transducing graph structures to sequences for text generation. To capture structural information associated with graphs, we investigate the probl"
Q19-1019,N18-1202,0,0.109248,"d Schmidhuber, 1997), makes predictions based on hidden representations and the context vector. where a ∈ R2dhidden is a weight vector, φ is the activation function (here we use LeakyReLU [Girshick et al., 2014]). These coefficients are used to compute a linear combination of the node representations. Modifying the convolution computation for attention, we arrive at:    (l ) αvu W (l) gu(l) + b(l) (7) h(vl) = ρ combination layer between multi-layer DCGCNs to filter the representations from different DCGCN layers, reaching a more expressive representation. This strategy is inspired by ELMo (Peters et al., 2018), which combines the hidden states from different LSTM layers. We also use a residual connection (He et al., 2016) to incorporate the initial inputs of multi-layer GCNs into the linear combination layer, see Figure 3. Formally, the output of the linear combination layer is defined as:   hcomb = Wcomb hout + xv + bcomb (8) u∈N (v ) (l ) where αvu are normalized attention coefficients computed by the attention mechanism at l-th layer. Note that these coefficients will not change the dimension of the output representations. 3 Graph-to-Sequence Model In the following we will explain the model ar"
Q19-1019,W17-4770,0,0.124885,"f parameters; ‘‘S’’ and ‘‘E’’ denote single and ensemble models, respectively. batch size (Batch) candidates are {16, 20, 24}. We determine when to stop training based on the perplexity change in the development set. For decoding, we use beam search with beam size 10. Through preliminary experiments, we find that the combinations (Block = 4, d = 360, Batch = 16) and (Block = 2, d = 360, Batch = 24) give best results on AMR and NMT tasks, respectively. Following previous work, we evaluate the results in terms of both BLEU (B) scores (Papineni et al., 2002) and sentence-level CHRF++ (C) scores (Popovic, 2017; Beck et al., 2018). Particularly, we use case-insensitive BLEU scores for AMR and case sensitive BLEU scores for NMT. For ensemble models, we train five models with different random seeds and then use Sockeye (Felix et al., 2017) to perform default ensemble decoding. 4.2 Main Results on AMR-to-text Generation We compare the performance of DCGCNs with the other three kinds of models: (1) sequence-tosequence (Seq2Seq) models, which use linearized graphs as inputs; (2) recurrent graph encoders (GGNN2Seq, GraphLSTM); (3) models trained with external resources. For convenience, we denote the LSTM"
Q19-1019,P16-1008,0,0.045951,"d = T × dhidden . T is the size of the edge type vocabulary and dhidden is the hidden dimension in DCGCN layers as described in Section 2.2. bf ∈ Rdhidden is a bias vector. Finally, the convolution computation becomes:   (l ) (l ) h(vl) = ρ f ([v1 ; · · · ; vT ]) (11) 3.4 Decoder We use an attention-based LSTM decoder (Bahdanau et al., 2015). The initial state of the decoder is the representation of the global node described in Section 3.2. The decoder yields the natural language sequence by calculating a sequence of hidden states sequentially. Here we also include the coverage mechanism (Tu et al., 2016). Therefore, when generating the t-th token, the decoder considers five factors: the attention memory, the word embedding of the (t − 1)-th token, the previous hidden state of LSTM, the previous context vector, and the previous coverage vector. 4 Experiments  4.1 Experimental Setup We assess the effectiveness of our models on two typical graph-to-sequence learning tasks, (9) 302 Dataset Train AMR15 (LDC2015E86) AMR17 (LDC2017T10) 16,833 1,368 1,371 36,521 1,368 1,371 English-Czech English-German Dev Test 181,112 2,656 2,999 226,822 2,169 2,999 Table 1: The number of sentences in four datasets"
Q19-1019,W16-6603,0,0.345383,"luations. DCGCN also outperforms GraphLSTM by 2.0 BLEU points in the fully supervised setting as shown in Table 3. Note that GraphLSTM uses char-level neural representations and pretrained word embeddings, whereas our model solely relies on word-level representations with random initializations. This empirically shows that compared with recurrent graph encoders, DCGCNs can learn better representations for graphs. Moreover, we compare our results with the state-of-the-art semi-supervised models on the AMR15 test set (Table 3), including non-neural methods such as TSP (Song et al., 2016), PBMT (Pourdamghani et al., 2016), Tree2Str (Flanigan et al., 2016), and SNRG (Song et al., 2017). All these non-neural models train language models on the whole Gigaword corpus. Our ensemble model gives 28.2 BLEU points without external data, which is better than these other methods. Following Konstas et al. (2017) and Song et al. (2018), we also evaluate our model using external Gigaword sentences as training data. We first use the additional data to pretrain the model, then fine tune it on the gold data. Using additional 0.1M data, the single DCGCN model achieves a BLEU score of 29.0, which is higher than Seq2SeqK (Konstas"
Q19-1019,P16-1162,0,0.280994,"an AMR graph. We follow Konstas et al. (2017) to apply entity simplification in the preprocessing steps. We then transform each preprocessed AMR graph into its extended Levi graph as described in Section 3.2. For the syntax-based NMT task, we evaluate our model on both the En-De and the En-Cs News Commentary v11 dataset from the WMT16 translation task.2 We parse English sentences after tokenization to generate the dependency trees on the source side using SyntaxNet (Alberti et al., 2017).3 We tokenize Czech and German using the Moses tokenizer.4 On the target side, we use byte-pair encodings (Sennrich et al., 2016) with 8,000 merge operations to obtain subwords. We transform the labelled dependency trees into their corresponding extended Levi graphs as described in Section 3.2. Table 1 shows the statistics of these four datasets. The AMR-to-text datasets contain about 16 K ∼ 36 K training instances. The NMT datasets are relatively large, consisting of around 200 K training instances. We tune model hyper-parameters using random layouts based on the results of the development set. We choose the number of DCGCN blocks (Block ) from {1, 2, 3, 4}. We select the feature dimension d from {180, 240, 300, 360, 4"
Q19-1019,P17-2002,0,0.213752,"ly supervised setting as shown in Table 3. Note that GraphLSTM uses char-level neural representations and pretrained word embeddings, whereas our model solely relies on word-level representations with random initializations. This empirically shows that compared with recurrent graph encoders, DCGCNs can learn better representations for graphs. Moreover, we compare our results with the state-of-the-art semi-supervised models on the AMR15 test set (Table 3), including non-neural methods such as TSP (Song et al., 2016), PBMT (Pourdamghani et al., 2016), Tree2Str (Flanigan et al., 2016), and SNRG (Song et al., 2017). All these non-neural models train language models on the whole Gigaword corpus. Our ensemble model gives 28.2 BLEU points without external data, which is better than these other methods. Following Konstas et al. (2017) and Song et al. (2018), we also evaluate our model using external Gigaword sentences as training data. We first use the additional data to pretrain the model, then fine tune it on the gold data. Using additional 0.1M data, the single DCGCN model achieves a BLEU score of 29.0, which is higher than Seq2SeqK (Konstas et al., 2017) and GraphLSTM (Song et al., 2018) trained with 0."
Q19-1019,P18-1030,0,0.0545122,"ntation of each node is iteratively updated based on those of its adjacent nodes in the graph through an information propagation scheme. For example, the first layer of GCNs can only capture the graph’s adjacency information between immediate neighbors, while with the second layer one will be able to capture second-order proximity information (neighborhood information two hops away from one node) as shown in Figure 1. Formally, L layers will be needed in order to capture neighborhood information that is L hops away. GCNs have been successfully applied to many NLP tasks (Bastings et al., 2017; Zhang et al., 2018b). Interestingly, although deeper GCNs with more layers will be able to capture richer neighborhood information of a graph, empirically it has been observed that the best performance is achieved with a 2-layer model (Li et al., 2018). Therefore, recent efforts that leverage recurrencebased graph neural networks have been explored as the alternatives to encode the structural information of graphs. Examples include graph-state long short-term memory (LSTM) networks (Song et al., 2018) and gated graph neural networks (GGNNs) (Beck et al., 2018). Deep architectures based on such recurrence-based"
Q19-1019,D16-1224,0,0.0812786,"res for sentence-level evaluations. DCGCN also outperforms GraphLSTM by 2.0 BLEU points in the fully supervised setting as shown in Table 3. Note that GraphLSTM uses char-level neural representations and pretrained word embeddings, whereas our model solely relies on word-level representations with random initializations. This empirically shows that compared with recurrent graph encoders, DCGCNs can learn better representations for graphs. Moreover, we compare our results with the state-of-the-art semi-supervised models on the AMR15 test set (Table 3), including non-neural methods such as TSP (Song et al., 2016), PBMT (Pourdamghani et al., 2016), Tree2Str (Flanigan et al., 2016), and SNRG (Song et al., 2017). All these non-neural models train language models on the whole Gigaword corpus. Our ensemble model gives 28.2 BLEU points without external data, which is better than these other methods. Following Konstas et al. (2017) and Song et al. (2018), we also evaluate our model using external Gigaword sentences as training data. We first use the additional data to pretrain the model, then fine tune it on the gold data. Using additional 0.1M data, the single DCGCN model achieves a BLEU score of 29.0, whic"
Q19-1019,D18-1244,0,0.0614802,"pture thirdorder neighborhood information (nodes that are 3 hops away from the current node). Each layer concatenates all preceding outputs as the input. 2 Densely Connected GCNs In this section, we will present the basic components used for constructing our DCGCN model. (Gehring et al., 2017). It is therefore worthwhile to explore the possibility of applying deeper GCNs that are able to capture more non-local information associated with the graph for graphto-sequence learning. Prior efforts have tried to train deep GCNs by incorporating residual connections (Bastings et al., 2017). Xu et al. (2018) show that vanilla residual connections proposed by He et al. (2016) are not effective for graph neural networks. They next attempt to resolve this issue by adding additional recurrent layers on top of graph convolutional layers. However, they are still confined to relatively shallow GCNs architectures (at most 6 layers in their experiments), which may not be able to capture the rich nonlocal interactions for larger graphs. In this paper, to better address the issue of learning deeper GCNs, we introduce dense connectivity to GCNs and propose the novel densely connected graph convolutional netw"
S18-1113,J92-4003,0,0.36803,"ty et al., 2001) models, we used the CRF++ implementation (Kudo, 2005). For the feature extraction, we used spaCy5 to extract the part-of-speech (POS) features and a C++ implementation (Liang, 2005) of the Brown clustering algorithm. For SubTask 1, our baseline models are the SVM and NB baselines with bag-of-words features. We also performed some hyper-parameter tuning based on the development set. Other simple baselines, such as random uniform and stratified, are also included as a comparison. For SubTask 2, we used the CRF baseline with unigrams, bigrams, POS, and Brown clustering features (Brown et al., 1992). CRF model was trained only on the malware related sentences in https://cuckoosandbox.org/ http://www.statnlp.org/research/resources 5 701 https://spacy.io/ Train Dev Test Action 3,202 122 125 Entity 6,875 254 249 Modifier 2,011 79 79 Total 12,088 455 453 Table 3: Data distribution of SubTask 2. Train Dev Test #Root 3,378 111 97 #ActionMod 1,859 74 52 #ActionObj 2,552 110 86 #ModObj 1,760 74 53 #SubjAction 2,307 82 72 Total 11,856 451 360 Table 4: Data distribution of SubTask 3. Train Dev Test #ActName 1,154 46 34 #Capability 2,817 102 88 #StratObj 2,206 77 70 #TactObj 1,783 63 64 Total 7,960"
S18-1113,D14-1162,0,0.0820519,"Test Action 3,202 122 125 Entity 6,875 254 249 Modifier 2,011 79 79 Total 12,088 455 453 Table 3: Data distribution of SubTask 2. Train Dev Test #Root 3,378 111 97 #ActionMod 1,859 74 52 #ActionObj 2,552 110 86 #ModObj 1,760 74 53 #SubjAction 2,307 82 72 Total 11,856 451 360 Table 4: Data distribution of SubTask 3. Train Dev Test #ActName 1,154 46 34 #Capability 2,817 102 88 #StratObj 2,206 77 70 #TactObj 1,783 63 64 Total 7,960 288 256 Table 5: Data distribution of SubTask 4. Villani (Loyola et al., 2018) submitted only to SubTask 1. They used word-embeddings initialized using Glove vectors (Pennington et al., 2014) trained on Wikipedia text to represent the tokens. In addition to that, they also used an LSTM to get another token representation from the characters. After that, they trained a binary classifier using Bi-directional Long Short-Term Memory network (BiLSTM) (Graves et al., 2013). They made use of attention mechanism (Luong et al., 2015) to weigh the importance of the tokens. the training set. The Brown clustering features for words were trained on the 84 additional unannotated APT reports provided with the training materials. For SubTask 3, a simple rule-based model was utilized. The rules ar"
S18-1113,D14-1181,0,0.00659993,"Missing"
S18-1113,S18-1140,0,0.0248205,"Missing"
S18-1113,S18-1144,0,0.0333073,"Missing"
S18-1113,P17-1143,1,0.5114,"est of our knowledge, we introduce the world’s largest publicly available dataset of annotated malware reports in this task. This task received in total 18 submissions from 9 participating teams. 1 Figure 1: Annotated sentence and sentence fragment from MalwareTextDB. Such annotations provide semantic-level information to the text. area. Even though there exists a large repository of malware related texts online, the sheer volume and diversity of these texts make it difficult for NLP researchers to quickly move to this research field. Another challenge is that most of the data is unannotated. Lim et al. (2017) has introduced a dataset of annotated malware reports for facilitating future NLP work in cybersecurity. In the light of that, we improved Lim’s malware dataset to create, to the best of our knowledge, the world’s largest publicly available dataset of annotated malware reports. The aim of our annotation is to mark the words and phrases in malware reports that describe the behaviour and capabilities of the malware and assign them to some certain categories. Most of the machine learning efforts in the task of malware detection were based on the system calls. Rieck et al. (2011) and Alazab et al"
S18-1113,D15-1166,0,0.0101439,"54 46 34 #Capability 2,817 102 88 #StratObj 2,206 77 70 #TactObj 1,783 63 64 Total 7,960 288 256 Table 5: Data distribution of SubTask 4. Villani (Loyola et al., 2018) submitted only to SubTask 1. They used word-embeddings initialized using Glove vectors (Pennington et al., 2014) trained on Wikipedia text to represent the tokens. In addition to that, they also used an LSTM to get another token representation from the characters. After that, they trained a binary classifier using Bi-directional Long Short-Term Memory network (BiLSTM) (Graves et al., 2013). They made use of attention mechanism (Luong et al., 2015) to weigh the importance of the tokens. the training set. The Brown clustering features for words were trained on the 84 additional unannotated APT reports provided with the training materials. For SubTask 3, a simple rule-based model was utilized. The rules are listed in the Appendix section of our ACL paper. They consist of simple rules, such as connecting a Modifier token to the nearest Action token with ActionMod relation. Finally, for SubTask 4, we trained SVM and NB model with bag-of-words features. The features for SubTask 4 are extracted from token groups, which are the set of tokens c"
S18-1113,P16-1101,0,0.0593973,"Unfortunately, none of the teams submitted to SubTask 3. Participants generally submitted to both SubTask 1 and 2. Here is the list of the participants who submitted a system description paper together with a brief summary of the method they used: DM NLP (Ma et al., 2018) also submitted to 702 documents from the web and the training corpus. SubTask 1 and 2, but focuses on SubTask 2 and just used the predicted output labels from SubTask 2 to get the predictions for SubTask 1. They model this task as a sequence labeling task and used a hybrid approach with BiLSTM-CNN-CRF following the method of Ma and Hovy (2016). The CNN layer was used to extract char-level feature representation. They then added other features, such as POS, dependency labels, chunk labels, NER labels, and brown clustering labels as the input to BiLSTM layer. They also made use of word-embeddings, pre-trained using unlabeled data. The output of the BiLSTM layer is then fed into a CRF layer that makes the entity label prediction. UMBC (Padia et al., 2018) participated in SubTask 1, 2 and 4. They are the only team participated in SubTask 4. They used a MultiLayer Perceptron model for the submission of SubTask 1. After the submission de"
