2008.jeptalnrecital-long.18,steinberger-etal-2006-jrc,0,0.0288159,"Missing"
2008.jeptalnrecital-long.18,W97-0213,0,0.0620634,"Missing"
2008.jeptalnrecital-long.18,E03-1026,0,0.0235623,"Missing"
2020.iwltp-1.5,L18-1515,1,0.861276,"Missing"
2020.iwltp-1.5,L18-1210,1,0.883378,"Missing"
2020.iwltp-1.5,gavrilidou-etal-2012-meta,1,0.873564,"Missing"
2020.iwltp-1.5,2020.lrec-1.420,1,0.867196,"Missing"
2020.iwltp-1.5,L18-1205,1,0.86912,"Missing"
2020.iwltp-1.5,piperidis-2012-meta,1,0.79099,"semantic and structural mapping had 4.1. META META-SHARE36 has been developed as the infrastructural arm of META-NET37 and has served as a component of a language technology marketplace for researchers, developers, professionals and industrial players, catering for the full development cycle of language technology, from research to innovative products and services. It has been designed as a network of repositories that store language resources (data, tools and processing services) documented with high-quality metadata, aggregated in central inventories allowing for uniform search and access (Piperidis, 2012). Repositories can be local, set up and maintained 28 https://www.sshopencloud.eu/news/using-corporaimplementing-validation-sshoc-masterclass 29 http://delad.net 30 https://ace.ruhosting.nl 31 https://tla.mpi.nl/ 32 https://talkbank.org/ 33 https://gdpr-info.eu/ 34 https://www.europeana.eu/ 35 https://www.eric-forum.eu/the-eric-landscape/ www.meta-share.eu 37 www.meta-net.eu 36 31 cessing service (e.g. sentence splitting, part-of-speech tagging), running either locally or remotely. This implementation has been used for powering the language processing layer of the CLARIN:EL node (Piperidis et"
2020.iwltp-1.5,2020.lrec-1.407,1,0.850312,"Missing"
2020.iwltp-1.5,2020.lrec-1.406,1,0.820998,"Missing"
2020.iwltp-1.5,2020.lrec-1.405,1,0.789844,"Missing"
2020.lrec-1.417,broeder-etal-2008-building,0,0.0816072,"Missing"
2020.lrec-1.417,L18-1515,1,0.568782,"Missing"
2020.lrec-1.417,L18-1210,1,0.800222,"Missing"
2020.lrec-1.417,hinrichs-krauwer-2014-clarin,0,0.0177308,"nfrastructures. It is strongly rooted in the humanities and the field of Natural Language Processing (NLP) and has the mission to create and maintain an infrastructure to support the sharing, use and sustainable availability of language data and tools for research in the humanities and social sciences (SSH) 1 See also https://www.clarin.eu/fair Figure 1: Map of CLARIN members, observers, and participating centres by February 2020. and beyond.2 Since its early days, the CLARIN consortium has aimed at building both a technical infrastructure and a sustainable organisation (Broeder et al., 2008; Hinrichs and Krauwer, 2014) while adhering to the interoperability paradigm at a range of levels. CLARIN has always operated in line with the European agenda for Open Science3 and it can be seen as an adopter of the FAIR data principles avant la lettre (de Jong et al., 2018). The focus on interoperability can be further illustrated along several dimensions, including the ambition to address the challenge of overcoming the obstacles stem2 See https://www.clarin.eu/content/vision-and-strategy See the 2016 Background note on Open Science available at https://ec.europa.eu/research/openscience/pdf/openaccess/ background note"
2020.lrec-1.417,P10-4005,0,0.0419607,"ort for scholarly workflows will be described in more detail. 2.1. In an infrastructure for research data both static resources, i.e. the data, and tools to process the data, are in place. The tools of a distributed infrastructure must be accessible from different locations. Interoperability ensures that linguistic tools can be combined with language data in a common processing pipeline. In CLARIN, web services have been put in place to encapsulate these tools and combine them in a common serviceoriented architecture. The first CLARIN activity in this field led to the development of WebLicht (Hinrichs et al., 2010). The web-based linguistic chaining tool provides an environment that allows the processing of textually given resources in a pipeline architecture. WebLicht has been applied in different processing tasks and for resources of different languages (Schmidt et al., 2016; C¸o¨ ltekin, 2015). More recently, a tool has been developed (Zinn, 2016) to provide guidance on which service is recommended for which data, known as the Language Resource Switchboard.8 The basic assumption behind the Switchboard is the focus on achieving a fairly basic but well-tested and robust level of interoperability, based"
2020.lrec-1.417,van-uytvanck-etal-2012-semantic,1,0.860288,"Missing"
2020.peoples-1.15,2020.acl-main.112,0,0.0407795,"Missing"
2020.peoples-1.15,W18-6218,1,0.860007,"Missing"
2020.peoples-1.15,S18-1001,0,0.0303931,"ovene) and two topics (migrants and LGBT). We show significant and consistent improvements in automatic classification across all languages and topics, as well as consistent (and expected) emotion distributions across all languages and topics, proving for the manually corrected lexicons to be a useful addition to the severely lacking area of emotion lexicons, the crucial resource for emotive analysis of text. 1 Introduction Emotion lexicons are rather scarce resources for most languages (Buechel et al., 2020), although they are a very important ingredient for robust emotion detection in text (Mohammad et al., 2018). They are mostly differentiated between depending on the way they encode emotions – either via continuous or discrete representations (Calvo and Mac Kim, 2013). In this work, we present emotion lexicons for three languages – Croatian, Dutch and Slovene, developed by manually correcting automatic translations that are part of the NRC Emotion Lexicon (Mohammad and Turney, 2013). In that lexicon, sentiment (positive or negative) and a discrete model of emotion covering anger, anticipation, disgust, fear, joy, sadness, surprise and trust, are encoded via a binary variable for each emotion. The si"
2021.wassa-1.16,W19-3501,0,0.0194155,"ied. We propose the hypothesis that stylometric characteristics of hateful writing are distinctive enough to contribute to the hate speech detection task. In other words, hate speech acts as a specific text type with an associated writing style. On the other hand, we are motivated by psychological and sociological studies, which correlate toxic behaviour online with the emotional profile of the user (Kokkinos and Kipritsi, 2012). However, unlike previous research that used sentiment information for detecting unacceptable content (Davidson et al., 2017; Dani et al., 2017; Van Hee et al., 2018; Brassard-Gourdeau and Khoury, 2019), we test whether we are able to capture some of these phenomena by going beyond the sentiment level (positive / negative / neutral) to a more fine-grained emotion level. We compare the performance of stylometric and emotion-based features with commonly used features for hate speech detection: words, character n-grams, and their combination, and with more recent deep learning models that currently provide the state-of-the-art results for the hate speech detection task (Mandl et al., 2019; Basile et al., 2019): convolutional neural networks (CNN), long shortterm memory networks (LSTM), and bidi"
2021.wassa-1.16,N19-1423,0,0.0584378,"Missing"
2021.wassa-1.16,P82-1020,0,0.702682,"Missing"
2021.wassa-1.16,W14-0908,0,0.349442,"that the style and emotional dimension of hateful textual content may provide useful cues for its detection. We investigate this through a binary hate speech classification task using features that model such information, i.e., function words and emotion-based features. The latter are operationalized in terms of the types of emotions expressed and the frequency of emotionconveying words in the data. Function word usage is one of the most important and revealing aspects of style in written language, as shown by numerous studies in stylometric analysis for authorship attribution (Grieve, 2007; Kestemont, 2014; Markov et al., 2018). While stylometric characteristics have been implicitly included in some hate speech detection studies (e.g., in bag-of-words or character-level models), 149 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 149–159 April 19, 2021. ©2021 Association for Computational Linguistics their impact on the task has not been studied. We propose the hypothesis that stylometric characteristics of hateful writing are distinctive enough to contribute to the hate speech detection task. In other words, hate speech a"
2021.wassa-1.16,D14-1181,0,0.00897115,"Missing"
2021.wassa-1.16,2020.peoples-1.15,1,0.377744,"Missing"
2021.wassa-1.16,K18-2016,0,0.0269953,"Missing"
2021.wassa-1.16,P19-1163,0,0.0452067,"onality, religion, or other characteristics (Nockleby, 2000). The exact definition of hate speech, however, remains a disputed topic, as it is a subjective and multi-interpretable concept (Waseem et al., 2017; Poletto et al., 2020). The lack of a consensus on its definition poses a challenge to hate speech annotation. Annotating hateful content remains prone to personal bias and is culture-dependent, which often results in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggering events (Florio et al., 2020), moreover, users may adapt their lexical choices as a countermeasure against identification or introduce minor misspellings to bypass filtering systems (Berger and Perez, 2006; Vidgen et al., 2019). Therefore, we intend to investigate more abstract features, less susceptible to specific vocabulary, topic or corpus bias, which we examine in in-domain and crossdomain settings:"
2021.wassa-1.16,N03-1033,0,0.184881,"llness on parade. Table 3 shows an example of the representation of this message through the features described above. From the POS & FW & emotion word representations, n-grams (n = 1– 3) are built.4 The count of emotionally-charged words and the emotion associations were added as additional feature vectors. Part-of-speech (POS) POS features capture the morpho-syntactic patterns in a text, and are indicative of hate speech, especially when used in combination with other types of features (Warner and Hirschberg, 2012; Robinson et al., 2018). POS tags were obtained with the Stanford POS Tagger (Toutanova et al., 2003). We used the same 17 universal POS tags for the three languages and built n-grams from this representation with n = 1–3. Stylometric features Function words (FW) are considered one of the most important stylometric feature types (Kestemont, 2014). They clarify the relationships between the content-carrying elements of a sentence, and introduce syntactic structures like verbal complements, relative clauses, and questions (Smith and Witten, 1993). With respect to emotion features, FW can appear as quantifiers, intensifiers (e.g., very good) or modify the emotion phrase Mental illness on parade"
2021.wassa-1.16,R15-1086,1,0.828623,"Missing"
2021.wassa-1.16,W19-3509,0,0.0395717,"esults in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggering events (Florio et al., 2020), moreover, users may adapt their lexical choices as a countermeasure against identification or introduce minor misspellings to bypass filtering systems (Berger and Perez, 2006; Vidgen et al., 2019). Therefore, we intend to investigate more abstract features, less susceptible to specific vocabulary, topic or corpus bias, which we examine in in-domain and crossdomain settings: training and testing on social media datasets belonging to same/different domains, for three languages: English, Slovene, and Dutch. Our hypothesis is that the style and emotional dimension of hateful textual content may provide useful cues for its detection. We investigate this through a binary hate speech classification task using features that model such information, i.e., function words and emotion-based feature"
2021.wassa-1.16,W12-2103,0,0.0286242,"sage is. Consider the following English comment from our data belonging to the hate speech class: Mental illness on parade. Table 3 shows an example of the representation of this message through the features described above. From the POS & FW & emotion word representations, n-grams (n = 1– 3) are built.4 The count of emotionally-charged words and the emotion associations were added as additional feature vectors. Part-of-speech (POS) POS features capture the morpho-syntactic patterns in a text, and are indicative of hate speech, especially when used in combination with other types of features (Warner and Hirschberg, 2012; Robinson et al., 2018). POS tags were obtained with the Stanford POS Tagger (Toutanova et al., 2003). We used the same 17 universal POS tags for the three languages and built n-grams from this representation with n = 1–3. Stylometric features Function words (FW) are considered one of the most important stylometric feature types (Kestemont, 2014). They clarify the relationships between the content-carrying elements of a sentence, and introduce syntactic structures like verbal complements, relative clauses, and questions (Smith and Witten, 1993). With respect to emotion features, FW can appear"
2021.wassa-1.16,W16-5618,0,0.00931299,"entation, nationality, religion, or other characteristics (Nockleby, 2000). The exact definition of hate speech, however, remains a disputed topic, as it is a subjective and multi-interpretable concept (Waseem et al., 2017; Poletto et al., 2020). The lack of a consensus on its definition poses a challenge to hate speech annotation. Annotating hateful content remains prone to personal bias and is culture-dependent, which often results in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggering events (Florio et al., 2020), moreover, users may adapt their lexical choices as a countermeasure against identification or introduce minor misspellings to bypass filtering systems (Berger and Perez, 2006; Vidgen et al., 2019). Therefore, we intend to investigate more abstract features, less susceptible to specific vocabulary, topic or corpus bias, which we examine in in-domain and cr"
2021.wassa-1.16,W17-3012,0,0.0138072,"orms words and character n-gram features under cross-domain conditions, and provides a significant boost to deep learning models, which currently obtain the best results, when combined with them in an ensemble. 1 Introduction Hate speech is commonly defined as communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics (Nockleby, 2000). The exact definition of hate speech, however, remains a disputed topic, as it is a subjective and multi-interpretable concept (Waseem et al., 2017; Poletto et al., 2020). The lack of a consensus on its definition poses a challenge to hate speech annotation. Annotating hateful content remains prone to personal bias and is culture-dependent, which often results in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggeri"
erjavec-etal-2010-jos,erjavec-2010-multext,1,\N,Missing
erjavec-etal-2010-jos,dzeroski-etal-2006-towards,1,\N,Missing
erjavec-etal-2010-jos,erjavec-krek-2008-jos,1,\N,Missing
erjavec-fiser-2006-building,W02-0808,0,\N,Missing
erjavec-fiser-2006-building,erjavec-2004-multext,1,\N,Missing
fiser-etal-2010-learning,W09-4405,0,\N,Missing
fiser-etal-2010-learning,muresan-klavans-2002-method,0,\N,Missing
fiser-etal-2010-learning,W06-0203,0,\N,Missing
fiser-etal-2010-learning,C92-2082,0,\N,Missing
fiser-etal-2010-learning,erjavec-fiser-2006-building,1,\N,Missing
fiser-etal-2010-learning,storrer-wellinghoff-2006-automated,0,\N,Missing
fiser-etal-2010-learning,walter-2008-linguistic,0,\N,Missing
fiser-etal-2010-learning,vintar-fiser-2008-harvesting,1,\N,Missing
fiser-etal-2010-learning,W09-4410,0,\N,Missing
fiser-etal-2010-learning,W09-4406,0,\N,Missing
fiser-etal-2012-addressing,N09-5005,0,\N,Missing
fiser-etal-2012-addressing,E09-1005,0,\N,Missing
fiser-etal-2012-addressing,N03-1015,0,\N,Missing
fiser-etal-2012-addressing,P99-1067,0,\N,Missing
fiser-etal-2012-addressing,P06-1014,0,\N,Missing
fiser-etal-2012-addressing,W11-1204,1,\N,Missing
fiser-etal-2014-slowcrowd,sagot-fiser-2012-cleaning,1,\N,Missing
L16-1573,W11-2123,0,0.0839554,"Missing"
L16-1573,W14-0405,1,0.889826,"Missing"
L16-1573,ljubesic-etal-2014-tweetcat,1,0.901883,"Missing"
L16-1573,R15-1049,1,0.901157,"Missing"
L16-1573,W02-2021,0,0.889319,"Missing"
L16-1573,D15-1275,0,0.0399206,"Missing"
L16-1573,W98-1504,0,0.311496,"Missing"
L16-1573,tufis-ceausu-2008-diac,0,0.0661917,"Missing"
L16-1573,P94-1013,0,0.196687,"Missing"
L18-1210,hinrichs-krauwer-2014-clarin,0,0.165251,"rn are newspaper, parliamentary, CMC (computer-mediated communication), and parallel corpora. We focus on their presentation within the infrastructure, their metadata in terms of size, temporal coverage, annotation, accessibility and license, and discuss current problems. Keywords: language resources, research infrastructure, open science, digital humanities and social sciences, CLARIN 1. Introduction CLARIN is a European Research Infrastructure that has been established to support the accessibility of language resources and technologies to researchers from the Humanities and Social Sciences (Krauwer and Hinrichs, 2014). CLARIN’s vision, mission and design are aimed at findability, accessibility, interoperability and re-usability of its resources, tools and services to support researchers in the Humanities and Social Sciences (SSH) (de Jong et al., 2018; De Smedt et al., 2018). At the time of writing, CLARIN has 20 member and 2 observer countries which provide numerous language resources and tools through certified data centres. Access to these resources is enhanced by the Virtual Language Observatory (VLO) portal which enables searching for resources and provides a uniform display of highly-granular Compone"
L18-1210,van-uytvanck-etal-2012-semantic,0,0.330742,"Missing"
L18-1515,broeder-etal-2008-building,0,0.633208,"Missing"
L18-1515,L18-1210,1,0.860668,"Missing"
L18-1515,hinrichs-krauwer-2014-clarin,0,0.0971141,"rastructure with data centres (nodes) across Europe and beyond. The activities in CLARIN basically take place at two levels. One is the central level: the Board of Directors and the technical, communication, and administrative staff. The other is the national level: in each member country, the 1 http://www.clarin.eu national activities have by far the largest volume. Previous CLARIN activities have been described in multiple publications that have been disseminated through many channels. At LREC 2014 an overview of language resources, tools, and services on offer through CLARIN was presented (Hinrichs and Krauwer, 2014). The current paper focuses on the overall vision and in particular the adherence to the principles of FAIR and Responsible Data Science. 1.1. A Bit of History In the preparatory phase 2008–2011 CLARIN was funded by the European Commission. It was established as a European Research Infrastructure Consortium (ERIC)2 in 2012; the basic funding comes from the member countries. When it was established in 2012 CLARIN had nine members, and in 2017 it has grown to 19 members and two observers. Additionally, CLARIN has a special agreement with Carnegie Mellon University in the USA. This growth, which"
L18-1515,J16-3007,1,0.884286,"Missing"
L18-1515,trilsbeek-etal-2008-grid,0,0.069527,"Missing"
L18-1515,van-uytvanck-etal-2012-semantic,1,0.911593,"Missing"
ljubesic-etal-2014-tweetcat,W10-0513,0,\N,Missing
ljubesic-etal-2014-tweetcat,W14-0405,1,\N,Missing
ljubesic-etal-2014-tweetcat,baroni-bernardini-2004-bootcat,0,\N,Missing
R11-1018,P02-1051,0,0.11789,"Missing"
R11-1018,W11-1204,1,0.736877,"Missing"
R11-1018,C10-2055,0,0.0310454,", Hissar, Bulgaria, 12-14 September 2011. precision of the extracted translation equivalents that consequently results in a more usable resource in a real-world setting. And finally, we are not limiting our experiments to nouns, but are working with all content words. term and its translation share similar contexts. The method consists of two steps: first, contexts of words are modeled and then similarity between the source-language and target-language contexts are measured with the help of a dictionary. Most approaches represent contexts as weighted collections of words using log-likelihood (Ismail and Manandhar, 2010), TF-IDF (Fung, 1998) or PMI (Shezaf and Rappoport, 2010). After building context vectors for words in both languages, the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure, such as cosine (Fung, 1998), Jaccard (Otero and Campos, 2005) or Dice (Otero, 2007). 3 Building Resources In this section we present two resources we built for this experiment: the comparable corpus and the seed lexicon. Since our goal in the experiment reported in this paper is the extraction of translation equivalents for the genera"
R11-1018,W02-0902,0,0.827561,"Missing"
R11-1018,P99-1067,0,0.948155,"elated Languages Darja Fiˇser Nikola Ljubeˇsi´c Faculty of Arts, Faculty of Humanities and Social Sciences, Univeristy of Ljubljana University of Zagreb darja.fiser@ff.uni-lj.si nikola.ljubesic@ffzg.hr Abstract This is why an alternative approach has become popular in recent years. It relies on texts in two languages which are not parallel but comparable (Fung, 1998; Rapp, 1999) and therefore easier to compile, especially from the increasingly rich web data (Xiao and McEnery, 2006). The approach relies on the assumption that the term and its translation appear in similar contexts (Fung, 1998; Rapp, 1999). This means that the translation of a source word can be found by identifying a target word which has the most similar context vector in a comparable corpus. However, a direct comparison of vectors in two different languages is not possible, which is why a dictionary is needed to first translate the features of source context vectors into the target language and compute similarity on those. But this step seems paradoxical: the very reason why we are applying the complex comparable corpus technique for extracting translation equivalents is the fact that we do not have a bilingual dictionary at"
R11-1018,C04-1137,0,0.432778,"used as similarity measure. Finally, ten top-ranking translation candidates are kept for automatic and manual evaluation. We try to improve the results by extending the seed lexicon with contextually confirmed cognates as well as with first translations of the most frequent 4.3 Extending the Seed Lexicon with Cognates In order to beat the baseline we first extended the seed lexicon with cognates. We calculated them with BI-SIM, the longest common subsequence of 127 bigrams with a space prefix added to the beginning of each word in order to punish the differences at the beginning of the words (Kondrak and Dorr, 2004). The threshold for cognates has been empirically set to 0.7. In this step, translation equivalents were calculated as explained above for all content words (nouns, adjectives, verbs and adverbs), taking into account 20 top-ranking translations and analyzing them for cognate clues in that order. If we found a translation equivalent that met the cognate threshold of 0.7, we added that pair to the lexicon. If the seed lexicon already contained a translation for a cognate we identified with this procedure, we replaced the existing lexicon entry with the new identified cognate pair. Replacing entr"
R11-1018,C04-1089,0,0.139573,"Missing"
R11-1018,P10-1011,0,0.141428,"e extracted translation equivalents that consequently results in a more usable resource in a real-world setting. And finally, we are not limiting our experiments to nouns, but are working with all content words. term and its translation share similar contexts. The method consists of two steps: first, contexts of words are modeled and then similarity between the source-language and target-language contexts are measured with the help of a dictionary. Most approaches represent contexts as weighted collections of words using log-likelihood (Ismail and Manandhar, 2010), TF-IDF (Fung, 1998) or PMI (Shezaf and Rappoport, 2010). After building context vectors for words in both languages, the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure, such as cosine (Fung, 1998), Jaccard (Otero and Campos, 2005) or Dice (Otero, 2007). 3 Building Resources In this section we present two resources we built for this experiment: the comparable corpus and the seed lexicon. Since our goal in the experiment reported in this paper is the extraction of translation equivalents for the general vocabulary, we built a Croatian-Slovene comparable news"
R11-1018,P00-1056,0,0.358778,"Missing"
R15-1049,N13-1037,0,0.0510739,"Missing"
R15-1049,I11-1100,0,0.0306528,"Missing"
R15-1049,P11-2008,0,0.320414,"Missing"
R15-1049,D12-1039,0,0.063609,"Missing"
R15-1049,1981.tc-1.7,0,0.844586,"Missing"
R15-1049,ljubesic-etal-2014-tweetcat,1,0.813486,"Missing"
R15-1049,I13-1041,0,0.0267248,"Missing"
sagot-fiser-2012-cleaning,widdows-ferraro-2008-semantic,0,\N,Missing
sagot-fiser-2012-cleaning,P10-1023,0,\N,Missing
sagot-fiser-2012-cleaning,Y09-1013,1,\N,Missing
sagot-fiser-2012-cleaning,bond-etal-2008-boot,0,\N,Missing
sagot-fiser-2012-cleaning,S10-1002,0,\N,Missing
vintar-fiser-2008-harvesting,steinberger-etal-2006-jrc,0,\N,Missing
vintar-fiser-2008-harvesting,W03-1805,0,\N,Missing
vintar-fiser-2008-harvesting,W02-0808,0,\N,Missing
vintar-fiser-2008-harvesting,P02-1033,0,\N,Missing
vintar-fiser-2008-harvesting,W03-1807,0,\N,Missing
vintar-fiser-2008-harvesting,erjavec-fiser-2006-building,1,\N,Missing
vintar-fiser-2008-harvesting,dias-nunes-2004-evaluation,0,\N,Missing
W11-1204,W02-0902,0,0.339098,"Missing"
W11-1204,C10-1085,0,0.0143031,"n equivalent of a term is therefore reduced to finding the word in the target language whose context vector is most similar to the source term’s context vector based on their occurrence in a comparable corpus. This is basically a three-step procedure: (1) Building context vectors. When representing a word’s context, some approaches look at a simple co-occurrence window of a certain size while others include some syntactic information as well. For example, Otero (2007) proposes binary dependences previously extracted from a parallel corpus, while Yu and Tsujii (2009) use dependency parsers and Marsi and Krahmer (2010) use syntactic trees. Instead of context windows, Shao and Ng (2004) use language models. Next, words in co-occurrence vectors can be represented as binary features, by term frequency or weighted by different association measures, such as TF-IDF (Fung, 1998), PMI (Shezaf and Rappoport, 2010) or, one of the most popular, the log likelihood score. Approaches also exist that weigh cooccurrence terms differently if they appear closer to or further from the nucleus word in the context (e.g. Saralegi et al., 2008). (2) Translating context vectors. Finding the most similar context vectors in the sour"
W11-1204,P00-1056,0,0.315855,"Missing"
W11-1204,P99-1067,0,0.863106,"quency threshold for context vectors, the drop in precision is much slower than the increase of recall. 1 Introduction Research into using comparable corpora in NLP has gained momentum in the past decade largely due to limited availability of parallel data for many language pairs and domains. As an alternative to already established parallel approaches (e.g. Och 2000, Tiedemann 2005) the comparable corpusbased approach relies on texts in two or more languages which are not parallel but nevertheless share several parameters, such as topic, time of publication and communicative goal (Fung 1998, Rapp 1999). The main advantage of this approach is the simpler, faster and more time efficient compilation of comparable corpora, especially from the rich web data (Xiao & McEnery 2006). In this paper we describe the compilation process of a large comparable corpus of texts on healthrelated topics for Slovene and English that were published on the web. Then we report on a set of experiments we conducted in order to automatically extract translation equivalents for terms from the health domain. The parameters we tested and analysed are: 1- and 2-way translations of context vectors with a seed lexicon, th"
W11-1204,C04-1089,0,0.680665,"get language whose context vector is most similar to the source term’s context vector based on their occurrence in a comparable corpus. This is basically a three-step procedure: (1) Building context vectors. When representing a word’s context, some approaches look at a simple co-occurrence window of a certain size while others include some syntactic information as well. For example, Otero (2007) proposes binary dependences previously extracted from a parallel corpus, while Yu and Tsujii (2009) use dependency parsers and Marsi and Krahmer (2010) use syntactic trees. Instead of context windows, Shao and Ng (2004) use language models. Next, words in co-occurrence vectors can be represented as binary features, by term frequency or weighted by different association measures, such as TF-IDF (Fung, 1998), PMI (Shezaf and Rappoport, 2010) or, one of the most popular, the log likelihood score. Approaches also exist that weigh cooccurrence terms differently if they appear closer to or further from the nucleus word in the context (e.g. Saralegi et al., 2008). (2) Translating context vectors. Finding the most similar context vectors in the source and target language is not straightforward because a direct compa"
W11-1204,steinberger-etal-2006-jrc,0,0.250641,"Missing"
W11-1204,2009.mtsummit-posters.26,0,0.0448074,". The task of finding the appropriate translation equivalent of a term is therefore reduced to finding the word in the target language whose context vector is most similar to the source term’s context vector based on their occurrence in a comparable corpus. This is basically a three-step procedure: (1) Building context vectors. When representing a word’s context, some approaches look at a simple co-occurrence window of a certain size while others include some syntactic information as well. For example, Otero (2007) proposes binary dependences previously extracted from a parallel corpus, while Yu and Tsujii (2009) use dependency parsers and Marsi and Krahmer (2010) use syntactic trees. Instead of context windows, Shao and Ng (2004) use language models. Next, words in co-occurrence vectors can be represented as binary features, by term frequency or weighted by different association measures, such as TF-IDF (Fung, 1998), PMI (Shezaf and Rappoport, 2010) or, one of the most popular, the log likelihood score. Approaches also exist that weigh cooccurrence terms differently if they appear closer to or further from the nucleus word in the context (e.g. Saralegi et al., 2008). (2) Translating context vectors."
W11-1204,P10-1011,0,\N,Missing
W12-0112,D07-1007,0,0.0339038,"te collocations, while Salam et al. (2009) used wordnet for disambiguation and the choice of the correct translation equivalent in an English to Bengali SMT system. WSD for machine translation purposes slightly differs from traditional WSD, because distinct source language senses, which share the same translation equivalent, need not be differentiated in WSD (Vickrey et al. 2005). This phenomenon is known as parallel ambiguities and is particularly common among related languages (Resnik and Yarowsky 2000). Although early experiments failed to provide convincing proof that WSD can improve SMT, Carpuat and Wu (2007), Chan et al. (2007) and Ali et al. (2009) clearly demonstrate that incorporating a word sense disambiguation system on the lexical level brings significant improvement according to all common MT evaluation metrics. Still, using wordnet as the source of sense inventories has been heavily criticized not just in the context of MT (Apidianaki 2009), but also within other language processing tasks. The most notorious arguments against wordnet are its high granularity and - as a consequence - high similarity between some senses, but its global availability and universality seem to be advantages tha"
W12-0112,P07-1005,0,0.0262657,"Salam et al. (2009) used wordnet for disambiguation and the choice of the correct translation equivalent in an English to Bengali SMT system. WSD for machine translation purposes slightly differs from traditional WSD, because distinct source language senses, which share the same translation equivalent, need not be differentiated in WSD (Vickrey et al. 2005). This phenomenon is known as parallel ambiguities and is particularly common among related languages (Resnik and Yarowsky 2000). Although early experiments failed to provide convincing proof that WSD can improve SMT, Carpuat and Wu (2007), Chan et al. (2007) and Ali et al. (2009) clearly demonstrate that incorporating a word sense disambiguation system on the lexical level brings significant improvement according to all common MT evaluation metrics. Still, using wordnet as the source of sense inventories has been heavily criticized not just in the context of MT (Apidianaki 2009), but also within other language processing tasks. The most notorious arguments against wordnet are its high granularity and - as a consequence - high similarity between some senses, but its global availability and universality seem to be advantages that prevail in many ca"
W12-0112,erjavec-etal-2010-jos,1,0.812372,"mewhat in between; on the one hand we demonstrate the potential of WSD in MT, especially for cases where different MT systems disagree, and on the other hand we attribute most WSD errors to the inadequacy of the sense splitting in wordnet (see Discussion). 3 3.1 two belonging to the family of freely available statistical systems and the latter being a rulebased MT system developed by the Slovenian company Amebis. For the purposes of further analysis and comparison with our disambiguated corpus all texts original and translations - have been PoS-tagged and lemmatized using the JOS web service (Erjavec et al. 2010) for Slovene and ToTaLe (Erjavec et al. 2005) for English. Because we can only disambiguate content words, we retained only nouns, verbs, adjectives and adverbs and discarded the rest. After all these preprocessing steps our texts end up looking as follows: English: It was a bright cold day in April and the clocks were striking thirteen. English-preprocessed: be bright cold day April clock be strike Slovene-reference: Bil je jasen, mrzel aprilski dan in ure so bile trinajst. Slovene-reference-preprocessed: biti biti jasen mrzel aprilski dan ura biti biti Slovene-Google: Bilo je svetlo mrzel da"
W12-0112,H93-1036,0,0.260231,"ation version. Our results show that the ad hoc WSD strategies used by the evaluated MT systems can definitely be improved by a proper WSD algorithm, but also that wordnet is not the ideal semantic resource to help resolve translation dilemmas, mainly due to its fine sense granularity. 2 Word Sense Disambiguation Machine Translation and Wordnet-based approaches to improving MT have been successfully employed by numerous authors, on the one hand as a semantic resource to help resolve ambiguity, and on the other hand as a rich source of domain-specific translation equivalents. As early as 1993 (Knight 1993), wordnet was used as the lower ontology within 87 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 87–92, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics the PANGLOSS MT system. Yuseop et al. (2002) have employed LSA and the semantic similarity of wordnet literals to translate collocations, while Salam et al. (2009) used wordnet for disambiguation and the choice of the correct translation equivalent in an English to Bengali SMT system. WSD for machine translation purposes slightly differs f"
W12-0112,E09-1005,0,0.0872385,"Missing"
W12-0112,E09-1010,0,0.0176898,"d in WSD (Vickrey et al. 2005). This phenomenon is known as parallel ambiguities and is particularly common among related languages (Resnik and Yarowsky 2000). Although early experiments failed to provide convincing proof that WSD can improve SMT, Carpuat and Wu (2007), Chan et al. (2007) and Ali et al. (2009) clearly demonstrate that incorporating a word sense disambiguation system on the lexical level brings significant improvement according to all common MT evaluation metrics. Still, using wordnet as the source of sense inventories has been heavily criticized not just in the context of MT (Apidianaki 2009), but also within other language processing tasks. The most notorious arguments against wordnet are its high granularity and - as a consequence - high similarity between some senses, but its global availability and universality seem to be advantages that prevail in many cases (Edmonds and Kilgarriff 2002). Our experiments lie somewhat in between; on the one hand we demonstrate the potential of WSD in MT, especially for cases where different MT systems disagree, and on the other hand we attribute most WSD errors to the inadequacy of the sense splitting in wordnet (see Discussion). 3 3.1 two bel"
W12-0112,H05-1097,0,\N,Missing
W12-1001,W10-1835,0,0.0445592,"Missing"
W12-1001,W11-1505,1,0.442983,"latter option is more interesting for our case, as TEI files can already be structurally and linguistically annotated. Zip files are also supported, which enables uploading large datasets with many separate files. The Slovene corpora are encoded in TEI, and each corpus file contains the transcription of a single page, together with the link to its facsimile image. The page is also annotated with paragraphs, line breaks, etc. Such annotation is imported into CoBaLT but not displayed or modified, and appears again only in the export. The texts in our project were first automatically annotated (Erjavec, 2011): each text was sentence segmented and tokenised into words. Punctuation symbols (periods, commas, etc.) and white-spaces were preserved in the annotation so the original text and layout can be reconstructed from the annotated text. Each word form was assigned its modern-day equivalent, its PoS tag and modern day lemma. 3 4 <entry> <form type=&quot;lemma&quot;> <orth type=&quot;hypothetical&quot;>glasnik</orth> <gramGrp> <gram type=&quot;msd&quot;>Ncm</gram> <gram type=&quot;PoS&quot;>Noun</gram> <gram type=&quot;Type&quot;>common</gram> <gram type=&quot;Gender&quot;>masculine</gram> </gramGrp> <gloss>samoglasnik</gloss> <bibl>kontekst, Pleteršnik</bib"
W12-1001,erjavec-2012-goo300k,1,0.832245,"ate lemmatisation, which is especially useful for highly inflecting languages as it abstracts away from the inflectional variants of words, thereby enabling better text searching. To develop such resources, a good editor is needed that caters to the peculiarities of historical texts. Preferably it would combine the production of annotated corpora and corpus-based lexica. This paper presents CoBaLT, a Web-based editor which has already been used for developing language resources for several languages. We describe it within the framework of developing a gold-standard annotated reference corpus (Erjavec, 2012) and a large lexicon of historical Slovene. This paper is structured as follows: in the next section we describe the implementation and functionality of CoBaLT. In Section 3 we present the input and output corpus and lexicon formats, in particular from the perspective of our project. In Section 4 we compare existing tools serving a similar purpose to CoBaLT and discuss the advantages and disadvantages of the CoBaLT environment. The last section summarizes and lists our conclusions. Abstract This paper describes a Web-based editor called CoBaLT (Corpus-Based Lexicon Tool), developed to construc"
W12-1001,rognvaldsson-etal-2012-icelandic,0,\N,Missing
W13-2411,C10-1070,0,0.143276,"ng of pairs of words (RANDOM). Since the result of the procedure of identifying false friends in this setting is a single ranked list of lemma pairs where the ranking is performed by contextual or frequency dissimilarity, the same evaluation method can be applied as to evaluating a single query response in information retrieval. That is why we evaluated the output of each setting with average precision (AP), which averages over all precisions calculated on lists of false friend candidates built from each positive example upwards. 3. discounted log-odds (LO) first used in lexicon extraction by Laroche and Langlais (2010), showing consistently better performance than LL ; it is calculated from contingency table information as follows: LO = log X f We took under consideration the following association measures: IDF (t, V ) = log KL(v1 |v2 ) KL(v2 |v1 ) + 2 2 (O11 + 0.5)(O22 + 0.5) (O12 + 0.5)(O21 + 0.5) The following similarity measures were taken into account: 1. the well-known cosine measure (COSINE), 2. the Dice measure (DICE), defined in (Otero, 2008) as DiceMin, which has proven to be very good in various tasks of distributional 72 As three categories were encoded in the GOLD 2 gold standard, we weighted F"
W13-2411,R09-1054,0,0.148747,"etup and Section 6 reports on the results. We conclude the paper with final remarks and ideas for future work. 69 Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 69–77, c Sofia, Bulgaria, 8-9 August 2013. 2010 Association for Computational Linguistics 2 Related Work most similar but contextually most dissimilar word pairs. The feature weighting used throughout the related work is mostly plain frequency with one case of using TF-IDF (Nakov and Nakov, 2007) whereas cosine is the most widely used similarity measure (Nakov and Nakov, 2007; Nakov and Nakov, 2009; Schulz et al., 2004) while Mitkov et al. (2007) use skew divergence which is very similar to Jensen-Shannon divergence. The main differences between the work we report on in this paper and the related work are: Automatic detection of false friends was initially limited to parallel corpora but has been extended to comparable corpora and web snippets (Nakov et al., 2007). The approaches to automatically identify false friends fall into two categories: those that only look at orthographic features of the source and the target word, and those that combine orthographic features with the semantic"
W13-2411,J93-1003,0,0.241396,"Missing"
W13-2411,C04-1117,0,0.0588284,"Missing"
W13-2411,2007.jeptalnrecital-long.8,0,0.621539,"tify false friends fall into two categories: those that only look at orthographic features of the source and the target word, and those that combine orthographic features with the semantic ones. Orthographic approaches typically rely on combinations of a number of orthographic similarity measures and machine learning techniques to classify source and target word pairs to cognates, false friends or unrelated words and evaluate the different combinations against a manually compiled list of legitimate and illegitimate cognates. This has been attempted for English and French (Inkpen et al., 2005; Frunza and Inkpen, 2007) as well as for Spanish and Portuguese (Torres and Alu´ısio, 2011). Most of the approaches that combine orthographic features with the semantic ones have been performed on parallel corpora where word frequency information and alignments at paragraph, sentence as well as word level play a crucial role at singling out false friends, which has been tested on Bulgarian and Russian (Nakov and Nakov, 2009). Work on non-parallel data, on the other hand, often treats false friend candidates as search queries, and considers the retrieved web snippets for these queries as contexts that are used to estab"
W13-2411,W11-4508,0,0.116182,"Missing"
W13-2501,fiser-etal-2012-addressing,1,0.850739,"(Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resources, combined to the skewed distribution of the translations corresponding to different word senses, often lead to satisfying results. Nevertheless, the applicability of the methods is limited to languages and domains where bilingual resources are available. Moreover, by promoting the most frequent sense/translation, this approach neglects polysemy. We believe that feature disambiguation can lead to the production of cleaner vectors and, consequently, to higher quality results. The need to bypass pre-existing dictionaries has been addressed"
W13-2501,N03-1015,0,0.0355119,"howed that the results with an automatically created seed lexicon, based on language similarity, can be as good as with a pre-existing dictionary. But all these approaches work on closely-related languages and cannot be used as successfully for language pairs with little lexical overlap, such as English and Slovene, which is the case in this experiment. Regarding the translation of the source vectors, we use contextual information to disambiguate their features and translate them using clusters of semantically similar translations in the target language. A similar idea has been implemented by Kaji (2003) who performed sense-based word 3 3.1 Resources Comparable corpus The comparable corpus from which the bilingual lexicon will be extracted is a collection of English (EN) and Slovene (SL) texts extracted from Wikipedia. The February 2013 dumps of Wikipedia articles were downloaded and cleaned for both languages after which the English corpus was tokenized, part-of-speech (PoS) tagged and lemmatized with the TreeTagger (Schmid, 1994). The same pre-processing was applied to the Slovene corpus with the ToTaLe analyzer (Erjavec et al., 2010) which uses the TnT tagger (Brants, 2000) and was trained"
W13-2501,apidianaki-2008-translation,1,0.80435,"rd We evaluate the quality of the bilingual lexicons extracted from the comparable corpus by comparing them to a gold standard lexicon, which was built from the aligned English (Fellbaum, 1998) and Slovene wordnets (Fiˇser and Sagot, 2008). We extracted all English synsets from the Base Concept sets that belong to the Factotum domain and contain literals with polysemy levels 1-5 and their 4.2 Translation clustering The translations of the English words in the lexicon built as described in 3.2 are clustered according to their semantic proximity using a crosslingual Word Sense Induction method (Apidianaki, 2008). For each translation Ti of a word w, a vector is built from the content word co3 Language POS Nouns Source word sphere address {obravnava, reˇsevanje, obravnavanje} (dealing with) {naslov} (postal address) portion {kos} (piece) {obrok, porcija} (serving) {deleˇz} (share) figure {ˇstevilka, podatek, znesek} (amount) {slika} (image) {osebnost} (person) seal EN–SL weigh Verbs Slovene sense clusters {krogla} (geometrical shape) {sfera, podroˇcje} (area) {tesniti} (to be water-/airtight) {zapreti, zapeˇcatiti} (to close an envelope or some other container) {pretehtati} (consider possibilities) {t"
W13-2501,W02-0902,0,0.0403888,"e high quality of the exploited handcrafted resources, combined to the skewed distribution of the translations corresponding to different word senses, often lead to satisfying results. Nevertheless, the applicability of the methods is limited to languages and domains where bilingual resources are available. Moreover, by promoting the most frequent sense/translation, this approach neglects polysemy. We believe that feature disambiguation can lead to the production of cleaner vectors and, consequently, to higher quality results. The need to bypass pre-existing dictionaries has been addressed by Koehn and Knight (2002) who built the initial seed dictionary automatically, based on identical spelling features between English and German. Cognate detection has also been used by Saralegi et al. (2008) for extracting word translations from English-Basque comparable corpora. The cognate and seed lexicon approaches have been successfully combined by Fiˇser and Ljubeˇsi´c (2011) who showed that the results with an automatically created seed lexicon, based on language similarity, can be as good as with a pre-existing dictionary. But all these approaches work on closely-related languages and cannot be used as successf"
W13-2501,E09-1010,1,0.937001,"r that contains the feature. The translations found in the disambiguation output convey the sense of the features in the source vector, while the use of translation clusters permits to expand their translation with several variants. As a consequence, the translated vectors are less noisy and richer, and allow for the extraction of higher quality lexicons compared to simpler methods. 1 In this paper, we show how source vectors can be translated into the target language by a cross-lingual Word Sense Disambiguation (WSD) method which exploits the output of data-driven Word Sense Induction (WSI) (Apidianaki, 2009), and demonstrate how feature disambiguation enhances the quality of the translations extracted from the comparable corpus. This study extends our previous work on the topic (Apidianaki et al., 2012) by applying the proposed methods to a comparable corpus of general language (built from Wikipedia) and optimizing various parameters that affect the quality of the extracted translations. We expect the disambiguation to have a beneficial impact on the results given that polysemy is a frequent phenomenon in a general, mixed-domain corpus. Our experiments are carried out on the English-Slovene langu"
W13-2501,2005.mtsummit-papers.11,0,0.00864014,"approach to a specialized comparable corpus from the health domain (Apidianaki et al., 2012). The results were encouraging, showing how translation clustering and vector disambiguation help to improve the quality of the translations extracted from the comparable corpus. We believe that the positive impact of this approach will be more significant on lexicon extraction from a general language comparable corpus, in which polysemy is more prominent. 3.2 Parallel corpus The parallel corpus used for clustering and word sense induction consists of the Slovene-English parts of Europarl (release v6) (Koehn, 2005) and of JRC-Acquis (Steinberger et al., 2006) and amounts to approximately 35M words per language. A number of pre-processing steps are applied to the corpus prior to sense induction, such 2 Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD. Slovene equivalents which have been validated by a lexicographer. Of 1,589 such synsets, 200 were randomly selected and used as a gold standard for automatic evaluation of the method proposed in this paper. as elimination of sentence pairs with a great difference in length, lemmatization and PoS tagging with the TreeT"
W13-2501,C10-1085,0,0.0140729,"ranslation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resources, combined to the skewe"
W13-2501,A00-1031,0,0.0109396,"en implemented by Kaji (2003) who performed sense-based word 3 3.1 Resources Comparable corpus The comparable corpus from which the bilingual lexicon will be extracted is a collection of English (EN) and Slovene (SL) texts extracted from Wikipedia. The February 2013 dumps of Wikipedia articles were downloaded and cleaned for both languages after which the English corpus was tokenized, part-of-speech (PoS) tagged and lemmatized with the TreeTagger (Schmid, 1994). The same pre-processing was applied to the Slovene corpus with the ToTaLe analyzer (Erjavec et al., 2010) which uses the TnT tagger (Brants, 2000) and was trained on MultextEast corpora. The Wikipedia corpus contains about 1.5 billion tokens for English and almost 24 million tokens for Slovene. In previous work, we applied our approach to a specialized comparable corpus from the health domain (Apidianaki et al., 2012). The results were encouraging, showing how translation clustering and vector disambiguation help to improve the quality of the translations extracted from the comparable corpus. We believe that the positive impact of this approach will be more significant on lexicon extraction from a general language comparable corpus, in"
W13-2501,J05-4003,0,0.0420579,"polysemy is a frequent phenomenon in a general, mixed-domain corpus. Our experiments are carried out on the English-Slovene language pair but as the methods are totally data-driven, the approach can be easily applied to other languages. Introduction Large-scale comparable corpora are available in many language pairs and are viewed as a source of valuable information for multilingual applications. Identifying translation correspondences in this type of corpora permits to construct bilingual lexicons for low-resourced languages, and to complement and reduce the sparseness of existing resources (Munteanu and Marcu, 2005; Snover et al., 2008). The main assumption behind translation extraction from comparable corpora is that a source word and its translation appear in similar contexts (Fung, 1998; Rapp, 1999). So, in order to identify a translation correspondence between the two languages, the contexts of the source word and the candidate translation have to be compared. For this comparison to take place, the same vector space has to be produced, which means that the vectors of the one language have to be translated The paper is organized as follows: In the next section, we present some related work on bilingu"
W13-2501,J03-1002,0,0.00375309,"processing steps are applied to the corpus prior to sense induction, such 2 Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD. Slovene equivalents which have been validated by a lexicographer. Of 1,589 such synsets, 200 were randomly selected and used as a gold standard for automatic evaluation of the method proposed in this paper. as elimination of sentence pairs with a great difference in length, lemmatization and PoS tagging with the TreeTagger (for English) and ToTaLe (for Slovene) (Erjavec et al., 2010). Next, the corpus is word-aligned with GIZA++ (Och and Ney, 2003) and two bilingual lexicons are extracted, one for each translation direction (EN–SL/SL– EN). To clean the lexicons from noisy alignments, the translations are filtered on the basis of their alignment score and PoS, keeping only translations that pertain to the same grammatical category as the source word. We retain only intersecting alignments and use for clustering translations that translate a source word more than 10 times in the training corpus. This threshold reduces data sparseness issues that affect the clustering and eliminates erroneous word alignments. The filtered EN-SL lexicon con"
W13-2501,erjavec-etal-2010-jos,1,0.876362,"Missing"
W13-2501,2007.mtsummit-papers.26,0,0.0099246,"usters permits to expand feature translation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploit"
W13-2501,2009.mtsummit-posters.14,0,0.0248424,"d most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resources, combined to the skewed distribution of the translations corresponding to different word senses, often lead to satisfying results. Nevertheless, the applicability of the methods is limited to languages and domains where bilingual resources are available. Moreover, by promoting the most frequent sense/translation, this approach neglects polysemy. We believe that feature disambiguation can lead to the production of cleaner vectors and, consequently, to higher quality results. The need to bypass pre-existing diction"
W13-2501,R11-1018,1,0.902304,"Missing"
W13-2501,P99-1067,0,0.343625,"sily applied to other languages. Introduction Large-scale comparable corpora are available in many language pairs and are viewed as a source of valuable information for multilingual applications. Identifying translation correspondences in this type of corpora permits to construct bilingual lexicons for low-resourced languages, and to complement and reduce the sparseness of existing resources (Munteanu and Marcu, 2005; Snover et al., 2008). The main assumption behind translation extraction from comparable corpora is that a source word and its translation appear in similar contexts (Fung, 1998; Rapp, 1999). So, in order to identify a translation correspondence between the two languages, the contexts of the source word and the candidate translation have to be compared. For this comparison to take place, the same vector space has to be produced, which means that the vectors of the one language have to be translated The paper is organized as follows: In the next section, we present some related work on bilingual lexicon extraction from comparable corpora. Section 3 presents the data used in our experiments and Section 4 provides details on the approach and the experimental setup. In Section 5, we"
W13-2501,C04-1089,0,0.0319701,"sing translation clusters permits to expand feature translation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality o"
W13-2501,N09-2031,0,0.0211288,"s to expand feature translation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resour"
W13-2501,D08-1090,0,0.033079,"Missing"
W13-2501,steinberger-etal-2006-jrc,0,0.0167182,"able corpus from the health domain (Apidianaki et al., 2012). The results were encouraging, showing how translation clustering and vector disambiguation help to improve the quality of the translations extracted from the comparable corpus. We believe that the positive impact of this approach will be more significant on lexicon extraction from a general language comparable corpus, in which polysemy is more prominent. 3.2 Parallel corpus The parallel corpus used for clustering and word sense induction consists of the Slovene-English parts of Europarl (release v6) (Koehn, 2005) and of JRC-Acquis (Steinberger et al., 2006) and amounts to approximately 35M words per language. A number of pre-processing steps are applied to the corpus prior to sense induction, such 2 Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD. Slovene equivalents which have been validated by a lexicographer. Of 1,589 such synsets, 200 were randomly selected and used as a gold standard for automatic evaluation of the method proposed in this paper. as elimination of sentence pairs with a great difference in length, lemmatization and PoS tagging with the TreeTagger (for English) and ToTaLe (for Slovene)"
W16-3904,R11-1018,1,0.825495,"Missing"
W16-3904,ljubesic-etal-2014-tweetcat,1,0.682135,"Missing"
W16-3904,W13-2411,1,0.897699,"Missing"
W16-3904,C00-2137,0,0.0480892,"Missing"
W16-3904,D11-1120,0,\N,Missing
W17-1410,W13-2408,1,0.89407,"Missing"
W17-1410,W16-2606,0,0.030665,"Missing"
W17-1410,J92-4003,0,0.54702,"Missing"
W17-1410,P07-1033,0,0.262882,"Missing"
W17-1410,N13-1037,0,0.0793345,"cribes the tagging experiments we performed, Section 5 reports on the error analysis of the results and Section 6 gives some conclusions and directions for further research. 2 Related Work Early work on PoS tagging social media was, as usual, mostly focused on English (Gimpel et al., 2011; Owoputi et al., 2013). Recently there has been more work on other languages, primarily through the organization of shared tasks, such the EmpiriST on German (Beißwenger et al., 2016) and PoSTWITA on Italian.1 There are two main approaches to processing non-standard data: normalization and domain adaptation (Eisenstein, 2013). Most approaches nowadays follow the domain adaptation path al1 http://corpora.ficlit.unibo.it/ PoSTWITA/ Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 60–68, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics though the literature still lacks a detailed comparison of the two strategies on specific tasks. In domain adaptation there are, again, two main strategies (Horsmann and Zesch, 2015): adding more labeled data (Daum´e III, 2007; Hovy et al., 2015) and incorporating external knowledge (Owoputi et al., 2013). Horsmann and Ze"
W17-1410,P11-2008,0,0.414792,"Missing"
W17-1410,P07-2053,0,0.238028,"Missing"
W17-1410,N15-1135,0,0.0120933,"roaches to processing non-standard data: normalization and domain adaptation (Eisenstein, 2013). Most approaches nowadays follow the domain adaptation path al1 http://corpora.ficlit.unibo.it/ PoSTWITA/ Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 60–68, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics though the literature still lacks a detailed comparison of the two strategies on specific tasks. In domain adaptation there are, again, two main strategies (Horsmann and Zesch, 2015): adding more labeled data (Daum´e III, 2007; Hovy et al., 2015) and incorporating external knowledge (Owoputi et al., 2013). Horsmann and Zesch (2015) show that (1) adding manually annotated in-domain data is highly effective (but costly) and (2) adding out-of-domain training data or machine-tagged data is less effective than adding more external knowledge, especially word clustering information. The contribution of our paper is the following: First, we perform the first experiments in annotating Slavic non-standard texts with part-of-speech and morphosyntactic information, therefore dealing with several hundreds of tags. Next, we investigate the impact o"
W17-1410,R15-1049,1,0.891607,"Missing"
W17-1410,L16-1242,1,0.893411,"Missing"
W17-1410,L16-1676,1,0.915116,"Missing"
W17-1410,N13-1039,0,0.107593,"Missing"
W17-1410,C14-1168,0,0.045415,"Missing"
W17-1410,P10-1040,0,0.189388,"Missing"
W17-2901,W15-2913,0,0.0713611,"Missing"
W17-2901,D11-1120,0,0.101339,"arious languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances. 1 Introduction Gender prediction is a well-established task in author profiling, useful for a series of downstream analyses (Schler et al., 2006; Schwartz et al., 2013; Bamman et al., 2014) as well as predictive model improvements (Hovy, 2015). Most existing work on predicting gender focuses on exploiting the linguistic production of the users (Koppel et al., 2003; Schler et al., 2006; Kucukyilmaz et al., 2006; Burger et al., 2011; Miller et al., 2012; Rangel et al., 2016), just rarely using nonlinguistic information such as metadata (Plank 1 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 1–6, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 The Dataset working hours, posting during weekends, truncated tweets, favorited tweets, quotes, retweeted tweets). By following the three types, mean, med and var, we encode the following distributions in our feature space: retweet count, favorite count, posting hour, day of week the twe"
W17-2901,L16-1258,0,0.0841781,"average daily number of tweets, overall number of tweets, number of tweets the user has favorited, number of followers, number of friends, the ratio of follower to friend numbers, number of lists the user is on, whether the user has a background image defined, whether the user has the default profile image, whether the user has a profile description, whether the user has a location defined, and red, green and blue color component intensity (two-digit hexadecimal code from the RGB color definition) of the user’s text and background color. In our experiments we fully rely on the TwiSty corpus (Verhoeven et al., 2016) which was developed for research in author profiling. It contains personality (MBTI) and gender annotations for a total of 18,168 authors posting in German, Italian, Dutch, French, Portuguese or Spanish. The manual gender annotations in the TwiSty corpus are based on the user’s name, handle, description and profile picture and follow the performative view of gender, i.e., that gender is discriminated by performances that respond to societal norms or conventions (Larson, 2017). The corpus is distributed in the form of Twitter user IDs and specific tweet IDs of that user. In this work we use on"
W17-2901,P15-1073,0,0.0203148,"f-words model when training and testing on the same language, it regularly outperforms the bag-of-words model when applied to different languages, showing very stable results across various languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances. 1 Introduction Gender prediction is a well-established task in author profiling, useful for a series of downstream analyses (Schler et al., 2006; Schwartz et al., 2013; Bamman et al., 2014) as well as predictive model improvements (Hovy, 2015). Most existing work on predicting gender focuses on exploiting the linguistic production of the users (Koppel et al., 2003; Schler et al., 2006; Kucukyilmaz et al., 2006; Burger et al., 2011; Miller et al., 2012; Rangel et al., 2016), just rarely using nonlinguistic information such as metadata (Plank 1 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 1–6, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 The Dataset working hours, posting during weekends, truncated tweets, favorited tweets, quotes, r"
W17-2901,W17-1601,0,0.0138108,"definition) of the user’s text and background color. In our experiments we fully rely on the TwiSty corpus (Verhoeven et al., 2016) which was developed for research in author profiling. It contains personality (MBTI) and gender annotations for a total of 18,168 authors posting in German, Italian, Dutch, French, Portuguese or Spanish. The manual gender annotations in the TwiSty corpus are based on the user’s name, handle, description and profile picture and follow the performative view of gender, i.e., that gender is discriminated by performances that respond to societal norms or conventions (Larson, 2017). The corpus is distributed in the form of Twitter user IDs and specific tweet IDs of that user. In this work we use only the user IDs and their gender and language annotations to collect timelines of users through the Twitter API. For each user we collect up to 3,200 tweets (API restriction) and discard users with less than 100 tweets. By doing so we collected 45 million tweets for 16,156 users across the six languages. 3 4 The Features Experimental Setup In this section we outline the setup of our gender classification experiments, whose results we report in Section 5.1. We train models base"
W17-3007,N16-2013,0,0.0293422,"ew exceptions for Dutch (van Halteren and Oostdijk, 2013) and German (Ross et al., 2017). State-of-the-art approaches tackle this task through supervised machine learning (Sood et al., 2012; Dadvar et al., 2013). For this, of course, manually annotated datasets are needed. A major limitation of most existing work in this area is that it is based on an ad-hoc treatment of SUD classification in natural language processing and a lack of detailed guidelines that are necessary for reliable annotation (Ross et al., 2017). Annotated datasets have started to emerge only recently (Nobata et al., 2016; Waseem and Hovy, 2016), but nevertheless they lack precise documentation on data annotation and make use of only very basic In this paper we present the legal framework, dataset and annotation schema of socially unacceptable discourse practices on social networking platforms in Slovenia. On this basis we aim to train an automatic identification and classification system with which we wish contribute towards an improved methodology, understanding and treatment of such practices in the contemporary, increasingly multicultural information society. 1 Introduction In Slovenia, Socially Unacceptable Discourse (SUD) pract"
W18-3028,S15-2102,0,0.0458372,"Missing"
W18-3028,N16-1091,0,0.0838501,"Missing"
W18-3028,P16-2083,0,0.152164,"Missing"
W18-3028,C14-1018,0,0.0646429,"Missing"
W18-3028,P14-1024,0,0.13915,"are very important notions in psycholinguistic research, building on the theory of the double, verbal and non-verbal, modality of representation of concrete words in the mental lexicon, contrasted to single verbal representation of abstract words (Paivio, 1975, 2010). Although often correlated with concreteness, imageability is not a redundant property. While most abstract things are hard to visualize, some call up images, e.g., torture calls up an emotional and even visual image. There are concrete things that are hard to visualize too, for example, abbey is harder to visualize than banana (Tsvetkov et al., 2014). 217 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 217–222 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics ing campaign in which each word was labeled by 20 annotators on a 1–5 scale. For Croatian we use the MEGAHR database (MEGA onwards), consisting of 3,000 words, with concreteness and imageability ratings summarized through arithmetic mean and standard deviation. The ratings were collected in an annotation campaign among university students, with each word obtaining 30 annotations per variable on a 1–5 scale. For performing"
W18-3028,D11-1063,0,0.148297,"Missing"
W18-3028,D16-1057,0,0.0475519,"Missing"
W18-3028,E17-1072,0,0.0311061,"ned between languages with a linear transformation learned via SVD (Smith et al., 2017) on a bilingual dictionary of 500 out of the 1000 most frequent English words, obtained via the Google Translate API3 . We also experimented with another crosslingual embedding collection (Conneau et al., 2017), obtaining similar results and backing all our conclusions. This is in line with recent work on comparing cross-lingual embedding models which suggests that the actual choice of monolingual and bilingual signal is more important for the final model performance than the actual underlying architecture (Levy et al., 2017; Ruder et al., 2017). Given that one of our goals is to transfer concreteness and imageability annotations to as many languages as possible, using cross-lingual word embeddings based on Wikipedia dumps and dictionaries obtained through a translation API is the most plausible option. Contributions In this paper we perform a systematic investigation of transfer of two lexical notions, concreteness and imageability, (1) to the remainder of the lexicon not covered in an annotation campaign, and (2) to other languages. While there were already successful transfers within a language based on word e"
W18-5116,N16-2013,0,0.0486845,"r-generated content, there is increased pressure to manage inappropriate online content with (semi)automated methods. The research community is by now well aware of the multiple faces of inappropriateness in on-line communication, which preclude the use of simple vocabulary-based approaches, and are therefore turning to more robust machine learning methods (Pavlopoulos et al., 2017). These, however, require training data. Currently available datasets of inappropriate on-line communication are primarily datasets of English, such as a Twitter dataset annotated for racist and sexist hate speech (Waseem and Hovy, 2016)1 , the Wikimedia Toxicity Data Set (Wulczyn et al., 2017)2 , the Hate Speech Identifica3 https://data.world/crowdflower/ hate-speech-identification 4 https://github.com/sfu-discourse-lab/ SOCC 5 https://github.com/UCSM-DUE/IWG_ hatespeech_public 6 https://straintek.wediacloud.net/ static/gazzetta-comments-dataset/ gazzetta-comments-dataset.tar.gz 7 https://straintek.wediacloud.net/ static/gazzetta-comments-dataset/README. txt 1 https://github.com/ZeerakW/hatespeech https://figshare.com/projects/ Wikipedia_Talk/16731 2 124 Proceedings of the Second Workshop on Abusive Language Online (ALW2), p"
W18-5116,E17-2068,0,0.0753478,"Missing"
W18-5116,D17-1117,0,0.256497,"ja Fiˇser Faculty of Arts, University of Ljubljana Aˇskerˇceva cesta 2, 1000 Ljubljana, Slovenia darja.fiser@ff.uni-lj.si Abstract tion dataset containing tweets annotated as hate speech, offensive language, or neither (Davidson et al., 2017)3 , and the SFU Opinion and Comment Corpus consisting of online opinion articles and their comments annotated for toxicity4 . Datasets in other languages have recently also started to emerge, with a German Twitter dataset focused on the topic of refugees in Germany (Ross et al., 2017)5 and a Greek Sport News Comment dataset containing moderation metadata (Pavlopoulos et al., 2017)6 . In this paper we present two new and large datasets of news comments, one in Slovene, and one in Croatian. Apart from the texts, they also contain various metadata, the primary being whether the comment was removed by the site administrators. Given the sensitivity of the content, we publish the datasets in full-text form, but with user metadata semi-anonymised and the comment content encrypted via a simple character replacement method using a random, undisclosed bijective mapping, similar to the encryption method applied to the Gazzetta Greek Sport News Comments dataset7 introduced in Pavl"
