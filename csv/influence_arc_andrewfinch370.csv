2004.iwslt-evaluation.2,J90-2002,0,0.724612,"imal solution due to the enormous search space. However, SMT can sort translations in the order of their quality according to its statistical models. We show two different EBMT systems here, briefly explain each system, and then compare them. Finally, we ex1. Introduction There are two main strategies used in corpus-based translation: 1. Example-Based Machine Translation (EBMT) [1]: EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and then adjusts the examples to obtain the translation. 2. Statistical Machine Translation (SMT) [2]: SMT learns statistical models for translation from corpora and dictionaries and then searches for the best translation at run-time according to the statistical models for language and translation. By using the IWSLT04 task, this paper describes two endeavors that are independent at this moment: (a) a hybridization of EBMT and statistical models, and (b) a new approach for SMT, phrase-based HMM. (a) is used in the “unrestricted” Japanese-to-English track (Section 2), and (b) is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technolog"
2004.iwslt-evaluation.2,W01-1401,1,0.863919,"on is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven transDucer), which exploits DP-matching between word sequences. Let’s illustrate the process with a simple sample below. Suppose we are translating a Japanese sentence into English. The Japanese input sentence (1-j) is translated into the English sentence (1-e) by utilizing the English sentence (2-e), whose source sentence (2-j) is similar to (1-j). The common parts are unchanged, and the different portions, shown in bold face, are substituted by consulting a bilingual dictionary. ;;; A Japanese input (1-j) iro/ga/ki/ni/iri/masen ;;; the most similar example in co"
2004.iwslt-evaluation.2,P91-1024,1,0.754066,"source sentence of examples from a bilingual corpus. For this, we use DP-matching, which tells us the edit distance between word sequences while giving us the matched portions between the input and the example. The edit distance is calculated as follows. The count of the inserted words, the count of the deleted words, and the semantic distance of the substituted words are summed. Then, this total is normalized by the sum of the lengths of the input and the source part of translation example. The semantic distance between two substituted words is calculated by using the hierachy of a thesaurus[4]. Our language resources in addition to a bilingual corpus are a bilingual dictionary, which is used for generating target sentences, and thesauri of both languages, which are used for incorporating the semantic distance between words into the distance between word sequences. Furthermore, lexical resources are also used for word alignment. Table 1: Resources used for two EBMTs in IWSLT04 unresticted Japanese-to-English track. bilingual corpus bilingual dictionary thesaurus grammar D3 travel domain (20K) in-house in-house N.A. HPAT travel domain (20K) in-house in-house in-house D3 achieves a go"
2004.iwslt-evaluation.2,C94-1015,0,0.0154509,"is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technologies, which are not used in the IWSLT04 task but boost translation performance, are also introduced in Section 4. 13 plain the selector used to determine the best from multiple translations based on SMT models. of the same syntactic category. Imamura [6] subsequently proposed HPA-based translation (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial"
2004.iwslt-evaluation.2,P03-1057,1,0.787621,"s. of the same syntactic category. Imamura [6] subsequently proposed HPA-based translation (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven tran"
2004.iwslt-evaluation.2,C02-1076,1,0.866736,"7.00 70.00 77.60 83.40 16.60 HPAT 38.60 59.80 77.40 83.40 16.60 SELECT 59.80 73.00 82.40 87.80 12.20 DIFF. +2.80 +3.00 +4.80 +4.40 -4.40 Next, the relationship between translation quality of element systems and gain by the selector was analyzed. Table 5 shows that the proposed selector reduces the number of low-quality translations (ranked “D”) while it increases the number of high-quality translations (ranked “S” to “B”). 2.3. SMT-based Selector We proposed an SMT-based method of automatically selecting the best translation among outputs generated by multiple machine translation (MT) systems [9]. Conventional approaches to the selection problem include a method that automatically selects the output to which the highest probability is assigned according to a language model (LM). [10] These existing methods have two problems. First, they do not check whether information on source sentences is adequately translated into MT outputs, although they do check the fluency of MT outputs. Second, they do not take the statistical behavior of assigned scores into consideration. The proposed approach scores MT outputs by using not only the language but also a translation model (TM). To conduct a s"
2004.iwslt-evaluation.2,hogan-frederking-1998-evaluation,0,0.0713962,"e drastic reduction in mWER has been demonstrated (Table 6). However, the quality with the small corpus is not so bad in the subjective evaluation shown in Table 7. We conjecture that adequacy is not low even with the supplied corpus, and the translation become similar to native English, that is, its fluency improves as the size of corpus increases. 2.4. Results 2.4.1. Selecting Effect 2.5. Discussion As shown in Table 4, all of the metrics taken together show that the proposed selector outperforms both element transRelated works have proposed ways to merge MT outputs from multiple MT systems [12] in order to output better translations. When the source language and the target language have similar sentence structures, this merging apGood: easy to understand, with either some unimportant information missing or flawed grammar; (C) Fair: broken, but understandable with effort; (D) Unacceptable: important information has been translated incorrectly. 15 translations with additional constraints [17, 18, 19]:  P (¯fi |¯ ea i ) P (f |e) ≈ Table 7: ATR’s Overall Subjective Evaluation - IWSLT supplied corpus. S S,A S,A,B S,A,B,C D D3 34.80 47.40 62.60 73.40 26.60 HPAT 25.20 44.20 70.40 80.40 19"
2004.iwslt-evaluation.2,P01-1050,0,0.0620593,"age and the target language have different sentence structures, such as English and Japanese, we often have translations whose structures are different from each other for a single input sentences. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sent"
2004.iwslt-evaluation.2,P01-1030,0,0.0810071,"nces. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the ¯ and translation probability of the two phrase sequences of e ¯f . The last term is the likelihoo"
2004.iwslt-evaluation.2,2003.mtsummit-papers.54,1,0.856638,"s regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the ¯ and translation probability of the two phrase sequences of e ¯f . The last term is the likelihood of the phrase-segmen"
2004.iwslt-evaluation.2,N03-1017,0,0.00448749,"ndling long Japanese input. The latter was attributed to the fact that we tuned our parameter to mWER and we exploited phrase models as well. Table 8: Evaluation - IWSLT Chinese-to-English supplied task. System Top Our Bottom (15) where count(¯ e, f¯) is the cooccurrence frequency of the two phrases e¯ and f¯. The basic idea of Equation 15 is to capture the bilingual correspondence while considering two directions. Additional phrases were exhaustively induced based on the intersection/union of the viterbi word alignments of the two directional models, P (e|f ) and P (f |e), computed by GIZA++ [17]. After the extraction of phrase translation pairs, their monolingual phrase lexicons were extracted and used as the possible segmentation for the source and target sentences. mWER 45.59 46.99 61.69 Fluency 38.20 38.20 25.04 Adequacy 33.38 29.50 29.06 4. Other Features of C3 This section introduces another feature of C3: paraphrasing and filtering corpora, which are not used in the IWSLT04 task but are useful for boosting MT performance. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. Specifically, theis variety makes it m"
2004.iwslt-evaluation.2,2003.mtsummit-papers.53,0,0.0727491,"Missing"
2004.iwslt-evaluation.2,W03-1001,0,0.0212046,"Missing"
2004.iwslt-evaluation.2,W04-3216,0,0.0140311,"regarded as the distortion tion 5, the term P (f |¯f , e probability of how a phrase segmented sentence ¯f will be reordered to form the source sentence f . Instead, we model this as the likelihood of a particular phrase segment ¯fj observed in f : ¯, e) ∝ P (f |¯f , e P (¯f |f )  P (¯fj |f ) ≈ Equation 12 can be regarded as a Hidden Markov Model in ¯ j in the lattice F ¯ is treated as an which each source phrase F ¯ observation emitted from a state Ei , a target phrase, in the ¯ as shown in Figure 2. lattice E, (8) (9) j The use of the phrase-based HMM structure has already been proposed in [20] in the context of aligning documents and abstracts. In their approach, jump probabilities were explicitly encoded as the state transitions that roughly corresponded to the alignment probabilities in the context of the word-based statistical translation model. The use of the explicit jump or alignment probabilities served for the completeness of the translation modeling at the cost of the enormous search space needed to train the phrase-based HMM structure. The segmentation model is realized as the unigram posterior probability of the phrase ngram model presented in Section 3.1. To briefly sum"
2004.iwslt-evaluation.2,J03-1002,0,0.00549884,"rocedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the wordbased trigram language model and the class 5-gram model. The second pass uses A* strategy to search for the best path of translation on the generated word-graph. j fj2 ∩fj 2 =∅ 1  1 2 j  j ×P (eii2 +1 |eii1 )P (fj 2 |eii2 +1 )P (fj 2 |f ) 1 1 (14) To overcome the problem of local convergence often observed in the EM algorithm [21], we use the lexicon model from the GIZA++ [22] training as the initial parameters for the phrase translation model. In addition, the phrase ngram model and the phrase segmentation models are individually trained over the monolingual corpus and remained fixed during the HMM iterations. 3.8. Results The results appear strange in two points: (1) Our proposal didn’t work well for the Japanese-to-English track but did work well for the Chinese-to-English track; (2) Our proposal achieved high fluency but marked low adequacy. 3.6. Phrase Segment Induction Equations 13 and 14 involve summation over all possible contexts, either in its left-hand-s"
2004.iwslt-evaluation.2,P02-1038,0,0.0308911,"e segmentation model, and the phrase translation model – Equation 4 can be rewritten as  P (¯fj |f )P (¯fj |¯ ei )P (¯ ei |¯ ei ) (12) P (f |e) ≈ ¯,¯ e f j,i ¯ and ¯f are expanded If the phrase segmented sentences e ¯ and F, ¯ then into the corresponding lattice structures of E Therefore, the Forward-Backward algorithm can be for17 where P rj (e, f ) are the subcomponents of translation models, such as the phrase ngram model or the language model, and λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure ge"
2004.iwslt-evaluation.2,P03-1021,0,0.00794031,"an be rewritten as  P (¯fj |f )P (¯fj |¯ ei )P (¯ ei |¯ ei ) (12) P (f |e) ≈ ¯,¯ e f j,i ¯ and ¯f are expanded If the phrase segmented sentences e ¯ and F, ¯ then into the corresponding lattice structures of E Therefore, the Forward-Backward algorithm can be for17 where P rj (e, f ) are the subcomponents of translation models, such as the phrase ngram model or the language model, and λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input"
2004.iwslt-evaluation.2,W02-1021,0,0.0253988,"λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the wordbased trigram language model and the class 5-gram model. The second pass uses A* strategy to search for the best path of translation on the generated word-graph. j fj2 ∩fj 2 =∅ 1  1 2 j  j ×P (eii2 +1 |eii1 )P (fj 2 |eii2 +1 )P (fj 2 |f ) 1 1 (1"
2004.iwslt-evaluation.2,shimohata-sumita-2002-automatic,1,0.850547,"xtract good transfer patterns for HPAT, and to estimate the parameters for SMT. 3.7. Decoder The decision rule to compute the best translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23]. 1  ˆ = argmax λj log P rj (e, f ) (16) e Z(f ) j e We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. 18 4.1. Paraphrasing for providing the Ruigo-Shin-Jiten. Three methods have been investigated for automatic paraphrasing. (1) Shimohata et al. [27] grouped sentences by the equivalence of the translation and extract rules of paraphrasing by DP-matching. (2) Finch et al. [28] clustered sentences in a paraphrase corpus to obtain pairs that are similar to each other for training SMT models. Then by using the models, the decoder generates a paraphrase. (3) Finch et al. [29] developed a paraphraser based on data-oriented parsing, which utilizes synatactic information within an examplebased framework. The experimental results indicate that the EBMT based on normalization of the source side had increased coverage [30] and that the SMT created o"
2004.iwslt-evaluation.2,2002.tmi-tutorials.2,0,0.02481,"st translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23]. 1  ˆ = argmax λj log P rj (e, f ) (16) e Z(f ) j e We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. 18 4.1. Paraphrasing for providing the Ruigo-Shin-Jiten. Three methods have been investigated for automatic paraphrasing. (1) Shimohata et al. [27] grouped sentences by the equivalence of the translation and extract rules of paraphrasing by DP-matching. (2) Finch et al. [28] clustered sentences in a paraphrase corpus to obtain pairs that are similar to each other for training SMT models. Then by using the models, the decoder generates a paraphrase. (3) Finch et al. [29] developed a paraphraser based on data-oriented parsing, which utilizes synatactic information within an examplebased framework. The experimental results indicate that the EBMT based on normalization of the source side had increased coverage [30] and that the SMT created on the normalized target sentences had a reduced word-error rate [31]. Finch et al. [32] demonstrated that the expansion of refer"
2004.iwslt-evaluation.2,C04-1017,1,0.875016,"Missing"
2004.iwslt-evaluation.2,E03-1029,1,0.895696,"Missing"
2004.iwslt-evaluation.2,W02-1611,1,\N,Missing
2004.iwslt-evaluation.2,watanabe-etal-2002-statistical,1,\N,Missing
2006.iwslt-evaluation.12,C02-1076,1,0.827325,"ntactic transfer; and EM by exact match. For the OPEN track, only TATR was used; for the CSTAR track, a hybrid system using three engines and Selector was used. See Figure 1. 5.4. Selector 5.1. TATR In order to select the best translation among outputs generated by multiple MT systems, we employ an SMT-based method that scores MT outputs by using multiple language (LM) and translation model (TM) pairs trained on different subsets of the training data. It uses a statistical test to check whether the obtained TM·LM scores of one MT output are significantly higher than those of another MT output [9]. Given an input sentence, m translation hypotheses are produced by the component MT engines (m = 1 for this evaluation), whereby n different TM·LM scores are assigned to each hypothesis. In order to check whether the highest scoring hypothesis is significantly better then the other MT outputs, a multiple comparison test based on the Kruskal-Wallis test is used [10]. If one of the MT outputs is significantly better, this output is selected. Otherwise, the output of the MT engine that performs best on a development set is selected. TATR is a phrase-based SMT system built within the framework of"
2006.iwslt-evaluation.12,J03-1002,0,0.0511227,"Missing"
2006.iwslt-evaluation.12,2006.iwslt-evaluation.15,0,0.0505268,"Missing"
2006.iwslt-evaluation.12,2005.mtsummit-papers.35,1,0.807212,"Missing"
2006.iwslt-evaluation.12,2005.iwslt-1.5,1,0.8833,"Missing"
2006.iwslt-evaluation.12,N06-2049,1,0.870022,"ation. Our main translation engine for this year’s IWSLT evaluation, TATR, is also a phrase-based SMT. The hybrid multiple engine approach, that was used last year [1], was used again this year. But we replaced the 2005 SMTs (PBHMTM and SAT) with TATR, partly for simplification reasons. In addition to TATR, two other engines are included in this year’s hybrid system: HPATR3, a SMT based on syntactic transfer; and EM, the translation memory based on exact match. We employed new approaches for pre-processing, postprocessing, and language modeling. We used subword-based Chinese word segmentation [2]. This word segmentation achieved the highest F-score rate for the second Sighan test data, and can recognize numerical expressions and foreign names. We built a conversion model to implement capitalization and punctuation by using the maximum entropy principle and the conditional random field (CRF) approach, which can integrate long-range features to enhance performance. We applied sentence-splitting techniques to all languages. This approach significantly improved CE and JE translation. 2. Preprocessing 2.1. Arabic segmentation Of the released data, we threw away all end-of-utterance markers"
2006.iwslt-evaluation.12,N06-2013,0,\N,Missing
2007.iwslt-1.15,N06-2049,1,0.896044,"Missing"
2007.iwslt-1.15,W06-1626,0,0.0485171,"/MX is/AL CEO/AU of/AL a/AL British/IU company/AL. The CRF tagging model is expressed by the following equation: 1 http://www.speech.sri.com/projects/srilm 6. Hit-rate-based Skip n-gram Rescoring This section describes the re-scoring of the statistical MT decoder hypotheses based on skip n-gram counts extracted from a large-scale corpus consisting of collections of webpages. In order to handle very large amounts of training data to build language models, recent research focuses on distributed language modeling that use a two-pass approach to store corpora in suffix arrays and serve raw counts [6, 7] or a single-pass approach that provides smoothed probabilities 2 http://www.chasen.org/˜taku/software/CRF++ using simple smoothing techniques [8]. Although such approaches are to be preferred when available, the computational and hardware requirements are still immense and not always practicable. In order to make use of very large training corpora with fewer resources, we use a method based on n-gram occurrence counts . The hit-rate of a word sequence is defined to be: X HitRate(w1L ) = δ(wij ) (2) i,j;i<j δ(wij ) =  1 0 : f (wij ) &gt; 0 : f (wij ) = 0 The hit-rate counts can be easily calcula"
2007.iwslt-1.15,D07-1054,1,0.871459,"e in the additional corpus (Tanaka corpus, Yomiuri News corpus, SLDB corpus, and Beijing Olympic corpus included in ChineseLDC) was calculated 3. Only those sentences for which the perplexity was lower than 100 were used as training sentences. After the selection process we were left with 40K sentences from the supplied corpus and 117K additional sentences from the external corpora, giving us a total of 157K sentences for training. 2.4. Modeling Issues Word-trigram language models were used, these were smoothed using Knesser-Ney discounting. In addition, topicdependent models were constructed [1]. We built bilingual cluster-based models from 157K bilingual training sentence pairs. The sentence pairs were clustered into 10 sub-corpora. These sub-corpora intutitively represent sub-domains of the main corpus. The motivation behind this strategy was to build models specific to these sub-domains and then predict the sub-domain of the text to be translated, and use the appropriate model for the translation process. A strong improvement was demonstrated using this technique for all language pairs in the IWSLT06 evaluation campaign. In this year’s campaign we only apply this technique to the"
2007.iwslt-1.15,P07-2046,1,0.884488,"a method for re-segmenting the tokens in the confusion network. 3. Chinese-English 3.1. Corpora We used the supplied corpus in combination with the Beijing Olympic Corpus, and other corpora provided by the LDC. These corpora and their respective sizes are shown in Table 1. 3.2. Lemmatization Data sparseness is one of the key factors that degrade statistical machine translation (SMT). Especially for a translation task like IWSLT, where collecting a large amount of indomain data is very expensive. One method to reduce the translation degradation caused by this approach is by using lemmatization [2]. Lemmatization is shallow morphological analysis, which uses single a lexical entry to replace a whole range of derived inflected words. For example, the three words: “doing”, “did” and “done”, can be replaced by one word: “do”. In fact, they should all be mapped to the same Chinese target word during alignment. It is easy to see that as a result, the process reduces the number of types observed in the data, thereby easing the problems associate with sparse data, and in Chinese at least we expect the process to preserve as much of the semantic information as possible. We used Moses to impleme"
2007.iwslt-1.15,W07-0717,0,0.0516508,"he only difference in training with lemmatization from that without is the alignment factor. The former uses Chinese surface words and English lemmas as the alignment factor, but the latter uses Chinese surface words and English surface words. Therefore, the lemmatized English is only used in the word alignment stage of the training. All the other aspects of the training process are the same for both the lemmatized translation training and nonlemmatized training. 3.3. Translation model combination Linear interpolation of translation models has been shown to be effective in machine translation [2, 3]. In this campaign we apply this approach as the main means of integrating models built from the external resources with the primary models built from the supplied corpus. More formally, we use the following equation for model combination: p(e|f ) = α1 p1 (e|f ) + α2 p2 (e|f ) where p1 and p2 are two models to be integrated, and the weight α1 and α2 must sum to unity. We did not use automatic optimization methods to select the α1 and α2 . Instead, we hand-selected the values by evaluating the performance of multiple runs on the development data. We consider this approach reasonable since the s"
2007.iwslt-1.15,D07-1090,0,0.0387915,"/srilm 6. Hit-rate-based Skip n-gram Rescoring This section describes the re-scoring of the statistical MT decoder hypotheses based on skip n-gram counts extracted from a large-scale corpus consisting of collections of webpages. In order to handle very large amounts of training data to build language models, recent research focuses on distributed language modeling that use a two-pass approach to store corpora in suffix arrays and serve raw counts [6, 7] or a single-pass approach that provides smoothed probabilities 2 http://www.chasen.org/˜taku/software/CRF++ using simple smoothing techniques [8]. Although such approaches are to be preferred when available, the computational and hardware requirements are still immense and not always practicable. In order to make use of very large training corpora with fewer resources, we use a method based on n-gram occurrence counts . The hit-rate of a word sequence is defined to be: X HitRate(w1L ) = δ(wij ) (2) i,j;i<j δ(wij ) =  1 0 : f (wij ) &gt; 0 : f (wij ) = 0 The hit-rate counts can be easily calculated, even for very large training corpora like the Web-Corpus introduced in Section 6.1. For the IWSLT experiments, we calculated the hitrate feat"
2007.iwslt-1.15,W08-0334,1,\N,Missing
2007.iwslt-1.15,P02-1040,0,\N,Missing
2007.iwslt-1.15,N07-1061,0,\N,Missing
2007.iwslt-1.15,W05-0909,0,\N,Missing
2007.iwslt-1.15,P07-2045,0,\N,Missing
2007.iwslt-1.15,J03-1002,0,\N,Missing
2007.iwslt-1.15,W08-0335,1,\N,Missing
2007.iwslt-1.15,2006.iwslt-evaluation.12,1,\N,Missing
2007.iwslt-1.15,I05-3017,0,\N,Missing
2007.iwslt-1.15,P03-1021,0,\N,Missing
2007.tmi-papers.19,2001.mtsummit-papers.3,1,0.893267,"ers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various learning approaches for the multiclass classification problem for a very small data set in the domain of technical documentation. (Akiba et al., 2001) utilized DT classifiers trained on multiple edit-distance features where combinations of lexical (stem, word, part-of-speech) and semantic (thesaususbased semantic class) matches were used to compare MT system outputs with reference translations and to approximate human scores of acceptability directly. (Kulesza and Shieber, 2004) trained a binary SVM classifier based on automatic scoring features in order to distinguish between “human-produced” and “machine-generated” translations of newswire data instead of predicting human judgments directly. The approach proposed in this paper also utiliz"
2007.tmi-papers.19,W05-0909,0,0.0288594,"cceptable Translation Nonsense Table 2: Automatic Evaluation Metrics BLEU: NIST: METEOR: GTM: WER: PER: TER: the geometric mean of n-gram precision of the system output with respect to reference translations. Scores range between 0 (worst) and 1 (best) (Papineni et al., 2002) a variant of BLEU using the arithmetic mean of weighted n-gram precision values. Scores are positive with 0 being the worst possible (Doddington, 2002) calculates unigram overlaps between a translation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discrimina"
2007.tmi-papers.19,2004.tmi-1.8,0,0.153171,"automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various learning approaches for the multiclass classification problem for a very small data set in the domain of technical documentation. (Akiba et al., 2001) utilized DT classifiers trained on multiple edit-distance features where combinations of lexical (stem, word, part-of-speech) and semantic (thesaususbased semantic class) matches were used to compare MT system outputs with reference translations and to approximate human scores of acceptability directly. (Kulesza and Shieber, 2004) trained a binary SVM classifier based on automatic scoring features in order to distinguish between “human-produced” and “machine-generated” translations of newswire data instead of predicting human judgments directly. The approach proposed in this paper also utilizes a supervised learning method to predict human assessments of translation quality, but differs in the following two aspects: (1) Reduction of Classification Perplexity: The decomposition of a multiclass classification task into a set of binary classification problems reduces the complexity of the learning task resulting in higher"
2007.tmi-papers.19,N07-1006,0,0.115946,"Missing"
2007.tmi-papers.19,niessen-etal-2000-evaluation,0,0.0624038,"tive with 0 being the worst possible (Doddington, 2002) calculates unigram overlaps between a translation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binar"
2007.tmi-papers.19,2001.mtsummit-papers.46,0,0.0107964,"anslation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguis"
2007.tmi-papers.19,P02-1040,0,0.0751806,"Missing"
2007.tmi-papers.19,quirk-2004-training,0,0.0918346,"system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various learning approaches for the multiclass classification problem for a very small data set in the domain of technical documentation. (Akiba et al., 2001) utilized DT classifiers trained on multiple edit-distance features where combinations of lexical (stem, w"
2007.tmi-papers.19,2006.amta-papers.25,0,0.0596957,"cores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various"
2007.tmi-papers.19,1999.mtsummit-1.34,1,0.721734,"uality of a translation have been proposed. In this paper, human assessments of translation quality with respect to the fluency, the adequacy and the acceptability of the translation are investigated. Fluency indicates how natural the evaluation segment sounds to a native speaker of English. For adequacy, the evaluator was presented with the source language input as well as a “gold standard” translation and has to judge how much of the information from the original translation is expressed in the translation (White et al., 1994). Acceptability judges how easy-to-understand the translation is (Sumita et al., 1999). The fluency, adequacy and acceptability judgments consist of one of the grades listed in Table 1. The high cost of such human evaluation metrics has triggered a huge interest in the development of automatic evaluation metrics for machine translation. Table 2 introduces some metrics that are widely used in the MT research community. 3 Prediction of Human Assessments Most of the previously proposed approaches to predict human assessments of translation quality utilize supervised learning methods like decision trees (DT), support vector ma5 4 3 2 1 acceptability Perfect Translation Good Transla"
2007.tmi-papers.19,2003.mtsummit-papers.51,0,0.0312584,"ion of the system output with respect to reference translations. Scores range between 0 (worst) and 1 (best) (Papineni et al., 2002) a variant of BLEU using the arithmetic mean of weighted n-gram precision values. Scores are positive with 0 being the worst possible (Doddington, 2002) calculates unigram overlaps between a translation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from hu"
2007.tmi-papers.19,1994.amta-1.25,0,0.116906,"Missing"
2008.iwslt-evaluation.11,W08-0334,1,0.872755,"ation task, we integrated two strategies for pivot translation by linear interpolation. 1. Introduction This paper describes the NICT/ATR SMT system used in the International Workshop on Spoken Language Translation (IWSLT) 2008 evaluation campaign. We participated in the following translation tasks: Chinese–English (Challenge Task), English–Chinese (Challenge Task), Chinese–English (BTEC Task), Chinese–Spanish (BTEC Task), and Chinese– English–Spanish (PIVOT Task). Although our theme for each task was different, our systems were based on a fairly common phrase-based machine translation system [1], which was built within the framework of a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapte"
2008.iwslt-evaluation.11,P07-2045,0,0.0149744,"e based on a fairly common phrase-based machine translation system [1], which was built within the framework of a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese tran"
2008.iwslt-evaluation.11,J03-1002,0,0.00224261,"a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various fact"
2008.iwslt-evaluation.11,P03-1021,0,0.0223499,"r can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various factors affecting English–Chinese translation. Table 1 summarizes the BLEU scores for correct recognition results (CRR). The BLEU scores [6] for “devset” are obtained with the small Challenge Task devset corpus (comprising 251 sentences). The devset corpus was also used for MERT.1 Thus, the results in Table 1 for devset were obtained from closed experiments. The results for “devset3” (506 sentences) were obtained by using the param"
2008.iwslt-evaluation.11,P02-1040,0,0.0774265,"The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various factors affecting English–Chinese translation. Table 1 summarizes the BLEU scores for correct recognition results (CRR). The BLEU scores [6] for “devset” are obtained with the small Challenge Task devset corpus (comprising 251 sentences). The devset corpus was also used for MERT.1 Thus, the results in Table 1 for devset were obtained from closed experiments. The results for “devset3” (506 sentences) were obtained by using the parameters tuned on devset (open experiments). The BLEU scores were calculated based on Chinese character n-grams. When calculating BLEU scores, we removed out-of-vocabulary (OOV) words from the machine translated text and ignored punctuation. 1 We used 3-gram language models for performing MERT and used 5gra"
2008.iwslt-evaluation.11,W08-0335,1,0.86956,"Missing"
2008.iwslt-evaluation.11,N06-2049,1,0.880518,"Missing"
2008.iwslt-evaluation.11,I05-3017,0,0.087631,"Missing"
2008.iwslt-evaluation.11,D07-1054,1,0.880592,"Missing"
2008.iwslt-evaluation.11,N07-1061,1,0.865684,"of the language X to language Y SMT system by using corpus X of the “X-E corpus” and the newly created corpus Y’. After developing the models, as described above, we remove all the phrase table entries that have OOV words on the target side of the phrase table. We will call the system developed above the X2Y’ system and the strategy PseudoCorpusY. In this system, the target side of the phrase table is not completely reliable. For training these systems, we develop and use a language model using corpus Y of the “Y-E corpus.” 4.4. Phrase Table Composition This strategy was introduced by Utiyama [12]. In order to implement this strategy, we first develop the X2E system using the “X-E corpus” and the E2Y system using the “Y-E corpus.” Then, we compose a new phrase table from the phrase tables of the X2E and E2Y systems. For the purpose of integrating two models, we extend this strategy to include the lexicalized reordering model. 4.5. Linear Interpolation This strategy is used to develop new models from those described above, by linear interpolation: the phrase translation model and the lexicalized reordering model. First, we interpolate two PseudoCorpus models. These models are de- 82 - n"
2008.iwslt-evaluation.11,W05-0909,0,0.115895,"Missing"
2010.iwslt-evaluation.18,N03-1017,0,0.00288212,"words by replacing them with known words that have the same lemmas but different inflections. The structure of the remainder of the paper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous cont"
2010.iwslt-evaluation.18,P05-1033,0,0.0604414,"that have the same lemmas but different inflections. The structure of the remainder of the paper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY alg"
2010.iwslt-evaluation.18,koen-2004-pharaoh,0,0.0968916,"aper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language m"
2010.iwslt-evaluation.18,P03-1021,0,0.0147245,"translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language model, a hierarchical phrase translation model, and phrase penalty. The feature weights are also tuned using MERT [4]. 2.2. Integration of Multiple Segmentation Schemes The task of word segmentation, i.e., i"
2010.iwslt-evaluation.18,J07-2003,0,0.0812834,"en in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language model, a hierarchical phrase translation model, and phrase penalty. The feature weights are also tuned using MERT [4]. 2.2. Integration of Multiple Segmentation Schemes The task of word segmentation, i.e., identifying word boundaries in continuous text, is one of the fundamental preprocessing steps of data-"
2010.iwslt-evaluation.18,W10-1760,1,0.791009,"on of Multiple Segmentation Schemes The task of word segmentation, i.e., identifying word boundaries in continuous text, is one of the fundamental preprocessing steps of data-driven NLP applications like Machine Translation (MT). In contrast to Indo-European languages like English, many Asian languages like Chinese do not use a whitespace character to separate meaningful word units. We use an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches [6]. Word segmentations that are consistent with the phrasal segmentations of SMT translation models are learned from the SMT training corpus by aligning character-wise source language sentences to word units separated by a whitespace in the target language. Successive characters aligned to the same target words are merged into a larger source language unit. Therefore, the granularity of the translation unit is defined in the given bitext context. In order to minimize the side effects of alignment errors and to achieve segmentation consistency, a Maximum-Entropy (ME) algorithm is applied to learn"
2010.iwslt-evaluation.18,P07-1040,0,0.0206781,"er the same surface string but differ only in the segmentation of the source language phrase. Therefore, the more often such a translation pair is learned by different iterative models, the more often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorith"
2010.iwslt-evaluation.18,N04-1022,0,0.0213991,"language phrase. Therefore, the more often such a translation pair is learned by different iterative models, the more often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm"
2010.iwslt-evaluation.18,2006.amta-papers.25,0,0.017203,"often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argm"
2010.iwslt-evaluation.18,P02-1034,0,0.0171555,"output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, w"
2010.iwslt-evaluation.18,P05-1012,0,0.0195886,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,D07-1080,1,0.900751,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,D08-1024,0,0.0536492,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,2008.iwslt-evaluation.13,1,0.855795,"n using the loss biased maximization in Equation 2 largely inspired by [14]. For the loss function lij and the underlying BLEU score b(·), we applied document scaled BLEU which computes BLEU by replacing one translation ei1 with another eij in a set of 1-best translations {ei1 }i=1...m [13]. Oracle translations are selected with respect to b(·). When multiple oracle translations are found, we select the one which maximizes ∆h(eij ) · w [14]. 2.4.2. Feature Functions for Re-ranking We used a large number of sparse binary features together with real valued features from decoders as described in [17]. Word pair features We used all possible pairs of source word and target word as our primary features. POS pairs were also extracted by replacing source words and target words with their corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignme"
2010.iwslt-evaluation.18,N03-1033,0,0.0440416,"t of 1-best translations {ei1 }i=1...m [13]. Oracle translations are selected with respect to b(·). When multiple oracle translations are found, we select the one which maximizes ∆h(eij ) · w [14]. 2.4.2. Feature Functions for Re-ranking We used a large number of sparse binary features together with real valued features from decoders as described in [17]. Word pair features We used all possible pairs of source word and target word as our primary features. POS pairs were also extracted by replacing source words and target words with their corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-c"
2010.iwslt-evaluation.18,N06-1014,0,0.0794113,"ir corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the"
2010.iwslt-evaluation.18,P08-1012,0,0.0154613,". As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from"
2010.iwslt-evaluation.18,P03-1054,0,0.00620711,"our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from the current translated utterance and bags of words (BOW) from the previously “translated” la"
2010.iwslt-evaluation.18,P08-1067,0,0.0126776,"a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from the current translated utterance and bags of words (BOW) from the previously “translated” last utterance from both speakers. The BOWs were collected from the n-best list of the tra"
2010.iwslt-evaluation.18,2009.iwslt-evaluation.12,1,0.830023,"DIALOG corpus, the BTEC corpus and the DEVSET corpus. All the data in the DEVSET for the BTEC task, using on single reference, was included for training. Only the devset for DIALOG was reserved for development testing. All of our experiment results presented in this paper are based on this testset. In total, we had around 35K sentence pairs for training. The devset used for MERT is sampled from all of the DEVSET for BTEC. In the last year’s IWSLT campaign, we introduced a devset sampling technique in which the development data were sampled from training data that are similar to the input text [24]. The similarity is measured by the BLEU using the input sentences as references. This year, we sampled from bilingual data with multiple reference translations, rather than from large amounts of DIALOG data with single reference translations, in order to avoid overfitting. We extracted 500 sentences for each translation direction. During MERT, only the training corpus for DIALOG and BTEC were used to train the translation model, but all of the data was used to build final translation model. Some pre-processing was also carried out on the corpus before training. First, in order to avoid ambigu"
2010.iwslt-evaluation.18,I08-4033,1,0.828764,"ces are split if multiple sentences are found in one line. At the end of translation, these multiple sentences are concatenated into a single line. We also did some normalization to the text. For English text, all the words were lowercased, any hyphens or commas were removed from between numeral words and tokenized using the standard tools provided by the Moses toolkit1 . The Chinese word segmentation originally provided contained inconsistencies and was not usable to build the translation model. The Chinese word segmentation was therefore redone using three methods: character-based, Achilles [25] and ICTCLAS2 . We will explain the usage of different segmentation standards in the next section. Basically, the numeral words in Chinese can be written either using Chinese characters or Arabic numbers. We converted all of the Arabic numbers to Chinese characters using a simple set of heuristics. Our translation model was built from data containing the punctuation for both source and target languages. In the official testing, the test data is provided without punctuation to remain consistent with the format of ASR output. So, before sending the test data for translation, we restored the punc"
2011.iwslt-evaluation.22,W08-0510,0,0.0280144,"M training text consisted of 85k sentences and had a vocabulary of 59k words. We built a modified KneserNey smoothed tri-gram using the MITLM toolkit[14]. Prior to training the LM the Japanese text was segmented using the same propriety tool as used to segment the IWSLT SMT training text. This was done to make coupling easier and remove the need for a possible re-segmentation after recognition. Our in-house decoder SprinTra is a general one-pass Viterbi decoder. To parallel decode the gender dependent 3.2. SMT Training The SMT system was based around the Moses training and decoding components [16]. The SMT systems were all trained on the standard IWSLT 2007 training data which consisted of 20k training sentence pairs. The phrase tables were all built using the standard Moses training scripts. In all cases the English text was lower-cased and tokenized using the standard Moses scripts prior to training. After SMT decoding the casing was recovered prior to evaluation. To build the target LM we used the mono-lingual component of the IWSLT training text. Again a modified KneserNey trigram was built using the MITLM toolkit. We observed no improvement in translation performance when using hi"
2011.iwslt-evaluation.22,P11-2001,0,0.0408993,"in ASR performance with or without the tuple form of the vocabulary entries. Although, for a large vocabulary Japanese spontaneous speech task we have observed significantly larger search networks and slower decoding speeds by removing the POS tags. A problem with stripping POS tags from the lattice and applying optimizations is it may increase the chances that the lattice cannot be determinized. For future work in this area we plan to investigate the use of POS tags and pronunciation data in a factored SMT based system, or to extend the Lexicographic semiring idea as proposed by Roark et al [17]. 4.1.3. Punctuation Recovery In the first experiments we looked a several simple heuristics to add punctuation. The silences in the recognition output are either removed or mapped to periods or commas. We found that removing all silences and simply adding a final period to the recognition output performed best. On inspection of the Japanese test-set we found that the only types of punctuation present were final periods. 4.2. Decoder Tuning and Translation Performance In this section we considered how the SMT performed as we changed the speech decoder parameters. We were not only interested in"
2011.iwslt-evaluation.22,P03-1006,0,0.0355375,"ence scores problematic. • The determinization operation will push the output labels forward towards the initial state of the WFST. Furthermore, the context-dependency transducer will introduce a delay between the input and output labels [20]. This means that we cannot interpret the output labels as corresponding to the actual word endings. • The determinization and weight pushing algorithms will move the weights closer to the initial state of the WFST. • Repeated lower order language model paths are introduced by the back-off approximation used in the WFST representation of the language model[21]. The lattices generated from the WFST decoder will have paths replicated that correspond to each of the lower order n-grams sequences. We initially looked at converting the WFST phone lattices to word lattices or confusion networks and using them as the input to the Moses decoder. However, we found that the system did not perform well, possibly due to the previously mentioned issues. Given these issues, we decided to investigate the use of a WFST based translation decoder. When direct coupling WFST implementations the total path scores not individual label scores are important, and this will"
2011.iwslt-evaluation.22,J10-3008,0,0.088634,"cribe our approach to optimizing the log-linear weights λn in the WFST framework. 5.2. Optimization of WFST Feature Weights The area we address in this section is the problem of tuning the log-linear weights within the WFST framework. The optimization of the feature weights requires that during decoding we maintain the individual feature contributions. However in the WFST framework after composition and optimization the contributions from each of the underlying knowledge sources is lost. One way to obtain the feature contributions is perform a re-alignment phase after decoding as described in [23], the drawback with this approach is the 171 need for a second decoding pass. An alternative approach is to access the internals of the composition process and try to recover the component state sequences from the state pairs the composition algorithm maintains internally . This approach would fail if during or after the decoding process we apply operations such as determinization or epsilon removal. The method we describe uses a tuple semiring which can track individual score contributions from a cascade of WFSTs. This allows for the preservation of scores after any composition or even optimi"
2011.iwslt-evaluation.22,2007.iwslt-1.1,0,\N,Missing
2011.iwslt-evaluation.5,2011.eamt-1.17,1,0.76364,"tion Case and punct No case and no punct BLEU 0.1190 0.1106 NIST 4.6929 4.7142 WER 0.7177 0.7523 PER 0.5746 0.5977 GTM 0.4844 0.4620 METEOR 0.4847 0.4503 TER 67.1430 71.8380 Table 1: The official results for the NICT system in terms of a variety of automatic evaluation metrics. ent sections of the sentence that could more effectively be translated separately. Since the input utterances are punctuated and contain spaces that indicate word boundaries we exploited this punctuation and space information as cues to determine the likely positions to delimit segmentation boundaries. In previous work [1] it has been shown that constraints of this type can be useful in managing the decoding of longer sentences. Constraining the search in the right way leaves a simpler problem for the machine translation decoder to solve, and one that can be performed considerably more efficiently than unconstrained decoding over the full search space.. The second strategy was to build the translation model for the system using two heterogeneous methods. The translation model is a key component in any phrase-based SMT system, and building this model using two different techniques can potentially bring benefits"
2011.iwslt-evaluation.5,W08-0336,0,0.0384903,"• Discontinuous (previous phrase-pair) • Swap (previous phrase-pair) 5. A word insertion penalty feature Based on a set of pilot experiments we decoded with no limit on the distances phrases could be moved in the reordering process during decoding. The base model above was augmented with an additional translation model feature intended to be an indicator of the quality/reliability of each phrase-pair; this feature will be explained later in Section 4.2. 2.2. Pre-processing The Chinese data supplied for this task was not segmented into words. We used the Stanford Chinese word segmentation tool [4, 5] with the Peking University (PKU) model to wordsegment this data. The English data was tokenized by applying a number of regular expressions to separate punctuation, 50 and split contractions such as “it’s” and “hasn’t” into two separate tokens. We also removed all case information from the English text to help to minimize issues of data sparseness in the models of the translation system. All punctuation was left in both source and target. We took the decision to generate target punctuation directly using the process of translation, rather than as a punctuation restoration step in post process"
2011.iwslt-evaluation.5,P03-1021,0,0.0477536,"lt in the same manner using the SRI language modeling toolkit [7]. 5-gram models were built for decoding the development and test data for evaluation, and 3-gram models were built for decoding during the parameter tuning process to speed up decoding. The language models were smoothed using modified KnesserNey smoothing. 2.4.2. Translation Model The translation model for the base system was built in the the standard manner using a 2-step process. First the training data To tune the values for the log-linear weights in our system, we use the standard minimum error-rate training procedure (MERT) [9]. The weights for the models were tuned using the development data supplied for the task. To perform the MERT tuning we used the publicly available ZMERT framework [10], and this allowed us to easily add and tune additional features into our models. The models were tuned with respect the BLEU metric [11]: ‘BLEU4 Closest’ that is built into the tool. 3. Rule-based Decoding Constraints 3.1. Motivation Translating long and complex sentences has been a critical problem in machine translation. A standard phrase-based statistical machine translation system cannot solve the problem of word reordering"
2011.iwslt-evaluation.5,P02-1040,0,0.0804854,"serNey smoothing. 2.4.2. Translation Model The translation model for the base system was built in the the standard manner using a 2-step process. First the training data To tune the values for the log-linear weights in our system, we use the standard minimum error-rate training procedure (MERT) [9]. The weights for the models were tuned using the development data supplied for the task. To perform the MERT tuning we used the publicly available ZMERT framework [10], and this allowed us to easily add and tune additional features into our models. The models were tuned with respect the BLEU metric [11]: ‘BLEU4 Closest’ that is built into the tool. 3. Rule-based Decoding Constraints 3.1. Motivation Translating long and complex sentences has been a critical problem in machine translation. A standard phrase-based statistical machine translation system cannot solve the problem of word reordering in the target when the source sentence has a complex structure. A syntax-based machine translation system could solve the problem by running a parser on the source sentence in order to get the syntactic structure, but when a sentence is long and complex, the parser may fail to give a correct parse tree."
2011.iwslt-evaluation.5,P03-1054,0,0.0202175,"hat is built into the tool. 3. Rule-based Decoding Constraints 3.1. Motivation Translating long and complex sentences has been a critical problem in machine translation. A standard phrase-based statistical machine translation system cannot solve the problem of word reordering in the target when the source sentence has a complex structure. A syntax-based machine translation system could solve the problem by running a parser on the source sentence in order to get the syntactic structure, but when a sentence is long and complex, the parser may fail to give a correct parse tree. Klein and Manning [12] have shown that the accuracy of parsing decreases as sentence length increases, and the parsing time increases dramatically. However, in this research, we found that even when a sentence is long and complex, it is possible to split a sentence into smaller units which can be translated separately with minor consideration of the context. The main problem here is locating the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almo"
2011.iwslt-evaluation.5,W09-0429,0,0.0596069,"is long and complex, it is possible to split a sentence into smaller units which can be translated separately with minor consideration of the context. The main problem here is locating the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a"
2011.iwslt-evaluation.5,C94-1069,0,0.0497907,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,1995.iwpt-1.8,0,0.0237346,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,P96-1025,0,0.0947943,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,W04-1101,0,0.0282081,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,P11-1064,1,0.886228,"t out that this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a supervised discriminative model that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results. In our system we employ a related technique that is able to perform direct phrase-to-phrase alignment and extraction in a single unified framework in a fully unsupervised manner [26]. The technique is based on a Pitman-Yor process model. Bayesian models of this form have recently proved themselves useful in the field of natural language processing, as they typically offer benefits over more traditional techniques based on maximum likelihood. In particular, they model the data according to a power law distribution that is often observed in linguistic data. Moreover, by encouraging the re-use of parameters in the model during training, Bayesian models of this type will prefer to build very compact models with few parameters that have a tendency not to over-fit the data. In"
2011.iwslt-evaluation.5,J07-2003,0,0.148834,"Missing"
2011.iwslt-evaluation.5,D10-1087,0,0.0658909,"Missing"
2011.iwslt-evaluation.5,W00-1308,0,0.0546915,"Missing"
2011.iwslt-evaluation.5,N03-1033,0,0.0127114,"Missing"
2011.iwslt-evaluation.5,P00-1056,0,0.27701,"man here is an out-of-vocabulary word and is marked with a ‘|’ symbol.). Decoding constraints Unconstrained decoding Constrained decoding BLEU score 10.84 11.16 Table 4: The effect of re-ordering constraints on translation quality. 4. Bayesian Alignment 4.1. Motivation In a standard phrase-based statistical machine translation system (and in the base system we used in this shared evaluation), a two-step alignment and extraction process is commonly used. In the first step, word-level alignment is performed both from source-to-target and from target-to-source using the publicly available GIZA++ [24] tool. In the second step, these two word-level alignments are combined and by means of a set of heuristics, a large set of bilingual phrase pairs that are consistent with these alignments are extracted. This approach although inelegant has proven itself to be highly effective in practice, and this is the reason for its pervasiveness. However, other approaches are possible. DeNero and Klein [25] point out that this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a"
2011.iwslt-evaluation.5,P10-1147,0,0.0337203,"two-step alignment and extraction process is commonly used. In the first step, word-level alignment is performed both from source-to-target and from target-to-source using the publicly available GIZA++ [24] tool. In the second step, these two word-level alignments are combined and by means of a set of heuristics, a large set of bilingual phrase pairs that are consistent with these alignments are extracted. This approach although inelegant has proven itself to be highly effective in practice, and this is the reason for its pervasiveness. However, other approaches are possible. DeNero and Klein [25] point out that this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a supervised discriminative model that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results. In our system we employ a related technique that is able to perform direct phrase-to-phrase alignment and extraction in a single unified framework in a fully unsupervised ma"
2011.iwslt-evaluation.5,J93-2003,0,\N,Missing
2011.iwslt-evaluation.5,D08-1076,0,\N,Missing
2011.iwslt-evaluation.5,P07-2045,0,\N,Missing
2011.iwslt-evaluation.5,2010.iwslt-evaluation.18,1,\N,Missing
2012.iwslt-evaluation.16,koen-2004-pharaoh,0,\N,Missing
2012.iwslt-evaluation.16,P04-1021,0,\N,Missing
2012.iwslt-evaluation.16,P07-2045,0,\N,Missing
2012.iwslt-evaluation.16,D09-1024,0,\N,Missing
2012.iwslt-evaluation.16,P09-1012,0,\N,Missing
2012.iwslt-evaluation.16,P12-1049,0,\N,Missing
2012.iwslt-evaluation.16,P11-1044,0,\N,Missing
2012.iwslt-evaluation.16,W11-3208,1,\N,Missing
2012.iwslt-evaluation.16,2010.iwslt-papers.7,1,\N,Missing
2012.iwslt-evaluation.16,2010.iwslt-evaluation.18,1,\N,Missing
2012.iwslt-evaluation.16,D08-1076,0,\N,Missing
2012.iwslt-evaluation.16,2012.eamt-1.60,0,\N,Missing
2013.mtsummit-papers.3,2012.iwslt-papers.6,0,0.0204834,"Missing"
2013.mtsummit-papers.3,2010.iwslt-papers.7,1,0.834779,"nd the work in this paper are the only romanization induction techniques reported in the literature to date. The advantage of these methods is that they can be applied to many different languages without the need for an existing romanization system, and can be optimized to fit a specific purpose. Furthermore, as we will show later, the strategy seems quite robust to noise in the data, and we were able to build effective systems from data that contained non-transliteration pairs. 4 Methodology Our method induces a romanization system directly from a non-parametric Bayesian bilingual alignment (Finch and Sumita, 2010) between source and target grapheme sequences. This model has been shown to align consistently, without a tendency to overfit the data, and is therefore suitable for both one-to-many and many-to-many alignment. We use Levenshtein distance (LD) to select an appropriate romanization from a set of candidates derived from the alignment. More formally, let S = (s1 , s2 , . . . , sI ) and T = (t1 , t2 , . . . , tI ) be corpora of source and target words respectively. Each si and ti are represented as sequences of graphemes in their respective writing systems. Let Π and Ω be sets of grapheme sequence"
2013.mtsummit-papers.3,W11-3208,1,0.788419,")] (2) ck ∈Cj Where D(ck ) is the cost in terms of Levenshtein distance from using romanization rule (oj , ck ). For a single occurrence of oj in the corpus, this cost is LD(ck , ψ(oj )), the Levenshtein distance between romanization candidate sequence ck and ψ(oj ), the target grapheme sequence aligned to oj . The expected value of this cost over the corpus is calculated according to: ∑ p(cl )LD(ck , cl ) (3) E[D(ck )] = l=1..K 5 Experiments 5.1 Inducing Japanese Romanization 5.1.1 Data For training and evaluation in our experiments we used the Japanese-English translation mining corpus of (Fukunishi et al., 2011). This corpus consists of 4339 Japanese-English word pairs extracted from Wikipedia interlanguage link titles, all of which are annotated as correct/incorrect transliteration pairs. 3800 of the word pairs were correct transliterations and 539 word pairs were noise. 5.1.2 Induced Systems We induced two different romanization systems from the data. The simplest method (Unigram) discovered romanizations for each individual kana character. A more sophisticated method learned romanizations for multiple sequences of kana (Ngram). Table 1 shows example romanization rules Kana カ ク グ ケ コ シ ジ ス ズ ゼ ツ ト"
2013.mtsummit-papers.3,W10-2405,0,0.0809689,"xisting systems. We will show later in this paper that in our chosen application, it is possible to induce a romanization system that is more effective than simply choosing from well-established existing schemes. 3 Related Work In many transliteration mining approaches (Aransa et al., 2012; Htun et al., 2012), romanization is required to compare words across languages, typically using normalized edit distance metrics. Statistical transliteration systems can be used, but these need large amounts of training data which may not be available. A simple system of automatic romanization was used by (Jiampojamarn et al., 2010) to great effect in the shared mining task of the NEWS2010 workshop. Their system allowed cross language comparison between word pairs in different scripts by aligning single characters in one script to either single Roman characters or to NULL. Their romanization rules roman20 ized by substitution with the most representative single character, or by deletion. Our work differs from theirs in that we are aiming to induce a full romanization involving multiple characters on the target side1 without the deletion of the characters to be romanized. The fact that romanization was only performed with"
2013.mtsummit-papers.3,W10-2403,0,0.0383865,"Missing"
2014.iwslt-evaluation.20,2012.eamt-1.60,0,0.0123428,"statistical machine translation (SMT) systems. Our focus was in several areas, specifically system combination, word alignment, and various language modeling techniques including the use of neural network joint models. Our experiments on the test set from the 2013 shared task, showed that an improvement in BLEU score can be gained in translation performance through all of these techniques, with the largest improvements coming from using large data sizes to train the language model. 1. Introduction In the IWSLT 2014 machine translation evaluation campaign, the NICT team participated in the TED [1] translation shared-task for Chinese-English. This paper describes the machine translation approach adopted for this campaign. Our system was a combination of phrase-based and hierarchical SMT systems. The combination was performed by reranking the n-best hypotheses from these systems. A loglinear model which used the hypothesis scores of the component systems as features was used to calculate the score used in reranking. Additional features were also added into the log-linear model, for example features from a neural network model, or talk-level language model scores. In addition to system co"
2014.iwslt-evaluation.20,P14-1129,0,0.135961,"mple features from a neural network model, or talk-level language model scores. In addition to system combination, we put emphasis on language modeling. We used three approaches to improve the language modeling in the system. In the first approach we used a language model that was an interpolation of an indomain language model, and a language model trained on the GIGAWORD data. In the second approach, we incorporated a language model trained on the machine translations of each talk in the test dataset into the reranking procedure. In the third approach, a bilingual feed-forward neural network [2] was used in the reranker. Finally, we also improved the word alignment by using combining the alignments from two independent aligners: GIZA++ [3] and a modified version of the CICADA aligner [4]. 2. Data We used same Chinese-English data sets in all of the experiments in this paper. The supplied bilingual data consisted of 179901 sentence pairs. From this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual tra"
2014.iwslt-evaluation.20,J03-1002,0,0.00988323,"ling. We used three approaches to improve the language modeling in the system. In the first approach we used a language model that was an interpolation of an indomain language model, and a language model trained on the GIGAWORD data. In the second approach, we incorporated a language model trained on the machine translations of each talk in the test dataset into the reranking procedure. In the third approach, a bilingual feed-forward neural network [2] was used in the reranker. Finally, we also improved the word alignment by using combining the alignments from two independent aligners: GIZA++ [3] and a modified version of the CICADA aligner [4]. 2. Data We used same Chinese-English data sets in all of the experiments in this paper. The supplied bilingual data consisted of 179901 sentence pairs. From this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual training data for the component SMT systems. We used the IWSLT 2013 test set for evaluation. For some of the experiments we used language models train"
2014.iwslt-evaluation.20,2005.mtsummit-papers.11,0,0.0128404,"rom this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual training data for the component SMT systems. We used the IWSLT 2013 test set for evaluation. For some of the experiments we used language models trained on the English LDC Gigaword dataset, a collection of approximately 4 billion words of international newswire text. 2.1. Pre-processing The English data was tokenized by applying the EUROPARL tokenizer [5]. We also removed all case information from the English text to help to minimize issues of data sparseness in the models of the translation system. All punctuation was left in both source and target. We took the decision to generate target punctuation directly using the process of translation, rather than as a punctuation restoration step in post processing based on experiments carried out for the 2010 IWSLT shared evaluation [6]. 2.2. Post-processing The output of the translation system was subject to the following post-processing steps which were carried out in the following order: 1. In all"
2014.iwslt-evaluation.20,P03-1021,0,0.0455445,"Missing"
2014.iwslt-evaluation.20,2013.iwslt-evaluation.1,0,0.0146617,"cores of the component systems (MERT) [10]. The weights for the models were tuned using the development data supplied for the task. 3.5. Evaluation We evaluated each of these systems on the IWSLT 2013 test set, and the results are shown in Table 3.5. The evaluation in all of the experiments in this report was carried out on tokenized, lowercase data, using the “multi-bleu.perl” evaluation script included in release version 2.1 of the MOSES toolkit. The systems are roughly comparable in performance, and about 1.5 BLEU percentage points higher than the caseinsensitive MOSES baseline reported in [11], we believe this can be explained by differences in the tokenization used for evaluation, and also by differences in the development sets used for tuning. We found that when tuned and evaluated on different data sets, the relative rankings of the systems may vary. 4. Methodology 4.1. Language Modeling 4.1.1. Neural Network Model We implemented the neural network joint models proposed in [2] and used the output as a feature in the reranker. We ran a set of experiments to determine the optimal network architecture. We varied the size of the context on both source and sides, and also the scale o"
2014.iwslt-evaluation.20,D13-1140,0,0.0203032,"d the output as a feature in the reranker. We ran a set of experiments to determine the optimal network architecture. We varied the size of the context on both source and sides, and also the scale of the neural network. We found the settings used in [2] gave rise the highest performance, and we therefore adopted these settings in our system. These settings were: 11-word source context, 3-word target context, 192-unit shared embedding layer, and two additional 512unit hidden layers. We set both input and output vocabulary size to 32000. The neural network was implemented using the NPLM toolkit [12]. The results are shown in Table 4.3. The gain using from this approach was approximately 0.5 BLEU points. This was lower than the gains reported in [2], however, in their experiments the neural network was directly integrated into the decoding process. We integrated monolingual neural network model into the OCTAVIAN decoder, however, the experiments were not completed due to time limitations. 4.1.2. Gigaword We combined language models trained on the source of the parallel TED corpus, and the Gigaword newswire corpus by linear interpolation. The interpolated language model was then used direc"
2014.iwslt-evaluation.20,P96-1041,0,0.370762,"Missing"
2014.iwslt-evaluation.20,P07-2045,0,0.0124426,"the following order: 1. In all experiments, the out of vocabulary words (OOVs) were passed through the translation process unchanged, some of these OOVs were Chinese and some English. For the primary submission, we took 139 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the decision to delete only those OOVs containing Chinese characters not included in the ASCII character set and leave words containing only ASCII characters in the output. 2. The output was de-tokenized using the de-tokenizer included with the MOSES toolkit [7]. 3. The output was re-cased using the re-casing tool supplied with the MOSES toolkit. We trained the recasing tool on cased text from the TED talk training data. 3. The Base Systems 3.1. Decoders Our submission used two SMT systems within a system combination framework; these systems were: 1. OCTAVIAN, an in-house phrase-based decoder. 2. A hierarchical version of the MOSES decoder [7]. The OCTAVIAN decoder used in these experiments is an in-house phrase-based statistical machine translation decoder that can operate in a similar manner to the publicly available MOSES decoder [7]. The base dec"
2014.iwslt-papers.5,N03-1017,0,0.00933623,"Missing"
2014.iwslt-papers.5,P07-2045,0,0.0182188,"Missing"
2014.iwslt-papers.5,2005.mtsummit-papers.11,0,0.0574603,"Missing"
2014.iwslt-papers.5,W12-4207,0,0.0512408,"Missing"
2014.iwslt-papers.5,N09-1028,0,0.0455172,"Missing"
2014.iwslt-papers.5,D12-1077,0,0.0272006,"Missing"
2014.iwslt-papers.5,C04-1073,0,0.107251,"Missing"
2014.iwslt-papers.5,P05-1066,0,0.116818,"Missing"
2014.iwslt-papers.5,C10-1043,0,0.0500847,"Missing"
2014.iwslt-papers.5,P03-1056,0,0.0731239,"Missing"
2014.iwslt-papers.5,P13-1045,0,0.0374312,"Missing"
2014.iwslt-papers.5,N03-1033,0,0.0314199,"Missing"
2014.iwslt-papers.5,2008.jeptalnrecital-long.17,0,0.133128,"Missing"
2014.iwslt-papers.5,J03-1002,0,0.0078425,"Missing"
2014.iwslt-papers.5,N04-4026,0,0.0740724,"Missing"
2014.iwslt-papers.5,P03-1021,0,0.080262,"Missing"
2014.iwslt-papers.5,P02-1040,0,0.0911959,"Missing"
2014.iwslt-papers.5,D10-1092,0,0.0676014,"Missing"
2014.iwslt-papers.5,Y13-1026,0,0.0235984,"Missing"
2014.iwslt-papers.8,N13-1023,0,0.0474077,"method to integrate them. 2. Related Work The work in this paper is based upon the stream decoder [1], an extension to a phrase-based statistical machine translation decoder that allows it to decode directly from continuous stream of tokens. We describe this methodology in more detail in Section 3. In [2] the prosody information in the speech signal was used to segment a continuous stream of speech input for translation. In their experiments, a silence duration of approximately 100ms was found to be suitable for segmentation. A number of diverse strategies for pre-segmentation were studied in [3]. They studied both non-linguistic techniques, that included ﬁxed-length segments, and a “hold-output” method. The hold-output method method is relevant to the research in this paper because it relies the same principle used by the stream decoder. It identiﬁes contiguous blocks of text that do not contain alignments to words outside them. An SVM was used to predict these blocks prior to the decoding process; the stream decoder operates by identifying similar structures during decoding. Their experimental results showed this method to be ineffective. Linguisticallymotivated segmentation techniq"
2014.iwslt-papers.8,P14-2090,0,0.337584,"this paper because it relies the same principle used by the stream decoder. It identiﬁes contiguous blocks of text that do not contain alignments to words outside them. An SVM was used to predict these blocks prior to the decoding process; the stream decoder operates by identifying similar structures during decoding. Their experimental results showed this method to be ineffective. Linguisticallymotivated segmentation techniques were also considered. Conjunctions, sentence boundaries and commas were investigated, with commas being the most effective segmentation cue in their investigation. In [4] a strategy for pre-segmentation based on searching for segmentation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmentation can be controlled 206 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. The automatic interpretation from English into Japanese has been studied in [5]. Their approach used heuristics to identify predicates that are lik"
2014.iwslt-papers.8,P07-2045,0,0.00667062,"In this alternative strategy, the stream decoder will undertake a new decoding pass in which it is forced to make a monotonic step as the ﬁrst step in the decoding process. Then, a state is selected from the best hypothesis using Algorthim 1. This process may also fail if the monotonic step would lead to the violation of Lmin . In our implementation, we allow the decoder to violate Lmin only in this case. Our stream decoder was implemented within the framework of the OCTAVIAN decoder, a phrase-based statistical machine translation decoder that operates in a similar manner to the MOSES decoder [11]. The training procedure was quite typical: 5-gram language models were used, trained with modiﬁed Kneser-Ney smoothing; MERT [12] was used to train the log-linear weights of the models; the decoding was performed with no limit on the distortion. 4.3. Evaluation The BLEU score [13] was used to evaluate the machine translation quality in all our experiments. Where sentence segmentation was known we used both talk and sentencelevel BLEU, and for the experiments where true stream decoding was performed on a stream of tokens with no segmentation information, talk-level BLEU was used. In talk level"
2014.iwslt-papers.8,P03-1021,0,0.00994997,"as the ﬁrst step in the decoding process. Then, a state is selected from the best hypothesis using Algorthim 1. This process may also fail if the monotonic step would lead to the violation of Lmin . In our implementation, we allow the decoder to violate Lmin only in this case. Our stream decoder was implemented within the framework of the OCTAVIAN decoder, a phrase-based statistical machine translation decoder that operates in a similar manner to the MOSES decoder [11]. The training procedure was quite typical: 5-gram language models were used, trained with modiﬁed Kneser-Ney smoothing; MERT [12] was used to train the log-linear weights of the models; the decoding was performed with no limit on the distortion. 4.3. Evaluation The BLEU score [13] was used to evaluate the machine translation quality in all our experiments. Where sentence segmentation was known we used both talk and sentencelevel BLEU, and for the experiments where true stream decoding was performed on a stream of tokens with no segmentation information, talk-level BLEU was used. In talk level BLEU each talk is considered to be a single sentence in the BLEU computation. For consistency only the talk level BLEU results ar"
2014.iwslt-papers.8,2001.mtsummit-papers.68,0,0.00881081,"onic step would lead to the violation of Lmin . In our implementation, we allow the decoder to violate Lmin only in this case. Our stream decoder was implemented within the framework of the OCTAVIAN decoder, a phrase-based statistical machine translation decoder that operates in a similar manner to the MOSES decoder [11]. The training procedure was quite typical: 5-gram language models were used, trained with modiﬁed Kneser-Ney smoothing; MERT [12] was used to train the log-linear weights of the models; the decoding was performed with no limit on the distortion. 4.3. Evaluation The BLEU score [13] was used to evaluate the machine translation quality in all our experiments. Where sentence segmentation was known we used both talk and sentencelevel BLEU, and for the experiments where true stream decoding was performed on a stream of tokens with no segmentation information, talk-level BLEU was used. In talk level BLEU each talk is considered to be a single sentence in the BLEU computation. For consistency only the talk level BLEU results are reported in this paper, but the results from the sentence-level BLEU experiments were similar in character. 1 http://www.ted.com 208 Proceedings of th"
2014.iwslt-papers.8,P06-2088,0,0.0770843,"eing the most effective segmentation cue in their investigation. In [4] a strategy for pre-segmentation based on searching for segmentation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmentation can be controlled 206 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. The automatic interpretation from English into Japanese has been studied in [5]. Their approach used heuristics to identify predicates that are likely to be invertible from a dependency structure derived from a phrase-structure parse of the English. They exploit the somewhat free word order of Japanese to re-order the Japanese tokens into an order that is appropriate for interpretation. The resulting word order may be a little dis-ﬂuent, but is nonetheless grammatically valid and is typical of the kind of compromise that needs to be made during interpretation. There are also some related studies in translation process research (for example, [6, 7]) that study in detail t"
2014.iwslt-papers.8,2008.iwslt-papers.5,0,0.0164548,"nterpretation. There are also some related studies in translation process research (for example, [6, 7]) that study in detail the process of human simultaneous interpretation. In [8] it was shown that the prediction and use of soft boundaries in the source language text, when used as reordering constraints can improve the quality of a speech translation system. 3. Stream Decoding The stream decoding strategy differs from approaches based on the pre-segmentation of the stream of input tokens in that the segmentation decisions are able to exploit information from the decoding process itself. In [9], it is stated that long segments of around 10-40 words are required in a presegmentation strategy in order to achieve performance close to the underlying machine translation system. These long segments give rise to long latencies, and the penalty for reducing the segment size in order to achieve acceptably latencies is typically severe. These issues have been addressed recently with more intelligent strategies for choosing the segmentation points [4], but nonetheless we believe the stream decoding approach deserves more attention in the literature, and merits further study for the following r"
2014.iwslt-papers.8,I05-3027,0,0.0448543,"ore the operation of and enhancements to the stream decoder we use the TED1 talks data sets from the IWSLT2014 campaign. We studied English to: Spanish, Italian, French, German and Chinese, and found the results on the set of European language pairs were mostly similar in character, and we therefore report results on only English-Spanish (a typical pair) and English-German (an exceptional pair) from this set. Statistics on the corpora are given in Table 1. The European language data was tokenized by the Stanford PTBTokenizer. The Chinese was segmented using the Stanford Chinese word segmenter [10] according to the Chinese Penn Treebank standard. 4.2. Decoder Test 1701 1397 1700 Table 1: Statistics on corpora using in the stream decoding experiments. The numbers given are in segments, representing individual subtitles, corresponding approximately to sentences. such state exists, in which case the algorithm returns s0 , and since the stream decoder is required to make an output, it must use an alternative strategy. In this alternative strategy, the stream decoder will undertake a new decoding pass in which it is forced to make a monotonic step as the ﬁrst step in the decoding process. Th"
2015.mtsummit-papers.4,P96-1041,0,0.203437,"ated to lemmatized words in the English sentences by using a Japanese-English dictionary. 4.2.2 Phrase-based SMT The phrase-based SMT experiments were performed with the two models, in which the parameters θ and θ¯ were estimated on DEV.1 and DEV.2 of the phrase-pair extraction experiment, using the phrase-pairs extracted using the Moses toolkit. We added the inverse of the error distance used in the ranking experiments, as a feature into the log-linear model of the Moses decoder. The 5-gram language models were built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996). For word and phrase alignments, we used MGIZA++ and grow-diag-final-and heuristics. To tune the weights with respect to the BLEU score (Papineni et al., 2002), we used n-best batch MIRA (Cherry and Foster, 2012). The distortion limit parameter was set to 10. We evaluated each model on BLEU using the NIST’s mteval-v13a.pl 2 script. Statistical significance testing of the BLEU differences was performed using paired bootstrap resampling (Koehn, 2004). For both experiments, we tested with two models. The first was the proposed Bilingual Phrase Representation Model (BPRM). The second was the same"
2015.mtsummit-papers.4,N12-1047,0,0.0198592,"θ¯ were estimated on DEV.1 and DEV.2 of the phrase-pair extraction experiment, using the phrase-pairs extracted using the Moses toolkit. We added the inverse of the error distance used in the ranking experiments, as a feature into the log-linear model of the Moses decoder. The 5-gram language models were built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996). For word and phrase alignments, we used MGIZA++ and grow-diag-final-and heuristics. To tune the weights with respect to the BLEU score (Papineni et al., 2002), we used n-best batch MIRA (Cherry and Foster, 2012). The distortion limit parameter was set to 10. We evaluated each model on BLEU using the NIST’s mteval-v13a.pl 2 script. Statistical significance testing of the BLEU differences was performed using paired bootstrap resampling (Koehn, 2004). For both experiments, we tested with two models. The first was the proposed Bilingual Phrase Representation Model (BPRM). The second was the same BPRM model with the autoencoder layer removed. 5 Results and Analysis Tables 2 and 3 present the results of the phrase pair extraction task, and Table 4 presents the results of the phrase-based SMT task. 2 ftp://"
2015.mtsummit-papers.4,D14-1179,0,0.1104,"Missing"
2015.mtsummit-papers.4,P14-1129,0,0.0179666,"otation: the left-side RNN is referred to as the source-side and the right-side RNN (we use overbars on the symbols to differentiate it) is referred to as the target-side. 2 Related work In this section, we review recent work on neural network phrase representation models. Continuous phrase representation models with a feed-forward neural network were studied in Schwenk (2007, 2012). The models estimated translation probabilities for unseen phrases with a continuous vector space of phrases. Le et al. (2012) proposed a similar approach to score phrase pairs using fixed-size inputs and outputs. Devlin et al. (2014) proposed a neural network joint model (NNJM) as an extension of the NNLM (Bengio et al., 2003). The NNJM calculates the target-side word probability by using a target-side language model in combination with a context from the source-side. The NNJM requires a maximum length for the source-side phrases. These approaches employ feed-forward neural networks and are constrained to operate on phrases of limited length. The use of recursive neural networks addresses the fixed-size issue by using a tree structure of phrases and sentences. The recursive neural network maps features from subsequences o"
2015.mtsummit-papers.4,W08-0509,0,0.0213167,"h contains the most documents among the domains, according to International Patent Classification (IPC) code 1 . 720K sentence pairs from the documents published between 1993 to 2005 were used as the training data, and 2.0K and 0.5K sentence pairs randomly sampled from the 2006 and 2007 documents were used as the development and test data respectively. We also used the 2006 and 2007 documents to extract the similar phrases. Furthermore, we used the English and Japanese monolingual corpus in NTCIR-10 for the training of word representations. For the extraction of phrase pairs, we used MGIZA++ (Gao and Vogel, 2008) and growdiag-final-and heuristics of the Moses toolkit (Koehn et al., 2007). To facilitate effective learning, we used only phrase pairs that contained content words (i.e. had at least one noun or verb word in the phrase) and had a high translation probability (a threshold on the source-given-target conditional probability was used). We extracted phrase pairs from the training, development and test data. The training phrase-pairs were used for training the neural network models. The development phrase-pairs were used to control the training of the models. The model was trained for 4,000 itera"
2015.mtsummit-papers.4,P13-1031,0,0.017613,"est word types en ja 0.8K 0.9K 2.4K 2.1K Monolingual sent en ja 41M 81M Table 1: Data sets where θ¯ is the set of target-side parameters, and α and β are the hyper-parameters for the balance of each error. We also use an L1 regularization term in the objective function. In Equation (8), we group the semantic error Ec terms of source- and target-side (which use the summary vectors c and ¯ c) into one term, and arrange the terms according to error type. The parameters θ and θ¯ are optimized to minimize Equation (8) using the AdaGrad stochastic adaptive subgradient algorithm (Duchi et al., 2011; Green et al., 2013): θi = θi−1 − η Gi = Gi−1 + ∂J −1/2 G ∂θi−1 i ∂J 2 ∂θi−1 (9) (10) where η is the learning rate, i is the number of the training iterations, and G is the sum of the squares of the past gradients. θ and θ¯ are learned and updated in every iteration through the training data of phrase-pairs. The number of training iterations was determined using development data. 3.2 Word representations Word representations, in which words are represented as real-valued vectors (Bengio et al., 2003; Mikolov et al., 2013), serve as the inputs to our model. The word representations r are calculated as: ri = Lui ∈"
2015.mtsummit-papers.4,D13-1176,0,0.0437439,"dicted phrase reorderings in SMT that was able to exploit syntactic and semantic information, Zhang et al. (2014, 2015) proposed bilingually-constrained recursive autoencoders, which generated phrasal embeddings for machine translation by learning to minimize the semantic distance between translation equivalents, and maximizing the distance between non-translation pairs. In contrast to the work on recursive networks, it is also possible to create continuous phrase representations with RNNs. Here, simpler models are possible that do not need take the tree structure of their input into account. Kalchbrenner and Blunsom (2013) proposed recurrent continuous translation models based on recurrent language models (RLMs), which predict target words from an unbounded history of both source and target words with a conditional probability. In their implementation convolutional neural networks were used to model the source-side. Cho et al. (2014) proposed a gated recurrent unit which adaptively remembers and forgets its state based on the input signal to the unit. This model was used to score each phrase pair in the phrase table for SMT. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |"
2015.mtsummit-papers.4,W04-3250,0,0.318544,"Missing"
2015.mtsummit-papers.4,P07-2045,0,0.00600191,"atent Classification (IPC) code 1 . 720K sentence pairs from the documents published between 1993 to 2005 were used as the training data, and 2.0K and 0.5K sentence pairs randomly sampled from the 2006 and 2007 documents were used as the development and test data respectively. We also used the 2006 and 2007 documents to extract the similar phrases. Furthermore, we used the English and Japanese monolingual corpus in NTCIR-10 for the training of word representations. For the extraction of phrase pairs, we used MGIZA++ (Gao and Vogel, 2008) and growdiag-final-and heuristics of the Moses toolkit (Koehn et al., 2007). To facilitate effective learning, we used only phrase pairs that contained content words (i.e. had at least one noun or verb word in the phrase) and had a high translation probability (a threshold on the source-given-target conditional probability was used). We extracted phrase pairs from the training, development and test data. The training phrase-pairs were used for training the neural network models. The development phrase-pairs were used to control the training of the models. The model was trained for 4,000 iterations, and estimated parameters θ and θ¯ by evaluating the highest accuracy"
2015.mtsummit-papers.4,N03-1017,0,0.0605517,"representations (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012) based on neural networks have outperformed the previous stateof-the-art approaches. These language models map each word to a dense, low-dimensional, real-valued vector, and estimate the probability of words in a continuous space. Representations for phrases have been used in the context of Statistical Machine Translation (SMT). Zou et al. (2013) used phrasal representations for computing the distance between phrase pairs and added a feature based on this distance into the log-linear model of a phrase-based SMT system (Koehn et al., 2003). Their method learned bilingual word representations, and subsequently obtained the phrase-level representations by simply averaging word vectors. Continuous representations for phrases or sentences with neural networks – such as a feed-forward, recursive, or recurrent neural networks – have also been used in SMT. A phrase representation model using a feed-forward neural network for phrase-based SMT was proposed by Schwenk (2007, 2012), and achieved significant BLEU score improvements. Since the model directly projects feature vectors not from words but from phrases or sentences onto a contin"
2015.mtsummit-papers.4,N12-1005,0,0.0172388,"ur model is symmetric and does not differentiate between source- and target-side, we use the following notation: the left-side RNN is referred to as the source-side and the right-side RNN (we use overbars on the symbols to differentiate it) is referred to as the target-side. 2 Related work In this section, we review recent work on neural network phrase representation models. Continuous phrase representation models with a feed-forward neural network were studied in Schwenk (2007, 2012). The models estimated translation probabilities for unseen phrases with a continuous vector space of phrases. Le et al. (2012) proposed a similar approach to score phrase pairs using fixed-size inputs and outputs. Devlin et al. (2014) proposed a neural network joint model (NNJM) as an extension of the NNLM (Bengio et al., 2003). The NNJM calculates the target-side word probability by using a target-side language model in combination with a context from the source-side. The NNJM requires a maximum length for the source-side phrases. These approaches employ feed-forward neural networks and are constrained to operate on phrases of limited length. The use of recursive neural networks addresses the fixed-size issue by usi"
2015.mtsummit-papers.4,D13-1054,0,0.0988804,"M (Bengio et al., 2003). The NNJM calculates the target-side word probability by using a target-side language model in combination with a context from the source-side. The NNJM requires a maximum length for the source-side phrases. These approaches employ feed-forward neural networks and are constrained to operate on phrases of limited length. The use of recursive neural networks addresses the fixed-size issue by using a tree structure of phrases and sentences. The recursive neural network maps features from subsequences of a phrase to a continuous vector on each node of the tree recursively. Li et al. (2013) described an ITG reordering classifier which predicted phrase reorderings in SMT that was able to exploit syntactic and semantic information, Zhang et al. (2014, 2015) proposed bilingually-constrained recursive autoencoders, which generated phrasal embeddings for machine translation by learning to minimize the semantic distance between translation equivalents, and maximizing the distance between non-translation pairs. In contrast to the work on recursive networks, it is also possible to create continuous phrase representations with RNNs. Here, simpler models are possible that do not need take"
2015.mtsummit-papers.4,P11-2093,0,0.0217739,"Word2Vec toolkit (Mikolov et al., 2013). The size of the word representation vector n is usually determined empirically. 4 Experiments We conducted two experiments with the Bilingual Phrase Representation Model: a phrase-pair extraction task and a phrase-based SMT task. 4.1 Data and model parameters Both experiments were conducted on two English-Japanese (en-ja) corpora. One was from IWSLT 2007 (Fordyce, 2007) which is in the domain of spoken travel conversation and the other was a patent translation corpus from NTCIR-10 (Goto et al., 2013). The Japanese sentences were tokenized using KyTea (Neubig et al., 2011). Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 48 Table 1 provides statistics on each corpus. The “sent” column indicates the number of sentence pairs, and the “word types” columns of “en” and “ja” indicate the number of English and Japanese unique words. The “Monolingual” column indicates the size of the monolingual data for the training of the word representations described in Section 3.2. For IWSLT 2007, we used the training data for the training of the word representations. For NTCIR-10, we used about 723K sentence pairs belonging to the physics"
2015.mtsummit-papers.4,P02-1040,0,0.0922584,"with the two models, in which the parameters θ and θ¯ were estimated on DEV.1 and DEV.2 of the phrase-pair extraction experiment, using the phrase-pairs extracted using the Moses toolkit. We added the inverse of the error distance used in the ranking experiments, as a feature into the log-linear model of the Moses decoder. The 5-gram language models were built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996). For word and phrase alignments, we used MGIZA++ and grow-diag-final-and heuristics. To tune the weights with respect to the BLEU score (Papineni et al., 2002), we used n-best batch MIRA (Cherry and Foster, 2012). The distortion limit parameter was set to 10. We evaluated each model on BLEU using the NIST’s mteval-v13a.pl 2 script. Statistical significance testing of the BLEU differences was performed using paired bootstrap resampling (Koehn, 2004). For both experiments, we tested with two models. The first was the proposed Bilingual Phrase Representation Model (BPRM). The second was the same BPRM model with the autoencoder layer removed. 5 Results and Analysis Tables 2 and 3 present the results of the phrase pair extraction task, and Table 4 presen"
2015.mtsummit-papers.4,C12-2104,0,0.0382569,"Missing"
2015.mtsummit-papers.4,D11-1014,0,0.0337723,"the last hidden layer hl is the summary representation c (c ≡ hl ). A shared semantic representation of both source and target is required, and therefore c and ¯ c are trained jointly using error signals based on the distance between them. The error is calculated using the semantic error defined in Equation (6). Equation (7) is the sum of the reconstruction error distance between each input rk in the source-side phrase and the autoencoder’s reconstruction ak . The autoencoder is used for learning representations of words (Chandar A P et al., 2014), phrases (Zhang et al., 2015), and sentences (Socher et al., 2011; Li et al., 2013). The target-side errors were calculated in the same manner. The objective function J is the sum of the total error distance from the source and target RNNs, and is represented by using Equations (5), (6), and (7) as: J = αEo (r|o; θ) + Ec (¯ c|c; θ) + βEa (r|a; θ) + λ∥θ∥ ¯ ¯ + βEa (¯ ¯ + λ∥θ∥ ¯ +αEo (¯ r|¯ o; θ) + Ec (c|¯ c; θ) r|¯ a; θ) ¯ = 2 · Ec (¯ c|c; θ, θ) ¯ +α(Eo (r|o; θ) + Eo (¯ r|¯ o; θ)) ¯ +β(Ea (r|a; θ) + Ea (¯ r|¯ a; θ)) ¯ +λ(∥θ∥ + ∥θ∥) Proceedings of MT Summit XV, vol.1: MT Researchers' Track (8) Miami, Oct 30 - Nov 3, 2015 |p. 47 Data IWSLT 2007 NTCIR-10 Traini"
2015.mtsummit-papers.4,P14-1011,0,0.0319151,"ce-side. The NNJM requires a maximum length for the source-side phrases. These approaches employ feed-forward neural networks and are constrained to operate on phrases of limited length. The use of recursive neural networks addresses the fixed-size issue by using a tree structure of phrases and sentences. The recursive neural network maps features from subsequences of a phrase to a continuous vector on each node of the tree recursively. Li et al. (2013) described an ITG reordering classifier which predicted phrase reorderings in SMT that was able to exploit syntactic and semantic information, Zhang et al. (2014, 2015) proposed bilingually-constrained recursive autoencoders, which generated phrasal embeddings for machine translation by learning to minimize the semantic distance between translation equivalents, and maximizing the distance between non-translation pairs. In contrast to the work on recursive networks, it is also possible to create continuous phrase representations with RNNs. Here, simpler models are possible that do not need take the tree structure of their input into account. Kalchbrenner and Blunsom (2013) proposed recurrent continuous translation models based on recurrent language mod"
2015.mtsummit-papers.4,D13-1141,0,0.0290209,"epresentations of word, phrase, and sentence which alleviate issues of sparsity have successfully been used in a number of natural language processing tasks. Language models with continuous word representations (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012) based on neural networks have outperformed the previous stateof-the-art approaches. These language models map each word to a dense, low-dimensional, real-valued vector, and estimate the probability of words in a continuous space. Representations for phrases have been used in the context of Statistical Machine Translation (SMT). Zou et al. (2013) used phrasal representations for computing the distance between phrase pairs and added a feature based on this distance into the log-linear model of a phrase-based SMT system (Koehn et al., 2003). Their method learned bilingual word representations, and subsequently obtained the phrase-level representations by simply averaging word vectors. Continuous representations for phrases or sentences with neural networks – such as a feed-forward, recursive, or recurrent neural networks – have also been used in SMT. A phrase representation model using a feed-forward neural network for phrase-based SMT"
2020.ngt-1.1,W19-5301,0,0.0795298,"Missing"
2020.ngt-1.1,N19-1423,0,0.00776584,"o 15 system submission papers. We elicted two double-blind reviews for each submission, avoiding conflicts of interest. With regards to thematology there were 8 papers with a focus on Natural Language Generation and 8 with the application of Machine Translation 1 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 1–9 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d in mind. The underlying emphasis across submissions was placed this year on capitalizing on the use of pre-training models (e.g., BERT; (Devlin et al., 2019) especially for low-resource datasets. The quality of the accepted publications was very high; there was a significant drop in numbers though in comparison to last year (36 accepted papers from 68 submissions) which is most likely due to the extra overhead on conducting research under lockdown policies sanctioned globally due to COVID19 pandemic. 3 GPU is relatively small compared to the NVIDIA V100 GPU, but the newer Turing architecture introduces support for 4-bit and 8-bit integer operations in Tensor Cores. In practice, however, participants used floating-point operations on the GPU even t"
2020.ngt-1.1,D13-1176,0,0.0312763,"Second, we describe the results of the three shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document-level generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language and 3) STAPLE task: creation of as many possible translations of a given input text. This last shared task was organised by Duolingo. 1 Introduction 2 Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are the workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 4th Workshop on Neural Machine Translation and Generation (WNGT 2020) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization, NLG from structured data, dialog response generation, among others). Overall, the workshop was held with two goals. First, it aimed to synthesize the current state of"
2020.ngt-1.1,W04-1013,0,0.0806851,"of parameters and 8-bit quantization. OpenNMT’s small lower-quality models have low CPU RAM and Docker image size; UEdin is Pareto-optimal for higher-quality models. OpenNMT was the only team to optimize for these metrics in their system description. In their multicore CPU submission, OpenNMT shared memory amongst processes while other participants simply used multiple processes with copies of the model. 4 4.1 Evaluation Measures We employ standard evaluation metrics for the tasks above along two axes following (Hayashi et al., 2019): Textual Accuracy: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for surface-level texutal accuracy compared to reference summaries. Document Generation and Translation Task Following the previous workshop, we continued with the shared task of document-level generation and translation. This task is motivated as the central evaluation testbed for document-level generation systems with different types of inputs by providing parallel dataset consisting of structured tables and text in two languages. We host various tracks within the testbed based on input and output constraints and investigate and contrast the system differences. In particular, we"
2020.ngt-1.1,2020.ngt-1.28,0,0.0265263,"tput, but in certain cases, it is desirable to have many possible translations of a given input text. At Duolingo, the world’s largest online language-learning platform,7 we grade translationbased challenges with sets of human-curated acceptable translation options. Given the many ways of expressing a piece of text, these sets are slow to create, and may be incomplete. This process is ripe for improvement with the aid of rich multi-output translation and paraphrase systems. To this end, we introduce a shared task called STAPLE: Simultaneous Translation and Paraphrasing for Language Education (Mayhew et al., 2020). 4.4 5.1 4.3 Baselines We prepared two baselines for different tracks: FairSeq-19 We use FairSeq (Ng et al., 2019) (WMT’19 single model6 ) for MT and MT+NLG tracks. Submitted Systems One team participated in the task, who focused on the German-English MT track of the task. In this shared task, participants are given a training set consisting of 2500 to 4000 English sentences (or prompts), each of which is paired with a list of comprehensive translations in the target language, weighted and ordered by normalized learner response frequency. At test time, participants are given 500 English promp"
2020.ngt-1.1,D18-1325,0,0.021768,"guage, weighted and ordered by normalized learner response frequency. At test time, participants are given 500 English prompts, and are required to produce the set of comprehensive translations for each prompt. We also provide a high-quality automatic reference translation for each prompt, in the event that a participant wants to work on paraphrase-only approaches. The target languages were Hungarian, Japanese, Korean, Portuguese, and Vietnamese. Team FJWU developed a system around Transformer-based sequence-to-sequence model. Additionally, the model employed hierarchical attention following (Miculicich et al., 2018) for both encoder and decoder to account for the documentlevel context. The system was trained in a twostage process, where a base (sentence-level) NMT model was trained followed by the training of hierarchcal attention networks component. To handle the scarcity of in-domain translation data, they experimented with upsizing the in-domain data up to three times to construct training data. Their ablation experiments showed that this upsizing of in-domain data is effective at increasing the BLEU score. 4.5 Task Description 5.2 Submitted Systems There were 20 participants who submitted to the deve"
2020.ngt-1.1,W19-5333,0,0.0306894,"Missing"
2020.ngt-1.1,P02-1040,0,0.107839,"guage. mostly driven by the number of parameters and 8-bit quantization. OpenNMT’s small lower-quality models have low CPU RAM and Docker image size; UEdin is Pareto-optimal for higher-quality models. OpenNMT was the only team to optimize for these metrics in their system description. In their multicore CPU submission, OpenNMT shared memory amongst processes while other participants simply used multiple processes with copies of the model. 4 4.1 Evaluation Measures We employ standard evaluation metrics for the tasks above along two axes following (Hayashi et al., 2019): Textual Accuracy: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for surface-level texutal accuracy compared to reference summaries. Document Generation and Translation Task Following the previous workshop, we continued with the shared task of document-level generation and translation. This task is motivated as the central evaluation testbed for document-level generation systems with different types of inputs by providing parallel dataset consisting of structured tables and text in two languages. We host various tracks within the testbed based on input and output constraints and investigate and contrast the system differen"
2020.ngt-1.1,W18-6319,0,0.0283845,"Missing"
C16-1291,D16-1162,0,0.0517274,"NMT1 and NMT2 are much worse than Moses with a substantial gap. This result is not difficult to understand: neural network systems typically require sufficient data to boost their performance, and thus low resource translation tasks are very challenging for them. Secondly, the proposed SA-NMT gains much over NMT2 similar to the case in the large scale task, and the gap towards Moses is narrowed substantially. While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine trans"
C16-1291,J16-2001,0,0.0929956,"ved by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://crea"
C16-1291,J93-2003,0,0.0944848,"igures out those source words will be translated next, even though the next target word yt is unavailable. From this point of view, the attention mechanism plays a role in reordering and thus can be considered as a reordering model. Unlike this attention model, conventional alignment models define the alignment α directly over x and y as follows: exp(F (x, y, α)) 0 α0 exp(F (x, y, α )) p(α |x, y) = P where F denotes a feature function over a pair of sentences x and y together with their word alignment α, and it is either a log-probability log p(y, α |x) for a generative model like IBM models (Brown et al., 1993) or a well-designed feature function for discriminative models (Liu and Sun, 2015). In order to infer αt , alignment models can readily use the entire y, of course including yt as well, thereby they can model the alignment between x and y more sufficiently. As a result, the attention based NMT might not deliver satisfying alignments, as reported in (Cheng et al., 2016), compared to conventional alignment models. This may be a sign that the potential of attention-based NMT is limited in end-to-end translation. 3 Supervised Attention In this section, we introduce supervised attention to improve"
C16-1291,2016.amta-researchers.10,0,0.330705,"vel reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs. Supervising the attention variables for attention-based neural networks is pioneered by Liu et al. 3100 (2016). On image caption task, Liu et al. (2016) supervise the attention with external guidances in either a strong or a weak supervision manner. Their method requires the training data to be associated with direct annotation or indirect annotation. In parallel to our work, particularly on machine translation, Mi et al. (2016) and Chen et al. (2016) guide the attention for NMT from conventional word alignment models as teachers without any annotation on machine translation task. The differences of our work lie in that: we consider the attention as a form of a reordering model, which is thereby straightforward to be learned from conventional word alignment models; and we also provide a theoretical explanation why the attention leads to the worse alignment accuracy than the conventional word alignment models, standing upon the point view of reordering. 6 Conclusion It has been shown that attention mechanism in NMT is worse than conventiona"
C16-1291,P14-1129,0,0.0162995,"e original paper, αt is not explicitly dependent on the yt−1 in Eq.(2), but this dependency was explicitly retained in our direct baseline NMT2. 5 Although the alignment is loosely related to the downstream translation (Liu and Sun, 2015), substantial improvements in alignment usually leads to the improvements in translation as observed in our experiments. 3095 Therefore, we apply the following heuristics to preprocess the hard alignment: if a target word does not align to any source words, we inherit its affiliation from the closest aligned word with preference given to the right, following (Devlin et al., 2014); if a target word is aligned to multiple source words, we assume it aligns to each one evenly. In addition, in the implementation of NMT, there are two special tokens ‘eol’ added to both source and target sentences. We assume they are aligned to each other. In this way, we can obtain the final supervision of attention, denoted as α ˆ. 3.2 Jointly Supervising Translation and Attention We propose a soft constraint method to jointly supervise the translation and attention as follows: − X log p(yi |xi ; θ) + λ × ∆(αi , α ˆ i ; θ) (4) i where αi is as defined in Eq. (1), ∆ is a loss function that"
C16-1291,N13-1073,0,0.108639,"allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more details). In contrast, conventional alignment models are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervise"
C16-1291,P07-2045,0,0.0608892,"the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons Attribution 4.0 Inter"
C16-1291,N06-1014,0,0.0172418,"because the supervision of α is more close to Ex than y as in Figure 1(b). In order to quantify the disagreement between αi and α ˆ i , three different methods are investigated in our experiments: • Mean Squared Error (MSE) ∆(αi , α ˆ i ; θ) = XX 1 m 2 n i α(θ)im,n − α ˆ m,n 2 MSE is widely used as a loss for regression tasks (Lehmann and Casella, 1998), and it directly i encourages α(θ)im,n to be equal to α ˆ m,n . • Multiplication (MUL) ∆(αi , α ˆ i ; θ) = − log XX m n i α(θ)im,n × α ˆ m,n  MUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006; Cheng et al., 2016). Note that different from those in (Cheng et al., 2016), α ˆ is not a parametrized variable but a constant in this paper. • Cross Entropy (CE) ∆(αi , α ˆ i ; θ) = − XX m n i α ˆ m,n × log α(θ)im,n Since for each t, α(θ)t is a distribution, it is natural to use CE as the metric to evaluate the disagreement (Rubinstein and Kroese, 2004). 4 Experiments We conducted experiments on two Chinese-to-English translation tasks: one is the NIST task oriented to NEWS domain, which is a large scale task and suitable to NMT; and the other is the speech translation oriented to travel do"
C16-1291,2015.iwslt-evaluation.11,0,0.28045,"1. The first row shows the alignments of the sentence pair from the training set while the second row shows the alignments from test sets. Methods GIZA++ NMT2 SA-NMT AER 30.6∗ 50.6 43.3∗ Table 4: Results on word alignment task for the large scale data. The evaluation metric is Alignment Error Rate (AER). ‘*’ denotes that the corresponding result is significanly better than NMT2 with p &lt; 0.01. Table 4 shows the overall alignment results on word alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in (Liu and Sun, 2015) as the test set. Following (Luong and Manning, 2015), we force-decode both the bilingual sentences including source and reference sentences to obtain the alignment matrices, and then for each target word we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment. From Table 4, we can see clearly that standard NMT (NMT2) is far behind GIZA++ in alignment quality. This shows that it is possible and promising to supervise the attention with GIZA++. With the help from GIZA++, our supervised attention based NMT (SA-NMT) significantly reduces the AER, compared with the unsupervised count"
C16-1291,D15-1166,0,0.100348,"ver the standard attention based NMT. 1 Introduction Neural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015; Sutskever et al., 2015). Generally, it relies on a recurrent neural network under the Encode-Decode framework: it firstly encodes a source sentence into context vectors and then generates its translation token-by-token, selecting from the target vocabulary. Among different variants of NMT, attention based NMT, which is the focus of this paper,1 is attracting increasing interests in the community (Bahdanau et al., 2015; Luong et al., 2015). One of its advantages is that it is able to dynamically make use of the encoded context through an attention mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more"
C16-1291,D16-1249,0,0.218565,"network based word-level reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs. Supervising the attention variables for attention-based neural networks is pioneered by Liu et al. 3100 (2016). On image caption task, Liu et al. (2016) supervise the attention with external guidances in either a strong or a weak supervision manner. Their method requires the training data to be associated with direct annotation or indirect annotation. In parallel to our work, particularly on machine translation, Mi et al. (2016) and Chen et al. (2016) guide the attention for NMT from conventional word alignment models as teachers without any annotation on machine translation task. The differences of our work lie in that: we consider the attention as a form of a reordering model, which is thereby straightforward to be learned from conventional word alignment models; and we also provide a theoretical explanation why the attention leads to the worse alignment accuracy than the conventional word alignment models, standing upon the point view of reordering. 6 Conclusion It has been shown that attention mechanism in NMT is"
C16-1291,W15-5003,0,0.0260557,"Moses is narrowed substantially. While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn"
C16-1291,P00-1056,0,0.102849,"mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more details). In contrast, conventional alignment models are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering mo"
C16-1291,P14-1138,1,0.532331,"). This shows that the proposed approach is able to realize our intuition: the alignment is improved, leading to better translation performance. Note that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge λ in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014). 3099 Systems Moses NMT1 NMT2 SA-NMT CSTAR03 44.1 33.4 36.5 39.8∗ IWSLT04 45.1 33.0 35.9 40.7∗ Table 5: BLEU comparison for low-resource translation task. CSTAR03 is the development set while IWSLT04 is the test set. ‘*’ denotes that SA-NMT is significantly better than both NMT1 and NMT2 with p &lt; 0.01. 4.2 Results on the Low Resource Translation Task For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out set"
C16-1291,P16-1008,0,0.0286294,"over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itse"
C16-1291,P06-1066,0,0.0297014,"p in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons A"
C16-1291,P13-1017,0,0.0104401,"d counterpart (NMT2). This shows that the proposed approach is able to realize our intuition: the alignment is improved, leading to better translation performance. Note that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge λ in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014). 3099 Systems Moses NMT1 NMT2 SA-NMT CSTAR03 44.1 33.4 36.5 39.8∗ IWSLT04 45.1 33.0 35.9 40.7∗ Table 5: BLEU comparison for low-resource translation task. CSTAR03 is the development set while IWSLT04 is the test set. ‘*’ denotes that SA-NMT is significantly better than both NMT1 and NMT2 with p &lt; 0.01. 4.2 Results on the Low Resource Translation Task For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 an"
C16-1291,C14-1179,0,\N,Missing
C16-2007,2014.iwslt-papers.8,1,0.731983,"s can be used to monitor the running of the system: the speech recognition log and the machine translation log (Figure 2). The speech recognition log shows the recognized words from the speakers. The machine translation log shows the recognized sentences and their translations. The content of both logs is updated in realtime. 4 Performance The performance of our method was measured in (Wang et al., 2016a). Experiments were performed on translation between Japanese and English in both directions. The time efficiency was measured by average latency per source word using the definition given in (Finch et al., 2014). The translation quality was measured by the BLEU of end-to-end translation. Because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The results of the measurement are presented in table 1. Different sentence segmentation methods were compared. Our system adopted the threshold-latency method which generally outperformed the other methods on both time efficiency and translation quality. 5 Example Analysis Here is an example of interpreting a TED ta"
C16-2007,2005.iwslt-1.19,0,0.0456124,"The content of both logs is updated in realtime. 4 Performance The performance of our method was measured in (Wang et al., 2016a). Experiments were performed on translation between Japanese and English in both directions. The time efficiency was measured by average latency per source word using the definition given in (Finch et al., 2014). The translation quality was measured by the BLEU of end-to-end translation. Because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The results of the measurement are presented in table 1. Different sentence segmentation methods were compared. Our system adopted the threshold-latency method which generally outperformed the other methods on both time efficiency and translation quality. 5 Example Analysis Here is an example of interpreting a TED talk from English to Japanese by the system. The talk is ”Your elusive creative genius “ given by Elizabeth Gilbert in 20097 . The oracle transcript is, I am a writer. Writing books is my profession but it’s more than that, of course. It is also my great lifelong love and fascinati"
C16-2007,2006.iwslt-papers.1,0,0.0264925,"mage is caused. The Online Sentence Segmenter converts the stream of words into sentences. The implementation is based on the method proposed in (Wang et al., 2016a). The implementation uses a linear combination of a language model, a length model and a prosodic model to calculate the confidence of segmentation boundaries, and uses a threshold-latency-based heuristic to make decisions. The Punctuation Predictor converts an un-punctuated sentence into a punctuated sentence. The implementation is based on the findings in (Wang et al., 2016b). It uses a hidden N-gram model (Stolcke et al., 1998; Matusov et al., 2006), which is available in the toolkit of SRILM (Stolcke, 2002), to insert punctuation. The Machine Translation Engine translates a source-language sentence into a target-language sentence. The implementation is our in-house pre-ordering translation system, called the General Purpose Machine Translation (GPMT) engine. The system is publicly accessible through a Web API 5 The Speech Synthesizer converts sentences into speech. The implementation is based on the HTS open-source toolkit (Tokuda et al., 2013)6 4 https://github.com/kaldi-asr/kaldi https://mt-auto-minhon-mlt.ucri.jgn-x.jp/ 6 http://hts."
C16-2007,N16-3017,0,0.0615103,"Missing"
C16-2007,W16-4613,1,0.864082,"Missing"
C98-1020,P97-1048,0,0.0352884,"Missing"
C98-1020,H92-1020,0,0.0223503,"us of around a million words. The subsequent experiments investigate the 1See Section 2. additional information gains accruing from triggerpair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger-pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et ah, 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et ah, 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information-theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes My) of a random variable Y be observed and used to predict a random variable X (with alp"
C98-1020,H93-1016,0,0.0218224,"Missing"
C98-1020,H94-1052,0,0.0537282,"Missing"
C98-1020,H89-1054,0,0.0385685,"n words. The subsequent experiments investigate the 1See Section 2. additional information gains accruing from triggerpair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger-pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et ah, 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et ah, 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information-theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes My) of a random variable Y be observed and used to predict a random variable X (with alphabet A x ) ."
C98-1020,P95-1037,0,0.0758488,"Missing"
C98-1020,W97-0301,0,0.0234852,"Missing"
C98-1020,C96-1020,1,0.670292,"Missing"
C98-1020,P96-1025,0,\N,Missing
C98-1020,W97-0105,1,\N,Missing
C98-1104,J92-4003,0,0.026876,"Missing"
C98-1104,A92-1018,0,0.0611342,"Missing"
C98-1104,A88-1019,0,0.0086381,"al systems to tackle this problem, uncontrolled heuristics are primarily used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on .Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate th"
C98-1104,C94-1032,0,0.148178,"ly used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on .Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters. Unfortunat"
C98-1104,W96-0113,0,0.489536,"use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on .Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters. Unfortunately, such a task"
D09-1117,W05-0909,0,0.0346751,"sented in this paper are given in terms of the BLEU score (Papineni et al., 2001). This metric measures the geometric mean of ngram precision of n-grams drawn from the output translation and a set of reference translations for that translation. There are large number of proposed methods for carrying out machine translation evaluation. Methods differ in their focus of characteristics of the translation (for example fluency or adequacy), and moreover anomolous results can occur if a single metric is relied on. Therefore, we also carried out evaluations using the NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), WER (Hunt, 1989), PER (Tillmann et al., 1997) and TER (Snover et al., 2005) machine translation evaluation techniques. 4 Results The results of the experiments in terms of the BLEU score are given in Tables ??, 5, 3 and 3. These results show the performance of the reverse and bidirectional decoding strategies relative to the usual forward decoding strategy. The cells in the tables that represent experiments in which 1128 ar da de en es fr id it ja ko ms nl pt ru th vi zh ar 58.3 53.8 63.6 57.6 57.8 54.7 54.1 38.2 34.4 54.5 55.1 56.8 51.4 53.8 53.6 32.0 da 47.8 55.5 65.8 58.2 58.3 52.8 53.4 3"
D09-1117,C02-1050,1,0.614722,"case that this is most effective strategy for all language pairs. In this paper we investigate the effect of direction in phrase-based SMT decoding. For the purposes of this paper, we will refer to target word sequence generation that follows the same order as human language production as forward generation, and generation in the opposite direction to human language production as reverse generation. These are often referred ”left-toright” and ”right-to-left” respectively in the literature, but we avoid this notation as many languages are naturally written from right-to-left. In earlier work (Watanabe and Sumita, 2002), it was hypothesized that the optimal direction for decoding was dependent on the characteristics of the target language. Their results show that for Japanese to English translation a reverse decoding strategy was the most effective, whereas for English to Japanese translation, a forward decoding strategy proved superior. In addition they implemented a bidirectional decoder, but their results were mixed. For English to Japanese translation, decoding bidirectionally gives higher performance, but for Japanese to English translation they were unable to improve performance by decoding bidirection"
D09-1117,P07-2045,0,0.0722139,"e effect of direction in decoding is likely to be strongly language dependent. The next section briefly describes the mechanisms underlying phrase-based decoding. Then we explain the principles behind the forward, reverse and bidirectional decoding strategies used in our experiments. Section 3 presents the experiments we performed. Section 4 gives the results and some analysis. Finally in Section 5, we conclude and offer possible directions for future research. 2 Phrase-based Translation For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al., 2007), integrating our models within a log-linear framework (Och and Ney, 2002). One of the advantages of a log-linear model is that it is possible to integrate a diverse set of features into the model. For the decoders used in the experiments in this paper, we included the following feature functions: • An n-gram language model over the target word sequence - Ensures the target word sequence is a likely sequence of words in the target language • A phrase translation model - Effects the segmentation of the source word sequence, and is also responsible for the transformation of source phrases into t"
D09-1117,koen-2004-pharaoh,0,0.101661,"y suggests that the effect of direction in decoding is likely to be strongly language dependent. The next section briefly describes the mechanisms underlying phrase-based decoding. Then we explain the principles behind the forward, reverse and bidirectional decoding strategies used in our experiments. Section 3 presents the experiments we performed. Section 4 gives the results and some analysis. Finally in Section 5, we conclude and offer possible directions for future research. 2 Phrase-based Translation For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al., 2007), integrating our models within a log-linear framework (Och and Ney, 2002). One of the advantages of a log-linear model is that it is possible to integrate a diverse set of features into the model. For the decoders used in the experiments in this paper, we included the following feature functions: • An n-gram language model over the target word sequence - Ensures the target word sequence is a likely sequence of words in the target language • A phrase translation model - Effects the segmentation of the source word sequence, and is also responsible for the transformation"
D09-1117,P02-1038,0,0.157398,"nt. The next section briefly describes the mechanisms underlying phrase-based decoding. Then we explain the principles behind the forward, reverse and bidirectional decoding strategies used in our experiments. Section 3 presents the experiments we performed. Section 4 gives the results and some analysis. Finally in Section 5, we conclude and offer possible directions for future research. 2 Phrase-based Translation For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al., 2007), integrating our models within a log-linear framework (Och and Ney, 2002). One of the advantages of a log-linear model is that it is possible to integrate a diverse set of features into the model. For the decoders used in the experiments in this paper, we included the following feature functions: • An n-gram language model over the target word sequence - Ensures the target word sequence is a likely sequence of words in the target language • A phrase translation model - Effects the segmentation of the source word sequence, and is also responsible for the transformation of source phrases into target phrases. • A target word sequence length model - Controls the length"
D09-1117,P03-1021,0,0.029344,"corpus for all experiments, and the corpus used for evaluation consisted of 5000 sentences with a single reference for each sentence. 3.2 Training Each instance of the decoder is a standard phrasebased machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al., 2007) SMT decoders. In these experiments 5-gram language models built with Witten-Bell smoothing were used along with a lexicalized distortion model. The system was trained in a standard manner, using a minimum error-rate training (MERT) procedure (Och, 2003) with respect to the BLEU score (Papineni et al., 2001) on held-out development data to optimize the loglinear model weights. For simplicity, the MERT procedure was performed on independently on the forward and reverse decoders for the bidirectional system, rather them attempting to tune the parameters for the full system. 3.3 3.3.1 Translation Engines Forward The forward decoding translation systems used in these experiments represent the baseline of our experiments. They consist of phrase-based, multistack, beam search decoders commonly used in the field. 3.3.2 Reverse The reverse decoding t"
D09-1117,2001.mtsummit-papers.68,0,0.246121,"used for evaluation consisted of 5000 sentences with a single reference for each sentence. 3.2 Training Each instance of the decoder is a standard phrasebased machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al., 2007) SMT decoders. In these experiments 5-gram language models built with Witten-Bell smoothing were used along with a lexicalized distortion model. The system was trained in a standard manner, using a minimum error-rate training (MERT) procedure (Och, 2003) with respect to the BLEU score (Papineni et al., 2001) on held-out development data to optimize the loglinear model weights. For simplicity, the MERT procedure was performed on independently on the forward and reverse decoders for the bidirectional system, rather them attempting to tune the parameters for the full system. 3.3 3.3.1 Translation Engines Forward The forward decoding translation systems used in these experiments represent the baseline of our experiments. They consist of phrase-based, multistack, beam search decoders commonly used in the field. 3.3.2 Reverse The reverse decoding translation systems used in these experiments were exact"
D09-1117,J93-2003,0,\N,Missing
D09-1117,P02-1040,0,\N,Missing
D09-1117,2006.iwslt-evaluation.1,0,\N,Missing
D09-1117,2002.tmi-tutorials.2,0,\N,Missing
D14-1173,li-etal-2010-enriching,0,0.0242748,"rst briefly describes the GALE WA corpus, then presents an analysis of the WS arising from a CTB-standard word segmenter with reference to the segmentation of the atomic blocks in the GALE WA corpus, finally the impact of the findings on SMT is discussed. 2.1 GALE WA corpus The GALE WA corpus was developed by the LDC, and was used as training data in the DARPA GALE global autonomous language exploitation program 5 . The corpus incorporates linguistic knowledge into word aligned text to help improve automatic WA and translation quality. It employs two annotation schemes: alignment and tagging (Li et al., 2010). Alignment identifies minimum translation units and translation relations; tagging adds contextual, syntactic and languagespecific features to the alignment annotation. For example, the sample shown in Figure 1 carries tags on both alignment edges and tokens. The GALE WA corpus contains 18,057 manually word aligned Chinese and English parallel sentences which are extracted from newswire and web blogs. Table 1 presents the statistics on the corpus. One third of the sentences are approximately newswire text, and the remainder consists of web blogs. 2.2 Analysis of WS In order to produce a Chine"
D14-1173,W08-0336,0,0.0775486,"s. Automated word segmenters built through supervised-learning methods, after decades of intensive research, have emerged as effective solutions to WS tasks and become widely used in many NLP applications. For example, the Stanford word segmenter (Xue et al., 2002)1 which is based on conditional random field (CRF) is employed to prepare the official corpus for NTCIR9 Chinese-English patent translation task (Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently"
D14-1173,D09-1075,0,0.0804234,"the boundaries in the CTB WS scheme. The extended features consists of four types – named entities, word frequency, word length and character-level unsupervised WA. For each type of the feature, the value and value concatenated with previous or current character are taken as sparse features (see Table 4 for details). The real values of word frequency, word length and characterlevel unsupervised WA are converted into sparse features due to the routine of CRF model. The character-level unsupervised alignment feature is inspired by the related works of unsupervised bilingual WS (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The idea is that the character-level WA can approximately capture the counterpart English expression of each Chinese token, and source tokens aligned to different target expressions should be split into different words (see Figure 4 for an illustration). The values of the character-level alignment features are obtained through building a dictionary. First, unsupervised WA is performed on the SMT training corpus where the Chinese sentences are treated as sequences of characters; then, the Chinese sentences are segmented by CTB segmenter and a dictio"
D14-1173,P07-1003,0,0.0245675,"re often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Conference on Empirical Methods"
D14-1173,P05-1045,0,0.00487485,"es were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from λ0 = 0 to λ0 = 32. The grid search showed that λ0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10 , a state-of-the-art phrase"
D14-1173,P09-1104,0,0.0131808,"T. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro"
D14-1173,P07-2045,0,0.00855791,"ord named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from λ0 = 0 to λ0 = 32. The grid search showed that λ0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10 , a state-of-the-art phrase-based SMT system, was employed to perform end-to-end SMT experiments. GIZA++ was employed to perform unsupervised WA. 4.2 Experimental Results 4.2.1 Word Segmentation The WS performance of CTB segmenter, length tuner and the proposed lexical splitter are presented in Table 6. The proposed method achieves the highest scores on all the criterion of F1 , precision and recall. The length tuner outperforms the CTB segmenter in terms of recall, but with lower precision. 8 http://nlp.stanford.edu/software/ CRF-NER.shtml 9 http://www.statmt.org/moses/giza/ GIZA++.html 10"
D14-1173,N06-1014,0,0.651297,"t the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Confere"
D14-1173,E09-1063,0,0.0167738,"There is large volume of research using bilingual unsupervised and semi-supervised WS to address the problem of optimizing WS for SMT (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The main difference with our approach is that they use automatic WA results, most often obtained using the same tools as are used in training SMT systems. One of the main problems of using unsupervised WA is that it is noisy, and therefore, employing iterative optimization methods to refine the results of unsupervised WA is a key issue in their research, for example boosting (Ma and Way, 2009; Michael et al., 2011), expectation maximization (Chung and Gildea, 2009), Bayesian sampling (Xu et al., 2008; Nguyen et al., 2010), or heuristic search (Zhao et al., 2013). Nevertheless, noisy WA makes both analyzing WS and improving SMT quality quite hard. In contrast, by using manual WA, we can clearly analyze the segmentation problems (Section 2), and train supervised models to solve the problem (Section 3). As far as we are aware, among related work on WS, our method achieves the highest BLEU improvement relative to the start-of-the-art WS – the Stanford Chinese word segmenter – on the C"
D14-1173,W03-0301,0,0.0319011,"orpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012"
D14-1173,C10-1092,0,0.107692,"B WS scheme. The extended features consists of four types – named entities, word frequency, word length and character-level unsupervised WA. For each type of the feature, the value and value concatenated with previous or current character are taken as sparse features (see Table 4 for details). The real values of word frequency, word length and characterlevel unsupervised WA are converted into sparse features due to the routine of CRF model. The character-level unsupervised alignment feature is inspired by the related works of unsupervised bilingual WS (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The idea is that the character-level WA can approximately capture the counterpart English expression of each Chinese token, and source tokens aligned to different target expressions should be split into different words (see Figure 4 for an illustration). The values of the character-level alignment features are obtained through building a dictionary. First, unsupervised WA is performed on the SMT training corpus where the Chinese sentences are treated as sequences of characters; then, the Chinese sentences are segmented by CTB segmenter and a dictionary of segmented wor"
D14-1173,C00-2163,0,0.802782,"Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,"
D14-1173,J03-1002,0,0.00860593,"posed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from λ0 = 0 to λ0 = 32. The grid search showed that λ0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10 , a state-of-the-art phrase-based SMT system, was employed to perform end-to-end SMT experiments. GIZA++ was employed to perform unsupervised WA. 4.2 Experimental Results 4.2.1 Word Segmentation The WS performance of CTB segmenter, length tuner a"
D14-1173,P03-1021,0,0.039167,"nce samples which contain one Chinese sentence and four English reference sentences. The experimental corpus for unsupervised WA was the union set of the NIST OpenMT training set and the 2000 test sentence pairs from GALE WA corpus. We removed the United Nations corpus from the NIST OpenMT constraint training resources because it is out of domain. The main result of this paper is the evaluation of the end-to-end performance of an SMT system. The experimental corpus for this task was the NIST OpenMT corpus. The data set of the NIST evaluation 2002 was used as a development set for MERT tuning (Och, 2003), and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The"
D14-1173,P02-1040,0,0.0990795,"aluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The characte"
D14-1173,C04-1081,0,0.0484789,"roach This paper employs a condition random field (CRF) to solve this sequence labeling task (Lafferty et al., 2001). A linear-chain CRF defines the conditional probability of y given x as, T 1 XX λk fk (yt−1 , yt , x, t)), ( Zx t=1 k (2) where Λ = {λ1 , . . .} are parameters, Zx is a perinput normalization that makes the probability of all state sequences sum to one; fk (yt−1 , yt , x, t) is a feature function which is often a binary-valued sparse feature. The training of CRF model is to maximize the likelihood of training data together with a regularization penalty to avoid over-fitting as (Peng et al., 2004; Peng and McCallum, 2006), PΛ (y|x) = X X λ2 k Λ∗ = argmax( logPΛ (yi |xi ) − 2 ), 2δ Λ k i k (3) where (x,y) are training samples; the hyperparameter δk can be understood as the variance of the prior distribution of λk . When predicting the labels of test samples, the CRF decoder searches for the optimal label sequence y ∗ that maximizes the conditional probability, y∗ = argmax PΛ (y|x). y (4) In (Chang et al., 2008) a method is proposed to select an appropriate level of segmentation granularity (in practical terms, to encourage smaller segments). We call their method “length tuner”. The fol"
D14-1173,W03-1719,0,0.02299,"nMT constraint training resources because it is out of domain. The main result of this paper is the evaluation of the end-to-end performance of an SMT system. The experimental corpus for this task was the NIST OpenMT corpus. The data set of the NIST evaluation 2002 was used as a development set for MERT tuning (Och, 2003), and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the"
D14-1173,I05-3027,0,0.0191438,"d by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid sear"
D14-1173,C08-1128,0,0.0713041,"Missing"
D14-1173,C02-1145,0,0.0532368,"on those languages that have no explicit space between words, such as Arabic, Chinese and Japanese. As the first processing step, WS affects all successive steps, thus it has a large potential impact on the final performance. For SMT, the unsupervised WA, building translation models and reordering models, and decoding are all based on segmented words. Automated word segmenters built through supervised-learning methods, after decades of intensive research, have emerged as effective solutions to WS tasks and become widely used in many NLP applications. For example, the Stanford word segmenter (Xue et al., 2002)1 which is based on conditional random field (CRF) is employed to prepare the official corpus for NTCIR9 Chinese-English patent translation task (Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually"
D14-1173,2002.tmi-tutorials.2,0,0.0201863,"ET (determiner) are tags of tokens. Genre Newswire Web blog Total # Files 2,175 2,560 4,735 # Sentences† 6,218 11,839 18,057 # CN tokens 246,371 373,818 620,189 # EN tokens 205,281 312,856 518,137 # Alignment edges 164,033 257,730 421,763 Table 1: GALE WA corpus. † Sentences rejected by the annotators are excluded. four atomic blocks; the CTB segmenter produces five words which all locate within the blocks, so they are all small enough. • Alignment inconsistent: the word aligns to more than one atomic block, but the target expression is contiguous, allowing for correct phrase pair extraction (Zens et al., 2002). For example, in Figure 2(b), the characters in the word “shuang fang”, which is produced by the CTB segmenter, contains two atomic blocks, but the span of the target “to both side” is continuous, therefore the phrase pair • Alignment inconsistent and extraction hindered: the word aligned to more than one atomic block, and the target expression is not contiguous, which hinders correct phrase pair extractions. For example, in Figure 2(c), the word “zeng chan” has to be split in order to match the target language. Table 2 shows the statistics of the three categories of CTB WS on the GALE WA cor"
D14-1173,W03-1726,0,\N,Missing
D15-1128,P07-2045,0,0.0216808,"lish-Spanish) and (English-French), and also language pairs that required a greater amount of word re-ordering for 1 http://www.ted.com example (English-Chinese). The Chinese corpus was segmented using the Stanford Chinese word segmenter (Tseng et al., 2005) according to the Chinese Penn Treebank standard. 3.2 Experimental Methodology Our stream decoder was implemented within the framework of the AUGUSTUS decoder, a hierarchical statistical machine translation decoder (Chiang, 2007) that operates in a similar manner to the moses-chart decoder provided in the Moses machine translation toolkit (Koehn et al., 2007). The training procedure was quite typical: 5-gram language models were used, trained with modified 1092 English input stream: ... we want to encourage a world of creators of inventors of contributors because this world that we live in this interactive world is ours ... Sequence of translated segments: Segment 1: Segment 2: Segment 3: Segment 4: Segment 5: Segment 6: Segment 7: Segment 8: queremos animar a un mundo de creadores de inventores de colaboradores porque este mundo en el que vivimos este interactiva mundo es la nuestra [we want to] [encourage a world of] [creators of inventors] [of"
D15-1128,2008.iwslt-papers.5,0,0.161611,"sing the number of segmentation boundaries to be inserted, prior to the segmentation process. In (Matusov et al., 2007) it was shown that the prediction and use of soft boundaries in the source language text, when used as re-ordering constraints can improve the quality of a speech translation system. (Siahbani et al., 2014) used a pre-segmenter in combination with a left-to-right hierarchical decoder (Watanabe et al., 2006) to achieve a considerably faster decoder in return for a small cost in terms of BLEU score. A phrase-based incremental decoder called the stream decoder was introduced in (Kolss et al., 2008b), and further studied in (Finch et al., 2014). Their results, conducted on translation between European languages, and also on English-Chinese, showed that this approach was able to maintain a high level of translation quality for practically useful levels of latency. The hierarchical decoding strategy proposed here is based on this work. 2.1 Stream Decoding The reader is referred to the original paper (Kolss et al., 2008a) for a complete description of the stream decoding process; in this section we provide a brief summary. Figure 1 depicts a stream decoding process, and the figure applies"
D15-1128,N13-1023,0,0.132338,"segmentation. This approach has the advantage that it can be implemented without the need to modify the machine translation decoding software. In the second type of strategy, which we will call incremental decoding, the segmentation process is performed during the decoding of the input stream. In this approach the segmentation process is able to exploit segmentation cues arising from the decoding process itself. That is to say, the order in which the decoder would prefer to generate the target sequence is taken into account. A number of diverse strategies for presegmentation were studied in (Sridhar et al., 2013). They studied both non-linguistic techniques, that included fixed-length segments, and a “hold-output” method which identifies contiguous blocks of text that do not contain alignments to words outside them, and linguistically-motivated segmentation techniques beased on segmenting on 1089 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1089–1094, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. conjunctions, sentence boundaries and commas. Commas were the most effective segmentation cue in their investigatio"
D15-1128,I05-3027,0,0.0491287,"Missing"
D15-1128,P06-1098,0,0.456205,"rching for segmentation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmentation could be controlled by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. In (Matusov et al., 2007) it was shown that the prediction and use of soft boundaries in the source language text, when used as re-ordering constraints can improve the quality of a speech translation system. (Siahbani et al., 2014) used a pre-segmenter in combination with a left-to-right hierarchical decoder (Watanabe et al., 2006) to achieve a considerably faster decoder in return for a small cost in terms of BLEU score. A phrase-based incremental decoder called the stream decoder was introduced in (Kolss et al., 2008b), and further studied in (Finch et al., 2014). Their results, conducted on translation between European languages, and also on English-Chinese, showed that this approach was able to maintain a high level of translation quality for practically useful levels of latency. The hierarchical decoding strategy proposed here is based on this work. 2.1 Stream Decoding The reader is referred to the original paper ("
D15-1128,P03-1021,0,0.146861,"Missing"
D15-1128,P14-2090,0,0.161343,"studied both non-linguistic techniques, that included fixed-length segments, and a “hold-output” method which identifies contiguous blocks of text that do not contain alignments to words outside them, and linguistically-motivated segmentation techniques beased on segmenting on 1089 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1089–1094, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. conjunctions, sentence boundaries and commas. Commas were the most effective segmentation cue in their investigation. In (Oda et al., 2014) a strategy for segmentation prior to decoding based on searching for segmentation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmentation could be controlled by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. In (Matusov et al., 2007) it was shown that the prediction and use of soft boundaries in the source language text, when used as re-ordering constraints can improve the quality of a speech translation system. (Siahbani et al., 2014) used a pre-segmenter"
D15-1128,2001.mtsummit-papers.68,0,0.0256782,"radores porque este mundo en el que vivimos este interactiva mundo es la nuestra [we want to] [encourage a world of] [creators of inventors] [of collaborators] [because this world] [in which we live] [this interactive world] [is ours] Figure 4: Example translation segmentation from the English-Spanish task (Lmax = 8 and Lmin = 4). Kneser-Ney smoothing; MERT (Och, 2003) was used to train the log-linear weights of the models; the decoding was performed with a distortion limit of 20 words. To allow the results to be directly comparable to those in (Finch et al., 2014), the talk level BLEU score (Papineni et al., 2001) was used to evaluate the machine translation quality in all experiments. 3.3 Results The results for decoding with various values of the latency parameters are shown in Figure 3 for English-French, English-Spanish, English-Arabic, English-Hebrew, English-Russian and EnglishChinese. Overall the behavior of the system was quite similar in character to the published results for phrase-based stream decoding for EnglishSpanish (Kolss et al., 2008b; Finch et al., 2014). The hierarchical system seemed to be more sensitive to small values of minimum latency, and less sensitive to larger values. The r"
D15-1128,P02-1040,0,\N,Missing
D15-1128,J07-2003,0,\N,Missing
D15-1128,D08-1076,0,\N,Missing
D15-1209,W13-2201,0,0.0199167,"ing the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted s, from the GALE ChineseEnglish Word Alignment and Tagging Training corpus (GALE WA corpus)1 with it’s humanannotated word alignment. The Chinese word “HE ZHANG,” denoted wr , which means river custodian, only occurs once in the whole corpus. We performe"
D15-1209,H93-1039,0,0.474031,"Missing"
D15-1209,J93-2003,0,0.167506,"and BLEU scores of end-to-end translation were raised by 0.03 – 1.30. The proposed method also outperformed l0 -normalized GIZA++ and Kneser-Ney smoothed GIZA++. 1 Introduction Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects"
D15-1209,2013.iwslt-evaluation.1,0,0.0234888,"Missing"
D15-1209,P07-1003,0,0.0292356,"essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted"
D15-1209,D08-1033,0,0.0481387,"Missing"
D15-1209,N13-1073,0,0.0288713,"st statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted s, from the GALE ChineseEnglish"
D15-1209,P08-1112,0,0.0529177,"Missing"
D15-1209,P07-2045,0,0.00687099,"nd converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Wat"
D15-1209,2005.mtsummit-papers.11,0,0.0582682,"GALE WA corpus and the OpenMT corpus. They are from the same domain, both contain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings"
D15-1209,N06-1014,0,0.168812,"e pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a rea"
D15-1209,P11-1064,1,0.897734,"Missing"
D15-1209,C00-2163,0,0.833427,"nd-to-end translation were raised by 0.03 – 1.30. The proposed method also outperformed l0 -normalized GIZA++ and Kneser-Ney smoothed GIZA++. 1 Introduction Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a"
D15-1209,J03-1002,0,0.26461,"d word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Gra"
D15-1209,P03-1021,0,0.015174,"d was tested on two language pairs: Chinese-English and JapaneseEnglish (Table 2). Performance was measured both directly using the agreement with reference to manual WA annotations, and indirectly using the BLEU score in end-to-end machine translation tasks. GIZA++ and our own implementation of standard EM were used as baselines. 4.1 Experimental Settings The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT corpus. They are from the same domain, both contain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server i"
D15-1209,C02-1145,0,0.0241296,"nMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We imp"
D15-1209,P14-1072,0,0.0607884,"ing phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al., 2012). Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach. Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., 3 The probability distribution of generating target language words from wr . The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)"
D15-1209,P13-1083,1,0.86032,"el 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F1 (Och and Ney, 2003). T"
D15-1209,J10-3007,0,0.035402,"Missing"
D15-1209,P12-1033,0,0.0623327,"r et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al., 2012). Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach. Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., 3 The probability distribution of generating target language words from wr . The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)                (b)"
D15-1209,P14-2122,1,0.835056,"ly based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)                (b)                    (c) Figure 1: Examples of supervised word alignment. (a) gold alignment; (b) standard EM (GIZA++); (c) Leave-one-out alignment (proposed). 2011), also attempt to address the issue of overfitting, however EM algorithms related to the proposed method have been shown to be more efficient (Wang et al., 2014). 3 Methodology This section first formulates the standard EM for WA, then presents the leave-one-out EM for WA, and finally briefly discusses handling singletons and effecient implementation. The main notation used in this section is shown in Table 1. 3.1 Standard EM for IBM Models 1, 2 and HMM Model To perform WA through EM, the parallel corpus is taken as observed data, the alignments are taken as latent data. In order to maximize the likelihood of the alignment model θ given the data S, the following two steps are conducted iteratively (Brown et al., 1993b; Och and Ney, 2000; Och and Ney,"
D15-1209,P11-1125,1,0.848372,"settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision"
D15-1209,N12-1026,1,0.860338,"07). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F1"
D15-1209,P10-1049,0,0.0224644,"e propose a leave-one-out EM algorithm for WA in this paper. Recently this technique has been applied to avoid over-fitting in kernel density estimation (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed. Figure 1(c) shows the effect of using our technique on the example. The garbage collection has not occurred, and the alignment of the word “HE ZHANG” is identical to the human annotation. 2 Related Work The most related work to this paper is training phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of proba"
D15-1209,W12-3158,0,0.0196466,"out EM algorithm for WA in this paper. Recently this technique has been applied to avoid over-fitting in kernel density estimation (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed. Figure 1(c) shows the effect of using our technique on the example. The garbage collection has not occurred, and the alignment of the word “HE ZHANG” is identical to the human annotation. 2 Related Work The most related work to this paper is training phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al.,"
D19-5601,W18-2716,0,0.0593982,"Missing"
D19-5601,W04-1013,0,0.0851055,"-text NLG and MT along two axes: • MT+NLG: RotoWire, WMT19, Monolingual RotoWire refers to the RotoWire dataset (Wiseman et al., 2017) (train/valid), WMT19 refers to the set of parallel corpora allowable by the WMT 2019 English-German task, and Monolingual refers to monolingual data allowable by the same WMT 2019 task, pre-trained embeddings (e.g., GloVe (Pennington et al., 2014)), pre-trained contextualized embeddings (e.g., BERT (Devlin et al., 2019)), pre-trained language models (e.g., GPT-2 (Radford et al., 2019)). Textual Accuracy Measures: We used BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for texutal accuracy compared to reference summaries. Content Accuracy Measures: We evaluate the fidelity of the generated content to the input data using relation generation (RG), content selection (CS), and content ordering (CO) metrics (Wiseman et al., 2017). 2 model for both languages together, using a shared BPE vocabulary obtained from target game summaries and by prefixing the target text with the target language indicator. For MT and MT+NLG tracks, they mined the in-domain data by extracting basketball-related texts from Newscrawl when one of the following conditions are m"
D19-5601,D14-1162,0,0.0824718,"Missing"
D19-5601,D15-1044,0,0.0588225,"types of inputs. The results of the shared task are summarized in Sections 3 and 4. Introduction 2 Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are now a workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 3rd Workshop on Neural Machine Translation and Generation (WNGT 2019) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization (Rush et al., 2015), NLG from structured data (Wen et al., 2015), dialog response generation (Vinyals and Le, 2015), among others). Overall, the workshop was held with two goals. First, it aimed to synthesize the current state of knowledge in neural machine translation and generation: this year we continued to encourage submissions that not only advance the state of the art through algorithmic advances, but also analyze and understand the current state of the art, pointing to future research directions. Towards this Summary of Research Contributions We published a call for long papers, extended abstracts for pre"
D19-5601,W18-6502,0,0.0159592,"ocument-level Generation and Translation # documents Avg. # tokens (En) Avg. # tokens (De) Vocabulary size (En) Vocabulary size (De) The first shared task at the workshop focused on document-level generation and translation. Many recent attempts at NLG have focused on sentencelevel generation (Lebret et al., 2016; Gardent et al., 2017). However, real world language generation applications tend to involve generation of much larger amount of text such as dialogues or multisentence summaries. The inputs to NLG systems also vary from structured data such as tables (Lebret et al., 2016) or graphs (Wang et al., 2018), to textual data (Nallapati et al., 2016). Because of such difference in data and domain, comparison between different methods has been nontrivial. This task aims to (1) push forward such document-level generation technology by providing a testbed, and (2) examine the differences between generation based on different types of inputs including both structured data and translations in another language. In particular, we provided the following 6 tracks which focus on different input/output requirements: Valid Test 242 323 320 4163 5425 240 328 324 - 241 329 325 - Table 1: Data statistics of Roto"
D19-5601,D13-1176,0,\N,Missing
D19-5601,D15-1199,0,\N,Missing
D19-5601,P02-1040,0,\N,Missing
D19-5601,P10-2041,0,\N,Missing
D19-5601,W14-3302,0,\N,Missing
D19-5601,D17-1239,0,\N,Missing
D19-5601,W14-7001,0,\N,Missing
D19-5601,W17-4717,0,\N,Missing
D19-5601,W17-3518,0,\N,Missing
D19-5601,N19-1423,0,\N,Missing
E03-1048,P02-1040,0,0.0780651,"ere an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the RED rank, and the BLEU score. While RED accords with HUMAN, BLEU fails to agree with HUMAN in the EJ evaluation. One reason for this is that the BLEU score favors SAT translations in that they are more similar to the reference translation from the viewpoint of Ngrams. Table 1 Quality Evaluation of Three MTs 5 (2) BLEU score: The MT translations are scored based on the precision of N-grams in an entire set of multiple reference translations (Papineni et al., 2002). It ranges from 1.0 (best) down to 0.0 (worst). (3) Estimated TOEIC score: It is important to interpret MT performance from the viewpoint of a language proficiency test such as TOEIC 4 . A translator compared MT translations with human ones, then, MT&apos;s proficiency is estimated by regression analysis (Sugaya et al., 2000). It ranges from 10 (lowest) to 990 points (perfect). 3.3 Results Table 1 wraps up the results. So far, SMT has been applied mainly to language pairs of similar European languages. Skeptical opinions dominate about Average is calculated: A, B, C, and D are assigned values of 4"
E03-1048,2001.mtsummit-papers.3,1,0.832851,"e and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks 2 : HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D 3 , surpassed SAT in the HUMAN rank. This is the reverse result obtained in a Verbmobil experiment (Ney, 2001) where an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the"
E03-1048,shimohata-sumita-2002-automatic,1,0.89347,"Missing"
E03-1048,C02-1076,1,0.819515,"ging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the source and target correspondences from the semantic point of view are maintained in a state-of-the-art translation system. However, the second assumption does not necessarily hold. To solve this problem, Akiba et al. (2002) used not only a language model but also a translation model of SMT derived from a corpus, and Sumita et al. (2002) exploited a corpus whose sentences are converted into semantic class sequences. These two selectors outperformed conventional selectors using the target N-gram in our experiments. 5 Paraphrasing and Filtering This section introduces another feature of C3 : paraphrasing and filtering corpora. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for"
E03-1048,1995.tmi-1.17,0,0.00827994,"w the HUMAN rank, as described above. Table 2. Sample of Translation Variety [B] Is the payment cash? Or is it the credit card? [A] Would you like to pay in cash or with a credit card? [C] Could you cash or credit card? In our experiment, while D3 , HPAT, and SAT for the E-to-J direction have A-ratios of 0.62, 0.55, and 0.53, respectively, the ideal selection would have an interestingly high A-ratio of 0.79. Thus, we could obtain a large increase in accuracy if it were possible to select the best one of the three different translations for each input sentence. Unlike other approaches such as (Brown and Frederking, 1995), we do not merge multiple results into a single one but we select the best one because the large difference between multiple translations for distant language pairs such as Japanese and English makes merging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the"
E03-1048,2001.mtsummit-papers.12,0,0.0767102,"Missing"
E03-1048,W02-1611,1,0.895885,"Missing"
E03-1048,suyaga-etal-2002-proposal,0,0.0182927,"or example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3 , to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the coun"
E03-1048,2002.tmi-tutorials.2,0,0.0456358,"corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3 , to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation"
E03-1048,W01-1401,1,0.84948,"stical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phra"
E03-1048,2002.tmi-papers.9,1,0.841258,"slation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phrasebooks for tourists. The size is about 150 thousand sentence pairs. A quality evaluation was done using a test set consisting of 345 sentences selected randomly from the above corpus, and the remaining sentences were used f"
E03-1048,1983.tc-1.13,0,0.350529,"Missing"
E03-1048,E03-1029,1,0.834211,"g. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the count of words in the translation pair. After abandoning the non-literal parts of the corpus, the acquisition of HPAT transfer patterns is done. The effect has been confirmed by an improvement in translation quality. 6 Conclusion Our project, called C3 , places corpora at the center of speech-to-speech technology. Good performance in translation components is demonstrated in the experiment"
E03-1048,1999.mtsummit-1.34,1,0.836105,"ezawa et al., 2002). 171 We used bilingual dictionaries and thesauri of about fifty thousand words for the travel domain. 3.2 Evaluation Measures We used the measures below. The BLEU score and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks 2 : HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D 3 , surpassed SAT in the HUMAN rank. This is the reverse result obtained in a Ver"
E03-1048,W01-1405,0,0.048248,"g a high-quality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3 . Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual tree"
E03-1048,C02-1050,1,0.897796,"Missing"
E03-1048,J90-2002,0,\N,Missing
E03-1048,takezawa-etal-2002-toward,1,\N,Missing
finch-etal-2002-beyond,C96-1020,1,\N,Missing
finch-etal-2002-beyond,J94-2001,0,\N,Missing
finch-etal-2002-beyond,P98-1020,1,\N,Missing
finch-etal-2002-beyond,C98-1020,1,\N,Missing
finch-etal-2004-automatic,niessen-etal-2000-evaluation,0,\N,Missing
finch-etal-2004-automatic,H93-1040,0,\N,Missing
finch-etal-2004-automatic,1993.mtsummit-1.24,0,\N,Missing
finch-etal-2004-automatic,N03-2021,0,\N,Missing
finch-etal-2004-automatic,P02-1040,0,\N,Missing
finch-etal-2004-automatic,takezawa-etal-2002-toward,1,\N,Missing
finch-etal-2004-automatic,suyaga-etal-2002-proposal,0,\N,Missing
finch-etal-2004-automatic,P00-1056,0,\N,Missing
I05-5003,C04-1046,0,0.00665543,"ed, that is MT output might not consist of grammatically correct sentences. Moreover, MT evaluation scoring need not necessarily be computed on a sentence-by-sentence basis, but can be based on statistics derived at the corpus level. Finally, the process of MT evaluation is asymmetrical. That is, there is a distinction between the references and the candidate machine translations. Fortunately, the automatic MT evaluation techniques commonly in use do not make any explicit attempt to score grammaticality, and (except BLEU) decompose naturally into their component scores at the sentence level. (Blatz et al., 2004) used a variant of the WER score and the NIST score at the sentence level to assign correct17 ness to translation candidates, by scoring them with respect to a reference set. These correctness labels were used as the ‘ground truth’ for classifiers for the correctness of translation candidates for candidate sentence confidence estimation. We too adopt sentence level versions of these scores and use them to classify paraphrase candidates. The motivation for these experiments is twofold: firstly to determine how useful the features used by these MT evaluation techniques to semantic equivalence cl"
I05-5003,O97-1002,0,0.043188,"Missing"
I05-5003,2001.mtsummit-papers.68,0,0.0180454,"t operations required to transform one sentence into another, defined as: W ER(si , ri ) = I(si , ri ) + D(si , ri ) + S(si , ri ) |ri | where I(si , ri ), D(si , ri ) and S(si , ri ) are the number of insertions, deletions and substitutions respectively. 2.2 PER Position-independent word error rate (PER) (Tillmann et al., 1997) is similar to WER except that word order is not taken into account, both sentences are treated as bags of words: P ER(si , ri ) = max[dif f (si , ri ), dif f (ri , si )] |ri | where dif f (si , ri ) is the number of words observed only in si . 2.3 BLEU The BLEU score (Papineni et al., 2001) is based on the geometric mean of n-gram precision. The score is given by: BLEU = BP × exp &quot; N X 1 n=1 N × log(pn ) # where N is the maximum n-gram size. The n-gram precision pn is given by: P P count(ngram) i=1..I ngram∈si pn = P P countsys (ngram) i=1..I ngram∈si where count(ngram) is the count of ngram found in both si and ri and countsys (ngram) is the count of ngram in si . The brevity penalty BP penalizes MT output for being shorter than the corresponding references and is given by: &quot; &quot; ## Lref BP = exp min 1 − ,1 Lsys where Lsys is the number of words in the MT output sentences and Lre"
I05-5003,C92-2067,0,0.00985787,"the job” of an MT evaluator. Our second motivation is the conjecture that successful techniques and strategies will be transferable between the two tasks. 2 MT Evaluation Methods MT evaluation schemes score a set of MT system output segments (sentences in our case) S = {s1 , s2 , ..., sI } with respect to a set of references R corresponding to correct translations for their respective segments. Since we classify sentence pairs, we only consider the case of using a single reference for evaluation. Thus the set of references is given by: R = {r1 , r2 , ..., rI }. 2.1 WER Word error rate (WER) (Su et al., 1992) is a measure of the number of edit operations required to transform one sentence into another, defined as: W ER(si , ri ) = I(si , ri ) + D(si , ri ) + S(si , ri ) |ri | where I(si , ri ), D(si , ri ) and S(si , ri ) are the number of insertions, deletions and substitutions respectively. 2.2 PER Position-independent word error rate (PER) (Tillmann et al., 1997) is similar to WER except that word order is not taken into account, both sentences are treated as bags of words: P ER(si , ri ) = max[dif f (si , ri ), dif f (ri , si )] |ri | where dif f (si , ri ) is the number of words observed only"
I05-5003,J93-2004,0,\N,Missing
I05-5003,P02-1040,0,\N,Missing
I08-8003,N03-1017,0,0.0548831,"tion systems have already been developed on these principles (Lepage and Denoual, 2006). Our system also takes a character-based approach but restricts itself to the translation of short phrases. This is to our advantage because machine translation systems struggle in the translation of longer sequences. Moreover, the process of transliteration tends to be a monotone process, and this assists us further. We will give only a brief overview of the process of phrase-based machine translation, for a fuller account of statistical machine translation we refer the reader to (Brown et al., 1991) and (Koehn, Och, and Marcu, 2003). During the process of phrase-based SMT the source sequence is segmented into sub-sequences , each sub-sequence being translated using bilingual sequence pairs (called phrase pairs when the translation proceeds at the word-level). The target generation process (for English-to-Japanese) at the character level is illustrated in Figure 3. The example is a real system output from an unseen phrase. The source sequence is segmented by the system into three segments. The translations of each of these segments have been gleaned from alignments of these segments where they occur in the training corpu"
I08-8003,J93-2003,0,\N,Missing
I08-8003,P97-1017,0,\N,Missing
L16-1249,N03-1017,0,0.0633206,"ASTREC, plans to coordinate the development of the ALT Corpus between 2014 to 2018. As a first step, the corpus is scheduled to cover: Indonesian, Japanese, Khmer, Laos, Malay, Myanmar, Philippine, Thai and Vietnamese languages by the end of this time span. In 2014, the project commenced development for the Japanese and Myanmar langauges. The domain is news and 1888 articles were randomly selected from English Wikinews (Wikinews, 2014). 20,000 sentences for building the corpus. Although preparing a parallel corpus may be sufficient for building a standard statistical phrase-based SMT system (Koehn et al., 2003), we also added manual alignment, POS tagging and constituency trees to facilitate further study on SMT and also for other NLP fundamental research. In order to create the corpus, we implemented a web-based tool. This tool will be used in collaboration with research institutions of several Asian countries. The data was represented in XML format for all development steps. The following is an example of the XML data for English sentence “Visitors at the hotel were evacuated to the exhibition hall at street level.”: &lt;source&gt; &lt;text&gt;&lt;![CDATA[Work began in 1900.]]&gt;&lt;/text&gt; &lt;words&gt; &lt;word&gt;&lt;![CDATA[Work"
L16-1249,petrov-etal-2012-universal,0,0.0113014,"ds to align to. For example, the alignment of “the last” in “the last cars to finish” would be aligned to ေနာကဆုံး and မှ in ေနာကဆုံး မှ ပနးဝငေသာ ကားများ and likewise, the alignment of “to finish” would align to ပနးဝငေသာ. All words on both sides were required to be aligned to words in the other language. Unaligned words (null alignments) were not allowed (see Figure 1). 3.4. POS Tagging For the ALT Project, we did not use existing POS tagsets for Myanmar such as (Phyu Hninn et al., 2011). Our POS tag set was intended to be simple and universal similar to proposal of (Slav et al., 2011) (Petrov et al., 2012). The main difference with the Universal POS Tagset is we added necessary language specific POS tags to a core tagset that will be shared with other languages. Myanmar parts of speech are different from English since the grammatical structure is subject, object, verb. Some English parts of speech like determiners, prepositions and auxiliary verbs are not used in Myanmar and some Myanmar parts of speech like post positional markers are not used in English. Although the Myanmar Thadda (a book on Myanmar grammar) 1575 (Commission, 2005) defined 10 parts of speech (adjectives, adverbs, conjunction"
N16-1046,P15-1033,0,0.0135531,"Missing"
N16-1046,D09-1117,1,0.942985,"Missing"
N16-1046,P15-1001,0,0.0309557,"Missing"
N16-1046,P07-2045,0,0.0575108,"Missing"
N16-1046,N06-1014,0,0.0285511,"Missing"
N16-1046,D14-1209,1,0.815238,"Missing"
N16-1046,P15-1002,0,0.0282201,"Missing"
N16-1046,P00-1056,0,0.0407959,"Missing"
N16-1046,C02-1050,1,0.742069,"Missing"
N16-1046,P15-1113,1,0.580203,"Missing"
N16-1046,N13-1002,0,0.091757,"Missing"
N16-1046,P13-1016,1,\N,Missing
N16-1046,2002.tmi-tutorials.2,0,\N,Missing
N16-1046,P14-1129,0,\N,Missing
N16-1124,P96-1041,0,0.189126,"tatistics are given in Table 2. it nl pt ru tl id ms vi th hi zh ja ko my 4.2 Experimental Methodology BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target interlocking, (2) source interlocking and (3) both source and target interlocking for all possible combinations of languages (i.e. 380 language pairs). We studied two methods for accomplishing (3). In the first, interlocking as defined in Sections 3.1 and 3.2 are permitted freely. In the second, the target is allowed to interlock if and only if the source is also interlocked."
N16-1124,P10-4002,0,0.0183497,"of using interlocking phrases to during the decoding process in phrase-based statistical machine translation (PBSMT). The motivation for this is two-fold. Firstly, during the phrase-pair extraction process that occurs in the training of a typical PBSMT system, all possible alternative phrase-pairs are extracted that are consistent with a set of alignment points. As a consequence, the source and target sides of these extracted phrase pairs may over(Karimova et al., 2014) presented a method to extract overlapping phrases offline for hierarchical phrase based SMT. They used the CDEC SMT decoder (Dyer et al., 2010) that offers several learners for discriminative tuning of weights for the new phrases. Their results showed improvements of 0.3 to 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. (Tribble and et al., 2003) proposed a method to generate longer new phrases by merging existing phraselevel alignments that have overlaping words on both source and target sides. Their experiments on translating Arabic-English text from the news domain were encouraging. 1076 Proceedings of NAACL-HLT 2016, pages 1076–1081, c San Die"
N16-1124,D09-1107,0,0.0656633,"Missing"
N16-1124,2014.iwslt-papers.12,0,0.0271547,"improve the performance of phrase-based decoders. 2 Related Work 1 Introduction In this paper we examine the effect on machine translation quality of using interlocking phrases to during the decoding process in phrase-based statistical machine translation (PBSMT). The motivation for this is two-fold. Firstly, during the phrase-pair extraction process that occurs in the training of a typical PBSMT system, all possible alternative phrase-pairs are extracted that are consistent with a set of alignment points. As a consequence, the source and target sides of these extracted phrase pairs may over(Karimova et al., 2014) presented a method to extract overlapping phrases offline for hierarchical phrase based SMT. They used the CDEC SMT decoder (Dyer et al., 2010) that offers several learners for discriminative tuning of weights for the new phrases. Their results showed improvements of 0.3 to 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. (Tribble and et al., 2003) proposed a method to generate longer new phrases by merging existing phraselevel alignments that have overlaping words on both source and target sides. Their expe"
N16-1124,W09-0429,0,0.0181259,"i (hi), Chinese (zh), Japanese (ja), Korean (ko) and Myanmar (my). 155,121 sentences were used for training, 5,000 sentences for development and 2,000 sentences for evaluation. In addition, we ran experiments on two language pairs from the Europarl corpus (Koehn, 2005). The language pairs were English-German, GermanEnglish, English-Spanish and Spanish-English. The corpus statistics are given in Table 2. it nl pt ru tl id ms vi th hi zh ja ko my 4.2 Experimental Methodology BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target inte"
N16-1124,N03-1017,0,0.208159,"Missing"
N16-1124,2005.mtsummit-papers.11,0,0.146899,"zh ja ko my ar da de en es fr Target Language collection of travel-related expressions (Kikui et al., 2003). The languages were Arabic (ar), Danish (da), German (de), English (en), Spenish (es), French (fr), Italian (it), Dutch (nl), Portugese (pt), Russian (ru), Tagalog (tl), Indonesian (id), Malaysian (ms), Vietnamese (vi), Thai (th), Hindi (hi), Chinese (zh), Japanese (ja), Korean (ko) and Myanmar (my). 155,121 sentences were used for training, 5,000 sentences for development and 2,000 sentences for evaluation. In addition, we ran experiments on two language pairs from the Europarl corpus (Koehn, 2005). The language pairs were English-German, GermanEnglish, English-Spanish and Spanish-English. The corpus statistics are given in Table 2. it nl pt ru tl id ms vi th hi zh ja ko my 4.2 Experimental Methodology BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM t"
N16-1124,P00-1056,0,0.295376,"a), Korean (ko) and Myanmar (my). 155,121 sentences were used for training, 5,000 sentences for development and 2,000 sentences for evaluation. In addition, we ran experiments on two language pairs from the Europarl corpus (Koehn, 2005). The language pairs were English-German, GermanEnglish, English-Spanish and Spanish-English. The corpus statistics are given in Table 2. it nl pt ru tl id ms vi th hi zh ja ko my 4.2 Experimental Methodology BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target interlocking, (2) source interlo"
N16-1124,P03-1021,0,0.0180612,"ogy BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target interlocking, (2) source interlocking and (3) both source and target interlocking for all possible combinations of languages (i.e. 380 language pairs). We studied two methods for accomplishing (3). In the first, interlocking as defined in Sections 3.1 and 3.2 are permitted freely. In the second, the target is allowed to interlock if and only if the source is also interlocked. This was similar to the method proposed by (Tribble and et al., 2003). 0 &lt; BLEU Differ"
N16-1124,2001.mtsummit-papers.68,0,0.0674947,"sed SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target interlocking, (2) source interlocking and (3) both source and target interlocking for all possible combinations of languages (i.e. 380 language pairs). We studied two methods for accomplishing (3). In the first, interlocking as defined in Sections 3.1 and 3.2 are permitted freely. In the second, the target is allowed to interlock if and only if the source is also interlocked. This was similar to the method proposed by (Tribble and et al., 2003). 0 &lt; BLEU Difference ≤ 0.3 4.3 Results In this section, we will first present the results of the experim"
N16-1124,2010.amta-papers.31,0,0.0204863,"ases. Their results showed improvements of 0.3 to 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. (Tribble and et al., 2003) proposed a method to generate longer new phrases by merging existing phraselevel alignments that have overlaping words on both source and target sides. Their experiments on translating Arabic-English text from the news domain were encouraging. 1076 Proceedings of NAACL-HLT 2016, pages 1076–1081, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (Roth and McCallum, 2010) proposed a conditional-random-field approach to discriminatively train phrase based machine translation in which training and decoding are both cast in a sampling framework. Different with traditional PBSMT decoding that infers both a Viterbi alignment and the target sentence, their approach produced a rich overlapping phrase alignment. Their approach leveraged arbitrary features of the entire source sentence, target sentence and alignment. (K¨aa¨ ri¨ainen, 2009) proposed a novel phrase-based conditional exponential family translation model for SMT. The model operates on a feature representat"
N16-1124,N04-4026,0,0.0655285,"om the Europarl corpus (Koehn, 2005). The language pairs were English-German, GermanEnglish, English-Spanish and Spanish-English. The corpus statistics are given in Table 2. it nl pt ru tl id ms vi th hi zh ja ko my 4.2 Experimental Methodology BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target interlocking, (2) source interlocking and (3) both source and target interlocking for all possible combinations of languages (i.e. 380 language pairs). We studied two methods for accomplishing (3). In the first, interlocking as d"
N16-1124,P02-1040,0,\N,Missing
N16-1124,2009.eamt-smart.4,0,\N,Missing
N16-1124,D08-1076,0,\N,Missing
P06-2028,P02-1044,0,0.0248088,"view we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the ∗ National Institute of Information and Communications Technology † ATR Spoken Language Communication Research Labs 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, c Sydney, July 2006. 2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before and after the word being labeled. The features used in both of these approaches resemble those present"
P06-2028,C96-1020,1,0.598072,"to 6 allowable tags for each word. During testing, only if the predicted tag fails to match any of the allowed tags is it considered an error. Experimental Data The primary corpus used for the experiments presented in this paper is the ATR General English Treebank. This consists of 518,080 words (approximately 20 words per sentence, on average) of text annotated with a detailed semantic and syntactic tagset. To understand the nature of the task involved in the experiments presented in this paper, one needs some familiarity with the ATR General English Tagset. For detailed presentations, see (Black et al., 1996b; Black et al., 1996a; Black and Finch, 2001). An apercu can be gained, however, from Figure 1, which shows two sample sentences from the ATR Treebank (and originally from a Chinese take–out food 4 Tagging Model 4.1 ME Model Our tagging framework is based on a maximum entropy model of the following form: p(t, c) = γ K Y k=0 where: 216 f (c,t) αkk p0 (1) (_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DD1 coupon_NN1DOCUMENT when_CSWHEN ordering_VVGINTER-ACT OR_CCOR ONE_MC1WORD FREE_JJMONEY FANTAIL_NN1ANIMAL SHRIMPS_NN1FOOD Figure 1: Two ATR Treebank Sentences from a Take–Out Food Flier - t"
P06-2028,J94-2001,0,0.408361,"addition to their syntactic function, a broad semantic class that signifies the semantics of the word in the context of the sentence, but does not necessarily provide information that is sufficiently finegrained as to disambiguate its sense. This differs 2 Related Work Our work is a synthesis of POS tagging and WSD, and as such, research from both these fields is directly relevant here. The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy (ME) tagger of (Ratnaparkhi, 1996) which in turn is related to the taggers of (Kupiec, 1992) and (Merialdo, 1994). The ME approach is well-suited to this kind of labeling because it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the ∗ National Institute of Information and Communications Technology † ATR Spoken Language Communication Research Labs 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, c Sydney, July 2006. 2006 Association for Computational Linguistic"
P06-2028,W98-0703,0,0.0163846,"l the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the ∗ National Institute of Information and Communications Technology † ATR Spoken Language Communication Research Labs 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, c Sydney, July 2006. 2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before and after the word being labe"
P06-2028,P98-1020,1,0.863874,"ver, from Figure 1, which shows two sample sentences from the ATR Treebank (and originally from a Chinese take–out food 4 Tagging Model 4.1 ME Model Our tagging framework is based on a maximum entropy model of the following form: p(t, c) = γ K Y k=0 where: 216 f (c,t) αkk p0 (1) (_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DD1 coupon_NN1DOCUMENT when_CSWHEN ordering_VVGINTER-ACT OR_CCOR ONE_MC1WORD FREE_JJMONEY FANTAIL_NN1ANIMAL SHRIMPS_NN1FOOD Figure 1: Two ATR Treebank Sentences from a Take–Out Food Flier - t is tag being predicted; syntax, then semantics given the syntax, whereas in (Black et al., 1998) both syntax and semantics were predicted together in one step. In using syntactic tags as features, we take a softer approach to the two-stage process. The tagger has access to accurate syntactic information; however, it is not necessarily constrained to accept this choice of syntax. Rather, it is able to decide both syntax and semantics while taking semantic context into account. In order to find the most probable sequence of tags, we tag in a left-to-right manner using a beam-search algorithm. - c is the context of t; - γ is a normalization coefficient that ensures: QK fk (c,t) ΣL p0 = 1; t"
P06-2028,P96-1025,0,0.0860888,"Missing"
P06-2028,1995.iwpt-1.15,0,0.0119554,"two-stage approach to prediction, first predicting 217 which provide little or no benefit to the model, thus speeding up the training. In some cases it even allows a model to be trained where it would not otherwise be possible to train one. For the purposes of our experiments, we use the top 50,000 predicates for each model to form the feature set. the model should be aware of the dependent object, and conversely when tagging that object, the model should have a feature imposing a constraint arising from the identity of the dependent verb. 5 We parsed our corpus using the parser detailed in (Grinberg et al., 1995). The dependencies output by this parser are labeled with the type of dependency (connector) involved. For example, subjects (connector type S) and direct objects of verbs (O) are explicitly marked by the process (a full list of connectors is provided in the paper). We used all of the dependencies output by the parser as features in the models. 5.1.1 External Knowledge Sources 5.1 Lexical Dependencies Features derived from n-grams of words and tags in the immediate vicinity of the word being tagged have underpinned the world of POS tagging for many years (Kupiec, 1992; Merialdo, 1994; Ratnapar"
P06-2028,C02-1115,0,0.0275414,"ications Technology † ATR Spoken Language Communication Research Labs 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, c Sydney, July 2006. 2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before and after the word being labeled. The features used in both of these approaches resemble those present in the feature set of a standard n-gram tagger, such as the one used as the baseline for the experiments in this paper. The semantic ta"
P06-2028,C96-2212,0,0.0353091,"redicate representing membership of this fruit synset should, if true, favor the selection of the correct tag for fruit words: NN1FOOD. The predicate will be true for the word pomegranate which will thereby benefit from the model’s knowledge of how to tag the other words in its class. Even if this is not so at this level in the hierarchy, it is likely to be so at some level of granularity. Precisely which levels of detail are useful will be learned by the model during training. 5.2.1 Automatic Clustering of Text We used the automatic agglomerative mutualinformation-based clustering method of (Ushioda, 1996) to form hierarchical clusters from approximately 50 million words of tokenized, unannotated text drawn from similar domains as the treebank used to train the tagger. Figure 5.2 shows the position of the word apple within the hierarchy of clusters. This example highlights both the strengths and weaknesses of this approach. One strength is that the process of clustering proceeds in a purely objective fashion and associations between words that may not have been considered by a human annotator are present. Moreover, the clustering process considers all types that actually occur in the corpus, an"
P06-2028,W04-0833,0,0.0419465,"Missing"
P06-2028,H93-1052,0,0.0921069,"ause it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the ∗ National Institute of Information and Communications Technology † ATR Spoken Language Communication Research Labs 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, c Sydney, July 2006. 2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and"
P06-2028,J93-2004,0,\N,Missing
P06-2028,W96-0213,0,\N,Missing
P06-2028,J04-1001,0,\N,Missing
P06-2028,C98-1020,1,\N,Missing
P13-2070,1993.eamt-1.1,0,0.438537,"Missing"
P13-2070,J03-1002,0,0.0172917,"Missing"
P13-2070,2010.iwslt-papers.7,1,0.585967,"Missing"
P13-2070,P11-2010,0,0.0396112,"Missing"
P13-2070,W12-4404,0,0.0551276,"Missing"
P13-2070,P11-2094,0,0.0189162,"ods have been proposed, such as the supervised language model-based approach of (Li et al., 2007), and the unsupervised approach of (Huang et al., 2005) that used a bottom-up clustering algorithm. (Li et al., 2007) proposed a supervised transliteration model which classifies names based on their origins and genders using a language model; it switches between transliteration models based on the input. (Hagiwara et al., 2011) tackled the issue by using an unsupervised method based on the EM algorithm to perform a soft classification. Recently, non-parametric Bayesian models (Finch et al., 2010; Huang et al., 2011; Hagiwara et al., 2012) have attracted much attention in the transliteration field. In comparison to many of the previous alignment models (Li et al., 2004; Jiampojamarn et al., 2007; Berg-Kirkpatrick et al., 2011), the nonparametric Bayesian models allow unconstrained monotonic many-to-many alignment and are able to overcome the inherent over-fitting problem. Until now most of the previous work (Li et al., 2007; Hagiwara et al., 2011) is either affected by the multi-origins factor, or has issues with overfitting. (Hagiwara et al., 2012) took these two factors into consideration, but their ap"
P13-2070,W09-3504,0,0.0286643,"Missing"
P13-2070,J98-4003,0,0.247889,"Missing"
P13-2070,P07-2045,0,0.00456523,"rned by sampling its value. Following (Blunsom et al., 2009) we used a vague gamma prior Γ(10−4 , 104 ), and sampled new values from a log-normal distribution whose mean was the value of the parameter, and variance was 0.3. We used the Metropolis-Hastings algorithm to determine whether this new sample would be accepted. The parameters λs and λt in Equation 4 were set to λs = 4 and λt = 1. We compare our alignment model with GIZA++ (Och et al., 2003) and the Bayesian bilingual alignment model (BBAM). We employ two decoding models: a phrase-based machine translation decoder (specifically Moses (Koehn et al., 2007)), and the DirecTL decoder (Jiampojamarn et al., 2009). They are based on different decoding strategies and optimization targets, and therefore make the comparison more comprehensive. For the Moses decoder, we applied the grow-diag-final-and heuristic algorithm to extract the phrase table, and tuned the parameters using the BLEU metric. Corpora EO ECJ-O Multi-O Corpus Scale Training Development 32,681 3,267 32,500 3,250 33,291 3,328 #(Clusters) Testing 3,267 3,250 3,328 #(Targets) Model cDPMM GIZA++ BBAM cDPMM EO 5.8 14.43 6.06 9.32 ECJ-O 9.5 5.35 2.45 3.45 Multi-O 14.3 6.62 2.91 4.28 Table 3:"
P13-2070,P04-1021,0,0.0726859,"sed a bottom-up clustering algorithm. (Li et al., 2007) proposed a supervised transliteration model which classifies names based on their origins and genders using a language model; it switches between transliteration models based on the input. (Hagiwara et al., 2011) tackled the issue by using an unsupervised method based on the EM algorithm to perform a soft classification. Recently, non-parametric Bayesian models (Finch et al., 2010; Huang et al., 2011; Hagiwara et al., 2012) have attracted much attention in the transliteration field. In comparison to many of the previous alignment models (Li et al., 2004; Jiampojamarn et al., 2007; Berg-Kirkpatrick et al., 2011), the nonparametric Bayesian models allow unconstrained monotonic many-to-many alignment and are able to overcome the inherent over-fitting problem. Until now most of the previous work (Li et al., 2007; Hagiwara et al., 2011) is either affected by the multi-origins factor, or has issues with overfitting. (Hagiwara et al., 2012) took these two factors into consideration, but their approach still operates within an EM framework and model order selection by hand is necessary prior to training. Introduction Machine transliteration methods"
P13-2070,P07-1016,0,0.12454,"re model for clustering, and a set of multinomial Dirichlet process models that perform bilingual alignment independently for each cluster. The experimental results show that our method considerably outperforms conventional alignment models. 1 “Kim Jong-il/金正恩” (Korea), “Kana Gaski/金崎” (Japan), “Haw King/霍金” (England), “Jin yong/金庸’ (China). The same Chinese character “金” should be aligned to different romanized character sequences: “Kim”, “Kana”, “King”, “Jin”. To address this issue, many name classification methods have been proposed, such as the supervised language model-based approach of (Li et al., 2007), and the unsupervised approach of (Huang et al., 2005) that used a bottom-up clustering algorithm. (Li et al., 2007) proposed a supervised transliteration model which classifies names based on their origins and genders using a language model; it switches between transliteration models based on the input. (Hagiwara et al., 2011) tackled the issue by using an unsupervised method based on the EM algorithm to perform a soft classification. Recently, non-parametric Bayesian models (Finch et al., 2010; Huang et al., 2011; Hagiwara et al., 2012) have attracted much attention in the transliteration f"
P13-2070,P09-1012,0,0.217435,"rst generate an infinite number of clusters, choose one, then generate a transliteration pair using the parameters that describe the cluster. The basic sampling unit of the cDPMM for the clustering process is a transliteration pair, but the basic sampling unit for BBAM is a TU. In order to integrate the two processes in a single model we treat a transliteration pair as a sequence of TUs generated by a BBAM model. The BBAM generates a sequence (a transliteration pair) based on the joint source-channel model (Li et al., 2004). We use a blocked version of a Gibbs sampler to train each BBAM (see (Mochihashi et al., 2009) for details of this process). 2.1 Terminology In this paper, we concentrate on the alignment process for transliteration. The proposed cDPMM segments a bilingual corpus of transliteration pairs into bilingual character sequence-pairs. We will call these sequence-pairs Transliteration Units (TUs). We denote the source and target of n a TU as sm 1 = ⟨s1 , ..., sm ⟩ and t1 = ⟨t1 , ..., tn ⟩ respectively, where si (ti ) is a single character in source (target) language. We use the same notation (s, t) = (⟨s1 , ..., sm ⟩, ⟨t1 , ..., tn ⟩) to denote a transliteration pair, which we can write as n x"
P13-2070,P00-1037,0,\N,Missing
P13-2070,N07-1047,0,\N,Missing
P13-2070,P09-1088,0,\N,Missing
P13-2070,W12-4401,0,\N,Missing
P13-2070,W12-4402,0,\N,Missing
P13-2070,D11-1029,0,\N,Missing
P14-2122,J93-2003,0,0.0813025,"h E, formulated as: ′ P ∗ (i|j, I, J)PB (fj |ei ) (9) i=1 ∗ 2.1.2 Bilingual Expectation P (Fkk |F, E, B) = I X 2.2 Maximization P (aj |j, I, J)PB (fj |eaj ), Inspired by (Teh, 2006; Mochihashi et al., 2009; Neubig et al., 2010; Teh and Jordan, 2010), we employ a Pitman-Yor process model to build the segmentation model M or B. The monolingual model M is F ∈F j=1 a ′ fjk =Fkk (6) where J and I are the number of foreign and English words, respectively, and aj is the position of the English word that is aligned to fj in the alignment a. For the alignment we employ an approximation to IBM model 2 (Brown et al., 1993; Och and Ney, 2003) described below. We define the conditional probability of fj given the corresponding English sentence E and the model B as: PM (fj ) = ¡ ¢ max n(fj ) − d, 0 + (θ + d · nM )G0 (fj ) P ′ fj′ n(fj ) + θ ¯ ¯ nM = ¯{fj |n(fj ) &gt; d}¯, (11) (7) where fj is a foreign language word, and n(fj ) is the observed counts of fj , θ is named the strength parameter, G0 (fj ) is named the base distribution of fj , and d is the discount. The bilingual model is Then, the previous dynamic programming method can be extended to the bilingual expectation PB (fj |ei ) = ¡ ¢ max n(fj , ei ) − d, 0"
P14-2122,C10-1092,0,0.0157702,"being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training. We aware that variational bayes (VB) may be used for speeding up the training of DP-based Unsupervised word segmentation (UWS) can provide domain-adaptive segmentation for statistical machine transl"
P14-2122,W08-0336,0,0.0239815,"able to supervised segmenters on the in-domain NIST OpenMT corpus, and yields a 0.96 BLEU relative increase on NTCIR PatentMT corpus which is out-of-domain. 1 Introduction Many languages, especially Asian languages such as Chinese, Japanese and Myanmar, have no explicit word boundaries, thus word segmentation (WS), that is, segmenting the continuous texts of these languages into isolated words, is a prerequisite for many natural language processing applications including SMT. Though supervised-learning approaches which involve training segmenters on manually segmented corpora are widely used (Chang et al., 2008), yet the criteria for manually annotating words are arbitrary, and the available annotated corpora are limited in both quantity and genre variety. For example, in machine translation, there are various parallel corpora such as 1 http://ntcir.nii.ac.jp/PatentMT 752 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752–758, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Monolingual and bilingual WS can be formulated as follows, respectively, or PYP-based bilingual UWS. However, VB requires f"
P14-2122,J03-1002,0,0.00915462,"′ P ∗ (i|j, I, J)PB (fj |ei ) (9) i=1 ∗ 2.1.2 Bilingual Expectation P (Fkk |F, E, B) = I X 2.2 Maximization P (aj |j, I, J)PB (fj |eaj ), Inspired by (Teh, 2006; Mochihashi et al., 2009; Neubig et al., 2010; Teh and Jordan, 2010), we employ a Pitman-Yor process model to build the segmentation model M or B. The monolingual model M is F ∈F j=1 a ′ fjk =Fkk (6) where J and I are the number of foreign and English words, respectively, and aj is the position of the English word that is aligned to fj in the alignment a. For the alignment we employ an approximation to IBM model 2 (Brown et al., 1993; Och and Ney, 2003) described below. We define the conditional probability of fj given the corresponding English sentence E and the model B as: PM (fj ) = ¡ ¢ max n(fj ) − d, 0 + (θ + d · nM )G0 (fj ) P ′ fj′ n(fj ) + θ ¯ ¯ nM = ¯{fj |n(fj ) &gt; d}¯, (11) (7) where fj is a foreign language word, and n(fj ) is the observed counts of fj , θ is named the strength parameter, G0 (fj ) is named the base distribution of fj , and d is the discount. The bilingual model is Then, the previous dynamic programming method can be extended to the bilingual expectation PB (fj |ei ) = ¡ ¢ max n(fj , ei ) − d, 0 + (θ + d · nei )G0 ("
P14-2122,D09-1075,0,0.0283316,"r the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it. Thus its complexity is U 2 times the unigram model’s complexity: Omonoling = O(Ni |F|KU 4 ). Type Mono. Mono. Biling. Biling. (17) 4.1.3 Parameter settings The parameters are tuned on held-out data sets. The maximum length of foreign language words is set to 4. For the PYP model, the base distribution adopts the formula in (Chung and Gildea, 2009), and the strength parameter is set to 1.0, and the discount is set to 1.0 × 10−6 . For bilingual segmentation,the size of the alignment window is set to 6; the probability λφ of foreign language words being generated by an empty Experiments In this section, the proposed method is first validated on monolingual segmentation tasks, and then evaluated in the context of SMT to study whether the translation quality, measured by BLEU, can be improved. 4.1 Experimental Settings 4.1.1 Experimental Corpora Two monolingual corpora and two bilingual corpora are used (Table 2). CHILDES (MacWhinney and Sn"
P14-2122,P03-1021,0,0.0177603,"noling = O(Ni |F|KU 2 ), unigram bigram unigram 4 = O(Ni |F|KU 2 δb ). # Characters 95,809 4,234,824 19,692,605 63,130,757 corpus for UWS methods. The SIGHAN-MSR corpus (Emerson, 2005) consists of manually segmented simplified Chinese news text, released in the SIGHAN bakeoff 2005 shared tasks. The first bilingual corpus: OpenMT06 was used in the NIST open machine translation 2006 Evaluation 2 . We removed the United Nations corpus and the traditional Chinese data sets from the constraint training resources. The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). This data set mainly consists of news text 3 . PatentMT9 is from the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. (15) 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with d"
P14-2122,I05-3017,0,0.194131,"t al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model bilingual UWS under a similar framework with monolingual UWS"
P14-2122,P02-1040,0,0.0934044,"shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. (15) 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al., 2002) was used to evaluate the quality. Character-based segmentation, LDC segmenter and Stanford Chinese segmenters were used as the baseline methods. (16) The bilingual expectation is given by Eq. 8, whose complexity is the same as the monolingual case. However, the complexity of calculating the transition probability, in Eqs. 9 and 10, is O(δb ). Thus its overall complexity is: Obiling # Sentences 9,790 90,903 437,004 1,004,000 Table 2: Experimental Corpora where Ni is the number of iterations, K is the average number of characters per sentence, and U is the predefined maximum length of words. Fo"
P14-2122,P06-1085,0,0.0420447,") which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy"
P14-2122,P07-2045,0,0.0109645,"or MERT tuning (Och, 2003). This data set mainly consists of news text 3 . PatentMT9 is from the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. (15) 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al., 2002) was used to evaluate the quality. Character-based segmentation, LDC segmenter and Stanford Chinese segmenters were used as the baseline methods. (16) The bilingual expectation is given by Eq. 8, whose complexity is the same as the monolingual case. However, the complexity of calculating the transition probability, in Eqs. 9 and 10, is O(δb ). Thus its overall complexity is: Obiling # Sentences 9,790 90,903 437,004 1,004,000 Table 2: Experimental Corpora where Ni is the number of iterations, K is"
P14-2122,P06-1124,0,0.0821838,"Missing"
P14-2122,I05-3027,0,0.0342293,"Missing"
P14-2122,P09-1012,0,0.285732,"d news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilin"
P14-2122,J01-3002,0,0.456396,"Missing"
P14-2122,C08-1128,0,0.047334,"Missing"
P14-2122,I08-1002,0,0.179395,"le to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model"
P14-2122,2008.iwslt-evaluation.1,0,\N,Missing
P17-2089,D11-1033,0,0.776056,"2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains a"
P17-2089,2014.iwslt-evaluation.1,0,0.0320202,"Missing"
P17-2089,P15-1001,0,0.0256648,"er, we exploit the NMT’s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver,"
P17-2089,2016.amta-researchers.8,0,0.537509,"hod can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only foc"
P17-2089,D15-1147,0,0.0659529,"Missing"
P17-2089,P07-2045,0,0.0260817,"Missing"
P17-2089,P13-2119,0,0.356115,"e training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able"
P17-2089,2015.mtsummit-papers.10,0,0.319092,"a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and"
P17-2089,2015.iwslt-evaluation.11,0,0.719606,"Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelr"
P17-2089,P16-2021,0,0.0875788,"bedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30 - August 4, 2017."
P17-2089,D14-1062,0,0.0613509,"Missing"
P17-2089,P10-2041,0,0.40117,"od (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the sam"
P17-2089,P02-1040,0,0.117011,"Missing"
P17-2089,E12-1055,0,0.0294002,"ma,eiichiro.sumita}@nict.go.jp Abstract a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained"
P17-2089,P13-1082,0,0.0794448,"ta}@nict.go.jp Abstract a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and"
P17-2089,P16-1008,0,0.0219846,"NMT’s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30"
P17-2089,C16-1295,1,0.865382,"ms apply a similar sentence representation. In comparison, we adopt a transition layer between the source and target layers and don’t use test data. 2 It is possible to use a sample of the out-of-domain data. In this paper, we use all of them. 561 vector center CFin as d(vf , CFin ) and out-ofdomain vector center CFout as d(vf , CFout ), respectively. We use the difference δ of these two distances to classify each sentence: δf = d(vf , CFin ) − d(vf , CFout ). blog texts. The statistics on data sets were shown in Table 1. These adaptation corpora settings were nearly the same as that used in (Wang et al., 2016). The differences were: (5) By using an English-to-French NMT system NEF , we can obtain a target sentence embedding ve , in-domain target vector center CEin and out-of-domain target vector center CEout . Corresponding distance difference δe is, δe = d(ve , CEin ) − d(ve , CEout ). • For IWSLT, they chose FR-EN translation task, which is popular in PBSMT. We chose EN-FR, which is more popular in NMT; • For NIST, they chose 02-05 as dev set, and we chose 02-04. Because we would report results on two test sets (MT05 and MT06) in comparison with only one (MT06). (6) δf , δe and δf e = δf + δe can"
P17-2089,P14-2122,1,0.851211,", the maximum sequence length were 50, and the beam size for decoding was 10. Default dropout were applied. We used a mini-batch Stochastic Gradient Descent (SGD) algorithm together with ADADELTA optimizer (Zeiler, 2012). Training was conducted on a single Tesla K80 GPU. Each NMT model was trained for 500K batches, taking 7-10 days. For sentence embedding and selection, it only took several hours to process all of sentences in the training data, because decoding was not necessary. • NIST 2006 Chinese (ZH) to English corpus5 was used as the in-domain training corpus, following the settings of (Wang et al., 2014). Chinese-to-English UN data set (LDC2013T06) and NTCIR-9 (Goto et al., 2011) patent data set were used as out-ofdomain data. NIST MT 2002-2004 and NIST MT 2005/2006 were used as the development and test data, respectively. We are aware of that there are additional NIST corpora in a similar domain, but because this task was for domain adaptation, we only selected a small subset, which is mainly focused on news and 3 https://wit3.fbk.eu/mt.php?release=2014-01 http://statmt.org/wmt15/translation-task.html 5 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 4 6 https://github.com/lisa-groundhog/ Gro"
P17-2089,D16-1050,0,0.0127489,"urce sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for"
P98-1020,P97-1048,0,0.19381,"s and tags over our full corpus of around a million words. The subsequent experiments investigate the ]See Section 2. additional information gains accruing from triggerpair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger-pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information-theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes A y ) of a random variable Y be observed and used to predict"
P98-1020,W97-0105,1,0.859821,"Missing"
P98-1020,C96-1020,1,0.692474,"; Collins, 1996; Jelinek et al., 1994; Magerman, 1995; Ratnaparkhi, 1997). All such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results 7 The window was not allowed to cross a document boundary. The perplexity of the task before taking the trigger-pair information into account for tags was 224.0 and for rules was 57.0. The characteristics of the training corpus we employ are given in Table 1. The corpus, a subset s of the ATR/Lancaster General-English Treebank (Black et al., 1996), consists of a sequence of sentences which have been tagged and parsed by human experts in terms of the ATR English Grammar; a broad-coverage grammar of English with a high level of analytic detail (Black et al., 1996; Black et al., 1997). For instance, the tagset is both seman¢By rule assignment, we mean the task of assigning a rule-name to a node in a parse tree, given that the constituent boundaries have already been defined. 7This is not completely possible, since correlations, even if slight, will exist between intra- and extrasentential information Sspecifically, a roughly-900,000-word"
P98-1020,P96-1025,0,0.0863972,"Missing"
P98-1020,H92-1020,0,0.0228416,"s of around a million words. The subsequent experiments investigate the ]See Section 2. additional information gains accruing from triggerpair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger-pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information-theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes A y ) of a random variable Y be observed and used to predict a random variable X (with"
P98-1020,H93-1016,0,0.0241345,"Missing"
P98-1020,H94-1052,0,0.0545148,"Missing"
P98-1020,H89-1054,0,0.0367426,"words. The subsequent experiments investigate the ]See Section 2. additional information gains accruing from triggerpair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger-pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information-theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes A y ) of a random variable Y be observed and used to predict a random variable X (with alphabet .Ax)."
P98-1020,P95-1037,0,0.0780142,"Missing"
P98-1020,W97-0301,0,0.0237869,"Missing"
P98-1108,J92-4003,0,0.0285758,"Missing"
P98-1108,A92-1018,0,0.0646998,"Missing"
P98-1108,A88-1019,0,0.00839802,"al systems to tackle this problem, uncontrolled heuristics are primarily used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the"
P98-1108,C94-1032,0,0.143546,"ly used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters. Unfortunate"
P98-1108,W96-0113,0,0.494336,"use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters. Unfortunately, such a task"
W08-0334,W05-0909,0,0.0298061,"riments. Tuning the interpolation weights The interpolation weights were tuned by maximizing the BLEU score on the development set over a set of weights ranging from 0 to 1 in increments of 0.1. Figure 1 shows the behavior of two of our models with respect to their weight parameter. Evaluation schemes To obtain a balanced view of the merits of our proposed approach, in our experiments we used 6 evaluation techniques to evaluate our systems. These were: BLEU (Papineni, 2001), NIST (Doddington, 2002), WER (Word Error Rate), PER (Position-independent WER), GTM (General Text Matcher), and METEOR (Banerjee and Lavie, 2005). 4.2 Classification Accuracy The performance of the classifier (from 10-fold cross-validation on the training set) is shown in Table 2. We give classification accuracy figures for predicting both source (same language) and target (English) punctuation. Unsurprisingly, all systems were better at predicting their own punctuation. The poorer scores in the table might reflect linguistic characteristics (perhaps questions in the source language are often expressed as statements in the target), or characteristics of the corpus itself. For all languages the accuracy of the classifier seemed satisfac"
W08-0334,A94-1010,0,0.0397707,"hat fall into one of two classes of dialog sentence: questions and declarations, with a third set of models built to handle the general class. The technique was thoroughly evaluated on data from 17 language pairs using 6 machine translation evaluation metrics. We found the results were corpus-dependent, but in most cases our system was able to improve translation performance, and for some languages the improvements were substantial. 1 Introduction Topic-dependent modeling has proven to be an effective way to improve quality the quality of models in speech recognition (Iyer and Osendorf, 1994; Carter, 1994). Recently, experiments in the field of machine translation (Hasan and Ney, 2005; Yamamoto and Sumita, 2007; Finch et al. 2007, Foster and Kuhn, 2007) have shown that classspecific models are also useful for translation. † National Institute for Science and Technology ‡ Advanced Telecommunications Research Laboratories In the method proposed by Yamamoto and Sumita (2007), topic dependency was implemented by partitioning the data into sets before the decoding process commenced, and subsequently decoding these sets independently using different models that were specific to the class predicted fo"
W08-0334,W07-0722,0,0.175874,"Missing"
W08-0334,W07-0717,0,0.570953,". The technique was thoroughly evaluated on data from 17 language pairs using 6 machine translation evaluation metrics. We found the results were corpus-dependent, but in most cases our system was able to improve translation performance, and for some languages the improvements were substantial. 1 Introduction Topic-dependent modeling has proven to be an effective way to improve quality the quality of models in speech recognition (Iyer and Osendorf, 1994; Carter, 1994). Recently, experiments in the field of machine translation (Hasan and Ney, 2005; Yamamoto and Sumita, 2007; Finch et al. 2007, Foster and Kuhn, 2007) have shown that classspecific models are also useful for translation. † National Institute for Science and Technology ‡ Advanced Telecommunications Research Laboratories In the method proposed by Yamamoto and Sumita (2007), topic dependency was implemented by partitioning the data into sets before the decoding process commenced, and subsequently decoding these sets independently using different models that were specific to the class predicted for the source sentence by a classifier that was run over the source sentences in a pre-processing pass. Our approach is in many ways a generalization o"
W08-0334,N03-1017,0,0.0292428,"Missing"
W08-0334,koen-2004-pharaoh,0,0.038143,"Before adopting the mixture-based approach set out in this paper, we first pursued an obvious and intuitively appealing way of using this classifier. We applied it as a filter to the output of the decoder, to force source sentences that the classifier predicts should generate questions in the target to actually generate questions in the target. This approach was unsuccessful due to a number of issues. 211 4.1 Experimental Conditions Decoder The decoder used to in the experiments, CleopATRa is an in-house phrase-based statistical decoder that can operate on the same principles as the PHARAOH (Koehn, 2004) and MOSES (Koehn et Source BLEU NIST WER PER GTM METEOR ar 0.4457 (0.00) 8.9386 (0.00) 0.4458 (0.00) 0.3742 (0.00) 0.7469 (0.00) 0.6766 (0.00) da 0.6640 (0.64) 11.4500 (1.64) 0.2560 (0.08) 0.2174 (2.42) 0.8338 (0.68) 0.8154 (1.23) de 0.6642 (0.79) 11.4107 (0.44) 0.2606 (2.18) 0.2105 (0.14) 0.8348 (-0.13) 0.8132 (-0.07) es 0.7345 (0.00) 12.1384 (0.00) 0.2117 (0.00) 0.1668 (0.00) 0.8519 (0.00) 0.8541 (0.00) fr 0.6666 (0.95) 11.7443 (0.63) 0.2548 (4.82) 0.2172 (6.50) 0.8408 (0.48) 0.8293 (1.29) id 0.5295 (9.56) 10.3459 (4.11) 0.3899 (21.17) 0.3239 (4.65) 0.7960 (1.35) 0.7521 (2.35) it 0.6702 (1."
W08-0334,J03-1002,0,0.00321587,"Missing"
W08-0334,P03-1021,0,0.00883029,"improvement of the proposed method’s score relative to the alternative method. Cells with bold borders indicate those conditions where performance was degraded. sary to decode the sentence in hand. This reduced the memory overhead considerably when loading multiple models, without noticeably affecting decoding time. Moreover, it is also possible to precompute the interpolated probabilities for most of the models for each sentence before the search commences, reducing both search memory and processing time. 213 Decoding Conditions For tuning of the decoder&apos;s parameters, minimum error training (Och 2003) with respect to the BLEU score using was conducted using the respective development corpus. A 5-gram language model, built using the SRI language modeling toolkit (Stolcke, 1999) with Witten-Bell smoothing was used. The model included a length model, and also the simple distance-based distortion model used by the PHARAOH decoder (Koehn, 2004). 4.3 zh id BLEU score 0.46 0.44 0.42 0.4 0.38 0 0.2 0.4 0.6 0.8 Model interpolation weight 1 Figure 3. Graph showing the BLEU score on the developmment set plotted against the general model’s interpolation weight (a weight of 0 meaning no contribution fr"
W08-0334,2001.mtsummit-papers.68,0,0.0414493,"Missing"
W08-0334,2006.amta-papers.25,0,0.0360089,"Missing"
W08-0334,P02-1040,0,\N,Missing
W08-0334,2005.eamt-1.17,0,\N,Missing
W08-0334,2007.iwslt-1.15,1,\N,Missing
W09-0418,2007.mtsummit-papers.48,1,0.737342,"ocabulary (OOV) words, i.e., source language words that do not occur in the training corpus. For unknown words, no translation entry is available in the statistical translation model (phrase-table). As a result, these OOV words cannot be translated. Dealing with languages with a rich morphology like Spanish and having a limited amount of bilingual resources make this problem even more severe. There have been several efforts in dealing with OOV words to improve translation quality. In addition to parallel text corpora, external bilingual dictionaries can be exploited to reduce the OOV problem (Okuma et al., 2007). However, these approaches depend on the coverage of the utilized external dictionaries. Data sparseness problems due to inflectional variations were previously addressed by applying word transformations using stemming or lemmatization (Popovic and Ney, 2005; Gupta and Federico, 2006). A tight integration of morphosyntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. However, these approaches still suffer from"
W09-0418,W05-0909,0,0.0471768,"Missing"
W09-0418,P02-1040,0,0.0750922,"Missing"
W09-0418,W08-0309,0,0.0308403,"n quality of the baseline phrasebased SMT system. 1 Introduction This paper describes the NICT statistical machine translation (SMT) system used for the shared task of the Fourth Workshop on Statistical Machine Translation. We participated in the SpanishEnglish translation task under the Constrained Condition. For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the new"
W09-0418,W08-0334,1,0.83212,", we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the news domain, we applied statistical model adaptation techniques to combine translation models (tm), language models (lm) and disProceedings of the Fourth Workshop on Statistical Machine Translation , pages 105–109, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 105 katakana1 words with phonetically"
W09-0418,I08-8003,1,0.924723,", we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the news domain, we applied statistical model adaptation techniques to combine translation models (tm), language models (lm) and disProceedings of the Fourth Workshop on Statistical Machine Translation , pages 105–109, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 105 katakana1 words with phonetically"
W09-0418,W07-0717,0,0.0215096,"ning of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the news domain, we applied statistical model adaptation techniques to combine translation models (tm), language models (lm) and disProceedings of the Fourth Workshop on Statistical Machine Translation , pages 105–109, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 105 katakana1"
W09-0418,P97-1017,0,0.0702819,"posed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. However, these approaches still suffer from the data sparseness problem, since lemmata and inflectional forms never seen in the training corpus cannot be translated. In order to generate translations for unknown words, previous approaches focused on transliteration methods, where a sequence of characters is mapped from one writing system into another. For example, in order to translate names and technical terms, (Knight and Graehl, 1997) introduced a probabilistic model that replaces Japanese Abstract This paper describes the NICT statistical machine translation (SMT) system used for the WMT 2009 Shared Task (WMT09) evaluation. We participated in the Spanish-English translation task. The focus of this year’s participation was to investigate model adaptation and transliteration techniques in order to improve the translation quality of the baseline phrasebased SMT system. 1 Introduction This paper describes the NICT statistical machine translation (SMT) system used for the shared task of the Fourth Workshop on Statistical Machi"
W09-0418,D07-1091,0,0.0188625,"have been several efforts in dealing with OOV words to improve translation quality. In addition to parallel text corpora, external bilingual dictionaries can be exploited to reduce the OOV problem (Okuma et al., 2007). However, these approaches depend on the coverage of the utilized external dictionaries. Data sparseness problems due to inflectional variations were previously addressed by applying word transformations using stemming or lemmatization (Popovic and Ney, 2005; Gupta and Federico, 2006). A tight integration of morphosyntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. However, these approaches still suffer from the data sparseness problem, since lemmata and inflectional forms never seen in the training corpus cannot be translated. In order to generate translations for unknown words, previous approaches focused on transliteration methods, where a sequence of characters is mapped from one writing system into another. For example, in order to translate names and technical terms, (Knight and Graehl, 1997) introdu"
W09-0418,2005.mtsummit-papers.11,0,0.0369022,"ation task. The focus of this year’s participation was to investigate model adaptation and transliteration techniques in order to improve the translation quality of the baseline phrasebased SMT system. 1 Introduction This paper describes the NICT statistical machine translation (SMT) system used for the shared task of the Fourth Workshop on Statistical Machine Translation. We participated in the SpanishEnglish translation task under the Constrained Condition. For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined"
W09-0418,niessen-etal-2000-evaluation,0,0.0181976,"8.36 4.3 Effects of Transliteration In order to investigate the effects of transliteration, we trained three different transliteration using the phrase-table of the baseline systems trained on (a) only the NC corpus, (b) only the EP corpus, and (c) on the merged corpus (NC+EP). The performance of these phrase-based transliteration models is evaluated for 2000 randomly selected transliteration examples. Table 5 summarizes the haracter-based automatic evaluation scores for the word error rate (WER) metrics, i.e., the edit distance between the system output and the closest reference translation (Niessen et al., 2000), as well as the BLEU and METEOR metrics. The best performance is achieved when training examples from both domains are exploit to transliterate unknown Spanish words into English. Therefore, the NC+EP transliteration model was applied to the translation outputs of all mixture models described in Section 4.2. The effects of the transliteration post-process are summarized in Table 6. Transliteration consisTable 7: Testset 2009 Performance weight NC Eval optimization BLEU tm 21.07 tm+lm 20.95 tm+dm 21.45 tm+lm+dm 21.67∗ EP Eval BLEU 20.81 20.59 21.32 21.27 5 Conclusion The work for this year’s s"
W09-0418,J03-1002,0,0.0112243,"Missing"
W09-0418,P03-1021,0,0.0130472,"Phrase penalty Language model probability Lexical reordering probability Simple distance-based distortion model Word penalty For the training of the statistical models, standard word alignment (GIZA++ (Och and Ney, 2003)) and language modeling (SRILM (Stolcke, 2002)) tools were used. We used 5-gram language models trained with modified Knesser-Ney smoothing. The language models were trained on the target side of the provided training corpora. Minimum error rate training (MERT) with respect to BLEU score was used to tune the decoder’s parameters, and performed using the technique proposed in (Och, 2003). For the translation, the inhouse multi-stack phrase-based decoder CleopATRa was used. 4.1 Baseline Our baseline system is a fairly typical phrasebased machine translation system (Finch and Sumita, 2008a) built within the framework of a feature-based exponential model containing the following features: The automatic evaluation scores of the baseline systems trained on (a) only the NC corpus and (b) only on the EP corpus are summarized in Table 3. 107 Table 3: Baseline Performance baseline NC Eval BLEU METEOR 17.56 40.52 Table 5: Transliteration Performance EP Eval BLEU METEOR 33.00 56.50 Trai"
W09-0418,N03-1017,0,\N,Missing
W09-3510,N03-1017,0,0.0157969,"erated in the target • The number of types (the vocabulary size) in both source and target languages is considerably less for the transliteration task We take a statistical machine translation paradigm (Brown at al., 1991) as the basis for our systems. The work in this paper is related to the work of (Finch and Sumita, 2008) who also use SMT directly to transliterate. 3 Experimental Conditions In our experiments we used an in-house phrasebased statistical machine translation decoder called CleopATRa. This decoder operates on exactly the same principles as the publicly available MOSES decoder (Koehn et al., 2003). Like MOSES we utilize a future cost in our calculations. Our decoder was modified to be able to run two instances of the decoder at the same 52 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 52–56, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP Word 1 Word 2 Word m Segment into individual words and decode each word independently Decode Decode Decode n-best n-best n-best hypothesis 1 hypothesis 2 hypothesis 1 hypothesis 1 Search for the best path hypothesis 2 hypothesis 2 ... ... ... hypothesis n hypothesis n hypothesis n Figure 1: The decoding process for mul"
W09-3510,P03-1021,0,0.00484161,"hase (where output translations are compared against a set of references) we restored the case for Russian by simply capitalizing the first character of each word. We chose not to perform any tokenization for any of the language pairs in the shared task. We chose this approach for several reasons: • It allowed us to have a single unified approach for all language pairs • It was in the spirit of the evaluation, as it did not require specialist knowledge outside of the supplied corpora 3.3 Parameter Tuning The SMT systems were tuned using the minimum error rate training procedure introduced in (Och, 2003). For convenience, we used BLEU as a proxy for the various metrics used in the shared task evaluation. The BLEU score is 53 En-Ch En-Ja En-Ko En-Ru Jn-Jk After tuning 0.908 0.772 0.622 0.914 0.769 Before tuning 0.871 0.635 0.543 0.832 0.737 Table 1: The effect on 1-best accuracy by tuning with respect to BLEU score were able to perform a full search. For the rare long word sequences in the data, a beam search strategy was adopted. commonly used to evaluate the performance of machine translation systems and is a function of the geometric mean of n-gram precision. Table 1 shows the effect of tun"
W09-3510,J93-2003,0,\N,Missing
W09-3510,P04-1021,0,\N,Missing
W09-3510,P97-1017,0,\N,Missing
W09-3510,D08-1076,0,\N,Missing
W10-1760,W07-0718,0,0.0433782,"Missing"
W10-1760,P02-1040,0,0.0790848,"th), ICTCLAS (zh), HanTagger (ko). No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with replacement from the evaluation data set, (2) calculates the evaluation metric score of each engine for the sampled test sentences and the difference between the two MT system scores, (3) repeats the sampling/scoring step iteraExperiments The effects of using different word segmentations and integrating them into an SMT engine are investigated using the multilingual Basic Travel Expressions Corpus"
W10-1760,W08-0336,0,0.0752938,"stent with the extracted dictionary to create a word lattice for decoding. The method proposed in this papers differs from previous approaches in the following points: “consistent with SMT models” is one that identifies translation units that are small enough to be translatable, but large enough to be meaningful in the context of the given input sentence, achieving a trade-off between the coverage and the translation task complexity of the statistical models in order to improve translation quality. The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al., 2008). However, improvements have been reported for approaches taking into account not only monolingual, but also bilingual information, to derive a word segmentation suitable for SMT. Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suita"
W10-1760,W96-0213,0,0.138718,"e shown themselves to be highly effective in a broad range of NLP tasks including sentence boundary detection or part-of-speech tagging (Berger et al., 1996). A maximum entropy classifier is an exponential model consisting of a number of binary feature functions and their weights (Pietra et al., 1997). The model is trained by adjusting the weights to maximize the entropy of the probabilistic model given constraints imposed by the training data. In our experiments, we use a conditional maximum entropy model, where the conditional probability of the outcome given the set of features is modeled (Ratnaparkhi, 1996). The model has the form: Iteration 1 segmented SRC Iteration 2 segmented SRC SMT1 … (7) extract (4) annotate (5) resegment ME1 classifier (9) resegment decode evalchr decode eval1 decode evalJ-1 better (6) train SRCtoken TRGword alignment (8) annotate better (J-1) train SMTJ-1 … extract (J) train SMTJ worse evalJ SRCtoken TRGword alignment ME2 classifier … MEJ-1 classifier Iteration J-1 segmented SRC Selected Word Segmenter … Figure 2: Iterative Bootstrap Method The feature set is given in Table 1. The lexical context features consist of target words annotated with a tag t. w0 denotes the wor"
W10-1760,D09-1075,0,0.0563363,"erive a word segmentation suitable for SMT. Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation. • it works for any language pair where the source language is unsegmented and the target language segmentation is known. • it can be applied for the translation of a source language where no linguistically motivated word segmentation tools are available. • it applies machine learning techniques to identify segmentation schemes that improve translation quality for a gi"
W10-1760,N09-2019,0,0.0478899,"Missing"
W10-1760,J01-3002,0,0.0418668,"els trained on any of the learned word segmentations and performs comparably to available state-ofthe-art monolingually-built segmentation tools. 1 (2) unknown words, i.e., existing words can be combined into new words such as proper nouns, e.g. “White House”. Purely dictionary-based approaches like (Cheng et al., 1999) addressed these problems by maximum matching heuristics. Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods. For example, (Brent, 1999) proposes a probabilistic segmentation model based on unigram word distributions, whereas (Venkataraman, 2001) uses standard n-gram language models. An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in (Goldwater et al., 2006). The focus of this paper, however, is to learn word segmentations that are consistent with phrasal segmentations of SMT translation models. In case of small translation units, e.g. single Chinese or Japanese characters, it is likely that such tokens have been seen in the training corpus, thus these tokens can be translated by an SMT engine. However, the contextual information p"
W10-1760,P08-1115,0,0.067533,"learned using a parallel corpus by aligning character-wise source language sentences to word units separated by a whitespace in the target language. Successive characters aligned to the same target words are merged into a larger source language unit. Therefore, the granularity of the translation unit is defined in the given bitext context. In order to minimize the side effects of alignment errors and to achieve segmentation consistency, a Maximum-Entropy (ME) algorithm is applied to learn the source language word In order to integrate multiple word segmentation schemes into the SMT decoder, (Dyer et al., 2008) proposed to generate word lattices covering all possible segmentations of the input sentence 401 dicator where only two tags are used, i.e, “E” (end-of-word character tag) and “I” (in-word character tag). The annotations are derived from the SMT training corpus as described in Figure 1. segmentation that is consistent with the translation model of an SMT system trained on the resegmented bitext. The process is iterated until no further improvement in translation quality is achieved. In order to integrate multiple word segmentation into a single SMT system, the statistical translation models t"
W10-1760,N09-1046,0,0.0169293,"ons like Machine Translation (MT). In contrast to Indo-European languages like English, many Asian languages like Chinese do not use a whitespace character to separate meaningful word units. The problems of word segmentation are:   400 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 400–408, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics and to decode the lattice input. An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. The method of (Ma and Way, 2009) also uses a word lattice decoding approach, but they iteratively extract multiple word segmentation schemes from the training bitext. This dictionary-based approach uses heuristics based on the maximum matching algorithm to obtain an agglomeration of segments that are covered by the dictionary. It uses all possible source segmentations that are consistent with the extracted dict"
W10-1760,zhang-etal-2004-interpreting,0,0.0314116,"Missing"
W10-1760,P06-1085,0,0.207335,"rds such as proper nouns, e.g. “White House”. Purely dictionary-based approaches like (Cheng et al., 1999) addressed these problems by maximum matching heuristics. Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods. For example, (Brent, 1999) proposes a probabilistic segmentation model based on unigram word distributions, whereas (Venkataraman, 2001) uses standard n-gram language models. An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in (Goldwater et al., 2006). The focus of this paper, however, is to learn word segmentations that are consistent with phrasal segmentations of SMT translation models. In case of small translation units, e.g. single Chinese or Japanese characters, it is likely that such tokens have been seen in the training corpus, thus these tokens can be translated by an SMT engine. However, the contextual information provided by these tokens might not be enough to obtain a good translation. For example, a Japanese-English SMT engine might translate the two successive characters “ ” (“white”) and “ ” (“bird”) as “white bird”, while a"
W10-1760,W08-0335,1,0.835024,"of a source language where no linguistically motivated word segmentation tools are available. • it applies machine learning techniques to identify segmentation schemes that improve translation quality for a given language pair. • it decodes directly from unsegmented text using segmentation information implicit in the phrase-table to generate the target and thus avoids issues of consistency between phrasetable and input representation. Recent research on SMT is also focusing on the usage of multiple word segmentation schemes for the source language to improve translation quality. For example, (Zhang et al., 2008) combines dictionary-based and CRF-based approaches for Chinese word segmentation in order to avoid outof-vocabulary (OOV) words. Moreover, the combination of different morphological decomposition of highly inflected languages like Arabic or Finnish is proposed in (de Gispert et al., 2009) to reduce the data sparseness problem of SMT approaches. Similarly, (Nakov et al., 2009) utilizes SMT engines trained on different word segmentation schemes and combines the translation outputs using system combination techniques as a postprocess to SMT decoding. • it uses segmentations at all iterative leve"
W10-1760,W02-0704,0,0.0334855,"of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation. • it works for any language pair where the source language is unsegmented and the target language segmentation is known. • it can be applied for the translation of a source language where no linguistically motivated word segmentation tools are available. • it applies machine learning techniques to identify segmentation schemes that improve translation quality for a given language pair. • it decodes directly from unsegmented text using"
W10-1760,W07-0734,0,0.0113547,"No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with replacement from the evaluation data set, (2) calculates the evaluation metric score of each engine for the sampled test sentences and the difference between the two MT system scores, (3) repeats the sampling/scoring step iteraExperiments The effects of using different word segmentations and integrating them into an SMT engine are investigated using the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of sent"
W10-1760,E09-1063,0,0.162335,"segmentation are:   400 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 400–408, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics and to decode the lattice input. An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. The method of (Ma and Way, 2009) also uses a word lattice decoding approach, but they iteratively extract multiple word segmentation schemes from the training bitext. This dictionary-based approach uses heuristics based on the maximum matching algorithm to obtain an agglomeration of segments that are covered by the dictionary. It uses all possible source segmentations that are consistent with the extracted dictionary to create a word lattice for decoding. The method proposed in this papers differs from previous approaches in the following points: “consistent with SMT models” is one that identifies translation units that are"
W10-1760,J03-1002,0,0.0117672,"s of the iterative bootstrap method (dev), and the evaluation of translation quality (test). Besides the number of sentences (sen) and the vocabulary (voc), the sentence length (len) is also given as the average number of words per sentence. The given statistics are obtained using commonly-used linguistic segmentation tools available for the respective language, i.e., CHASEN (ja), WORDCUT (th), ICTCLAS (zh), HanTagger (ko). No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with repl"
W10-1760,C08-1128,0,\N,Missing
W10-1760,J96-1002,0,\N,Missing
W10-2406,W09-3528,0,0.110337,"ize - • Maximum likelihood training using an EM algorithm (Deligne and Bimbot, 1995) The probability of sequences of joint multigrams is modeled using an n-gram model In these respects the model can be viewed as a close relative of the joint source channel model proposed by (Li et al., 2004) for transliteration. Phrase-based SMT 2.2 It is possible to view the process of transliteration as a process of translation at the character level, without re-ordering. From this perspective it is possible to directly employ a phrase-based SMT system in the task of transliteration (Finch and Sumita, 2008; Rama and Gali, 2009). A phrase-based SMT system has the following characteristics: • The symbols in the source and target are aligned one to many in both directions. Joint sequences of source and target symbols are heuristically extracted given these alignments • Transliteration is performed using a loglinear model with weights tuned on development data • The models include: a translation model (with 5 sub-models), and a target language model SMT Decoder In our experiments we used an in-house phrasebased statistical machine translation decoder called CleopATRa. This decoder operates on exactly the same principles"
W10-2406,W09-0438,0,0.022089,"each other well. We now briefly describe the component systems. Joint Multigram Model 2.1 Introduction The joint multigram approach proposed by (Deligne and Bimbot, 1995) has arisen as an extension of the use of variable-length n-grams (multigrams) in language modeling. In a joint multigram, the units in the model consist of multiple input and output symbols. (Bisani and Ney, 2008) refined the approach and applied to it grapheme-to-phoneme conversion, where its performance was shown to be comparable to state-of-the-art systems. The approach was later applied to Arabic-English transliteration (Deselaers et al., 2009) again with promising results. Joint multigram models have the following characteristics: In statistical machine translation the re-scoring of hypotheses produced by a system with additional models that incorporate information not available to the original system has been shown to be an effective technique to improve system performance (Paul et al., 2006). Our approach uses a re-scoring technique to integrate the models of two transliteration systems that are each capable in their own right: a phrase-based statistical machine translation system (Koehn et al., 2003), and a joint multigram model"
W10-2406,I08-8003,1,0.873708,"ect to joint multigram size - • Maximum likelihood training using an EM algorithm (Deligne and Bimbot, 1995) The probability of sequences of joint multigrams is modeled using an n-gram model In these respects the model can be viewed as a close relative of the joint source channel model proposed by (Li et al., 2004) for transliteration. Phrase-based SMT 2.2 It is possible to view the process of transliteration as a process of translation at the character level, without re-ordering. From this perspective it is possible to directly employ a phrase-based SMT system in the task of transliteration (Finch and Sumita, 2008; Rama and Gali, 2009). A phrase-based SMT system has the following characteristics: • The symbols in the source and target are aligned one to many in both directions. Joint sequences of source and target symbols are heuristically extracted given these alignments • Transliteration is performed using a loglinear model with weights tuned on development data • The models include: a translation model (with 5 sub-models), and a target language model SMT Decoder In our experiments we used an in-house phrasebased statistical machine translation decoder called CleopATRa. This decoder operates on exact"
W10-2406,W09-3510,1,0.607773,"hai ➝ English 0.397 0.873 0.525 0.397 English ➝ Hindi 0.445 0.884 0.574 0.445 English ➝ Tamil 0.390 0.887 0.522 0.390 English ➝ Kannada 0.371 0.871 0.506 0.371 English ➝ Japanese 0.378 0.783 0.510 0.378 Arabic ➝ English 0.403 0.891 0.512 0.327 English ➝ Bangla 0.412 0.883 0.550 0.412 Table 1: The results of our system in the official evaluation on the test data on all performance metrics. machine translation systems and is a function of the geometric mean of n-gram precision. The use of BLEU score as a proxy has been shown to be a reasonable strategy for the metrics used in these experiments (Finch and Sumita, 2009). Nonetheless, it is reasonable to assume that one would be able to improve the performance in a particular evaluation metric by doing minimum error rate training specifically for that metric. The interpolation weight was tuned by a grid search to find the value that gave the maximal fscore (according to the official f-score evaluation metric for the shared task) on the development data, the process for English-Japanese is shown in Figure 3. 4 existed between our system and the top-ranked system, there is much room for improvement. One of the strengths in terms of the utility of our approach i"
W10-2406,N03-1017,0,0.0612569,"English transliteration (Deselaers et al., 2009) again with promising results. Joint multigram models have the following characteristics: In statistical machine translation the re-scoring of hypotheses produced by a system with additional models that incorporate information not available to the original system has been shown to be an effective technique to improve system performance (Paul et al., 2006). Our approach uses a re-scoring technique to integrate the models of two transliteration systems that are each capable in their own right: a phrase-based statistical machine translation system (Koehn et al., 2003), and a joint multigram model (Deligne and Bimbot, 1995; Bisani and Ney, 2008). In this work we treat the process of transliteration as a process of direct transduction from sequences of tokens in the source language to sequences of tokens in the target language with • The symbols in the source and target are co-segmented 48 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 48–52, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics F-Score 0.9 3 0.7 3.1 0.4 0.3 2 3 4 5 6 7 8 9 10 Joint Multigram Size Figure 1: The effect on F-score by tuning with resp"
W10-2406,P03-1021,0,0.00726501,"g Multi-Word Sequences 3.4 The data for some languages contained some multi-word sequences. To handle these we had to consider the following strategies: • • Building the Models Introduce a <space&gt; token into the sequence, and treat it as one long character sequence to transliterate; or 3.6 Segment the word sequences into individual words and transliterate these independently, combining the n-best hypothesis lists for all the individual words in the sequence into a single output sequence. Parameter Tuning The SMT systems were tuned using the minimum error rate training procedure introduced in (Och, 2003). For convenience, we used BLEU as a proxy for the various metrics used in the shared task evaluation. The BLEU score is commonly used to evaluate the performance of 50 Language Pair Accuracy in top-1 Mean F-score MRR MAPref English ➝ Thai 0.412 0.883 0.550 0.412 Thai ➝ English 0.397 0.873 0.525 0.397 English ➝ Hindi 0.445 0.884 0.574 0.445 English ➝ Tamil 0.390 0.887 0.522 0.390 English ➝ Kannada 0.371 0.871 0.506 0.371 English ➝ Japanese 0.378 0.783 0.510 0.378 Arabic ➝ English 0.403 0.891 0.512 0.327 English ➝ Bangla 0.412 0.883 0.550 0.412 Table 1: The results of our system in the official"
W10-2406,P04-1021,0,0.321009,"target are co-segmented 48 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 48–52, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics F-Score 0.9 3 0.7 3.1 0.4 0.3 2 3 4 5 6 7 8 9 10 Joint Multigram Size Figure 1: The effect on F-score by tuning with respect to joint multigram size - • Maximum likelihood training using an EM algorithm (Deligne and Bimbot, 1995) The probability of sequences of joint multigrams is modeled using an n-gram model In these respects the model can be viewed as a close relative of the joint source channel model proposed by (Li et al., 2004) for transliteration. Phrase-based SMT 2.2 It is possible to view the process of transliteration as a process of translation at the character level, without re-ordering. From this perspective it is possible to directly employ a phrase-based SMT system in the task of transliteration (Finch and Sumita, 2008; Rama and Gali, 2009). A phrase-based SMT system has the following characteristics: • The symbols in the source and target are aligned one to many in both directions. Joint sequences of source and target symbols are heuristically extracted given these alignments • Transliteration is performed"
W10-2406,W10-2402,0,\N,Missing
W10-2406,J93-2003,0,\N,Missing
W10-2406,P97-1017,0,\N,Missing
W10-2406,D08-1076,0,\N,Missing
W10-3804,P06-2014,0,0.0240457,"word alignment model to find a set of high-precision anchor links and then aligned the remaining words relying on dependency information invoked by the acquired anchor links. The similarity is that both Ma et al. (2008) and this work utilize structure information to find appropriate translations for words which are difficult to align. The difference is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage. There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008; Hermjakob, 2009). Johnson et al., (2007) presented a technique for pruning the phrase table in a PBMT system using Fisher’s exact test. They compute the significance value of each phrase pair and prune the table by deleting phrase pairs with significance values smaller than a certain threshold. Yang and Zheng (2008) extended the work in Johnson et al., (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG). Tomeh et al., (2009) described an approach for filtering phrase tables in a statistical machine trans"
W10-3804,P08-1009,0,0.0407929,"Missing"
W10-3804,P05-1033,0,0.165473,"Missing"
W10-3804,D07-1079,0,0.0344199,"Missing"
W10-3804,P07-1003,0,0.0198412,"o find a set of high-precision anchor links and then aligned the remaining words relying on dependency information invoked by the acquired anchor links. The similarity is that both Ma et al. (2008) and this work utilize structure information to find appropriate translations for words which are difficult to align. The difference is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage. There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008; Hermjakob, 2009). Johnson et al., (2007) presented a technique for pruning the phrase table in a PBMT system using Fisher’s exact test. They compute the significance value of each phrase pair and prune the table by deleting phrase pairs with significance values smaller than a certain threshold. Yang and Zheng (2008) extended the work in Johnson et al., (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG). Tomeh et al., (2009) described an approach for filtering phrase tables in a statistical machine translation system, which rel"
W10-3804,W08-0334,1,0.852581,"e state-of-the-art alignment tool such as GIZA++ 2 can not always find alignments for every word in the sentence pair. The possible reasons could be: its frequency is too low, noisy data, auxiliary words or function words which have no obvious correspondence in the opposite language. In the automatically aligned parallel corpus, unaligned words are frequent enough to be noticeable (see section 4.1 in this paper). How to decide the translation of unaligned word is left to the phrase extraction algorithm. An unaligned 2 Experiments Our SMT system is based on a fairly typical phrase-based model (Finch and Sumita, 2008). For the training of our SMT model, we use a modified training toolkit adapted from the MOSES decoder. Our decoder can operate on the same principles as the MOSES decoder. Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoder’s parameters, and it is performed using the standard technique of Och (2003). A lexicalized reordering model was built by using the “msdbidirectional-fe” configuration in our experiments. The translation model was created from the FBIS parallel corpus. We used a 5-gram language model trained with modified Kneser-Ney smoothing. The lang"
W10-3804,W08-0306,0,0.0146045,"cision anchor links and then aligned the remaining words relying on dependency information invoked by the acquired anchor links. The similarity is that both Ma et al. (2008) and this work utilize structure information to find appropriate translations for words which are difficult to align. The difference is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage. There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008; Hermjakob, 2009). Johnson et al., (2007) presented a technique for pruning the phrase table in a PBMT system using Fisher’s exact test. They compute the significance value of each phrase pair and prune the table by deleting phrase pairs with significance values smaller than a certain threshold. Yang and Zheng (2008) extended the work in Johnson et al., (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG). Tomeh et al., (2009) described an approach for filtering phrase tables in a statistical machine translation system, which relies on a statistical"
W10-3804,W04-3243,0,0.0702217,"Missing"
W10-3804,2002.tmi-papers.9,0,0.0838836,"Missing"
W10-3804,W99-0604,0,0.0822253,"is pair’s contribution to the translation model. In typical PBMT systems such as MOSES (Koehn, 2007), phrase pairs are extracted from wordaligned parallel corpora. Figure 1 shows the form of training example. f1 | e1 f2 | e2 f3 f4 | e3 Figure 1: An example parallel sentence pair and word alignment Since there is no phrase segmentation information in the word-aligned sentence pair, in practice all pairs of “source word sequence target word sequence” that are consistent with word alignments are collected. The words in a legal phrase pair are only aligned to each other, and not to words outside (Och et al., 1999). For example, given a sentence pair and its word alignments shown in Figure1, the following nine phrase pairs will be extracted: Table 1: Phrase pairs extracted from the example in Figure 1 Note that neither the source phrase nor the not a legal phrase pair. Phrase pairs are extracted over the entire training corpus. Given all the collected phrase pairs, we can estimate the phrase translation probability distribution by relative frequency. The collected phrase pairs will also be used to 29 build the lexicalized reordering model. For more details of the lexicalized reordering model, please ref"
W10-3804,P03-1021,0,0.0181132,"requent enough to be noticeable (see section 4.1 in this paper). How to decide the translation of unaligned word is left to the phrase extraction algorithm. An unaligned 2 Experiments Our SMT system is based on a fairly typical phrase-based model (Finch and Sumita, 2008). For the training of our SMT model, we use a modified training toolkit adapted from the MOSES decoder. Our decoder can operate on the same principles as the MOSES decoder. Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoder’s parameters, and it is performed using the standard technique of Och (2003). A lexicalized reordering model was built by using the “msdbidirectional-fe” configuration in our experiments. The translation model was created from the FBIS parallel corpus. We used a 5-gram language model trained with modified Kneser-Ney smoothing. The language model was trained on the target side of the FBIS corpus and the Xinhua news in the GIGAWORD corpus. The development and test sets are from the NIST MT08 evaluation campaign. Table 3 shows the statistics of the corpora used in our experiments. Table 2: Phrase pairs extracted from the example in Figure 2 f1 source word should be trans"
W10-3804,N03-1017,0,0.0379239,"Missing"
W10-3804,P06-1077,0,0.0605332,"Missing"
W10-3804,P07-1089,0,0.0319879,"Missing"
W10-3804,W08-0409,0,0.039275,"Missing"
W10-3804,W06-1606,0,0.0522068,"Missing"
W10-3804,P08-1114,0,0.0319459,"Missing"
W10-3804,P02-1040,0,0.0855271,"to automatically align them to the target words. Unaligned word 的 , 在 一 中 个 是 上 了 不 Frequency 77776 29051 9414 8768 8543 7471 7365 6155 5945 5450 Syntactic Constraints Number of distinct phrase pairs BLEU None 14,195,686 17.26 Full constraint 4,855,108 16.51 Selectively constraint 10,733,731 17.78 Table 5: Comparison of different constraints on phrase pair extraction by translation quality Table 4: Frequently unaligned words from the training corpus 4.2 settings were the identical in the three experiments. Table 5 summarizes the SMT performance. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002) which estimates the accuracy of translation output with respect to a set of reference translations. Experiments on Chinese-English SMT In order to confirm that it is advantageous to apply appropriate syntactic constraints on phrase extraction, we performed three translation experiments by using different ways of phrase extraction. In the first experiment, we used the method introduced in Section 2 to extract all possible phrase translation pairs without using any constraints arising from knowledge of syntax. The second experiment used source language syntactic constraints to filter out all no"
W10-3804,P05-1034,0,0.067201,"Missing"
W10-3804,P05-1069,0,0.0198178,"example, given a sentence pair and its word alignments shown in Figure1, the following nine phrase pairs will be extracted: Table 1: Phrase pairs extracted from the example in Figure 1 Note that neither the source phrase nor the not a legal phrase pair. Phrase pairs are extracted over the entire training corpus. Given all the collected phrase pairs, we can estimate the phrase translation probability distribution by relative frequency. The collected phrase pairs will also be used to 29 build the lexicalized reordering model. For more details of the lexicalized reordering model, please refer to Tillmann and Zhang (2005) and section 2.7.2 of the MOSES’s manual 1 . The main problem of such a phrase pair extraction procedure is the resulting phrase translation table is very large, especially when a large quantity of parallel data is available. This is not desirable in real application where speed and memory consumption are often critical concerns. In addition, some phrase translation pairs are generated from training data errors and word alignment noise. Therefore, we need to filter the phrase table in an appropriate way for both efficiency and translation quality (Johnson et al., 2007; Yang and Zheng, 2009). 3"
W10-3804,2009.mtsummit-papers.17,0,0.0779538,"Missing"
W10-3804,P09-1036,0,0.0378316,"Missing"
W10-3804,P09-2060,0,0.0164599,"illmann and Zhang (2005) and section 2.7.2 of the MOSES’s manual 1 . The main problem of such a phrase pair extraction procedure is the resulting phrase translation table is very large, especially when a large quantity of parallel data is available. This is not desirable in real application where speed and memory consumption are often critical concerns. In addition, some phrase translation pairs are generated from training data errors and word alignment noise. Therefore, we need to filter the phrase table in an appropriate way for both efficiency and translation quality (Johnson et al., 2007; Yang and Zheng, 2009). 3 Syntactic Constraints on Phrase Pair Extraction We can divide all the possible phrases into two types: syntactic phrases and non-syntactic phrases. A “syntactic phrase” is defined as a word sequence that is covered by a single subtree in a syntactic parse tree (Imamura, 2002). Intuitively, we would think syntactic phrases are much more reliable while the non-syntactic phrases are useless. However, (Koehn et al., 2003) showed that restricting phrasal translation to only syntactic phrases yields poor translation performance – the ability to translate nonsyntactic phrases (such as “there are”"
W10-3804,P08-1064,0,0.0285611,"Missing"
W10-3804,W06-3119,0,0.063016,"Missing"
W10-3804,D07-1103,0,\N,Missing
W10-3804,P09-1058,0,\N,Missing
W10-3804,P01-1067,0,\N,Missing
W10-3804,P07-2045,0,\N,Missing
W10-3804,D09-1024,0,\N,Missing
W10-3804,P06-1121,0,\N,Missing
W11-2601,2008.iwslt-papers.1,0,0.33914,"language resources available to train the statistical models. There are several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large-sized text data sets are readily available. However, for less frequently used language pairs, only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on multilingual SMT focuses on the use of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Koehn et al., 2009). Instead of a direct translation between two languages where only a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improve translation quality o"
W11-2601,J93-2003,0,0.0213426,"rce channel model, also called the n-gram transliteration model, is a joint probability model that captures information on how the source and target sentences can be generated simultaneously using transliteration pairs, i.e., the most likely sequence of source characters and target words according to a joint language model built from the co-segmentation from the Bayesian model. 4 (5) where p(src|trg) is called a translation model (T M ) and represents the generation probability from trg into src, and p(trg) is called a language model (LM ) and represents the likelihood of the target language (Brown et al., 1993). During the translation process (decoding), a score based on the statistical model probabilities is assigned to each translation hypothesis and the one that gives the highest probability is selected as the best translation. The translation quality of SMT approaches heavily depends on the amount and coverage of the bilingual language resources available to train the statistical models. In the context of dialect translation, where only few bilingual language resources (if any at all) are available for the dialect and the foreign language, only a relatively low translation quality can be obtaine"
W11-2601,P08-2006,0,0.0306173,"and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often insufficient. For example, the task of word segmentat"
W11-2601,D08-1033,0,0.0535578,"Missing"
W11-2601,2010.iwslt-papers.7,1,0.645429,"rsion of a Gibbs sampler for training, which is similar to that of (Mochihashi et al., 2009). We extended their forward filtering / backward sampling (FFBS) dynamic programing algorithm in order to deal with bilingual segmentations (see Algorithm 1). We found our sampler converged rapidly without annealing. The number of iterations was set by hand after observing the convergence behavior of the algorithm in pilot experiments. We used a value of 75 iterations through the corpus in all experiments reported in this paper. For more details on the Bayesian co-segmentation process, please refer to (Finch and Sumita, 2010). For the experiments reported in this paper, we implemented the joint-source channel model approach as a weighted finite state transducer (FST) using the OpenFst toolkit (Allauzen et al., 2007). The FST takes the sequence of dialect characters as its input and outputs the co-segmented bilingual segments from which the standard language segments are extracted. 2.3 Pivot-based SMT Recent research on speech translation focuses on corpus-based approaches, and in particular on statistical machine translation (SMT), which is a machine translation paradigm where translations are generated on the bas"
W11-2601,W05-0703,0,0.031257,"lects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often insufficient. For example, the task of word segmentation, i.e., the identification of word boundaries in continuous text, is one of the fundamental preprocessing steps of MT applications. In contrast to Indo-European languages like English, many Asian languages like Japanese do not use a whitespace character to separate meaningful word units. However, the application of a linguistically motivated standard language word segmentation tool to a dialect corpus results in a poor segmen"
W11-2601,W06-1108,0,0.0145447,"guage) is a dialect that is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect reso"
W11-2601,W10-2405,0,0.0150812,"ained on a large amount of bilingual data is applied to obtain high-quality foreign language translations as described in Section 2.3. 2.1 Bayesian Co-segmentation The method for mapping the dialect sentences into the standard language word segments is a direct character-to-character mapping between the languages. This process is known as transliteration. Many transliteration methods have previously been proposed, including methods based on stringsimilarity measures between character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric with respect to source and target language. Intuitively, the model has two basic components: a model for"
W11-2601,W03-0317,0,0.0363333,"a state-of-the-art phrase-based SMT system trained on a large amount of bilingual data is applied to obtain high-quality foreign language translations as described in Section 2.3. 2.1 Bayesian Co-segmentation The method for mapping the dialect sentences into the standard language word segments is a direct character-to-character mapping between the languages. This process is known as transliteration. Many transliteration methods have previously been proposed, including methods based on stringsimilarity measures between character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric with respect to source and target language. Intuiti"
W11-2601,P04-1021,0,0.270354,"tween character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric with respect to source and target language. Intuitively, the model has two basic components: a model for generating an outcome that has already been generated at least once before, and a second model that assigns a probability to an outcome that has not yet been produced. Ideally, to encourage the re-use of model parameters, the probability of generating a novel bilingual sequence pair should be considerably lower then the probability of generating a previously observed sequence pair. The probability distribution over these bilingual sequence pairs (including a"
W11-2601,P09-1012,0,0.0156796,"teration units. Then, an n-gram transliteration model is defined as the transliteration probability of a transliteration pair < l, s >k depending on its immediate n preceding transliteration pairs: P (σ, ω, γ) = K Y P (< l, s >k |< l, s >k−1 k−n+1 ) (4) k=1 In this equation, N is the total number of bilingual sequence pairs generated so far and N ((sk , tk )) is the number of times the sequence pair (sk , tk ) has occurred in the history. G0 and α are the base measure and concentration parameter as before. We used a blocked version of a Gibbs sampler for training, which is similar to that of (Mochihashi et al., 2009). We extended their forward filtering / backward sampling (FFBS) dynamic programing algorithm in order to deal with bilingual segmentations (see Algorithm 1). We found our sampler converged rapidly without annealing. The number of iterations was set by hand after observing the convergence behavior of the algorithm in pilot experiments. We used a value of 75 iterations through the corpus in all experiments reported in this paper. For more details on the Bayesian co-segmentation process, please refer to (Finch and Sumita, 2010). For the experiments reported in this paper, we implemented the join"
W11-2601,W97-1102,0,0.0712968,"to a foreign language. A standard dialect (or standard language) is a dialect that is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguis"
W11-2601,W10-2408,0,0.0192295,"input and the translation models. In the second step, a state-of-the-art phrase-based SMT system trained on a large amount of bilingual data is applied to obtain high-quality foreign language translations as described in Section 2.3. 2.1 Bayesian Co-segmentation The method for mapping the dialect sentences into the standard language word segments is a direct character-to-character mapping between the languages. This process is known as transliteration. Many transliteration methods have previously been proposed, including methods based on stringsimilarity measures between character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric wit"
W11-2601,J03-1002,0,0.00720165,"Missing"
W11-2601,P02-1040,0,0.0804258,"Missing"
W11-2601,N09-2056,1,0.856333,"nly a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improve translation quality of certain language pairs, especially when translating from or into Asian languages (Paul et al., 2009). This paper focuses on the translation of dialects, i.e., a variety of a language that is characteristic of a particular group of the language’s speakers, into a foreign language. A standard dialect (or standard language) is a dialect that is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–"
W11-2601,2010.amta-papers.5,0,0.0321811,"distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often insufficient. For example, the task of word segmentation, i.e., the identification of word boundaries in continuous text, is one of the fundamental preprocessing steps of MT applications. In contrast to Indo-European languages like English, many Asian languages like Japanese do not use a whitespace character to separate meaningful word units. However, the application of a linguistically motivated standard language word segmentation tool to a dialect corpus results in a poor segmentation quality"
W11-2601,P07-3010,0,0.0202508,"t is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often in"
W11-2601,N07-1061,0,0.180595,"ds on the amount and coverage of the bilingual language resources available to train the statistical models. There are several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large-sized text data sets are readily available. However, for less frequently used language pairs, only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on multilingual SMT focuses on the use of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Koehn et al., 2009). Instead of a direct translation between two languages where only a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot l"
W11-2601,P07-1108,0,0.202943,"ge of the bilingual language resources available to train the statistical models. There are several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large-sized text data sets are readily available. However, for less frequently used language pairs, only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on multilingual SMT focuses on the use of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Koehn et al., 2009). Instead of a direct translation between two languages where only a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improv"
W11-2601,C08-1128,0,0.022935,"hen the sequence itself is generated given the length. Note that this model is able to assign a probability to arbitrary bilingual sequence pairs of any length in the source and target sequence, but favors shorter sequences in both. The generative model is given in Equation 3. The equation assigns a probability to the k th bilingual sequence pair (sk , tk ) in a derivation of the corpus, given all of the other sequence pairs in the history so far (s−k , t−k ). Here −k is read as: “up to but not including k”. p((sk , tk ))|(s−k , t−k )) N ((sk , tk )) + αG0 ((sk , tk )) = N +α (3) 3 Following (Xu et al., 2008), we assign the parameters λs , λt and α, the values 2, 2 and 0.3 respectively. Input: Random initial corpus segmentation Output: Unsupervised co-segmentation of the corpus according to the model foreach iter=1 to NumIterations do foreach bilingual word-pair w ∈ randperm(W) do foreach co-segmentation γi of w do Compute probability p(γi |h) where h is the set of data (excluding w) and its hidden co-segmentation end Sample a co-segmentation γi from the distribution p(γi |h) Update counts end end Algorithm 1: Blocked Gibbs Sampling Suppose that we have a dialect sentence σ = l1 l2 . . . lL and a"
W11-2601,2009.mtsummit-papers.7,0,\N,Missing
W11-3203,P04-1021,0,0.233247,"ase-based translation model lacks contextual information, and in the experiments of (Finch and Sumita, 2010a), the model gained this contextual information implicitly by the use of agglomerated phrases. In other words, Introduction In the NEWS2010 workshop, (Finch and Sumita, 2010b) reported that the performance of a phrasebased statistical machine transliteration system (Finch and Sumita, 2008; Rama and Gali, 2009) could be improved significantly by combining it with a model based on the n-gram context of source-target grapheme sequence pairs: a joint source-channel model similar to that of (Li et al., 2004). Their system integrated the two approaches by using a re-scoring step at the end of the decoding process. Our system goes one step further and integrates a joint source-channel model directly into the SMT decoder to allow the probabilities from it to be taken into account within a single search process in the similar manner to (Banchs et al., 2005). 23 Proceedings of the 2011 Named Entities Workshop, IJCNLP 2011, pages 23–27, Chiang Mai, Thailand, November 12, 2011. the target language model to generate derivations that are too short. the longer phrases carried with them their own built-in c"
W11-3203,2005.mtsummit-papers.36,0,0.0302575,"ne transliteration system (Finch and Sumita, 2008; Rama and Gali, 2009) could be improved significantly by combining it with a model based on the n-gram context of source-target grapheme sequence pairs: a joint source-channel model similar to that of (Li et al., 2004). Their system integrated the two approaches by using a re-scoring step at the end of the decoding process. Our system goes one step further and integrates a joint source-channel model directly into the SMT decoder to allow the probabilities from it to be taken into account within a single search process in the similar manner to (Banchs et al., 2005). 23 Proceedings of the 2011 Named Entities Workshop, IJCNLP 2011, pages 23–27, Chiang Mai, Thailand, November 12, 2011. the target language model to generate derivations that are too short. the longer phrases carried with them their own built-in context. In our model these contextual dependencies are made explicit and modeled directly by the joint source-channel model. The termination condition for our Bayesian cosegmentation algorithm was set based on pilot experiments that showed very little gain in system performance after iteration 10, and no loss in performance by continuing the training"
W11-3203,J03-1002,0,0.00277059,"xt of graphemes and grapheme sequences in both source and target languages. The segmentation for our approach was performed using a non-parametric Bayesian co-segmentation model, and in this paper we present experiments comparing the effectiveness of this segmentation relative to the publicly available state-of-the-art m2m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment. 1 The typical method of deriving a translation-model for a machine translation is to use GIZA++ (Och and Ney, 2003) to perform word alignment and a set of heuristics for phrase-pair extraction. A commonly used set of heuristics is known as growdiag-final-and. This type of approach was taken by (Finch and Sumita, 2010b; Rama and Gali, 2009) to train their models. An alternative approach is to use a nonparametric Bayesian technique to co-segment both source and target in a single step (Finch and Sumita, 2010a; Huang et al., 2011). This approach has the advantage of being symmetric with respect to source and target languages, and furthermore Bayesian techniques tend to give rise to models with few parameters"
W11-3203,P03-1021,0,0.00383897,"ource-channel model was trained from the Viterbi co-segmentation arising from the final iteration of the Bayesian segmentation process on the training data (for model used in parameter tuning), and the training data added to the development data (for the model used to decode the test data). We used the MIT language modeling toolkit (Bo-june et al., 2008) with modified Knesser-Ney smoothing to build this model. In all experiments we used a language model of order 5. The exponential log-linear model weights of our system are set by tuning the system on development data using the MERT procedure (Och, 2003) by means of the publicly available ZMERT toolkit 1 (Zaidan, 2009). The systems reported in this paper used a metric based on the word-level F-score, an official evaluation metric for the shared tasks, which measures the relationship of the longest common subsequence of the transliteration pair to the lengths of both source and target sequences. 2.2.2 2.4 Official Results Target Language model The target model was trained from target side of the training data (for model used in parameter tuning), and the training data added to the development data (for the model used to decode the test data)."
W11-3203,I08-8003,1,0.657881,"ramework as (Finch and Sumita, 2010a), and replace the agglomeration heuristics by incorporating a joint source-channel model directly into the decoder as an additional feature. Our motivation for this was simply that the phrase-based translation model lacks contextual information, and in the experiments of (Finch and Sumita, 2010a), the model gained this contextual information implicitly by the use of agglomerated phrases. In other words, Introduction In the NEWS2010 workshop, (Finch and Sumita, 2010b) reported that the performance of a phrasebased statistical machine transliteration system (Finch and Sumita, 2008; Rama and Gali, 2009) could be improved significantly by combining it with a model based on the n-gram context of source-target grapheme sequence pairs: a joint source-channel model similar to that of (Li et al., 2004). Their system integrated the two approaches by using a re-scoring step at the end of the decoding process. Our system goes one step further and integrates a joint source-channel model directly into the SMT decoder to allow the probabilities from it to be taken into account within a single search process in the similar manner to (Banchs et al., 2005). 23 Proceedings of the 2011"
W11-3203,W09-3528,0,0.151423,"aring the effectiveness of this segmentation relative to the publicly available state-of-the-art m2m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment. 1 The typical method of deriving a translation-model for a machine translation is to use GIZA++ (Och and Ney, 2003) to perform word alignment and a set of heuristics for phrase-pair extraction. A commonly used set of heuristics is known as growdiag-final-and. This type of approach was taken by (Finch and Sumita, 2010b; Rama and Gali, 2009) to train their models. An alternative approach is to use a nonparametric Bayesian technique to co-segment both source and target in a single step (Finch and Sumita, 2010a; Huang et al., 2011). This approach has the advantage of being symmetric with respect to source and target languages, and furthermore Bayesian techniques tend to give rise to models with few parameters that do not overfit the data in the same way as traditional maximum likelihood training. In experiments on an EnglishJapanese transliteration task, (Finch and Sumita, 2010a) showed that that a Bayesian approach offered higher"
W11-3203,2010.iwslt-papers.7,1,0.667333,"present experiments comparing the effectiveness of this segmentation relative to the publicly available state-of-the-art m2m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment. 1 The typical method of deriving a translation-model for a machine translation is to use GIZA++ (Och and Ney, 2003) to perform word alignment and a set of heuristics for phrase-pair extraction. A commonly used set of heuristics is known as growdiag-final-and. This type of approach was taken by (Finch and Sumita, 2010b; Rama and Gali, 2009) to train their models. An alternative approach is to use a nonparametric Bayesian technique to co-segment both source and target in a single step (Finch and Sumita, 2010a; Huang et al., 2011). This approach has the advantage of being symmetric with respect to source and target languages, and furthermore Bayesian techniques tend to give rise to models with few parameters that do not overfit the data in the same way as traditional maximum likelihood training. In experiments on an EnglishJapanese transliteration task, (Finch and Sumita, 2010a) showed that that a Bayesian a"
W11-3203,W10-2406,1,0.87351,"present experiments comparing the effectiveness of this segmentation relative to the publicly available state-of-the-art m2m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment. 1 The typical method of deriving a translation-model for a machine translation is to use GIZA++ (Och and Ney, 2003) to perform word alignment and a set of heuristics for phrase-pair extraction. A commonly used set of heuristics is known as growdiag-final-and. This type of approach was taken by (Finch and Sumita, 2010b; Rama and Gali, 2009) to train their models. An alternative approach is to use a nonparametric Bayesian technique to co-segment both source and target in a single step (Finch and Sumita, 2010a; Huang et al., 2011). This approach has the advantage of being symmetric with respect to source and target languages, and furthermore Bayesian techniques tend to give rise to models with few parameters that do not overfit the data in the same way as traditional maximum likelihood training. In experiments on an EnglishJapanese transliteration task, (Finch and Sumita, 2010a) showed that that a Bayesian a"
W11-3203,P11-2094,0,0.0579273,"h. Each of the language pairs were processed automatically with no special treatment. 1 The typical method of deriving a translation-model for a machine translation is to use GIZA++ (Och and Ney, 2003) to perform word alignment and a set of heuristics for phrase-pair extraction. A commonly used set of heuristics is known as growdiag-final-and. This type of approach was taken by (Finch and Sumita, 2010b; Rama and Gali, 2009) to train their models. An alternative approach is to use a nonparametric Bayesian technique to co-segment both source and target in a single step (Finch and Sumita, 2010a; Huang et al., 2011). This approach has the advantage of being symmetric with respect to source and target languages, and furthermore Bayesian techniques tend to give rise to models with few parameters that do not overfit the data in the same way as traditional maximum likelihood training. In experiments on an EnglishJapanese transliteration task, (Finch and Sumita, 2010a) showed that that a Bayesian approach offered higher performance than using GIZA++ together with heuristic phrase-pair extraction. Their approach unfortunately required a simple set of agglomeration heuristics in order get good performance from"
W11-3203,N07-1047,0,0.0537841,"rding to the correct linguistic readings of the kanji. We investigate this further in the next section. shown (Finch and Sumita, 2010a) that in transliteration, this Bayesian approach can give rise to a smaller and more useful phrase-table than that derived by using GIZA++ for alignment and the grow-diag-final-and heuristics which have been shown to be effective for transliteration (Rama and Gali, 2009). In these experiments we compare the Bayesian segmenter to a similar state-of-the-art segmentation tool that is capable of many-to-many alignments: the publicly available m2m alignment tool 2 (Jiampojamarn et al., 2007) that is trained using the EM algorithm and is based on the principles set out in (Ristad and Yianilos, 1998). We used a similar system to that in the shared task, but without the maximum entropy model. The experiments were run in the same way using the same script, the only difference being the choice of aligner used. We used data from the 2009 NEWS workshop for our experiments, and evaluated using the F-score metric used for the shared task evaluation. The aligners were run with their default settings, and with the same limits for source and target segment size. It may have been possible to"
W11-3203,P07-2045,0,0.00954349,"es are made explicit and modeled directly by the joint source-channel model. The termination condition for our Bayesian cosegmentation algorithm was set based on pilot experiments that showed very little gain in system performance after iteration 10, and no loss in performance by continuing the training. We arbitrarily chose iteration 30 in all our experiments as the final iteration. The decoding was performed using a specially modified version of the CLEOPATRA decoder (Finch et al., 2007), an in-house multi-stack phrase-based decoder that operates on the same principles as the MOSES decoder (Koehn et al., 2007). The system we used in this shared task is a log-linear combination of 5 different models, the following sections describe each of these models in detail. Due to the small size of many of the data sets in the shared tasks, we used all of the data to build models for the final systems. 2.2.4 Maximum-entropy model In a typical phrase-based SMT system, the translation model contains a context-independent probability of the target grapheme sequence (phrase) given the source. Our system replaces this with a more sophisticated maximum entropy model that takes the local context of source and target"
W11-3203,D08-1076,0,\N,Missing
W11-3203,2007.iwslt-1.15,1,\N,Missing
W12-4406,P11-2094,0,\N,Missing
W12-4406,P07-1081,0,\N,Missing
W12-4406,2010.iwslt-papers.7,1,\N,Missing
W12-4406,P03-1021,0,\N,Missing
W12-4406,W11-3203,1,\N,Missing
W14-5503,P06-1085,0,0.0349653,"odel The generative model is given in Equation 3 below. The equation assignes a probability to the k th segment sk in a derivation of the corpus, given all of the other segments in the history so far s−k . Here −k is read as: “up to but not including k”. p(sk |s−k ) = N (sk ) + αG0 (sk ) N +α (3) In this equation, N is the total number of segments generated so far, N (sk ) is the number of times the segment sk has occurred in the history. G0 and α are the base measure and concentration parameter as before. 3.1.3 Bayesian Inference We used a blocked version of a Gibbs sampler for training. In (Goldwater et al., 2006) they report issues with mixing in the sampler that were overcome using annealing. In (Mochihashi et al., 2009) this issue was overcome by using a blocked sampler together with a dynamic programming approach. Our algorithm is an extension of application the forward filtering backward sampling (FFBS) algorithm (Scott, 2002) to the problem of word segmentation presented in (Mochihashi et al., 2009). We extend their approach to handle the joint segmentation and alignment of character sequences. We refer the reader to (Mochihashi et al., 2009) for a complete description of the FFBS process. In ess"
W14-5503,I08-7006,0,0.116272,"mar, there is no freely available corpus and dictionary based or rule based methods are being used as a temporary solution. If we only focus on Myanmar language word segmentation, as far as the authors are aware there have been only two published methodologies, and one study. Both of the proposed methodologies operate according using a process of syllable breaking followed by Maximum Matching; the differences in the approaches come from the manner in which the segmentation boundary decision is made. In (Thet et al., 2008) statistical information is used (based on bigram information), whereas (Htay and Murthy, 2008) utilize a word list extracted from a monolingual Myanmar corpus. In a related study (Thu et al., 2013a), various Myanmar word segmentation approaches including character segmentation, syllable segmentation, human lexical/phrasal segmentation, unsupervised and semi-supervised word segmentation, were investigated. They reported that the highest quality machine translation was attained either without word segmentation using simply sequences of syllables, or by a process of Maximum Matching with a monolingual dictionary. In this study the effectiveness of approaches unsupervised word segmentation"
W14-5503,O91-1004,0,0.433216,"Missing"
W14-5503,P98-2206,0,0.104297,"hop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 20–27, Dublin, Ireland, August 23-29 2014. 2 Related Work In this section, we will briefly introduce some proposed word segmentation methods with an emphasis on the schemes that have been applied to Myanmar. Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chinese and Japanese languages. These methods can be roughly classified into dictionary-based (Sornlertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993; Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In dictionary-based methods, only words that are stored in the dictionary can be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they requir"
W14-5503,P09-1012,0,0.122441,"ational Conference on Computational Linguistics, pages 20–27, Dublin, Ireland, August 23-29 2014. 2 Related Work In this section, we will briefly introduce some proposed word segmentation methods with an emphasis on the schemes that have been applied to Myanmar. Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chinese and Japanese languages. These methods can be roughly classified into dictionary-based (Sornlertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993; Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In dictionary-based methods, only words that are stored in the dictionary can be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they require large amounts of data; the processing time require"
W14-5503,H94-1054,0,0.265939,"Missing"
W14-5503,J00-3004,0,0.023792,"n be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they require large amounts of data; the processing time required; and the difficulty in incorporating linguistic knowledge effectively into the segmentation process (Teahan et al., 2000). For low-resource languages such as Myanmar, there is no freely available corpus and dictionary based or rule based methods are being used as a temporary solution. If we only focus on Myanmar language word segmentation, as far as the authors are aware there have been only two published methodologies, and one study. Both of the proposed methodologies operate according using a process of syllable breaking followed by Maximum Matching; the differences in the approaches come from the manner in which the segmentation boundary decision is made. In (Thet et al., 2008) statistical information is used"
W14-5503,H01-1057,0,0.0397598,"d from the data, as to whether the segment would be generated from the unsupervised sub-model or the dictionary sub-model. In this model, the decision to generate from the dictionary model is refined into a number of decisions to generate from a number of subsets of the dictionary, each with its own probability. These probabilities were re-estimated from the sampled segmentation of the corpus at the end of each iteration of the training (in a similar manner to the dictionary augmented model). A diagram showing the generative process is shown in Figure 1. 3.4 Language Model Augmented Model In (Theeramunkong and Usanavasin, 2001) dictionary approaches were deliberately avoided in order to address issues with unknown words. Instead a decision tree model for segmentation was proposed. Our approach although different in character (since a generative model is used), shares the insight that knowledge of how words are constructed is key to segmentation when dictionary information is absent. In this model we used the dictionary resource, but in a more indirect manner. We use a language model to capture the notion of exactly what constitutes a segment. To do this words in the dictionary were first segmented into syllables. Th"
W14-5503,1985.tmi-1.4,0,0.361798,"Missing"
W14-5503,C98-2201,0,\N,Missing
W15-3909,P04-1021,0,\N,Missing
W15-3909,P02-1040,0,\N,Missing
W15-3909,P07-1081,0,\N,Missing
W15-3909,W14-4012,0,\N,Missing
W15-3909,W12-4401,0,\N,Missing
W15-3909,2010.iwslt-papers.7,1,\N,Missing
W15-3909,2007.iwslt-1.15,1,\N,Missing
W15-3909,D08-1076,0,\N,Missing
W15-3909,W11-3203,1,\N,Missing
W16-2711,I08-8003,1,0.781894,"Agtarbidir. 1 • A target-bidirectional agreement model was employed. • Ensembles of neural networks were used rather than just a single network. • The ensembles were selected from different training runs and different training epochs according to their performance on development (and test) data. Introduction Our primary system for the NEWS shared evaluation on transliteration generation is different in character from all our previous systems. In past years, all our systems have been based on phrase-based statistical machine translation (PBSMT) techniques, stemming from the system proposed in (Finch and Sumita, 2008). This year’s system is a pure end-to-end neural network transducer. In (Finch et al., 2012) auxiliary neural network language models (both monolingual and bilingual (Li et al., 2004)) were introduced as features to augment the log-linear model of a phrasebased transduction system, and led to modest gains in system performance. In the NEWS 2015 workshop (Finch et al., 2015) neural transliteration systems using attention-based sequence-to-sequence neural network transducers (Bahdanau et al., 2014) were applied to transliteration generation. In isolation, the performance was found to be lower th"
W16-2711,W12-4406,1,0.904887,"Missing"
W16-2711,W15-3909,1,0.851947,"eneration is different in character from all our previous systems. In past years, all our systems have been based on phrase-based statistical machine translation (PBSMT) techniques, stemming from the system proposed in (Finch and Sumita, 2008). This year’s system is a pure end-to-end neural network transducer. In (Finch et al., 2012) auxiliary neural network language models (both monolingual and bilingual (Li et al., 2004)) were introduced as features to augment the log-linear model of a phrasebased transduction system, and led to modest gains in system performance. In the NEWS 2015 workshop (Finch et al., 2015) neural transliteration systems using attention-based sequence-to-sequence neural network transducers (Bahdanau et al., 2014) were applied to transliteration generation. In isolation, the performance was found to be lower than that of the phrase-based system on all of the In all our experiments we have taken a strictly language independent approach. Each of the language pairs was processed automatically from the character sequence representation supplied for the shared tasks, with no language specific treatment for any of the language pairs. Furthermore no preprocessing was performed on any of"
W16-2711,P07-1081,0,0.524071,"Missing"
W16-2711,P04-1021,0,0.182904,"training runs and different training epochs according to their performance on development (and test) data. Introduction Our primary system for the NEWS shared evaluation on transliteration generation is different in character from all our previous systems. In past years, all our systems have been based on phrase-based statistical machine translation (PBSMT) techniques, stemming from the system proposed in (Finch and Sumita, 2008). This year’s system is a pure end-to-end neural network transducer. In (Finch et al., 2012) auxiliary neural network language models (both monolingual and bilingual (Li et al., 2004)) were introduced as features to augment the log-linear model of a phrasebased transduction system, and led to modest gains in system performance. In the NEWS 2015 workshop (Finch et al., 2015) neural transliteration systems using attention-based sequence-to-sequence neural network transducers (Bahdanau et al., 2014) were applied to transliteration generation. In isolation, the performance was found to be lower than that of the phrase-based system on all of the In all our experiments we have taken a strictly language independent approach. Each of the language pairs was processed automatically"
W16-4613,N12-1048,0,0.152918,"Missing"
W16-4613,2014.iwslt-papers.8,1,0.818639,"ines output steams of tokens which are split by long pauses that may contain a few sentences, a random number (from 1 to 10) of sentences were concatenated to form the input. After segmentation using the proposed methods, punctuation was inserted into the sentences with a hidden N-gram model model (Stolcke et al., 1998; Matusov et al., 2006) prior to translation. In (Anonymous, 2016), this method was shown to be the most effective strategy for the translation of unpunctuated text. The time efficiency of segmenters were measured by average latency per source word using the definition given in (Finch et al., 2014). The quality of segmenters were measured by the BLEU of end-to-end translation, and because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The parameters (all of the θ’s in the ‘Parameters’ column in Table 2) were set by grid search to maximize the BLEU score on the development set. 5-gram interpolated modified Kneser-Ney smoothed language models were used to calculate the confidence. These were trained on the training corpus using the SRILM (Sto"
W16-4613,2015.iwslt-evaluation.9,0,0.106224,"Missing"
W16-4613,2005.mtsummit-papers.11,0,0.0404732,"imental corpus was a union of corpora from multiple sources, including shared tasks such as the Basic Travel Expression Corpus (Takezawa et al., 2002), the NTCIR Patent Machine Translation Corpus (Goto et al., 2013), crawled web data and several in-house parallel resources. Table 1 shows the statistics of sentences and words in the training, development and test sets. The corpora were pre-processed using standard procedures for MT. The Japanese text was segmented into words using Mecab (Kudo, 2005). The English text was tokenized with the tokenization script released with the Europarl corpus (Koehn, 2005) and converted to lowercase. Two treatments were applied to the development and test sets in order to simulate the output from ASR engines. First, because ASR engines normally do not output punctuation, punctuation was removed. Second, because ASR engines output steams of tokens which are split by long pauses that may contain a few sentences, a random number (from 1 to 10) of sentences were concatenated to form the input. After segmentation using the proposed methods, punctuation was inserted into the sentences with a hidden N-gram model model (Stolcke et al., 1998; Matusov et al., 2006) prior"
W16-4613,D10-1018,0,0.31642,"Missing"
W16-4613,2005.iwslt-1.19,0,0.666287,"m model model (Stolcke et al., 1998; Matusov et al., 2006) prior to translation. In (Anonymous, 2016), this method was shown to be the most effective strategy for the translation of unpunctuated text. The time efficiency of segmenters were measured by average latency per source word using the definition given in (Finch et al., 2014). The quality of segmenters were measured by the BLEU of end-to-end translation, and because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The parameters (all of the θ’s in the ‘Parameters’ column in Table 2) were set by grid search to maximize the BLEU score on the development set. 5-gram interpolated modified Kneser-Ney smoothed language models were used to calculate the confidence. These were trained on the training corpus using the SRILM (Stolcke and others, 2002) tools. The machine translation system was an in-house phrasebased system that pre-ordered the input. 4.2 Experimental Results The performance of the interpretation systems using different sentence segmenters is presented in Table 2. The following observations can"
W16-4613,2006.iwslt-papers.1,0,0.207774,"roparl corpus (Koehn, 2005) and converted to lowercase. Two treatments were applied to the development and test sets in order to simulate the output from ASR engines. First, because ASR engines normally do not output punctuation, punctuation was removed. Second, because ASR engines output steams of tokens which are split by long pauses that may contain a few sentences, a random number (from 1 to 10) of sentences were concatenated to form the input. After segmentation using the proposed methods, punctuation was inserted into the sentences with a hidden N-gram model model (Stolcke et al., 1998; Matusov et al., 2006) prior to translation. In (Anonymous, 2016), this method was shown to be the most effective strategy for the translation of unpunctuated text. The time efficiency of segmenters were measured by average latency per source word using the definition given in (Finch et al., 2014). The quality of segmenters were measured by the BLEU of end-to-end translation, and because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The parameters (all of the θ’s in t"
W16-4613,P14-2090,0,0.0468592,"k based on confidence scores, denoted as bi . The final output is a segmented sentence, e.g. w0 , · · · , wi . The proposed segmenters work in an online manner as follows: words are input one by one. The sequence of input words and the derived confidence scores are maintained as states. Once an word is input, its confidence score is calculated and added into the sequence (which is labeled as a in Figure 2). Then a segmentation strategy is applied to the sequence (labeled as (b) in Figure 2). In case that the 1 Fujita et al. (2013)’s method may work on word streams without sentence boundaries; Oda et al. (2014)s’ segmentation model uses linear SVMs and local features extracted from just three word lookahead, so it might be adapted. 141 Algorithm 1 Online Sentence Segmenter Require: w0 , w1 , w2 , . . ., 1: W ← []; S ← [] 2: for wk in stream of words do 3: W ← W + [wk ] 4: sk−1 ← confidence of segmenting before wk 5: S ← S + [sk−1 ] 6: B ← apply segmentation strategy to S 7: if bi = 1 (0 ≤ i ≤ k − 1) then 8: output [w0 , w1 , . . . , wi ] as a segment 9: remove first i elements from W and S 10: end if 11: end for ⊲ assume W = [w0 , w1 , . . . , wk−1 , wk ] ⊲ assume S = [s0 , s1 , . . . , sk−1 ] ⊲ ass"
W16-4613,P02-1040,0,0.101875,"and MT, yet most of them require a long context of future words that follow sentence boundaries. In addition, they are often computationally expensive. These shortages make them unattractive for use in simultaneous interpretation. To the best of our knowledge, there are no published ready-to-use online sentence segmenters, and this motivated this paper. The proposed method is crafted in a way that requires little computation and minimum future words in order to achieve efficiency. Also the proposed method is directly optimized against the widely used measurement of translation quality – BLEU (Papineni et al., 2002) – in order to achieve effectiveness. We believe that this work can directly contribute to the development of real-world simultaneous interpretation systems. The main contributions of this paper are, • proposing a segment boundary confidence score; 139 Proceedings of the 3rd Workshop on Asian Translation, pages 139–148, Osaka, Japan, December 11-17 2016. Figure 1: Illustration of Online Sentence Segmenter in Simultaneous Interpretation System • proposing a hybrid online sentence segmenter; • an empirical study and analysis of the proposed method on two translation tasks. The rest of this paper"
W16-4613,2011.iwslt-papers.7,0,0.0928464,"tegy is not only efficient in terms of average latency per word, but also achieved end-to-end translation quality close to an offline baseline, and close to oracle segmentation. 1 Introduction Simultaneous interpretation performs spoken language translation in a online manner. A spoken language translation system automatically translates text from an automatic speech recognition (ASR) system into another language. Spoken language translation itself is an important application of machine translation (MT) because it takes one of the most natural forms of human communication – speech – as input (Peitz et al., 2011). Simultaneous interpretation is even more demanding than spoken language translation because the processing must occur online. Simultaneous interpretation can bridge the language gap in people’s daily lives transparently because of its ability to respond immediately to users’ speech input. Simultaneous interpretation systems recognize and translate speech at the same time the speakers are speaking, thus the audience can hear the translation and catch the meaning without delay. Potential applications of simultaneous interpretation include interpreting speeches and supporting cross-lingual conv"
W16-4613,takezawa-etal-2002-toward,1,0.387634,"95,054 103,638 95,176 Table 1: Experimental Corpora.† Including punctuations. 4 Experiments 4.1 Experimental Settings Experiments were performed on translation between Japanese and English in both directions. The word orders of these two languages are very different, thus long-distance reordering is often obligatory during translation. This makes simultaneous interpretation a very challenging task, and therefore we choose this language pair for experiments. The experimental corpus was a union of corpora from multiple sources, including shared tasks such as the Basic Travel Expression Corpus (Takezawa et al., 2002), the NTCIR Patent Machine Translation Corpus (Goto et al., 2013), crawled web data and several in-house parallel resources. Table 1 shows the statistics of sentences and words in the training, development and test sets. The corpora were pre-processed using standard procedures for MT. The Japanese text was segmented into words using Mecab (Kudo, 2005). The English text was tokenized with the tokenization script released with the Europarl corpus (Koehn, 2005) and converted to lowercase. Two treatments were applied to the development and test sets in order to simulate the output from ASR engines"
W16-4613,I13-1141,0,0.0397647,"Missing"
W18-2701,W18-2713,0,0.0259017,"LSTMbased encoder-decoders with attention (Bahdanau et al., 2015). 3.4 Submitted Systems Four teams, Team Amun, Team Marian, Team OpenNMT, and Team NICT submitted to the shared task, and we will summarize each below. Before stepping in to the details of each system, we first note general trends that all or many systems attempted. The first general trend was a fast C++ decoder, with Teams Amun, Marian, and NICT using the Amun or Marian decoders included in the Marian toolkit,4 and team OpenNMT 4 5 https://marian-nmt.github.io 3 http://opennmt.net 3.4.1 3.4.4 Team Amun Team NICT’s contribution (Imamura and Sumita, 2018) to the shared task was centered around using self-training as a way to improve NMT accuracy without changing the architecture. Specifically, they used a method of randomly sampling pseudo-source sentences from a back-translation model (Imamura et al., 2018) and used this to augment the data set to increase coverage. They tested two basic architectures for the actual translation model, a recurrent neural network-based model trained using OpenNMT, and a self-attentional model trained using Marian, finally submitting the self-attentional model using Marian as their sole contribution to the share"
W18-2701,W18-2716,0,0.0254636,"xamination for future shared tasks. Next, considering memory usage, we can see again that the submissions from the Marian team tend to be the most efficient. One exception is the extremely small memory system OpenNMT-Tiny, which achieves significantly lower translation accuracies, but fits in a mere 220MB of memory on the CPU. In this first iteration of the task, we attempted to establish best practices and strong baselines upon which to build efficient test-time methods for NMT. One characteristic of the first iteration of the task was that the basic model architectures Team Marian’s system (Junczys-Dowmunt et al., 2018) used the Marian C++ decoder, and concentrated on new optimizations for the CPU. The team distilled a large self-attentional model into two types of “student” models: a smaller self-attentional model using average attention networks (Zhang et al., 2018), a new higherspeed version of the original Transformer model (Vaswani et al., 2017), and a standard RNN-based decoder. They also introduced an auto-tuning approach that chooses which of multiple matrix multiplication implementations is most efficient in the current context, then uses this implementation going forward. This resulted in the Maria"
W18-2701,W18-2708,0,0.0238604,"ization methods for adaptation (Khayrallah et al., 2018) and “extreme adaptation” to individual speakers (Michel and Neubig, 2018) Accuracy Measures: As a measure of translation accuracy, we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores. Data augmentation: A number of the contributed papers examined ways to augment data for more efficient training. These include methods for considering multiple back translations (Imamura et al., 2018), iterative back translation (Hoang et al., 2018b), bidirectional multilingual training (Niu et al., 2018), and document level adaptation (Kothur et al., 2018) Computational Efficiency Measures: We measured the amount of time it takes to translate the entirety of the test set on CPU or GPU. Time for loading models was measured by having the model translate an empty file, then subtracting this from the total time to translate the test set file. Inadequate resources: Several contributions involved settings in which resources were insufficient, such as investigating the impact of noise (Khayrallah and Koehn, 2018), missing data in multi-source settings (Nishimura et al., 2018) and one-shot learning (Pham et al., 2018). Memory Efficiency Measures: We me"
W18-2701,N18-2078,0,0.0233178,"r Computational Linguistics Translation (Nakazawa et al., 2017)) was focused on creating systems for NMT that are not only accurate, but also efficient. Efficiency can include a number of concepts, including memory efficiency and computational efficiency. This task concerns itself with both, and we cover the detail of the evaluation below. model research, with the contributions being concentrated on the following topics: Linguistic structure: How can we incorporate linguistic structure in neural MT or generation models? Contributions examined the effect of considering semantic role structure (Marcheggiani et al., 2018), latent structure (Bastings et al., 2018), and structured self-attention (Bisk and Tran, 2018). 3.1 The first step to the evaluation was deciding what we want to measure. In the case of the shared task, we used metrics to measure several different aspects connected to how good the system is. These were measured for systems that were run on CPU, and also systems that were run on GPU. Domain adaptation: Some contributions examined regularization methods for adaptation (Khayrallah et al., 2018) and “extreme adaptation” to individual speakers (Michel and Neubig, 2018) Accuracy Measures: As a meas"
W18-2701,D15-1199,0,0.0577417,"Missing"
W18-2701,P18-2050,1,0.824817,"g semantic role structure (Marcheggiani et al., 2018), latent structure (Bastings et al., 2018), and structured self-attention (Bisk and Tran, 2018). 3.1 The first step to the evaluation was deciding what we want to measure. In the case of the shared task, we used metrics to measure several different aspects connected to how good the system is. These were measured for systems that were run on CPU, and also systems that were run on GPU. Domain adaptation: Some contributions examined regularization methods for adaptation (Khayrallah et al., 2018) and “extreme adaptation” to individual speakers (Michel and Neubig, 2018) Accuracy Measures: As a measure of translation accuracy, we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores. Data augmentation: A number of the contributed papers examined ways to augment data for more efficient training. These include methods for considering multiple back translations (Imamura et al., 2018), iterative back translation (Hoang et al., 2018b), bidirectional multilingual training (Niu et al., 2018), and document level adaptation (Kothur et al., 2018) Computational Efficiency Measures: We measured the amount of time it takes to translate the entirety of the t"
W18-2701,P18-1166,0,0.0179704,"lation accuracies, but fits in a mere 220MB of memory on the CPU. In this first iteration of the task, we attempted to establish best practices and strong baselines upon which to build efficient test-time methods for NMT. One characteristic of the first iteration of the task was that the basic model architectures Team Marian’s system (Junczys-Dowmunt et al., 2018) used the Marian C++ decoder, and concentrated on new optimizations for the CPU. The team distilled a large self-attentional model into two types of “student” models: a smaller self-attentional model using average attention networks (Zhang et al., 2018), a new higherspeed version of the original Transformer model (Vaswani et al., 2017), and a standard RNN-based decoder. They also introduced an auto-tuning approach that chooses which of multiple matrix multiplication implementations is most efficient in the current context, then uses this implementation going forward. This resulted in the MarianTinyRNN system using an RNN-based model, and the Marian-Trans-Small-AAN, MarianTrans-Base-AAN, Marian-Trans-Big, MarianTrans-Big-int8 systems, which use different varieties and sizes of self-attentional models. 3.4.3 Team NICT Team OpenNMT Team OpenNMT"
W18-2701,W18-2712,0,0.0187161,", and document level adaptation (Kothur et al., 2018) Computational Efficiency Measures: We measured the amount of time it takes to translate the entirety of the test set on CPU or GPU. Time for loading models was measured by having the model translate an empty file, then subtracting this from the total time to translate the test set file. Inadequate resources: Several contributions involved settings in which resources were insufficient, such as investigating the impact of noise (Khayrallah and Koehn, 2018), missing data in multi-source settings (Nishimura et al., 2018) and one-shot learning (Pham et al., 2018). Memory Efficiency Measures: We measured: (1) the size on disk of the model, (2) the number of parameters in the model, and (3) the peak consumption of the host memory and GPU memory. These metrics were measured by having participants submit a container for the virtualization environment Docker1 , then measuring from outside the container the usage of computation time and memory. All evaluations were performed on dedicated instances on Amazon Web Services2 , specifically of type m5.large for CPU evaluation, and p3.2xlarge (with a NVIDIA Tesla V100 GPU). Model analysis: There were also many me"
W18-2701,D15-1044,0,0.0437311,"cipants were tasked with creating NMT systems that are both accurate and efficient. 1 Introduction Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are now a workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 2nd Workshop on Neural Machine Translation and Generation (WNMT 2018) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization (Rush et al., 2015), NLG from structured data (Wen et al., 2015), dialog response generation (Vinyals and Le, 2015), among others). Overall, the workshop was held with two goals: First, it aimed to synthesize the current state of knowledge in neural machine translation and generation: This year we will continue to encourage submissions that not only advance the state of 2 Summary of Research Contributions We published a call for long papers, extended abstracts for preliminary work, and crosssubmissions of papers submitted to other venues. The goal was to encourage discussion and interaction with researchers from"
W18-2701,W18-2715,0,0.0477416,"Missing"
W18-2701,P17-2091,0,0.117559,"most efficient in the current context, then uses this implementation going forward. This resulted in the MarianTinyRNN system using an RNN-based model, and the Marian-Trans-Small-AAN, MarianTrans-Base-AAN, Marian-Trans-Big, MarianTrans-Big-int8 systems, which use different varieties and sizes of self-attentional models. 3.4.3 Team NICT Team OpenNMT Team OpenNMT (Senellart et al., 2018) built a system based on the OpenNMT toolkit. The model was based on a large self-attentional teacher model distilled into a smaller, fast RNN-based model. The system also used a version of vocabulary selection (Shi and Knight, 2017), and a method to increase the size of the encoder but decrease the size of the decoder to improve the efficiency of beam search. They submitted two systems, OpenNMT-Small and OpenNMT-Tiny, which were two variously-sized implementations of this model. 4 used relatively standard, with the valuable contributions lying in solid engineering work and best practices in neural network optimization such as low-precision calculation and model distillation. With these contributions, we now believe we have very strong baselines upon which future iterations of the task can build, examining novel architect"
W18-2701,W18-2702,0,0.0225666,"ntainer for the virtualization environment Docker1 , then measuring from outside the container the usage of computation time and memory. All evaluations were performed on dedicated instances on Amazon Web Services2 , specifically of type m5.large for CPU evaluation, and p3.2xlarge (with a NVIDIA Tesla V100 GPU). Model analysis: There were also many methods that analyzed modeling and design decisions, including investigations of individual neuron contributions (Bau et al., 2018), parameter sharing (Jean et al., 2018), controlling output characteristics (Fan et al., 2018), and shared attention (Unanue et al., 2018) 3 Evaluation Measures Shared Task 3.2 Many shared tasks, such as the ones run by the Conference on Machine Translation (Bojar et al., 2017), aim to improve the state of the art for MT with respect to accuracy: finding the most accurate MT system regardless of computational cost. However, in production settings, the efficiency of the implementation is also extremely important. The shared task for WNMT (inspired by the “small NMT” task at the Workshop on Asian Data The data used was from the WMT 2014 EnglishGerman task (Bojar et al., 2014), using the preprocessed corpus provided by the Stanford"
W18-2701,D13-1176,0,\N,Missing
W18-2701,P02-1040,0,\N,Missing
W18-2701,W14-3302,0,\N,Missing
W18-2701,W17-4717,0,\N,Missing
W18-2701,W18-2704,0,\N,Missing
W18-2701,W18-2711,1,\N,Missing
W18-2701,W18-2707,0,\N,Missing
W18-2701,W18-2706,0,\N,Missing
W18-2701,W18-2709,0,\N,Missing
W18-2701,W18-2705,0,\N,Missing
W99-0607,P98-1020,1,0.921064,"inding good candidates quickly, and the disadvantage of ignoring any duplication of information in the features it selects. A more principled approach is to select features by actually adding them one-by-one into the ME model (Della Pietra et al., 1997); however, using this approach is very time-consuming and we decided on the MI approach for the sake of speed. 3 The Constraints To understand what extrasentential semantic constraints were added to the base tagging model in the current experiments, one needs some familiarity with the ATR General English Tagset. For detailed presentations, see (Black et al., 1998; Black et al., 1996). An apercu can be gained, however, from Figure 1, which shows two sample sentences from 1 (Black et al., 1998) determined a 6-sentence window to be optimal for this task. 48 (_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DDl coupon_NNIDOCUMENT when_CSWHEN ordering_VVGINTER-ACT 0R_CCOR 0NE_MCIWORD FREE_JJMONEY FANTAIL_NNiANIMAL SHRIMPS_NNiF00D Figure h Two ATR Treebank Sentences from Chinese Take-Out Food Flier (Tagged Only - i.e. Parses Not Displayed) occur extremely often. On the other hand, constraints of the first, second, and third classes, above, are more likely"
W99-0607,C96-1020,1,0.83672,"es quickly, and the disadvantage of ignoring any duplication of information in the features it selects. A more principled approach is to select features by actually adding them one-by-one into the ME model (Della Pietra et al., 1997); however, using this approach is very time-consuming and we decided on the MI approach for the sake of speed. 3 The Constraints To understand what extrasentential semantic constraints were added to the base tagging model in the current experiments, one needs some familiarity with the ATR General English Tagset. For detailed presentations, see (Black et al., 1998; Black et al., 1996). An apercu can be gained, however, from Figure 1, which shows two sample sentences from 1 (Black et al., 1998) determined a 6-sentence window to be optimal for this task. 48 (_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DDl coupon_NNIDOCUMENT when_CSWHEN ordering_VVGINTER-ACT 0R_CCOR 0NE_MCIWORD FREE_JJMONEY FANTAIL_NNiANIMAL SHRIMPS_NNiF00D Figure h Two ATR Treebank Sentences from Chinese Take-Out Food Flier (Tagged Only - i.e. Parses Not Displayed) occur extremely often. On the other hand, constraints of the first, second, and third classes, above, are more likely to occur, but less fo"
W99-0607,J94-2001,0,0.04681,"tag t; K P(tlh) = 7 I ~ ~ k(h't)p° ~L w is word whose tag we are predicting; ]k(h,t) sures: ~t=oTllk=o ak P0 = 1; L is the number of tags in our tag set; - ak is the weight of trigger fk; fk are trigger functions and f~e{0, 1}; t-2 is tag to the left of tag t - l ; Our baseline model differs from Ratnaparkhi's in that it does not use any information about the occurrence of words in the history or their properties (other than in constraint 1). Our model exploits the same kind of t a g - n - g r a m information that forms the core of many successful tagging models, for example, (Kupiec, 1992), (Merialdo, 1994), (Ratnaparkhi, 1996). We refer to this type of tagger as a t a g - n - g r a m tagger. 2.2 T r i g g e r s e l e c t i o n We use mutual information (MI) to select the most useful trigger pairs (for more details, see the ATR Treebank (and originally from a Chinese take-out food flier), tagged with respect to the ATR GenerM English Tagset. Each verb, noun, adjective and adverb in the ATR tagset includes a semantic label, chosen from 42 noun/adjective/adverb categories and 29 verb/verbal categories, some overlap existing between these category sets. Proper nouns, plus certain adjectives and cer"
W99-0607,C98-1020,1,\N,Missing
Y15-1030,2012.eamt-1.42,0,0.201113,"SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is represented explicitly rather than encoded into a lexicalized re-ordering model (commonly used in purely phrase-based approaches). This makes the approach particularly applicable to languages pairs that require long-distance re-ordering during the translation process (Braune et al., 2012). For PACLIC 29 Source-Target Syllable Word PBSMT HPBSMT OSM PBSMT HPBSMT OSM km-ar 29.87 23.33 30.08 42.74 42.46 42.60 km-da 41.53 23.68 40.88 52.22 52.05 52.66 km-de 35.03 19.44 35.03 48.79 47.58 48.99 km-en 49.07 36.79 49.20 59.51 57.83 60.02 km-es 42.17 30.82 41.14 52.97 52.45 53.53 km-fr 40.85 34.00 40.96 50.79 49.76 51.63 km-hi 26.30 8.82 26.22 40.53 42.05 40.87 km-id 43.26 32.18 43.78 53.26 52.14 53.65 km-it 37.60 29.15 37.03 47.27 46.87 47.79 km-ja 23.46 16.06 23.43 34.27 36.42 33.78 km-ko 21.37 22.57 21.53 32.21 33.61 32.13 km-ms 42.90 33.55 43.03 53.85 52.52 53.56 km-my 27.43 24.40 2"
Y15-1030,P96-1041,0,0.398816,"ed SMT system provided by the Moses toolkit (Koehn and Haddow, 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is r"
Y15-1030,J07-2003,0,0.12215,"gnment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is represented explicitly rather than encoded into a lexicalized re-ordering model (commonly used in purely phrase-based approaches). This makes the approach particularly applicable to languages pairs that require long-distance re-ordering during the translation process (Braune et al., 2012). For P"
Y15-1030,J15-2001,0,0.188134,".95 km-vi 45.67 27.20 46.91 53.39 52.57 53.86 km-zh 23.72 8.14 23.87 32.09 32.99 32.22 Table 2: BLEU scores for translating from Khmer. the experiments in this paper we used the implementation of hierarchical model provided by the Moses machine translation toolkit (both the hierarchical decoder and training procedure provided by the experiment management system), using the default settings. 3.4 Operation Sequence Model (OSM) The operation sequence model is a model for statistical MT that combines the benefits of two state-of-the-art SMT frameworks, namely ngram-based SMT and phrase-based SMT (Durrani et al., 2015). It is a generative model that performs the translation process as a linear sequence of operations that jointly generate the source and target sentences. The operation 263 types are (i) generation of a sequence of source and/or target words (ii) insertion of gaps as explicit target positions for reordering operations, and (iii) forward and backward jump operations which perform the actual reordering. The probability of a sequence of operations is given by an n-gram model. The OSM integrates translation and reordering into a single model which provides a natural reordering mechanism that is ab"
Y15-1030,D10-1092,0,0.0465491,"5 53.78 53.78 54.39 ru-km 39.22 38.28 40.00 50.30 50.02 51.34 th-km 46.19 46.46 47.59 53.16 52.40 53.27 tl-km 43.93 42.66 44.06 53.34 53.39 52.76 vi-km 47.93 47.80 48.60 54.26 54.45 55.07 zh-km 32.21 31.16 32.66 39.20 39.49 39.05 Table 3: BLEU scores for translating into Khmer. 4 4.1 Results Evaluation Criteria We used two automatic criteria for the evaluation of the machine translation output. One was the de facto standard automatic evaluation metric Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2001) and the other was the Rank-based Intuitive Bilingual Evaluation Measure (RIBES) (Isozaki et al., 2010). The BLEU score measures the precision of n-grams (over all n ≤ 4 in our case) with respect to a reference translation with a penalty for short translations (Papineni et al., 2001). Intuitively, the BLEU score measures the adequacy of the translations and large BLEU scores are better. RIBES is 264 an automatic evaluation metric based on rank correlation coefficients modified with precision and special care is paid to word order of the translation results. The RIBES score is suitable for distant language pairs such as Khmer and English, Khmer and Korean, Khmer and Myanmar (Isozaki et al., 2010"
Y15-1030,W09-0429,0,0.131977,"others domains. The CRF segmenter achieved 99.15 Precision, 95.72 Recall and 97.31 F-Score. This CRF word segmenter was used to segment the Khmer BTEC data for the experiments in the next section. 3 Experimental Methodology 3.1 Corpus Statistics We used twenty one languages from the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of travel-related expressions (Kikui et al., 2003). The languages were Arabic (ar), Chinese (zh), Danish (da), Dutch (nl), English (en), French (fr), German (de), Hindi 262 We used the phrase based SMT system provided by the Moses toolkit (Koehn and Haddow, 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 20"
Y15-1030,N03-1017,0,0.0900442,"Missing"
Y15-1030,W04-3250,0,0.114954,"is the target language. It is clear from the results in the experiments, that syllable segmentation is a far worse segmentation strategy for SMT than word segmentation. This is not always the case, and for 265 languages such as Myanmar it has been shown (Thu et al., 2013) that syllable segmentation can give rise to machine translation scores that are competitive with other approaches. However, for Khmer the proposed word segmentation strategy gave rise to considerable gains in performance and is therefore to be preferred in all cases. Statistical significance tests using bootstrap resampling (Koehn, 2004) were run for all experiments involving the two segmentation schemes. For all experiments the differences were significant (p &lt; 0.01). For most languages combinations the OSM approach gave the highest scores. It is not surprising that is was able to exceed the performance of the phrase-based approach which it extends. However, in all-but-one of the evaluations involving Japanese and Korean the HPBSMT approach gave rise to the highest scores. Looking at the Kendall’s tau distances in Figure 2 it can be seen that Japanese and Korean are the two most distant languages from Khmer in terms of this"
Y15-1030,P00-1056,0,0.278169,"us Statistics We used twenty one languages from the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of travel-related expressions (Kikui et al., 2003). The languages were Arabic (ar), Chinese (zh), Danish (da), Dutch (nl), English (en), French (fr), German (de), Hindi 262 We used the phrase based SMT system provided by the Moses toolkit (Koehn and Haddow, 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT a"
Y15-1030,P03-1021,0,0.0861983,", 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is represented explicitly rather than encoded into a"
Y15-1030,2001.mtsummit-papers.68,0,0.0744377,"-km 33.82 25.84 33.94 38.25 31.83 38.15 nl-km 44.85 43.05 45.22 53.51 53.98 53.96 pt-km 44.89 44.13 45.55 53.78 53.78 54.39 ru-km 39.22 38.28 40.00 50.30 50.02 51.34 th-km 46.19 46.46 47.59 53.16 52.40 53.27 tl-km 43.93 42.66 44.06 53.34 53.39 52.76 vi-km 47.93 47.80 48.60 54.26 54.45 55.07 zh-km 32.21 31.16 32.66 39.20 39.49 39.05 Table 3: BLEU scores for translating into Khmer. 4 4.1 Results Evaluation Criteria We used two automatic criteria for the evaluation of the machine translation output. One was the de facto standard automatic evaluation metric Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2001) and the other was the Rank-based Intuitive Bilingual Evaluation Measure (RIBES) (Isozaki et al., 2010). The BLEU score measures the precision of n-grams (over all n ≤ 4 in our case) with respect to a reference translation with a penalty for short translations (Papineni et al., 2001). Intuitively, the BLEU score measures the adequacy of the translations and large BLEU scores are better. RIBES is 264 an automatic evaluation metric based on rank correlation coefficients modified with precision and special care is paid to word order of the translation results. The RIBES score is suitable for dist"
Y15-1030,N04-4026,0,\N,Missing
Y15-1030,P02-1040,0,\N,Missing
Y15-1030,D08-1076,0,\N,Missing
