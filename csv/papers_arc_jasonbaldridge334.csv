2021.splurobonlp-1.9,Multi-Level Gazetteer-Free Geocoding,2021,-1,-1,4,0,1068,sayali kulkarni,Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics,0,"We present a multi-level geocoding model (MLG) that learns to associate texts to geographic coordinates. The Earth{'}s surface is represented using space-filling curves that decompose the sphere into a hierarchical grid. MLG balances classification granularity and accuracy by combining losses across multiple levels and jointly predicting cells at different levels simultaneously. It obtains large gains without any gazetteer metadata, demonstrating that it can effectively learn the connection between text spans and coordinates{---}and thus makes it a gazetteer-free geocoder. Furthermore, MLG obtains state-of-the-art results for toponym resolution on three English datasets without any dataset-specific tuning."
2021.findings-emnlp.293,"{MURAL}: Multimodal, Multitask Representations Across Languages",2021,-1,-1,8,0,7132,aashi jain,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Both image-caption pairs and translation pairs provide the means to learn deep representations of and connections between languages. We use both types of pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual encoder that solves two tasks: 1) image-text matching and 2) translation pair matching. By incorporating billions of translation pairs, MURAL extends ALIGN (Jia et al.){--}a state-of-the-art dual encoder learned from 1.8 billion noisy image-text pairs. When using the same encoders, MURAL{'}s performance matches or exceeds ALIGN{'}s cross-modal retrieval performance on well-resourced languages across several datasets. More importantly, it considerably improves performance on under-resourced languages, showing that text-text learning can overcome a paucity of image-caption examples for these languages. On the Wikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean recall by 8.1{\%} on average for eight under-resourced languages and by 6.8{\%} on average when fine-tuning. We additionally show that MURAL{'}s text representations cluster not only with respect to genealogical connections but also based on areal linguistics, such as the Balkan Sprachbund."
2021.eacl-main.111,On the Evaluation of Vision-and-Language Navigation Instructions,2021,-1,-1,6,0,10662,ming zhao,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Vision-and-Language Navigation wayfinding agents can be enhanced by exploiting automatically generated navigation instructions. However, existing instruction generators have not been comprehensively evaluated, and the automatic evaluation metrics used to develop them have not been validated. Using human wayfinders, we show that these generators perform on par with or only slightly better than a template-based generator and far worse than human instructors. Furthermore, we discover that BLEU, ROUGE, METEOR and CIDEr are ineffective for evaluating grounded navigation instructions. To improve instruction evaluation, we propose an instruction-trajectory compatibility model that operates without reference instructions. Our model shows the highest correlation with human wayfinding outcomes when scoring individual instructions. For ranking instruction generation systems, if reference instructions are available we recommend using SPICE."
2021.eacl-main.249,Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for {MS}-{COCO},2021,-1,-1,2,0,10880,zarana parekh,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on representation learning. Unfortunately, datasets have limited cross-modal associations: images are not paired with other images, captions are only paired with other captions of the same image, there are no negative associations and there are missing positive cross-modal associations. This undermines research into how inter-modality learning impacts intra-modality tasks. We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra- and inter-modality pairs. We report baseline results on CxC for strong existing unimodal and multimodal models. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC{'}s value for measuring the influence of intra- and inter-modality learning."
2021.alvr-1.5,{P}an{GEA}: The Panoramic Graph Environment Annotation Toolkit,2021,-1,-1,4,1,10666,alexander ku,Proceedings of the Second Workshop on Advances in Language and Vision Research,0,"PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight toolkit for collecting speech and text annotations in photo-realistic 3D environments. PanGEA immerses annotators in a web-based simulation and allows them to move around easily as they speak and/or listen. It includes database and cloud storage integration, plus utilities for automatically aligning recorded speech with manual transcriptions and the virtual pose of the annotators. Out of the box, PanGEA supports two tasks {--} collecting navigation instructions and navigation instruction following {--} and it could be easily adapted for annotating walking tours, finding and labeling landmarks or objects, and similar tasks. We share best practices learned from using PanGEA in a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We hope that our open-source annotation toolkit and insights will both expedite future data collection efforts and spur innovation on the kinds of grounded language tasks such environments can support."
2020.splu-1.7,Retouchdown: Releasing Touchdown on {S}treet{L}earn as a Public Resource for Language Grounding Tasks in Street View,2020,-1,-1,3,0,14571,harsh mehta,Proceedings of the Third International Workshop on Spatial Language Understanding,0,"The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in (Chen et al., 2019) and show that the panoramas we have added to StreetLearn support both Touchdown tasks and can be used effectively for further research and comparison."
2020.emnlp-main.356,Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding,2020,-1,-1,5,1,10666,alexander ku,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by addressing known biases in paths and eliciting more references to visible entities. Furthermore, each word in an instruction is time-aligned to the virtual poses of instruction creators and validators. We establish baseline scores for monolingual and multilingual settings and multitask learning when including Room-to-Room annotations (Anderson et al., 2018). We also provide results for a model that learns from synchronized pose traces by focusing only on portions of the panorama attended to in human demonstrations. The size, scope and detail of RxR dramatically expands the frontier for research on embodied language agents in photorealistic simulated environments."
2020.acl-main.729,Mapping Natural Language Instructions to Mobile {UI} Action Sequences,2020,24,0,5,0,9363,yang li,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it. For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in How-To instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. We use a Transformer to extract action phrase tuples from long-range natural language instructions. A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions. Given a starting screen and instruction, our model achieves 70.59{\%} accuracy on predicting complete ground-truth action sequences in PixelHelp."
W19-1605,Multi-modal Discriminative Model for Vision-and-Language Navigation,2019,21,3,4,0,24822,haoshuo huang,Proceedings of the Combined Workshop on Spatial Language Understanding ({S}p{LU}) and Grounded Communication for Robotics ({R}obo{NLP}),0,"Vision-and-Language Navigation (VLN) is a natural language grounding task where agents have to interpret natural language instructions in the context of visual scenes in a dynamic environment to achieve prescribed navigation goals. Successful agents must have the ability to parse natural language of varying linguistic styles, ground them in potentially unfamiliar scenes, plan and react with ambiguous environmental feedback. Generalization ability is limited by the amount of human annotated data. In particular, paired vision-language sequence data is expensive to collect. We develop a discriminator that evaluates how well an instruction explains a given path in VLN task using multi-modal alignment. Our study reveals that only a small fraction of the high-quality augmented data from Fried et al., as scored by our discriminator, is useful for training VLN agents with similar performance. We also show that a VLN agent warm-started with pre-trained components from the discriminator outperforms the benchmark success rates of 35.5 by 10{\%} relative measure."
P19-1181,Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation,2019,0,10,6,1,10664,vihan jain,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Advances in learning and representations have reinvigorated work that connects language to other modalities. A particularly exciting direction is Vision-and-Language Navigation(VLN), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language under-standing plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current metrics for the Room-to-Room dataset (Anderson et al.,2018b) and propose a new metric, Coverage weighted by Length Score (CLS). We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to form more challenging extended paths to create a new data set, Room-for-Room (R4R). Using R4R and CLS, we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion."
N19-1131,{PAWS}: Paraphrase Adversaries from Word Scrambling,2019,0,18,2,0.855676,3417,yuan zhang,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS ({\textless}40{\%} accuracy); however, including PAWS training data for these models improves their accuracy to 85{\%} while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons."
N19-1319,Text Classification with Few Examples using Controlled Generalization,2019,0,2,2,0,25774,abhijit mahabal,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Training data for text classification is often limited in practice, especially for applications with many output classes or involving many related classification problems. This means classifiers must generalize from limited evidence, but the manner and extent of generalization is task dependent. Current practice primarily relies on pre-trained word embeddings to map words unseen in training to similar seen ones. Unfortunately, this squishes many components of meaning into highly restricted capacity. Our alternative begins with sparse pre-trained representations derived from unlabeled parsed corpora; based on the available training data, we select features that offers the relevant generalizations. This produces task-specific semantic vectors; here, we show that a feed-forward network over these vectors is especially effective in low-data scenarios, compared to existing state-of-the-art methods. By further pairing this network with a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets."
K19-1006,Large-Scale Representation Learning from Visually Grounded Untranscribed Speech,2019,0,2,3,0,4490,gabriel ilharco,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Systems that can associate images with their spoken audio captions are an important step towards visually grounded language learning. We describe a scalable method to automatically generate diverse audio for image captioning datasets. This supports pretraining deep networks for encoding both audio and images, which we do via a dual encoder that learns to align latent representations from both modalities. We show that a masked margin softmax loss for such models is superior to the standard triplet loss. We fine-tune these models on the Flickr8k Audio Captions Corpus and obtain state-of-the-art results{---}improving recall in the top 10 from 29.6{\%} to 49.5{\%}. We also obtain human ratings on retrieval outputs to better assess the impact of incidentally matching image-caption pairs that were not associated in the data, finding that automatic evaluation substantially underestimates the quality of the retrieved results."
K19-1049,Learning Dense Representations for Entity Retrieval,2019,0,4,5,1,12528,daniel gillick,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"We show that it is feasible to perform entity linking by training a dual encoder (two-tower) model that encodes mentions and entities in the same dense vector space, where candidate entities are retrieved by approximate nearest neighbor search. Unlike prior work, this setup does not rely on an alias table followed by a re-ranker, and is thus the first fully learned entity retrieval model. We show that our dual encoder, trained using only anchor-text links in Wikipedia, outperforms discrete alias table and BM25 baselines, and is competitive with the best comparable results on the standard TACKBP-2010 dataset. In addition, it can retrieve candidates extremely fast, and generalizes well to a new dataset derived from Wikinews. On the modeling side, we demonstrate the dramatic value of an unsupervised negative mining algorithm for this task."
D19-1382,{PAWS}-{X}: A Cross-lingual Adversarial Dataset for Paraphrase Identification,2019,12,3,4,0,7138,yinfei yang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Most existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23{\%} over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information."
W18-1406,"Points, Paths, and Playscapes: Large-scale Spatial Language Understanding Tasks Set in the Real World",2018,0,2,1,1,1071,jason baldridge,Proceedings of the First International Workshop on Spatial Language Understanding,0,"Spatial language understanding is important for practical applications and as a building block for better abstract language understanding. Much progress has been made through work on understanding spatial relations and values in images and texts as well as on giving and following navigation instructions in restricted domains. We argue that the next big advances in spatial language understanding can be best supported by creating large-scale datasets that focus on points and paths based in the real world, and then extending these to create online, persistent playscapes that mix human and bot players, where the bot players must learn, evolve, and survive according to their depth of understanding of scenes, navigation, and interactions."
Q18-1042,Mind the {GAP}: A Balanced Corpus of Gendered Ambiguous Pronouns,2018,0,0,4,0,9606,kellie webster,Transactions of the Association for Computational Linguistics,0,"Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun{--}name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9{\%} F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge."
D18-1030,"A Fast, Compact, Accurate Model for Language Identification of Codemixed Text",2018,0,6,5,0.855676,3417,yuan zhang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We address fine-grained multilingual language identification: providing a language code for every token in a sentence, including codemixed text containing multiple languages. Such text is prevalent online, in documents, social media, and message boards. We show that a feed-forward network with a simple globally constrained decoder can accurately and rapidly label both codemixed and monolingual text in 100 languages and 100 language pairs. This model outperforms previously published multilingual approaches in terms of both accuracy and speed, yielding an 800x speed-up and a 19.5{\%} averaged absolute gain on three codemixed datasets. It furthermore outperforms several benchmark systems on monolingual language identification."
D18-1080,Learning To Split and Rephrase From {W}ikipedia Edit History,2018,14,1,4,0,10226,jan botha,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Split and rephrase is the task of breaking down a sentence into shorter ones that together convey the same meaning. We extract a rich new dataset for this task by mining Wikipedia{'}s edit history: WikiSplit contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task. Incorporating WikiSplit as training data produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark."
W16-1721,Creating a Novel Geolocation Corpus from Historical Texts,2016,17,11,3,0,33962,grant delozier,Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with {ACL} 2016 ({LAW}-X 2016),0,None
P15-1134,Parse Imputation for Dependency Annotations,2015,20,6,3,0,37530,jason mielens,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Syntactic annotation is a hard task, but it can be made easier by allowing annotators flexibility to leave aspects of a sentence underspecified. Unfortunately, partial annotations are not typically directly usable for training parsers. We describe a method for imputing missing dependencies from sentences that have been partially annotated using the Graph Fragment Language, such that a standard dependency parser can then be trained on all annotations. We show that this strategy improves performance over not using partial annotations for English, Chinese, Portuguese and Kinyarwanda, and that performance competitive with state-of-the-art unsupervised and weakly-supervised parsers can be reached with just a few hours of annotation."
K15-1003,A Supertag-Context Model for Weakly-Supervised {CCG} Parser Learning,2015,26,2,3,1,8780,dan garrette,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,"Combinatory Categorial Grammar (CCG) is a lexicalized grammar formalism in which words are associated with categories that specify the syntactic configurations in which they may occur. We present a novel parsing model with the capacity to capture the associative adjacent-category relationships intrinsic to CCG by parameterizing the relationships between each constituent label and the preterminal categories directly to its left and right, biasing the model toward constituent categories that can combine with their contexts. This builds on the intuitions of Klein and Manningxe2x80x99s (2002) xe2x80x9cconstituentcontextxe2x80x9d model, which demonstrated the value of modeling context, but has the advantage of being able to exploit the properties of CCG. Our experiments show that our model outperforms a baseline in which this context information is not captured."
W14-1615,Weakly-Supervised {B}ayesian Learning of a {CCG} Supertagger,2014,33,4,3,1,8780,dan garrette,Proceedings of the Eighteenth Conference on Computational Natural Language Learning,0,"We present a Bayesian formulation for weakly-supervised learning of a Combinatory Categorial Grammar (CCG) supertagger with an HMM. We assume supervision in the form of a tag dictionary, and our prior encourages the use of crosslinguistically common category structures as well as transitions between tags that can combine locally according to CCGxe2x80x99s combinators. Our prior is theoretically appealing since it is motivated by languageindependent, universal properties of the CCG formalism. Empirically, we show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary."
D14-1035,Parsing low-resource languages using {G}ibbs sampling for {PCFG}s with latent annotations,2014,41,5,3,0,37531,liang sun,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing. We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations. For PCFG-LA, we present an additional Gibbs sampler algorithm to learn annotations from training data, which are parse trees with coarse (unannotated) symbols. We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches. Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach."
D14-1039,Hierarchical Discriminative Classification for Text-Based Geolocation,2014,47,34,2,0,40102,benjamin wing,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids. These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions. We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid, which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter, Wikipedia, and Flickr. We also show that logistic regression performs feature selection effectively, assigning high weights to geocentric terms."
W13-2307,A Framework for (Under)specifying Dependency Syntax without Overloading Annotators,2013,25,5,8,0,794,nathan schneider,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"We introduce a framework for lightweight dependency syntax annotation. Our formalism builds upon the typical representation for unlabeled dependencies, permitting a simple notation and annotation workflow. Moreover, the formalism encourages annotators to underspecify parts of the syntax if doing so would streamline the annotation process. We demonstrate the e cacy of this annotation on three languages and develop algorithms to evaluate and compare underspecified annotations."
P13-1057,Real-World Semi-Supervised Learning of {POS}-Taggers for Low-Resource Languages,2013,25,29,3,1,8780,dan garrette,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Developing natural language processing tools for low-resource languages often requires creating resources from scratch. While a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. We discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging. We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and evaluate how the amounts of various kinds of data affect performance of a trained POS-tagger. Our results show that annotation of word types is the most important, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available."
P13-1144,Text-Driven Toponym Resolution using Indirect Supervision,2013,40,26,2,1,41509,michael speriosu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Toponym resolvers identify the specific locations referred to by ambiguous placenames in text. Most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document, or metadata such as population. This paper shows that text-driven disambiguation for toponyms is far more effective. We exploit document-level geotags to indirectly generate training instances for text classifiers for toponym resolution, and show that textual cues can be straightforwardly integrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles."
N13-1014,Learning a Part-of-Speech Tagger from Two Hours of Annotation,2013,17,59,2,1,8780,dan garrette,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Most work on weakly-supervised learning for part-of-speech taggers has been based on unrealistic assumptions about the amount and quality of training data. For this paper, we attempt to create true low-resource scenarios by allowing a linguist just two hours to annotate data and evaluating on the languages Kinyarwanda and Malagasy. Given these severely limited amounts of either type supervision (tag dictionaries) or token supervision (labeled sentences), we are able to dramatically improve the learning of a hidden Markov model through our method of automatically generalizing the annotations, reducing noise, and inducing word-tag frequency information."
D12-1075,Type-Supervised Hidden {M}arkov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries,2012,21,19,2,1,8780,dan garrette,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the min-greedy algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to performance over the original min-greedy algorithm for both English and Italian data."
D12-1137,Supervised Text-based Geolocation Using Language Models on an Adaptive Grid,2012,24,120,5,0,3621,stephen roller,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"The geographical properties of words have recently begun to be exploited for geolocating documents based solely on their text, often in the context of social media and online content. One common approach for geolocating texts is rooted in information retrieval. Given training documents labeled with latitude/longitude coordinates, a grid is overlaid on the Earth and pseudo-documents constructed by concatenating the documents within a given grid cell; then a location for a test document is chosen based on the most similar pseudo-document. Uniform grids are normally used, but they are sensitive to the dispersion of documents over the earth. We define an alternative grid construction using k-d trees that more robustly adapts to data, especially with larger training sets. We also provide a better way of choosing the locations for pseudo-documents. We evaluate these strategies on existing Wikipedia and Twitter corpora, as well as a new, larger Twitter corpus. The adaptive grid achieves competitive results with a uniform grid on small training sets and outperforms it on the large Twitter corpus. The two grid constructions can also be combined to produce consistently strong results across all training sets."
W11-2207,{T}witter Polarity Classification with Label Propagation over Lexical Links and the Follower Graph,2011,21,205,4,1,41509,michael speriosu,Proceedings of the First workshop on Unsupervised Learning in {NLP},0,"There is high demand for automated tools that assign polarity to microblog content such as tweets (Twitter posts), but this is challenging due to the terseness and informality of tweets in addition to the wide variety and rapid evolution of language in Twitter. It is thus impractical to use standard supervised machine learning techniques dependent on annotated training examples. We do without such annotations by using label propagation to incorporate labels from a maximum entropy classifier trained on noisy labels and knowledge about word types encoded in a lexicon, in combination with the Twitter follower graph. Results on polarity classification for several datasets show that our label propagation approach rivals a model supervised with in-domain annotated tweets, and it outperforms the noisily supervised classifier it exploits as well as a lexicon-based polarity ratio classifier."
P11-1096,Simple supervised document geolocation with geodesic grids,2011,20,161,2,0,40102,benjamin wing,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We investigate automatic geolocation (i.e. identification of the location, expressed as latitude/longitude coordinates) of documents. Geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval. We describe several simple supervised methods for document geolocation using only the document's raw text as evidence. All of our methods predict locations in the context of geodesic grids of varying degrees of resolution. We evaluate the methods on geotagged Wikipedia articles and Twitter feeds. For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset."
P11-1108,Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models,2011,34,23,2,1,44683,elias ponvert,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing---the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsu-pervised parser, Seginer's (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL."
I11-1022,Semantic Role Labeling Without Treebanks?,2011,16,3,3,0,44758,stephen boxwell,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We describe a method for training a semantic role labeler for CCG in the absence of gold-standard syntax derivations. Traditionally, semantic role labeling is performed by placing human-annotated semantic roles on gold-standard syntactic parses, identifying patterns in the syntaxsemantics relationship, and then predicting roles on novel syntactic analyses. The gold standard syntactic training data can be eliminated from the process by extracting training instances from semantic roles projected onto a packed parse chart. This process can be used to rapidly develop NLP tools for resource-poor languages of interest."
P10-1051,Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons,2010,19,10,2,0,1424,sujith ravi,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization."
D10-1020,"Crouching {D}irichlet, Hidden {M}arkov Model: Unsupervised {POS} Tagging with Context Local Tag Generation",2010,32,12,3,1,46389,taesun moon,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We define the crouching Dirichlet, hidden Markov model (CDHMM), an HMM for part-of-speech tagging which draws state prior distributions for each local document context. This simple modification of the HMM takes advantage of the dichotomy in natural language between content and function words. In contrast, a standard HMM draws all prior distributions once over all states and it is known to perform poorly in unsupervised and semi-supervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM) is surprisingly effective."
Y09-2043,Supertagging with Factorial Hidden {M}arkov Models,2009,23,0,2,0,46719,srivatsan ramanujam,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Factorial Hidden Markov Models (FHMM) support joint inference for multiple sequence prediction tasks. Here, we use them to jointly predict part-of-speech tag and supertag sequences with varying levels of supervision. We show that supervised training of FHMM models improves performance compared to standard HMMs, especially when labeled training data is scarce. Secondly, we show that an FHMM and a maximum entropy Markov model in a single step co-training setup improves the performance of both models when there is limited labeled training data. Finally, we find that FHMMs trained from tag dictionaries rather than labeled examples also perform better than a standard HMM."
W09-1905,Evaluating Automation Strategies in Language Documentation,2009,13,10,3,0.952381,1316,alexis palmer,Proceedings of the {NAACL} {HLT} 2009 Workshop on Active Learning for Natural Language Processing,0,"This paper presents pilot work integrating machine labeling and active learning with human annotation of data for the language documentation task of creating interlinearized gloss text (IGT) for the Mayan language Uspanteko. The practical goal is to produce a totally annotated corpus that is as accurate as possible given limited time for manual annotation. We describe ongoing pilot studies which examine the influence of three main factors on reducing the time spent to annotate IGT: suggestions from a machine labeler, sample selection methods, and annotator expertise."
D09-1031,How well does active learning \\textit{actually} work? {T}ime-based evaluation of cost-reduction strategies for language documentation.,2009,18,42,1,1,1071,jason baldridge,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Machine involvement has the potential to speed up language documentation. We assess this potential with timed annotation experiments that consider annotator expertise, example selection methods, and suggestions from a machine classifier. We find that better example selection and label suggestions improve efficiency, but effectiveness depends strongly on annotator expertise. Our expert performed best with uncertainty selection, but gained little from suggestions. Our non-expert performed best with random selection and suggestions. The results underscore the importance both of measuring annotation cost reductions with respect to time and of the need for cost-sensitive learning methods that adapt to annotators."
D09-1070,Unsupervised morphological segmentation and clustering with document boundaries,2009,28,8,3,1,46389,taesun moon,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,Many approaches to unsupervised morphology acquisition incorporate the frequency of character sequences with respect to each other to identify word stems and affixes. This typically involves heuristic search procedures and calibrating multiple arbitrary thresholds. We present a simple approach that uses no thresholds other than those involved in standard application of X2 significance testing. A key part of our approach is using document boundaries to constrain generation of candidate stems and affixes and clustering morphological variants of a given word stem. We evaluate our model on English and the Mayan language Uspanteko; it compares favorably to two benchmark systems which use considerably more complex strategies and rely more on experimentally chosen threshold values.
W08-0201,"Teaching Computational Linguistics to a Large, Diverse Student Body: Courses, Tools, and Interdepartmental Interaction",2008,11,3,1,1,1071,jason baldridge,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"We describe course adaptation and development for teaching computational linguistics for the diverse body of undergraduate and graduate students the Department of Linguistics at the University of Texas at Austin. We also discuss classroom tools and teaching aids we have used and created, and we mention our efforts to develop a campus-wide computational linguistics program."
W08-0208,Multidisciplinary Instruction with the Natural Language Toolkit,2008,18,36,4,0,8953,steven bird,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"The Natural Language Toolkit (NLTK) is widely used for teaching natural language processing to students majoring in linguistics or computer science. This paper describes the design of NLTK, and reports on how it has been used effectively in classes that involve different mixes of linguistics and computer science students. We focus on three key issues: getting started with a course, delivering interactive demonstrations in the classroom, and organizing assignments and projects. In each case, we report on practical experience and make recommendations on how to use NLTK to maximum effect."
P08-1038,A Logical Basis for the {D} Combinator and Normal Form in {CCG},2008,26,6,2,0,47909,frederick hoyt,Proceedings of ACL-08: HLT,1,"The standard set of rules defined in Combinatory Categorial Grammar (CCG) fails to provide satisfactory analyses for a number of syntactic structures found in natural languages. These structures can be analyzed elegantly by augmenting CCG with a class of rules based on the combinator D (Curry and Feys, 1958). We show two ways to derive the D rules: one based on unary composition and the other based on a logical characterization of CCGxe2x80x99s rule base (Baldridge, 2002). We also show how Eisnerxe2x80x99s (1996) normal form constraints follow from this logic, ensuring that the D rules do not lead to spurious ambiguities."
D08-1069,Specialized Models and Ranking for Coreference Resolution,2008,35,87,2,1,11461,pascal denis,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"This paper investigates two strategies for improving coreference resolution: (1) training separate models that specialize in particular types of mentions (e.g., pronouns versus proper nouns) and (2) using a ranking loss function rather than a classification function. In addition to being conceptually simple, these modifications of the standard single-model, classification-based approach also deliver significant performance improvements. Specifically, we show that on the ACE corpus both strategies produce f-score gains of more than 3% across the three coreference evaluation metrics (MUC, B3, and CEAF)."
C08-1008,Weakly Supervised Supertagging with Grammar-Informed Initialization,2008,26,13,1,1,1071,jason baldridge,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Much previous work has investigated weak supervision with HMMs and tag dictionaries for part-of-speech tagging, but there have been no similar investigations for the harder problem of supertagging. Here, I show that weak supervision for supertagging does work, but that it is subject to severe performance degradation when the tag dictionary is highly ambiguous. I show that lexical category complexity and information about how supertags may combine syntactically can be used to initialize the transition distributions of a first-order Hidden Markov Model for weakly supervised learning. This initialization proves more effective than starting with uniform transitions, especially when the tag dictionary is highly ambiguous."
P07-1113,A Sequencing Model for Situation Entity Classification,2007,17,14,3,0.952381,1316,alexis palmer,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Situation entities (SEs) are the events, states, generic statements, and embedded facts and propositions introduced to a discourse by clauses of text. We report on the first datadriven models for labeling clauses according to the type of SE they introduce. SE classification is important for discourse mode identification and for tracking the temporal progression of a discourse. We show that (a) linguistically-motivated cooccurrence features and grammatical relation information from deep syntactic analysis improve classification accuracy and (b) using a sequencing model provides improvements over assigning labels based on the utterance alone. We report on genre effects which support the analysis of discourse modes having characteristic distributions and sequences of SEs."
N07-1030,Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming,2007,19,135,2,1,11461,pascal denis,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides f score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets."
D07-1041,Part-of-Speech Tagging for {M}iddle {E}nglish through Alignment and Projection of Parallel Diachronic Texts,2007,16,16,2,1,46389,taesun moon,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We demonstrate an approach for inducing a tagger for historical languages based on existing resources for their modern varieties. Tags from Present Day English source text are projected to Middle English text using alignments on parallel Biblical text. We explore the use of multiple alignment approaches and a bigram tagger to reduce the noise in the projected tags. Finally, we train a maximum entropy tagger on the output of the bigram tagger on the target Biblical text and test it on tagged Middle English text. This leads to tagging accuracy in the low 80xe2x80x99s on Biblical test material and in the 60xe2x80x99s on other Middle English material. Our results suggest that our bootstrapping methods have considerable potential, and could be used to semi-automate an approach based on incremental manual annotation."
W05-0613,Probabilistic Head-Driven Parsing for Discourse Structure,2005,18,8,1,1,1071,jason baldridge,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"We describe a data-driven approach to building interpretable discourse structures for appointment scheduling dialogues. We represent discourse structures as headed trees and model them with probabilistic head-driven parsing techniques. We show that dialogue-based features regarding turn-taking and domain specific goals have a large positive impact on performance. Our best model achieves an f-score of 43.2% for labelled discourse relations and 67.9% for unlabelled ones, significantly beating a right-branching baseline that uses the most frequent relations."
W04-3202,Active Learning and the Total Cost of Annotation,2004,11,80,1,1,1071,jason baldridge,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"Active learning (AL) promises to reduce the cost of annotating labeled datasets for trainable human language technologies. Contrary to expectations, when creating labeled training material for HPSG parse selection and later reusing it with other models, gains from AL may be negligible or even negative. This has serious implications for using AL, showing that additional cost-saving strategies may need to be adopted. We explore one such strategy: using a model during annotation to automate some of the decisions. Our best results show an 80% reduction in annotation cost compared with labeling randomly selected data with a single model."
N04-1012,Ensemble-based Active Learning for Parse Selection,2004,22,45,2,0,34710,miles osborne,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,None
C04-1028,Generalizing Dimensionality in {C}ombinatory {C}ategorial {G}rammar,2004,15,6,2,0,45127,geertjan kruijff,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We extend Combinatory Categorial Grammar (CCG) with a generalized notion of multidimensional sign, inspired by the types of representations found in constraint-based frameworks like HPSG or LFG. The generalized sign allows multiple levels to share information, but only in a resource-bounded way through a very restricted indexation mechanism. This improves representational perspicuity without increasing parsing complexity, in contrast to full-blown unification used in HPSG and LFG. Well-formedness of a linguistic expressions remains entirely determined by the CCG derivation. We show how the multidimensionality and perspicuity of the generalized signs lead to a simplification of previous CCG accounts of how word order and prosody can realize information structure."
W03-2316,Adapting Chart Realization to {CCG},2003,18,53,2,0,1437,michael white,Proceedings of the 9th {E}uropean Workshop on Natural Language Generation ({ENLG}-2003) at {EACL} 2003,0,"We describe a bottom-up chart realization algorithm adapted for use with Combinatory Categorial Grammar (CCG), and show how it can be used to efficiently realize a wide range of coordination phenomena, including argument cluster coordination and gapping. The algorithm has been implemented as an extension to the OpenNLP open source CCG parser. As an avenue for future exploration, we also suggest how the realizer could be used to simplify the treatment of aggregation in conjunction with higher level content planning components."
W03-0403,Active learning for {HPSG} parse selection,2003,18,35,1,1,1071,jason baldridge,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,We describe new features and algorithms for HPSG parse selection models and address the task of creating annotated material to train them. We evaluate the ability of several sample selection methods to reduce the number of annotated sentences necessary to achieve a given level of performance. Our best method achieves a 60% reduction in the amount of training material without any loss in accuracy.
E03-1036,Multi-{M}odal {C}ombinatory {C}ategorial {G}rammar,2003,9,105,1,1,1071,jason baldridge,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The paper shows how Combinatory Categorial Grammar (CCG) can be adapted to take advantage of the extra resource-sensitivity provided by the Categorial Type Logic framework. The resulting reformulation, Multi-Modal CCG, supports lexically specified control over the applicability of combinatory rules, permitting a universal rule component and shedding the need for language-specific restrictions on rules. We discuss some of the linguistic motivation for these changes, define the Multi-Modal CCG system and demonstrate how it works on some basic examples. We furthermore outline some possible extensions and address computational aspects of Multi-Modal CCG."
P02-1041,Coupling {CCG} and Hybrid Logic Dependency Semantics,2002,15,87,1,1,1071,jason baldridge,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Categorial grammar has traditionally used the xcexbb-calculus to represent meaning. We present an alternative, dependency-based perspective on linguistic meaning and situate it in the computational setting. This perspective is formalized in terms of hybrid logic and has a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomena to be represented in a single meaning formalism. Finally, we show how we can couple this formalization to Combinatory Categorial Grammar to produce interpretations compositionally."
baldridge-etal-2002-leo,{L}eo: an Architecture for Sharing Resources for Unification-Based Grammars,2002,13,2,1,1,1071,jason baldridge,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,Many mature systems for parsing unification-based grammars have been developed over the last two decades. They incorporate a variety of design decisions both in implementation and in the representations they use for grammatical information. The Leo project aims to provide an architecture for automating the sharing of grammatical resources among various systems so that one system can take advantage of specialized algorithms and tools that are implemented for the representations used by another. The project furthermore seeks to learn about best practice in the design of these representations and encode their principles in a new XML-based format. This paper describes initial work toward creating the Leo architecture and tools that convert between different representations.
M98-1022,Description of the {UPENN} {CAMP} System as Used for Coreference,1998,0,16,4,0,52865,breck baldwin,"Seventh Message Understanding Conference ({MUC}-7): Proceedings of a Conference Held in Fairfax, Virginia, {A}pril 29 - May 1, 1998",0,None
