2021.repl4nlp-1.23,{B}ayesian Model-Agnostic Meta-Learning with Matrix-Valued Kernels for Quality Estimation,2021,-1,-1,3,0,2507,abiola obamuyide,Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),0,"Most current quality estimation (QE) models for machine translation are trained and evaluated in a fully supervised setting requiring significant quantities of labelled training data. However, obtaining labelled data can be both expensive and time-consuming. In addition, the test data that a deployed QE model would be exposed to may differ from its training data in significant ways. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. Thus, it is desirable to be able to adapt QE models efficiently to new user data with limited supervision data. To address these challenges, we propose a Bayesian meta-learning approach for adapting QE models to the needs and preferences of each user with limited supervision. To enhance performance, we further propose an extension to a state-of-the-art Bayesian meta-learning approach which utilizes a matrix-valued kernel for Bayesian meta-learning of quality estimation. Experiments on data with varying number of users and language characteristics demonstrates that the proposed Bayesian meta-learning approach delivers improved predictive performance in both limited and full supervision settings."
2021.naacl-main.14,"Backtranslation Feedback Improves User Confidence in {MT}, Not Quality",2021,-1,-1,9,0,3171,vilem zouhar,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Translating text into a language unknown to the text{'}s author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality."
2021.naacl-main.252,{S}ent{S}im: Crosslingual Semantic Evaluation of Machine Translation,2021,-1,-1,3,0,3997,yurun song,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Machine translation (MT) is currently evaluated in one of two ways: in a monolingual fashion, by comparison with the system output to one or more human reference translations, or in a trained crosslingual fashion, by building a supervised model to predict quality scores from human-labeled data. In this paper, we propose a more cost-effective, yet well performing unsupervised alternative SentSim: relying on strong pretrained multilingual word and sentence representations, we directly compare the source with the machine translated sentence, thus avoiding the need for both reference translations and labelled training data. The metric builds on state-of-the-art embedding-based approaches {--} namely BERTScore and Word Mover{'}s Distance {--} by incorporating a notion of sentence semantic similarity. By doing so, it achieves better correlation with human scores on different datasets. We show that it outperforms these and other metrics in the standard monolingual setting (MT-reference translation), a well as in the source-MT bilingual setting, where it performs on par with glass-box approaches to quality estimation that rely on MT model information."
2021.mtsummit-up.22,"Validating Quality Estimation in a Computer-Aided Translation Workflow: Speed, Cost and Quality Trade-off",2021,-1,-1,2,1,1658,fernando alvamanchego,Proceedings of Machine Translation Summit XVIII: Users and Providers Track,0,"In modern computer-aided translation workflows, Machine Translation (MT) systems are used to produce a draft that is then checked and edited where needed by human translators. In this scenario, a Quality Estimation (QE) tool can be used to score MT outputs, and a threshold on the QE scores can be applied to decide whether an MT output can be used as-is or requires human post-edition. While this could reduce cost and turnaround times, it could harm translation quality, as QE models are not 100{\%} accurate. In the framework of the APE-QUEST project (Automated Post-Editing and Quality Estimation), we set up a case-study on the trade-off between speed, cost and quality, investigating the benefits of QE models in a real-world scenario, where we rely on end-user acceptability as quality metric. Using data in the public administration domain for English-Dutch and English-French, we experimented with two use cases: assimilation and dissemination. Results shed some light on how QE scores can be explored to establish thresholds that suit each use case and target language, and demonstrate the potential benefits of adding QE to a translation workflow."
2021.findings-emnlp.271,Visual Cues and Error Correction for Translation Robustness,2021,-1,-1,3,1,7085,zhenhao li,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Neural Machine Translation models are sensitive to noise in the input texts, such as misspelled words and ungrammatical constructions. Existing robustness techniques generally fail when faced with unseen types of noise and their performance degrades on clean texts. In this paper, we focus on three types of realistic noise that are commonly generated by humans and introduce the idea of visual context to improve translation robustness for noisy texts. In addition, we describe a novel error correction training regime that can be used as an auxiliary task to further improve translation robustness. Experiments on English-French and English-German translation show that both multimodal and error correction components improve model robustness to noisy texts, while still retaining translation quality on clean texts."
2021.findings-acl.443,Uncertainty Aware Review Hallucination for Science Article Classification,2021,-1,-1,5,0,8524,korbinian friedl,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.452,Knowledge Distillation for Quality Estimation,2021,-1,-1,7,0,8547,amit gajbhiye,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.474,Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications,2021,-1,-1,5,0.909091,9657,shuo sun,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Sentence-level Quality estimation (QE) of machine translation is traditionally formulated as a regression task, and the performance of QE models is typically measured by Pearson correlation with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and make them infeasible for real-world applications. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a regression task. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications."
2021.emnlp-main.536,A Generative Framework for Simultaneous Machine Translation,2021,-1,-1,3,0,7526,yishu miao,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by reinforcement learning. Here we formulate simultaneous translation as a structural sequence-to-sequence learning problem. A latent variable is introduced to model read or translate actions at every time step, which is then integrated out to consider all the possible translation policies. A re-parameterised Poisson prior is used to regularise the policies which allows the model to explicitly balance translation quality and latency. The experiments demonstrate the effectiveness and robustness of the generative framework, which achieves the best BLEU scores given different average translation latencies on benchmark datasets."
2021.emnlp-demo.42,deep{Q}uest-py: {L}arge and Distilled Models for Quality Estimation,2021,-1,-1,6,1,1658,fernando alvamanchego,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"We introduce deepQuest-py, a framework for training and evaluation of large and light-weight models for Quality Estimation (QE). deepQuest-py provides access to (1) state-of-the-art models based on pre-trained Transformers for sentence-level and word-level QE; (2) light-weight and efficient sentence-level models implemented via knowledge distillation; and (3) a web interface for testing models and visualising their predictions. deepQuest-py is available at \url{https://github.com/sheffieldnlp/deepQuest-py} under a CC BY-NC-SA licence."
2021.eacl-main.50,Quality Estimation without Human-labeled Data,2021,-1,-1,6,0,10584,yilin tuan,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Quality estimation aims to measure the quality of translated content without access to a reference translation. This is crucial for machine translation systems in real-world scenarios where high-quality translation is needed. While many approaches exist for quality estimation, they are based on supervised machine learning requiring costly human labelled data. As an alternative, we propose a technique that does not rely on examples from human-annotators and instead uses synthetic training data. We train off-the-shelf architectures for supervised quality estimation on our synthetic data and show that the resulting models achieve comparable performance to models trained on human-annotated data, both for sentence and word-level prediction."
2021.eacl-main.112,Cross-lingual Visual Pre-training for Multimodal Machine Translation,2021,-1,-1,7,1,10667,ozan caglayan,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded cross-lingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision {\&} language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain state-of-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations."
2021.eacl-main.164,Exploring Supervised and Unsupervised Rewards in Machine Translation,2021,-1,-1,4,1,10027,julia ive,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Reinforcement Learning (RL) is a powerful framework to address the discrepancy between loss functions used during training and the final evaluation metrics to be used at test time. When applied to neural Machine Translation (MT), it minimises the mismatch between the cross-entropy loss and non-differentiable evaluation metrics like BLEU. However, the suitability of these metrics as reward function at training time is questionable: they tend to be sparse and biased towards the specific words used in the reference texts. We propose to address this problem by making models less reliant on such metrics in two ways: (a) with an entropy-regularised RL method that does not only maximise a reward function but also explore the action space to avoid peaky distributions; (b) with a novel RL method that explores a dynamic unsupervised reward function to balance between exploration and exploitation. We base our proposals on the Soft Actor-Critic (SAC) framework, adapting the off-policy maximum entropy model for language generation applications such as MT. We demonstrate that SAC with BLEU reward tends to overfit less to the training data and performs better on out-of-domain data. We also show that our dynamic unsupervised reward can lead to better translation of ambiguous words."
2021.eacl-main.281,Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation,2021,-1,-1,6,1,10027,julia ive,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"This paper addresses the problem of simultaneous machine translation (SiMT) by exploring two main concepts: (a) adaptive policies to learn a good trade-off between high translation quality and low latency; and (b) visual information to support this process by providing additional (visual) contextual information which may be available before the textual input is produced. For that, we propose a multimodal approach to simultaneous machine translation using reinforcement learning, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low."
2021.cinlp-1.4,What Makes a Scientific Paper be Accepted for Publication?,2021,-1,-1,3,0,11664,panagiotis fytas,Proceedings of the First Workshop on Causal Inference and NLP,0,"Despite peer-reviewing being an essential component of academia since the 1600s, it has repeatedly received criticisms for lack of transparency and consistency. We posit that recent work in machine learning and explainable AI provide tools that enable insights into the decisions from a given peer-review process. We start by simulating the peer-review process using an ML classifier and extracting global explanations in the form of linguistic features that affect the acceptance of a scientific paper for publication on an open peer-review dataset. Second, since such global explanations do not justify causal interpretations, we propose a methodology for detecting confounding effects in natural language and generating explanations, disentangled from textual confounders, in the form of lexicons. Our proposed linguistic explanation methodology indicates the following on a case dataset of ICLR submissions: a) the organising committee follows, for the most part, the recommendations of reviewers, and b) the paper{'}s main characteristics that led to reviewers recommending acceptance for publication are originality, clarity and substance."
2021.cinlp-1.6,A Survey of Online Hate Speech through the Causal Lens,2021,-1,-1,2,0,11667,antigoni founta,Proceedings of the First Workshop on Causal Inference and NLP,0,"The societal issue of digital hostility has previously attracted a lot of attention. The topic counts an ample body of literature, yet remains prominent and challenging as ever due to its subjective nature. We posit that a better understanding of this problem will require the use of causal inference frameworks. This survey summarises the relevant research that revolves around estimations of causal effects related to online hate speech. Initially, we provide an argumentation as to why re-establishing the exploration of hate speech in causal terms is of the essence. Following that, we give an overview of the leading studies classified with respect to the direction of their outcomes, as well as an outline of all related research, and a summary of open research problems that can influence future work on the topic."
2021.acl-short.25,Continual Quality Estimation with Online {B}ayesian Meta-Learning,2021,-1,-1,3,0,2507,abiola obamuyide,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Most current quality estimation (QE) models for machine translation are trained and evaluated in a static setting where training and test data are assumed to be from a fixed distribution. However, in real-life settings, the test data that a deployed QE model would be exposed to may differ from its training data. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. To address this challenge, we propose an online Bayesian meta-learning framework for the continuous training of QE models that is able to adapt them to the needs of different users, while being robust to distributional shifts in training and test data. Experiments on data with varying number of users and language characteristics validate the effectiveness of the proposed approach."
2021.acl-long.503,{BERTG}en: Multi-task Generation through {BERT},2021,-1,-1,4,0,13425,faidon mitzalis,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We present BERTGen, a novel, generative, decoder-only model which extends BERT by fusing multimodal and multilingual pre-trained models VL-BERT and M-BERT, respectively. BERTGen is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting. With a comprehensive set of evaluations, we show that BERTGen outperforms many strong baselines across the tasks explored. We also show BERTGen{'}s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that BERTGen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models."
2020.wmt-1.4,Findings of the {WMT} 2020 Shared Task on Machine Translation Robustness,2020,-1,-1,1,1,2509,lucia specia,Proceedings of the Fifth Conference on Machine Translation,0,"We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in the real world, including domain diversity and non-standard texts common in user generated content, especially in social media. We cover two language pairs {--} English-German and English-Japanese and provide test sets in zero-shot and few-shot variants. Participating systems are evaluated both automatically and manually, with an additional human evaluation for {''}catastrophic errors{''}. We received 59 submissions by 11 participating teams from a variety of types of institutions."
2020.wmt-1.79,Findings of the {WMT} 2020 Shared Task on Quality Estimation,2020,-1,-1,1,1,2509,lucia specia,Proceedings of the Fifth Conference on Machine Translation,0,"We report the results of the WMT20 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word, sentence and document levels. This edition included new data with open domain texts, direct assessment annotations, and multiple language pairs: English-German, English-Chinese, Russian-English, Romanian-English, Estonian-English, Sinhala-English and Nepali-English data for the sentence-level subtasks, English-German and English-Chinese for the word-level subtask, and English-French data for the document-level subtask. In addition, we made neural machine translation models available to participants. 19 participating teams from 27 institutions submitted altogether 1374 systems to different task variants and language pairs."
2020.wmt-1.116,{BERGAMOT}-{LATTE} Submissions for the {WMT}20 Quality Estimation Shared Task,2020,-1,-1,8,1,2508,marina fomicheva,Proceedings of the Fifth Conference on Machine Translation,0,"This paper presents our submission to the WMT2020 Shared Task on Quality Estimation (QE). We participate in Task and Task 2 focusing on sentence-level prediction. We explore (a) a black-box approach to QE based on pre-trained representations; and (b) glass-box approaches that leverage various indicators that can be extracted from the neural MT systems. In addition to training a feature-based regression model using glass-box quality indicators, we also test whether they can be used to predict MT quality directly with no supervision. We assess our systems in a multi-lingual setting and show that both types of approaches generalise well across languages. Our black-box QE models tied for the winning submission in four out of seven language pairs inTask 1, thus demonstrating very strong performance. The glass-box approaches also performed competitively, representing a light-weight alternative to the neural-based models."
2020.tacl-1.35,Unsupervised Quality Estimation for Neural Machine Translation,2020,63,0,9,1,2508,marina fomicheva,Transactions of the Association for Computational Linguistics,0,"Quality Estimation (QE) is an important component in making Machine Translation (MT) useful in real-world applications, as it is aimed to inform the user on the quality of the MT output at test time. Existing approaches require large amounts of expert annotated data, computation, and time for training. As an alternative, we devise an unsupervised approach to QE where no training or access to additional resources besides the MT system itself is required. Different from most of the current work that treats the MT system as a black box, we explore useful information that can be extracted from the MT system as a by-product of translation. By utilizing methods for uncertainty quantification, we achieve very good correlation with human judgments of quality, rivaling state-of-the-art supervised QE models. To evaluate our approach we collect the first dataset that enables work on both black-box and glass-box approaches to QE."
2020.ngt-1.19,Exploring Model Consensus to Generate Translation Paraphrases,2020,-1,-1,3,1,7085,zhenhao li,Proceedings of the Fourth Workshop on Neural Generation and Translation,0,"This paper describes our submission to the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). This task focuses on improving the ability of neural MT systems to generate diverse translations. Our submission explores various methods, including N-best translation, Monte Carlo dropout, Diverse Beam Search, Mixture of Experts, Ensembling, and Lexical Substitution. Our main submission is based on the integration of multiple translations from multiple methods using Consensus Voting. Experiments show that the proposed approach achieves a considerable degree of diversity without introducing noisy translations. Our final submission achieves a 0.5510 weighted F1 score on the blind test set for the English-Portuguese track."
2020.lrec-1.455,A Post-Editing Dataset in the Legal Domain: Do we Underestimate Neural Machine Translation Quality?,2020,-1,-1,2,1,10027,julia ive,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We introduce a machine translation dataset for three pairs of languages in the legal domain with post-edited high-quality neural machine translation and independent human references. The data was collected as part of the EU APE-QUEST project and comprises crawled content from EU websites with translation from English into three European languages: Dutch, French and Portuguese. Altogether, the data consists of around 31K tuples including a source sentence, the respective machine translation by a neural machine translation system, a post-edited version of such translation by a professional translator, and - where available - the original reference translation crawled from parallel language websites. We describe the data collection process, provide an analysis of the resulting post-edits and benchmark the data using state-of-the-art quality estimation and automatic post-editing models. One interesting by-product of our post-editing analysis suggests that neural systems built with publicly available general domain data can provide high-quality translations, even though comparison to human references suggests that this quality is quite low. This makes our dataset a suitable candidate to test evaluation metrics. The data is freely available as an ELRC-SHARE resource."
2020.emnlp-main.24,{FIND}: {H}uman-in-the-{L}oop {D}ebugging {D}eep {T}ext {C}lassifiers,2020,-1,-1,2,0,4042,piyawat lertvittayakumjorn,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting. In this paper, we propose FIND {--} a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar train-test distributions)."
2020.emnlp-main.184,Simultaneous Machine Translation with Visual Context,2020,-1,-1,6,1,10667,ozan caglayan,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French."
2020.eamt-1.16,"Quality In, Quality Out: Learning from Actual Mistakes",2020,-1,-1,3,1,3257,frederic blain,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"Approaches to Quality Estimation (QE) of machine translation have shown promising results at predicting quality scores for translated sentences. However, QE models are often trained on noisy approximations of quality annotations derived from the proportion of post-edited words in translated sentences instead of direct human annotations of translation errors. The latter is a more reliable ground-truth but more expensive to obtain. In this paper, we present the first attempt to model the task of predicting the proportion of actual translation errors in a sentence while minimising the need for direct human annotation. For that purpose, we use transfer-learning to leverage large scale noisy annotations and small sets of high-fidelity human annotated translation errors to train QE models. Experiments on four language pairs and translations obtained by statistical and neural models show consistent gains over strong baselines."
2020.coling-main.210,Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale,2020,-1,-1,3,1,10667,ozan caglayan,Proceedings of the 28th International Conference on Computational Linguistics,0,"Automatic evaluation of language generation systems is a well-studied problem in Natural Language Processing. While novel metrics are proposed every year, a few popular metrics remain as the de facto metrics to evaluate tasks such as image captioning and machine translation, despite their known limitations. This is partly due to ease of use, and partly because researchers expect to see them and know how to interpret them. In this paper, we urge the community for more careful consideration of how they automatically evaluate their models by demonstrating important failure cases on multiple datasets, language pairs and tasks. Our experiments show that metrics (i) usually prefer system outputs to human-authored texts, (ii) can be insensitive to correct translations of rare words, (iii) can yield surprisingly high scores when given a single sentence as system output for the entire test set."
2020.cl-1.4,Data-Driven Sentence Simplification: Survey and Benchmark,2020,91,1,3,1,1658,fernando alvamanchego,Computational Linguistics,0,"Sentence Simplification (SS) aims to modify a sentence in order to make it easier to read and understand. In order to do so, several rewriting transformations can be performed such as replacement, reordering, and splitting. Executing these transformations while keeping sentences grammatical, preserving their main idea, and generating simpler output, is a challenging and still far from solved problem. In this article, we survey research on SS, focusing on approaches that attempt to learn how to simplify using corpora of aligned original-simplified sentence pairs in English, which is the dominant paradigm nowadays. We also include a benchmark of different approaches on common data sets so as to compare them and highlight their strengths and limitations. We expect that this survey will serve as a starting point for researchers interested in the task and help spark new ideas for future developments."
2020.acl-main.113,Multi-Hypothesis Machine Translation Evaluation,2020,-1,-1,2,1,2508,marina fomicheva,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15{\%}."
2020.acl-main.114,Multimodal Quality Estimation for Machine Translation,2020,-1,-1,3,0,22631,shu okabe,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE. We compare various multimodality integration and fusion strategies. For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality."
2020.acl-main.424,{ASSET}: {A} Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,2020,44,1,6,1,1658,fernando alvamanchego,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed."
2020.acl-main.558,Are we Estimating or Guesstimating Translation Quality?,2020,-1,-1,3,0.909091,9657,shuo sun,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation (QE) for machine translation. A carefully engineered ensemble of such models won the QE shared task at WMT19. Our in-depth analysis, however, shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets: (i) The distributions of quality scores are imbalanced and skewed towards good quality scores; (iii) QE models can perform well on these datasets while looking at only source or translated sentences; (iii) They contain statistical artifacts that correlate well with human-annotated QE labels. Our findings suggest that although QE models might capture fluency of translated sentences and complexity of source sentences, they cannot model adequacy of translations effectively."
2020.aacl-main.39,An Exploratory Study on Multilingual Quality Estimation,2020,-1,-1,8,0.909091,9657,shuo sun,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Predicting the quality of machine translation has traditionally been addressed with language-specific models, under the assumption that the quality label distribution or linguistic features exhibit traits that are not shared across languages. An obvious disadvantage of this approach is the need for labelled data for each given language pair. We challenge this assumption by exploring different approaches to multilingual Quality Estimation (QE), including using scores from translation models. We show that these outperform single-language models, particularly in less balanced quality label distributions and low-resource settings. In the extreme case of zero-shot QE, we show that it is possible to accurately predict quality for any given new language from models trained on other languages. Our findings indicate that state-of-the-art neural QE models based on powerful pre-trained representations generalise well across languages, making them more applicable in real-world settings."
W19-6717,{APE}-{QUEST},2019,-1,-1,7,0,17598,joachim bogaert,"Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks",0,None
W19-5324,A Comparison on Fine-grained Pre-trained Embeddings for the {WMT}19{C}hinese-{E}nglish News Translation Task,2019,0,0,2,1,7085,zhenhao li,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,This paper describes our submission to the WMT 2019 Chinese-English (zh-en) news translation shared task. Our systems are based on RNN architectures with pre-trained embeddings which utilize character and sub-character information. We compare models with these different granularity levels using different evaluating metics. We find that a finer granularity embeddings can help the model according to character level evaluation and that the pre-trained embeddings can also be beneficial for model performance marginally when the training data is limited.
W19-5356,{WMDO}: Fluency-based Word Mover{'}s Distance for Machine Translation Evaluation,2019,0,3,2,0,23889,julian chow,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"We propose WMDO, a metric based on distance between distributions in the semantic vector space. Matching in the semantic space has been investigated for translation evaluation, but the constraints of a translation{'}s word order have not been fully explored. Building on the Word Mover{'}s Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of metrics."
W19-5026,Is artificial data useful for biomedical Natural Language Processing algorithms?,2019,27,0,4,1,10767,zixu wang,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"A major obstacle to the development of Natural Language Processing (NLP) methods in the biomedical domain is data accessibility. This problem can be addressed by generating medical data artificially. Most previous studies have focused on the generation of short clinical text, and evaluation of the data utility has been limited. We propose a generic methodology to guide the generation of clinical text with key phrases. We use the artificial data as additional training data in two key biomedical NLP tasks: text classification and temporal relation extraction. We show that artificially generated training data used in conjunction with real training data can lead to performance boosts for data-greedy neural network algorithms. We also demonstrate the usefulness of the generated data for NLP setups where it fully replaces real training data."
W19-3656,Cross-Sentence Transformations in Text Simplification,2019,-1,-1,3,1,1658,fernando alvamanchego,Proceedings of the 2019 Workshop on Widening NLP,0,"Current approaches to Text Simplification focus on simplifying sentences individually. However, certain simplification transformations span beyond single sentences (e.g. joining and re-ordering sentences). In this paper, we motivate the need for modelling the simplification task at the document level, and assess the performance of sequence-to-sequence neural models in this setup. We analyse parallel original-simplified documents created by professional editors and show that there are frequent rewriting transformations that are not restricted to sentence boundaries. We also propose strategies to automatically evaluate the performance of a simplification model on these cross-sentence transformations. Our experiments show the inability of standard sequence-to-sequence neural models to learn these transformations, and suggest directions towards document-level simplification."
W19-1808,Grounded Word Sense Translation,2019,-1,-1,3,1,24792,chiraag lala,Proceedings of the Second Workshop on Shortcomings in Vision and Language,0,"Recent work on visually grounded language learning has focused on broader applications of grounded representations, such as visual question answering and multimodal machine translation. In this paper we consider grounded word sense translation, i.e. the task of correctly translating an ambiguous source word given the corresponding textual and visual context. Our main objective is to investigate the extent to which images help improve word-level (lexical) translation quality. We do so by first studying the dataset for this task to understand the scope and challenges of the task. We then explore different data settings, image features, and ways of grounding to investigate the gain from using images in each of the combinations. We find that grounding on the image is specially beneficial in weaker unidirectional recurrent translation models. We observe that adding structured image information leads to stronger gains in lexical translation accuracy."
P19-1653,Distilling Translations with Visual Awareness,2019,0,4,3,1,10027,julia ive,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language."
P19-1654,{VIFIDEL}: Evaluating the Visual Fidelity of Image Descriptions,2019,0,3,3,1,10154,pranava madhyastha,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We address the task of evaluating image description generation systems. We propose a novel image-aware metric for this task: VIFIDEL. It estimates the faithfulness of a generated caption with respect to the content of the actual image, based on the semantic similarity between labels of objects depicted in images and words in the description. The metric is also able to take into account the relative importance of objects mentioned in human reference descriptions during evaluation. Even if these human reference descriptions are not available, VIFIDEL can still reliably evaluate system descriptions. The metric achieves high correlation with human judgments on two well-known datasets and is competitive with metrics that depend on and rely exclusively on human references."
N19-1422,Probing the Need for Visual Context in Multimodal Machine Translation,2019,31,2,3,1,10667,ozan caglayan,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model."
J19-3004,Taking {MT} Evaluation Metrics to Extremes: Beyond Correlation with Human Judgments,2019,34,0,2,1,2508,marina fomicheva,Computational Linguistics,0,"Automatic Machine Translation (MT) evaluation is an active field of research, with a handful of new metrics devised every year. Evaluation metrics are generally benchmarked against manual assessment of translation quality, with performance measured in terms of overall correlation with human scores. Much work has been dedicated to the improvement of evaluation metrics to achieve a higher correlation with human judgments. However, little insight has been provided regarding the weaknesses and strengths of existing approaches and their behavior in different settings. In this work we conduct a broad meta-evaluation study of the performance of a wide range of evaluation metrics focusing on three major aspects. First, we analyze the performance of the metrics when faced with different levels of translation quality, proposing a local dependency measure as an alternative to the standard, global correlation coefficient. We show that metric performance varies significantly across different levels of MT quality: Metrics perform poorly when faced with low-quality translations and are not able to capture nuanced quality distinctions. Interestingly, we show that evaluating low-quality translations is also more challenging for humans. Second, we show that metrics are more reliable when evaluating neural MT than the traditional statistical MT systems. Finally, we show that the difference in the evaluation accuracy for different metrics is maintained even if the gold standard scores are based on different criteria."
D19-5543,Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back-Translation,2019,27,2,2,1,7085,zhenhao li,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"Neural Machine Translation (NMT) models have been proved strong when translating clean texts, but they are very sensitive to noise in the input. Improving NMT models robustness can be seen as a form of {``}domain{''} adaption to noise. The recently created Machine Translation on Noisy Text task corpus provides noisy-clean parallel data for a few language pairs, but this data is very limited in size and diversity. The state-of-the-art approaches are heavily dependent on large volumes of back-translated data. This paper has two main contributions: Firstly, we propose new data augmentation methods to extend limited noisy data and further improve NMT robustness to noise while keeping the models small. Secondly, we explore the effect of utilizing noise from external data in the form of speech transcripts and show that it could help robustness."
D19-3009,{EASSE}: Easier Automatic Sentence Simplification Evaluation,2019,18,2,4,1,1658,fernando alvamanchego,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"We introduce EASSE, a Python package aiming to facilitate and standardise automatic evaluation and comparison of Sentence Simplification (SS) systems. EASSE provides a single access point to a broad range of evaluation resources: standard automatic metrics for assessing SS outputs (e.g. SARI), word-level accuracy scores for certain simplification transformations, reference-independent quality estimation features (e.g. compression ratio), and standard test data for SS evaluation (e.g. TurkCorpus). Finally, EASSE generates easy-to-visualise reports on the various metrics and features above and on how a particular SS output fares against reference simplifications. Through experiments, we show that these functionalities allow for better comparison and understanding of the performance of SS systems."
D19-1318,Deep Copycat Networks for Text-to-Text Generation,2019,0,0,3,1,10027,julia ive,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Most text-to-text generation tasks, for example text summarisation and text simplification, require copying words from the input to the output. We introduce Copycat, a transformer-based pointer network for such tasks which obtains competitive results in abstractive text summarisation and generates more abstractive summaries. We propose a further extension of this architecture for automatic post-editing, where generation is conditioned over two inputs (source language and machine translation), and the model is capable of deciding where to copy information from. This approach achieves competitive performance when compared to state-of-the-art automated post-editing systems. More importantly, we show that it addresses a well-known limitation of automatic post-editing - overcorrecting translations - and that our novel mechanism for copying source language words improves the results."
W18-6402,Findings of the Third Shared Task on Multimodal Machine Translation,2018,0,22,3,0.552503,8740,loic barrault,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We present the results from the third shared task on multimodal machine translation. In this task a source sentence in English is supplemented by an image and participating systems are required to generate a translation for such a sentence into German, French or Czech. The image can be used in addition to (or instead of) the source sentence. This year the task was extended with a third target language (Czech) and a new test set. In addition, a variant of this task was introduced with its own test set where the source sentence is given in multiple languages: English, French and German, and participating systems are required to generate a translation in Czech. Seven teams submitted 45 different systems to the two variants of the task. Compared to last year, the performance of the multimodal submissions improved, but text-only systems remain competitive."
W18-6442,{S}heffield Submissions for {WMT}18 Multimodal Translation Shared Task,2018,0,2,4,1,24792,chiraag lala,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper describes the University of Sheffield{'}s submissions to the WMT18 Multimodal Machine Translation shared task. We participated in both tasks 1 and 1b. For task 1, we build on a standard sequence to sequence attention-based neural machine translation system (NMT) and investigate the utility of multimodal re-ranking approaches. More specifically, n-best translation candidates from this system are re-ranked using novel multimodal cross-lingual word sense disambiguation models. For task 1b, we explore three approaches: (i) re-ranking based on cross-lingual word sense disambiguation (as for task 1), (ii) re-ranking based on consensus of NMT n-best lists from German-Czech, French-Czech and English-Czech systems, and (iii) data augmentation by generating English source data through machine translation from French to English and from German to English followed by hypothesis selection using a multimodal-reranker."
W18-6451,Findings of the {WMT} 2018 Shared Task on Quality Estimation,2018,0,14,1,1,2509,lucia specia,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We report the results of the WMT18 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems at various granularity levels: word, phrase, sentence and document. This year we include four language pairs, three text domains, and translations produced by both statistical and neural machine translation systems. Participating teams from ten institutions submitted a variety of systems to different task variants and language pairs."
W18-6463,{S}heffield Submissions for the {WMT}18 Quality Estimation Shared Task,2018,0,1,4,1,10027,julia ive,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"In this paper we present the University of Sheffield submissions for the WMT18 Quality Estimation shared task. We discuss our submissions to all four sub-tasks, where ours is the only team to participate in all language pairs and variations (37 combinations). Our systems show competitive results and outperform the baseline in nearly all cases."
W18-6320,Exploring gap filling as a cheaper alternative to reading comprehension questionnaires when evaluating machine translation for gisting,2018,0,0,3,0,5037,mikel forcada,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"A popular application of machine translation (MT) is \textit{gisting}: MT is consumed \textit{as is} to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses \textit{reading comprehension questionnaires} (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, \textit{gap-filling} (GF), a form of \textit{cloze} testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are: (a) both RCQ and GF clearly identify MT to be useful; (b) global RCQ and GF rankings for the MT systems are mostly in agreement; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike RCQ, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of GF as a cheaper alternative to RCQ."
W18-5455,End-to-end Image Captioning Exploits Distributional Similarity in Multimodal Space,2018,0,2,3,1,10154,pranava madhyastha,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn {`}distributional similarity{'} in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the {`}image{'} side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. Our analysis indicates that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace."
W18-1804,Combining Quality Estimation and Automatic Post-editing to Enhance Machine Translation output,2018,0,3,5,0.640689,13898,rajen chatterjee,Proceedings of the 13th Conference of the Association for Machine Translation in the {A}mericas (Volume 1: Research Track),0,None
W18-0507,A Report on the Complex Word Identification Shared Task 2018,2018,16,0,5,0,282,seid yimam,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We report the findings of the second Complex Word Identification (CWI) shared task organized as part of the BEA workshop co-located with NAACL-HLT{'}2018. The second CWI shared task featured multilingual and multi-genre datasets divided into four tracks: English monolingual, German monolingual, Spanish monolingual, and a multilingual track with a French test set, and two tasks: binary classification and probabilistic classification. A total of 12 teams submitted their results in different task/track combinations and 11 of them wrote system description papers that are referred to in this report and appear in the BEA workshop proceedings."
P18-2113,Learning Simplifications for Specific Target Audiences,2018,0,4,2,1,7140,carolina scarton,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text. Most recent work is based on sequence-to-sequence neural models similar to those used for machine translation (MT). Different from MT, TS data comprises more elaborate transformations, such as sentence splitting. It can also contain multiple simplifications of the same original text targeting different audiences, such as school grade levels. We explore these two features of TS to build models tailored for specific grade levels. Our approach uses a standard sequence-to-sequence architecture where the original sequence is annotated with information about the target audience and/or the (predicted) type of simplification operation. We show that it outperforms state-of-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning."
N18-5015,Vis-Eval Metric Viewer: A Visualisation Tool for Inspecting and Evaluating Metric Scores of Machine Translation Output,2018,0,1,2,1,29284,david steele,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"Machine Translation systems are usually evaluated and compared using automated evaluation metrics such as BLEU and METEOR to score the generated translations against human translations. However, the interaction with the output from the metrics is relatively limited and results are commonly a single score along with a few additional statistics. Whilst this may be enough for system comparison it does not provide much useful feedback or a means for inspecting translations and their respective scores. VisEval Metric Viewer VEMV is a tool designed to provide visualisation of multiple evaluation scores so they can be easily interpreted by a user. VEMV takes in the source, reference, and hypothesis files as parameters, and scores the hypotheses using several popular evaluation metrics simultaneously. Scores are produced at both the sentence and dataset level and results are written locally to a series of HTML files that can be viewed on a web browser. The individual scored sentences can easily be inspected using powerful search and selection functions and results can be visualised with graphical representations of the scores and distributions."
N18-2069,Defoiling Foiled Image Captions,2018,4,1,3,1,10154,pranava madhyastha,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We address the task of detecting foiled image captions, i.e. identifying whether a caption contains a word that has been deliberately replaced by a semantically similar word, thus rendering it inaccurate with respect to the image being described. Solving this problem should in principle require a fine-grained understanding of images to detect subtle perturbations in captions. In such contexts, encoding sufficiently descriptive image information becomes a key challenge. In this paper, we demonstrate that it is possible to solve this task using simple, interpretable yet powerful representations based on explicit object information over multilayer perceptron models. Our models achieve state-of-the-art performance on a recently published dataset, with scores exceeding those achieved by humans on the task. We also measure the upper-bound performance of our models using gold standard annotations. Our study and analysis reveals that the simpler model performs well even without image information, suggesting that the dataset contains strong linguistic bias."
N18-1198,Object Counts! Bringing Explicit Detections Back into Image Captioning,2018,11,3,3,1,25940,josiah wang,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"The use of explicit object detectors as an intermediate step to image captioning {--} which used to constitute an essential stage in early work {--} is often bypassed in the currently dominant end-to-end approaches, where the language model is conditioned directly on a mid-level image embedding. We argue that explicit detections provide rich semantic information, and can thus be used as an interpretable representation to better understand why end-to-end image captioning systems work well. We provide an in-depth analysis of end-to-end image captioning by exploring a variety of cues that can be derived from such object detections. Our study reveals that end-to-end image captioning systems rely on matching image representations to generate captions, and that encoding the frequency, size and position of objects are complementary and all play a role in forming a good image representation. It also reveals that different object categories contribute in different ways towards image captioning."
L18-1553,Text Simplification from Professionally Produced Corpora,2018,0,3,3,1,7140,carolina scarton,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1602,Multimodal Lexical Translation,2018,0,5,2,1,24792,chiraag lala,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1685,{S}im{PA}: A Sentence-Level Simplification Corpus for the Public Administration Domain,2018,0,1,3,1,7140,carolina scarton,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1266,deep{Q}uest: A Framework for Neural-based Quality Estimation,2018,0,9,3,1,10027,julia ive,Proceedings of the 27th International Conference on Computational Linguistics,0,"Predicting Machine Translation (MT) quality can help in many practical tasks such as MT post-editing. The performance of Quality Estimation (QE) methods has drastically improved recently with the introduction of neural approaches to the problem. However, thus far neural approaches have only been designed for word and sentence-level prediction. We present a neural framework that is able to accommodate neural QE approaches at these fine-grained levels and generalize them to the level of documents. We test the framework with two sentence-level neural QE approaches: a state of the art approach that requires extensive pre-training, and a new light-weight approach that we propose, which employs basic encoders. Our approach is significantly faster and yields performance improvements for a range of document-level quality estimation tasks. To our knowledge, this is the first neural architecture for document-level QE. In addition, for the first time we apply QE models to the output of both statistical and neural MT systems for a series of European languages and highlight the new challenges resulting from the use of neural MT."
W17-5910,Complex Word Identification: Challenges in Data Annotation and System Performance,2017,9,0,4,0.156008,622,marcos zampieri,Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017),0,"This paper revisits the problem of complex word identification (CWI) following up the SemEval CWI shared task. We use ensemble classifiers to investigate how well computational methods can discriminate between complex and non-complex words. Furthermore, we analyze the classification performance to understand what makes lexical complexity challenging. Our findings show that most systems performed poorly on the SemEval CWI dataset, and one of the reasons for that is the way in which human annotation was performed."
W17-4716,Guiding Neural Machine Translation Decoding with External Knowledge,2017,24,15,5,0.640689,13898,rajen chatterjee,Proceedings of the Second Conference on Machine Translation,0,None
W17-4717,Findings of the 2017 Conference on Machine Translation ({WMT}17),2017,0,109,15,0,292,ondvrej bojar,Proceedings of the Second Conference on Machine Translation,0,"This paper presents the results of the WMT17 shared tasks, which includedn three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task."
W17-4718,Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description,2017,41,12,5,0.526316,2490,desmond elliott,Proceedings of the Second Conference on Machine Translation,0,"We present the results from the second shared task on multimodal machine translation and multilingual image description. Nine teams submitted 19 systems to two tasks. The multimodal translation task, in which the source sentence is supplemented by an image, was extended with a new language (French) and two new test sets. The multilingual image description task was changed such that at test time, only the image is given. Compared to last year, multimodal systems improved, but text-only systems remain competitive."
W17-4734,The {QT}21 Combined Machine Translation System for {E}nglish to {L}atvian,2017,0,0,16,0.512486,30412,janthorsten peter,Proceedings of the Second Conference on Machine Translation,0,None
W17-4752,{S}heffield {M}ulti{MT}: Using Object Posterior Predictions for Multimodal Machine Translation,2017,0,7,3,1,10154,pranava madhyastha,Proceedings of the Second Conference on Machine Translation,0,"This paper describes the University ofn Sheffieldxe2x80x99s submission to the WMT17n Multimodal Machine Translation sharedn task. We participated in Task 1 to developn an MT system to translate an imagen description from English to Germann and French, given its corresponding image.n Our proposed systems are based onn the state-of-the-art Neural Machine Translationn approach. We investigate the effectn of replacing the commonly-used imagen embeddings with an estimated posteriorn probability prediction for 1,000 objectn categories in the images."
W17-4760,Bilexical Embeddings for Quality Estimation,2017,5,3,3,1,3257,frederic blain,Proceedings of the Second Conference on Machine Translation,0,None
W17-4765,Feature-Enriched Character-Level Convolutions for Text Regression,2017,14,0,2,1,1610,gustavo paetzold,Proceedings of the Second Conference on Machine Translation,0,None
S17-2001,{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation,2017,51,219,5,0,9653,daniel cer,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in \textit{all language tracks}. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the \textit{STS Benchmark} is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017)."
I17-5005,"The Ultimate Presentation Makeup Tutorial: How to {P}olish your Posters, Slides and Presentations Skills",2017,0,0,2,1,1610,gustavo paetzold,"Proceedings of the {IJCNLP} 2017, Tutorial Abstracts",0,"There is no question that our research community have, and still has been producing an insurmountable amount of interesting strategies, models and tools to a wide array of problems and challenges in diverse areas of knowledge. But for as long as interesting work has existed, we{'}ve been plagued by a great unsolved mystery: how come there is so much interesting work being published in conferences, but not as many interesting and engaging posters and presentations being featured in them? In this tutorial, we present practical step-by-step makeup solutions for poster, slides and oral presentations in order to help researchers who feel like they are not able to convey the importance of their research to the community in conferences."
I17-3001,{MASSA}lign: Alignment and Annotation of Comparable Documents,2017,7,6,3,1,1610,gustavo paetzold,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"We introduce MASSAlign: a Python library for the alignment and annotation of monolingual comparable documents. MASSAlign offers easy-to-use access to state of the art algorithms for paragraph and sentence-level alignment, as well as novel algorithms for word-level annotation of transformation operations between aligned sentences. In addition, MASSAlign provides a visualization module to display and analyze the alignments and annotations performed."
I17-3007,{MUSST}: A Multilingual Syntactic Simplification Tool,2017,0,1,5,1,7140,carolina scarton,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"We describe MUSST, a multilingual syntactic simplification tool. The tool supports sentence simplifications for English, Italian and Spanish, and can be easily extended to other languages. Our implementation includes a set of general-purpose simplification rules, as well as a sentence selection module (to select sentences to be simplified) and a confidence model (to select only promising simplifications). The tool was implemented in the context of the European project SIMPATICO on text simplification for Public Administration (PA) texts. Our evaluation on sentences in the PA domain shows that we obtain correct simplifications for 76{\%} of the simplified cases in English, 71{\%} of the cases in Spanish. For Italian, the results are lower (38{\%}) but the tool is still under development."
I17-1030,Learning How to Simplify From Explicit Labeling of Complex-Simplified Text Pairs,2017,0,6,5,1,1658,fernando alvamanchego,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degree that generalization becomes difficult. End-to-end models also make it hard to interpret what is actually learned from data. We propose a method that decomposes the task of TS into its sub-problems. We devise a way to automatically identify operations in a parallel corpus and introduce a sequence-labeling approach based on these annotations. Finally, we provide insights on the types of transformations that different approaches can model."
E17-2006,Lexical Simplification with Neural Ranking,2017,0,8,2,1,1610,gustavo paetzold,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We present a new Lexical Simplification approach that exploits Neural Networks to learn substitutions from the Newsela corpus - a large set of professionally produced simplifications. We extract candidate substitutions by combining the Newsela corpus with a retrofitted context-aware word embeddings model and rank them using a new neural regression model that learns rankings from annotated data. This strategy leads to the highest Accuracy, Precision and F1 scores to date in standard datasets for the task."
E17-1101,Personalized Machine Translation: Preserving Original Author Traits,2017,26,16,4,0,8815,ella rabinovich,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"The language that we produce reflects our personality, and various personal and demographic characteristics can be detected in natural language texts. We focus on one particular personal trait of the author, gender, and study how it is manifested in original texts and in translations. We show that author{'}s gender has a powerful, clear signal in originals texts, but this signal is obfuscated in human and machine translation. We then propose simple domain-adaptation techniques that help retain the original gender traits in the translation, without harming the quality of the translation, thereby creating more personalized machine translation systems."
W16-3407,The Trouble with Machine Translation Coherence,2016,23,3,3,1,31609,karin smith,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W16-3413,Semantic Textual Similarity in Quality Estimation,2016,18,3,4,0,33757,hanna bechara,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W16-3417,Predicting and Using Implicit Discourse Elements in {C}hinese-{E}nglish Translation,2016,0,1,2,1,29284,david steele,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W16-3210,{M}ulti30{K}: Multilingual {E}nglish-{G}erman Image Descriptions,2016,15,51,4,0.526316,2490,desmond elliott,Proceedings of the 5th Workshop on Vision and Language,0,"We introduce the Multi30K dataset to stimulate multilingual multimodal research. Recent advances in image description have been demonstrated on English-language datasets almost exclusively, but image description should not be limited to English. This dataset extends the Flickr30K dataset with i) German translations created by professional translators over a subset of the English descriptions, and ii) descriptions crowdsourced independently of the original English descriptions. We outline how the data can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks."
W16-2301,Findings of the 2016 Conference on Machine Translation,2016,113,137,18,0,292,ondvrej bojar,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper presents the results of the WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT quality), and an automatic post-editing task and bilingual document alignment task. This year, 102 MT systems from 24 institutions (plus 36 anonymized online systems) were submitted to the 12 translation directions in the news translation task. The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions. Evaluation was both automatic and manual (relative ranking and 100-point scale assessments). The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries."
W16-2307,{S}heffield Systems for the {E}nglish-{R}omanian {WMT} Translation Task,2016,0,1,3,1,3257,frederic blain,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2320,The {QT}21/{H}im{L} Combined Machine Translation System,2016,5,6,12,0.512486,30412,janthorsten peter,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the joint submission of the QT21 and HimL projects for the Englishxe2x86x92Romanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). The submission is a system combination which combines twelve different statistical machine translation systems provided by the different groups (RWTH Aachen University, LMU Munich, Charles University in Prague, University of Edinburgh, University of Sheffield, Karlsruhe Institute of Technology, LIMSI, University of Amsterdam, Tilde). The systems are combined using RWTHxe2x80x99s system combination approach. The final submission shows an improvement of 1.0 BLEU compared to the best single system on newstest2016."
W16-2339,{C}obalt{F}: A Fluent Metric for {MT} Evaluation,2016,13,3,3,1,2508,marina fomicheva,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"Comunicacio presentada a la First Conference on Machine Translation (WMT), que es va dur a terme durant el 54th Annual Meeting of the Association for Computational Linguistics, els dies 7 a 12 d'agost de 2016 a Berlin, Alemanya."
W16-2346,A Shared Task on Multimodal Machine Translation and Crosslingual Image Description,2016,33,74,1,1,2509,lucia specia,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper introduces and summarises the findings of a new shared task at the intersection of Natural Language Processing and Computer Vision: the generation of image descriptions in a target language, given an image and/or one or more descriptions in a different (source) language. This challenge was organised along with the Conference on Machine Translation (WMT16), and called for system submissions for two task variants: (i) a translation task, in which a source language image description needs to be translated to a target language, (optionally) with additional cues from the corresponding image, and (ii) a description generation task, in which a target language description needs to be generated for an image, (optionally) with additional cues from source language descriptions of the same image. In this first edition of the shared task, 16 systems were submitted for the translation task and seven for the image description task, from a total of 10 teams."
W16-2363,{SHEF}-Multimodal: Grounding Machine Translation on Images,2016,23,18,3,1,695,kashif shah,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the University ofn Sheffieldxe2x80x99s submission for the WMT16n Multimodal Machine Translation sharedn task, where we participated in Task 1 ton develop German-to-English and Englishto-Germann statistical machine translationn (SMT) systems in the domain of imagen descriptions. Our proposed systems aren standard phrase-based SMT systems basedn on the Moses decoder, trained only on then provided data. We investigate how imagen features can be used to re-rank the n-bestn list produced by the SMT model, with then aim of improving performance by groundingn the translations on images. Our submissionsn are able to outperform the strong,n text-only baseline system for both directions"
W16-2381,{SHEF}-{MIME}: Word-level Quality Estimation Using Imitation Learning,2016,4,1,4,1,5907,daniel beck,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2386,{USFD}{'}s Phrase-level Quality Estimation Systems,2016,5,0,3,1,1966,varvara logacheva,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We describe the submissions of the University of Sheffield (USFD) for the phraselevel Quality Estimation (QE) shared task of WMT16. We test two different approaches for phrase-level QE: (i) we enrich the provided set of baseline features with information about the context of the phrases, and (ii) we exploit predictions at other granularity levels (word and sentence). These approaches perform closely in terms of multiplication of F1-scores (primary evaluation metric), but are considerably different in terms of the F1scores for individual classes."
W16-2388,{S}imple{N}ets: Quality Estimation with Resource-Light Neural Networks,2016,5,3,2,1,1610,gustavo paetzold,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2391,Word embeddings and discourse information for Quality Estimation,2016,0,3,5,1,7140,carolina scarton,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2392,{SHEF}-{LIUM}-{NN}: Sentence level Quality Estimation with Neural Network Features,2016,0,4,4,1,695,kashif shah,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
S16-1085,{S}em{E}val 2016 Task 11: Complex Word Identification,2016,6,21,2,1,1610,gustavo paetzold,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
S16-1095,{SAARSHEFF} at {S}em{E}val-2016 Task 1: Semantic Textual Similarity with Machine Translation Evaluation Metrics and (e{X}treme) Boosted Tree Ensembles,2016,19,4,3,0.304191,10447,liling tan,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
S16-1149,{SV}000gg at {S}em{E}val-2016 Task 11: Heavy Gauge Complex Word Identification with System Voting,2016,8,2,2,1,1610,gustavo paetzold,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
P16-2013,Reference Bias in Monolingual Machine Translation Evaluation,2016,10,6,2,1,2508,marina fomicheva,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In the translation industry, human translations are assessed by comparison with the source texts. In the Machine Translation (MT) research community, however, it is a common practice to perform quality assessment using a reference translation instead of the source text. In this paper we show that this practice has a serious issue - annotators are strongly biased by the reference translation provided, and this can have a negative impact on the assessment of MT quality."
P16-2095,Metrics for Evaluation of Word-level Machine Translation Quality Estimation,2016,6,3,3,1,1966,varvara logacheva,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,None
N16-1050,Inferring Psycholinguistic Properties of Words,2016,12,7,2,1,1610,gustavo paetzold,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
N16-1069,Large-scale Multitask Learning for Machine Translation Quality Estimation,2016,14,3,2,1,695,kashif shah,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
L16-1356,Phrase Level Segmentation and Labelling of Machine Translation Errors,2016,0,2,3,1,3257,frederic blain,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents our work towards a novel approach for Quality Estimation (QE) of machine translation based on sequences of adjacent words, the so-called phrases. This new level of QE aims to provide a natural balance between QE at word and sentence-level, which are either too fine grained or too coarse levels for some applications. However, phrase-level QE implies an intrinsic challenge: how to segment a machine translation into sequence of words (contiguous or not) that represent an error. We discuss three possible segmentation strategies to automatically extract erroneous phrases. We evaluate these strategies against annotations at phrase-level produced by humans, using a new dataset collected for this purpose."
L16-1491,Benchmarking Lexical Simplification Systems,2016,0,4,2,1,1610,gustavo paetzold,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Lexical Simplification is the task of replacing complex words in a text with simpler alternatives. A variety of strategies have been devised for this challenge, yet there has been little effort in comparing their performance. In this contribution, we present a benchmarking of several Lexical Simplification systems. By combining resources created in previous work with automatic spelling and inflection correction techniques, we introduce BenchLS: a new evaluation dataset for the task. Using BenchLS, we evaluate the performance of solutions for various steps in the typical Lexical Simplification pipeline, both individually and jointly. This is the first time Lexical Simplification systems are compared in such fashion on the same data, and the findings introduce many contributions to the field, revealing several interesting properties of the systems evaluated."
L16-1579,A Reading Comprehension Corpus for Machine Translation Evaluation,2016,13,2,2,1,7140,carolina scarton,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Effectively assessing Natural Language Processing output tasks is a challenge for research in the area. In the case of Machine Translation (MT), automatic metrics are usually preferred over human evaluation, given time and budget constraints.However, traditional automatic metrics (such as BLEU) are not reliable for absolute quality assessment of documents, often producing similar scores for documents translated by the same MT system.For scenarios where absolute labels are necessary for building models, such as document-level Quality Estimation, these metrics can not be fully trusted. In this paper, we introduce a corpus of reading comprehension tests based on machine translated documents, where we evaluate documents based on answers to questions by fluent speakers of the target language. We describe the process of creating such a resource, the experiment design and agreement between the test takers. Finally, we discuss ways to convert the reading comprehension test into document-level quality scores."
L16-1582,{MARMOT}: A Toolkit for Translation Quality Estimation at the Word Level,2016,8,10,3,1,1966,varvara logacheva,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present Marmot{\textasciitilde} a new toolkit for quality estimation (QE) of machine translation output. Marmot contains utilities targeted at quality estimation at the word and phrase level. However, due to its flexibility and modularity, it can also be extended to work at the sentence level. In addition, it can be used as a framework for extracting features and learning models for many common natural language processing tasks. The tool has a set of state-of-the-art features for QE, and new features can easily be added. The tool is open-source and can be downloaded from https://github.com/qe-team/marmot/"
L16-1649,{C}ohere: A Toolkit for Local Coherence,2016,7,5,3,1,31609,karin smith,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We describe COHERE, our coherence toolkit which incorporates various complementary models for capturing and measuring different aspects of text coherence. In addition to the traditional entity grid model (Lapata, 2005) and graph-based metric (Guinaudeau and Strube, 2013), we provide an implementation of a state-of-the-art syntax-based model (Louis and Nenkova, 2012), as well as an adaptation of this model which shows significant performance improvements in our experiments. We benchmark these models using the standard setting for text coherence: original documents and versions of the document with sentences in shuffled order."
K16-1021,Exploring Prediction Uncertainty in Machine Translation Quality Estimation,2016,22,1,2,1,5907,daniel beck,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,0,None
C16-3004,Quality Estimation for Language Output Applications,2016,9,0,3,1,7140,carolina scarton,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Tutorial Abstracts",0,"Quality Estimation (QE) of language output applications is a research area that has been attracting significant attention. The goal of QE is to estimate the quality of language output applications without the need of human references. Instead, machine learning algorithms are used to build supervised models based on a few labelled training instances. Such models are able to generalise over unseen data and thus QE is a robust method applicable to scenarios where human input is not available or possible. One such a scenario where QE is particularly appealing is that of Machine Translation, where a score for predicted quality can help decide whether or not a translation is useful (e.g. for post-editing) or reliable (e.g. for gisting). Other potential applications within Natural Language Processing (NLP) include Text Summarisation and Text Simplification. In this tutorial we present the task of QE and its application in NLP, focusing on Machine Translation. We also introduce QuEst++, a toolkit for QE that encompasses feature extraction and machine learning, and propose a practical activity to extend this toolkit in various ways."
C16-2017,{A}nita: An Intelligent Text Adaptation Tool,2016,6,0,2,1,1610,gustavo paetzold,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"We introduce Anita: a flexible and intelligent Text Adaptation tool for web content that provides Text Simplification and Text Enhancement modules. Anita{'}s simplification module features a state-of-the-art system that adapts texts according to the needs of individual users, and its enhancement module allows the user to search for a word{'}s definitions, synonyms, translations, and visual cues through related images. These utilities are brought together in an easy-to-use interface of a freely available web browser extension."
C16-1069,Understanding the Lexical Simplification Needs of Non-Native Speakers of {E}nglish,2016,7,1,2,1,1610,gustavo paetzold,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We report three user studies in which the Lexical Simplification needs of non-native English speakers are investigated. Our analyses feature valuable new insight on the relationship between the non-natives{'} notion of complexity and various morphological, semantic and lexical word properties. Some of our findings contradict long-standing misconceptions about word simplicity. The data produced in our studies consists of 211,564 annotations made by 1,100 volunteers, which we hope will guide forthcoming research on Text Simplification for non-native speakers of English."
C16-1157,Collecting and Exploring Everyday Language for Predicting Psycholinguistic Properties of Words,2016,15,4,2,1,1610,gustavo paetzold,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Exploring language usage through frequency analysis in large corpora is a defining feature in most recent work in corpus and computational linguistics. From a psycholinguistic perspective, however, the corpora used in these contributions are often not representative of language usage: they are either domain-specific, limited in size, or extracted from unreliable sources. In an effort to address this limitation, we introduce SubIMDB, a corpus of everyday language spoken text we created which contains over 225 million words. The corpus was extracted from 38,102 subtitles of family, comedy and children movies and series, and is the first sizeable structured corpus of subtitles made available. Our experiments show that word frequency norms extracted from this corpus are more effective than those from well-known norms such as Kucera-Francis, HAL and SUBTLEXus in predicting various psycholinguistic properties of words, such as lexical decision times, familiarity, age of acquisition and simplicity. We also provide evidence that contradict the long-standing assumption that the ideal size for a corpus can be determined solely based on how well its word frequencies correlate with lexical decision times."
2016.eamt-2.3,Multi-level quality prediction with {Q}u{E}st++,2016,-1,-1,2,0,36158,gustavo hpaetzold,Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products,0,None
W15-4907,The role of artificially generated negative data for quality estimation of machine translation,2015,15,2,2,1,1966,varvara logacheva,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W15-4915,Truly Exploring Multiple References for Machine Translation Evaluation,2015,18,6,2,0,13835,ying qin,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"Multiple references in machine translation evaluation are usually under-explored: they are ignored by alignment-based metrics and treated as bags of n-grams in string matching evaluation metrics, none of which take full advantage of the recurring information in these references. By exploring information on the n-gram distribution and on divergences in multiple references, we propose a method of ngram weighting and implement it to generate new versions of the popular BLEU and NIST metrics. Our metrics are tested in two into-English machine translation datasets. They lead to a significant increase in Pearsonxe2x80x99s correlation with human fluency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation."
W15-4916,Searching for Context: a Study on Document-Level Labels for Translation Quality Estimation,2015,17,13,5,1,7140,carolina scarton,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"In this paper we analyse the use of popular automatic machine translation evaluation metrics to provide labels for quality estimation at document and paragraph levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level."
W15-4940,{O}kapi+{Q}u{E}st: Translation Quality Estimation within Okapi,2015,0,0,2,1,1610,gustavo paetzold,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W15-3001,Findings of the 2015 Workshop on Statistical Machine Translation,2015,78,107,13,0.07424,292,ondvrej bojar,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper presents the results of the WMT15 shared tasks, which included a standard news translation task, a metrics task, a tuning task, a task for run-time estimation of machine translation quality, and an automatic post-editing task. This year, 68 machine translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries."
W15-3020,{S}heffield Systems for the {F}innish-{E}nglish {WMT} Translation Task,2015,10,1,3,1,29284,david steele,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper provides an overview of the Sheffield University submission to the WMT15 Translation Task for the FinnishEnglish language pair. The submitted translations were created from a system built using the CDEC decoder. Finnish is a morphologically rich language with elements such as nouns and verbs carrying a large number of inflectional types. Consequently, our improvements are based on morphology and include preprocessing steps to handle of morphological inflections inherent in the language, and which otherwise result in lexical sparsity and loss of information."
W15-3039,Data enhancement and selection strategies for the word-level Quality Estimation,2015,5,4,3,1,1966,varvara logacheva,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes the DCU-SHEFF word-level Quality Estimation (QE) system submitted to the QE shared task at WMT15. Starting from a baseline set of features and a CRF algorithm to learn a sequence tagging model, we propose improvements in two ways: (i) by filtering out the training sentences containing too few errors, and (ii) by adding incomplete sequences to the training data to enrich the model with new information. We also experiment with considering the task as a classification problem, and report results using a subset of the features with Random Forest classifiers."
W15-3040,{USHEF} and {USAAR}-{USHEF} participation in the {WMT}15 {QE} shared task,2015,0,4,3,1,7140,carolina scarton,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,None
W15-3041,{SHEF}-{NN}: Translation Quality Estimation with Neural Networks,2015,16,10,7,1,695,kashif shah,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"We describe our systems for Tasks 1 and 2 of the WMT15 Shared Task on Quality Estimation. Our submissions use (i) a continuous space language model to extract additional features for Task 1 (SHEFGP, SHEF-SVM), (ii) a continuous bagof-words model to produce word embeddings as features for Task 2 (SHEF-W2V) and (iii) a combination of features produced by QuEst and a feature produced with word embedding models (SHEFQuEst). Our systems outperform the baseline as well as many other submissions. The results are especially encouraging for Task 2, where our best performing system (SHEF-W2V) only uses features learned in an unsupervised fashion."
W15-2507,A Proposal for a Coherence Corpus in Machine Translation,2015,40,3,3,1,31609,karin smith,Proceedings of the Second Workshop on Discourse in Machine Translation,0,"Coherence in Machine Translation (MT) has received little attention to date. One of the main issues we face in work in this area is the lack of labelled data. While coherent (human authored) texts are abundant and incoherent texts could be taken from MT output, the latter also contains other errors which are not specifically related to coherence. This makes it difficult to identify and quantify issues of coherence in those texts. We introduce an initiative to create a corpus consisting of data artificially manipulated to contain errors of coherence common in MT output. Such a corpus could then be used as a benchmark for coherence models in MT, and potentially as training data for coherence models in supervised settings."
S15-2015,{USAAR}-{SHEFFIELD}: Semantic Textual Similarity with Deep Regression and Machine Translation Evaluation Metrics,2015,21,6,3,0.304191,10447,liling tan,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the USAARSHEFFIELD systems that participated in the Semantic Textual Similarity (STS) English task of SemEval-2015. We extend the work on using machine translation evaluation metrics in the STS task. Different from previous approaches, we regard the metricsxe2x80x99 robustness across different text types and conflate the training data across different subcorpora. In addition, we introduce a novel deep regressor architecture and evaluated its efficiency in the STS task."
Q15-1033,Learning Structural Kernels for Natural Language Processing,2015,44,6,4,1,5907,daniel beck,Transactions of the Association for Computational Linguistics,0,"Structural kernels are a flexible learning paradigm that has been widely used in Natural Language Processing. However, the problem of model selection in kernel-based methods is usually overlooked. Previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search, which is slow and coarse-grained. In contrast, Bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods. In this paper we show how to perform this in the context of structural kernels by using Gaussian Processes. Experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search. The framework proposed in this paper can be adapted to other structures besides trees, e.g., strings and graphs, thereby extending the utility of kernel-based methods."
P15-4015,{LEX}enstein: A Framework for Lexical Simplification,2015,17,10,2,1,1610,gustavo paetzold,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"Lexical Simplification consists in replacing complex words in a text with simpler alternatives. We introduce LEXenstein, the first open source framework for Lexical Simplification. It covers all major stages of the process and allows for easy benchmarking of various approaches. We test the toolxe2x80x99s performance and report comparisons on different datasets against the state of the art approaches. The results show that combining the novel Substitution Selection and Substitution Ranking approaches introduced in LEXenstein is the most effective approach to Lexical Simplification."
P15-4020,Multi-level Translation Quality Prediction with {Q}u{E}st++,2015,12,41,1,1,2509,lucia specia,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"This paper presents QUEST , an open source tool for quality estimation which can predict quality for texts at word, sentence and document level. It also provides pipelined processing, whereby predictions made at a lower level (e.g. for words) can be used as input to build models for predictions at a higher level (e.g. sentences). QUEST allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that QUEST achieves state-of-the-art performance."
P15-4021,{WA}-Continuum: Visualising Word Alignments across Multiple Parallel Sentences Simultaneously,2015,6,0,2,1,29284,david steele,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"Word alignment (WA) between a pair of sentences in the same or different languages is a key component of many natural language processing tasks. It is commonly used for identifying the translation relationships between words and phrases in parallel sentences from two different languages. WA-Continuum is a tool designed for the visualisation of WAs. It was initially built to aid research studying WAs and ways to improve them. The tool relies on the automated mark-up of WAs, as typically produced by WA tools. Different from most previous work, it presents the alignment information graphically in a WA matrix that can be easily understood by users, as opposed to text connected by lines. The key features of the tool are the ability to visualise WA matrices for multiple parallel aligned sentences simultaneously in a single place, coupled with powerful search and selection components to find and inspect particular sentences as required."
D15-1125,Investigating Continuous Space Language Models for Machine Translation Quality Estimation,2015,18,2,4,1,695,kashif shah,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present novel features designed with a deep neural network for Machine Translation (MT) Quality Estimation (QE). The features are learned with a Continuous Space Language Model to estimate the probabilities of the source and target segments. These new features, along with standard MT system-independent features, are benchmarked on a series of datasets with various quality labels, including postediting effort, human translation edit rate, post-editing time and METEOR. Results show significant improvements in prediction over the baseline, as well as over systems trained on state of the art feature sets for all datasets. More notably, the addition of the newly proposed features improves over the best QE systems in WMT12 and WMT14 by a significant margin."
2015.tc-1.3,The {EXPERT} project: Advancing the state of the art in hybrid translation technologies,2015,8,0,9,0.386034,12551,constantin orasan,Proceedings of Translating and the Computer 37,0,None
2015.iwslt-papers.4,Phrase-level estimation for machine translation,2015,-1,-1,2,1,1966,varvara logacheva,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,None
2015.eamt-1.8,The role of artificially generated negative data for quality estimation of machine translation,2015,15,2,2,1,1966,varvara logacheva,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
2015.eamt-1.16,Truly Exploring Multiple References for Machine Translation Evaluation,2015,18,6,2,0,13835,ying qin,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"Multiple references in machine translation evaluation are usually under-explored: they are ignored by alignment-based metrics and treated as bags of n-grams in string matching evaluation metrics, none of which take full advantage of the recurring information in these references. By exploring information on the n-gram distribution and on divergences in multiple references, we propose a method of ngram weighting and implement it to generate new versions of the popular BLEU and NIST metrics. Our metrics are tested in two into-English machine translation datasets. They lead to a significant increase in Pearsonxe2x80x99s correlation with human fluency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation."
2015.eamt-1.17,Searching for Context: a Study on Document-Level Labels for Translation Quality Estimation,2015,17,13,5,1,7140,carolina scarton,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"In this paper we analyse the use of popular automatic machine translation evaluation metrics to provide labels for quality estimation at document and paragraph levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level."
2015.eamt-1.41,{O}kapi+{Q}u{E}st: Translation Quality Estimation within Okapi,2015,0,0,2,1,1610,gustavo paetzold,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
W14-3302,Findings of the 2014 Workshop on Statistical Machine Translation,2014,75,148,12,0.0864455,292,ondvrej bojar,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries"
W14-3338,{SHEF}-Lite 2.0: Sparse Multi-task {G}aussian Processes for Translation Quality Estimation,2014,14,7,3,1,5907,daniel beck,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We describe our systems for the WMT14 Shared Task on Quality Estimation (subtasks 1.1, 1.2 and 1.3). Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting. Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speed up training and prediction by providing sensible sparse approximations."
W14-3343,Exploring Consensus in Machine Translation for Quality Estimation,2014,11,3,2,1,7140,carolina scarton,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,This paper presents the use of consensus among Machine Translation (MT) systems for the WMT14 Quality Estimation shared task. Consensus is explored here by comparing the MT system output against several alternative machine translations using standard evaluation metrics. Figures extracted from such metrics are used as features to complement baseline prediction models. The hypothesis is that knowing whether the translation of interest is similar or dissimilar to translations from multiple different MT systems can provide useful information regarding the quality of such a translation.
W14-1214,An Analysis of Crowdsourced Text Simplifications,2014,19,2,2,0,38775,marcelo amancio,Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations ({PITR}),0,"We present a study on the text simplification operations undertaken collaboratively by Simple English Wikipedia contributors. The aim is to understand whether a complex-simple parallel corpus involving this version of Wikipedia is appropriate as data source to induce simplification rules, and whether we can automatically categorise the different operations performed by humans. A subset of the corpus was first manually analysed to identify its transformation operations. We then built machine learning models to attempt to automatically classify segments based on such transformations. This classification could be used, e.g., to filter out potentially noisy transformations. Our results show that the most common transformation operations performed by humans are paraphrasing (39.80%) and drop of information (26.76%), which are some of the most difficult operations to generalise from data. They are also the most difficult operations to identify automatically, with the lowest overall classifier accuracy among all operations (73% and 59%, respectively)."
W14-0312,Confidence-based Active Learning Methods for Machine Translation,2014,13,0,2,1,1966,varvara logacheva,Proceedings of the {EACL} 2014 Workshop on Humans and Computer-assisted Translation,0,"The paper presents experiments with active learning methods for the acquisition of training data in the context of machine translation. We propose a confidencebased method which is superior to the state-of-the-art method both in terms of quality and complexity. Additionally, we discovered that oracle selection techniques that use real quality scores lead to poor results, making the effectiveness of confidence-driven methods of active learning for machine translation questionable."
logacheva-specia-2014-quality,A Quality-based Active Sample Selection Strategy for Statistical Machine Translation,2014,17,3,2,1,1966,varvara logacheva,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents a new active learning technique for machine translation based on quality estimation of automatically translated sentences. It uses an error-driven strategy, i.e., it assumes that the more errors an automatically translated sentence contains, the more informative it is for the translation system. Our approach is based on a quality estimation technique which involves a wider range of features of the source text, automatic translation, and machine translation system compared to previous work. In addition, we enhance the machine translation system training data with post-edited machine translations of the sentences selected, instead of simulating this using previously created reference translations. We found that re-training systems with additional post-edited data yields higher quality translations regardless of the selection strategy used. We relate this to the fact that post-editions tend to be closer to source sentences as compared to references, making the rule extraction process more reliable."
shah-etal-2014-efficient,An efficient and user-friendly tool for machine translation quality estimation,2014,14,6,3,1,695,kashif shah,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a new version of QUEST  an open source framework for machine translation quality estimation  which brings a number of improvements: (i) it provides a Web interface and functionalities such that non-expert users, e.g. translators or lay-users of machine translations, can get quality predictions (or internal features of the framework) for translations without having to install the toolkit, obtain resources or build prediction models; (ii) it significantly improves over the previous runtime performance by keeping resources (such as language models) in memory; (iii) it provides an option for users to submit the source text only and automatically obtain translations from Bing Translator; (iv) it provides a ranking of multiple translations submitted by users for each source text according to their estimated quality. We exemplify the use of this new version through some experiments with the framework."
D14-1131,Exact Decoding for Phrase-Based Statistical Machine Translation,2014,43,3,3,1,5041,wilker aziz,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model. We replace this intractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model. Exact optimisation is achieved through a coarseto-fine strategy with connections to adaptive rejection sampling. We perform exact optimisation with unpruned language models of order 3 to 5 and show searcherror curves for beam search and cube pruning on standard test sets. This is the first work to tractably tackle exact optimisation with language models of orders higher than 3.
D14-1190,Joint Emotion Analysis via Multi-task {G}aussian Processes,2014,22,19,3,1,5907,daniel beck,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We propose a model for jointly predicting multiple emotions in natural language sentences. Our model is based on a low-rank coregionalisation approach, which combines a vector-valued Gaussian Process with a rich parameterisation scheme. We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches."
2014.iwslt-evaluation.11,The {USFD} {SLT} system for {IWSLT} 2014,2014,22,1,9,0.739121,11254,raymond ng,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"The University of Sheffield (USFD) participated in the International Workshop for Spoken Language Translation (IWSLT) in 2014. In this paper, we will introduce the USFD SLT system for IWSLT. Automatic speech recognition (ASR) is achieved by two multi-pass deep neural network systems with adaptation and rescoring techniques. Machine translation (MT) is achieved by a phrase-based system. The USFD primary system incorporates state-of-the-art ASR and MT techniques and gives a BLEU score of 23.45 and 14.75 on the English-to-French and English-to-German speech-to-text translation task with the IWSLT 2014 data. The USFD contrastive systems explore the integration of ASR and MT by using a quality estimation system to rescore the ASR outputs, optimising towards better translation. This gives a further 0.54 and 0.26 BLEU improvement respectively on the IWSLT 2012 and 2014 evaluation data."
2014.eamt-1.7,Data selection for discriminative training in statistical machine translation,2014,42,12,2,1,16931,xingyi song,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,"We introduce submodular optimization to the problem of training data subset selection for statistical machine translation (SMT). By explicitly formulating data selection as a submodular program, we obtain fast scalable selection algorithms with mathematical performance guarantees, resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible. We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks. Our results show that our best submodular method significantly outperforms several baseline methods, including the widely-used cross-entropy based data selection method. In addition, our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing."
2014.eamt-1.21,Document-level translation quality estimation: exploring discourse and pseudo-references,2014,-1,-1,2,1,7140,carolina scarton,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,None
2014.eamt-1.22,Quality estimation for translation selection,2014,-1,-1,2,0,40398,kahif shah,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,None
2014.amta-wptp.12,{Q}u{E}st: A framework for translation quality estimation,2014,-1,-1,1,1,2509,lucia specia,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas,0,"We present QUEST, an open source framework for translation quality estimation. QUEST provides a wide range of feature extractors from source and translation texts and external resources and tools. These go from simple, language-independent features, to advanced, linguistically motivated features. They include features that rely on information from the translation system and features that are oblivious to the way translations were produced. In addition, it provides wrappers for a well-known machine learning toolkit, scikit-learn, including techniques for feature selection and model building, as well as parameter optimisation. We also present a Web interface and functionalities for non-expert users. Using this interface, quality predictions (or internal features of the framework) can be obtained without the installation of the toolkit and the building of prediction models. The interface also provides a ranking method for multiple translations given for the same source text according to their predicted quality."
2014.amta-researchers.22,Predicting human translation quality,2014,-1,-1,1,1,2509,lucia specia,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"We present a first attempt at predicting the quality of translations produced by human, professional translators. We examine datasets annotated for quality at sentence- and word-level for four language pairs and provide experiments with prediction models for these datasets. We compare the performance of such models against that of models built from machine translations, highlighting a number of challenges in estimating quality and detecting errors in human translations."
W13-4813,Text Simplification as Tree Transduction,2013,18,12,2,1,1610,gustavo paetzold,Proceedings of the 9th {B}razilian Symposium in Information and Human Language Technology,0,"Lexical and syntactic simplification aim to make texts more accessible to certain audiences. Syntactic simplification uses either hand-crafted linguis- tic rules for deep syntactic transformations, or machine learning techniques to model simpler transformations. Lexical simplification performs a lookup for synonyms followed by context and/or frequency-based models. In this paper we investigate modelling both syntactic and lexical simplification through the learning of general tree transduction rules. Experiments with the Simple En- glish Wikipedia corpus show promising results but highlight the need for clever filtering strategies to remove noisy transformations."
W13-3522,Multilingual {WSD}-like Constraints for Paraphrase Extraction,2013,23,2,2,1,5041,wilker aziz,Proceedings of the Seventeenth Conference on Computational Natural Language Learning,0,"The use of pivot languages and wordalignment techniques over bilingual corpora has proved an effective approach for extracting paraphrases of words and short phrases. However, inherent ambiguities in the pivot language(s) can lead to inadequate paraphrases. We propose a novel approach that is able to extract paraphrases by pivoting through multiple languages while discriminating word senses in the input language, i.e., the language to be paraphrased. Text in the input language is annotated with xe2x80x9csensesxe2x80x9d in the form of foreign phrases obtained from bilingual parallel data and automatic word-alignment. This approach shows 62% relative improvement over previous work in generating paraphrases that are judged both more accurate and more fluent."
W13-2201,Findings of the 2013 {W}orkshop on {S}tatistical {M}achine {T}ranslation,2013,86,192,10,0.0864455,292,ondvrej bojar,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present the results of the WMT13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task. This year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. The quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries."
W13-2241,{SHEF}-{L}ite: When Less is More for Translation Quality Estimation,2013,12,20,4,1,5907,daniel beck,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We describe the results of our submissions to the WMT13 Shared Task on Quality Estimation (subtasks 1.1 and 1.3). Our submissions use the framework of Gaussian Processes to investigate lightweight approaches for this problem. We focus on two approaches, one based on feature selection and another based on active learning. Using only 25 (out of 160) features, our model resulting from feature selection ranked 1st place in the scoring variant of subtask 1.1 and 3rd place in the ranking variant of the subtask, while the active learning model reached 2nd place in the scoring variant using only 25% of the available instances for training. These results give evidence that Gaussian Processes achieve the state of the art performance as a modelling approach for translation quality estimation, and that carefully selecting features and instances for the problem can further improve or at least maintain the same performance levels while making the problem less resource-intensive."
P13-4014,{Q}u{E}st - A translation quality estimation framework,2013,10,99,1,1,2509,lucia specia,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We describe QUEST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benchmark the framework on a number of datasets and discuss the efficacy of features and algorithms."
P13-2097,Reducing Annotation Effort for Quality Estimation via Active Learning,2013,16,7,2,1,5907,daniel beck,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Quality estimation models provide feedback on the quality of machine translated texts. They are usually trained on humanannotated datasets, which are very costly due to its task-specific nature. We investigate active learning techniques to reduce the size of these datasets and thus annotation effort. Experiments on a number of datasets show that with as little as 25% of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets. In other words, our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors."
P13-1004,Modelling Annotator Bias with Multi-task {G}aussian Processes: An Application to Machine Translation Quality Estimation,2013,30,74,2,0.121493,1787,trevor cohn,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Annotating linguistic data is often a complex, time consuming and expensive endeavour. Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpretations of the task and levels of consistency. We present novel techniques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour. These techniques use multi-task Gaussian Processes to learn jointly a series of annotator and metadata specific models, while explicitly representing correlations between models which can be learned directly from data. Our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning, and consistently outperform strong baselines."
2013.tc-1.10,Machine translation quality estimation,2013,6,6,1,1,2509,lucia specia,Proceedings of Translating and the Computer 35,0,None
2013.mtsummit-posters.12,Key Problems in Conversion from Simplified to Traditional {C}hinese Characters Topic Models for Translation Quality Estimation for Gisting Purposes,2013,-1,-1,4,0,8609,raphael rubino,Proceedings of Machine Translation Summit XIV: Posters,0,None
2013.mtsummit-posters.13,Topic Models for Translation Quality Estimation for Gisting Purposes,2013,22,19,4,0,8609,raphael rubino,Proceedings of Machine Translation Summit XIV: Posters,0,"This paper addresses the problem of predicting how adequate a machine translation is for gisting purposes. It focuses on the contribution of lexicalised features based on different types of topic models, as we believe these features are more robust than those used in previous work, which depend on linguistic processors that are often unreliable on automatic translations. Experiments with a number of datasets show promising results: the use of topic models outperforms the state-of-the-art approaches by a large margin in all datasets annotated for adequacy."
2013.mtsummit-papers.21,An Investigation on the Effectiveness of Features for Translation Quality Estimation,2013,-1,-1,3,1,695,kashif shah,Proceedings of Machine Translation Summit XIV: Papers,0,None
2013.mtsummit-european.7,Pangeanic in the {EXPERT} Project: Exploiting Empirical app{R}oaches to Translation,2013,-1,-1,5,0,4981,manuel herranz,Proceedings of Machine Translation Summit XIV: European projects,0,None
2013.mtsummit-european.17,{QTL}aunchpad,2013,-1,-1,7,0,40954,stephen doherty,Proceedings of Machine Translation Summit XIV: European projects,0,None
W12-3102,Findings of the 2012 Workshop on Statistical Machine Translation,2012,63,247,6,0,3274,chris callisonburch,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. We introduced a new quality estimation task this year, and evaluated submissions from 11 teams."
W12-3110,Linguistic Features for Quality Estimation,2012,25,51,2,0,21029,mariano felice,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes a study on the contribution of linguistically-informed features to the task of quality estimation for machine translation at sentence level. A standard regression algorithm is used to build models using a combination of linguistic and non-linguistic features extracted from the input text and its machine translation. Experiments with English-Spanish translations show that linguistic features, although informative on their own, are not yet able to outperform shallower features based on statistics from the input text, its translation and additional corpora. However, further analysis suggests that linguistic information is actually useful but needs to be carefully combined with other features in order to produce better results."
S12-1046,{S}em{E}val-2012 Task 1: {E}nglish Lexical Simplification,2012,14,54,1,1,2509,lucia specia,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"We describe the English Lexical Simplification task at SemEval-2012. This is the first time such a shared task has been organized and its goal is to provide a framework for the evaluation of systems for lexical simplification and foster research on context-aware lexical simplification approaches. The task requires that annotators and systems rank a number of alternative substitutes -- all deemed adequate -- for a target word in context, according to how simple these substitutes are. The notion of simplicity is biased towards non-native speakers of English. Out of nine participating systems, the best scoring ones combine context-dependent and context-independent information, with the strongest individual contribution given by the frequency of the substitute regardless of its context."
S12-1066,{UOW}-{SHEF}: {S}imp{L}ex {--} Lexical Simplicity Ranking based on Contextual and Psycholinguistic Features,2012,10,21,2,0,3909,sujay jauhar,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"This paper describes SimpLex, a Lexical Simplification system that participated in the English Lexical Simplification shared task at SemEval-2012. It operates on the basis of a linear weighted ranking function composed of context sensitive and psycholinguistic features. The system outperforms a very strong baseline, and ranked first on the shared task."
S12-1100,{UOW}: Semantically Informed Text Similarity,2012,12,10,3,1,25928,miguel rios,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"The UOW submissions to the Semantic Textual Similarity task at SemEval-2012 use a supervised machine learning algorithm along with features based on lexical, syntactic and semantic similarity metrics to predict the semantic equivalence between a pair of sentences. The lexical metrics are based on word-overlap. A shallow syntactic metric is based on the overlap of base-phrase labels. The semantically informed metrics are based on the preservation of named entities and on the alignment of verb predicates and the overlap of argument roles using inexact matching. Our submissions outperformed the official baseline, with our best system ranked above average, but the contribution of the semantic metrics was not conclusive."
aziz-etal-2012-pet,{PET}: a Tool for Post-editing and Assessing Machine Translation,2012,7,63,3,1,5041,wilker aziz,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Given the significant improvements in Machine Translation (MT) quality and the increasing demand for translations, post-editing of automatic translations is becoming a popular practice in the translation industry. It has been shown to allow for much larger volumes of translations to be produced, saving time and costs. In addition, the post-editing of automatic translations can help understand problems in such translations and this can be used as feedback for researchers and developers to improve MT systems. Finally, post-editing can be used as a way of evaluating the quality of translations in terms of how much post-editing effort these translations require. We describe a standalone tool that has two main purposes: facilitate the post-editing of translations from any MT system so that they reach publishable quality and collect sentence-level information from the post-editing process, e.g.: post-editing time and detailed keystroke statistics."
C12-2020,Linguistic and Statistical Traits Characterising Plagiarism,2012,17,4,2,0,43687,miranda chong,Proceedings of {COLING} 2012: Posters,0,"This paper investigates the problem of distinguishing between original and rewritten text materials, with focus on the application of plagiarism detection. The hypothesis is that original texts and rewritten texts exhibit significant and measurable differences, and that these can be captured through statistical and linguistic indicators. We propose and analyse a number of these indicators (including language models, syntactic trees, etc.) using machine learning algorithms in two main settings: (i) the classification of individual text segments as original or rewritten, and (ii) the ranking of two or more versions of a text segment according to their xe2x80x9coriginalityxe2x80x9d, thus rendering the rewriting direction. Different from standard plagiarism detection approaches, our settings do not involve comparisons between supposedly rewritten text and (a large number of) original texts. Instead, our work focuses on the sub-problem of finding segments that exhibit rewriting traits. Identifying such segments has a number of potential applications, from a first-stage filtering for standard plagiarism detection approaches, to intrinsic plagiarism detection and authorship identification. The corpus used in the experiments was extracted from the PAN-PC-10 plagiarism detection task, with two subsets containing manually and artificially generated plagiarism cases. The accuracies achieved are well above a by chance baseline across datasets and settings, with the statistical indicators being particularly effective."
C12-2112,Automatic Question Generation in Multimedia-Based Learning,2012,13,4,3,0,43719,yvonne skalban,Proceedings of {COLING} 2012: Posters,0,"We investigate whether questions generated automatically by two Natural Language Processing (NLP) based systems (one developed by the authors, the other a state-of-the-art system) can successfully be used to assist multimedia-based learning. We examine the feasibility of using a Question Generation (QG) systemxe2x80x99s output as pre-questions; with different types of pre-questions used: text-based and with images. We also compare the psychometric parameters of the automatically generated questions by the two systems and of those generated manually. Specifically, we analyse the effect such pre-questions have on test-takersxe2x80x99 performance on a comprehension test about a scientific video documentary. We also compare the discrimination power of the questions generated automatically against that of questions generated manually. The results indicate that the presence of pre-questions (preferably with images) improves the performance of test-takers. They indicate that the psychometric parameters of the questions generated by our system are comparable if not better than those of the state-of-the-art system."
2012.tc-1.5,{PET}: a standalone tool for assessing machine translation through post-editing,2012,7,63,2,1,5041,wilker aziz,Proceedings of Translating and the Computer 34,0,"Given the significant improvements in Machine Translation (MT) quality and the increasing demand for translations, post-editing of automatic translations is becoming a popular practice in the translation industry. It has been shown to allow for larger volumes of translations to be produced, saving time and costs. In addition, the post-editing of automatic translations can help understand problems in such translations and this can be used as feedback for researchers and developers to improve MT systems. Finally, post-editing can be used as a way of evaluating the quality of translations in terms of how much effort these translations require in order to be fixed. We describe a standalone tool that has two main purposes: facilitate the post-editing of translations from any MT system so that they reach publishable quality and collect sentence-level information from the post-editing process, e.g.: post-editing time and detailed keystroke statistics."
2012.eamt-1.33,Cross-lingual Sentence Compression for Subtitles,2012,-1,-1,3,1,5041,wilker aziz,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,None
2012.eamt-1.39,Relevance Ranking for Translated Texts,2012,20,2,3,0.168339,5084,marco turchi,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"The usefulness of a translated text for gisting purposes strongly depends on the overall translation quality of the text, but especially on the translation quality of the most informative portions of the text. In this paper we address the problems of ranking translated sentences within a document and ranking translated documents within a set of documents on the same topic according to their informativeness and translation quality. An approach combining quality estimation and sentence ranking methods is used. Experiments with French-English translation using four sets of news commentary documents show promising results for both sentence and document ranking. We believe that this approach can be useful in several practical scenarios where translation is aimed at gisting, such as multilingual media monitoring and news analysis applications."
2012.amta-wptp.2,Post-editing time as a measure of cognitive effort,2012,13,36,4,0,20837,maarit koponen,Workshop on Post-Editing Technology and Practice,0,"Post-editing machine translations has been attracting increasing attention both as a common practice within the translation industry and as a way to evaluate Machine Translation (MT) quality via edit distance metrics between the MT and its post-edited version. Commonly used metrics such as HTER are limited in that they cannot fully capture the effort required for post-editing. Particularly, the cognitive effort required may vary for different types of errors and may also depend on the context. We suggest post-editing time as a way to assess some of the cognitive effort involved in post-editing. This paper presents two experiments investigating the connection between post-editing time and cognitive effort. First, we examine whether sentences with long and short post-editing times involve edits of different levels of difficulty. Second, we study the variability in post-editing time and other statistics among editors."
W11-4533,Fully Automatic Compilation of {P}ortuguese-{E}nglish and {P}ortuguese-{S}panish Parallel Corpora,2011,10,13,2,1,5041,wilker aziz,Proceedings of the 8th {B}razilian Symposium in Information and Human Language Technology,0,"This paper reports the fully automatic compilation of parallel cor- pora for Brazilian Portuguese. Scientific news texts available in Brazilian Por- tuguese, English and Spanish are automatically crawled from a multilingual Brazilian magazine. The texts are then automatically aligned at document- and sentence-level. The resulting corpora contain about 2,700 parallel documents totaling over 150,000 aligned sentences each. The quality of the corpora and their usefulness are tested in an experiment with machine translation."
W11-2315,Towards an on-demand Simple {P}ortuguese {W}ikipedia,2011,20,1,3,0,7636,arnaldo jr,Proceedings of the Second Workshop on Speech and Language Processing for Assistive Technologies,0,"The Simple English Wikipedia provides a simplified version of Wikipedia's English articles for readers with special needs. However, there are fewer efforts to make information in Wikipedia in other languages accessible to a large audience. This work proposes the use of a syntactic simplification engine with high precision rules to automatically generate a Simple Portuguese Wikipedia on demand, based on user interactions with the main Portuguese Wikipedia. Our estimates indicated that a human can simplify about 28,000 occurrences of analysed patterns per million words, while our system can correctly simplify 22,200 occurrences, with estimated f-measure 77.2%."
W11-2112,{TINE}: A Metric to Assess {MT} Adequacy,2011,18,20,3,1,25928,miguel rios,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"We describe TINE, a new automatic evaluation metric for Machine Translation that aims at assessing segment-level adequacy. Lexical similarity and shallow-semantics are used as indicators of adequacy between machine and reference translations. The metric is based on the combination of a lexical matching component and an adequacy component. Lexical matching is performed comparing bags-of-words without any linguistic annotation. The adequacy component consists in: i) using ontologies to align predicates (verbs), ii) using semantic roles to align predicate arguments (core arguments and modifiers), and iii) matching predicate arguments using distributional semantics. TINE's performance is comparable to that of previous metrics at segment level for several language pairs, with average Kendall's tau correlation from 0.26 to 0.29. We show that the addition of the shallow-semantic component improves the performance of simple lexical matching strategies and metrics such as BLEU."
W11-2136,Shallow Semantic Trees for {SMT},2011,15,19,3,1,5041,wilker aziz,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,We present a translation model enriched with shallow syntactic and semantic information about the source language. Base-phrase labels and semantic role labels are incorporated into an hierarchical model by creating shallow semantic trees. Results show an increase in performance of up to 6% in BLEU scores for English-Spanish translation over a standard phrase-based SMT baseline.
R11-1014,Assessing the Post-Editing Effort for Automatic and Semi-Automatic Translations of {DVD} Subtitles,2011,8,22,3,1,43071,sheila sousa,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"With the increasing demand for fast and accurate audiovisual translation, subtitlers are starting to consider the use of translation technologies to support their work. An important issue that arises from the use of such technologies is measuring how much effort needs to be put in by the subtitler in post-editing (semi-)automatic translations. In this paper we present an objective way of measuring post-editing effort in terms of time. In experiments with English-Portuguese subtitles, we measure the post-editing effort of texts translated using machine translation and translation memory systems. We also contrast this effort against that of translating the texts without any tools. Results show that post-editing is on average 40% faster than translating subtitles from scratch. With our best system, more than 69% of the translations require little or no post"
R11-1031,Improving Chunk-based Semantic Role Labeling with Lexical Features,2011,15,3,3,1,5041,wilker aziz,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"We present an approach for Semantic Role Labeling (SRL) using Conditional Random Fields in a joint identification/classification step. The approach is based on shallow syntactic information (chunks) and a number of lexicalized features such as selectional preferences and automatically inferred similar words, extracted using lexical databases and distributional similarity metrics. We use semantic annotations from the Proposition Bank for training and evaluate the system using CoNLL-2005 test sets. The additional lexical information led to improvements of 15% (in-domain evaluation) and 12% (out-of-domain evaluation) on overall semantic role classification in terms of F-measure. The gains come mostly from a better recall, which suggests that the addition of richer lexical information can improve the coverage of existing SRL models even when very little syntactic knowledge is available."
R11-1102,Lexical Generalisation for Word-level Matching in Plagiarism Detection,2011,10,11,2,0,43687,miranda chong,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"Plagiarism has always been a concern in many sectors, particularly in education. With the sharp rise in the number of electronic resources available online, an increasing number of plagiarism cases has been observed in recent years. As the amount of source materials is vast, the use of plagiarism detection tools has become the norm to aid the investigation of possible plagiarism cases. This paper describes an approach to improve plagiarism detection by incorporating a lexical generalisation technique. The goal is to identify plagiarised texts even if they are paraphrased using different words. Experiments performed on a subset of the PANxe2x80x9f10 corpus show that the matching approach involving lexical generalisation yields promising results, as compared to standard n-gram matching"
2011.mtsummit-papers.58,Predicting Machine Translation Adequacy,2011,-1,-1,1,1,2509,lucia specia,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.eamt-1.12,Exploiting Objective Annotations for Minimising Translation Post-editing Effort,2011,16,100,1,1,2509,lucia specia,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"With the noticeable improvement in the overall quality of Machine Translation (MT) systems in recent years, post-editing of MT output is starting to become a common practice among human translators. However, it is well known that the quality of a given MT system can vary significantly across translation segments and that post-editing bad quality translations is a tedious task that may require more effort than translating texts from scratch. Previous research dedicated to learning quality estimation models to flag such segments has shown that models based on human annotation achieve more promising results. However, it is not yet clear what is the most appropriate form of human annotation for building such models. We experiment with models based on three annotation types (post-editing time, post-editing distance and post-editing effort scores) and show that estimations resulting from using post-editing time, a simple and objective annotation, can reliably indicate translation post-editing effort in a practical, taskbased scenario. We also discuss some perspectives on the effectiveness, reliability and cost of each type of annotation."
W10-1001,Readability Assessment for Text Simplification,2010,27,68,2,0,7637,sandra aluisio,Proceedings of the {NAACL} {HLT} 2010 Fifth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We describe a readability assessment approach to support the process of text simplification for poor literacy readers. Given an input text, the goal is to predict its readability level, which corresponds to the literacy level that is expected from the target reader: rudimentary, basic or advanced. We complement features traditionally used for readability assessment with a number of new features, and experiment with alternative ways to model this problem using machine learning methods, namely classification, regression and ranking. The best resulting model is embedded in an authoring tool for Text Simplification."
S10-1024,Combining Dictionaries and Contextual Information for Cross-Lingual Lexical Substitution,2010,4,0,2,1,5041,wilker aziz,Proceedings of the 5th International Workshop on Semantic Evaluation,0,None
specia-etal-2010-dataset,A Dataset for Assessing Machine Translation Evaluation Metrics,2010,20,21,1,1,2509,lucia specia,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We describe a dataset containing 16,000 translations produced by four machine translation systems and manually annotated for quality by professional translators. This dataset can be used in a range of tasks assessing machine translation evaluation metrics, from basic correlation analysis to training and test of machine learning-based metrics. By providing a standard dataset for such tasks, we hope to encourage the development of better MT evaluation metrics."
2010.jec-1.5,Estimating Machine Translation Post-Editing Effort with {HTER},2010,-1,-1,1,1,2509,lucia specia,Proceedings of the Second Joint EM+/CNGL Workshop: Bringing MT to the User: Research on Integrating MT in the Translation Industry,0,"Although Machine Translation (MT) has been attracting more and more attention from the translation industry, the quality of current MT systems still requires humans to post-edit translations to ensure their quality. The time necessary to post-edit bad quality translations can be the same or even longer than that of translating without an MT system. It is well known, however, that the quality of an MT system is generally not homogeneous across all translated segments. In order to make MT more useful to the translation industry, it is therefore crucial to have a mechanism to judge MT quality at the segment level to prevent bad quality translations from being post-edited within the translation workflow. We describe an approach to estimate translation post-editing effort at sentence level in terms of Human-targeted Translation Edit Rate (HTER) based on a number of features reflecting the difficulty of translating the source sentence and discrepancies between the source and translation sentences. HTER is a simple metric and obtaining HTER annotated data can be made part of the translation workflow. We show that this approach is more reliable at filtering out bad translations than other simple criteria commonly used in the translation industry, such as sentence length."
2010.eamt-1.31,Learning an Expert from Human Annotations in Statistical Machine Translation: the Case of Out-of-Vocabulary Words,2010,-1,-1,3,1,5041,wilker aziz,Proceedings of the 14th Annual conference of the European Association for Machine Translation,0,None
2010.amta-papers.3,Combining Confidence Estimation and Reference-based Metrics for Segment-level {MT} Evaluation,2010,-1,-1,1,1,2509,lucia specia,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"We describe an effort to improve standard reference-based metrics for Machine Translation (MT) evaluation by enriching them with Confidence Estimation (CE) features and using a learning mechanism trained on human annotations. Reference-based MT evaluation metrics compare the system output against reference translations looking for overlaps at different levels (lexical, syntactic, and semantic). These metrics aim at comparing MT systems or analyzing the progress of a given system and are known to have reasonably good correlation with human judgments at the corpus level, but not at the segment level. CE metrics, on the other hand, target the system in use, providing a quality score to the end-user for each translated segment. They cannot rely on reference translations, and use instead information extracted from the input text, system output and possibly external corpora to train machine learning algorithms. These metrics correlate better with human judgments at the segment level. However, they are usually highly biased by difficulty level of the input segment, and therefore are less appropriate for comparing multiple systems translating the same input segments. We show that these two classes of metrics are complementary and can be combined to provide MT evaluation metrics that achieve higher correlation with human judgments at the segment level."
W09-2105,Supporting the Adaptation of Texts for Poor Literacy Readers: a Text Simplification Editor for {B}razilian {P}ortuguese,2009,17,41,3,0,46973,arnaldo candido,Proceedings of the Fourth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper we investigate the task of text simplification for Brazilian Portuguese. Our purpose is three-fold: to introduce a simplification tool for such language and its underlying development methodology, to present an on-line authoring system of simplified text based on the previous tool, and finally to discuss the potentialities of such technology for education. The resources and tools we present are new for Portuguese and innovative in many aspects with respect to previous initiatives for other languages."
P09-1089,Source-Language Entailment Modeling for Translating Unknown Terms,2009,34,47,2,0.147679,27123,shachar mirkin,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,This paper addresses the task of handling unknown terms in SMT. We propose using source-language monolingual models and resources to paraphrase the source text prior to translation. We further present a conceptual extension to prior work by allowing translations of entailed texts rather than paraphrases only. A method for performing this process efficiently is presented and applied to some 2500 sentences with unknown terms. Our experiments show that the proposed approach substantially increases the number of properly translated texts.
2009.mtsummit-papers.16,Improving the Confidence of Machine Translation Quality Estimates,2009,15,60,1,1,2509,lucia specia,Proceedings of Machine Translation Summit XII: Papers,0,We investigate the problem of estimating the quality of the output of machine translation systems at the sentence level when reference translations are not available. The focus is on automatically identifying a threshold to map a continuous predicted score into xe2x80x9cgoodxe2x80x9d / xe2x80x9cbadxe2x80x9d categories for filtering out bad-quality cases in a translation post-edition task. We use the theory of Inductive Confidence Machines (ICM) to identify this threshold according to a confidence level that is expected for a given task. Experiments show that this approach gives improved estimates when compared to those based on classification or regression algorithms without ICM.
2009.eamt-smart.10,Sentence-level confidence estimation for {MT},2009,-1,-1,1,1,2509,lucia specia,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,None
2009.eamt-1.5,Estimating the Sentence-Level Quality of Machine Translation Systems,2009,18,145,1,1,2509,lucia specia,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"We investigate the problem of predicting the quality of sentences produced by machine translation systems when reference translations are not available. The problem is addressed as a regression task and a method that takes into account the contribution of different features is proposed. We experiment with this method for translations produced by various MT systems and different language pairs, annotated with quality scores both automatically and manually. Results show that our method allows obtaining good estimates and that identifying a reduced set of relevant features plays an important role. The experiments also highlight a number of outstanding features that were consistently selected as the most relevant and could be used in different ways to improve MT performance or to enhance MT evaluation."
S07-1099,{USP}-{IBM}-1 and {USP}-{IBM}-2: The {ILP}-based Systems for Lexical Sample {WSD} in {S}em{E}val-2007,2007,4,5,1,1,2509,lucia specia,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We describe two systems participating of the English Lexical Sample task in SemEval-2007. The systems make use of Inductive Logic Programming for supervised learning in two different ways: (a) to build Word Sense Disambiguation (WSD) models from a rich set of background knowledge sources; and (b) to build interesting features from the same knowledge sources, which are then used by a standard model-builder for WSD, namely, Support Vector Machines. Both systems achieved comparable accuracy (0.851 and 0.857), which outperforms considerably the most frequent sense baseline (0.787)."
P07-1006,Learning Expressive Models for Word Sense Disambiguation,2007,22,22,1,1,2509,lucia specia,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We present a novel approach to the word sense disambiguation problem which makes use of corpus-based evidence combined with background knowledge. Employing an inductive logic programming algorithm, the approach generates expressive disambiguation rules which exploit several knowledge sources and can also model relations between them. The approach is evaluated in two tasks: identification of the correct translation for a set of highly ambiguous verbs in EnglishPortuguese translation and disambiguation of verbs from the Senseval-3 lexical sample task. The average accuracy obtained for the multilingual task outperforms the other machine learning techniques investigated. In the monolingual task, the approach performs as well as the state-of-the-art systems which reported results for the same set of verbs."
W06-2505,Multilingual versus Monolingual {WSD},2006,18,8,1,1,2509,lucia specia,Proceedings of the Workshop on Making Sense of Sense: Bringing Psycholinguistics and Computational Linguistics Together,0,None
W06-0508,A hybrid approach for extracting semantic relations from texts,2006,18,28,1,1,2509,lucia specia,Proceedings of the 2nd Workshop on Ontology Learning and Population: Bridging the Gap between Text and Knowledge,0,"We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases. The relations extracted can be used for various tasks, including semantic web annotation and ontology learning. We suggest that the use of knowledge intensive strategies to process the input text and corpusbased techniques to deal with unpredicted cases and ambiguity problems allows to accurately discover the relevant relations between pairs of entities in that text."
P06-3010,A Hybrid Relational Approach for {WSD} {--} First Results,2006,13,10,1,1,2509,lucia specia,Proceedings of the {COLING}/{ACL} 2006 Student Research Workshop,0,"We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge. It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process, producing a rule-based WSD model. We experimented with this approach to disambiguate 7 highly ambiguous verbs in English-Portuguese translation. Results showed that the approach is promising, achieving an average accuracy of 75%, which outperforms the other machine learning techniques investigated (66%)."
2006.eamt-1.28,Translation Context Sensitive {WSD},2006,18,3,1,1,2509,lucia specia,Proceedings of the 11th Annual conference of the European Association for Machine Translation,0,None
