2020.semeval-1.98,{S}em{E}val-2020 Task 7: Assessing Humor in Edited News Headlines,2020,-1,-1,3,0.769231,15129,nabil hossain,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper describes the SemEval-2020 shared task {``}Assessing Humor in Edited News Headlines.{''} The task{'}s dataset contains news headlines in which short edits were applied to make them funny, and the funniness of these edited headlines was rated using crowdsourcing. This task includes two subtasks, the first of which is to estimate the funniness of headlines on a humor scale in the interval 0-3. The second subtask is to predict, for a pair of edited versions of the same original headline, which is the funnier version. To date, this task is the most popular shared computational humor task, attracting 48 teams for the first subtask and 31 teams for the second."
N19-1012,{``}President Vows to Cut {\\textless}Taxes{\\textgreater} Hair{''}: Dataset and Analysis of Creative Text Editing for Humorous Headlines,2019,27,0,3,0.769231,15129,nabil hossain,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We introduce, release, and analyze a new dataset, called Humicroedit, for research in computational humor. Our publicly available data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny. We carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline. The simple edits, usually just a single word replacement, mean we can apply straightforward analysis techniques to determine what makes our edited headlines humorous. We show how the data support classic theories of humor, such as incongruity, superiority, and setup/punchline. Finally, we develop baseline classifiers that can predict whether or not an edited headline is funny, which is a first step toward automatically generating humorous headlines as an approach to creating topical humor."
D19-1505,Modeling the Relationship between User Comments and Edits in Document Revision,2019,0,0,3,0,4557,xuchao zhang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document{'}s evolution. Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. A number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. Thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: Comment Ranking and Edit Anchoring. We begin by collecting a dataset with more than half a million comment-edit pairs based on Wikipedia revision histories. We then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. In a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. We are able to achieve a precision@1 of 71.0{\%} and a precision@3 of 94.4{\%} for Comment Ranking, while we achieve 74.4{\%} accuracy on Edit Anchoring."
N16-1171,Activity Modeling in Email,2016,24,10,2,0,29454,ashequl qadir,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce a latent activity model for workplace emails, positing that communication at work is purposeful and organized by activities. We pose the problem as probabilistic inference in graphical models that jointly capture the interplay between latent activities and the email contexts they govern, such as the recipients, subject and body. The model parameters are learned using maximum likelihood estimation with an expectation maximization algorithm. We present three variants of the model that incorporate the recipients, co-occurrence of the recipients, and email body and subject. We demonstrate the modelxe2x80x99s effectiveness in an email recipient recommendation task and show that it outperforms a state-of-the-art generative model. Additionally, we show that the activity model can be used to identify email senders who engage in similar activities, resulting in further improvements in recipient recommendation."
D15-1174,Representing Text for Joint Embedding of Text and Knowledge Bases,2015,31,215,6,0,9781,kristina toutanova,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013). In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity, knowledge base, and textual relation representations. The proposed model significantly improves performance over a model that does not share parameters among textual relations with common sub-structure."
P14-1143,Smart Selection,2014,0,0,2,0,34734,patrick pantel,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
D14-1002,Modeling Interestingness with Deep Neural Networks,2014,123,138,3,0,3502,jianfeng gao,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"An xe2x80x9cInterestingness Modelerxe2x80x9d uses deep neural networks to learn deep semantic models (DSM) of xe2x80x9cinterestingness.xe2x80x9d The DSM, consisting of two branches of deep neural networks or their convolutional versions, identifies and predicts target documents that would interest users reading source documents. The learned model observes, identifies, and detects naturally occurring signals of interestingness in click transitions between source and target documents derived from web browser logs. Interestingness is modeled with deep neural networks that map source-target document pairs to feature vectors in a latent space, trained on document transitions in view of a xe2x80x9ccontextxe2x80x9d and optional xe2x80x9cfocusxe2x80x9d of source and target documents. Network parameters are learned to minimize distances between source documents and their corresponding xe2x80x9cinterestingxe2x80x9d targets in that space. The resulting interestingness model has applicable uses, including, but not limited to, contextual entity searches, automatic text highlighting, prefetching documents of likely interest, automated content recommendation, automated advertisement placement, etc."
C14-1140,Predicting Interesting Things in Text,2014,28,10,1,1,15131,michael gamon,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"While reading a document, a user may encounter concepts, entities, and topics that she is interested in exploring more. We propose models of xe2x80x9cinterestingnessxe2x80x9d, which aim to predict the level of interest a user has in the various text spans in a document. We obtain naturally occurring interest signals by observing user browsing behavior in clicks from one page to another. We cast the problem of predicting interestingness as a discriminative learning problem over this data. We leverage features from two principal sources: textual context features and topic features that assess the semantics of the document transition. We learn our topic features without supervision via probabilistic inference over a graphical model that captures the latent joint topic space of the documents in the transition. We train and test our models on millions of realworld transitions between Wikipedia documents as observed from web browser session logs. On the task of predicting which spans are of most interest to users, we show significant improvement over various baselines and highlight the value of our latent semantic model."
R13-1055,Revisiting the Old Kitchen Sink: Do we Need Sentiment Domain Adaptation?,2013,17,7,3,0,26353,riham mansour,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"In this paper we undertake a large crossdomain investigation of sentiment domain adaptation, challenging the practical necessity of sentiment domain adaptation algorithms. We first show that across a wide set of domains, a simple xe2x80x9call-in-onexe2x80x9d classifier that utilizes all available training data from all but the target domain tends to outperform published domain adaptation methods. A very simple ensemble classifier also performs well in these scenarios. Combined with the fact that labeled data nowadays is inexpensive to come by, the xe2x80x9ckitchen sinkxe2x80x9d approach, while technically nonglamorous, might be perfectly adequate in practice. We also show that the common anecdotal evidence for sentiment terms that xe2x80x9cflipxe2x80x9d polarity across domains is not borne out empirically."
P12-1059,Mining Entity Types from Query Logs via User Intent Modeling,2012,27,45,3,0,34734,patrick pantel,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We predict entity type distributions in Web search queries via probabilistic inference in graphical models that capture how entity-bearing queries are generated. We jointly model the interplay between latent user intents that govern queries and unobserved entity types, leveraging observed signals from query formulations and document clicks. We apply the models to resolve entity types in new queries and to assign prior type distributions over an existing knowledge base. Our models are efficiently trained using maximum likelihood estimation over millions of real-world Web search queries. We show that modeling user intent significantly improves entity type resolution for head queries over the state of the art, on several metrics, without degradation in tail query performance."
N12-3006,"{MSR} {SPLAT}, a language analysis toolkit",2012,13,30,6,0.187324,4460,chris quirk,Proceedings of the Demonstration Session at the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We describe MSR SPLAT, a toolkit for language analysis that allows easy access to the linguistic analysis tools produced by the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages."
N12-1074,Predicting Responses to Microblog Posts,2012,31,73,3,0,6799,yoav artzi,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Microblogging networks serve as vehicles for reaching and influencing users. Predicting whether a message will elicit a user response opens the possibility of maximizing the virality, reach and effectiveness of messages and ad campaigns on these networks. We propose a discriminative model for predicting the likelihood of a response or a retweet on the Twitter network. The approach uses features derived from various sources, such as the language used in the tweet, the user's social network and history. The feature design process leverages aggregate statistics over the entire social network to balance sparsity and informativeness. We use real-world tweets to train models and empirically show that they are capable of generating accurate predictions for a large number of tweets."
C12-1143,Underspecified Query Refinement via Natural Language Question Generation,2012,14,3,3,0,3156,hassan sajjad,Proceedings of {COLING} 2012,0,"Underspecified queries are common in vertical search engines, leading to large result sets that are difficult for users to navigate. In this paper, we show that we can automatically guide users to their target results by engaging them in a dialog consisting of well-formed binary questions mined from unstructured data. We propose a system that extracts candidate attribute-value question terms from unstructured descriptions of records in a database. These terms are then filtered using a Maximum Entropy classifier to identify those that are suitable for question formation given a user query. We then select question terms via a novel ranking function that aims to minimize the number of question turns necessary for a user to find her target result. We evaluate the quality of system-generated questions for grammaticality and refinement effectiveness. Our final system shows best results in effectiveness, percentage of well-formed questions, and percentage of answerable questions over three baseline systems."
W11-1825,{MSR}-{NLP} Entry in {B}io{NLP} Shared Task 2011,2011,16,24,3,0.187324,4460,chris quirk,Proceedings of {B}io{NLP} Shared Task 2011 Workshop,0,"We describe the system from the Natural Language Processing group at Microsoft Research for the BioNLP 2011 Shared Task. The task focuses on event extraction, identifying structured and potentially nested events from unannotated text. Our approach follows a pipeline, first decorating text with syntactic information, then identifying the trigger words of complex events, and finally identifying the arguments of those events. The resulting system depends heavily on lexical and syntactic features. Therefore, we explored methods of maintaining ambiguities and improving the syntactic representations, making the lexical information less brittle through clustering, and of exploring novel feature combinations and feature reduction. The system ranked 4th in the GENIA task with an F-measure of 51.5%, and 3rd in the EPI task with an F-measure of 64.9%."
W11-1422,High-Order Sequence Modeling for Language Learner Error Detection,2011,29,19,1,1,15131,michael gamon,Proceedings of the Sixth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We address the problem of detecting English language learner errors by using a discriminative high-order sequence model. Unlike most work in error-detection, this method is agnostic as to specific error types, thus potentially allowing for higher recall across different error types. The approach integrates features from many sources into the error-detection model, ranging from language model-based features to linguistic analysis features. Evaluation results on a large annotated corpus of learner writing indicate the feasibility of our approach on a realistic, noisy and inherently skewed set of data. High-order models consistently outperform low-order models in our experiments. Error analysis on the output shows that the calculation of precision on the test set represents a lower bound on the real system performance."
W10-1005,Search right and thou shalt find ... Using Web Queries for Learner Error Detection,2010,19,12,1,1,15131,michael gamon,Proceedings of the {NAACL} {HLT} 2010 Fifth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We investigate the use of web search queries for detecting errors in non-native writing. Distinguishing a correct sequence of words from a sequence with a learner error is a baseline task that any error detection and correction system needs to address. Using a large corpus of error-annotated learner data, we investigate whether web search result counts can be used to distinguish correct from incorrect usage. In this investigation, we compare a variety of query formulation strategies and a number of web resources, including two major search engine APIs and a large web-based n-gram corpus."
N10-1019,Using Mostly Native Data to Correct Errors in Learners{'} Writing,2010,25,37,1,1,15131,michael gamon,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We present results from a range of experiments on article and preposition error correction for non-native speakers of English. We first compare a language model and errorspecific classifiers (all trained on large English corpora) with respect to their performance in error detection and correction. We then combine the language model and the classifiers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the metaclassifier. The meta-classifier in turn is trained on error-annotated learner data, optimizing the error detection and correction performance on this domain. The meta-classification approach results in substantial gains over the classifieronly and language-model-only scenario. Since the meta-classifier requires error-annotated data for training, we investigate how much training data is needed to improve results over the baseline of not using a meta-classifier. All evaluations are conducted on a large errorannotated corpus of learner English."
W09-2111,User Input and Interactions on {M}icrosoft {R}esearch {ESL} Assistant,2009,20,16,2,0,40176,claudia leacock,Proceedings of the Fourth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"ESL Assistant is a prototype web-based writing-assistance tool that is being developed for English Language Learners. The system focuses on types of errors that are typically made by non-native writers of American English. A freely-available prototype was deployed in June 2008. User data from this system are manually evaluated to identify writing domain and measure system accuracy. Combining the user log data with the evaluated rewrite suggestions enables us to determine how effectively English language learners are using the system, across rule types and across writing domains. We find that repeat users typically make informed choices and can distinguish correct suggestions from incorrect."
I08-1059,Using Contextual Speller Techniques and Language Modeling for {ESL} Error Correction,2008,20,118,1,1,15131,michael gamon,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We present a modular system for detection and correction of errors made by nonnative (English as a Second Language = ESL) writers. We focus on two error types: the incorrect use of determiners and the choice of prepositions. We use a decisiontree approach inspired by contextual spelling systems for detection and correction suggestions, and a large language model trained on the Gigaword corpus to provide additional information to filter out spurious suggestions. We show how this system performs on a corpus of non-native English text and discuss strategies for future enhancements."
J07-2010,"Book Reviews: Computing Attitude and Affect in Text: Theory and Applications, edited by {J}ames {G}. Shanahan, Yan Qu, and Janyce Wiebe",2007,0,0,1,1,15131,michael gamon,Computational Linguistics,0,None
W06-3803,Graph-Based Text Representation for Novelty Detection,2006,21,34,1,1,15131,michael gamon,Proceedings of {T}ext{G}raphs: the First Workshop on Graph Based Methods for Natural Language Processing,0,"We discuss several feature sets for novelty detection at the sentence level, using the data and procedure established in task 2 of the TREC 2004 novelty track. In particular, we investigate feature sets derived from graph representations of sentences and sets of sentences. We show that a highly connected graph produced by using sentence-level term distances and pointwise mutual information can serve as a source to extract features for novelty detection. We compare several feature sets based on such a graph representation. These feature sets allow us to increase the accuracy of an initial novelty classifier which is based on a bag-of-word representation and KL divergence. The final result ties with the best system at TREC 2004."
P06-2058,Obfuscating Document Stylometry to Preserve Author Anonymity,2006,11,56,2,0,49933,gary kacmarcik,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"This paper explores techniques for reducing the effectiveness of standard authorship attribution techniques so that an author A can preserve anonymity for a particular document D. We discuss feature selection and adjustment and show how this information can be fed back to the author to create a new document D' for which the calculated attribution moves away from A. Since it can be labor intensive to adjust the document in this fashion, we attempt to quantify the amount of effort required to produce the anonymized document and introduce two levels of anonymization: shallow and deep. In our test set, we show that shallow anonymization can be achieved by making 14 changes per 1000 words to reduce the likelihood of identifying A as the author by an average of more than 83%. For deep anonymization, we adapt the unmasking work of Koppel and Schler to provide feedback that allows the author to choose the level of anonymization."
P06-1032,Correcting {ESL} Errors Using Phrasal {SMT} Techniques,2006,17,147,3,0,4421,chris brockett,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL). Using examples of mass noun errors found in the Chinese Learner Error Corpus (CLEC) to guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers. Our system was able to correct 61.81% of mistakes in a set of naturally-occurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners."
W05-0408,Automatic Identification of Sentiment Vocabulary: Exploiting Low Association with Known Sentiment Terms,2005,14,97,1,1,15131,michael gamon,Proceedings of the {ACL} Workshop on Feature Engineering for Machine Learning in Natural Language Processing,0,"We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Turney (2002) and Turney and Littman (2002). Their basic assumption is that sentiment terms of similar orientation tend to co-occur at the document level. We add a second assumption, namely that sentiment terms of opposite orientation tend not to co-occur at the sentence level. This additional assumption allows us to identify sentiment-bearing terms very reliably. We then use these newly identified terms in various scenarios for the sentiment classification of sentences. We show that our approach outperforms Turney's original approach. Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance. We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data."
2005.eamt-1.15,Sentence-level {MT} evaluation without reference translations: beyond language modeling,2005,20,86,1,1,15131,michael gamon,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"In this paper we investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations. We measure the correlation between automatically-generated scores and human judgments, and we evaluate the per- formance of our system when used as a classifier for identifying highly dysfluent and ill- formed sentences. We show that we can substantially improve on the correlation between language model perplexity scores and human judgment by combining these perplexity scores with class probabilities from a machine-learned classifier. The classifier uses linguis- tic features and has been trained to distinguish human translations from machine transla- tions. We show that this approach also performs well in identifying dysfluent sentences."
W04-1008,Task-Focused Summarization of Email,2004,10,85,3,1,159,simon corstonoliver,Text Summarization Branches Out,0,"We describe SmartMail, a prototype system for automatically identifying action items (tasks) in email messages. SmartMail presents the user with a task-focused summary of a message. The summary consists of a list of action items extracted from the message. The user can add these action items to their xe2x80x9cto doxe2x80x9d list."
C04-1088,Linguistic correlates of style: authorship classification with deep linguistic analysis features,2004,14,124,1,1,15131,michael gamon,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"The identification of authorship falls into the category of style classification, an interesting sub-field of text categorization that deals with properties of the form of linguistic expression as opposed to the content of a text. Various feature sets and classification methods have been proposed in the literature, geared towards abstracting away from the content of a text, and focusing on its stylistic properties. We demonstrate that in a realistically difficult authorship attribution scenario, deep linguistic analysis features such as context free production frequencies and semantic relationship frequencies achieve significant error reduction over more commonly used shallow features such as function word frequencies and part of speech trigrams. Modern machine learning techniques like support vector machines allow us to explore large feature vectors, combining these different feature sets to achieve high classification accuracy in style-based tasks."
C04-1097,Linguistically Informed Statistical Models of Constituent Structure for Ordering in Sentence Realization,2004,20,39,2,0,30821,eric ringger,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We present several statistical models of syntactic constituent order for sentence realization. We compare several models, including simple joint models inspired by existing statistical parsing models, and several novel conditional models. The conditional models leverage a large set of linguistic features without manual feature selection. We apply and evaluate the models in sentence realization for French and German and find that a particular conditional model outperforms all others. We employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization."
C04-1121,"Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis",2004,19,349,1,1,15131,michael gamon,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We demonstrate that it is possible to perform automatic sentiment classification in the very noisy domain of customer feedback data. We show that by using large feature vectors in combination with feature reduction, we can train linear support vector machines that achieve high classification accuracy on data that present classification challenges even for a human annotator. We also show that, surprisingly, the addition of deep linguistic analysis features to a set of surface level word n-gram features contributes consistently to classification accuracy in this domain."
corston-oliver-gamon-2004-normalizing,Normalizing {G}erman and {E}nglish inflectional morphology to improve statistical word alignment,2004,15,28,2,1,159,simon corstonoliver,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"German has a richer system of inflectional morphology than English, which causes problems for current approaches to statistical word alignment. Using Giza++ as a reference implementation of the IBM Model 1, an HMMbased alignment and IBM Model 4, we measure the impact of normalizing inflectional morphology on German-English statistical word alignment. We demonstrate that normalizing inflectional morphology improves the perplexity of models and reduces alignment errors."
E03-1006,{F}rench Amalgam: a quick adaptation of a sentence realization system to {F}rench,2003,0,1,2,0,51288,martine smets,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
2003.mtsummit-papers.8,Combining decision trees and transformation-based learning to correct transferred linguistic representations,2003,14,10,2,1,159,simon corstonoliver,Proceedings of Machine Translation Summit IX: Papers,0,We approach to correcting features in transferred linguistic representations in machine translation. The hybrid approach combines decision trees and transformation-based learning. Decision trees serve as a filter on the intractably large search space of possible interrelations among features. Transformation-based learning results in a simple set of ordered rules that can be compiled and executed after transfer and before sentence realization in the target language. We measure the reduction in noise in the linguistic representations and the results of human evaluations of end-to-end English-German machine translation.
2003.mtsummit-papers.48,High quality machine translation using a machine-learned sentence realization component,2003,14,2,2,0,51288,martine smets,Proceedings of Machine Translation Summit IX: Papers,0,"We describe the implementation of two new language pairs (English-French and English-German) which use machine-learned sentence realization components instead of hand-written generation components. The resulting systems are evaluated by human evaluators, and in the technical domain, are equal to the quality of highly respected commercial systems. We comment on the difficulties that are encountered when using machine-learned sentence realization in the context of MT."
2003.jeptalnrecital-long.23,{F}rench Amalgam: A machine-learned sentence realization system,2003,11,6,2,0,51288,martine smets,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"This paper presents the French implementation of Amalgam, a machine-learned sentence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models."
W02-2105,An Overview of Amalgam: A Machine-learned Generation Module,2002,16,48,2,1,159,simon corstonoliver,Proceedings of the International Natural Language Generation Conference,0,"We present an overview of Amalgam, a sentence realization module that combines machine-learned and knowledgeengineered components to produce natural language sentences from logical form inputs. We describe the decomposition of the task of sentence realization into a linguistically informed series of steps, with particular attention to the linguistic issues that arise in German. We report on the evaluation of component steps and of the overall system."
P02-1004,Machine-learned contexts for linguistic operations in {G}erman sentence realization,2002,13,13,1,1,15131,michael gamon,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"We show that it is possible to learn the contexts for linguistic operations which map a semantic representation to a surface syntactic tree in sentence realization with high accuracy. We cast the problem of learning the contexts for the linguistic operations as classification tasks, and apply straightforward machine learning techniques, such as decision tree learning. The training data consist of linguistic features extracted from syntactic and semantic representations produced by a linguistic analysis system. The target features are extracted from links to surface syntax trees. Our evidence consists of four examples from the German sentence realization system code-named Amalgam: case assignment, assignment of verb position features, extraposition, and syntactic aggregation."
C02-1036,{E}xtraposition: A Case Study in {G}erman Sentence Realization,2002,4,14,1,1,15131,michael gamon,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We profile the occurrence of clausal extraposition in corpora from different domains and demonstrate that extraposition is a pervasive phenomenon in German that must be addressed in German sentence realization. We present two different approaches to the modeling of extraposition, both based on machine learned decision tree classifiers. The two approaches differ in their view of the movement operation: one approach models multi-step movement through intermediate nodes to the ultimate target node, while the other approach models one-step movement to the target node. We compare the resulting models, trained on data from two domains and discuss the differences between the two types of models and between the results obtained in the different domains."
P01-1020,A Machine Learning Approach to the Automatic Evaluation of Machine Translation,2001,13,69,2,1,159,simon corstonoliver,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"We present a machine learning approach to evaluating the well-formedness of output of a machine translation system, using classifiers that learn to distinguish human reference translations from machine translations. This approach can be used to evaluate an MT system, tracking improvements over time; to aid in the kind of failure analysis that can help guide system development; and to select among alternative output strings. The method presented is fully automated and independent of source language, target language and domain."
2001.mtsummit-papers.21,Using machine learning for system-internal evaluation of transferred linguistic representations,2001,7,3,1,1,15131,michael gamon,Proceedings of Machine Translation Summit VIII,0,"We present an automated, system-internal evaluation technique for linguistic representations in a large-scale, multilingual MT system. We use machine-learned classifiers to recognize the differences between linguistic representations generated from transfer in an MT context from representations that are produced by ``native'' analysis of the target language. In the MT scenario, convergence of the two is the desired result. Holding the feature set and the learning algorithm constant, the accuracy of the classifiers provides a measure of the overall difference between the two sets of linguistic representations: classifiers with higher accuracy correspond to more pronounced differences between representations. More importantly, the classifiers yield the basis for error-analysis by providing a ranking of the importance of linguistic features. The more salient a linguistic criterion is in discriminating transferred representations from ``native'' representations, the more work will be needed in order to get closer to the goal of producing native-like MT. We present results from using this approach on the Microsoft MT system and discuss its advantages and possible extensions."
W97-0908,Practical Experience with Grammar Sharing in Multilingual {NLP},1997,6,19,1,1,15131,michael gamon,From Research to Commercial Applications: Making {NLP} Work in Practice,0,"In the Microsoft Natural Language Processing System (MSNLP), grammar sharing between English, French, Spanish, and German has been an important means for speeding up the development time for the latter grammars. Despite significant typological differences between these languages, a mature English grammar was taken as the starting point for each of the other three grarnmars. In each case, through a combination of adding and deleting a modest number of grammar rules, and modifying the conditions on many others, a broad-coverage target grammar emerged. Tests indicate that this approach has been successful in achieving a high degree of coverage in a relatively short period of time."
