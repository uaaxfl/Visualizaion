2020.acl-main.299,D16-1257,0,0.134774,"018) investigates such a model which predicts probability on each candidate span. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe and Charniak (2016) and transition-based systems (Cross and Huang, 2016; Liu and Zhang, 2017a). With tree linearizations, the training time can be further accelerated to O(n), but the parsers often sacrifice a clear connection with original spans in trees, which makes both features and supervision signals from spans hard to use. In this work, we propose a novel linearization of constituent trees tied on their span representations. Given a sentence W and its parsing tree T , for each split point after wi in the sentence, we assign it a parsing target di , where (di , i) is the longest span ending with i in T . We"
2020.acl-main.299,D16-1001,0,0.536505,"point, and then predicts a tree span from them. Compared with global models, our model is fast and parallelizable. Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.1 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models. 1 Introduction Constituent parsers map natural language sentences to hierarchically organized spans (Cross and Huang, 2016). According to the complexity of decoders, two types of parsers have been studied, globally normalized models which normalize probability of a constituent tree on the whole candidate tree space (e.g. chart parser (Stern et al., 2017a)) and locally normalized models which normalize tree probability on smaller subtrees or spans. It is believed that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsin"
2020.acl-main.299,N19-1423,0,0.0307641,"Here the length l represents lengths between [l, l + 4]. for CTB. For character encoding, we randomly initialize the character embeddings with dimension 64. We use Adam optimizer with initial learning rate 1.0 and epsilon 10−9 . For LSTM encoder, we use a hidden size of 1024, with 0.33 dropout in all the feed-forward and recurrent connections. For Transformer encoder, we use the same hyperparameters as Kitaev and Klein (2018a). For split point representation, we apply two 1024-dimensional hidden size feed-forward networks. All the dropout we use in the decoder layer is 0.33. We also use BERT (Devlin et al., 2019) (uncased, 24 layers, 16 attention heads per layer and 1024-dimensional hidden vectors) and use the output of the last layer as the pre-trained word embeddings. 5 Training Details We use PyTorch as our neural network toolkit and run the code on a NVIDIA GeForce GTX Titan Xp GPU and Intel Xeon E52603 v4 CPU. All models are trained for up to 150 epochs with batch size 150 (Zhou and Zhao, 2019). 4.2 Main Results Table 2 shows the final results on PTB test set. Our models (92.6 F1 with LSTM, 93.7 F1 with Trans5 The source code for our model is publicly available: https://github.com/AntNLP/ span-li"
2020.acl-main.299,N16-1024,0,0.132807,"Missing"
2020.acl-main.299,P18-2075,0,0.0283238,"Missing"
2020.acl-main.299,P17-2025,0,0.0272976,"Missing"
2020.acl-main.299,N18-1091,0,0.0565128,"ieved that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsing accuracy while enjoying fast training and testing speed, and thus become an active research topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of decomposition, the probability of trees can be factorized, for example, on individual spans. Teng and Zhang (2018) investigates such a model which predicts probability on each candidate span. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe"
2020.acl-main.299,D18-1162,0,0.0553168,"ieved that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsing accuracy while enjoying fast training and testing speed, and thus become an active research topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of decomposition, the probability of trees can be factorized, for example, on individual spans. Teng and Zhang (2018) investigates such a model which predicts probability on each candidate span. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe"
2020.acl-main.299,P82-1020,0,0.79849,"Missing"
2020.acl-main.299,P18-2076,0,0.038615,"Missing"
2020.acl-main.299,P19-1237,1,0.850836,"arallel. Sequence labeling models regard tree prediction as sequence prediction problem (G´omez-Rodr´ıguez and Vilares, 2018; Shen et al., 2018). These models have high efficiency, but their linearizations have no direct relation to the spans, so the performance is much worse than span-based models. We propose a novel linearization method closely related to the spans and decode the tree in O(n log n) complexity. Compared with Teng and Zhang (2018), we do normalization on more spans, thus achieve a better performance. In future work, we will apply graph neural network (Velickovic et al., 2018; Ji et al., 2019; Sun et al., 2019) to enhance the span representation. Due to the excellent properties of our linearization, we can jointly learn constituent parsing and dependency parsing in one graph-based model. In addition, there is also a right linearization defined on the set of right child spans. We can study how to combine the two linear representations to further improve the performance of the model. 6 Conclusion In this work, we propose a novel linearization of constituent trees tied on the spans tightly. In addition, we build a new normalization method, which can add constraints on all the spans w"
2020.acl-main.299,P18-1249,0,0.259343,"ed by a character-level LSTM and a randomly initialized part-of-speech tag embedding pi . We concatenate these three embeddings to generate a representation of word wi , xi = [ei ; ci ; pi ]. To get the representation of the split points, the word representation matrix X = [x1 , x2 , . . . , xn ] is fed into a bidirectional LSTM or Transformer (Vaswani et al., 2017) firstly. Then we calculate the representation of the split point between wi and wi+1 using the outputs from the encoders, 3269 → ← hi = [ h i ; h i+1 ]. (1) → Note that for Transformer encoder, h i is calculated in the same way as Kitaev and Klein (2018a). 3.2 Decoder Since a split point can play two different roles when it is the left or right boundary of a span, we use two different vectors to represent the two roles inspired by Dozat and Manning (2017). Concretely, we use two multi-layer perceptrons to generate two different representations, li = MLPl (hi ), ri = MLPr (hi ). (2) Then we can define the score of span (i, j) using a biaffine attention function (Dozat and Manning, 2017; Li et al., 2019), &gt; αij = li&gt; Wrj + b&gt; 1 li + b2 rj , where W, b1 and b2 are all model parameters. αij measures the possibility of (i, j) being a left child s"
2020.acl-main.299,N15-1142,0,0.0671447,"Missing"
2020.acl-main.299,Q17-1029,0,0.0938574,"an. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe and Charniak (2016) and transition-based systems (Cross and Huang, 2016; Liu and Zhang, 2017a). With tree linearizations, the training time can be further accelerated to O(n), but the parsers often sacrifice a clear connection with original spans in trees, which makes both features and supervision signals from spans hard to use. In this work, we propose a novel linearization of constituent trees tied on their span representations. Given a sentence W and its parsing tree T , for each split point after wi in the sentence, we assign it a parsing target di , where (di , i) is the longest span ending with i in T . We can show that, for a binary parsing tree, the set {(di , i)} includes al"
2020.acl-main.299,Q17-1004,0,0.417416,"an. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe and Charniak (2016) and transition-based systems (Cross and Huang, 2016; Liu and Zhang, 2017a). With tree linearizations, the training time can be further accelerated to O(n), but the parsers often sacrifice a clear connection with original spans in trees, which makes both features and supervision signals from spans hard to use. In this work, we propose a novel linearization of constituent trees tied on their span representations. Given a sentence W and its parsing tree T , for each split point after wi in the sentence, we assign it a parsing target di , where (di , i) is the longest span ending with i in T . We can show that, for a binary parsing tree, the set {(di , i)} includes al"
2020.acl-main.299,J93-2004,0,0.0698688,"eviewer for pointing out the connection. The following discussions are based on his/her detailed reviews. same span (i, j), their normalization is constrained within (i, j), while ours is over all i0 &lt; j. The main advantage of our parser is simpler span representations (not depend on parent spans): it makes the parser easy to batch for sentences with different lengths and tree structures since each di can be calculated offline before training. 4 Experiments 4.1 Data and Settings Datasets and Preprocessing All models are trained on two standard benchmark treebanks, English Penn Treebank (PTB) (Marcus et al., 1993) and Chinese Penn Treebank (CTB) 5.1. The POS tags are predicted using Stanford Tagger (Toutanova et al., 2003). To clean the treebanks, we strip the leaf nodes with POS tag -NONE- from the two treebanks and delete the root nodes with constituent type ROOT. For evaluating the results, we use the standard evaluation tool 4 . For words in the testing corpus but not in the training corpus, we replace them with a unique label &lt;UNK&gt;. We also replace the words in the training corpus with the unknown label &lt;UNK&gt; z with probability punk (w) = z+c(w) , where c(w) is the number of time word w appears in"
2020.acl-main.299,D14-1162,0,0.111998,"Missing"
2020.acl-main.299,P18-1108,0,0.471876,"earch topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of decomposition, the probability of trees can be factorized, for example, on individual spans. Teng and Zhang (2018) investigates such a model which predicts probability on each candidate span. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe and Charniak (2016) and transition-based systems (Cross and Huang, 2016; Liu and Zhang, 2017a). With tree linearizations, the training time can be further accelerated to O(n), but the parsers often sacrifice a clear connection with original spans in trees, which makes both features and supervision signals from spans hard to use. In this work, we propose a nov"
2020.acl-main.299,P17-1076,0,0.0899763,"atures when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.1 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models. 1 Introduction Constituent parsers map natural language sentences to hierarchically organized spans (Cross and Huang, 2016). According to the complexity of decoders, two types of parsers have been studied, globally normalized models which normalize probability of a constituent tree on the whole candidate tree space (e.g. chart parser (Stern et al., 2017a)) and locally normalized models which normalize tree probability on smaller subtrees or spans. It is believed that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsing accuracy while enjoying fast training and testing speed, and thus become an active research topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of de"
2020.acl-main.299,D17-1178,0,0.0539423,"atures when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.1 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models. 1 Introduction Constituent parsers map natural language sentences to hierarchically organized spans (Cross and Huang, 2016). According to the complexity of decoders, two types of parsers have been studied, globally normalized models which normalize probability of a constituent tree on the whole candidate tree space (e.g. chart parser (Stern et al., 2017a)) and locally normalized models which normalize tree probability on smaller subtrees or spans. It is believed that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsing accuracy while enjoying fast training and testing speed, and thus become an active research topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of de"
2020.acl-main.299,P19-1131,1,0.84054,"labeling models regard tree prediction as sequence prediction problem (G´omez-Rodr´ıguez and Vilares, 2018; Shen et al., 2018). These models have high efficiency, but their linearizations have no direct relation to the spans, so the performance is much worse than span-based models. We propose a novel linearization method closely related to the spans and decode the tree in O(n log n) complexity. Compared with Teng and Zhang (2018), we do normalization on more spans, thus achieve a better performance. In future work, we will apply graph neural network (Velickovic et al., 2018; Ji et al., 2019; Sun et al., 2019) to enhance the span representation. Due to the excellent properties of our linearization, we can jointly learn constituent parsing and dependency parsing in one graph-based model. In addition, there is also a right linearization defined on the set of right child spans. We can study how to combine the two linear representations to further improve the performance of the model. 6 Conclusion In this work, we propose a novel linearization of constituent trees tied on the spans tightly. In addition, we build a new normalization method, which can add constraints on all the spans with the same right"
2020.acl-main.299,C18-1011,0,0.0671695,"pans. It is believed that global models have better parsing performance (Gaddy et al., 2018). But with the fast development of neural-network-based feature representations (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), local models are able to get competitive parsing accuracy while enjoying fast training and testing speed, and thus become an active research topic in constituent parsing. Locally normalized parsers usually rely on tree decompositions or linearizations. From the perspective of decomposition, the probability of trees can be factorized, for example, on individual spans. Teng and Zhang (2018) investigates such a model which predicts probability on each candidate span. It achieves quite promising parsing results, while the simple local probability factorization still leaves room for improvements. From the perspective of linearization, there are many ways to transform a structured tree into a shallow sequence. As a recent example, Shen et al. (2018) linearizes a tree with a sequence of numbers, each of which indicates words’ syntactic distance in the tree (i.e., height of the lowest common ancestor of two adjacent words). Similar ideas are also applied in Vinyals et al. (2015), Choe"
2020.acl-main.299,P19-1230,0,0.169526,"rameters as Kitaev and Klein (2018a). For split point representation, we apply two 1024-dimensional hidden size feed-forward networks. All the dropout we use in the decoder layer is 0.33. We also use BERT (Devlin et al., 2019) (uncased, 24 layers, 16 attention heads per layer and 1024-dimensional hidden vectors) and use the output of the last layer as the pre-trained word embeddings. 5 Training Details We use PyTorch as our neural network toolkit and run the code on a NVIDIA GeForce GTX Titan Xp GPU and Intel Xeon E52603 v4 CPU. All models are trained for up to 150 epochs with batch size 150 (Zhou and Zhao, 2019). 4.2 Main Results Table 2 shows the final results on PTB test set. Our models (92.6 F1 with LSTM, 93.7 F1 with Trans5 The source code for our model is publicly available: https://github.com/AntNLP/ span-linearization-parser former) significantly outperform the single locally normalized models. Compared with globally normalized models, our models also outperform those parsers with LSTM encoder and achieve a competitive result with Transformer encoder parsers. With the help of BERT (Devlin et al., 2018), our models with two encoders both achieve the same performance (95.8 F1) as the best parser"
2020.acl-main.299,N03-1033,0,0.294773,"me span (i, j), their normalization is constrained within (i, j), while ours is over all i0 &lt; j. The main advantage of our parser is simpler span representations (not depend on parent spans): it makes the parser easy to batch for sentences with different lengths and tree structures since each di can be calculated offline before training. 4 Experiments 4.1 Data and Settings Datasets and Preprocessing All models are trained on two standard benchmark treebanks, English Penn Treebank (PTB) (Marcus et al., 1993) and Chinese Penn Treebank (CTB) 5.1. The POS tags are predicted using Stanford Tagger (Toutanova et al., 2003). To clean the treebanks, we strip the leaf nodes with POS tag -NONE- from the two treebanks and delete the root nodes with constituent type ROOT. For evaluating the results, we use the standard evaluation tool 4 . For words in the testing corpus but not in the training corpus, we replace them with a unique label &lt;UNK&gt;. We also replace the words in the training corpus with the unknown label &lt;UNK&gt; z with probability punk (w) = z+c(w) , where c(w) is the number of time word w appears in the training corpus and we set z = 0.8375 as Cross and Huang (2016). Hyperparameters We use 100D GloVe (Pennin"
2020.acl-main.299,Q17-1019,0,0.018773,"ilares (2018). However, the performance is very poor, and this is largely due to the loss of structural information in the label prediction. Therefore, how to balance efficiency and label prediction accuracy might be a research Inference Algorithms Related Work Globally normalized parsers often have high performance on constituent parsing due to their search on the global state space (Stern et al., 2017a; Kitaev and Klein, 2018a; Zhou and Zhao, 2019). However, they suffer from high time complexity and are difficult to parallelize. Thus many efforts have been made to optimize their efficiency (Vieira and Eisner, 2017). Recently, the rapid development of encoders (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017) and pre-trained language models (Devlin et al., 2018) have enabled local models to achieve similar performance as global models. Teng and Zhang (2018) propose two local models, one does normalization on each candidate span and one on each grammar rule. Their models even outperform the global model in Stern et al. (2017a) thanks to the better representation of spans. However, they still need an O(n3 ) complexity inference algorithm to reconstruct the final parsing tree. 3274 6 https://cython.o"
2020.acl-main.299,N19-1341,0,0.060902,"mpler settings. Our own implementation achieves the same result as they report (92.4 F1). For convenience, we call their model per-span-normalization (PSN for short) model in the following. Influence of Span Length First, we analyse the influence of different lengths of spans and the results are shown in Figure 3. We find that for sentences of lengths between [11, 45], our model significantly outperforms PSN model. For short 3272 Model LR LP Global Model Stern et al. (2017a) 90.6 93.0 Gaddy et al. (2018) 91.8 92.4 Kitaev and Klein (2018a)♠ 93.2 93.9 Zhou and Zhao (2019)♠ 93.6 93.9 Local Model Vilares et al. (2019) Liu et al. (2018) Ma et al. (2017) Shen et al. (2018) 91.7 92.0 Liu and Zhang (2017a) Hong and Huang (2018) 91.5 92.5 Teng and Zhang (2018) 92.2 92.5 Dyer et al. (2016)♥ ♥ Stern et al. (2017b) 92.6 92.6 Our Model 92.3 92.9 Our Model♠ 93.3 94.1 Pre-training/Ensemble/Re-ranking Liu et al. (2018) Choe and Charniak (2016) Liu and Zhang (2017a) Fried et al. (2017) Kitaev and Klein (2018a)♠ 94.9 95.4 Kitaev and Klein (2018b)♠ 95.5 95.7 Zhou and Zhao (2019)♠ 95.7 96.0 Our Model (+BERT) 95.6 96.0 Our Model (+BERT)♠ 95.5 96.1 F1 Model LR LP Global Model Kitaev and Klein (2018a)♠ 86.8 88.1 Zhou and Zha"
2020.acl-main.299,P15-1113,0,0.112726,"018) have enabled local models to achieve similar performance as global models. Teng and Zhang (2018) propose two local models, one does normalization on each candidate span and one on each grammar rule. Their models even outperform the global model in Stern et al. (2017a) thanks to the better representation of spans. However, they still need an O(n3 ) complexity inference algorithm to reconstruct the final parsing tree. 3274 6 https://cython.org/ Meanwhile, many work do research on faster sequential models. Transition-based models predict a sequence of actions and achieve an O(n) complexity (Watanabe and Sumita, 2015; Cross and Huang, 2016; Liu and Zhang, 2017a). However, they suffer from the issue of error propagation and cannot be parallel. Sequence labeling models regard tree prediction as sequence prediction problem (G´omez-Rodr´ıguez and Vilares, 2018; Shen et al., 2018). These models have high efficiency, but their linearizations have no direct relation to the spans, so the performance is much worse than span-based models. We propose a novel linearization method closely related to the spans and decode the tree in O(n log n) complexity. Compared with Teng and Zhang (2018), we do normalization on more"
2020.acl-main.299,N03-1031,0,\N,Missing
2020.semeval-1.129,N16-1016,0,0.118173,"al 2020 Task 7: “Assessing Humor in Edited News Headlines”. We participated in all subtasks, in which the main goal is to predict the mean funniness of the edited headline given the original and the edited headline. Our system involves two similar sub-networks, which generate vector representations for the original and edited headlines respectively. And then we do a subtract operation of the outputs from two sub-networks to predict the funniness of the edited headline. 1 Introduction Humor can be defined as the aspiration of provoking laughter and provides amusement from expressions intended (Bertero and Fung, 2016). The task of humor recognition refers to determining whether a sentence in a given context contains some level of humorous content. The Semeval 2020 Task 7 (Hossain et al., 2020a) aims to automatically computes the funniness of edited news headlines which are generated using an insertion of a single-word noun or verb to replace an existing entity or single-word noun or verb in original headline (Hossain et al., 2019). There are two sub-tasks in Task 7. The sub-task 1 is to predict the mean funniness of the edited headline given the original and edited headline. The sub-task 2 is based on sub-"
2020.semeval-1.129,N19-1012,0,0.245829,"s to predict the funniness of the edited headline. 1 Introduction Humor can be defined as the aspiration of provoking laughter and provides amusement from expressions intended (Bertero and Fung, 2016). The task of humor recognition refers to determining whether a sentence in a given context contains some level of humorous content. The Semeval 2020 Task 7 (Hossain et al., 2020a) aims to automatically computes the funniness of edited news headlines which are generated using an insertion of a single-word noun or verb to replace an existing entity or single-word noun or verb in original headline (Hossain et al., 2019). There are two sub-tasks in Task 7. The sub-task 1 is to predict the mean funniness of the edited headline given the original and edited headline. The sub-task 2 is based on sub-task 1, which aims to determine which version of edits makes the headline more humorous given the original headline and two edited versions. In prior studies, humor recognition has been approached as a binary classification problem. Traditional classification algorithms like SVM and Naive Bayes (Mihalcea and Strapparava, 2005), and a deep learning CNN architecture (Chen and Lee, 2017) are adopted to distinguish betwee"
2020.semeval-1.129,2020.semeval-1.98,0,0.362793,"original and the edited headline. Our system involves two similar sub-networks, which generate vector representations for the original and edited headlines respectively. And then we do a subtract operation of the outputs from two sub-networks to predict the funniness of the edited headline. 1 Introduction Humor can be defined as the aspiration of provoking laughter and provides amusement from expressions intended (Bertero and Fung, 2016). The task of humor recognition refers to determining whether a sentence in a given context contains some level of humorous content. The Semeval 2020 Task 7 (Hossain et al., 2020a) aims to automatically computes the funniness of edited news headlines which are generated using an insertion of a single-word noun or verb to replace an existing entity or single-word noun or verb in original headline (Hossain et al., 2019). There are two sub-tasks in Task 7. The sub-task 1 is to predict the mean funniness of the edited headline given the original and edited headline. The sub-task 2 is based on sub-task 1, which aims to determine which version of edits makes the headline more humorous given the original headline and two edited versions. In prior studies, humor recognition h"
2020.semeval-1.129,2020.acl-demos.28,0,0.474231,"original and the edited headline. Our system involves two similar sub-networks, which generate vector representations for the original and edited headlines respectively. And then we do a subtract operation of the outputs from two sub-networks to predict the funniness of the edited headline. 1 Introduction Humor can be defined as the aspiration of provoking laughter and provides amusement from expressions intended (Bertero and Fung, 2016). The task of humor recognition refers to determining whether a sentence in a given context contains some level of humorous content. The Semeval 2020 Task 7 (Hossain et al., 2020a) aims to automatically computes the funniness of edited news headlines which are generated using an insertion of a single-word noun or verb to replace an existing entity or single-word noun or verb in original headline (Hossain et al., 2019). There are two sub-tasks in Task 7. The sub-task 1 is to predict the mean funniness of the edited headline given the original and edited headline. The sub-task 2 is based on sub-task 1, which aims to determine which version of edits makes the headline more humorous given the original headline and two edited versions. In prior studies, humor recognition h"
2020.semeval-1.129,H05-1067,0,0.422301,"ngle-word noun or verb to replace an existing entity or single-word noun or verb in original headline (Hossain et al., 2019). There are two sub-tasks in Task 7. The sub-task 1 is to predict the mean funniness of the edited headline given the original and edited headline. The sub-task 2 is based on sub-task 1, which aims to determine which version of edits makes the headline more humorous given the original headline and two edited versions. In prior studies, humor recognition has been approached as a binary classification problem. Traditional classification algorithms like SVM and Naive Bayes (Mihalcea and Strapparava, 2005), and a deep learning CNN architecture (Chen and Lee, 2017) are adopted to distinguish between humorous and non-humorous texts. However, humor is not just a binary concept and it occurs in various intensities. In addition, in the past, the research objective for humor recognition is a sentence or text. However, it is interesting to study how short edits applied to a text can turn it from non-funny to funny, which can help us focus on the humorous effects of atomic changes and pointing out the key difference between non-humorous and humorous text. In this paper, we present our funniness predict"
2020.semeval-1.129,D14-1162,0,0.0847538,"to obtain contextual token representations. We also use attention mechanism in order to get the headline representations. 2.1 Pre-trained Word Embeddings The input to each sub-network is a news headline, treated as a sequence of tokens. We use a token representation layer to project the headline W = (w1 , w2 , ..., wt ) to a low-dimensional vector space RE , where E is the size of the representation layer and t is the number of tokens in the headline. By projection, the sequence of tokens can be represented as X = (x1 , x2 , ..., xt ). We obtain token representations using GloVe word vectors (Pennington et al., 2014) and BERT pre-trained word embeddings (Devlin et al., 2018). Here, we just use BERT as a feature extraction model to extract token features. Model Out put Fully connect ed(sigmoid) ... Feat ure Concat enat ion Layer Subtract Layer ... Hand-crafted Features ... Attention Layer Output Attention Layer α2 α1 αt α2 α1 αt ... Bi-LSTM Output h1 h2 h3 LSTM LSTM LSTM LSTM LSTM LSTM ... ... ht h1 h2 h3 LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM ht ... LSTM Bi-LSTM Layer ... ... Word Embeddings x1 x2 x3 How w1 America w2 LSTM ... x4 x1 x2 Pret rained Model Original Text ... x3 x4 Pret rained Model feels abo"
2021.emnlp-main.226,P19-1140,0,0.318335,"ational Linguistics linear operations and massive parameters makes GNN hard to be interpreted thoroughly. As a result, it is hard to judge whether the new designs are universal or just over-fitting on a specific dataset. A recent summary (Zhang et al., 2020) notes that several &quot;advanced&quot; EA methods are even beaten by the conventional methods on several public datasets. (2) Low Efficiency: To further increase the performance, newly proposed EA methods try to stack novel techniques, e.g., Graph Attention Networks (Wu et al., 2019a), Graph Matching Networks (Xu et al., 2019), and Joint Learning (Cao et al., 2019a). Consequently, the overall architectures become more and more unnecessarily complex, resulting in their time-space complexities also dramatically increase. Zhao et al. (2020) present that the running time of complex methods (e.g., RDGCN (Wu et al., 2019a)) is 10× more than that of vanilla GCN (Wang et al., 2018). In this paper, we notice that existing GNN-based EA methods inherit considerable complexity from their neural network lineage. Naturally, we consider eliminating the redundant designs from existing EA methods to enhance interpretability and efficiency without losing accuracy. Lever"
2021.emnlp-main.226,2021.eacl-main.53,0,0.0294768,"ts on DBP15K and SRPRS. Baselines are separated in accord with the three groups described in Section 5.2. Most results are from the original papers. Some recent papers are failed to run on missing datasets or do not release the source code yet. We will fill in these blanks after contacting their authors. 5.2 Baselines We compare our method against the following three groups of advanced EA methods: (1) Structure: These methods only use the structure information (i.e., triples): GCN-Align (Wang et al., 2018), MuGNN (Cao et al., 2019a), BootEA (Sun et al., 2018), MRAEA (Mao et al., 2020), JEANS (Chen et al., 2021). (2) Word-level: These methods average the pre-trained entity name vectors to construct the initial features: GM-Align (Xu et al., 2019), RDGCN (Wu et al., 2019a), HGCN (Wu et al., 2019b), DAT (Zeng et al., 2020b), DGMC (Fey et al., 2020). (3) Char-level: These EA methods further adopt the char-level textual features: AttrGNN (Liu et al., 2020), CEA (Zeng et al., 2020a), EPEA (Wang et al., 2020). For our proposed method, SEU(word) and SEU(char) represent the model only using the word and char features as the inputs, respectively. SEU(w+c) represents concatenating the word and char features to"
2021.emnlp-main.226,2020.emnlp-main.515,0,0.114609,"to map entity names into a unified semantic space and then average the pretrained entity name vectors to construct the initial features. To make fair comparisons, we adopt the same entity name translations and word vectors provided by Xu et al. (2019). Char-Level. Because of the contradiction between the extensive existence of proper nouns (e.g., person and city name) and the limited size of word vocabulary, the word-level EA methods suffer from a serious out of vocabulary (OOV) issue. Therefore, many EA methods explore the char-level features, using char-CNN (Wang et al., 2020) or name-BERT (Liu et al., 2020) to extract the char/sub-word features of entities. In order to keep the simplicity and consistency of our proposed method, we adopt the character bigrams of translated entity names as the char-level input textual features instead of complex neural networks. In addition to these text-based methods, we notice that some structure-based EA methods (Wang et al., 2018; Guo et al., 2019) do not require any textual information at all, where the entity features are randomly initialized. Section 5.6 will discuss the connection between text-based and structure-based methods and challenge the necessity o"
2021.emnlp-main.226,D18-1032,0,0.379971,"n by the conventional methods on several public datasets. (2) Low Efficiency: To further increase the performance, newly proposed EA methods try to stack novel techniques, e.g., Graph Attention Networks (Wu et al., 2019a), Graph Matching Networks (Xu et al., 2019), and Joint Learning (Cao et al., 2019a). Consequently, the overall architectures become more and more unnecessarily complex, resulting in their time-space complexities also dramatically increase. Zhao et al. (2020) present that the running time of complex methods (e.g., RDGCN (Wu et al., 2019a)) is 10× more than that of vanilla GCN (Wang et al., 2018). In this paper, we notice that existing GNN-based EA methods inherit considerable complexity from their neural network lineage. Naturally, we consider eliminating the redundant designs from existing EA methods to enhance interpretability and efficiency without losing accuracy. Leveraging the core premise of GNN-based EA methods, we restate the assumption that both structures and textual features of source and target KGs are isomorphic. With this assumption, we are able to successfully transform the cross-lingual EA problem into an assignment problem, which is a fundamental and well-studied co"
2021.emnlp-main.226,2020.emnlp-main.130,0,0.0207116,"er-fitting on a specific dataset. A recent summary (Zhang et al., 2020) notes that several &quot;advanced&quot; EA methods are even beaten by the conventional methods on several public datasets. (2) Low Efficiency: To further increase the performance, newly proposed EA methods try to stack novel techniques, e.g., Graph Attention Networks (Wu et al., 2019a), Graph Matching Networks (Xu et al., 2019), and Joint Learning (Cao et al., 2019a). Consequently, the overall architectures become more and more unnecessarily complex, resulting in their time-space complexities also dramatically increase. Zhao et al. (2020) present that the running time of complex methods (e.g., RDGCN (Wu et al., 2019a)) is 10× more than that of vanilla GCN (Wang et al., 2018). In this paper, we notice that existing GNN-based EA methods inherit considerable complexity from their neural network lineage. Naturally, we consider eliminating the redundant designs from existing EA methods to enhance interpretability and efficiency without losing accuracy. Leveraging the core premise of GNN-based EA methods, we restate the assumption that both structures and textual features of source and target KGs are isomorphic. With this assumption"
2021.emnlp-main.226,D19-1023,0,0.265298,"1 Introduction (Kipf and Welling, 2017) and subsequent Graph Neural Network (GNN) variants have achieved The knowledge graph (KG) represents a collection state-of-the-art results in various graph application. of interlinked descriptions of real-world objects and events, or abstract concepts (e.g., documents), Intuitively, GNN is better in capturing structural inwhich has facilitated many downstream applica- formation of KGs to compensate for the shortcomtions, such as recommendation systems (Cao et al., ing of conventional methods. Specifically, several GNN-based EA methods (Xu et al., 2019; Wu et al., 2019b; Wang et al., 2019) and question-answering (Zhao et al., 2020; Qiu et al., 2020). Over re- 2019a; Wang et al., 2020) indeed demonstrate decent performance improvements on public datasets. cent years, a large number of KGs are constructed All these GNN-based EA methods are built upon from different domains and languages by different a core premise, i.e., entities and their counterparts organizations. These cross-lingual KGs usually have similar neighborhood structures. However, hold unique information individually but also share better performance is not the only outcome of ussome overlapping"
2021.emnlp-main.226,P19-1304,0,0.313814,"nal Network (GCN) 1 Introduction (Kipf and Welling, 2017) and subsequent Graph Neural Network (GNN) variants have achieved The knowledge graph (KG) represents a collection state-of-the-art results in various graph application. of interlinked descriptions of real-world objects and events, or abstract concepts (e.g., documents), Intuitively, GNN is better in capturing structural inwhich has facilitated many downstream applica- formation of KGs to compensate for the shortcomtions, such as recommendation systems (Cao et al., ing of conventional methods. Specifically, several GNN-based EA methods (Xu et al., 2019; Wu et al., 2019b; Wang et al., 2019) and question-answering (Zhao et al., 2020; Qiu et al., 2020). Over re- 2019a; Wang et al., 2020) indeed demonstrate decent performance improvements on public datasets. cent years, a large number of KGs are constructed All these GNN-based EA methods are built upon from different domains and languages by different a core premise, i.e., entities and their counterparts organizations. These cross-lingual KGs usually have similar neighborhood structures. However, hold unique information individually but also share better performance is not the only outcome of u"
2021.emnlp-main.226,D19-1451,0,0.0191904,"isomorphic graph. Based on the vanilla GCN, many EA methods design task-specific modules for improving the performance of EA. Cao et al. (2019a) propose a multichannel GCN to learn multi-aspect information from KGs. Wu et al. (2019a) use a relation-aware dual-graph network to incorporate relation information with structural information. Moreover, due to the lack of labeled data, some methods (Sun et al., 2018; Mao et al., 2020) apply iterative strategies to generate semi-supervised data. In order to provide a multi-aspect view from both structure and semantic, some methods (Wu et al., 2019b; Yang et al., 2019) use word vectors of translated entity names as the input features of GNNs. 3.2 Assignment Problem The assignment problem is a fundamental and wellstudied combinatorial optimization problem. An intuitive instance is to assign N jobs for N workers. Assuming that each worker can do each job at a term, though with varying degrees of efficiency, let xij be the profit if the i-th worker is assigned to the j-th job. Then the problem is to find the best assignment plan (which job should be assigned to which person in one-to-one basis) so that the total profit of performing all jobs is maximum. Formal"
2021.emnlp-main.226,2020.coling-industry.17,0,0.0428977,"er-fitting on a specific dataset. A recent summary (Zhang et al., 2020) notes that several &quot;advanced&quot; EA methods are even beaten by the conventional methods on several public datasets. (2) Low Efficiency: To further increase the performance, newly proposed EA methods try to stack novel techniques, e.g., Graph Attention Networks (Wu et al., 2019a), Graph Matching Networks (Xu et al., 2019), and Joint Learning (Cao et al., 2019a). Consequently, the overall architectures become more and more unnecessarily complex, resulting in their time-space complexities also dramatically increase. Zhao et al. (2020) present that the running time of complex methods (e.g., RDGCN (Wu et al., 2019a)) is 10× more than that of vanilla GCN (Wang et al., 2018). In this paper, we notice that existing GNN-based EA methods inherit considerable complexity from their neural network lineage. Naturally, we consider eliminating the redundant designs from existing EA methods to enhance interpretability and efficiency without losing accuracy. Leveraging the core premise of GNN-based EA methods, we restate the assumption that both structures and textual features of source and target KGs are isomorphic. With this assumption"
C10-2172,W05-0613,0,0.0392846,"sohn, 2007) extended the work of (Marcu and Echihabi, 2002) by reﬁning the training and classiﬁcation process using parameter optimization, topic segmentation and syntactic parsing. (Lapata and Lascarides, 2004) dealt with temporal links between main and subordinate clauses by inferring the temporal markers linking them. They extracted clause pairs with explicit temporal markers from BLLIP corpus as training data. Another research line is to use humanannotated corpora as training data, e.g., the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (?), (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) beneﬁts the researchers with a large discourse annotated corpora, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classiﬁcation on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classiﬁcation baseline. (Lin et al., 2009) presented an impli"
C10-2172,W01-1605,0,0.643567,"Missing"
C10-2172,W03-1210,0,0.0480087,"Missing"
C10-2172,N04-1020,0,0.0089935,"rns as features to recognize implicit relations between adjacent sentences in a Japanese corpus. They showed that phrasal patterns extracted from a text span pair provide useful evidence in the relation classiﬁcation. (Sporleder and Lascarides, 2008) discovered that Marcu and Echihabi’s models do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. (Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by reﬁning the training and classiﬁcation process using parameter optimization, topic segmentation and syntactic parsing. (Lapata and Lascarides, 2004) dealt with temporal links between main and subordinate clauses by inferring the temporal markers linking them. They extracted clause pairs with explicit temporal markers from BLLIP corpus as training data. Another research line is to use humanannotated corpora as training data, e.g., the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (?), (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) beneﬁts the researchers wit"
C10-2172,P02-1047,0,0.383456,"e a part of many natural language processing systems, e.g., text summarization system, question answering system. If there are discourse connectives between textual units to explicitly mark their relations, the recognition task on these texts is deﬁned as explicit discourse relation recognition. Otherwise it is deﬁned as implicit discourse relation recognition. However, for implicit relations, there are no connectives to explicitly mark the relations, which makes the recognition task quite difﬁcult. Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). They use unambiguous patterns such as [Arg1, but Arg2] to create synthetic examples of implicit relations and then use [Arg1, Arg2] as an training example of an implicit relation. Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al., 2009a) and (Lin et al., 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses"
C10-2172,P09-1077,0,0.648462,"ional features We predict implicit connectives on both training set and test set. Then we can use the predicted implicit connectives as additional features for supervised implicit relation recognition. Previous works exploited various linguistically informed features under the framework of supervised models. In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, ﬁrst-last words of arguments, cross-argument word pairs, ever used in (Pitler et al., 2009a), production rules of parse trees of arguments used in (Lin et al., 2009), and intra-argument word pairs inspired by the work of (Saito et al., 2006). Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al., 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). In addition, the average length of verb phrase and the part of speech tags of main verb are also included as verb features. Context: If the immediately preceding"
C10-2172,P09-2004,0,0.281099,"Missing"
C10-2172,prasad-etal-2008-penn,0,0.418208,"syntactic parsing. (Lapata and Lascarides, 2004) dealt with temporal links between main and subordinate clauses by inferring the temporal markers linking them. They extracted clause pairs with explicit temporal markers from BLLIP corpus as training data. Another research line is to use humanannotated corpora as training data, e.g., the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (?), (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) beneﬁts the researchers with a large discourse annotated corpora, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classiﬁcation on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classiﬁcation baseline. (Lin et al., 2009) presented an implicit discourse relation classiﬁer in PDTB with the use of contextual relations, constituent Parse Features, dependency parse features and crossargument w"
C10-2172,N06-2034,0,0.0171666,"Missing"
C10-2172,W06-1317,0,0.205213,"Missing"
C10-2172,D09-1036,0,\N,Missing
C10-2172,N03-1030,0,\N,Missing
D17-1134,D15-1262,0,0.269261,"text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 200"
D17-1134,D16-1020,0,0.10315,"Missing"
D17-1134,P16-1163,0,0.391635,"many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs tw"
D17-1134,Q15-1024,0,0.123199,"ognition on natural (i.e., genuine) discourse data with the use of traditional NLP techniques to extract linguistically informed features and traditional machine learning algorithms (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015). Later, to make a full use of unlabelled data, several studies performed multi-task or unsupervised learning methods (Lan et al., 2013; Braud and Denis, 2015; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Recently, with the development of deep learning, researchers resorted to neural networks methods (Ji and Eisenstein, 2015; Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). 5.2 Multi-task learning Multi-task learning framework adopts traditional machine learning with human-selected effective knowledge and the shared part is integrated into the cost function to prefer the main task learning. (Collobert and Weston, 2008) proposed a multitask neural network trained jointly on the relevant tasks using weight-sharing (sharing the word embeddings with tasks). (Liu et al., 2016a) proposed the multi-task neural network by modifying the"
D17-1134,P13-1047,1,0.763423,"isher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at the same time. An attention-based neural network conducts discourse relationship representation learning through interaction between two discourse arguments. Meanwhile, a multi-task learning framework leverages knowledge from auxiliary task to enhance the performance of main task. Furthermore, these two types of learn"
D17-1134,D09-1036,0,0.0378244,"Missing"
D17-1134,D16-1130,0,0.582461,"analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at the same time. An"
D17-1134,P09-1077,0,0.0836555,"tion (or rhetorical relation) identification is to recognize how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognit"
D17-1134,prasad-etal-2008-penn,0,0.0261204,"r proposed model outperforms the state-of-the-art systems on benchmark corpora. 1 Introduction The task of implicit discourse relation (or rhetorical relation) identification is to recognize how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the sh"
D17-1134,C16-1180,0,0.627907,", QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at"
D17-1134,D16-1246,0,0.534199,", QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at"
D17-1134,N15-1081,0,0.107539,"ask. 5 5.1 Related Work Implicit Discourse With the release of PDTB 2.0, a number of studies performed discourse relation recognition on natural (i.e., genuine) discourse data with the use of traditional NLP techniques to extract linguistically informed features and traditional machine learning algorithms (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015). Later, to make a full use of unlabelled data, several studies performed multi-task or unsupervised learning methods (Lan et al., 2013; Braud and Denis, 2015; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Recently, with the development of deep learning, researchers resorted to neural networks methods (Ji and Eisenstein, 2015; Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). 5.2 Multi-task learning Multi-task learning framework adopts traditional machine learning with human-selected effective knowledge and the shared part is integrated into the cost function to prefer the main task learning. (Collobert and Weston, 2008) proposed a multitask neural network trained jointly on the relevant tasks using weight-sh"
D17-1134,K16-2007,0,0.175529,"Missing"
D17-1134,P16-1044,0,0.0113504,"Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-task attention-based neural network model for implicit discourse relationship representation and identification. Our method captures both the discourse relationships through interactio"
D17-1134,P16-1122,0,0.0143279,"ed system which can effectively use synthetic data for implicit discourse relation recognition. (Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-task attention-based neural network model for implicit discourse relationship represent"
D17-1134,K16-2004,1,0.887562,"Missing"
D17-1134,P16-1123,0,0.0214013,"Missing"
D17-1134,P10-1073,0,0.0615245,"ation is to recognize how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine lea"
D17-1134,D16-1058,0,0.0138575,"ed system which can effectively use synthetic data for implicit discourse relation recognition. (Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-task attention-based neural network model for implicit discourse relationship represent"
D17-1134,D16-1253,0,0.70684,"Missing"
D17-1134,C16-1164,0,0.0159977,"Missing"
D17-1134,D15-1266,0,0.09134,"tion and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognit"
D17-1134,P16-2034,0,0.00957092,"k for text classification tasks. (Lan et al., 2013) present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. (Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-tas"
D17-1134,C10-2172,1,0.40213,"ze how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Co"
D18-1249,P11-1056,0,0.448582,"nt learning paradigm based on minimum risk training for the joint entity relation extraction task. 2. implementing a strong and simple neuralnetwork-based entity relation extraction model which carries the proposed MRT algorithm. 1 1 Our implementation is available at https:// github.com/changzhisun/entrel-joint-mrt. 3. achieving state-of-the-art results on two benchmark datasets (ACE05 and NYT). 2 Related Work In many pipelined entity relation extraction systems, one first learns an entity model, then learns a relation model based on entities generated by the entity model (Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Such systems are often flexible to incorporate different data sources and different learning algorithms. However, they may also suffer from error propagation and data inefficiency. To tackle the problem, many recent studies try to develop joint extraction algorithms. Parameter sharing is a basic strategy in joint learning paradigms. For example, in (Miwa and Bansal, 2016), the entity model is a sentence-level RNN, and the relation model is a dependency tree path RNN which takes hidden states of the entity model as features (i.e., the shared parameters). Our basic extractio"
D18-1249,N10-1112,0,0.0245376,"al. (2017) cannot handle entities which appear in multiple relations). On the other side, Li and Ji (2014) develop a joint decoding algorithm based on beam search. Zhang et al. (2017) study a globally normalized joint model. They retain capacities of submodels, while their decoding algorithms are inexact. Here, we introduce MRT to the task, which is a more lightweight setting of joint learning. Minimum risk training is a learning framework which tries to handle models with arbitrary discrepancy metrics (i.e., losses of a model output w.r.t. the true answer) (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010). It has been successfully applied to many NLP tasks. Some recent work include (He and Deng, 2012; Shen et al., 2016) which apply MRT to (neural) ma2257 chine translation, (Xu et al., 2016) which develops a shift-reduce CCG parser to directly optimize F1, and (Ayana et al., 2016) which uses a MRT-based model for summarization. We note that most previous applications of MRT focus on a single job, while the joint entity relation extraction consists of two sub-tasks. Investigating MRT in joint learning scenarios is the main topic of this work. Finally, the sampling algorithm of solving MRT is sim"
D18-1249,P82-1020,0,0.858045,"Missing"
D18-1249,P16-1087,0,0.0831244,"odel can share some input features or internal hidden states. It has an advantage that no additional constraint is required on the two submodels. But the connections among sub-models are still not fully explored due to independent submodel decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied (Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Zheng et al., 2017). In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models. In this work, we propose a joint minimum risk training (MRT) (Och, 2003; Smith and Eisner, 2006) method for the entity and relation extraction 2256 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics task. It provides a lightweight way to stre"
D18-1249,P17-1085,0,0.448424,"ipeline model and the joint model. In the pipeline setting, the task is broken down into independently learned components (an entity model and a relation model). Despite its flexibility, the pipeline ignores interactions between the two models. For example, the entity model doesn’t look at relation annotations which are useful for identifying entities (e.g., if an ORG-AFF relation exists, the entity model can only assign ORG and AFF to its entities). The joint setting, on the other hand, extracts entities One simple joint learning paradigm is through sharing parameters (Miwa and Bansal, 2016; Katiyar and Cardie, 2017). Typically, instead of training two independent models, the entity and relation model can share some input features or internal hidden states. It has an advantage that no additional constraint is required on the two submodels. But the connections among sub-models are still not fully explored due to independent submodel decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding ent"
D18-1249,P17-1138,0,0.0277482,"Missing"
D18-1249,D16-1127,0,0.0227046,"optimize F1, and (Ayana et al., 2016) which uses a MRT-based model for summarization. We note that most previous applications of MRT focus on a single job, while the joint entity relation extraction consists of two sub-tasks. Investigating MRT in joint learning scenarios is the main topic of this work. Finally, the sampling algorithm of solving MRT is similar to the policy gradient algorithm in reinforcement learning (RL) (Sutton and Barto, 1998). Some recent NLP applications which share the key idea of MRT but are described with RL language also show promising results (e.g., dialog systems (Li et al., 2016), machine translation (Nguyen et al., 2017)). The idea of learning loss functions from data is similar to inverse reinforcement learning (Abbeel and Ng, 2004; Ratliff et al., 2006). 3 tity and ∗ ∈ Te represents different entity types. For example, for a person (PER) entity “Patrick McDowell”, we assign B-PER to “Patrick” and L-PER to “McDowell”. Given an input sentence s, the entity model predicts the tags of words ˆt = tˆ1 , tˆ2 , . . . , tˆ|s |by learning from the true tags t = t1 , t2 , . . . , t|s |. We use a bidirectional long short term memory (bi-LSTM) network (Hochreiter and Schmidhube"
D18-1249,P14-1038,0,0.455952,"ty and relation model can share some input features or internal hidden states. It has an advantage that no additional constraint is required on the two submodels. But the connections among sub-models are still not fully explored due to independent submodel decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied (Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Zheng et al., 2017). In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models. In this work, we propose a joint minimum risk training (MRT) (Och, 2003; Smith and Eisner, 2006) method for the entity and relation extraction 2256 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics task. It provides"
D18-1249,P16-1200,0,0.0429372,"based on minimum risk training for the joint entity relation extraction task. 2. implementing a strong and simple neuralnetwork-based entity relation extraction model which carries the proposed MRT algorithm. 1 1 Our implementation is available at https:// github.com/changzhisun/entrel-joint-mrt. 3. achieving state-of-the-art results on two benchmark datasets (ACE05 and NYT). 2 Related Work In many pipelined entity relation extraction systems, one first learns an entity model, then learns a relation model based on entities generated by the entity model (Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Such systems are often flexible to incorporate different data sources and different learning algorithms. However, they may also suffer from error propagation and data inefficiency. To tackle the problem, many recent studies try to develop joint extraction algorithms. Parameter sharing is a basic strategy in joint learning paradigms. For example, in (Miwa and Bansal, 2016), the entity model is a sentence-level RNN, and the relation model is a dependency tree path RNN which takes hidden states of the entity model as features (i.e., the shared parameters). Our basic extraction model is similar"
D18-1249,P16-1105,0,0.341187,"extraction task, the pipeline model and the joint model. In the pipeline setting, the task is broken down into independently learned components (an entity model and a relation model). Despite its flexibility, the pipeline ignores interactions between the two models. For example, the entity model doesn’t look at relation annotations which are useful for identifying entities (e.g., if an ORG-AFF relation exists, the entity model can only assign ORG and AFF to its entities). The joint setting, on the other hand, extracts entities One simple joint learning paradigm is through sharing parameters (Miwa and Bansal, 2016; Katiyar and Cardie, 2017). Typically, instead of training two independent models, the entity and relation model can share some input features or internal hidden states. It has an advantage that no additional constraint is required on the two submodels. But the connections among sub-models are still not fully explored due to independent submodel decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g.,"
D18-1249,D09-1013,0,0.394332,"proposing a new joint learning paradigm based on minimum risk training for the joint entity relation extraction task. 2. implementing a strong and simple neuralnetwork-based entity relation extraction model which carries the proposed MRT algorithm. 1 1 Our implementation is available at https:// github.com/changzhisun/entrel-joint-mrt. 3. achieving state-of-the-art results on two benchmark datasets (ACE05 and NYT). 2 Related Work In many pipelined entity relation extraction systems, one first learns an entity model, then learns a relation model based on entities generated by the entity model (Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Such systems are often flexible to incorporate different data sources and different learning algorithms. However, they may also suffer from error propagation and data inefficiency. To tackle the problem, many recent studies try to develop joint extraction algorithms. Parameter sharing is a basic strategy in joint learning paradigms. For example, in (Miwa and Bansal, 2016), the entity model is a sentence-level RNN, and the relation model is a dependency tree path RNN which takes hidden states of the entity model as features (i.e., the shared parameters)"
D18-1249,P16-1218,0,0.032729,"build fe2 for e2 with another CNN. where θR = {θe1 , θe2 , θmiddle , W1 , W2 } contains parameters of the relation model (shared parameters with the entity model are omitted). Given an input sentence s, the training objective is to minimize For context features of the entity pair (e1 , e2 ), we build three feature vectors by looking at words between e1 and e2 (fmiddle ), words on the left of the pair (fleft ) and words on the right of the pair (fright ). For fmiddle , we run a CNN on words between e1 and e2 like the case of fe1 , fe2 . For fleft and fright , we use the “LSTM-Minus” method as (Wang and Chang, 2016; Zhang et al., 2017). Assume that the left context of (e1 , e2 ) is from sentence position 0 to i, then fleft = hi ⊕ (h0 − hi+1 ). Similarly, if the right context of (e1 , e2 ) is from j to |s |− 1, then fright = (h|s|−1 − hj−1 ) ⊕ hj . We also use a onehot feature fdist to describe the distance between e1 and e2 in the sentence. Finally, fe1 , fe2 , fmiddle , fleft , fright and fdist are concatenated to a single vector fe1 ,e2 . To get the Prel (ˆl|s, e1 , e2 ; θR ) = Softmax(W2 · ReLU(W1 · fe1 ,e2 )), (2) Lrel (θR ) = − ∑ log Prel (ˆl = l|s, e1 , e2 ; θR ) , ˆ E| ˆ − 1) |E|(| e1 ,e2 ∈"
D18-1249,D17-1153,0,0.0433919,"Missing"
D18-1249,P03-1021,0,0.290309,"annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied (Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Zheng et al., 2017). In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models. In this work, we propose a joint minimum risk training (MRT) (Och, 2003; Smith and Eisner, 2006) method for the entity and relation extraction 2256 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics task. It provides a lightweight way to strengthen connections between the entity model and the relation model, and keeps their capacities unaffected. Given an input x and a loss function ∆(ˆ y, y) (measuring the difference between model output ˆ and the true y), MRT seeks a posterior P (ˆ y y|x) to minimize the expecte"
D18-1249,N16-1025,0,0.0519631,"Missing"
D18-1249,D17-1182,0,0.345808,"Missing"
D18-1249,P17-1113,0,0.329403,"hidden states. It has an advantage that no additional constraint is required on the two submodels. But the connections among sub-models are still not fully explored due to independent submodel decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied (Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Zheng et al., 2017). In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models. In this work, we propose a joint minimum risk training (MRT) (Och, 2003; Smith and Eisner, 2006) method for the entity and relation extraction 2256 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics task. It provides a lightweight way to strengthen connections between the entity mod"
D18-1249,P16-1159,0,0.370465,"decoding algorithm based on beam search. Zhang et al. (2017) study a globally normalized joint model. They retain capacities of submodels, while their decoding algorithms are inexact. Here, we introduce MRT to the task, which is a more lightweight setting of joint learning. Minimum risk training is a learning framework which tries to handle models with arbitrary discrepancy metrics (i.e., losses of a model output w.r.t. the true answer) (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010). It has been successfully applied to many NLP tasks. Some recent work include (He and Deng, 2012; Shen et al., 2016) which apply MRT to (neural) ma2257 chine translation, (Xu et al., 2016) which develops a shift-reduce CCG parser to directly optimize F1, and (Ayana et al., 2016) which uses a MRT-based model for summarization. We note that most previous applications of MRT focus on a single job, while the joint entity relation extraction consists of two sub-tasks. Investigating MRT in joint learning scenarios is the main topic of this work. Finally, the sampling algorithm of solving MRT is similar to the policy gradient algorithm in reinforcement learning (RL) (Sutton and Barto, 1998). Some recent NLP applic"
D18-1249,P06-2101,0,0.284037,", the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction between decoders, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied (Li and Ji, 2014; Katiyar and Cardie, 2016; Zhang et al., 2017; Zheng et al., 2017). In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models. In this work, we propose a joint minimum risk training (MRT) (Och, 2003; Smith and Eisner, 2006) method for the entity and relation extraction 2256 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2256–2265 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics task. It provides a lightweight way to strengthen connections between the entity model and the relation model, and keeps their capacities unaffected. Given an input x and a loss function ∆(ˆ y, y) (measuring the difference between model output ˆ and the true y), MRT seeks a posterior P (ˆ y y|x) to minimize the expected loss Eyˆ ∼P (ˆy|x) ∆(ˆ"
D19-1635,W19-3621,0,0.376517,"ociation graph. It turns out that both kinds of graphs exhibit “small world” properties, but they have different hub words and connecting mechanisms. We then run the same stereotype propagation algorithm and compare the derived gender bias scores. Third, as a case study, we use word-association-based scores as a benchmark to see the effectiveness of existing debias algorithms on word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018). The results suggest that it is hard for both de-bias algorithms to remove gender stereotypes in word embeddings completely, which matches the conclusion in (Gonen and Goldberg, 2019). 2 Word Association Test Word association test is a simple and sometimes entertaining game in which participants are asked to respond with the first several words that come out in their mind (the response) after being presented with a word (the cue)2 . Table 1 lists some examples of the test. It is considered to be one of the most straightforward approaches for gaining insight into our semantic knowledge (Steyvers and Tenenbaum, 2005), and is also a common aphttps://en.wikipedia.org/wiki/Word_ Association R1 R2 R3 way extra i come than son mind path plus you go then daughter brain via special"
D19-1635,D16-1057,0,0.0399865,"Missing"
D19-1635,D14-1162,0,0.0851124,"e composition of these hub words. It turns out that most genderspecific words (except for he, she, daughter, aunt) in L are contained in the hub words of the word association graph, while none of them lie within those of the embedding-based graphs. Therefore, there is no wonder that L in the word association graph could distribute its gender information to other words in the graph more efficiently, leading to better performance in detecting gender bias of words. Co-occurrence Due to the fact that most commonly used word embedding models are trained on word co-occurrence (Mikolov et al., 2013; Pennington et al., 2014), we observe lots of connections with such type in bias paths on word embedding graphs, which also widely exists in those on the word association graph, such as birth-day and even-more. To summarize, frequency effects, neighborhood effects, and more concentrated hubs may work together to help spread gender stereotypes. It shows the value of introducing word association test into gender stereotype research. 5.5 Case Study: Do De-bias Methods Really Remove Gender Bias? Some works have proposed approaches to reduce gender stereotypes in word embeddings. They can mainly be divided into two categor"
D19-1635,N10-1119,0,0.029935,"e Influence of L Pearson’s r Under Various α 0.58 census data human judgments census 0.56 0.56 0.58 0.60 0.62 0.64 0.66 0.655 0.650 0.52 0.645 0.68 0.50 0.640 human judgments 0.54 0.660 0.54 census data human 0.665 Pearson’s r calculate the confidence intervals of correlations between our results and gender stereotypes in the real world in Figure 3. The results show that the words’ bias scores are stable with respect to reasonable settings of L. 5.3 Variants of Stereotype Propagation There are many variants of the random walk method in previous literature (Zhou et al., 2003; Zhu et al., 2003; Velikovich et al., 2010; Vicente et al., 2017). We experiment with another approach in (Zhu et al., 2003), to test whether bias scores of words are insensitive towards random walk algorithms. In this method, we consider only local consistency (without the second term of Equation 1). As a result, we find high consistency between their bias scores (Pearson’s r = 0.789, p = 0.0). We also examine the influence of hyper parameter α. Here, we conduct similar correlation analysis in Section 5.1 under various α (Figure 4). We find that stereotype propagation has a stable performance regarding different α, while a proper α ("
D19-1635,D18-1521,0,0.310971,"vanilla wordembedding-based scores. Second, as embeddings could also be applied for building graphs, we investigate such graphs and look into their differences with the word association graph. It turns out that both kinds of graphs exhibit “small world” properties, but they have different hub words and connecting mechanisms. We then run the same stereotype propagation algorithm and compare the derived gender bias scores. Third, as a case study, we use word-association-based scores as a benchmark to see the effectiveness of existing debias algorithms on word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018). The results suggest that it is hard for both de-bias algorithms to remove gender stereotypes in word embeddings completely, which matches the conclusion in (Gonen and Goldberg, 2019). 2 Word Association Test Word association test is a simple and sometimes entertaining game in which participants are asked to respond with the first several words that come out in their mind (the response) after being presented with a word (the cue)2 . Table 1 lists some examples of the test. It is considered to be one of the most straightforward approaches for gaining insight into our semantic knowledge (Steyve"
E17-1097,N15-1146,0,0.0240204,". We aim to make all algorithms simple, fast and scalable for large-scale corpus. Our system is tested on Amazon review data which contains 15 different domains and 33 million reviews. The output database contains 72.5 million pairs of opinion relations. Extensive experiments have been conducted on various aspects of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annot"
E17-1097,P15-1061,0,0.0341312,"orithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al.,"
E17-1097,C08-1031,0,0.0342543,"ception”, “unit”), and (“touch your heart”, “The Passion of The Christ”). Extracting opinion bearing relations is usually the first step towards fine-gained analysis of opinion in texts, and plays an important role in other sentiment related applications (e.g., sentiment summarization). The goal of this paper is to extract opinion relations from open domain largescale opinion bearing texts. Previous works on fine-grained opinion information extraction have achieved notable success on many aspects: various domains were examined (Pontiki et al., 2015), different types of relations were studied (Ganapathibhotla and Liu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff a"
E17-1097,J13-3002,0,0.0219279,"hich contains 15 different domains and 33 million reviews. The output database contains 72.5 million pairs of opinion relations. Extensive experiments have been conducted on various aspects of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootst"
E17-1097,klinger-cimiano-2014-usage,0,0.156827,"figurations We extract opinion relations on a subset of Amazon product review corpus provided by (McAuley et al., 2015), which contains 15 domains and 33 million reviews. The statistics of extracted relations are in Table 2. For quantitative evaluation, we select four domains (Cell Phones, Movie and TV, Food, Pet Supplies) for detailed analyses. We manually label all correct opinion relations in 1000 sentences, and select 200 sentences as the development set, the rest 800 as the test set 3 . Furthermore, to compare with previous supervised methods, we also conduct experiments on USAGE corpus (Klinger and Cimiano, 2014) which annotates 4481 opinion relations for 8 products. We use NLTK (Bird et al., 2009) for sentence splitting and word segmentation, Stanford parser 4 for getting POS tags, phrase chunks and dependency trees, and scikit-learn toolkit (Pedregosa et al., 2011) and TensorFlow 5 for machine learning algorithms. The general purpose opinion lexicon is from (Wilson et al., 2005). 4.2 Main Results Table 4 shows results on four domains. The methods for comparison are: • Adjacent is a simple baseline system from (Hu and Liu, 2004). It first identifies words in the general purpose opinion lexicon, then"
E17-1097,D07-1114,0,0.0320979,"on Amazon review data which contains 15 different domains and 33 million reviews. The output database contains 72.5 million pairs of opinion relations. Extensive experiments have been conducted on various aspects of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Appr"
E17-1097,C10-1074,0,0.0363762,"thm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models"
E17-1097,D15-1278,0,0.0212734,"tantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu"
E17-1097,S14-2004,0,0.183539,"Missing"
E17-1097,P14-1030,0,0.0171181,"t candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and"
E17-1097,S15-2082,0,0.0608991,"Missing"
E17-1097,P09-1113,0,0.724846,"rdie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network struc"
E17-1097,P16-1105,0,0.0208472,"or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al., 2015). Instead of locating the opinion expressions, aspect-based opinion mining di"
E17-1097,P12-1036,0,0.0202368,"d joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models"
E17-1097,D09-1019,0,0.0283461,"your heart”, “The Passion of The Christ”). Extracting opinion bearing relations is usually the first step towards fine-gained analysis of opinion in texts, and plays an important role in other sentiment related applications (e.g., sentiment summarization). The goal of this paper is to extract opinion relations from open domain largescale opinion bearing texts. Previous works on fine-grained opinion information extraction have achieved notable success on many aspects: various domains were examined (Pontiki et al., 2015), different types of relations were studied (Ganapathibhotla and Liu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff and Wiebe, 2003)), or har"
E17-1097,J11-1002,0,0.140584,"Missing"
E17-1097,W03-1014,0,0.19536,"iu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff and Wiebe, 2003)), or hard-tocontrol noise (e.g., bootstrapping approaches (Qiu et al., 2011)). On the other hand, supervised models can achieve better performances than patterns on manually labeled datasets, but it is often difficult to obtain large number of annotations for the relation extraction task, and the trained models are 1033 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1033–1043, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics also limited to specified domains. Thus, we s"
E17-1097,P08-1036,0,0.0501098,"Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wan"
E17-1097,N16-1065,0,0.0589031,"Missing"
E17-1097,P15-1060,0,0.041169,"Missing"
E17-1097,H05-1044,0,0.0959387,"nion relations in 1000 sentences, and select 200 sentences as the development set, the rest 800 as the test set 3 . Furthermore, to compare with previous supervised methods, we also conduct experiments on USAGE corpus (Klinger and Cimiano, 2014) which annotates 4481 opinion relations for 8 products. We use NLTK (Bird et al., 2009) for sentence splitting and word segmentation, Stanford parser 4 for getting POS tags, phrase chunks and dependency trees, and scikit-learn toolkit (Pedregosa et al., 2011) and TensorFlow 5 for machine learning algorithms. The general purpose opinion lexicon is from (Wilson et al., 2005). 4.2 Main Results Table 4 shows results on four domains. The methods for comparison are: • Adjacent is a simple baseline system from (Hu and Liu, 2004). It first identifies words in the general purpose opinion lexicon, then finds the nearest noun or verb phrase to them as their opinion targets. 3 https://github.com/AntNLP/OpinionRelationCorpus http://nlp.stanford.edu/software/lex-parser.shtml 5 https://www.tensorflow.org/ Domain #Reviews #Sents #Relations Cell Phones Movie and TV Food Pet Supplies Automotive Digital Music Beauty Toys and Games Instruments Office Products Patio Baby Clothing S"
E17-1097,D09-1159,1,0.780249,"cts of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabili"
E17-1097,D11-1123,1,0.742611,"n of The Christ”). Extracting opinion bearing relations is usually the first step towards fine-gained analysis of opinion in texts, and plays an important role in other sentiment related applications (e.g., sentiment summarization). The goal of this paper is to extract opinion relations from open domain largescale opinion bearing texts. Previous works on fine-grained opinion information extraction have achieved notable success on many aspects: various domains were examined (Pontiki et al., 2015), different types of relations were studied (Ganapathibhotla and Liu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff and Wiebe, 2003)), or hard-tocontrol noise"
E17-1097,P13-1173,0,0.0194606,"hich first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural"
E17-1097,D15-1203,0,0.027498,"al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation class"
E17-1097,D10-1006,0,0.0325526,"gated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al., 2015). Instead of locating the opinion expressions, aspect-based opinion mining directly analyzes polarities of different opinion targets. The targets are usually constrained to be some predefined set. Shared tasks (SemEval2014, SemEval2015) have been held on the task, and various systems are proposed and evaluated (Pontiki et al., 2014; Pontiki et al., 2015). Comparing with aspect-based opinion mining, we will extract opinion expressions which are more informative, and we won’t constrain opinion target types which helps us to handle open domain texts. 3 The App"
E17-1097,D15-1062,0,0.181134,"babilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which hel"
E17-1097,D15-1206,0,0.170705,"babilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which hel"
E17-1097,D12-1122,0,0.0219912,"ormances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 20"
E17-1097,P13-1161,0,0.0209364,"tion extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et a"
E17-1097,Q14-1039,0,0.0194001,"mportant task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use rela"
E17-1097,D11-1013,0,0.0190978,"015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al., 2015). Instead of locating the opinion expressions, aspect-based opinion mining directly analyzes polarities of different opinion targets. The targets are usually constrained to be some predefined set. Shared tasks (SemEval2014, SemEval2015) have been held on the task, and various systems are proposed and evaluated (Pontiki et al., 2014; Pontiki et al., 2015). Comparing with aspect-based opinion mining, we will extract opinion expressions which are more informative, and we won’t constrain opinion target types which helps us to handle open domain texts. 3 The Approach Given an in"
E17-1097,N10-1122,0,\N,Missing
K15-2002,I11-1120,0,0.0564733,"rser Jianxiang Wang, Man Lan∗ Shanghai Key Laboratory of Multidimensional Information Processing Department of Computer Science and Technology, East China Normal University, Shanghai 200241, P. R. China 51141201062@ecnu.cn, mlan@cs.ecnu.edu.cn∗ Abstract fying Explicit or Implicit relations. To identify discourse connectives from non-discourse ones and to classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of e"
K15-2002,D14-1008,0,0.473415,"China 51141201062@ecnu.cn, mlan@cs.ecnu.edu.cn∗ Abstract fying Explicit or Implicit relations. To identify discourse connectives from non-discourse ones and to classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of explicit discourse connective as auxiliary learning tasks to improve the performance. The CoNLL-2015 shared task focuses on shallow discourse parsing, which takes a piece of newswire text as input"
K15-2002,P13-1047,1,0.876779,"improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of explicit discourse connective as auxiliary learning tasks to improve the performance. The CoNLL-2015 shared task focuses on shallow discourse parsing, which takes a piece of newswire text as input and returns the discourse relations in a PDTB style. In this paper, we describe our discourse parser that participated in the shared task. We use 9 components to construct the whole parser to identify discourse connectives, label arguments and classify the sense of Explicit or Non-Explicit relations in free texts. Compared t"
K15-2002,D09-1036,0,0.28656,"Missing"
K15-2002,P09-2004,0,0.692619,"Missing"
K15-2002,prasad-etal-2008-penn,0,0.812352,"PDTB Section 23 and a blind test dataset. 1 Introduction An end-to-end discourse parser is given free texts as input and returns discourse relations in a PDTB style, where a connective acts as a predicate that takes two text spans as its arguments. It can benefit many downstream NLP applications, such as information retrieval, question answering and automatic summarization, etc. The extraction of exact argument spans and Non-Explicit sense identification have been shown to be the main challenges of the discourse parsing (Lin et al., 2014). Since the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), much research has been carried out on PDTB to perform the subtasks of a full end-to-end parser, such as identifying discourse connectives, labeling arguments and classiAll of these research focus on the subtasks of the PDTB, and can be viewed as isolated components of a full parser. (Lin et al., 2014) constructed a full parser on the top of these subtasks, which contained multiple components joined in a sequential pipeline architecture including a connective classifier, argument labeler, explicit classifier, non-explicit classifier, and attribution span labeler. In this paper, we followed th"
K15-2002,E14-1068,0,0.368804,"classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of explicit discourse connective as auxiliary learning tasks to improve the performance. The CoNLL-2015 shared task focuses on shallow discourse parsing, which takes a piece of newswire text as input and returns the discourse relations in a PDTB style. In this paper, we describe our discourse parser that participated in the shared task. We use 9 components to const"
K15-2002,K15-2001,0,\N,Missing
K16-2004,D15-1262,0,0.244377,"nd on the blind datasets. 1 Introduction A discourse relation between two segments of textual units expresses how they are logically connected to one another (cause or contrast), which is considered a crucial step for the ability to properly interpret or produce discourse. It can be of great benefit to many downstream natural language processing (NLP) applications and has attracted lots of research (Pitler and Nenkova, 2009; Lin et al., 2009; Pitler et al., 2009; Lan et al., 2013; Rutherford and Xue, 2014; Lin et al., 2014; Kong et al., 2014; Ji and Eisenstein, 2015; Fisher and Simmons, 2015; Braud and Denis, 2015; Zhang et al., 2015). Following the first edition in CoNLL-2015 (Xue et al., 2015), CoNLL-2016 (Xue et al., 2016) is the 2nd edition of the CoNLL Shared Task on Shallow Discourse Parsing, which contains following tasks: discourse parsing task and supplementary task (sense classification using gold standard argument pairs) in English and Chinese. To build an English parser, we follow (Wang and Lan, 2015b)’s work except for several modifications described later in section 2. In consideration of distinct linguistic and syntactic difference between English and Chinese, for Chinese parser, we desi"
K16-2004,P15-2015,0,0.0186729,"two parsers both rank second on the blind datasets. 1 Introduction A discourse relation between two segments of textual units expresses how they are logically connected to one another (cause or contrast), which is considered a crucial step for the ability to properly interpret or produce discourse. It can be of great benefit to many downstream natural language processing (NLP) applications and has attracted lots of research (Pitler and Nenkova, 2009; Lin et al., 2009; Pitler et al., 2009; Lan et al., 2013; Rutherford and Xue, 2014; Lin et al., 2014; Kong et al., 2014; Ji and Eisenstein, 2015; Fisher and Simmons, 2015; Braud and Denis, 2015; Zhang et al., 2015). Following the first edition in CoNLL-2015 (Xue et al., 2015), CoNLL-2016 (Xue et al., 2016) is the 2nd edition of the CoNLL Shared Task on Shallow Discourse Parsing, which contains following tasks: discourse parsing task and supplementary task (sense classification using gold standard argument pairs) in English and Chinese. To build an English parser, we follow (Wang and Lan, 2015b)’s work except for several modifications described later in section 2. In consideration of distinct linguistic and syntactic difference between English and Chinese, for"
K16-2004,Q15-1024,0,0.0981802,"encouraging results. Our two parsers both rank second on the blind datasets. 1 Introduction A discourse relation between two segments of textual units expresses how they are logically connected to one another (cause or contrast), which is considered a crucial step for the ability to properly interpret or produce discourse. It can be of great benefit to many downstream natural language processing (NLP) applications and has attracted lots of research (Pitler and Nenkova, 2009; Lin et al., 2009; Pitler et al., 2009; Lan et al., 2013; Rutherford and Xue, 2014; Lin et al., 2014; Kong et al., 2014; Ji and Eisenstein, 2015; Fisher and Simmons, 2015; Braud and Denis, 2015; Zhang et al., 2015). Following the first edition in CoNLL-2015 (Xue et al., 2015), CoNLL-2016 (Xue et al., 2016) is the 2nd edition of the CoNLL Shared Task on Shallow Discourse Parsing, which contains following tasks: discourse parsing task and supplementary task (sense classification using gold standard argument pairs) in English and Chinese. To build an English parser, we follow (Wang and Lan, 2015b)’s work except for several modifications described later in section 2. In consideration of distinct linguistic and syntactic difference between"
K16-2004,D14-1008,0,0.134719,"se parser achieves encouraging results. Our two parsers both rank second on the blind datasets. 1 Introduction A discourse relation between two segments of textual units expresses how they are logically connected to one another (cause or contrast), which is considered a crucial step for the ability to properly interpret or produce discourse. It can be of great benefit to many downstream natural language processing (NLP) applications and has attracted lots of research (Pitler and Nenkova, 2009; Lin et al., 2009; Pitler et al., 2009; Lan et al., 2013; Rutherford and Xue, 2014; Lin et al., 2014; Kong et al., 2014; Ji and Eisenstein, 2015; Fisher and Simmons, 2015; Braud and Denis, 2015; Zhang et al., 2015). Following the first edition in CoNLL-2015 (Xue et al., 2015), CoNLL-2016 (Xue et al., 2016) is the 2nd edition of the CoNLL Shared Task on Shallow Discourse Parsing, which contains following tasks: discourse parsing task and supplementary task (sense classification using gold standard argument pairs) in English and Chinese. To build an English parser, we follow (Wang and Lan, 2015b)’s work except for several modifications described later in section 2. In consideration of distinct linguistic and syn"
K16-2004,P13-1047,1,0.879006,"ance than the best system of CoNLL2015 and the Chinese discourse parser achieves encouraging results. Our two parsers both rank second on the blind datasets. 1 Introduction A discourse relation between two segments of textual units expresses how they are logically connected to one another (cause or contrast), which is considered a crucial step for the ability to properly interpret or produce discourse. It can be of great benefit to many downstream natural language processing (NLP) applications and has attracted lots of research (Pitler and Nenkova, 2009; Lin et al., 2009; Pitler et al., 2009; Lan et al., 2013; Rutherford and Xue, 2014; Lin et al., 2014; Kong et al., 2014; Ji and Eisenstein, 2015; Fisher and Simmons, 2015; Braud and Denis, 2015; Zhang et al., 2015). Following the first edition in CoNLL-2015 (Xue et al., 2015), CoNLL-2016 (Xue et al., 2016) is the 2nd edition of the CoNLL Shared Task on Shallow Discourse Parsing, which contains following tasks: discourse parsing task and supplementary task (sense classification using gold standard argument pairs) in English and Chinese. To build an English parser, we follow (Wang and Lan, 2015b)’s work except for several modifications described late"
K16-2004,D09-1036,0,0.0492921,"iscourse parser achieves better performance than the best system of CoNLL2015 and the Chinese discourse parser achieves encouraging results. Our two parsers both rank second on the blind datasets. 1 Introduction A discourse relation between two segments of textual units expresses how they are logically connected to one another (cause or contrast), which is considered a crucial step for the ability to properly interpret or produce discourse. It can be of great benefit to many downstream natural language processing (NLP) applications and has attracted lots of research (Pitler and Nenkova, 2009; Lin et al., 2009; Pitler et al., 2009; Lan et al., 2013; Rutherford and Xue, 2014; Lin et al., 2014; Kong et al., 2014; Ji and Eisenstein, 2015; Fisher and Simmons, 2015; Braud and Denis, 2015; Zhang et al., 2015). Following the first edition in CoNLL-2015 (Xue et al., 2015), CoNLL-2016 (Xue et al., 2016) is the 2nd edition of the CoNLL Shared Task on Shallow Discourse Parsing, which contains following tasks: discourse parsing task and supplementary task (sense classification using gold standard argument pairs) in English and Chinese. To build an English parser, we follow (Wang and Lan, 2015b)’s work except f"
K16-2004,P09-2004,0,0.602883,"Xue, 2015). Our English discourse parser achieves better performance than the best system of CoNLL2015 and the Chinese discourse parser achieves encouraging results. Our two parsers both rank second on the blind datasets. 1 Introduction A discourse relation between two segments of textual units expresses how they are logically connected to one another (cause or contrast), which is considered a crucial step for the ability to properly interpret or produce discourse. It can be of great benefit to many downstream natural language processing (NLP) applications and has attracted lots of research (Pitler and Nenkova, 2009; Lin et al., 2009; Pitler et al., 2009; Lan et al., 2013; Rutherford and Xue, 2014; Lin et al., 2014; Kong et al., 2014; Ji and Eisenstein, 2015; Fisher and Simmons, 2015; Braud and Denis, 2015; Zhang et al., 2015). Following the first edition in CoNLL-2015 (Xue et al., 2015), CoNLL-2016 (Xue et al., 2016) is the 2nd edition of the CoNLL Shared Task on Shallow Discourse Parsing, which contains following tasks: discourse parsing task and supplementary task (sense classification using gold standard argument pairs) in English and Chinese. To build an English parser, we follow (Wang and Lan, 2015"
K16-2004,D15-1266,0,0.134929,"s. 1 Introduction A discourse relation between two segments of textual units expresses how they are logically connected to one another (cause or contrast), which is considered a crucial step for the ability to properly interpret or produce discourse. It can be of great benefit to many downstream natural language processing (NLP) applications and has attracted lots of research (Pitler and Nenkova, 2009; Lin et al., 2009; Pitler et al., 2009; Lan et al., 2013; Rutherford and Xue, 2014; Lin et al., 2014; Kong et al., 2014; Ji and Eisenstein, 2015; Fisher and Simmons, 2015; Braud and Denis, 2015; Zhang et al., 2015). Following the first edition in CoNLL-2015 (Xue et al., 2015), CoNLL-2016 (Xue et al., 2016) is the 2nd edition of the CoNLL Shared Task on Shallow Discourse Parsing, which contains following tasks: discourse parsing task and supplementary task (sense classification using gold standard argument pairs) in English and Chinese. To build an English parser, we follow (Wang and Lan, 2015b)’s work except for several modifications described later in section 2. In consideration of distinct linguistic and syntactic difference between English and Chinese, for Chinese parser, we design a new pipeline sys"
K16-2004,P12-1008,0,0.147052,"s, we also use a rule-based extractor to extract the arguments. We label the previous sentence of the connective as Arg1, and the text span between the connective and the beginning of the next sentence as Arg2. 3.1.5 Explicit Sense Classifier To build the Explicit sense classifier, we extract features from the connective C, its context and the parse tree of its sentence, which are listed in the following: C string, C + previous word of C, C + self category, C + left sibling category, C + right sibling category, C + the node context of parent category. 3.1.6 Explicit Punctuations According to (Zhou and Xue, 2012), the general annotation procedure for Chinese parser is to scan the text for finding punctuations, and to judge whether there is a discourse relation when a punctuation is encountered. If yes, annotators then characterize the relation and if not, they keep on scanning, whereas they identify Explicit connective firstly. Inspired by this annotation, we present this component to obtain the Explicit punctuation for each Explicit relation according to the connective and its two arguments using the following strategy: (1) If the two arguments are not embedded into each other, we use the punctuation"
K16-2004,P09-1077,0,0.370432,"hieves better performance than the best system of CoNLL2015 and the Chinese discourse parser achieves encouraging results. Our two parsers both rank second on the blind datasets. 1 Introduction A discourse relation between two segments of textual units expresses how they are logically connected to one another (cause or contrast), which is considered a crucial step for the ability to properly interpret or produce discourse. It can be of great benefit to many downstream natural language processing (NLP) applications and has attracted lots of research (Pitler and Nenkova, 2009; Lin et al., 2009; Pitler et al., 2009; Lan et al., 2013; Rutherford and Xue, 2014; Lin et al., 2014; Kong et al., 2014; Ji and Eisenstein, 2015; Fisher and Simmons, 2015; Braud and Denis, 2015; Zhang et al., 2015). Following the first edition in CoNLL-2015 (Xue et al., 2015), CoNLL-2016 (Xue et al., 2016) is the 2nd edition of the CoNLL Shared Task on Shallow Discourse Parsing, which contains following tasks: discourse parsing task and supplementary task (sense classification using gold standard argument pairs) in English and Chinese. To build an English parser, we follow (Wang and Lan, 2015b)’s work except for several modificati"
K16-2004,E14-1068,0,0.186661,"Missing"
K16-2004,K15-2002,1,0.906686,"an∗ Shanghai Key Laboratory of Multidimensional Information Processing Department of Computer Science and Technology, East China Normal University, Shanghai 200241, P. R. China 51141201062@ecnu.cn, mlan@cs.ecnu.edu.cn∗ Abstract This paper describes our two discourse parsers (i.e., English discourse parser and Chinese discourse parser) for submission to CoNLL-2016 shared task on Shallow Discourse Parsing. For English discourse parser, we build two separate argument extractors for single sentence (SS) case, and adopt a convolutional neural network for Non-Explicit sense classification based on (Wang and Lan, 2015b)’s work. As for Chinese discourse parser, we build a pipeline system following the annotation procedure of Chinese Discourse Treebank in (Zhou and Xue, 2015). Our English discourse parser achieves better performance than the best system of CoNLL2015 and the Chinese discourse parser achieves encouraging results. Our two parsers both rank second on the blind datasets. 1 Introduction A discourse relation between two segments of textual units expresses how they are logically connected to one another (cause or contrast), which is considered a crucial step for the ability to properly interpret or"
K17-3025,P11-1068,0,0.0670787,"Missing"
K17-3025,L16-1262,0,0.0669038,"Missing"
K17-3025,W06-2920,0,0.290895,"Missing"
K17-3025,L16-1680,0,0.0593987,"Missing"
K17-3025,P15-1032,0,0.0127438,"CoNLL 2006 and CoNLL 2007, the focus of the CoNLL 2017 UD Shared Task is learning syntactic dependency parsers on a universal syntactic annotation standard. This shared task requires participants to parse raw texts from different languages, which vary both in typology and training set size. The CoNLL 2017 UD Shared Task provided universal dependencies description from LREC 2016 (Nivre et al., 2016), two datasets, which are UD version 2.0 datasets (Nivre et al., 2017b) and this task test datasets (Nivre et al., 2017a), two baseline models, which are UDPipe (Straka et al., 2016) and SyntaxNet (Weiss et al., 2015), and the evaluation platform TIRA (Potthast et al., 2014). 2 System Description We implement a transition-based projective parser following Kiperwasser and Goldberg (2016). The system consists of a BiLSTM feature extractor and an MLP classifier. We describe their model and our implementation in the following sections in detail. 2.1 Arc-Hybrid System In this work, we use the arc-hybrid transition system (Kuhlmann et al., 2011). In the arc-hybrid system, a configuration c = (α, β, A) consists of a stack α, a buffer β, and a set of dependency arcs A. Given n words sentence s = w1 , · · · , wn ,"
K17-3025,D14-1082,0,\N,Missing
K17-3025,D07-1096,0,\N,Missing
K18-2025,L16-1680,0,0.0487129,"Missing"
P13-1047,W01-1605,0,0.530443,"ion and syntactic parsing. However, (Sporleder and Lascarides, 2008) showed that the training model built on a synthetic data set, like the work of (Marcu and Echihabi, 2002), may not be a good strategy since the linguistic dissimilarity between explicit and implicit data may hurt the performance of a model on natural data when being trained on synthetic data. 2.1.2 Supervised approaches This line of research work approaches this relation prediction problem by recasting it as a classification problem. (Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al., 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988). (Wellner et al., 2006) presented a study of discourse relation disambiguation on GraphBank (Wolf et al., 2005). Recently, (Pitler et al., 2009) (Lin et al., 2009) and (Wang et al., 2010) conducted discourse relation study on PDTB (Prasad et al., 2008) which has been widely used in this field. 3 Multi-task Learning for Discourse Relation Prediction 3.1 Motivation The idea of using multi-task learning for implicit discourse relation classification is motivated by the observations that we have made on implicit dis"
P13-1047,D10-1039,0,0.242117,"ive. This indicates that in certain degree they must be similar to each other. If it is true, the synthetic implicit relations are expected to be helpful for implicit discourse relation classification. Therefore, what we have to do is to find a way to train a model which has the capabilities to learn from their similarity and to ignore their dissimilarity as well. To solve it, we propose a multi-task learning method for implicit discourse relation classi2.1.3 Semi-supervised approaches Research work in this category exploited both labeled and unlabeled data for discourse relation prediction. (Hernault et al., 2010) presented a semi-supervised method based on the analysis of co-occurring features in labeled and unlabeled data. Very recently, (Hernault et al., 2011) introduced a semi-supervised work using structure learning method for discourse relation classification, which is quite relevant to our work. However, they performed discourse relation classification on both explicit and implicit data. And their work is different from our work in many aspects, such as, feature sets, auxiliary task, auxiliary data, class labels, learning framework, and so on. Furthermore, there is no explicit conclusion or evid"
P13-1047,P05-1001,0,0.0226771,"ion on a Japanese corpus. Furthermore, (Blair-Goldensohn, 2007) improved previous work with the use of parameter optimization, 1 According to the PDTB Annotation Manual (PDTBGroup, 2008), if the insertion of connective leads to “redundancy”, the relation is annotated as Alternative lexicalizations (AltLex), not implicit. 477 other related auxiliary tasks at the same time, using a shared representation. This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks. Many multi-task learning methods have been proposed in recent years, (Ando and Zhang, 2005a), (Argyriou et al., 2008), (Jebara, 2004), (Bonilla et al., 2008), (Evgeniou and Pontil, 2004), (Baxter, 2000), (Caruana, 1997), (Thrun, 1996). One group uses task relations as regularization terms in the objective function to be optimized. For example, in (Evgeniou and Pontil, 2004) the regularization terms make the parameters of models closer for similar tasks. Another group is proposed to find the common structure from data and then utilize the learned structure for multi-task learning (Argyriou et al., 2008) (Ando and Zhang, 2005b). topic segmentation and syntactic parsing. However, (Spo"
P13-1047,D09-1036,0,0.878225,"rse relation recognition. Since there is ambiguity of a word or phrase serving for discourse connective (i.e., the ambiguity between discourse and non-discourse usage or the ambiguity between two or more discourse relations if the word or phrase is used as a discourse connective), the synthetic implicit data would contain a lot of noises. Later, with the release of manually annotated corpus, such as Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), recent studies performed implicit discourse relation recognition on natural (i.e., genuine) implicit discourse data (Pitler et al., 2009) (Lin et al., 2009) (Wang et al., 2010) with the use of linguistically informed features and machine learning algorithms. (Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. They found some evidence that this behavior is largely independent of the classifiers used and seems to lie in the data itself (e.g., marked and unmarked examples may be too dissimilar linguistically and To overcome the shortage of labeled data for implicit discour"
P13-1047,P10-1073,0,0.104607,"ition. Since there is ambiguity of a word or phrase serving for discourse connective (i.e., the ambiguity between discourse and non-discourse usage or the ambiguity between two or more discourse relations if the word or phrase is used as a discourse connective), the synthetic implicit data would contain a lot of noises. Later, with the release of manually annotated corpus, such as Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), recent studies performed implicit discourse relation recognition on natural (i.e., genuine) implicit discourse data (Pitler et al., 2009) (Lin et al., 2009) (Wang et al., 2010) with the use of linguistically informed features and machine learning algorithms. (Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. They found some evidence that this behavior is largely independent of the classifiers used and seems to lie in the data itself (e.g., marked and unmarked examples may be too dissimilar linguistically and To overcome the shortage of labeled data for implicit discourse relation recognit"
P13-1047,P02-1047,0,0.72482,"elations if the word or phrase is used as a discourse connective), the synthetic implicit data would contain a lot of noises. Later, with the release of manually annotated corpus, such as Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), recent studies performed implicit discourse relation recognition on natural (i.e., genuine) implicit discourse data (Pitler et al., 2009) (Lin et al., 2009) (Wang et al., 2010) with the use of linguistically informed features and machine learning algorithms. (Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. They found some evidence that this behavior is largely independent of the classifiers used and seems to lie in the data itself (e.g., marked and unmarked examples may be too dissimilar linguistically and To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a prev"
P13-1047,W06-1317,0,0.0404269,"on a synthetic data set, like the work of (Marcu and Echihabi, 2002), may not be a good strategy since the linguistic dissimilarity between explicit and implicit data may hurt the performance of a model on natural data when being trained on synthetic data. 2.1.2 Supervised approaches This line of research work approaches this relation prediction problem by recasting it as a classification problem. (Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al., 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988). (Wellner et al., 2006) presented a study of discourse relation disambiguation on GraphBank (Wolf et al., 2005). Recently, (Pitler et al., 2009) (Lin et al., 2009) and (Wang et al., 2010) conducted discourse relation study on PDTB (Prasad et al., 2008) which has been widely used in this field. 3 Multi-task Learning for Discourse Relation Prediction 3.1 Motivation The idea of using multi-task learning for implicit discourse relation classification is motivated by the observations that we have made on implicit discourse relation. On one hand, since building a hand-annotated implicit discourse relation corpus is costly"
P13-1047,J09-3003,0,0.0338523,"ense, 3rd person singular present, etc.) in each argument, i.e., MD, VB, VBD, VBG, VBN, VBP, VBZ, are recorded as features, where we simply use the first verb in each argument as the main verb. Polarity: This feature records the number of positive, negated positive, negative and neutral words in both arguments and their cross product as well. For negated positives, we first locate the negated words in text span and then define the closely behind positive word as negated positive. The polarity of each word in arguments is derived from Multi-perspective Question Answering Opinion Corpus (MPQA) (Wilson et al., 2009). Modality: We examine six modal words (i.e., can, may, must, need, shall, will) including their various tenses or abbreviation forms in both arguments. This feature records the presence or absence of modal words in both arguments and their cross product. 4.3 Classifiers used multi-task learning We extract the above linguistically informed features from two synthetic implicit data sets (i.e., BLLIP and exp) to learn the auxiliary classifier and from the natural implicit data set (i.e., imp) to learn the main classifier. Under the ASO-based multitask learning framework, the model of main task l"
P13-1047,C08-2022,0,0.252859,"Missing"
P13-1047,P09-1077,0,0.55002,"es for implicit discourse relation recognition. Since there is ambiguity of a word or phrase serving for discourse connective (i.e., the ambiguity between discourse and non-discourse usage or the ambiguity between two or more discourse relations if the word or phrase is used as a discourse connective), the synthetic implicit data would contain a lot of noises. Later, with the release of manually annotated corpus, such as Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), recent studies performed implicit discourse relation recognition on natural (i.e., genuine) implicit discourse data (Pitler et al., 2009) (Lin et al., 2009) (Wang et al., 2010) with the use of linguistically informed features and machine learning algorithms. (Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. They found some evidence that this behavior is largely independent of the classifiers used and seems to lie in the data itself (e.g., marked and unmarked examples may be too dissimilar linguistically and To overcome the shortage of labeled data f"
P13-1047,C10-2172,1,0.79837,"ry recently, (Hernault et al., 2011) introduced a semi-supervised work using structure learning method for discourse relation classification, which is quite relevant to our work. However, they performed discourse relation classification on both explicit and implicit data. And their work is different from our work in many aspects, such as, feature sets, auxiliary task, auxiliary data, class labels, learning framework, and so on. Furthermore, there is no explicit conclusion or evidence in their work to address the two questions raised in Section 1. Unlike their previous work, our previous work (Zhou et al., 2010) presented a method to predict the missing connective based on a language model trained on an unannotated corpus. The predicted connective was then used as a feature to classify the implicit relation. 2.2 Multi-task learning Multi-task learning is a kind of machine learning method, which learns a main task together with 478 1) Fix (Θ, Vℓ ), and find m predictors fℓ that minimize the above joint empirical risk. fication, where the classification model seeks the shared part through jointly learning main task and multiple auxiliary tasks. As a result, the model can be optimized by the similar sha"
P13-1047,prasad-etal-2008-penn,0,0.454334,"oach and then these words or phrases were removed from these sentences. Thus the resulting sentences were used as synthetic training examples for implicit discourse relation recognition. Since there is ambiguity of a word or phrase serving for discourse connective (i.e., the ambiguity between discourse and non-discourse usage or the ambiguity between two or more discourse relations if the word or phrase is used as a discourse connective), the synthetic implicit data would contain a lot of noises. Later, with the release of manually annotated corpus, such as Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), recent studies performed implicit discourse relation recognition on natural (i.e., genuine) implicit discourse data (Pitler et al., 2009) (Lin et al., 2009) (Wang et al., 2010) with the use of linguistically informed features and machine learning algorithms. (Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. They found some evidence that this behavior is largely independent of the classifiers used and seems to lie"
P13-1047,N06-2034,0,0.0572543,"recognize four discourse relations, i.e., Contrast, Explanation-evidence, Condition and Elaboration. They first used unambiguous pattern to extract explicit discourse examples from raw corpus. Then they generated synthetic implicit discourse data by removing explicit discourse connectives from sentences extracted. In their work, they collected word pairs from synthetic data set as features and used machine learning method to classify implicit discourse relation. Based on this work, several researchers have extended the work to improve the performance of relation classification. For example, (Saito et al., 2006) showed that the use of phrasal patterns as additional features can help a word-pair based system for discourse relation prediction on a Japanese corpus. Furthermore, (Blair-Goldensohn, 2007) improved previous work with the use of parameter optimization, 1 According to the PDTB Annotation Manual (PDTBGroup, 2008), if the insertion of connective leads to “redundancy”, the relation is annotated as Alternative lexicalizations (AltLex), not implicit. 477 other related auxiliary tasks at the same time, using a shared representation. This often leads to a better model for the main task, because it a"
P13-1047,N03-1030,0,0.079189,"ure for multi-task learning (Argyriou et al., 2008) (Ando and Zhang, 2005b). topic segmentation and syntactic parsing. However, (Sporleder and Lascarides, 2008) showed that the training model built on a synthetic data set, like the work of (Marcu and Echihabi, 2002), may not be a good strategy since the linguistic dissimilarity between explicit and implicit data may hurt the performance of a model on natural data when being trained on synthetic data. 2.1.2 Supervised approaches This line of research work approaches this relation prediction problem by recasting it as a classification problem. (Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al., 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988). (Wellner et al., 2006) presented a study of discourse relation disambiguation on GraphBank (Wolf et al., 2005). Recently, (Pitler et al., 2009) (Lin et al., 2009) and (Wang et al., 2010) conducted discourse relation study on PDTB (Prasad et al., 2008) which has been widely used in this field. 3 Multi-task Learning for Discourse Relation Prediction 3.1 Motivation The idea of using multi-task learning for implicit discourse r"
P13-1097,P10-1041,0,0.0995501,"icitly contain the yes or no keywords, but rather provide context information to infer the yes or no answer (e.g. Q: Was she the best one on that old show? A: She was simply funny). Clearly, the sentiment words in IQAPs are the pivots to infer the yes or no answers. We show that sentiment similarity between such words (e.g., here the adjectives best and Funny) can be used effectively to infer the answers. The second application (SO prediction) aims to determine the sentiment orientation of individual words. Previous research utilized the semantic relations between words obtained from WordNet (Hassan and Radev, 2010) and semantic similarity measures (e.g. Turney and Littman, 2003) for this purpose. In this paper, we show that sentiment similarity between word pairs can be effectively utilized to compute SO of words. The contributions of this paper are follows: • We propose an effective approach to predict the sentiment similarity between word pairs through hidden emotions at the sense level, • We show the utility of sentiment similarity prediction in IQAP inference and SO prediction tasks, and • Our hidden emotional model can infer the type and number of hidden emotions in a corpus. 983 Proceedings of the"
P13-1097,C04-1200,0,0.208477,"Missing"
P13-1097,H05-1044,0,0.224123,"Missing"
P13-1097,P10-1018,0,0.408416,". SO based on the similarity function A(.,.) 6 6.1 Evaluation and Results Data and Settings We used the review dataset employed by Maas et al. (2011) as the development dataset that contains movie reviews with star rating from one star (most negative) to 10 stars (most positive). We exclude the ratings 5 and 6 that are more neutral. We used this dataset to compute all the input matrices in Table 2 as well as the enriched matrix. The development dataset contains 50k movie reviews and 90k vocabulary. We also used two datasets for the evaluation purpose: the MPQA (Wilson et al., 2005) and IQAPs (Marneffe et al., 2010) datasets. The MPQA dataset is used for SO prediction experiments, while the IQAP dataset is used for the IQAP experiments. We ignored the neutral words in MPQA dataset and used the remaining 4k opinion words. Also, the IQAPs dataset (Marneffe et al., 2010) contains 125 IQAPs and their corresponding yes or no labels as the ground truth. 6.2 Experimental Results To evaluate our PSSS model, we perform experiments on the SO prediction and IQAPs inference tasks. Here, we consider six emotions for both bridged and series models. We study the effect of emotion numbers in Section 7.1. Also, we set a"
P13-1097,P11-1015,0,\N,Missing
P19-1131,D17-1209,0,0.025975,"learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 2018). Graph Convolutional Network (GCN) is one of the typical variants of GNN (Bruna et al., 2013; Defferrard et al., 2016; Kipf and Welling, 2017). It has been successfully applied to many NLP tasks such as text classification (Yao et al., 2018), semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018) machine translation (Bastings et al., 2017) and knowledge base completion (Shang et al., 2018). We note that most previous applications of GCN focus on a single job, while the joint entity relation extraction consists of multiple sub-tasks. Investigating GCN in joint learning scenarios is the main topic of this work. A closely related work is (Christopoulou et al., 2018), which focuses on relation extraction with golden entities. Our work can be viewed as an end-to-end extension of their work. 6 Conclusion We propose a novel and concise joint model based on GCN to perform joint type inference for entity relation extraction task. Compar"
P19-1131,P11-1056,0,0.147557,"republican gurad]ORG-AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mech"
P19-1131,P18-2014,0,0.141109,"tifies a PHYS relation between “[units]PER ” and “[captial]GPE ”, while the “NN” does not find this relation even the entities are correct. However, both models do not identify the relation ART between “[units]PER ” and “[weapons]WEA ”. We think advanced improvement methods which use more powerful graph neural network might be helpful in this situation. 4.2 Golden Entity Results on ACE05 In order to compare with relation classification methods, we evaluate our models with golden entities on ACE05 corpus in Table 4. We use the same data split to compare with their model (Miwa and Bansal, 2016; Christopoulou et al., 2018). We do not tune hyperparameters extensively. For example, we use the same setting in both end-to-end and golden entity rather than tune parameters on each of them. The baseline systems are (Miwa and Bansal, 2016) and (Christopoulou et al., 2018). In general, our “NN” is competitive, comparing to the dependency tree-based state-of-the-art model (Miwa and Bansal, 2016). It shows that our CNN-based neural networks are able to extract more powerful features to help relation extraction task. After adding GCN, our GCN-based models achieve the better performance. This indicates that the proposed mod"
P19-1131,P82-1020,0,0.824741,"Missing"
P19-1131,P16-1087,0,0.0251959,"tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great"
P19-1131,P17-1085,0,0.448054,"e first extracted by an entity model, and then these extracted entities are used as the inputs of a relation model. Pipeline models often ignore interactions between the two models and they suffer from error propagation. Joint models integrate information between entities and relations into a single model with the joint training, and have achieved better ∗ Work done while this author was an intern at Microsoft Research Asia. results than the pipeline models. In this paper, we focus on joint models. More and more joint methods have been applied to this task. Among them, Miwa and Bansal (2016); Katiyar and Cardie (2017) identify the entity with a sequence labelling model, and identify the relation type with a multi-class classifier. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. In addition, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully investigated, including Li and Ji (2014); Zhang et al. (2017); Zheng et al. (2017); Wang et al. (2018). They jointly handle span detection and type inference to achieve more interactions. By inspecting the performance of"
P19-1131,P14-1038,0,0.0836562,"el as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 20"
P19-1131,P16-1200,0,0.055208,"AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint"
P19-1131,D17-1159,0,0.0261108,"ation between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 2018). Graph Convolutional Network (GCN) is one of the typical variants of GNN (Bruna et al., 2013; Defferrard et al., 2016; Kipf and Welling, 2017). It has been successfully applied to many NLP tasks such as text classification (Yao et al., 2018), semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018) machine translation (Bastings et al., 2017) and knowledge base completion (Shang et al., 2018). We note that most previous applications of GCN focus on a single job, while the joint entity relation extraction consists of multiple sub-tasks. Investigating GCN in joint learning scenarios is the main topic of this work. A closely related work is (Christopoulou et al., 2018), which focuses on relation extraction with golden entities. Our work can be viewed as an end-to-end extension of their work. 6 Conclusion We propose a novel and concise joint model ba"
P19-1131,P16-1105,0,0.140999,"two stages: entities are first extracted by an entity model, and then these extracted entities are used as the inputs of a relation model. Pipeline models often ignore interactions between the two models and they suffer from error propagation. Joint models integrate information between entities and relations into a single model with the joint training, and have achieved better ∗ Work done while this author was an intern at Microsoft Research Asia. results than the pipeline models. In this paper, we focus on joint models. More and more joint methods have been applied to this task. Among them, Miwa and Bansal (2016); Katiyar and Cardie (2017) identify the entity with a sequence labelling model, and identify the relation type with a multi-class classifier. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. In addition, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully investigated, including Li and Ji (2014); Zhang et al. (2017); Zheng et al. (2017); Wang et al. (2018). They jointly handle span detection and type inference to achieve more interactions. By in"
P19-1131,D09-1013,0,0.0833432,"♠ PER:♥♣♠ ORG:♥♣♠ [republican gurad]ORG-AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN usi"
P19-1131,D14-1162,0,0.0870553,"s work in Table 1. In general, our “GCN” achieves the best entity performance 84.2 percent comparing with existing joint models. For relation performance, our “GCN” significantly outperforms all joint models except for (Sun et al., 2018) which uses more complex joint decoder. Comparing with our basic neural network “NN”, our “GCN” has large improvement both on entities and relations. Those observations demonstrate the effectiveness of our “GCN” for capturing information on multiple entity types and relation types from a sentence. 5 Our word embeddings is initialized with 100dimensional glove (Pennington et al., 2014) word embeddings. The dimensionality of the hidden units and node embedding are set to 128. For all CNN in our network, the kernel sizes are 2 and 3, and the output channels are 25. 1365 Model P Entity R F P Relation R F L&J (2014) Zhang (2017) Sun (2018) 85.2 83.9 76.9 83.2 80.8 83.5 83.6 65.4 64.9 39.8 55.1 49.5 57.5 59.6 M&B (2016) K&C (2017) NN GCN 82.9 84.0 85.7 86.1 83.9 81.3 82.1 82.4 83.4 82.6 83.9 84.2 57.2 55.5 65.6 68.1 54.0 51.8 50.7 52.3 55.6 53.6 57.2 59.1 Table 1: Results on the ACE05 test data. Li and Ji (2014) Zhang et al. (2017) and Sun et al. (2018) are joint decoding algori"
P19-1131,P17-1113,0,0.190846,"es hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battagli"
P19-1131,D18-1249,1,0.863882,"oncise joint model to handle the joint type inference problem based on graph convolutional network (GCN). 1362 In this work, we decompose the joint entity relation extraction task into two parts, namely, entity span detection and entity relation type deduction. We first treat entity span detection as a sequence labelling task (Section 3.1), and then construct an entity-relation bipartite graph (Section 3.2) to perform joint type inference on entity nodes and relation nodes (Section 3.3). All submodels share parameters and are trained jointly. Different from existing joint learning algorithms (Sun et al., 2018; Zhang et al., 2017; Katiyar and Cardie, 2017; Miwa and Bansal, 2016), we propose a concise joint model to perform joint type inference on entities and relations based on GCNs. It considers interactions among multiple entity types and relation types simultaneously in a sentence. 3.1 Entity Span Detection To extract entity spans from a sentence (Figure 2), we adopt the BILOU sequence tagging scheme: B, I, L and O denote the begin, inside, last and outside of a target span, U denotes a single word span. For example, for a person (PER) entity “Patrick McDowell”, we assign B to “Patrick” and L to"
P19-1131,P13-1161,0,0.0695515,"el RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been re"
P19-1131,D17-1182,0,0.112899,"Missing"
P19-1131,D18-1244,0,0.123934,"information from their neighborhood over the bipartite graph. It helps us to concisely capture information among entities and relations. For example, in Figure 1, to predict the PER (“Toefting”), our joint model can pool the information of PER-SOC, PHYS, PER (“teammates”) and GPE (captital). To further utilize the structure of the graph, we also propose assigning different weights on graph edges. In particular, we introduce a binary relation classification task, which is to determine whether the two entities form a valid relation. Different from previous GCN-based models (Shang et al., 2018; Zhang et al., 2018), the adjacency matrix of graph is based on the output of binary relation classification, which makes the proposed adjacency matrix more explanatory. To summarize, the main contributions of this work are 1 bipartite graph in a more efficient and interpretable way. • We show that the proposed joint model on ACE05 achieves best entity performance, and is competitive with the state-of-the-art in relation performance. 2 Background of GCN In this section, we briefly describe graph convolutional networks (GCNs). Given a graph with n nodes, the goal of GCNs is to learn structureaware node representat"
P19-1237,P16-1231,0,0.102021,"Missing"
P19-1237,D16-1211,0,0.0986309,"Missing"
P19-1237,P18-1026,0,0.117604,"as encoder to construct graph-to-sequence learning. To our knowledge, we are the first to investigate GNNs for dependency parsing task. GNN Parser UAS LAS 91.64 92.12 92.00 86.47 83.83 91.28 86.82 90.81 87.94 88.57 89.11 88.94 89.13 88.28 89.90 89.85 81.96 81.16 88.93 83.73 88.91 84.82 86.33 84.44 86.62 86.24 Table 5: UAS and LAS F1 scores on 12 UD 2.2 test sets from CoNLL 2018 shared task. than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline parser. 4.2 Error Analysis Following McDonald and Nivre (2011); Ma et al. (2018), we characterize the errors made by the baseline biaffine parser and our GNN parser. Analysis shows that most of the gains come from the difficult cases (e.g. long sentences or longrange dependencies), which represents an encouraging sign of the proposed method’s benefits. Sentence Length. Figure 4 (a) shows the accuracy relative to sentence length. Our parser significantly improves the performance of the baseline parser on long sentence, but is slightly worse on short sentence (length ≤ 10). Dependency Length. Figure 4 (b) shows the precision and recall relative to dependency length. Our par"
P19-1237,K18-2005,0,0.0306511,"esults for a number of NLP tasks. By introducing context neighbors, the graph structure is added to the sequence modeling tool LSTMs, which improves The design of the node representation network is a key problem in neural graph-based parsers. Kiperwasser and Goldberg (2016b) use BiRNNs to obtain node representation with sentence-level information. To better characterize the direction of edge, Dozat and Manning (2017) feed BiRNNs outputs to two MLPs to distinguish word as head or dependent, and then construct a biaffine mapping for prediction. It also performs well on multilingual UD datasets (Che et al., 2018). Given a graph, a GNN can embed the node by recursively aggregating the node representations of its neighbors (Battaglia et al., 2018). Based on a biaffine mapping, GNNs can enhance the node representation by recursively integrating neighbors’ information. The message passing neural network (MPNN) (Gilmer et al., 2017) and the non-local neural network (NLNN) (Wang et al., 2018) are two popular GNN methods. Due to the convenience of self-attention in handling variable sentence length, we use a GAT-like network (Velikovi et al., 2018) belonging to NLNN. Then, we further explore its aggregating"
P19-1237,D14-1082,0,0.296643,"Missing"
P19-1237,D16-1238,0,0.175368,"Missing"
P19-1237,P15-1033,0,0.0404541,"Missing"
P19-1237,C96-1058,0,0.232364,"be T = (V, E), where the node set V contains all words and a synthetic root node 0, and the edge set E contains node pairs (i, j, r) which represents a dependency relation r between wi (the head) and wj (the dependent). Following the general graph-based dependency parsing framework, for every word pair (i, j), a function σ(i, j) assigns it a score which measures how possible is wi to be the head of wj . 2 We denote G to be the directed complete graph in which all nodes in V are connected with weights given by σ. The correct tree T is obtained from G using a decoder (e.g., dynamic programming (Eisner, 1996), maximum spanning tree (McDonald et al., 2005), and greedy algorithm (Zhang et al., 2017)). In neural-network-based models, the score function σ(i, j) usually relies on vector representations of nodes (words) i and j. How to get informative encodings of tree nodes is important for training the parser. Basically, we want the tree node encoder to explore both the surface form and deep structure of the sentence. To encode the surface form of s, we can use recurrent neural networks (Kiperwasser and Goldberg, 2016b). Specifically, we apply a bidirectional long short-term memory network (biLSTM, (H"
P19-1237,P82-1020,0,0.840467,"Missing"
P19-1237,Q16-1032,0,0.0829883,"out how possible they hold valid dependency relations, and then use decoders (e.g., greedy, maximum spanning tree) to generate a full parse tree from the scores. The score function is a key component in graph-based parses. Commonly, a neural network is assigned to learn low dimension vectors for words (i.e., nodes of parse trees), and the score function depends on vectors of the word pair (e.g., inner products). The main task of this paper is to explore effective encoding systems for dependency tree nodes. Two remarkable prior works on node representation are recurrent neural networks (RNNs) (Kiperwasser and Goldberg, 2016b) and biaffine mappings (Dozat and Manning, 2017). RNNs are powerful tools to collect sentence-level information, but the representations ignore features related to dependency structures. The biaffine mappings improve vanilla RNNs via a key observation: the representation of a word should be different regarding whether it is a head or a dependent (i.e., dependency tree edges are directional). Therefore, Dozat and Manning (2017) suggest distinguishing head and dependent vector of a word. Following this line of thought, it is natural to ask whether we can introduce more structured knowledge int"
P19-1237,P05-1012,0,0.148532,"ntains all words and a synthetic root node 0, and the edge set E contains node pairs (i, j, r) which represents a dependency relation r between wi (the head) and wj (the dependent). Following the general graph-based dependency parsing framework, for every word pair (i, j), a function σ(i, j) assigns it a score which measures how possible is wi to be the head of wj . 2 We denote G to be the directed complete graph in which all nodes in V are connected with weights given by σ. The correct tree T is obtained from G using a decoder (e.g., dynamic programming (Eisner, 1996), maximum spanning tree (McDonald et al., 2005), and greedy algorithm (Zhang et al., 2017)). In neural-network-based models, the score function σ(i, j) usually relies on vector representations of nodes (words) i and j. How to get informative encodings of tree nodes is important for training the parser. Basically, we want the tree node encoder to explore both the surface form and deep structure of the sentence. To encode the surface form of s, we can use recurrent neural networks (Kiperwasser and Goldberg, 2016b). Specifically, we apply a bidirectional long short-term memory network (biLSTM, (Hochreiter and Schmidhuber, 1997)). At each sent"
P19-1237,J11-1007,0,0.0623737,"ted GNNs (Beck et al., 2018) are used as encoder to construct graph-to-sequence learning. To our knowledge, we are the first to investigate GNNs for dependency parsing task. GNN Parser UAS LAS 91.64 92.12 92.00 86.47 83.83 91.28 86.82 90.81 87.94 88.57 89.11 88.94 89.13 88.28 89.90 89.85 81.96 81.16 88.93 83.73 88.91 84.82 86.33 84.44 86.62 86.24 Table 5: UAS and LAS F1 scores on 12 UD 2.2 test sets from CoNLL 2018 shared task. than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline parser. 4.2 Error Analysis Following McDonald and Nivre (2011); Ma et al. (2018), we characterize the errors made by the baseline biaffine parser and our GNN parser. Analysis shows that most of the gains come from the difficult cases (e.g. long sentences or longrange dependencies), which represents an encouraging sign of the proposed method’s benefits. Sentence Length. Figure 4 (a) shows the accuracy relative to sentence length. Our parser significantly improves the performance of the baseline parser on long sentence, but is slightly worse on short sentence (length ≤ 10). Dependency Length. Figure 4 (b) shows the precision and recall relative to dependen"
P19-1237,E06-1011,0,0.237524,"Missing"
P19-1237,P15-1031,0,0.0305741,"collects information c i from the end of s to the position i: → → ← ← ← where xi is the input of a LSTM cell which includes a randomly initialized word embedding e(wi ), a pre-trained word embedding e′ (wi ) from Glove (Pennington et al., 2014) and a trainable embedding of wi ’s part-of-speech tag e(posi ), ( ) xi = e(wi ) + e′ (wi ) ⊕ e(posi ). Then, a context-dependent node representation of word i is the concatenation of the two hidden vectors, → ← ci = c i ⊕ c i . (1) With the node representations, we can define the score function σ using a multi-layer perceptron σ(i, j) = MLP(ci ⊕ cj ) (Pei et al., 2015), or using a normalized bilinear function (A, b1 , b2 are parameters), σ(i, j)= Softmaxi (c⊺i Acj + b⊺1 ci + b⊺2 cj ) ≜ P (i|j), 1 Following the convention of (Dozat and Manning, 2017), we use lowercase italic letters for scalars and indices, lowercase bold letters for vectors, uppercase italic letters for matrices. → c i = LSTM(xi , c i−1 ; θ ), c i = LSTM(xi , c i+1 ; θ ), 2 (2) We will focus on the unlabelled parsing when illustrating our parsing models. For predicting labels, we use the identical setting in (Dozat and Manning, 2017). 2476 x4 MST x4 x3 x3 x2 x2 x1 x1 Decoder RNN Encoder GNN"
P19-1237,Q17-1008,0,0.0293289,", our parser does not improve performance. For nl, our parser improves 0.22 UAS, although LAS is slightly lower 7 The results should not compare with the shared task’s official results. 8 https://github.com/facebookresearch/ fastText 2481 UD 2.2 Baseline Parser UAS LAS bg ca cs de en es fr it nl no ro ru Avg. 91.69 92.08 91.22 86.11 83.72 90.95 86.46 90.70 87.72 88.27 89.07 88.67 88.89 88.25 89.75 88.73 81.86 81.07 88.65 83.15 88.80 84.85 85.97 84.18 86.29 85.96 performance on text classification, POS tagging and NER tasks (Zhang et al., 2018a). Based on syntactic dependency trees, DAG LSTMs (Peng et al., 2017) and GCNs (Zhang et al., 2018b) are used to improve the performance of relation extraction task. Based on the AMR semantic graph representation, graph state LSTMs (Song et al., 2018), GCNs (Bastings et al., 2017) and gated GNNs (Beck et al., 2018) are used as encoder to construct graph-to-sequence learning. To our knowledge, we are the first to investigate GNNs for dependency parsing task. GNN Parser UAS LAS 91.64 92.12 92.00 86.47 83.83 91.28 86.82 90.81 87.94 88.57 89.11 88.94 89.13 88.28 89.90 89.85 81.96 81.16 88.93 83.73 88.91 84.82 86.33 84.44 86.62 86.24 Table 5: UAS and LAS F1 scores o"
P19-1237,Q16-1023,0,0.203845,"out how possible they hold valid dependency relations, and then use decoders (e.g., greedy, maximum spanning tree) to generate a full parse tree from the scores. The score function is a key component in graph-based parses. Commonly, a neural network is assigned to learn low dimension vectors for words (i.e., nodes of parse trees), and the score function depends on vectors of the word pair (e.g., inner products). The main task of this paper is to explore effective encoding systems for dependency tree nodes. Two remarkable prior works on node representation are recurrent neural networks (RNNs) (Kiperwasser and Goldberg, 2016b) and biaffine mappings (Dozat and Manning, 2017). RNNs are powerful tools to collect sentence-level information, but the representations ignore features related to dependency structures. The biaffine mappings improve vanilla RNNs via a key observation: the representation of a word should be different regarding whether it is a head or a dependent (i.e., dependency tree edges are directional). Therefore, Dozat and Manning (2017) suggest distinguishing head and dependent vector of a word. Following this line of thought, it is natural to ask whether we can introduce more structured knowledge int"
P19-1237,D14-1162,0,0.089215,"(Kiperwasser and Goldberg, 2016b). Specifically, we apply a bidirectional long short-term memory network (biLSTM, (Hochreiter and Schmidhuber, 1997)). At each sentence position i, a forward LSTM chain (with → → parameter θ ) computes a hidden state vector c i by collecting information from the beginning of s to the current position i. Similarly, a backward ← ← LSTM chain ( θ ) collects information c i from the end of s to the position i: → → ← ← ← where xi is the input of a LSTM cell which includes a randomly initialized word embedding e(wi ), a pre-trained word embedding e′ (wi ) from Glove (Pennington et al., 2014) and a trainable embedding of wi ’s part-of-speech tag e(posi ), ( ) xi = e(wi ) + e′ (wi ) ⊕ e(posi ). Then, a context-dependent node representation of word i is the concatenation of the two hidden vectors, → ← ci = c i ⊕ c i . (1) With the node representations, we can define the score function σ using a multi-layer perceptron σ(i, j) = MLP(ci ⊕ cj ) (Pei et al., 2015), or using a normalized bilinear function (A, b1 , b2 are parameters), σ(i, j)= Softmaxi (c⊺i Acj + b⊺1 ci + b⊺2 cj ) ≜ P (i|j), 1 Following the convention of (Dozat and Manning, 2017), we use lowercase italic letters for scalar"
P19-1237,D16-1180,0,0.235838,"Missing"
P19-1237,P18-1150,0,0.0200597,"ts. 8 https://github.com/facebookresearch/ fastText 2481 UD 2.2 Baseline Parser UAS LAS bg ca cs de en es fr it nl no ro ru Avg. 91.69 92.08 91.22 86.11 83.72 90.95 86.46 90.70 87.72 88.27 89.07 88.67 88.89 88.25 89.75 88.73 81.86 81.07 88.65 83.15 88.80 84.85 85.97 84.18 86.29 85.96 performance on text classification, POS tagging and NER tasks (Zhang et al., 2018a). Based on syntactic dependency trees, DAG LSTMs (Peng et al., 2017) and GCNs (Zhang et al., 2018b) are used to improve the performance of relation extraction task. Based on the AMR semantic graph representation, graph state LSTMs (Song et al., 2018), GCNs (Bastings et al., 2017) and gated GNNs (Beck et al., 2018) are used as encoder to construct graph-to-sequence learning. To our knowledge, we are the first to investigate GNNs for dependency parsing task. GNN Parser UAS LAS 91.64 92.12 92.00 86.47 83.83 91.28 86.82 90.81 87.94 88.57 89.11 88.94 89.13 88.28 89.90 89.85 81.96 81.16 88.93 83.73 88.91 84.82 86.33 84.44 86.62 86.24 Table 5: UAS and LAS F1 scores on 12 UD 2.2 test sets from CoNLL 2018 shared task. than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline"
P19-1237,P18-1130,0,0.245361,"y tree node representations. Given a weighted graph, a GNN embeds a node by recursively aggregating node representations of its neighbours. For the parsing task, we build GNNs on weighted complete graphs which are readily obtained in graphbased parsers. The graphs could be fixed in prior or revised during the parsing process. By stacking multiple layers of GNNs, the representation of a node gradually collects various high-order information and bring global evidence into decoders’ final decision. Comparing with recent approximate highorder parsers (Kiperwasser and Goldberg, 2016b; Zheng, 2017; Ma et al., 2018), GNNs extract highorder information in a similar incremental manner: node representations of a GNN layer are computed based on outputs of former layers. However, the main difference is that, instead of extracting highorder features on only one intermediate tree, the update of GNN node vectors is able to inspect all intermediate trees. Thus, it may reduce the influence of a suboptimal intermediate parsing result. Comparing with the syntactic graph network 2475 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2475–2485 c Florence, Italy, July 28 - A"
P19-1237,L16-1680,0,0.0359525,"Missing"
P19-1237,D17-1159,0,0.062292,"anner: node representations of a GNN layer are computed based on outputs of former layers. However, the main difference is that, instead of extracting highorder features on only one intermediate tree, the update of GNN node vectors is able to inspect all intermediate trees. Thus, it may reduce the influence of a suboptimal intermediate parsing result. Comparing with the syntactic graph network 2475 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2475–2485 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2018b) which runs GNNs on dependency trees given by external parsers, we use GNNs to build the parsing model. And instead of using different weight matrices for outgoing and ingoing edges, our way of handling directional edges is based on the separation of head and dependent representations, which requires new protocols for updating nodes. We discuss various configurations of GNNs, including strategies on neighbour vector aggregations, synchronized or asynchronized node vector update and graphs with different edge weights. Experiments on the benchmark Eng"
P19-1237,P16-1218,0,0.0784109,"Missing"
P19-1237,P15-1032,0,0.0588995,"Missing"
P19-1237,K18-2001,0,0.0383385,"Missing"
P19-1237,E17-1063,0,0.0232998,"and the edge set E contains node pairs (i, j, r) which represents a dependency relation r between wi (the head) and wj (the dependent). Following the general graph-based dependency parsing framework, for every word pair (i, j), a function σ(i, j) assigns it a score which measures how possible is wi to be the head of wj . 2 We denote G to be the directed complete graph in which all nodes in V are connected with weights given by σ. The correct tree T is obtained from G using a decoder (e.g., dynamic programming (Eisner, 1996), maximum spanning tree (McDonald et al., 2005), and greedy algorithm (Zhang et al., 2017)). In neural-network-based models, the score function σ(i, j) usually relies on vector representations of nodes (words) i and j. How to get informative encodings of tree nodes is important for training the parser. Basically, we want the tree node encoder to explore both the surface form and deep structure of the sentence. To encode the surface form of s, we can use recurrent neural networks (Kiperwasser and Goldberg, 2016b). Specifically, we apply a bidirectional long short-term memory network (biLSTM, (Hochreiter and Schmidhuber, 1997)). At each sentence position i, a forward LSTM chain (with"
P19-1237,P18-1030,0,0.0517714,"as encoder to construct graph-to-sequence learning. To our knowledge, we are the first to investigate GNNs for dependency parsing task. GNN Parser UAS LAS 91.64 92.12 92.00 86.47 83.83 91.28 86.82 90.81 87.94 88.57 89.11 88.94 89.13 88.28 89.90 89.85 81.96 81.16 88.93 83.73 88.91 84.82 86.33 84.44 86.62 86.24 Table 5: UAS and LAS F1 scores on 12 UD 2.2 test sets from CoNLL 2018 shared task. than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline parser. 4.2 Error Analysis Following McDonald and Nivre (2011); Ma et al. (2018), we characterize the errors made by the baseline biaffine parser and our GNN parser. Analysis shows that most of the gains come from the difficult cases (e.g. long sentences or longrange dependencies), which represents an encouraging sign of the proposed method’s benefits. Sentence Length. Figure 4 (a) shows the accuracy relative to sentence length. Our parser significantly improves the performance of the baseline parser on long sentence, but is slightly worse on short sentence (length ≤ 10). Dependency Length. Figure 4 (b) shows the precision and recall relative to dependency length. Our par"
P19-1237,D18-1244,0,0.13902,"ed based on outputs of former layers. However, the main difference is that, instead of extracting highorder features on only one intermediate tree, the update of GNN node vectors is able to inspect all intermediate trees. Thus, it may reduce the influence of a suboptimal intermediate parsing result. Comparing with the syntactic graph network 2475 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2475–2485 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2018b) which runs GNNs on dependency trees given by external parsers, we use GNNs to build the parsing model. And instead of using different weight matrices for outgoing and ingoing edges, our way of handling directional edges is based on the separation of head and dependent representations, which requires new protocols for updating nodes. We discuss various configurations of GNNs, including strategies on neighbour vector aggregations, synchronized or asynchronized node vector update and graphs with different edge weights. Experiments on the benchmark English Penn Treebank 3.0 and CoNLL2018 multil"
P19-1237,D17-1173,0,0.26325,"ing dependency tree node representations. Given a weighted graph, a GNN embeds a node by recursively aggregating node representations of its neighbours. For the parsing task, we build GNNs on weighted complete graphs which are readily obtained in graphbased parsers. The graphs could be fixed in prior or revised during the parsing process. By stacking multiple layers of GNNs, the representation of a node gradually collects various high-order information and bring global evidence into decoders’ final decision. Comparing with recent approximate highorder parsers (Kiperwasser and Goldberg, 2016b; Zheng, 2017; Ma et al., 2018), GNNs extract highorder information in a similar incremental manner: node representations of a GNN layer are computed based on outputs of former layers. However, the main difference is that, instead of extracting highorder features on only one intermediate tree, the update of GNN node vectors is able to inspect all intermediate trees. Thus, it may reduce the influence of a suboptimal intermediate parsing result. Comparing with the syntactic graph network 2475 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2475–2485 c Florence,"
P19-1514,Q16-1026,0,0.0889731,"Missing"
P19-1514,P82-1020,0,0.766664,"Missing"
P19-1514,P16-2018,0,0.464073,"Missing"
P19-1514,N16-1030,0,0.114235,"Missing"
P19-1514,P16-1101,0,0.0494643,"Missing"
P19-1514,D11-1144,0,0.391949,"Missing"
S10-1050,S10-1006,0,0.0938248,"Missing"
S10-1050,P08-1027,0,0.0146298,"uite different in definition of relations and granularities of various applications. That is, there is little agreement on relation inventories. SemEval 2010 Task 8 (Hendrickx et al., 2008) provides a new standard benchmark for semantic relation classification to a wider community, where it defines 9 relations including C AUSE -E FFECT, C OMPONENT-W HOLE, C ONTENT-C ONTAINER, E NTITY-D ESTINATION, E NTITY-O RIGIN, I NSTRUMENT-AGENCY, M EMBER -C OLLECTION, M ESSAGE -T OPIC, 2. Most previous work at SemEval 2007 Task 4 leveraged on external theauri or corpora (whether unannotated or annotated) (Davidov and Rappoport, 2008), (Costello, 2007), (Beamer et al., 2007) and (Nakov and Hearst, 2008) that make the task adaption to different domains and languages more difficult, since they would not have such manually classified or annotated corpus available. From a practical point of view, our system would make use of less resources. 3. Most previous work at Semeval 2007 Task 4 constructed several local classifiers on different algorithms or different feature subsets, one for each relation (Hendrickx et al., 2007) and (Davidov and Rappoport, 2008). Our approach is to build a global classifier for all relations in practi"
S10-1050,S07-1081,0,0.0168329,"relations and granularities of various applications. That is, there is little agreement on relation inventories. SemEval 2010 Task 8 (Hendrickx et al., 2008) provides a new standard benchmark for semantic relation classification to a wider community, where it defines 9 relations including C AUSE -E FFECT, C OMPONENT-W HOLE, C ONTENT-C ONTAINER, E NTITY-D ESTINATION, E NTITY-O RIGIN, I NSTRUMENT-AGENCY, M EMBER -C OLLECTION, M ESSAGE -T OPIC, 2. Most previous work at SemEval 2007 Task 4 leveraged on external theauri or corpora (whether unannotated or annotated) (Davidov and Rappoport, 2008), (Costello, 2007), (Beamer et al., 2007) and (Nakov and Hearst, 2008) that make the task adaption to different domains and languages more difficult, since they would not have such manually classified or annotated corpus available. From a practical point of view, our system would make use of less resources. 3. Most previous work at Semeval 2007 Task 4 constructed several local classifiers on different algorithms or different feature subsets, one for each relation (Hendrickx et al., 2007) and (Davidov and Rappoport, 2008). Our approach is to build a global classifier for all relations in practical NLP settings."
S10-1050,P08-1052,0,0.0293068,"ications. That is, there is little agreement on relation inventories. SemEval 2010 Task 8 (Hendrickx et al., 2008) provides a new standard benchmark for semantic relation classification to a wider community, where it defines 9 relations including C AUSE -E FFECT, C OMPONENT-W HOLE, C ONTENT-C ONTAINER, E NTITY-D ESTINATION, E NTITY-O RIGIN, I NSTRUMENT-AGENCY, M EMBER -C OLLECTION, M ESSAGE -T OPIC, 2. Most previous work at SemEval 2007 Task 4 leveraged on external theauri or corpora (whether unannotated or annotated) (Davidov and Rappoport, 2008), (Costello, 2007), (Beamer et al., 2007) and (Nakov and Hearst, 2008) that make the task adaption to different domains and languages more difficult, since they would not have such manually classified or annotated corpus available. From a practical point of view, our system would make use of less resources. 3. Most previous work at Semeval 2007 Task 4 constructed several local classifiers on different algorithms or different feature subsets, one for each relation (Hendrickx et al., 2007) and (Davidov and Rappoport, 2008). Our approach is to build a global classifier for all relations in practical NLP settings. 226 Proceedings of the 5th International Workshop on"
S10-1050,S07-1039,0,\N,Missing
S10-1050,S07-1085,0,\N,Missing
S12-1084,C10-2048,0,\N,Missing
S12-1084,P94-1019,0,\N,Missing
S13-2021,S12-1065,0,0.0347162,"Missing"
S13-2021,P12-1091,0,0.0391864,"e is a measure of similarity between two strings at the word level. In total, we can get 11 features in this feature set. 2.3 Sematic Similarity features Almost every previous work used the surface texts or exploited the meanings of words in the dictionary to calculate the similarity of two sentences rather than the actual meaning in the sentence. In this feature set (SS), we introduce a latent model to model the semantic representations of sentences since latent models are capable of capturing the contextual meaning of words in sentences. We used weighted textual matrix factorization (WTMF) (Guo and Diab, 2012) to model the semantics of the sentences. The model factorizes the original term-sentence matrix X into two matrices such that T Q , where P Xi,j ≈ P∗,i ∗,j ∗,i is a latent semantics vector profile for word wi and Q∗,j is the vector profile that represents the sentence sj . The weight matrix W is introduced in the optimization process in order to model the missing words at the right level of emphasis. We propose three similarity measures according to different strategies: wtw: word-to-word  based similarity defined as sim(A, B) = lg wi ∈A Wwi ·maxwj ∈B (P∗,i ,P∗,j )  . w ∈A Wwi i wts: word-t"
S13-2021,P06-1114,0,0.014831,"T to H; (2) backward: unidirectional entailment from H to T; (3) bidirectional: the two fragments entail each other (i.e., semantic equivalence); (4) non-entailment: there is no entailment between T and H. During the last decades, many researchers and communities have paid a lot of attention to resolve the TE detection (e.g., seven times of the Recognizing Textual Entailment Challenge, i.e., from RTE1 to RET7, have been held) since identifying the relationship between two sentences is at the core of many NLP applications, such as text summarization (Lloret et al., 2008) or question answering (Harabagiu and Hickl, 2006). For example, in text summarization, a redundant sentence should be omitted from the summary if this sentence can be entailed from other expressions in the summary. CLTE extends those tasks with lingual dimensionality, where more than one language is involved. Although it is a relatively new task, a basic solution has been provided in (Mehdad et al., 2010b), which brings the problem back to monolingual scenario using MT to translate H into the language of T. The promising performance indicates the potentialities of such a simple approach which integrates MT and monolingual TE algorithms (Cast"
S13-2021,S12-1102,0,0.345314,"e, in text summarization, a redundant sentence should be omitted from the summary if this sentence can be entailed from other expressions in the summary. CLTE extends those tasks with lingual dimensionality, where more than one language is involved. Although it is a relatively new task, a basic solution has been provided in (Mehdad et al., 2010b), which brings the problem back to monolingual scenario using MT to translate H into the language of T. The promising performance indicates the potentialities of such a simple approach which integrates MT and monolingual TE algorithms (Castillo, 2011; Jimenez et al., 2012; Mehdad et al., 2010a). In this work, we regard CLTE as a multiclass classification problem, in which multiple feature types are used in conjunction with a multiclass SVM classifier. Specifically, our approach can be divided into three steps. Firstly, following (Espl`a-Gomis et al., 2012; Meng et al., 2012), we use MT to 118 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 118–123, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics bridge the gap of lan"
S13-2021,W07-1407,0,0.057356,"er of non-repeated elements in this set. Once we view the text as a set of words, A − B means the set of words found in A but not in B, A ∪ B means the set of words found in either A or B and A∩ B means the set of shared words found in both A and B. Given a pair of texts, i.e., &lt;T,H&gt;, which are in different languages, we use MT to translate one of them to make them in the same language. Thus, we can get two pairs of texts, i.e., &lt;T t ,H&gt; and &lt;T,H t &gt;. We apply the above eight length measures to the two pairs, resulting in a total of 16 features. 2.2 Surface Text Similarity features Following (Malakasiotis and Androutsopoulos, 2007), the surface text similarity (STS) feature set contains nine similarity measures: 119 Edit distance: This is the minimum number of operations needed to transform A to B. We define an operation as an insertion, deletion or substitution of a word. Jaro-Winker distance: Following (Winkler and others, 1999), the Jaro-Winkler distance is a measure of similarity between two strings at the word level. In total, we can get 11 features in this feature set. 2.3 Sematic Similarity features Almost every previous work used the surface texts or exploited the meanings of words in the dictionary to calculate"
S13-2021,P09-3004,0,0.0185554,"l`a-Gomis et al., 2012; Meng et al., 2012), we use MT to 118 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 118–123, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics bridge the gap of language differences between T and H. Secondly, we perform a preprocessing procedure to maximize the similarity of the two text fragments so as to make a more accurate calculation of surface text similarity measures. Besides several features described in previous work (Malakasiotis, 2009; Espl`a-Gomis et al., 2012), we also propose several novel features regarding to sentence difference and semantic similarity. Finally, all these features are combined together and serves as input of a multiclass SVM classifier. After analyzing of the results obtained in preliminary experiments, we also cast this problem as a hierarchical classification problem. The remainder of the paper is organized as follows. Section 2 describes different features used in our systems. Section 3 presents the system settings including the datasets and preprocessing. Section 4 shows the results of different s"
S13-2021,N10-1146,0,0.0657784,"build portable and effective systems across languages using MT and multiple effective features; (2) our systems achieve the best results among the participants on two test datasets, i.e., FRA-ENG and DEU-ENG. 1 Introduction The Cross-lingual Textual Entailment (CLTE) task in SemEval 2013 consists in detecting the entailment relationship between two topic-related text fragments (usually called T(ext) and H(ypothesis)) in different languages, which is a cross-lingual extension of TE task in (Dagan and Glickman, 2004). We say T entails H if the meaning of H can be inferred from the meaning of T. Mehdad et al. (2010b) firstly proposed this problem within a new challenging application scenario, i.e., content synchronization. In consideration of the directionality, the task needs to assign one of the following entailment judgments to a pair of sentences (1) forward: unidirectional entailment from T to H; (2) backward: unidirectional entailment from H to T; (3) bidirectional: the two fragments entail each other (i.e., semantic equivalence); (4) non-entailment: there is no entailment between T and H. During the last decades, many researchers and communities have paid a lot of attention to resolve the TE dete"
S13-2021,N10-1045,0,0.126603,"build portable and effective systems across languages using MT and multiple effective features; (2) our systems achieve the best results among the participants on two test datasets, i.e., FRA-ENG and DEU-ENG. 1 Introduction The Cross-lingual Textual Entailment (CLTE) task in SemEval 2013 consists in detecting the entailment relationship between two topic-related text fragments (usually called T(ext) and H(ypothesis)) in different languages, which is a cross-lingual extension of TE task in (Dagan and Glickman, 2004). We say T entails H if the meaning of H can be inferred from the meaning of T. Mehdad et al. (2010b) firstly proposed this problem within a new challenging application scenario, i.e., content synchronization. In consideration of the directionality, the task needs to assign one of the following entailment judgments to a pair of sentences (1) forward: unidirectional entailment from T to H; (2) backward: unidirectional entailment from H to T; (3) bidirectional: the two fragments entail each other (i.e., semantic equivalence); (4) non-entailment: there is no entailment between T and H. During the last decades, many researchers and communities have paid a lot of attention to resolve the TE dete"
S13-2021,S12-1108,0,0.0185083,"en provided in (Mehdad et al., 2010b), which brings the problem back to monolingual scenario using MT to translate H into the language of T. The promising performance indicates the potentialities of such a simple approach which integrates MT and monolingual TE algorithms (Castillo, 2011; Jimenez et al., 2012; Mehdad et al., 2010a). In this work, we regard CLTE as a multiclass classification problem, in which multiple feature types are used in conjunction with a multiclass SVM classifier. Specifically, our approach can be divided into three steps. Firstly, following (Espl`a-Gomis et al., 2012; Meng et al., 2012), we use MT to 118 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 118–123, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics bridge the gap of language differences between T and H. Secondly, we perform a preprocessing procedure to maximize the similarity of the two text fragments so as to make a more accurate calculation of surface text similarity measures. Besides several features described in previous work (Malakasiotis, 2009; Espl`a-Gomis et al., 2"
S13-2021,S13-2005,0,0.0902437,"gative words. If the numbers are the same, then we set the feature to 1, otherwise -1. Also, we check whether one sentence entails the other using only the named entity information. We consider four categories of named entities, i.e., person, organization, location, number, which are recognized by using the Stanford NER toolkit. We set the feature to 1 if the named entities in one sentence are found in the other sentence, otherwise -1. As a result, this feature set contains 9 features. 3 Experimental Setting We evaluated our approach using the data sets provided in the task 8 of SemEval 2013 (Negri et al., 2013). The data sets consist of a collection of 1500 text fragment pairs (1000 for training consisting of training and test set in SemEval 2012 and 500 for test) in each language pair. Four different language pairs are provided: German-English, French-English, Italian-English and Spanish-English. See (Negri et al., 2013) for more detailed description. 3.1 Preprocess We performed the following text preprocessing. Firstly, we employed the state-of-the-art Statistical Machine Translator, i.e., Google translator, to translate each pair of texts &lt;T,H&gt; into &lt;T t ,H&gt; and &lt;T,H t &gt;, thus they were in the sa"
S13-2021,N10-4008,0,\N,Missing
S14-2041,J09-3003,0,0.0491989,"rested category seeds are given and used to extract more categorize aspect terms. Since sentiments always adhere to entities, several researchers worked on polarity classification of entity. For example, (Godbole et al., 2007) proposed a system that assigned scores representing positive or negative opinion to each distinct entity in the corpus. (Kim et al., 2013) presented a hierarchical aspect sentiment model to classify the polarity of aspect terms from unlabeled online reviews. Moreover, some sentiment lexicons, such as SentiWordNet (Baccianella et al., 2010) and MPQA Subjectivity Lexicon (Wilson et al., 2009), have been used to generate sentiment score features (Zhu et al., 2013). Introduction Recently, sentiment analysis has attracted a lot of attention from researchers. Most previous work attempted to detect overall sentiment polarity on a text span, such as document, paragraph and sentence. Since sentiments expressed in text always adhere to objects, it is much meaningful to identify the sentiment target and its orientation, which helps user gain precise sentiment insights on specific sentiment target. The aspect based sentiment analysis (ABSA) task (Task 4) (Pontiki et al., 2014) in SemEval 20"
S14-2041,S13-2067,1,0.888596,"Missing"
S14-2041,I11-1163,0,0.0240125,"ent polarity classification (Categorylevel tasks). For aspect term extraction, we present three methods, i.e., noun phrase (NP) extraction, Named Entity Recognition (NER) and a combination of NP and NER method. For aspect sentiment classification, we extracted several features, i.e., topic features, sentiment lexicon features, and adopted a Maximum Entropy classifier. Our submissions rank above average. 1 Generally, there are three methods to extract aspect terms: unsupervised learning method based on word frequency ((Ku et al., 2006), (Long et al., 2010)), supervised machine learning method (Kovelamudi et al., 2011) and semi-supervised learning method (Mukherjee and Liu, 2012) where only several user interested category seeds are given and used to extract more categorize aspect terms. Since sentiments always adhere to entities, several researchers worked on polarity classification of entity. For example, (Godbole et al., 2007) proposed a system that assigned scores representing positive or negative opinion to each distinct entity in the corpus. (Kim et al., 2013) presented a hierarchical aspect sentiment model to classify the polarity of aspect terms from unlabeled online reviews. Moreover, some sentimen"
S14-2041,C10-2088,0,0.0291968,"ks), aspect category detection and aspect category sentiment polarity classification (Categorylevel tasks). For aspect term extraction, we present three methods, i.e., noun phrase (NP) extraction, Named Entity Recognition (NER) and a combination of NP and NER method. For aspect sentiment classification, we extracted several features, i.e., topic features, sentiment lexicon features, and adopted a Maximum Entropy classifier. Our submissions rank above average. 1 Generally, there are three methods to extract aspect terms: unsupervised learning method based on word frequency ((Ku et al., 2006), (Long et al., 2010)), supervised machine learning method (Kovelamudi et al., 2011) and semi-supervised learning method (Mukherjee and Liu, 2012) where only several user interested category seeds are given and used to extract more categorize aspect terms. Since sentiments always adhere to entities, several researchers worked on polarity classification of entity. For example, (Godbole et al., 2007) proposed a system that assigned scores representing positive or negative opinion to each distinct entity in the corpus. (Kim et al., 2013) presented a hierarchical aspect sentiment model to classify the polarity of aspe"
S14-2041,P12-1036,0,0.0346634,"term extraction, we present three methods, i.e., noun phrase (NP) extraction, Named Entity Recognition (NER) and a combination of NP and NER method. For aspect sentiment classification, we extracted several features, i.e., topic features, sentiment lexicon features, and adopted a Maximum Entropy classifier. Our submissions rank above average. 1 Generally, there are three methods to extract aspect terms: unsupervised learning method based on word frequency ((Ku et al., 2006), (Long et al., 2010)), supervised machine learning method (Kovelamudi et al., 2011) and semi-supervised learning method (Mukherjee and Liu, 2012) where only several user interested category seeds are given and used to extract more categorize aspect terms. Since sentiments always adhere to entities, several researchers worked on polarity classification of entity. For example, (Godbole et al., 2007) proposed a system that assigned scores representing positive or negative opinion to each distinct entity in the corpus. (Kim et al., 2013) presented a hierarchical aspect sentiment model to classify the polarity of aspect terms from unlabeled online reviews. Moreover, some sentiment lexicons, such as SentiWordNet (Baccianella et al., 2010) an"
S14-2041,S14-2004,0,0.056142,"ivity Lexicon (Wilson et al., 2009), have been used to generate sentiment score features (Zhu et al., 2013). Introduction Recently, sentiment analysis has attracted a lot of attention from researchers. Most previous work attempted to detect overall sentiment polarity on a text span, such as document, paragraph and sentence. Since sentiments expressed in text always adhere to objects, it is much meaningful to identify the sentiment target and its orientation, which helps user gain precise sentiment insights on specific sentiment target. The aspect based sentiment analysis (ABSA) task (Task 4) (Pontiki et al., 2014) in SemEval 2014 is to extract aspect terms, determine its semantic category, and then to detect the sentiment orientation of the extracted aspect terms and its category. Specifically, it consists of 4 subtasks. The aspect term extraction (ATE) aims to extract the aspect terms from the sentences in two givThe rest of this paper is organized as follows. From Section 2 to Section 5, we describe our approaches to the Aspect Term Extraction task, the Aspect Category detection task, the Aspect Term Polarity task and the Aspect Category Polarity task respectively. Section 6 provides the conclusion."
S14-2041,baccianella-etal-2010-sentiwordnet,0,\N,Missing
S14-2042,W12-5500,0,0.0964378,"Missing"
S14-2042,S13-2052,0,0.443236,"vestors to discover product trends, identify customer preferences and categorize users by analyzing these tweets (Becker et al., 2013). The task of sentiment analysis in twitter in SemEval 2014 (Sara et al., 2014) aims to classify whether a tweet’s sentiment is positive, negative or neutral at expression level or message level. The expression-level subtask (i.e., subtask A) is to determine the sentiment of a marked instance of a word or phrase in the context of a given message, while the message-level subtask (i.e., subtask B) aims to determine the sentiment of a whole message. Previous work (Nakov et al., 2013) showed that message-level sentiment classification is more difficult than that of expression-level (i.e., 0.690 vs 0.889 in terms of F-measure) since a message may be composed of inconsistent sentiments. To date, lots of approaches have been proposed for conventional blogging sentiment analysis and a very broad overview is presented in (Pang and Lee, 2008). Inspired by that, many features used in microblogging mining are adopted from traditional blogging sentiment analysis task. For example, n-grams at the character or word level, part-of-speech tags, negations, sentiment lexicons were used i"
S14-2042,W11-0705,0,0.0323484,"el sentiment classification is more difficult than that of expression-level (i.e., 0.690 vs 0.889 in terms of F-measure) since a message may be composed of inconsistent sentiments. To date, lots of approaches have been proposed for conventional blogging sentiment analysis and a very broad overview is presented in (Pang and Lee, 2008). Inspired by that, many features used in microblogging mining are adopted from traditional blogging sentiment analysis task. For example, n-grams at the character or word level, part-of-speech tags, negations, sentiment lexicons were used in most of current work (Agarwal et al., 2011; Barbosa and Feng, 2010; Zhu et al., 2013; Mohammad et al., 2013; K¨okciyan et al., 2013). They found that n-grams are still effective in spite of the short length nature of microblogging and the distributions of different POS tags in tweets with different polarities are highly different (Pak and Paroubek, 2010). Compared with formal blog texts, tweets often contain many informal writings including slangs, emoticons, creMicroblogging websites (such as Twitter, Facebook) are rich sources of data for opinion mining and sentiment analysis. In this paper, we describe our approaches used for senti"
S14-2042,baccianella-etal-2010-sentiwordnet,0,0.0138879,"moticons. To identify the polarities of emoticons, we collected 36 positive emoticons and 33 negative emoticons from the Internet. Hashtag A hashtag is a short phrase that concatenates more than one words together without white spaces and users usually use hashtags to label the subject topic of a tweet, e.g., #toobad, #ihateschool, #NewGlee. Since a hashtag may contain a strong sentiment orientation, we first used the Viterbi algorithm (Berardi et al., 2011) to split hashtags and then calculated the sentiment scores of hashtags using the hashtag and emoticon lexicon in NRC. SentiWordNet(SWN) (Baccianella et al., 2010). This sentiment lexicon contains about 117 thousand items and each item corresponds to a synset of WordNet. Three sentiment scores: positivity, negativity, objectivity are provided and the sum of these three scores is always 1, for example, living#a#3, positivity: 0.5, negativity: 0.125, objectivity: 0.375. In experiment we used the most common sense of a word. NRC (Mohammad et al., 2013). Mohammad et al. collected two sets of tweets and each tweet contains the seed hashtags or emoticons and then they labeled the sentiment orientation for each tweet according to its hashtags or emoticons. The"
S14-2042,pak-paroubek-2010-twitter,0,0.299391,"in (Pang and Lee, 2008). Inspired by that, many features used in microblogging mining are adopted from traditional blogging sentiment analysis task. For example, n-grams at the character or word level, part-of-speech tags, negations, sentiment lexicons were used in most of current work (Agarwal et al., 2011; Barbosa and Feng, 2010; Zhu et al., 2013; Mohammad et al., 2013; K¨okciyan et al., 2013). They found that n-grams are still effective in spite of the short length nature of microblogging and the distributions of different POS tags in tweets with different polarities are highly different (Pak and Paroubek, 2010). Compared with formal blog texts, tweets often contain many informal writings including slangs, emoticons, creMicroblogging websites (such as Twitter, Facebook) are rich sources of data for opinion mining and sentiment analysis. In this paper, we describe our approaches used for sentiment analysis in twitter (task 9) organized in SemEval 2014. This task tries to determine whether the sentiment orientations conveyed by the whole tweets or pieces of tweets are positive, negative or neutral. To solve this problem, we extracted several simple and basic features considering the following aspects:"
S14-2042,C10-2005,0,0.0488064,"ation is more difficult than that of expression-level (i.e., 0.690 vs 0.889 in terms of F-measure) since a message may be composed of inconsistent sentiments. To date, lots of approaches have been proposed for conventional blogging sentiment analysis and a very broad overview is presented in (Pang and Lee, 2008). Inspired by that, many features used in microblogging mining are adopted from traditional blogging sentiment analysis task. For example, n-grams at the character or word level, part-of-speech tags, negations, sentiment lexicons were used in most of current work (Agarwal et al., 2011; Barbosa and Feng, 2010; Zhu et al., 2013; Mohammad et al., 2013; K¨okciyan et al., 2013). They found that n-grams are still effective in spite of the short length nature of microblogging and the distributions of different POS tags in tweets with different polarities are highly different (Pak and Paroubek, 2010). Compared with formal blog texts, tweets often contain many informal writings including slangs, emoticons, creMicroblogging websites (such as Twitter, Facebook) are rich sources of data for opinion mining and sentiment analysis. In this paper, we describe our approaches used for sentiment analysis in twitter"
S14-2042,S13-2055,0,0.0308849,"Missing"
S14-2042,S14-2009,0,0.0271155,"vel Sentiment Orientation Classification in Twitter Using Multiple Effective Features Jiang Zhao† , Man Lan∗ , Tian Tian Zhu† Department of Computer Science and Technology East China Normal University † 51121201042,51111201046@ecnu.cn; ∗ mlan@cs.ecnu.edu.cn Abstract SemEval 2013 (Nakov et al., 2013). It will benefit lots of real applications such as simultaneously businesses, media outlets, and help investors to discover product trends, identify customer preferences and categorize users by analyzing these tweets (Becker et al., 2013). The task of sentiment analysis in twitter in SemEval 2014 (Sara et al., 2014) aims to classify whether a tweet’s sentiment is positive, negative or neutral at expression level or message level. The expression-level subtask (i.e., subtask A) is to determine the sentiment of a marked instance of a word or phrase in the context of a given message, while the message-level subtask (i.e., subtask B) aims to determine the sentiment of a whole message. Previous work (Nakov et al., 2013) showed that message-level sentiment classification is more difficult than that of expression-level (i.e., 0.690 vs 0.889 in terms of F-measure) since a message may be composed of inconsistent s"
S14-2042,J09-3003,0,0.0487542,"he negation context as a snippet of a tweet that starts with a negation word and ends with punctuation marks. If a non-negation word is in a negation context and also in the sentiment lexicon, we reverse its polarity. For example, the word “bad” in phrase “not bad” originally has a negative score of 0.625, after reversal, this phrase has a positive score of 0.625. A manually made list containing 29 negation words (e.g., no, hardly, never, etc) was used in our experiment. Four sentiment lexicons were used to decide whether a word is subjective or objective and obtain its sentiment score. MPQA (Wilson et al., 2009). This subjectivity lexicon contains about 8000 subjective words and each word has two types of sentiment strength: strong subjective and weak subjective, and four kinds of sentiment polarities: positive, negative, both (positive and negative) and neutral. We used this lexicon to determine whether a word is positive, negative or objective and assign a value of 0.5 or 1 if it is weak or strong subjective (i.e., positive or negative) respectively. Our Systems We extracted eight types of features and the first six types were used in subtask A and all features were used in subtask B. Then, several"
S14-2042,S13-2067,1,0.859685,"Missing"
S14-2042,P11-1015,0,0.0294332,"ivity: 0.125, objectivity: 0.375. In experiment we used the most common sense of a word. NRC (Mohammad et al., 2013). Mohammad et al. collected two sets of tweets and each tweet contains the seed hashtags or emoticons and then they labeled the sentiment orientation for each tweet according to its hashtags or emoticons. They used pointwise mutual information (PMI) to calculate the sentiment score for each word and obtained two sentiment lexicons (i.e., hashtag lexicon and emoticon lexicon). IMDB. We generated an unigram lexicon by ourselves from a large movie review data set from IMDB website (Maas et al., 2011) which contains 25,000 positive and 25,000 negative movie reviews by calculating their PMI scores. 2.2.5 Word Cluster Apart from n-gram, we presented another word representations based on word clusters to explore shallow semantic meanings and reduced the sparsity of the word space. 1000 word clusters provided by CMU pos-tagging tool5 were used to represent tweet contents. For each tweet we recorded the number of words from each cluster, resulting in 1000 features. 2.2.3 Word n-Gram Words in themselves in tweets usually carry out the original sentiment orientation, so we consider word n-grams a"
S14-2042,S13-2053,0,0.427759,"ssion-level (i.e., 0.690 vs 0.889 in terms of F-measure) since a message may be composed of inconsistent sentiments. To date, lots of approaches have been proposed for conventional blogging sentiment analysis and a very broad overview is presented in (Pang and Lee, 2008). Inspired by that, many features used in microblogging mining are adopted from traditional blogging sentiment analysis task. For example, n-grams at the character or word level, part-of-speech tags, negations, sentiment lexicons were used in most of current work (Agarwal et al., 2011; Barbosa and Feng, 2010; Zhu et al., 2013; Mohammad et al., 2013; K¨okciyan et al., 2013). They found that n-grams are still effective in spite of the short length nature of microblogging and the distributions of different POS tags in tweets with different polarities are highly different (Pak and Paroubek, 2010). Compared with formal blog texts, tweets often contain many informal writings including slangs, emoticons, creMicroblogging websites (such as Twitter, Facebook) are rich sources of data for opinion mining and sentiment analysis. In this paper, we describe our approaches used for sentiment analysis in twitter (task 9) organized in SemEval 2014. This"
S14-2042,L08-1000,0,\N,Missing
S14-2042,W12-5504,0,\N,Missing
S14-2042,S13-2093,0,\N,Missing
S14-2043,S12-1094,0,0.0157772,"eported. For Ph-W level and W-Se level, since word and sense alone cannot be treated as sentence, we propose an information enrichment method to extend original text with the help of WordNet. Once the word or sense is enriched with its synonym and its definition description, we can thus adopt the previous features as well. Knowledge based features. Knowledge based similarity estimation relies on the semantic network of words. In this work we used the knowledge based features in our previous work (Zhu and Lan, 2013), which include four word similarity metrics based on WordNet: Path similarity (Banea et al., 2012), WUP similarity (Wu and Palmer, 1994), LCH similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998). Then two strategies, i.e., the best alignment strategy and the aggregation strategy, are employed to propagate the word similarity to the text similarity. Totally we get 8 knowledge based features. Corpus based features. Latent Semantic Analysis (LSA) (Landauer et al., 1997) is a widely used corpus based measure when evaluating text similarity. In this work we use the Vector Space Senˇ tence Similarity proposed by (Saric et al., 2012), which represents each sentence as a single d"
S14-2043,S12-1059,0,0.0627199,"Missing"
S14-2043,P13-1132,0,0.0587094,"Ph level, we regard the paragraph of P-S as a long sentence, and the phrase of SPh as a short sentence. Then we use various types of text similarity features including string features, knowledge based features, corpus based features, syntactic features, machine translation based features, multi-level text features and so on, to capture the semantic similarity between two texts. Some of these features are borrowed from our previous system in the Semantic Textual Similarity (STS) task in ∗ SEM Shared Task 2013 (Zhu and Lan, 2013). Others followed the previous work ˇ in (Saric et al., 2012) and (Pilehvar et al., 2013). For Ph-W level and W-Se level, since the text pairs lack contextual information, for example, word or sense alone no longer shares the property of sentence, most features used in P-S level and S-Ph level are not applicable or available. To overcome the problem of insufficient information in word and sense level, we propose several information enrichment methods to extend information with the aid of WordNet (Miller, 1995), which significantly improved the system performance. The rest of this paper is organized as follows. This paper reports our submissions to the Cross Level Semantic Similari"
S14-2043,S12-1076,0,0.0550361,"Missing"
S14-2043,S12-1060,0,0.0782575,"Missing"
S14-2043,S13-1017,1,0.725902,"a supervised regression system for each cross level separately. For P-S level and S-Ph level, we regard the paragraph of P-S as a long sentence, and the phrase of SPh as a short sentence. Then we use various types of text similarity features including string features, knowledge based features, corpus based features, syntactic features, machine translation based features, multi-level text features and so on, to capture the semantic similarity between two texts. Some of these features are borrowed from our previous system in the Semantic Textual Similarity (STS) task in ∗ SEM Shared Task 2013 (Zhu and Lan, 2013). Others followed the previous work ˇ in (Saric et al., 2012) and (Pilehvar et al., 2013). For Ph-W level and W-Se level, since the text pairs lack contextual information, for example, word or sense alone no longer shares the property of sentence, most features used in P-S level and S-Ph level are not applicable or available. To overcome the problem of insufficient information in word and sense level, we propose several information enrichment methods to extend information with the aid of WordNet (Miller, 1995), which significantly improved the system performance. The rest of this paper is orga"
S14-2043,P94-1019,0,\N,Missing
S14-2044,S12-1059,0,0.138574,"Missing"
S14-2044,H05-1079,0,0.0189174,"n 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 271 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271–277, Dublin, Ireland, August 23-24, 2014. method (Zhu and Lan, 2013; Croce et al., 2013) which integrates multiple similarity measures and adopts supervised machine learning algorithms to learn the different contributions of different features. The approaches to the task of TE can be roughly divided into two groups: (1) logic inference method (Bos and Markert, 2005) where automatic reasoning tools are used to check the logical representations derived from sentences and (2) machine learning method (Zhao et al., 2013; Gomaa and Fahmy, 2013) where a supervised model is built using a variety of similarity scores. Unlike previous work which separately addressed these two closely related tasks by using simple feature types, in this paper we endeavor to simultaneously solve these two tasks by using heterogenous features. Like semantic relatedness, TE task (surveyed in (Androutsopoulos and Malakasiotis, 2009)) is also closely related to STS task since in TE task"
S14-2044,S13-1007,0,0.0180175,"s are severely limited to model the semantic of long phrases or sentences since they ignore grammatical structures and logical words. Compositional Distributional Semantic Models (CDSMs)(Zanzotto et al., 2010; Socher et This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 271 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271–277, Dublin, Ireland, August 23-24, 2014. method (Zhu and Lan, 2013; Croce et al., 2013) which integrates multiple similarity measures and adopts supervised machine learning algorithms to learn the different contributions of different features. The approaches to the task of TE can be roughly divided into two groups: (1) logic inference method (Bos and Markert, 2005) where automatic reasoning tools are used to check the logical representations derived from sentences and (2) machine learning method (Zhao et al., 2013; Gomaa and Fahmy, 2013) where a supervised model is built using a variety of similarity scores. Unlike previous work which separately addressed these two closely relat"
S14-2044,D12-1110,0,0.0350731,"Missing"
S14-2044,D11-1129,0,0.021769,"Missing"
S14-2044,S12-1060,0,0.172031,"Missing"
S14-2044,P12-1091,0,0.0420436,"sed in our experiments. We also used three statistical correlation coefficients (i.e., Pearson, Spearmanr, Kendalltau) to measure similarity by regarding the vectorial representations as different variables. Thus we got ten features at last. 3.2.3 Semantic Similarity (ss) The above surface text similarity features only consider the surface words rather than their actual meanings in sentences. In order to build the semantic representations of sentences, we used a latent model to capture the contextual meanings of words. Specifically, we adopted the weighted textual matrix factorization (WTMF) (Guo and Diab, 2012) to model the semantics of sentences due to its reported good ability to model short texts. This model first factorizes the original termsentence matrix X into two matrices such that 3 4 http://nlp.stanford.edu/software/tagger.shtml 273 http://nlp.stanford.edu/software/lex-parser.shtml 3.2.6 String Features (str) Co-occurrence retrieval model (CRM) The CRM word similarity is calculated as follows: This set of features is taken from our previous work (Zhu and Lan, 2013) due to its superior performance. Longest common sequence (LCS) We computed the LCS similarity on the original and lemmatized s"
S14-2044,C10-1142,0,0.0162904,"Missing"
S14-2044,S13-2021,1,0.874865,"1 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271–277, Dublin, Ireland, August 23-24, 2014. method (Zhu and Lan, 2013; Croce et al., 2013) which integrates multiple similarity measures and adopts supervised machine learning algorithms to learn the different contributions of different features. The approaches to the task of TE can be roughly divided into two groups: (1) logic inference method (Bos and Markert, 2005) where automatic reasoning tools are used to check the logical representations derived from sentences and (2) machine learning method (Zhao et al., 2013; Gomaa and Fahmy, 2013) where a supervised model is built using a variety of similarity scores. Unlike previous work which separately addressed these two closely related tasks by using simple feature types, in this paper we endeavor to simultaneously solve these two tasks by using heterogenous features. Like semantic relatedness, TE task (surveyed in (Androutsopoulos and Malakasiotis, 2009)) is also closely related to STS task since in TE task lots of similarity measures at different levels are exploited to boost classification. For example, (Malakasiotis and Androutsopoulos, 2007) used ten s"
S14-2044,W07-1407,0,0.0153395,"and (2) machine learning method (Zhao et al., 2013; Gomaa and Fahmy, 2013) where a supervised model is built using a variety of similarity scores. Unlike previous work which separately addressed these two closely related tasks by using simple feature types, in this paper we endeavor to simultaneously solve these two tasks by using heterogenous features. Like semantic relatedness, TE task (surveyed in (Androutsopoulos and Malakasiotis, 2009)) is also closely related to STS task since in TE task lots of similarity measures at different levels are exploited to boost classification. For example, (Malakasiotis and Androutsopoulos, 2007) used ten string similarity measures such as cosine similarity at the word and the character level. Therefore, the first fundamental question arises, i.e., “Can we solve both of these two tasks together?” At the same time, since high similarity does not mean entailment holds, the TE task also utilizes other features besides similarity measures. For example, in our previous work (Zhao et al., 2014) text difference features were proposed and proved to be effective. Therefore, the second question surfaces here, i.e., “Are features proposed for TE task still effective for STS task?” To answer the"
S14-2044,S14-2001,0,0.073161,"tract al., 2012) extend DSMs to sentence level to capture the compositionality in the semantic vector space, which has seen a rapidly growing interest in recent years. Although several CDSMs have been proposed, benchmarks are lagging behind. Previous work (Grefenstette and Sadrzadeh, 2011; Socher et al., 2012) performed experiments on their own datasets or on the same datasets which are limited to a few hundred instances of very short sentences with a fixed structure. To provide a benchmark so as to compare different CDSMs, the sentences involving compositional knowledge task in SemEval 2014 (Marelli et al., 2014) develops a large dataset which is full of lexical, syntactic and semantic phenomena. It consists of two subtasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 (very related); and textual entailment (TE) task, which determines whether one of the following three relationships holds between two given sentences A and B: (1) entailment: the meaning of B can be inferred from A; (2) contradiction: A contradicts B; (3) neutral: the truth of B cannot be inferred on the basis o"
S14-2044,S13-1017,1,0.869762,"their success, DSMs are severely limited to model the semantic of long phrases or sentences since they ignore grammatical structures and logical words. Compositional Distributional Semantic Models (CDSMs)(Zanzotto et al., 2010; Socher et This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 271 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271–277, Dublin, Ireland, August 23-24, 2014. method (Zhu and Lan, 2013; Croce et al., 2013) which integrates multiple similarity measures and adopts supervised machine learning algorithms to learn the different contributions of different features. The approaches to the task of TE can be roughly divided into two groups: (1) logic inference method (Bos and Markert, 2005) where automatic reasoning tools are used to check the logical representations derived from sentences and (2) machine learning method (Zhao et al., 2013; Gomaa and Fahmy, 2013) where a supervised model is built using a variety of similarity scores. Unlike previous work which separately addressed th"
S15-2006,N06-2009,0,0.0851399,"Missing"
S15-2006,P12-1091,0,0.142318,"29 negation words (e.g., scarcely, no, little). Also, we checked whether one sentence entails the other only using the named entity information which was provided in the dataset. Finally, we obtained nineteen other features. 3.1 System Setups 2003) as another type of corpus based feature. The CRM was calculated based on a notion of substitutability, that is, the more appropriate it was to substitute word w1 in place of word w2 in a suitable natural language task, the more semantically similar they were. Besides, the extraction of aforementioned features rely on large external corpora, while (Guo and Diab, 2012) proposed a novel latent model, i.e., weighted textual matrix factorization (WTMF), to capture the contextual meanings of words in sentences based on internal term-sentence matrix. WTMF factorizes the original term-sentence matrix X into two matrices such that Xi,j ≈ T Q , where P P∗,i ∗,j ∗,i is a latent semantics vector profile for word wi and Q∗,j is the vector profile that represents the sentence sj . The weight matrix W is introduced in the optimization process in order to model the missing words at the right level of emphasis. Then, we used cosine, Manhattan, Euclidean functions and Pear"
S15-2006,J10-3003,0,0.153917,"Missing"
S15-2006,P14-5010,0,0.0059553,"Missing"
S15-2006,S14-2001,0,0.0265668,"opoulos and Malakasiotis, 2010) for completion. Most of previous work of paraphrase are on formal text. Recently with the rapidly growth To provide a benchmark so as to compare and develop different paraphrasing techniques in Twitter, the paraphrase and semantic similarity task in SemEval 2015 (Xu et al., 2015) requires the participants to determine whether two tweets express the same meaning or not and optionally a degree score between 0 and 1, which can be regarded as a binary classification problem. Paraphrasing task is very close to semantic textual similarity and textual entailment task (Marelli et al., 2014) since substantially these tasks all concentrated on modeling the underlying similarity between two sentences. The commonly-used features in these tasks can be categorized into several following groups: (1) string based which measures the sequence similarities of original strings with others, e.g., n-gram Overlap, cosine similarity; (2) corpus based which measures word or sentence similarities using word distributional vectors learned from large corpora using distributional models, like Latent Semantic Analysis (LSA), etc. (3) knowledge based which estimates similarities with the aid of extern"
S15-2006,P09-1089,0,0.0290802,"Missing"
S15-2006,N10-1020,0,0.0865279,"Missing"
S15-2006,P10-1040,0,0.0573695,"/nltk.org/ 35 https://catalog.ldc.upenn.edu/LDC2006T13 https://catalog.ldc.upenn.edu/LDC2008T19 We estimated the similarities of sentence pairs at syntactic level. Stanford CoreNLP toolkit (Manning and Surdeanu, 2014) was used to obtain POS tag sequences. Afterwards, we performed eight measure functions described in Section 2.2 over these sequences, which resulted in eight syntactic based features. speech recognition and natural language processing. As a consequence of its application in NLP, word embeddings have been building blocks in many tasks, e.g., named entity recognition and chunking (Turian et al., 2010), semantic word similarities (Mikolov et al., 2013a), etc. Being distributed representation of words, word embeddings usually are learned using neural networks over a large raw corpus and has outperformed LSA for preserving linear regularities among words (Mikolov et al., 2013a). Due to its superior performance, we adopted word embeddings to estimate the similarities of sentence pairs. In our experiments, we used seven different word embeddings with different dimensions: word2vec (Mikolov et al., 2013b), Collobert and Weston embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and"
S15-2006,S12-1060,0,0.0795379,"Missing"
S15-2006,S15-2001,0,0.031836,"ni and Dorr, 2010). Identifying paraphrase can improve the performance of several natural language processing (NLP) applications, such as query and pattern expansion (Metzler et al., 2007), machine translation (Mirkin et al., 2009), question answering (Duboue and Chu-Carroll, 2006), see survey (Androutsopoulos and Malakasiotis, 2010) for completion. Most of previous work of paraphrase are on formal text. Recently with the rapidly growth To provide a benchmark so as to compare and develop different paraphrasing techniques in Twitter, the paraphrase and semantic similarity task in SemEval 2015 (Xu et al., 2015) requires the participants to determine whether two tweets express the same meaning or not and optionally a degree score between 0 and 1, which can be regarded as a binary classification problem. Paraphrasing task is very close to semantic textual similarity and textual entailment task (Marelli et al., 2014) since substantially these tasks all concentrated on modeling the underlying similarity between two sentences. The commonly-used features in these tasks can be categorized into several following groups: (1) string based which measures the sequence similarities of original strings with other"
S15-2006,D11-1061,0,0.28368,"Missing"
S15-2021,S14-2010,0,0.0400776,"Missing"
S15-2021,S15-2045,0,0.0607656,"Missing"
S15-2021,S12-1059,0,0.0400953,"Missing"
S15-2021,W10-1201,0,0.176841,"st similar training dataset and separately construct a individual model for each test set; (3) adopt multi-task learning framework to make full use of available training sets. Results on the test datasets show that using all datasets as training set achieves the best averaged performance and our best system ranks 15 out of 73. 1 Introduction Estimating the degree of semantic similarity between two sentences is the building block of many natural language processing (NLP) applications, such as textual entailment (Zhao et al., 2014a), text summarization (Lloret et al., 2008), question answering (Celikyilmaz et al., 2010), etc. Therefore, semantic textual similarity (STS) has been received an increasing amount of attention in recent years, e.g., the Semantic Textual Similarity competitions in Semantic Evaluation Exercises have been held from 2012 to 2014. This year the participants in the STS task in SemEval 2015 (Agirre et al., 2015) are required to rate the similar degree of a pair of sentences by a value from 0 (no relation) to 5 (semantic equivalence) with an optional confidence score. To identify semantic textual similarity of text pairs, most existing works adopt at least one of the following feature typ"
S15-2021,S13-1005,0,0.0290299,"n-gram sequences; (2) ˇ c et al., 2012; Han et al., corpus based similarity (Sari´ 2013) where distributional models such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method (Shareghi and Bergler, 2013; Mihalcea et al., 2006) which estimates the similarities with the aid of external resources, such as WordNet1 . Among them, lots of researchers (Sultan et al., 2014; Han et al., 2013) leverage different word alignment strategies to bring word-level similarity to sentence-level similarity. In this work, we first borrow aforementioned effective types of similarity measurements including string-based, corpus-based, syntactic features and so on, to capture the semantic similarity between two sentences. Beside, we also present a novel feature type based on word embeddings that are induced using neural language models over a large raw cor1 http://wordnet.princeton.edu/ 117 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 117–122, c Denve"
S15-2021,S12-1061,0,0.0409276,"has been received an increasing amount of attention in recent years, e.g., the Semantic Textual Similarity competitions in Semantic Evaluation Exercises have been held from 2012 to 2014. This year the participants in the STS task in SemEval 2015 (Agirre et al., 2015) are required to rate the similar degree of a pair of sentences by a value from 0 (no relation) to 5 (semantic equivalence) with an optional confidence score. To identify semantic textual similarity of text pairs, most existing works adopt at least one of the following feature types: (1) string based similarity (B¨ar et al., 2012; Jimenez et al., 2012) which employs common functions to calculate similarities over string sequences extracted from original strings, e.g., lemma, stem, or n-gram sequences; (2) ˇ c et al., 2012; Han et al., corpus based similarity (Sari´ 2013) where distributional models such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method (Shareghi and Bergler, 2013; Mihalcea et al., 2006) which e"
S15-2021,P14-5010,0,0.0136557,"ty to measure the similarity of two sentences based on these vectors. Besides, we used the Co-occurrence Retrieval Model (CRM) (Weeds, 2003) as another type of corpus based feature. The CRM was calculated based on a notion of substitutability, that is, the more appropriate it was to substitute word w1 in place of word w2 in a suitable natural language task, the more semantically similar they were. At last, we obtained six corpus based features. 2.4 Syntactic Features Besides semantic similarity, we also estimated the similarities of sentence pairs at syntactic level. Stanford CoreNLP toolkit (Manning et al., 2014) was used to obtain the POS tag sequences for each sentence. Afterwards, we performed eight measure functions described above in Section 2.2 over these sequences, resulting in eight syntactic features. 2.5 Word Embedding Features Recently, deep learning has archived a great success in the fields of computer vision, automatic speech recognition and natural language processing. One result of its application in NLP, i.e., word embeddings, has been successfully explored in named entity recognition, chunking (Turian et al., 2010) and semantic word similarities(Mikolov et al., 2013a), etc. The distr"
S15-2021,S13-1029,0,0.0173398,"based similarity (B¨ar et al., 2012; Jimenez et al., 2012) which employs common functions to calculate similarities over string sequences extracted from original strings, e.g., lemma, stem, or n-gram sequences; (2) ˇ c et al., 2012; Han et al., corpus based similarity (Sari´ 2013) where distributional models such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method (Shareghi and Bergler, 2013; Mihalcea et al., 2006) which estimates the similarities with the aid of external resources, such as WordNet1 . Among them, lots of researchers (Sultan et al., 2014; Han et al., 2013) leverage different word alignment strategies to bring word-level similarity to sentence-level similarity. In this work, we first borrow aforementioned effective types of similarity measurements including string-based, corpus-based, syntactic features and so on, to capture the semantic similarity between two sentences. Beside, we also present a novel feature type based on word embeddings that are induced using ne"
S15-2021,S14-2039,0,0.026331,".g., lemma, stem, or n-gram sequences; (2) ˇ c et al., 2012; Han et al., corpus based similarity (Sari´ 2013) where distributional models such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; (3) knowledge based method (Shareghi and Bergler, 2013; Mihalcea et al., 2006) which estimates the similarities with the aid of external resources, such as WordNet1 . Among them, lots of researchers (Sultan et al., 2014; Han et al., 2013) leverage different word alignment strategies to bring word-level similarity to sentence-level similarity. In this work, we first borrow aforementioned effective types of similarity measurements including string-based, corpus-based, syntactic features and so on, to capture the semantic similarity between two sentences. Beside, we also present a novel feature type based on word embeddings that are induced using neural language models over a large raw cor1 http://wordnet.princeton.edu/ 117 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pag"
S15-2021,P10-1040,0,0.107026,"ties of sentence pairs at syntactic level. Stanford CoreNLP toolkit (Manning et al., 2014) was used to obtain the POS tag sequences for each sentence. Afterwards, we performed eight measure functions described above in Section 2.2 over these sequences, resulting in eight syntactic features. 2.5 Word Embedding Features Recently, deep learning has archived a great success in the fields of computer vision, automatic speech recognition and natural language processing. One result of its application in NLP, i.e., word embeddings, has been successfully explored in named entity recognition, chunking (Turian et al., 2010) and semantic word similarities(Mikolov et al., 2013a), etc. The distributed representations of words (i.e., word embeddings) learned using neural networks over a large raw corpus have been shown that they performed significantly better than LSA for preserving linear regularities among words (Mikolov et al., 2013a). Due to its superior performance, we adopted word embeddings to estimate the similarities of sentence pairs. In our experiments, we used two different word embeddings: word2vec (Mikolov et al., 2013b) and Collobert and Weston embeddings (Turian et al., 2010). The word embeddings fro"
S15-2021,S12-1060,0,0.126336,"Missing"
S15-2021,S14-2044,1,0.911347,"asets and build a unified supervised model for all test datasets; (2) select the most similar training dataset and separately construct a individual model for each test set; (3) adopt multi-task learning framework to make full use of available training sets. Results on the test datasets show that using all datasets as training set achieves the best averaged performance and our best system ranks 15 out of 73. 1 Introduction Estimating the degree of semantic similarity between two sentences is the building block of many natural language processing (NLP) applications, such as textual entailment (Zhao et al., 2014a), text summarization (Lloret et al., 2008), question answering (Celikyilmaz et al., 2010), etc. Therefore, semantic textual similarity (STS) has been received an increasing amount of attention in recent years, e.g., the Semantic Textual Similarity competitions in Semantic Evaluation Exercises have been held from 2012 to 2014. This year the participants in the STS task in SemEval 2015 (Agirre et al., 2015) are required to rate the similar degree of a pair of sentences by a value from 0 (no relation) to 5 (semantic equivalence) with an optional confidence score. To identify semantic textual si"
S15-2042,S12-1094,0,0.0220989,"onsiders the overlapped surface words or substrings in a QA pair and it may not capture the semantic information between a QA pair. Therefore, we presented the following semantic similarity features, which are borrowed from previous work. Determining semantic similarity of sentences commonly uses measures of semantic similarity be238 tween individual words. We used knowledge-based and corpus-based word similarity features. The knowledge-based similarity estimation relies on a semantic network of words such as WordNet. In this work, we employed four WordNet-based word similarity metrics: Path (Banea et al., 2012), WUP (Wu and Palmer, 1994), LCH (Leacock and Chodorow, 1998) and Lin (Lin, 1998) similarity. Following (Zhu and Man, 2013), the best alignment strategy and the aggregation strategy are employed to propagate the word similarity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vecˇ c et tor space sentence similarity proposed by (Sari´ al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In"
S15-2042,P12-1091,0,0.0294849,"rity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vecˇ c et tor space sentence similarity proposed by (Sari´ al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. Besides, following (Zhao et al., 2014), we adopted the weighted textual matrix factorization (WTMF) (Guo and Diab, 2012) to model the semantics representations of sentences and then employed the new representations to calculate the semantic similarity between QA pairs using Cosine, Manhattan, Euclidean, Person, Spearmanr, Kendalltau measures respectively. 2.3.5 Answerer Information (AI) Previous work (Zhou et al., 2012) showed that information about answerer has great impact on answer ranking in CQA. Inspired by this work, we designed two answerer-specific features to represent answerer level and answerer expert domain information. To calculate the answerer level feature, we used the number of answers and the p"
S15-2042,S15-2047,0,0.064452,"Missing"
S15-2042,S12-1060,0,0.0802457,"Missing"
S15-2042,J11-2003,0,0.0324996,"ot always of high quality. For example, a bad answer may present irrelevant opinions or issues, contain only URL links without direct answer, or even be written informally. Therefore, in order to achieve high-quality user experience and maintain high levels of adherence, it is critical to present high-quality answers and provide direct responses for users. The CQA task in SemEval-2015 (M`arquez et al., 2015) provides such a universal platform for reMost of the previous research on answer quality prediction has focused on extracting various features to employ ranking or classification methods (Surdeanu et al., 2011; Shah and Pomerantz, 2010), such as textual features (Agichtein et al., 2008; Blooma et al., 2010) including the length of an answer, overlapped words between a question-answer (QA) pair, etc. Another kind of widely used feature is extracted from answerer profile information (Shah and Pomerantz, 2010), such as the number of best answers, the achieved levels and the earned points. However, such information is not often available in real world. Moreover, a recent study (Toba et al., 2014) has taken question type into consideration to make the answers quality prediction. In this paper, we built"
S15-2042,S14-2044,1,0.848299,"t strategy and the aggregation strategy are employed to propagate the word similarity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vecˇ c et tor space sentence similarity proposed by (Sari´ al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. Besides, following (Zhao et al., 2014), we adopted the weighted textual matrix factorization (WTMF) (Guo and Diab, 2012) to model the semantics representations of sentences and then employed the new representations to calculate the semantic similarity between QA pairs using Cosine, Manhattan, Euclidean, Person, Spearmanr, Kendalltau measures respectively. 2.3.5 Answerer Information (AI) Previous work (Zhou et al., 2012) showed that information about answerer has great impact on answer ranking in CQA. Inspired by this work, we designed two answerer-specific features to represent answerer level and answerer expert domain information"
S15-2042,S13-1017,0,0.0129622,"A pair. Therefore, we presented the following semantic similarity features, which are borrowed from previous work. Determining semantic similarity of sentences commonly uses measures of semantic similarity be238 tween individual words. We used knowledge-based and corpus-based word similarity features. The knowledge-based similarity estimation relies on a semantic network of words such as WordNet. In this work, we employed four WordNet-based word similarity metrics: Path (Banea et al., 2012), WUP (Wu and Palmer, 1994), LCH (Leacock and Chodorow, 1998) and Lin (Lin, 1998) similarity. Following (Zhu and Man, 2013), the best alignment strategy and the aggregation strategy are employed to propagate the word similarity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vecˇ c et tor space sentence similarity proposed by (Sari´ al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. B"
S15-2042,P94-1019,0,\N,Missing
S15-2094,C14-1008,0,0.229255,"hun Wu, Man Lan∗ Shanghai Key Laboratory of Multidimensional Information Processing Department of Computer Science and Technology, East China Normal University Shanghai 200241, P. R. China {51131201039, 51141201064}@ecnu.cn, mlan@cs.ecnu.edu.cn∗ Abstract Jiang et al., 2011; Chen et al., 2012) have investigated various ways to settle these target dependent issues. Recently, inspired by (Mikolov et al., 2013a) using neural network to construct distributed word representation (word embedding), several researchers employed neural network to perform sentiment analysis. For example, (Kim, 2014; dos Santos and Gatti, 2014) adopted convolutional neural networks to learn sentiment-bearing sentence vectors, and (Mikolov et al., 2013b) proposed Paragraph vector which outperformed bag-of-words model for sentiment analysis. This paper reports our submission to task 10 (Sentiment Analysis on Tweet, SAT) (Rosenthal et al., 2015) in SemEval 2015 , which contains five subtasks, i.e., contextual polarity disambiguation (subtask A: expressionlevel), message polarity classification (subtask B: message-level), topic-based message polarity classification and detecting trends towards a topic (subtask C and D: topic-level), and"
S15-2094,S14-2096,0,0.0145988,"A aims at classifying the sentiment of a marked instance in a given message, subtask B is to determine the polarity of the whole message and subtask C focuses on identifying the sentiment of the message towards the given topic). The fourth subtask D is to detect the sentiment trends of a given set of messages towards a topic from the same period of time. The last subtask E is to predict a score between 0 and 1, which is indicative of the strength of association of twitter terms with positive sentiment. Following previous works (Rosenthal et al., 2014; Zhao et al., 2014; Mohammad et al., 2013; Evert et al., 2014; Mohammad et al., 2013; Wasi et al., 2014), we adopted a rich set of traditional features, e.g., linguistic features (e.g., n-gram at word level, part-of-speech (POS) tags, negations, etc), sentiment lexicon features (e.g., MPQA, Bing Liu opinion lexicon, SentiWordNet, etc) and twitter specif561 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 561–567, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics ical features (e.g., the number of URL, emoticons, capital words, elongated words, hashtags, etc). Besides, inspired by"
S15-2094,P11-1016,0,0.107994,"Missing"
S15-2094,C04-1200,0,0.266444,"al sentiment lexicons and then built a regression model to estimate the sentiment strength. Despite the simplicity of features, our systems rank above the average. 1 Introduction In the past few years, hundreds of millions of people shared and expressed their opinions through microblogging websites, such as Twitter. The study on this platform is increasingly drawing attention of many researchers and organizations. Given the character limitations on tweets, the sentiment orientation classification on tweets is usually analogous to the sentence-level sentiment analysis (Kouloumpis et al., 2011; Kim and Hovy, 2004; Yu and Hatzivassiloglou, 2003). However, considering opinions adhering on different topics and expressed by various expression words in tweets, (Wang et al., 2011; The task of Sentiment Analysis in Twitter (SAT) in SemEval 2015 consists of five subtasks. The first three subtasks focus on determining the polarity of the given tweet, phrase or topic (i.e., subtask A aims at classifying the sentiment of a marked instance in a given message, subtask B is to determine the polarity of the whole message and subtask C focuses on identifying the sentiment of the message towards the given topic). The"
S15-2094,D14-1181,0,0.0785092,"hua Zhang, Guoshun Wu, Man Lan∗ Shanghai Key Laboratory of Multidimensional Information Processing Department of Computer Science and Technology, East China Normal University Shanghai 200241, P. R. China {51131201039, 51141201064}@ecnu.cn, mlan@cs.ecnu.edu.cn∗ Abstract Jiang et al., 2011; Chen et al., 2012) have investigated various ways to settle these target dependent issues. Recently, inspired by (Mikolov et al., 2013a) using neural network to construct distributed word representation (word embedding), several researchers employed neural network to perform sentiment analysis. For example, (Kim, 2014; dos Santos and Gatti, 2014) adopted convolutional neural networks to learn sentiment-bearing sentence vectors, and (Mikolov et al., 2013b) proposed Paragraph vector which outperformed bag-of-words model for sentiment analysis. This paper reports our submission to task 10 (Sentiment Analysis on Tweet, SAT) (Rosenthal et al., 2015) in SemEval 2015 , which contains five subtasks, i.e., contextual polarity disambiguation (subtask A: expressionlevel), message polarity classification (subtask B: message-level), topic-based message polarity classification and detecting trends towards a topic (subta"
S15-2094,S14-2076,0,0.106106,"ent definitions. For example, since subtask B focuses on sentiment classification on whole tweet, we extract features from all words in tweet. However, the other three subtasks, i.e, A, C, and D, perform sentiment polarity classification only on a certain piece of tweet, i.e., expression words or topic in tweet. Since organizers have provided the annotated target words (for A) and topics (for C and D) for each tweet, we 562 only chose related words rather than all words in whole tweet as pending words for consequential feature extraction. To pick out related words from whole tweet, following (Kiritchenko et al., 2014), for each annotated target word we only treated the surrounding words from parse tree with distance d ≤ 2 as its relevant words. In this task, we used four types of features: sentiment lexicon features (the score calculated from seven sentiment lexicons), linguistic features (n-grams, POS tagger, negations, etc), tweet-specific features (emoticons, all-caps, hashtag, etc) and word embedding features. Sentiment Lexicon Features (SentiLexi): We employed the following seven sentiment lexicons to extract sentimental lexicon features: Bing Liu lexicon1 , General Inquirer lexicon2 , IMDB3 , MPQA4 ,"
S15-2094,S13-2053,0,0.0307199,"r topic (i.e., subtask A aims at classifying the sentiment of a marked instance in a given message, subtask B is to determine the polarity of the whole message and subtask C focuses on identifying the sentiment of the message towards the given topic). The fourth subtask D is to detect the sentiment trends of a given set of messages towards a topic from the same period of time. The last subtask E is to predict a score between 0 and 1, which is indicative of the strength of association of twitter terms with positive sentiment. Following previous works (Rosenthal et al., 2014; Zhao et al., 2014; Mohammad et al., 2013; Evert et al., 2014; Mohammad et al., 2013; Wasi et al., 2014), we adopted a rich set of traditional features, e.g., linguistic features (e.g., n-gram at word level, part-of-speech (POS) tags, negations, etc), sentiment lexicon features (e.g., MPQA, Bing Liu opinion lexicon, SentiWordNet, etc) and twitter specif561 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 561–567, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics ical features (e.g., the number of URL, emoticons, capital words, elongated words, hashtags, etc)."
S15-2094,S13-2052,0,0.127443,"Missing"
S15-2094,N13-1039,0,0.170449,"Missing"
S15-2094,S15-2078,0,0.058265,"tigated various ways to settle these target dependent issues. Recently, inspired by (Mikolov et al., 2013a) using neural network to construct distributed word representation (word embedding), several researchers employed neural network to perform sentiment analysis. For example, (Kim, 2014; dos Santos and Gatti, 2014) adopted convolutional neural networks to learn sentiment-bearing sentence vectors, and (Mikolov et al., 2013b) proposed Paragraph vector which outperformed bag-of-words model for sentiment analysis. This paper reports our submission to task 10 (Sentiment Analysis on Tweet, SAT) (Rosenthal et al., 2015) in SemEval 2015 , which contains five subtasks, i.e., contextual polarity disambiguation (subtask A: expressionlevel), message polarity classification (subtask B: message-level), topic-based message polarity classification and detecting trends towards a topic (subtask C and D: topic-level), and determining sentiment strength of twitter terms (subtask E: term-level). For the first four subtasks, we built supervised models using traditional features and word embedding features to perform sentiment polarity classification. For subtask E, we first expanded the training data with the aid of extern"
S15-2094,W03-1017,0,0.110964,"s and then built a regression model to estimate the sentiment strength. Despite the simplicity of features, our systems rank above the average. 1 Introduction In the past few years, hundreds of millions of people shared and expressed their opinions through microblogging websites, such as Twitter. The study on this platform is increasingly drawing attention of many researchers and organizations. Given the character limitations on tweets, the sentiment orientation classification on tweets is usually analogous to the sentence-level sentiment analysis (Kouloumpis et al., 2011; Kim and Hovy, 2004; Yu and Hatzivassiloglou, 2003). However, considering opinions adhering on different topics and expressed by various expression words in tweets, (Wang et al., 2011; The task of Sentiment Analysis in Twitter (SAT) in SemEval 2015 consists of five subtasks. The first three subtasks focus on determining the polarity of the given tweet, phrase or topic (i.e., subtask A aims at classifying the sentiment of a marked instance in a given message, subtask B is to determine the polarity of the whole message and subtask C focuses on identifying the sentiment of the message towards the given topic). The fourth subtask D is to detect th"
S15-2094,C14-1220,0,0.0455351,"from Internet and this binary feature records whether the corresponding emoticon is present or absent in pending words. - Punctuation: The numbers of exclamations (!) and questions (?) are also noted. - All-caps: It indicates the number of words with uppercase letters. - Hashtag: It is the number of hashtags in the sentence or phrase. - Elongated: It represents the number of words with one character repeated more than two times, e.g., “gooooood”. Word Embedding Features: Word embedding is a continuous-valued representation of the word which usually carries syntactic and semantic information (Zeng et al., 2014). Since a phrase or sentence contains more than one word, usually there are two strategies to convert the words vectors into a sentence vector: (1) summing up all words vectors; (2) rolling up the sequential words to obtain a 8 The 29 negations and other following manually collected data are available upon request. 563 For subtask A, B and C, we used the macroaveraged F score of positive and negative classes F +F (i.e., Fmacro = pos 2 neg ) to evaluate the performance, which considers a sense of effectiveness on small classes. For subtask D, the averaged absolute ∑ difference (i.e., avgAbsDif"
S15-2094,S14-2042,1,0.825041,"ven tweet, phrase or topic (i.e., subtask A aims at classifying the sentiment of a marked instance in a given message, subtask B is to determine the polarity of the whole message and subtask C focuses on identifying the sentiment of the message towards the given topic). The fourth subtask D is to detect the sentiment trends of a given set of messages towards a topic from the same period of time. The last subtask E is to predict a score between 0 and 1, which is indicative of the strength of association of twitter terms with positive sentiment. Following previous works (Rosenthal et al., 2014; Zhao et al., 2014; Mohammad et al., 2013; Evert et al., 2014; Mohammad et al., 2013; Wasi et al., 2014), we adopted a rich set of traditional features, e.g., linguistic features (e.g., n-gram at word level, part-of-speech (POS) tags, negations, etc), sentiment lexicon features (e.g., MPQA, Bing Liu opinion lexicon, SentiWordNet, etc) and twitter specif561 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 561–567, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics ical features (e.g., the number of URL, emoticons, capital words, elongated"
S15-2094,S14-2029,0,\N,Missing
S15-2094,S14-2009,0,\N,Missing
S15-2125,S14-2149,0,0.0159428,"SemEval 2015), pages 736–741, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics sentiments towards different aspects. (Hu and Liu, 2004; Ding et al., 2008) adopted lexicon-based approaches to detect the sentiment on different aspects. In addition, (Boiy and Moens, 2009; Jiang et al., 2011) explored the work to determine whether the reviews contain the aspect information. Unlike the above study, (Xiang et al., 2014) split the data into multiple subsets based on category distributions and then built seperate classifier for each category. Following previous work (Brun et al., 2014; Brychc´ın et al., 2014; Castellucci et al., 2014; Kiritchenko et al., 2014), a rich set of features are adopted in this work: linguistic features (e.g., ngrams, grammatical relationship, POS, negations), sentiment lexicon features (e.g., MPQA, General Inquirer, SentiWordNet, etc) and domain specific features (e.g., in-domain word list, punctuation, etc). We also performed a series of experiments to compare supervised machine learning algorithms with different parameters and to choose effective feature subsets for performance of classification. The rest of this paper is structured as follows."
S15-2125,S14-2145,0,0.0733118,"Missing"
S15-2125,S14-2135,0,0.03762,"orado, June 4-5, 2015. 2015 Association for Computational Linguistics sentiments towards different aspects. (Hu and Liu, 2004; Ding et al., 2008) adopted lexicon-based approaches to detect the sentiment on different aspects. In addition, (Boiy and Moens, 2009; Jiang et al., 2011) explored the work to determine whether the reviews contain the aspect information. Unlike the above study, (Xiang et al., 2014) split the data into multiple subsets based on category distributions and then built seperate classifier for each category. Following previous work (Brun et al., 2014; Brychc´ın et al., 2014; Castellucci et al., 2014; Kiritchenko et al., 2014), a rich set of features are adopted in this work: linguistic features (e.g., ngrams, grammatical relationship, POS, negations), sentiment lexicon features (e.g., MPQA, General Inquirer, SentiWordNet, etc) and domain specific features (e.g., in-domain word list, punctuation, etc). We also performed a series of experiments to compare supervised machine learning algorithms with different parameters and to choose effective feature subsets for performance of classification. The rest of this paper is structured as follows. In Section 2, we describe our system in details,"
S15-2125,P11-1016,0,0.0693939,"anavan et al., 2009; He et al., 2012; Mei et al., 2007) used topic or category information. (Lin and He, 2009; Jo and Oh, 2011) presented LDA-based models, which incorporate aspect and sentiment analysis together to model 736 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 736–741, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics sentiments towards different aspects. (Hu and Liu, 2004; Ding et al., 2008) adopted lexicon-based approaches to detect the sentiment on different aspects. In addition, (Boiy and Moens, 2009; Jiang et al., 2011) explored the work to determine whether the reviews contain the aspect information. Unlike the above study, (Xiang et al., 2014) split the data into multiple subsets based on category distributions and then built seperate classifier for each category. Following previous work (Brun et al., 2014; Brychc´ın et al., 2014; Castellucci et al., 2014; Kiritchenko et al., 2014), a rich set of features are adopted in this work: linguistic features (e.g., ngrams, grammatical relationship, POS, negations), sentiment lexicon features (e.g., MPQA, General Inquirer, SentiWordNet, etc) and domain specific fea"
S15-2125,S14-2076,0,0.172595,"5 Association for Computational Linguistics sentiments towards different aspects. (Hu and Liu, 2004; Ding et al., 2008) adopted lexicon-based approaches to detect the sentiment on different aspects. In addition, (Boiy and Moens, 2009; Jiang et al., 2011) explored the work to determine whether the reviews contain the aspect information. Unlike the above study, (Xiang et al., 2014) split the data into multiple subsets based on category distributions and then built seperate classifier for each category. Following previous work (Brun et al., 2014; Brychc´ın et al., 2014; Castellucci et al., 2014; Kiritchenko et al., 2014), a rich set of features are adopted in this work: linguistic features (e.g., ngrams, grammatical relationship, POS, negations), sentiment lexicon features (e.g., MPQA, General Inquirer, SentiWordNet, etc) and domain specific features (e.g., in-domain word list, punctuation, etc). We also performed a series of experiments to compare supervised machine learning algorithms with different parameters and to choose effective feature subsets for performance of classification. The rest of this paper is structured as follows. In Section 2, we describe our system in details, including preprocessing, fe"
S15-2125,S13-2053,0,0.0263346,"ds with 6 types of label: strong/weak positive, strong/weak negative, both (having positive and negative sentiment) and neutral. Then we transformed these above nominal labels to 1, 0.5, −1, −0.5, 0, 0 respectively. - SentiWordNet8 : The sentiment scores of each item in SentiWordNet is represented as a tuple i.e., positivity and negativity.We use the difference between positive and negative score as its sentiment score. When locating the corresponding item, we retrieved the word lemma and selected the first term in searched results according to its POS tag. - NRC Hashtag Sentiment Lexicon9 : (Mohammad et al., 2013) collected two tweet sets containing hashtags and used the sentiment of its hashtags as the sentiment label for each tweet. In this experiment, we used both unigrams and bigrams sentiment lexicons. - NRC Sentiment140 Lexicon10 : This lexicon is generated from a collection of 1.6 million tweets with positive or negative emoticons and contains about 62, 000 unigrams, 677, 000 bigrams and 480, 000 non-contiguous pairs. We used unigrams and bigrams. Linguistic Features 1 - Word n-grams: We converted all pending words into lowercase and removed low frequency terms (≤ 5). After that, we extracted wo"
S15-2125,pak-paroubek-2010-twitter,0,0.030479,"es to classify English words along http://nlp.stanford.edu/software/lex-parser.shtml http://nltk.org 3 We convert the sentiment scores in all sentiment lexicons to the range of [−1, 1], where “-” denotes negative sentiment. 4 http://www.cs.uic.edu/liub/FBS/sentimentanalysis.html#lexicon 5 http://www.wjh.harvard.edu/inquirer/homecat.htm 738 http://anthology.aclweb.org//S/S13/S13-2.pdf#page=444 http://mpqa.cs.pitt.edu/ 8 http://sentiwordnet.isti.cnr.it/ 9 http://www.umiacs.umd.edu/saif/WebDocs/NRCHashtag-Sentiment-Lexicon-v0.1.zip 10 http://help.sentiment140.com/for-students/ 7 - POS Features: (Pak and Paroubek, 2010) found that subjective texts often contain more adjectives or adverbs and less nouns than objective texts. Therefore, the POS tags are important features for sentiment analysis. We recorded the number of nouns (the corresponding POS tags are NN, NNP, NNS and NNPS), verbs (VB, VBD, VBG, VBN, VBP and VBZ), adjectives (JJ, JJR and JJS) and adverbs (RB, RBR and RBS) in pending words. - Grammatical Relationship: The grammatical relationship usually expresses the role of words in phrase and contains certain semantic information (Zhao et al., 2014). We obtained dependency information from parse tree"
S15-2125,P14-2071,0,0.00928351,"presented LDA-based models, which incorporate aspect and sentiment analysis together to model 736 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 736–741, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics sentiments towards different aspects. (Hu and Liu, 2004; Ding et al., 2008) adopted lexicon-based approaches to detect the sentiment on different aspects. In addition, (Boiy and Moens, 2009; Jiang et al., 2011) explored the work to determine whether the reviews contain the aspect information. Unlike the above study, (Xiang et al., 2014) split the data into multiple subsets based on category distributions and then built seperate classifier for each category. Following previous work (Brun et al., 2014; Brychc´ın et al., 2014; Castellucci et al., 2014; Kiritchenko et al., 2014), a rich set of features are adopted in this work: linguistic features (e.g., ngrams, grammatical relationship, POS, negations), sentiment lexicon features (e.g., MPQA, General Inquirer, SentiWordNet, etc) and domain specific features (e.g., in-domain word list, punctuation, etc). We also performed a series of experiments to compare supervised machine lea"
S15-2125,S14-2044,1,0.814858,"ntiment140.com/for-students/ 7 - POS Features: (Pak and Paroubek, 2010) found that subjective texts often contain more adjectives or adverbs and less nouns than objective texts. Therefore, the POS tags are important features for sentiment analysis. We recorded the number of nouns (the corresponding POS tags are NN, NNP, NNS and NNPS), verbs (VB, VBD, VBG, VBN, VBP and VBZ), adjectives (JJ, JJR and JJS) and adverbs (RB, RBR and RBS) in pending words. - Grammatical Relationship: The grammatical relationship usually expresses the role of words in phrase and contains certain semantic information (Zhao et al., 2014). We obtained dependency information from parse tree and the grammatical information is denoted as a tuple, e.g., amod(surprises, great), where amod represents the dependency relationship between surprises and great (here great is a modifier). We presented two types of features: the relationship with the first word in tuple as Rel1 and with the second word as Rel2. The size of each feature set is approximately 150. - Negation Features: We collected 29 negations from Internet and designed this binary feature to record if there is negation in pending words. Domain Specifical Features - In-domain"
S16-1040,S13-2053,0,0.0253157,"olarity classification of the whole tweet. The other four subtasks are at topic level, i.e., given one topic, the sentiment polarity of tweets are classified or assigned by a two-point scale (i.e., subtask Given the character limitations on tweets, sentiment orientation classification on tweets can be regarded as a sentence-level sentiment analysis. Many researchers focus on feature engineering to improve the performance of SAT. For example, (Turian et al., 2010; Liu, 2012; Zhang et al., 2006) showed that one-hot representation on n-gram features is a relatively strong baseline. Furthermore, (Mohammad et al., 2013) proposed a state-of-the-art model which implemented several sentiment lexicons and a variety of manual features. Apart from the traditional methods, more and more researchers have paid their attention to use deep learning methods. Word embedding is one of such methods, where each word is represented as a continuous, low-dimension vector and has been applied into NLP tasks as a critical and fundamental step. Commonly, there are several types of word embedding models, e.g., Bengio proposed a Neural Probabilistic Language Model (NNLM) in (Bengio et al., 2003) to learn distributed representation"
S16-1040,S13-2052,0,0.144834,"Missing"
S16-1040,S16-1001,0,0.0456725,"eatures including sentiment lexicon, linguistic and domain specific features, and word embedding features together with supervised machine learning methods. Officially released results showed that our systems rank above average. 1 Introduction In recent years, with the emergence of social media, more and more users have shared and obtained information through microblogging websites, such as Twitter. As a result, a huge amount of available data attracts a lot of researchers. SemEval 2016 provides such a universal platform for researchers to explore in the task of Sentiment Analysis in Twitter (Nakov et al., 2016) (Task 4), which includes five subtasks grouped into two levels, i.e., sentence level and topic level. Subtask A is a sentence level task aiming at sentiment polarity classification of the whole tweet. The other four subtasks are at topic level, i.e., given one topic, the sentiment polarity of tweets are classified or assigned by a two-point scale (i.e., subtask Given the character limitations on tweets, sentiment orientation classification on tweets can be regarded as a sentence-level sentiment analysis. Many researchers focus on feature engineering to improve the performance of SAT. For exam"
S16-1040,N13-1039,0,0.0950403,"Missing"
S16-1040,P14-1146,0,0.360009,"as a continuous, low-dimension vector and has been applied into NLP tasks as a critical and fundamental step. Commonly, there are several types of word embedding models, e.g., Bengio proposed a Neural Probabilistic Language Model (NNLM) in (Bengio et al., 2003) to learn distributed representation for each word and Mikolov simplified the structure of NNLM and presented t256 Proceedings of SemEval-2016, pages 256–261, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics wo efficient log-linear models in (Mikolov et al., 2013). Moreover, (Zhang and Lan, 2015; Tang et al., 2014) further proposed learning sentiment-based word embeddings to settle SAT. Meanwhile, topicbased opinion always adheres on certain words or phrases rather than whole tweet. To address topicbased SAT, (Wang et al., 2011) used the hashtag information, (Lin and He, 2009) utilized the topic model to extract topic information from tweets and (Zhang et al., 2015) picked out related words rather than all words in whole tweet as pending words for consequential feature extraction. Previous work showed that feature engineering has a significant impact on this task. Thus, in this work, we presented multip"
S16-1040,P10-1040,0,0.0525339,"4), which includes five subtasks grouped into two levels, i.e., sentence level and topic level. Subtask A is a sentence level task aiming at sentiment polarity classification of the whole tweet. The other four subtasks are at topic level, i.e., given one topic, the sentiment polarity of tweets are classified or assigned by a two-point scale (i.e., subtask Given the character limitations on tweets, sentiment orientation classification on tweets can be regarded as a sentence-level sentiment analysis. Many researchers focus on feature engineering to improve the performance of SAT. For example, (Turian et al., 2010; Liu, 2012; Zhang et al., 2006) showed that one-hot representation on n-gram features is a relatively strong baseline. Furthermore, (Mohammad et al., 2013) proposed a state-of-the-art model which implemented several sentiment lexicons and a variety of manual features. Apart from the traditional methods, more and more researchers have paid their attention to use deep learning methods. Word embedding is one of such methods, where each word is represented as a continuous, low-dimension vector and has been applied into NLP tasks as a critical and fundamental step. Commonly, there are several type"
S16-1040,S15-2094,1,0.757503,"and presented t256 Proceedings of SemEval-2016, pages 256–261, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics wo efficient log-linear models in (Mikolov et al., 2013). Moreover, (Zhang and Lan, 2015; Tang et al., 2014) further proposed learning sentiment-based word embeddings to settle SAT. Meanwhile, topicbased opinion always adheres on certain words or phrases rather than whole tweet. To address topicbased SAT, (Wang et al., 2011) used the hashtag information, (Lin and He, 2009) utilized the topic model to extract topic information from tweets and (Zhang et al., 2015) picked out related words rather than all words in whole tweet as pending words for consequential feature extraction. Previous work showed that feature engineering has a significant impact on this task. Thus, in this work, we presented multiple types of traditional NLP features to perform SAT, e.g., sentiment lexicon features (e.g., MPQA, IMDB, Bing Liu opinion lexicon, etc), linguistic features (e.g., negations, n-gram at the word level and character level, etc) and tweet specific features (e.g., emoticons, capital words, elongated words, hashtags, etc,). Besides, the word embedding features"
S16-1058,S15-2124,0,0.0629537,"Missing"
S16-1058,S15-2130,0,0.0275832,"is to identify the E#A pairs and the second phase is to assign the sentiment label (positive, negative, neutral or conflict) for each detected E#A. The conflict label is assigned when the dominant sentiment of the opinion is not clear. In previous work, (Kim et al., 2013) presented a hierarchical aspect sentiment model to classify the polarity of aspect terms from unlabeled online reviews. (Jim´enez-Zafra et al., 2015) proposed a syntactic approach for identifying the words that modify each aspect. (Branavan et al., 2009; He et al., 2012; Mei et al., 2007) used topic or category information. (Saias, 2015) used a 3-class classifier and 361 Proceedings of SemEval-2016, pages 361–366, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics some handcrafted features to perform ABSA. (Lin and He, 2009; Jo and Oh, 2011) presented LDAbased models, which incorporated aspect and sentiment analysis together to model sentiments towards different aspects. Unlike these work, which try to extract features from the whole sentence, we propose a method which just takes certain fragments related to the given aspect from the sentence into consideration to perform feature enginee"
S16-1073,W11-1701,0,0.0163642,"marized as supervised subtask (i.e., subtask A) and weakly supervised subtask (i.e., subtask B). The supervised subtask is to test the stances of certain tweets towards five predefined targets with labeled training data, while the weak supervised subtask is to detect the stances of tweets towards one target with the aid of a mass of unlabeled training data. Somasundaran showed that the stance classifier trained on unigram is a relatively strong baseline (Somasundaran and Wiebe, 2010). Based on Somasundaran and Wiebe’s work, Anand augmented the n-gram features with several linguistic features (Anand et al., 2011). Except for feature engineering, many researchers focused on other methods to improve performance. For example, Murakami and Sridhar took the forward posts of current post into consideration (Murakami and Raymond, 2010; Sridhar et al., 2014). The previous works usually processed the posts with co-posts or some additional information, such as its author, writing timeline (Faulkner, 2014; Rajadesingan and Liu, 2014; 451 Proceedings of SemEval-2016, pages 451–457, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Hasan and Ng, 2013). Differ from these work"
S16-1073,I13-1191,0,0.016631,"al linguistic features (Anand et al., 2011). Except for feature engineering, many researchers focused on other methods to improve performance. For example, Murakami and Sridhar took the forward posts of current post into consideration (Murakami and Raymond, 2010; Sridhar et al., 2014). The previous works usually processed the posts with co-posts or some additional information, such as its author, writing timeline (Faulkner, 2014; Rajadesingan and Liu, 2014; 451 Proceedings of SemEval-2016, pages 451–457, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Hasan and Ng, 2013). Differ from these works, the DST focuses on classifying the stance of tweets into three classes, i.e., Favor, Against and None, rather than two classes. Moreover, the organizers did not provide the related information of tweet, such as author information. Thus, to address this task, we decomposed the stance detecting model into two parts, i.e., relevance detection and orientation detection, which aim at determining whether the tweet is relevant or irrelevant to the given target and whether the tweet is in favor of or against the given target. Since the given 6 targets belong to different typ"
S16-1073,P03-1054,0,0.0201007,"ssifier. As for subtask B, we combined all labeled data in subtask A as training data and constructed two classifiers to perform relevance detection and orientation detection. 2.1 Data Preprocessing Due to the irregular writing form of tweets, we first convert the slangs or abbreviations to their formal forms with the aid of a pre-defined dictionary downloaded from Internet1 . For example, we convert “goooooood” into “good”, “gr8” to “great”. The processed data is fed into CMU TweetNLP tool (Owoputi et al., 2013) to perform tokenization, POS tagging. Meanwhile, we employ Stanford Parser tool (Klein and Manning, 2003) and LDA-C (Blei et al., 2003) to implement dependency parsing and topic parsing respectively. Finally, the NLTK tool (Bird et al., 2009) is used to conduct lemmatization and stemming. 2.2 Feature Engineering Since we decompose the stance detecting task into two steps, i.e., relevance detection and orientation detection, this task is related to similarity evaluation and stance orientation classification. Thus, we extract five types of features, i.e., Traditional Linguistic Features, Similarity Features, Topic Features, Sentiment Lexicon Features, Tweet Specific Features and Word Vector Feature"
S16-1073,C10-2100,0,0.0232559,"ng data, while the weak supervised subtask is to detect the stances of tweets towards one target with the aid of a mass of unlabeled training data. Somasundaran showed that the stance classifier trained on unigram is a relatively strong baseline (Somasundaran and Wiebe, 2010). Based on Somasundaran and Wiebe’s work, Anand augmented the n-gram features with several linguistic features (Anand et al., 2011). Except for feature engineering, many researchers focused on other methods to improve performance. For example, Murakami and Sridhar took the forward posts of current post into consideration (Murakami and Raymond, 2010; Sridhar et al., 2014). The previous works usually processed the posts with co-posts or some additional information, such as its author, writing timeline (Faulkner, 2014; Rajadesingan and Liu, 2014; 451 Proceedings of SemEval-2016, pages 451–457, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Hasan and Ng, 2013). Differ from these works, the DST focuses on classifying the stance of tweets into three classes, i.e., Favor, Against and None, rather than two classes. Moreover, the organizers did not provide the related information of tweet, such as autho"
S16-1073,N13-1039,0,0.0477184,"Missing"
S16-1073,W10-0214,0,0.0131379,"their opinions or standpoints in the past few years. Promoted by that growth, researchers have been enthusiastic about mining useful information in these abundant free texts from social platform, such as stance detection. Determining the stance expressed in a post written for certain target is a relatively new task in sentiment analysis. Classifying stance involves identifying the target of the post and determining its sentiment orientation. The general researches just focus on detecting the stance of posts where the provided posts are relevant to the given target (Somasundaran et al., 2007; Somasundaran and Wiebe, 2010). Besides, the previous work usually aims at the posts collected from forums which have co-posts as reference (Murakami et al., 2007; Agrawal et al., 2003). Some approachThe task of Detecting Stance in Tweets (DST) in SemEval 2016 aims at classifying the provided tweets into three stance classes, i.e., Favor (directly or indirectly by supporting given target), Against (directly or indirectly by opposing or criticizing given target) and None (none of the above) refer to a given target. The DST task consists of two subtasks which could be summarized as supervised subtask (i.e., subtask A) and we"
S16-1073,2007.sigdial-1.5,0,0.0131862,"people to share and express their opinions or standpoints in the past few years. Promoted by that growth, researchers have been enthusiastic about mining useful information in these abundant free texts from social platform, such as stance detection. Determining the stance expressed in a post written for certain target is a relatively new task in sentiment analysis. Classifying stance involves identifying the target of the post and determining its sentiment orientation. The general researches just focus on detecting the stance of posts where the provided posts are relevant to the given target (Somasundaran et al., 2007; Somasundaran and Wiebe, 2010). Besides, the previous work usually aims at the posts collected from forums which have co-posts as reference (Murakami et al., 2007; Agrawal et al., 2003). Some approachThe task of Detecting Stance in Tweets (DST) in SemEval 2016 aims at classifying the provided tweets into three stance classes, i.e., Favor (directly or indirectly by supporting given target), Against (directly or indirectly by opposing or criticizing given target) and None (none of the above) refer to a given target. The DST task consists of two subtasks which could be summarized as supervised s"
S16-1073,W14-2715,0,0.0426914,"Missing"
S16-1080,S15-2102,0,0.015521,"ases formed by a word and a modifier, where a modifier can be a negator, an auxiliary verb, a degree adverb, or even a combination of those above modifiers, e.g., “would be very easy”, “did not harm”, and “would have been nice”. The second English Twitter Mixed Polarity Set contains phrases made up of opposite polarity terms, such as “lazy sundays”, “best winter break”, “couldn’t stop smiling”, etc. The official evaluation measure is Kendall correlation coefficient (Lindskog et al., 2003). In previous work, the task was treated as a regression problem, the word embedding is used as a feature (Amir et al., 2015). In addition, (Hamdan et al., 2015) adopted unsupervised approach by using 491 Proceedings of SemEval-2016, pages 491–496, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Training Data Query Reconstructing Data Preprocessing Feature Engineering Ranking Model Test Results Test Data Figure 1: The framework of our proposed system. several sentiment lexicons for computing the score for each twitter term and ranked them. In this paper, we treated this task as a ranking problem, and used pair-wise strategy to train the model. The rest of the paper is organi"
S16-1080,S15-2095,0,0.0137332,"er, where a modifier can be a negator, an auxiliary verb, a degree adverb, or even a combination of those above modifiers, e.g., “would be very easy”, “did not harm”, and “would have been nice”. The second English Twitter Mixed Polarity Set contains phrases made up of opposite polarity terms, such as “lazy sundays”, “best winter break”, “couldn’t stop smiling”, etc. The official evaluation measure is Kendall correlation coefficient (Lindskog et al., 2003). In previous work, the task was treated as a regression problem, the word embedding is used as a feature (Amir et al., 2015). In addition, (Hamdan et al., 2015) adopted unsupervised approach by using 491 Proceedings of SemEval-2016, pages 491–496, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Training Data Query Reconstructing Data Preprocessing Feature Engineering Ranking Model Test Results Test Data Figure 1: The framework of our proposed system. several sentiment lexicons for computing the score for each twitter term and ranked them. In this paper, we treated this task as a ranking problem, and used pair-wise strategy to train the model. The rest of the paper is organized as follows. Section 2 elaborates"
S16-1080,S13-2053,0,0.0632538,"ssociated with positive sentiment, for English words or phrases. Multiple linguistic and sentiment features are adopted, e.g., Sentiment Lexicons, Sentiment Word Vectors, Word Vectors, Linguistic Features, etc. Officially released results showed that our systems rank the 1st among all submissions in English, which proves the effectiveness of the proposed method. 1 Introduction The study of sentiment analysis is increasingly drawing attention of Natural Language Processing (NLP). Many of the top performing sentiment analysis systems rely on sentiment lexicon (Tan et al., 2008; Na et al., 2009; Mohammad et al., 2013). A sentiment lexicon is a list of words and phrases, such as “excellent”, “awful” and “not bad”, each of them is assigned with a positive or negative score reflecting its sentiment polarity and strength (Tang et al., 2014a). Higher scores indicate stronger sentiment strength. However, many existing manually generated sentiment lexicons consist of lexicons with only sentiment orientation rather than sentiment strength. For example, the words in BL (Ding et al., 2008) are generally divided to two classes, i.e., The task of Determining Sentiment Intensity of English and Arabic Phrases intends to"
S16-1080,D14-1162,0,0.0787121,"Missing"
S16-1080,C14-1018,0,0.117388,"sed results showed that our systems rank the 1st among all submissions in English, which proves the effectiveness of the proposed method. 1 Introduction The study of sentiment analysis is increasingly drawing attention of Natural Language Processing (NLP). Many of the top performing sentiment analysis systems rely on sentiment lexicon (Tan et al., 2008; Na et al., 2009; Mohammad et al., 2013). A sentiment lexicon is a list of words and phrases, such as “excellent”, “awful” and “not bad”, each of them is assigned with a positive or negative score reflecting its sentiment polarity and strength (Tang et al., 2014a). Higher scores indicate stronger sentiment strength. However, many existing manually generated sentiment lexicons consist of lexicons with only sentiment orientation rather than sentiment strength. For example, the words in BL (Ding et al., 2008) are generally divided to two classes, i.e., The task of Determining Sentiment Intensity of English and Arabic Phrases intends to automatically create a sentiment lexicon with real-valued scores indicating the intensity of sentiment. The purpose of this task is to test the ability of an automatic system to predict a sentiment intensity score for a w"
S16-1080,P14-1146,0,0.241608,"sed results showed that our systems rank the 1st among all submissions in English, which proves the effectiveness of the proposed method. 1 Introduction The study of sentiment analysis is increasingly drawing attention of Natural Language Processing (NLP). Many of the top performing sentiment analysis systems rely on sentiment lexicon (Tan et al., 2008; Na et al., 2009; Mohammad et al., 2013). A sentiment lexicon is a list of words and phrases, such as “excellent”, “awful” and “not bad”, each of them is assigned with a positive or negative score reflecting its sentiment polarity and strength (Tang et al., 2014a). Higher scores indicate stronger sentiment strength. However, many existing manually generated sentiment lexicons consist of lexicons with only sentiment orientation rather than sentiment strength. For example, the words in BL (Ding et al., 2008) are generally divided to two classes, i.e., The task of Determining Sentiment Intensity of English and Arabic Phrases intends to automatically create a sentiment lexicon with real-valued scores indicating the intensity of sentiment. The purpose of this task is to test the ability of an automatic system to predict a sentiment intensity score for a w"
S16-1094,S15-2045,0,0.022513,"Missing"
S16-1094,S12-1059,0,0.0459056,"Missing"
S16-1094,N13-1092,0,0.0423899,"Missing"
S16-1094,S12-1061,0,0.109327,"Missing"
S16-1094,P14-5010,0,0.014163,"nslation similarity and alignment measures). In this work, we also present our highly interpretable and hyper-parameter free word embedding features from macro and micro views to boost the performance. 2.1 Preprocessing Several text preprocessing operations are performed before feature engineering: 1) Converting the contractions to their formal writing, e.g., “doesn’t” is rewritten as “does not”. 2) The WordNetbased Lemmatizer implemented in Natural Language Toolkit1 is used to lemmatize all words to their nearest based forms in WordNet, e.g., “was” is lemmatized to “be”. 3) Stanford CoreNLP (Manning et al., 2014) is adopted to get the Part-OfSpeech (POS) tag and Named Entity Recognition (NER) tag. 1 http://www.nltk.org 622 2.2 2.2.1 Traditional NLP Feature Engineering String-Based Similarity Length Features (len): We record the length information of given sentence pairs using the following eight measure functions: |A|, |B|, |A − B|, |B − |B−A| A|, |A ∪ B|, |A ∩ B|, |A−B| |B |, |A |, where |A |stands for the number of non-repeated words in sentence A. Syntactic Features (pos): Since two sentences with similar syntax structure convey similar meaning, we estimate the similarities of syntax structure. We"
S16-1094,P08-1028,0,0.0601493,"i.e., WordNet), to get the relations of words. Then Personalized PageRank is applied on the Lexical Knowledge Base (LKB) to rank the vertices of the LKB. The details of the method are described in Agirre et al. (2015b). It outputs a ranking vector of the sentence over KB nodes and the values of the weights are normalized so that all link weights of particular headword sum to one. Finally, we calculate the Cosine, Manhattan, Euclidean, Jaccard of the two sentence vectors. Vector Space Sentence Similarity (lsa): This measure is motivated by the idea of compositionality of distributional vector (Mitchell and Lapata, 2008). we adopt two distributional word sets released by ˇ c et al., 2012), where Latent SemanTakeLab (Sari´ tic Analysis (LSA) was performed on the New York Times Annotated Corpus (NYT)2 and Wikipedia. Then two strategies are used to convert the distributional meaning of words to sentence level: 1). simply summing up. 2). using tf to weigh each word vector. 2.2.4 Alignment Measures (Sultan et al., 2015) used delicate word aligner to compute proportion of aligned words across the two input sentences. It aligned words based on their semantic similarity in the two sentences, as well as the similarity"
S16-1094,D14-1162,0,0.0926248,"Missing"
S16-1094,S15-2027,0,0.170934,"common functions to calculate similarities over string sequences extracted from original strings, e.g., lemma, stem, or n-grams sequences; 2) corpus based ˇ c et al., 2012) where distributional similarity (Sari´ models such as Latent Semantic Analysis (LSA), are used to derive the distributional vectors of words from a large corpus according to their occurrence patterns, afterwards, similarities of sentence pairs are calculated using these vectors; 3) knowledge based method (Agirre et al., 2015b) which estimates the similarities with the aid of external resources, such as WordNet. Among them, Sultan et al. (2015) leverage different word alignment strategies to bring word-level similarity to sentence-level similarity. This paper presents our submissions for semantic textual similarity task in SemEval 2016. Based on several traditional features (i.e., string-based, corpus-based, machine translation similarity and alignment metrics), we leverage word embedding from macro (i.e., first get representation of sentence, then measure the similarity of sentence pair) and micro views (i.e., measure the similarity of word pairs separately) to boost performance. Due to the various domains of training data and test"
S16-1094,P10-1040,0,0.0623269,"O of words. The formula is described as Equation 4. 2 −INFO(w1 2 ) − INFO(w2 2 ) − . . . − INFO(wlen(S ) 2) (4) Our goal is to incorporate the semantic similarity between each word pairs into the micro information distance of sentence. Here, we adopted word mover’s distance (Kusner et al., 2015), the minimum cumulative distance that all word in sentence 1 need to travel to exactly match sentence 2, showed in Figure 2. For more details, see Kusner et al. (2015). Word Embedding Features: Zhao et al. (2014) shows that heterogenous feature outperform a single feature, and we use three embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) as our initial word vector input. Incidentally, the distance is substitutable, and we replace it with different measurements (i.e., cosine distance, Manhanttan distance, Euclidean distance, Pearson coefficient, Spearman coefficient, Kendall tau coefficient). Specially, because of the high time complexity of word mover’s distance, we only train it on word2vec (Mikolov et al., 2013), although other embeddings are also plausible. 3 Experiments 3.1 Datasets We collect all the datasets from 2012 to 2015 as training data. Each dataset consists of a nu"
S16-1094,S12-1060,0,0.0476633,"Missing"
S16-1094,S14-2044,1,0.929277,"lment, text summarization etc. Therefore, Semantic Textual Similarity (STS) has received an increasing amount of attention in recent years, e.g., the STS tasks in Semantic Evaluation Exercises have been held from 2012 to 2016. To identify semantic similarity of sentence pairs, most existing works adopt at least one of the following feature types: 1) string based similarity (B¨ar et Traditional NLP feature engineering often treat sentence as a bag of words or term frequency, and endeavor to evaluate the similarity according to the co-occurrence of words or other replacement words. For example, Zhao et al. (2014) built a supervised model using ensemble of heterogeneous features and achieved great performance on STS Task 2014. However, it is difficult to evaluate semantic relatedness if all the word in both sentences is unique. For example: A storm will spread snow over Shanghai; The earthquakes have shaken parts of Oklahoma. These sentences have no words in common, although they convey the similar information. In this work, we first borrow the aforementioned effective types of similarity measurements including string-based, corpus-based, machine translation similarity and alignment measures to capture"
S16-1135,J93-2003,0,0.0624948,"proportions of co-occurred words between a given sentence pair. Given a Q-Q pair, this feature type is calculated using five measures: |Q0 ∩ Q1 |, |Q0 ∪ Q1 |/|Q0 |, |Q0 ∩ Q1 |/|Q1 |, |Q1 − Q0 |/|Q1 |, |Q0 − Q1 |/|Q0 |, where |Q0 |and |Q1 |denote the number of the words of Q0 and Q1 . Translation Based Feature (TB): The above WM feature only considers the overlapped words between Q0 and Q1 and thus it may fail to “bridge the lexical gap” between Q-Q pair. One possible solution is to regard this task as a statistic machine translation problem between question and answer by using the IBM Model 1(Brown et al., 1993) to learn the word-to-word probabilities. Following (Xue et al., 2008; Surdeanu et al., 2011), we regarded P (Q0 |Q1 ), i.e., the translation probability of Q1 when given Q0 , as a translation based feature. The probabilities are calculated as: P (Q0 |Q1 ) = Y where P (Q0 |Q1 ) is the probability that the Q0 word w is generated from Q1 , λ is a smoothing parameter, C is a background collection. Pml (w|C) is computed by maximum likelihood estimator. P (w|a) denotes the translation probability from Q1 word a to Q0 word w. The GIZA++ Toolkit1 is used to compute these probabilities. Topic Model Ba"
S16-1135,S16-1083,0,0.0962944,"Missing"
S16-1135,D14-1162,0,0.0786295,"Missing"
S16-1135,J11-2003,0,0.0282316,"ture type is calculated using five measures: |Q0 ∩ Q1 |, |Q0 ∪ Q1 |/|Q0 |, |Q0 ∩ Q1 |/|Q1 |, |Q1 − Q0 |/|Q1 |, |Q0 − Q1 |/|Q0 |, where |Q0 |and |Q1 |denote the number of the words of Q0 and Q1 . Translation Based Feature (TB): The above WM feature only considers the overlapped words between Q0 and Q1 and thus it may fail to “bridge the lexical gap” between Q-Q pair. One possible solution is to regard this task as a statistic machine translation problem between question and answer by using the IBM Model 1(Brown et al., 1993) to learn the word-to-word probabilities. Following (Xue et al., 2008; Surdeanu et al., 2011), we regarded P (Q0 |Q1 ), i.e., the translation probability of Q1 when given Q0 , as a translation based feature. The probabilities are calculated as: P (Q0 |Q1 ) = Y where P (Q0 |Q1 ) is the probability that the Q0 word w is generated from Q1 , λ is a smoothing parameter, C is a background collection. Pml (w|C) is computed by maximum likelihood estimator. P (w|a) denotes the translation probability from Q1 word a to Q0 word w. The GIZA++ Toolkit1 is used to compute these probabilities. Topic Model Based Feature (TMB): We used the LDA (Blei et al., 2003) model to transform Q0 and Q1 into topi"
S16-1135,P13-1171,0,0.0426758,"word w is generated from Q1 , λ is a smoothing parameter, C is a background collection. Pml (w|C) is computed by maximum likelihood estimator. P (w|a) denotes the translation probability from Q1 word a to Q0 word w. The GIZA++ Toolkit1 is used to compute these probabilities. Topic Model Based Feature (TMB): We used the LDA (Blei et al., 2003) model to transform Q0 and Q1 into topic-based vectors and then took the cosine value of two topic vectors as feature. We use the GibbsLDA++ (Phan and Nguyen, 2007) Toolkit to train the topic model. Lexical Semantic Similarity Feature (LSS): Inspired by (Yih et al., 2013), we included the lexical semantic similarity features in our model. We used three different word vectors to represent LSS feature, i.e., the 300-dimensional version of word2vec (Mikolov et al., 2013) vectors, 300-dimensional Glove vectors (Pennington et al., 2014) and 300dimensional vectors which are pre-trained with the unsupervised neural language model (Mikolov et al., 2013) on the Qatar Living data 2 . Words not present in the set of pre-trained words are initialized randomly. There are two ways to calculate the LSS features. One is to calculate the cosine similarity by summing up all wor"
S17-2028,D14-1162,0,0.104934,"Missing"
S17-2028,S16-1091,0,0.0605867,"Missing"
S17-2028,S16-1103,0,0.0274219,"Missing"
S17-2028,S15-2027,0,0.0319514,"r. Note that the stopwords are removed and each word is lemmatized so as to estimate sequence similarity more accurately. As a result, we get 5 features. Syntactic Parse Features: In order to model tree structured similarity between two sentences rather than sequence-based similarity, inspired by Moschitti (2006), we adopt tree kernels to calculate the similarity between two syntactic parse trees. In particular, we calculate the number of common substructures in three different kernel spaces, i.e., subtree (ST), subset tree (SST), partial tree (PT). Thus we get 3 features. Alignment Features: Sultan et al. (2015) used word aligner to align matching words across a pair of sentences, and then computes the proportion of aligned words as follows: Gradient Boosting Single Sentence Features Bag-of-Words BOW Features Bag-ofDependencyTripes Dependency Features XGBoost Kernels Pooled Word Embeddings Word Embedding Features E n s e m b l e Deep Learning Sentence Representation Averaging Word Vectors Neural Network Dense+Softmax Projected Averaging Word Vectors (ƶ-ƶ,ƶ×ƶ) Deep Averaging Network Long Short-Term Memory Network ƶ ƶ 6 6 Figure 1: The system architecture Traditional NLP Module is to extracts two kin"
S17-2028,S17-2001,0,0.145879,"Missing"
S17-2028,P15-1150,0,0.00769724,"y network (LSTM, Hochreiter and Schmidhuber (1997)) to capture long-distance dependencies information. In order to obtain the vector of sentence pair, given two single sentence vectors, we first use a element-wise subtraction and a multiplication and then concatenate the two values as the final vector of sentence pair representation. At last, we use a fully-connected neural network and output the probability of similarity based on a softmax function. Thus we obtain 4 deep learning based scores. To learn model parameters, we minimize the KL-divergence between the outputs and gold labels, as in Tai et al. (2015). We adopt Adam (Kingma and Ba, 2014) as optimization method and set learning rate of 0.01. Track 4b SP-EN-WMT, the performance is very poor. So we perform 10 − f old cross validation (CV) on Track 4b SP-EN-WMT. Preprocessing: All sentences are translated into English via Google Translator. The Stanford CoreNLP (Manning et al., 2014) is used for tokenization, lemmatization, POS tagging and dependency parsing. Evaluation: For Track 1 to Track 6, Pearson correlation coefficient is used to evaluate each individual test set. For Primary Track, since it is achieved by submitting results of all the"
S17-2028,S16-1089,0,0.0492112,"Missing"
S17-2028,S12-1060,0,0.0938589,"Missing"
S17-2028,P15-1162,0,0.0300464,"Missing"
S17-2028,S15-2021,1,0.845385,"n(S2 ) where na (S) and n(S) is the number of aligned and non-repeated words in sentence S. To assign appropriate weights to different words, we adopt two weighting methods: i) weighted by five POS tags (i.e., noun, verb, adjective, adverb and others; we first group words in two sentences into 5 POS categories, then for each POS category we compute the proportion of aligned words, and we get 5 features as a result. ii) weighted by IDF values (calculated in each dataset separately). Totally, we collect 7 alignment features. MT based Features: Following previous work in (Zhao et al., 2014) and (Zhao et al., 2015), we use MT evaluation metrics to measure the semantic equivalence of the given sentence pairs. Nine Traditional NLP Module In this section, we give the details of feature engineering and learning algorithms. 2.1.1 |S1 | |S2 | + )−1 |S1 ∩ S2 ||S1 ∩ S2 | Sentence Pair Matching Features Five types of sentence pair matching features are designed to directly calculate the similarity of two sentences based on the overlaps of character/word/sequence, syntactic structure, alignment and even MT metrics. 192 Type Measures Cosine distance, Manhanttan distance, linear kernel Euclidean distance, Chebyshev"
S17-2028,P14-5010,0,0.00383595,"ion. At last, we use a fully-connected neural network and output the probability of similarity based on a softmax function. Thus we obtain 4 deep learning based scores. To learn model parameters, we minimize the KL-divergence between the outputs and gold labels, as in Tai et al. (2015). We adopt Adam (Kingma and Ba, 2014) as optimization method and set learning rate of 0.01. Track 4b SP-EN-WMT, the performance is very poor. So we perform 10 − f old cross validation (CV) on Track 4b SP-EN-WMT. Preprocessing: All sentences are translated into English via Google Translator. The Stanford CoreNLP (Manning et al., 2014) is used for tokenization, lemmatization, POS tagging and dependency parsing. Evaluation: For Track 1 to Track 6, Pearson correlation coefficient is used to evaluate each individual test set. For Primary Track, since it is achieved by submitting results of all the secondary sub-tracks, a macro-averaged weighted sum of all correlations on sub-tracks is used for evaluation. 2.3 A series of comparison experiments on English STS 2016 training set have been performed to explore different features and algorithms. 4 Ensemble Module The NLP-based scores and the deep learning based scores are averaged"
S17-2028,S14-2044,1,0.134401,"1 ) + na (S2 ) n(S1 ) + n(S2 ) where na (S) and n(S) is the number of aligned and non-repeated words in sentence S. To assign appropriate weights to different words, we adopt two weighting methods: i) weighted by five POS tags (i.e., noun, verb, adjective, adverb and others; we first group words in two sentences into 5 POS categories, then for each POS category we compute the proportion of aligned words, and we get 5 features as a result. ii) weighted by IDF values (calculated in each dataset separately). Totally, we collect 7 alignment features. MT based Features: Following previous work in (Zhao et al., 2014) and (Zhao et al., 2015), we use MT evaluation metrics to measure the semantic equivalence of the given sentence pairs. Nine Traditional NLP Module In this section, we give the details of feature engineering and learning algorithms. 2.1.1 |S1 | |S2 | + )−1 |S1 ∩ S2 ||S1 ∩ S2 | Sentence Pair Matching Features Five types of sentence pair matching features are designed to directly calculate the similarity of two sentences based on the overlaps of character/word/sequence, syntactic structure, alignment and even MT metrics. 192 Type Measures Cosine distance, Manhanttan distance, linear kernel Eucli"
S17-2060,S16-1135,1,0.847975,"tor of one-hot forMatch, Topic Model based, and Lexical Semantic Similarity features, we also extracted Search Engine Extensional feature. For subtask C, we ranked the comments by multiplying the probability of the pair /relevant question õ comment0 being Good by the reciprocal rank of the related question. 2.1 Features Engineering All three subtasks can be regarded as an estimation task of sentence semantic measures which can be modeled by various types of features. Besides Word Match, Topic Model Based, Lexical Semantic Similarity, and Comment Information Features used in our previous work (Wu and Lan, 2016), we also extract three types of novel features, i.e., Meta Data Features, Google Ranking Feature, and Search Engine Extensional Features. The details of features are described as follows. Here we took the Q-Q pair for example. Word Matching Feature (WM): Inspired by the work of (Zhao et al., 2015), we adopt word matching feature in our system. This feature represents the the proportions of co-occurred words that between a given sentence pair. Given a QQ pair, this feature is expressed in the following nine measures:|Q0 ∩ Q1 |, |Q0 ∩ Q1 |/|Q0 |, |Q0 ∩ Q1 |/|Q1 |, |Q1 − Q0 |/|Q1 |, |Q0 − Q1 |/|"
S17-2060,P13-1171,0,0.504954,"as question retrieval), which is to retrieve the simi2 Systems Description For subtask A, we presented two different methods i.e., using traditional linguistic features and learning a CNN model to represent question and comment sentences. For subtask B, besides Word 365 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 365–369, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics lexical semantic similarity feature in our model. Two types of 300-dimensional vectors are pretrained on Qatar Living data with word2vec (Yih et al., 2013b) and Glove (Pennington et al., 2014) toolkits. We select the maximum, minimum and average values for each dimension of words vectors to make up a vector to represent the sentence. After obtained the vector representation of Q0 and Q1 , we also calculated the nine distance measures mentioned in TMB. Note that all above three types of features are adopted in both answer ranking and question retrieval tasks. Search Engine Extensional Feature (SEE): We first got two lists of 10 snippets returned by search engine (i.e., Google, Bing) with the subjects of original question Q0 and related question"
S17-2060,P08-1019,0,0.021664,"matching feature in our system. This feature represents the the proportions of co-occurred words that between a given sentence pair. Given a QQ pair, this feature is expressed in the following nine measures:|Q0 ∩ Q1 |, |Q0 ∩ Q1 |/|Q0 |, |Q0 ∩ Q1 |/|Q1 |, |Q1 − Q0 |/|Q1 |, |Q0 − Q1 |/|Q0 |, |Q0 ∩ Q1 |/|Q0 − Q1 |, |Q0 ∩ Q1 |/|Q1 − Q0 |, |Q0 ∩ Q1 |/|Q0 ∪Q1 |, 2∗|Q0 ∩Q1 |/(|Q0 |+|Q1 |), where |Q0 |and |Q1 |are the number of the words of Q0 and Q1 . Topic Model based Feature (TMB): Topic model based feature has been proved beneficial for question retrieval and answer ranking tasks by the work of (Duan et al., 2008; Qin et al., 2009). We use the GibbsLDA++ (Phan and Nguyen, 2007) Toolkit with 100,000 random sampling question and answer pairs from Qatar Living data to train the topic model. In training and test phase, Q0 and Q1 are transformed into an 100-dimensional topicbased vectors using pre-trained topic model. After that we calculate the cosine similarity, Manhattan distance and Euclidean distance between these two vectors and regard the scores as TMB feature. Inspired by the work of (Filice et al., 2016), we also adopt four kinds of nonlinear kernel functions to calculate the distance between two"
S17-2060,S16-1172,0,0.0948099,"eature has been proved beneficial for question retrieval and answer ranking tasks by the work of (Duan et al., 2008; Qin et al., 2009). We use the GibbsLDA++ (Phan and Nguyen, 2007) Toolkit with 100,000 random sampling question and answer pairs from Qatar Living data to train the topic model. In training and test phase, Q0 and Q1 are transformed into an 100-dimensional topicbased vectors using pre-trained topic model. After that we calculate the cosine similarity, Manhattan distance and Euclidean distance between these two vectors and regard the scores as TMB feature. Inspired by the work of (Filice et al., 2016), we also adopt four kinds of nonlinear kernel functions to calculate the distance between two vectors, i.e., ”polynomial”, ”rbf”, ”laplacian” and ”sigmoid”. Lexical Semantic Similarity Feature (LSS): Inspired by (Yih et al., 2013a), we included the 366 m for each comment. (2) comment ner feature, we extracted nine types of name entity information in the comment, i.e., ”Duration”, ”Location”, ”Person”, ”Organization”, ”Percent”, ”Ordinal”, ”Time”, ”Date”, and ”Money” with the CoreNLP tool, generating a nine-dimensional one-hot forming vector. (3) comment special characters feature, We extracte"
S17-2060,S17-2003,0,0.0340973,"Missing"
S17-2060,D14-1162,0,0.0821803,"is to retrieve the simi2 Systems Description For subtask A, we presented two different methods i.e., using traditional linguistic features and learning a CNN model to represent question and comment sentences. For subtask B, besides Word 365 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 365–369, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics lexical semantic similarity feature in our model. Two types of 300-dimensional vectors are pretrained on Qatar Living data with word2vec (Yih et al., 2013b) and Glove (Pennington et al., 2014) toolkits. We select the maximum, minimum and average values for each dimension of words vectors to make up a vector to represent the sentence. After obtained the vector representation of Q0 and Q1 , we also calculated the nine distance measures mentioned in TMB. Note that all above three types of features are adopted in both answer ranking and question retrieval tasks. Search Engine Extensional Feature (SEE): We first got two lists of 10 snippets returned by search engine (i.e., Google, Bing) with the subjects of original question Q0 and related question Q1 as query. Then we counted the frequ"
S17-2078,S17-2005,0,0.057566,"ated on sense vectors or cluster center vectors. For subtask 2, we established an unsupervised system to locate the pun by scoring each word in the sentence and we assumed that the word with the smallest score is the pun. 1 Introduction A pun is a form of wordplay in which one signifier (e.g., a word or phrase) suggests two or more meanings by exploiting polysemy, or phonological similarity to another signifier, for an intended humorous or rhetorical effect. The study of puns can be seen as a respectable research topic in traditional linguistics and the cognitive sciences. Semeval 2017 task 7(Miller et al., 2017) contains three subtasks, i.e., pun detection, pun location, and pun interpretation. And we participated in the first two subtasks. The detection and location of English puns are to determine whether or not a sentence contains a pun and which word is a pun respectively, which differ from traditional word sense disambiguation (WSD). WSD is to determine an exact meaning of the target word in the given context. However, WSD algorithms could provide the lexical-semantic understanding for pun detection and location. And we adopted a knowledge-based WSD algorithm to obtain possible senses1 for each"
S17-2086,D14-1162,0,0.0814337,"are binary features. The aforementioned features form a 9-dimensions feature. looking up pre-trained GoogleW2V, then grouped into 80 clusters. Thus we adopted 80-dimensions binary feature to mark whether the words of a certain cluster appeared in the tweet. Word Vector Features A lot of recent studies on NLP applications are reported to have good performance using word vectors, such as ducument classification (Sebastiani, 2002), parsing (Socher et al., 2013), and question answering (Lan et al., 2016a), We adopted two widely-used word vectors, i.e., GoogleW2V (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, semantic word vectors find similar words with similar context rather than similar sentiment information. Several recent works focused on sentiment word vectors using neural network based models (Lan et al., 2016b). In this work, we also adopted two sentiment word vectors, one is SSWE (Tang et al., 2014) and the other is a home-made sentiment word vector from our previous work. To obtain the representation of a tweet, for each word in a tweet, we concatenated the maximum, minimum and mean of each dimension as a tweet vector [min-max-mean]. 2.2 Learning algorithms and Evaluation metri"
S17-2086,P14-1146,0,0.0364372,"ns are reported to have good performance using word vectors, such as ducument classification (Sebastiani, 2002), parsing (Socher et al., 2013), and question answering (Lan et al., 2016a), We adopted two widely-used word vectors, i.e., GoogleW2V (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, semantic word vectors find similar words with similar context rather than similar sentiment information. Several recent works focused on sentiment word vectors using neural network based models (Lan et al., 2016b). In this work, we also adopted two sentiment word vectors, one is SSWE (Tang et al., 2014) and the other is a home-made sentiment word vector from our previous work. To obtain the representation of a tweet, for each word in a tweet, we concatenated the maximum, minimum and mean of each dimension as a tweet vector [min-max-mean]. 2.2 Learning algorithms and Evaluation metrics Based on above multiple features, we explored several learning algorithms to build classification models, e.g., Logistic Regression (LR), supplied in liblinear tools6 , Support Vector Machines (SVM), Decision Trees (DT), Random Forests (RF), AdaBoost (ADB), and Gradient Tree Boosting (GDB), implemented in sciki"
S17-2152,P14-5010,0,0.00533602,"ve samples in the training data. Then we calculate the weight (i.e., rf ) for each token in unigram, bigram and trigram as follows:  rf = max ln(2 + Since the differences between the spans and the title described in section 1, for subtask 2, we replace the target company with “TCOMPANY” and replace other company with “OCOMPNAY” in the title. For both subtasks, the subsequent preprocessing is the same. We firstly replace all URLs with “url” and transform the abbreviations, punctuation with a special format, slangs and elongated words to their normal format. Then, we use Stanford CoreNLP tools(Manning et al., 2014) for tokenization, POS tagging, named entity recognizing (NER) and parsing. Finally, the WordNet-based Lemmatizer implemented in NLTK1 is adopted to lemmatize words to their base forms with the aid of their POS tags. And the word stemmer based on the Porter stemming algorithm and implemented in NLTK is adopted to remove morphological affixes from lemmatized words. Feature Engineering We extract the following four types of features to construct supervised regression models for two subtasks, i.e., linguistic features, sentiment lexicon features, domain-specific features and word embedding featur"
S17-2152,S16-1073,1,0.649296,"e are no symbols and characters before and after the number). Keyword+Number: Based on the Number features, we defined 4-dimensional Keyword+Number features to indicate whether there 2.2.4 4 http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html http://www.wjh.harvard.edu/inquirer/homecat.htm 6 http://mpqa.cs.pitt.edu/ 7 http://www2.imm.dtu.dk/pubdb/views/publication details .php?id=6010 8 http://sentiwordnet.isti.cnr.it/ 9 http://www.umiacs.umd.edu/saif/WebDocs/NRCHashtag-Sentiment-Lexicon-v0.1.zip 10 http://help.sentiment140.com/for-students/ 5 890 Word Embedding Features The previous work (Zhang and Lan, 2016; Jiang et al., 2016) on sentiment analysis task has proved the effectiveness of word embedding features. In this part, we utilize the Google word2vec to get the representation of the sentence. GoogleW2V: Unlike the word cluster features, the Google word2vec features (GoogleW2V) are extracted as follows: We firstly use the Google word2vec to get a 300-dimensional vector for each word in sentence. Then, the simple min, max, average pooling strategies are adopted to concatenate sentence vector representations with dimensionality of 900. 2.3 Learning Algorithms For both tasks, we explore 7 algori"
S17-2152,S13-2067,0,0.216212,"Missing"
S17-2152,S17-2089,0,0.157166,"al Microblogs and News task (i.e., Task 5) in SemEval-2017. This task includes two subtasks in microblogs and news headline domain respectively. To settle this problem, we extract four types of effective features, including linguistic features, sentiment lexicon features, domain-specific features and word embedding features. Then we employ these features to construct models by using ensemble regression algorithms. Our submissions rank 1st and rank 5th in subtask 1 and subtask 2 respectively. 1 Introduction SemEval-2017 Task 5 is Fine-Grained Sentiment Analysis on Financial Microblogs and News(Cortis et al., 2017), focusing on identifying positive (bullish; believing that the stock price will increase) and negative (bearish; believing that the stock price will decline) sentiment associated with stocks and companies from microblogs and news domains. Unlike previous sentiment analysis, in this task, the fine-grained sentiment analysis not only contains sentiment orientation (i.e., positive or negative of the sentiment score) but also sentiment strength (i.e., the value of the sentiment score) attached to a particular company or stock explicitly or implicitly expressed in given texts. Given a text instanc"
S17-2152,S16-1058,1,0.627075,"Missing"
S18-1035,S18-1001,0,0.0459373,"Missing"
S18-1035,L18-1030,0,0.0145841,"this tweet is an exclamation or question mark. Gradient Boosting Regressor (GBR) implemented in scikit-learn tools13 and XGBoost Regressor (XGB)14 . All these algorithms are used with default parameters. 3 Experiments 3.1 Dataset • Bag-of-Hashtags Hashtags reflect emotion orientation of tweets directly, so we constructed a vocabulary of hashtags appearing in the training set and development set, then adopted the bag-of-hashtags method for each tweet. The statistics of the English datasets provided by Semeval 2018 Task 1 are shown in Table 1 and 2. How the English data created is described in (Mohammad and Kiritchenko, 2018). Datasets train dev subtask 1 test subtask 2 • Emoticon We collected 67 emoticons from Internet11 , including 34 positive emoticons and 33 negative emoticons, then designed the following 4 binary features: anger 1,701 388 17,939 1,002 fear 2,252 689 17,923 986 joy 1,616 290 18,042 1,105 sadness 1,533 397 17,912 975 Table 1: The statistics of data sets for subtask 1 and 2. – to record whether the positive and negative emoticons are present in the tweet, respectively (1 for yes, 0 for no). – to record whether the last token is a positive or a negative emoticon. Subtask 3 4 5 • Intensity Words S"
S18-1035,S15-2094,1,0.744877,"ven a tweet, we calculated the following six scores: features, sentiment lexicon features, emotion lexicon features and domain-specific features. 2.2.1 Linguistic Features • Lemma unigram Considering there is similar emotion intensity expressed by “anger” and “angers”, we choose word lemma unigram features from tweets rather than word unigram features. • Negation Negation in a sentence often affects its sentiment orientation, and conveys its intensity of the sentiment. For example, a sentence with several negation words is more inclined to negative sentiment polarity. Following previous work (Zhang et al., 2015), we manually collected 29 negations2 and designed two binary features. One is to indicate whether there is any negation in the tweet and the other is to record whether this tweet contains more than one negation. – the ratio of positive words to all words. – the ratio of negative words to all words. – the maximum sentiment scores. – the minimum sentiment scores. – the sum of sentiment scores. – the sentiment score of the last word in tweet. • NER Given a tweet “@JackHoward the Christmas episode genuinely had me in tears of laughter”, it has useful information like person name and festival whic"
S18-1068,S18-1003,0,0.0242787,", we extract the following three types of features to capture effective information from the given tweets, i.e., linguistic features, sentiment lexicon features and tweet specific features. 2.1.1 Linguistic Features • N-grams: We extract 3 types of Bag-ofWords features as N-grams features, where N = 1,2,3 (i.e., unigram, bigram, and trigram features). Introduction Visual icons play a crucial role in providing information about the extra level of social media information. SemEval 2018 shared task for researchers to predict, given a tweet in English or Spanish, its most likely associated emoji (Barbieri et al., 2018, 2017) (Task 2, Multilingual Emoji Prediction), which is organized into two optional subtask (subtask 1 and subtask 2) respectively in English and Spanish. For subtask 1, we adopt a combination model to predict emojis, which consists of traditional Natural Language Processing (NLP) methods and deep learning methods. The results returned by the classifier with traditional NLP features, by the neural network model and by the combination model are voted to get the final result. For subtask 2, we only use deep learning model. 2 Traditional NLP Features • POS: Generally, the sentences carrying sub"
S18-1068,P15-1162,0,0.0893104,"Missing"
S18-1068,P14-5010,0,0.00448513,"Missing"
S18-1068,E17-2017,0,0.158018,"Missing"
S18-1098,S18-1005,0,0.0555792,"Missing"
S18-1098,P14-5010,0,0.00364884,"09). Otherwise, N-grams features with the use of Relevant Frequency (RF) (Lan et al., 2009) were also applied to this system. 600 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 600–606 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics • NER There are different types of words in tweets. NER feature can effectively express aforesaid information. The 12 types (i.e., DURATION, SET, NUMBER, LOCATION, PERSON, ORGANIATION, PERCENT, MISC, ORDINAL, TIME, DATE, MONEY) named entities are labeled by Stanford CoreNLP tools (Manning et al., 2014). We used a 12-dimensions binary feature to indicate the entities in tweets. 2.1.2 • Hashtags All the tokens begin with “#” symbol are called hashtags. We extracted all the hashtags, removed its “#” symbol and built unigram features for them. • Word N-grams in Hashtags We exploited hashtags by a small tool WordSegment11 to cut linked-together hashtags into a series of words, like ilikemonday into [‘i’, ‘like’, ‘monday’]. • Punctuation Online users often use emotion symbols (i.e., ! and ?) to express strongly feelings. Hence we extracted a 7-dimension binary features by recording the following"
S18-1098,D14-1162,0,0.087294,"Missing"
S18-1165,S18-1117,0,0.051104,"abel positive positive negative negative Table 1: Examples from the training data. ing methods which use PMI features and WordNet features. In recent years, more and more studies have focused on word embeddings as an alternative to traditional hand-crafted features (Pennington et al., 2014; Tang et al., 2014). Therefore we use word embeddings to obtain the semantic similarity as word embedding features. Besides, we perform a series of experiments to explore the effectiveness of feature types and supervised machine learning algorithms. Introduction The Capturing Discriminative Attributes task (Paperno et al., 2018) in SemEval 2018 is to provide a standard testbed for semantic difference detection, which will benefit many other applications in Natural Language Processing (NLP), such as automatized lexicography and machine translation (Krebs and Paperno, 2016). The goal of this task is to predict whether a word is a discriminative attribute between two concepts. Specifically, given two concepts and an attribute, the task is to predict whether the first concept has this attribute but the second concept does not. For example, given the concepts apple and pineapple, participants are required to predict wheth"
S18-1165,D14-1162,0,0.0889431,"fference or not. We design and investigate several word embedding features, PMI features and WordNet features together with supervised machine learning methods to address this task. Officially released results show that our system ranks above average. 1 word2 pineapple chandelier coconut cucumber attribute seeds melts brine seeds label positive positive negative negative Table 1: Examples from the training data. ing methods which use PMI features and WordNet features. In recent years, more and more studies have focused on word embeddings as an alternative to traditional hand-crafted features (Pennington et al., 2014; Tang et al., 2014). Therefore we use word embeddings to obtain the semantic similarity as word embedding features. Besides, we perform a series of experiments to explore the effectiveness of feature types and supervised machine learning algorithms. Introduction The Capturing Discriminative Attributes task (Paperno et al., 2018) in SemEval 2018 is to provide a standard testbed for semantic difference detection, which will benefit many other applications in Natural Language Processing (NLP), such as automatized lexicography and machine translation (Krebs and Paperno, 2016). The goal of this ta"
S18-1165,P13-4028,0,0.0266012,"eatures Pointwise mutual information (PMI) (Church and Hanks, 1990) is a measure of association between two things used in information theory and statistics. And in NLP, this metric can be used to measure the correlation between two words. The higher the PMI, the stronger the correlation between the two words. So we obtain the PMI features of the given triple (word1 , word2 , attribute). We record the PMI value of word1 and attribute as well as the PMI value of word2 and attribute as PMI features. The PMI values we used are calculated using Wikimedia dumps1 and directly obtained from SEMILAR (Rus et al., 2013). As a result, we get four PMI features. 2.1.3 Word Embedding Features Word embedding is a continuous-valued vector representation for each word, which usually carries syntactic and semantic information. In this work, we employ two types of word embeddings which are pre-trained word vectors downloaded from Internet with dimensionality of 300: GoogleW2V (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). The former is pretrained on News domain, available in Google2 . And the latter is pre-trained on tweets, available in GloVe3 . • WE similarity: 1 Given the triple (word1 , https://dumps"
S18-1165,P14-1146,0,0.0198255,"n and investigate several word embedding features, PMI features and WordNet features together with supervised machine learning methods to address this task. Officially released results show that our system ranks above average. 1 word2 pineapple chandelier coconut cucumber attribute seeds melts brine seeds label positive positive negative negative Table 1: Examples from the training data. ing methods which use PMI features and WordNet features. In recent years, more and more studies have focused on word embeddings as an alternative to traditional hand-crafted features (Pennington et al., 2014; Tang et al., 2014). Therefore we use word embeddings to obtain the semantic similarity as word embedding features. Besides, we perform a series of experiments to explore the effectiveness of feature types and supervised machine learning algorithms. Introduction The Capturing Discriminative Attributes task (Paperno et al., 2018) in SemEval 2018 is to provide a standard testbed for semantic difference detection, which will benefit many other applications in Natural Language Processing (NLP), such as automatized lexicography and machine translation (Krebs and Paperno, 2016). The goal of this task is to predict whe"
S18-1165,J90-1003,0,0.476851,"refore, we design the following features to record the semantic information. Given the triple (word1 , word2 , attribute), we first load all senses definitions of word1 , word2 and attribute. Then we implement four types of binary features: (1) whether attribute appears in the senses definitions of word1 , (2) whether attribute appears in the senses definitions of word2 , (3) whether word1 appears in the senses definitions of attribute and (4) whether word2 appears in the senses definitions of attribute. As a result, we get four features. 2.1.2 PMI Features Pointwise mutual information (PMI) (Church and Hanks, 1990) is a measure of association between two things used in information theory and statistics. And in NLP, this metric can be used to measure the correlation between two words. The higher the PMI, the stronger the correlation between the two words. So we obtain the PMI features of the given triple (word1 , word2 , attribute). We record the PMI value of word1 and attribute as well as the PMI value of word2 and attribute as PMI features. The PMI values we used are calculated using Wikimedia dumps1 and directly obtained from SEMILAR (Rus et al., 2013). As a result, we get four PMI features. 2.1.3 Wor"
S18-1165,W16-2509,0,0.0452318,"and-crafted features (Pennington et al., 2014; Tang et al., 2014). Therefore we use word embeddings to obtain the semantic similarity as word embedding features. Besides, we perform a series of experiments to explore the effectiveness of feature types and supervised machine learning algorithms. Introduction The Capturing Discriminative Attributes task (Paperno et al., 2018) in SemEval 2018 is to provide a standard testbed for semantic difference detection, which will benefit many other applications in Natural Language Processing (NLP), such as automatized lexicography and machine translation (Krebs and Paperno, 2016). The goal of this task is to predict whether a word is a discriminative attribute between two concepts. Specifically, given two concepts and an attribute, the task is to predict whether the first concept has this attribute but the second concept does not. For example, given the concepts apple and pineapple, participants are required to predict whether the attribute seeds characterizes the first concept but not the other. In other words, semantic difference detection is a binary classification task: given a triple (apple, pineapple, seeds), the task is to determine whether it exemplifies a sem"
S18-1175,D13-1020,0,0.180597,", this multiple-choice machine comprehension task can be expressed as a quadruple:&lt;D, Q, A, a&gt;. Where D represents a narrative text about everyday activities, Q represents a question for the content of the narrative text, A is the candidate answer choice set to the question(this task contains two candidate answers choice a0 and a1 ) and a represents the correct answer. The system is expected to select an answer from A that best answers Q according to the evidences in document D or commonsense knowledge. 2.2 Two Rule-based Baselines First of all, we implemented a rule-based system proposed in (Richardson et al., 2013), which used the sliding-window (SW) and word distance-based (WD) algorithms to calculate the answer scores according to the rules and return the highest-score answer. We also tried the improved SW and WD algorithms proposed in (Smith et al., 2015), and the system performance has improvement. Sliding-window and Word Distance-based algorithms are are described as follows: Sliding-Window: Given a data sample &lt;D, Q, a0 (or a1 ), a&gt;, firstly, we calculate the inverse word counts of each word in the document D. Then we set a window that slides word by word from the beginning of the document to the"
S18-1175,D15-1197,0,0.0214483,"e set to the question(this task contains two candidate answers choice a0 and a1 ) and a represents the correct answer. The system is expected to select an answer from A that best answers Q according to the evidences in document D or commonsense knowledge. 2.2 Two Rule-based Baselines First of all, we implemented a rule-based system proposed in (Richardson et al., 2013), which used the sliding-window (SW) and word distance-based (WD) algorithms to calculate the answer scores according to the rules and return the highest-score answer. We also tried the improved SW and WD algorithms proposed in (Smith et al., 2015), and the system performance has improvement. Sliding-window and Word Distance-based algorithms are are described as follows: Sliding-Window: Given a data sample &lt;D, Q, a0 (or a1 ), a&gt;, firstly, we calculate the inverse word counts of each word in the document D. Then we set a window that slides word by word from the beginning of the document to the end. When the window slides to a position, the sum of inverse word counts of all the words that appears in the question Q or the candidate choice a0 (or a1 ) is the score of the window at this moment. Until the window slides to the end of the passa"
S18-1175,P17-1171,0,0.0231758,"ate-of-art GatedAttention Reader which performs well on several datasets. When a sample data &lt;D, Q, A, a&gt; is given, the steps of the model processing this data sample are described below, Figure 1 shows the system. 2.3.1 Passage, Question and Choice Encoder First, each word in D, Q, and choices (two choices in candidate answer set A) is mapped to d-dimendional vector. The 300-dim GloVe embedding (Pennington et al., 2014) is used. For the input word vectors of D, we also include a 5-dim binary feature to indicates the overlap between the ducument and the question(or choices) which inspired by (Chen et al., 2017). Each dimension of the 5-dim binary match feature represent whether the word present in the query, in the choice a0 , in the choice a1 , in both question and 1 We use the following equations to estimate how many answers appear entirely in the document: if |answer word ∩ document word|/|answer word |= 1, it means the answer appears entirely in the document, where |A |means size of set A. Then we calculate |ansce |/|ansc |, where ansce means correct answers which entirely appeared in document, and ansc means correct answers. The percentage of the correct answers entirely appeared in document is"
S18-1175,L18-1564,0,0.0498242,"e-scale reading comprehension corpora has driven the development of technology for machine reading comprehension, and most of these machine comprehension datasets do not need commonsense knowledge to answer questions. The purpose of Machine Comprehension using Commonsense Knowledge task in Semeval 2018 is to provide a platform for finding a way for the machine to better understand the text and enable the machine answer questions based on the text, and encourage participants to make use any external resources (e.g., DeScript, narrative chains, Wikipedia, etc) to improve the system performance (Ostermann et al., 2018b). The task 11 is a multiple-choice machine comprehension, which requires a system read a narrative text about everyday activities (Ostermann et al., 2018a) and then answer multiple-choice questions based on this text. Some questions need to be answered according to the original text, and others can be answered by commonsense knowledge. Each question is associated with a set of two answers. Table 1 gives an example of the dataset. To address this machine comprehension task, we utilized rule-based methods and a deep learnTable 1: An example from the SemEval2018 task11 dataset. ing method. Our"
S18-1175,D14-1162,0,0.0813454,"that there is a limit to using the above method to improve system performance. Hence we used a deep learning approach to passage representations modeling. Inspired by (Lai et al., 2017), we use the state-of-art GatedAttention Reader which performs well on several datasets. When a sample data &lt;D, Q, A, a&gt; is given, the steps of the model processing this data sample are described below, Figure 1 shows the system. 2.3.1 Passage, Question and Choice Encoder First, each word in D, Q, and choices (two choices in candidate answer set A) is mapped to d-dimendional vector. The 300-dim GloVe embedding (Pennington et al., 2014) is used. For the input word vectors of D, we also include a 5-dim binary feature to indicates the overlap between the ducument and the question(or choices) which inspired by (Chen et al., 2017). Each dimension of the 5-dim binary match feature represent whether the word present in the query, in the choice a0 , in the choice a1 , in both question and 1 We use the following equations to estimate how many answers appear entirely in the document: if |answer word ∩ document word|/|answer word |= 1, it means the answer appears entirely in the document, where |A |means size of set A. Then we calcula"
S18-1184,D15-1075,0,0.0513053,"Missing"
S18-1184,P17-1152,0,0.137799,"Missing"
S18-1184,N18-1175,0,0.032383,"e window size, wj is the parameter of a filter, m is the number of the filters. We also adopt padding before the convolution operation. As a result, we obtain the spatial representations x0i ∈ Rm , which has the same length as the input sequence. After that, we utilize a bi-directional LSTM (BiLSTM) to obtain the temporal information. For each time step t, the LSTM unit computation corresponds to : it = σ(Wi x0t + Ui ht−1 + bi ) ft = ot = c˜t = σ(Wf x0t + Uf ht−1 + bf ) σ(Wo x0t + Uo ht−1 + bo ) tanh(Wc x0t + Uc ht−1 + bc ) ct = it c˜t + ft ct−1 ht = ot tanh(ct ) (3) (4) (7) (8) Inspired from Habernal et al. (2018a), we use an intra-temporal attention function to attend over specific parts of the input sequence. This kind of attention encourages the model to generate different representation according to the attention vector. Habernal et al. (2018a) have shown that such intra-temporal attention outperforms standard attention. We define va as the attention vector, and ht as the hidden states at time step t: ho = ReLU(Wo [attW0 , attW1 ]) pˆ = Softmax(Wp ho ) (10) ht = ht at (11) where at is the attention weights over the hidden states ht , is element-wise multiplication. (12) (13) As for the optimizatio"
S18-1184,S18-1121,0,0.0641325,"e window size, wj is the parameter of a filter, m is the number of the filters. We also adopt padding before the convolution operation. As a result, we obtain the spatial representations x0i ∈ Rm , which has the same length as the input sequence. After that, we utilize a bi-directional LSTM (BiLSTM) to obtain the temporal information. For each time step t, the LSTM unit computation corresponds to : it = σ(Wi x0t + Ui ht−1 + bi ) ft = ot = c˜t = σ(Wf x0t + Uf ht−1 + bf ) σ(Wo x0t + Uo ht−1 + bo ) tanh(Wc x0t + Uc ht−1 + bc ) ct = it c˜t + ft ct−1 ht = ot tanh(ct ) (3) (4) (7) (8) Inspired from Habernal et al. (2018a), we use an intra-temporal attention function to attend over specific parts of the input sequence. This kind of attention encourages the model to generate different representation according to the attention vector. Habernal et al. (2018a) have shown that such intra-temporal attention outperforms standard attention. We define va as the attention vector, and ht as the hidden states at time step t: ho = ReLU(Wo [attW0 , attW1 ]) pˆ = Softmax(Wp ho ) (10) ht = ht at (11) where at is the attention weights over the hidden states ht , is element-wise multiplication. (12) (13) As for the optimizatio"
S18-1184,W17-5301,0,0.0293064,"Missing"
W10-4326,W05-0613,0,0.0143873,". Moreover, this method can be applied in both supervised and unsupervised ways by generating features on labeled and unlabeled training data and then performing implicit discourse connectives recognition. Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 200"
W10-4326,W01-1605,0,0.141734,"Missing"
W10-4326,W03-1210,0,0.0256562,"lation recognition. Moreover, this method can be applied in both supervised and unsupervised ways by generating features on labeled and unlabeled training data and then performing implicit discourse connectives recognition. Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random clas"
W10-4326,N04-1020,0,0.0389146,"Missing"
W10-4326,D09-1036,0,0.101988,"Missing"
W10-4326,P02-1047,0,0.288883,"tal results showed that the two methods achieved comparable F-scores to the state-of-art methods. It indicates that the method using language model to predict connectives is very useful in solving this task. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 describes our methods for implicit discourse relation recognition. Section 4 presents experiments and results. Section 5 offers some conclusions. 2 corpora, their approaches are not very useful in the real word. Another line of research is to use the unsupervised methods on unhuman-annotated corpus. (Marcu and Echihabi, 2002) used several patterns to extract instances of discourse relations such as contrast and elaboration from unlabeled corpora. Then they used word-pairs between arguments as features for building classification models and tested their model on artificial data for implicit relations. Subsequently other studies attempt to extend the work of (Marcu and Echihabi, 2002). (Sporleder and Lascarides, 2008) discovered that Marcu and Echihabi’s models do not perform as well on implicit relations as one might expect from the test accuracy on synthetic data. (Goldensohn, 2007) extended the work of (Marcu and"
W10-4326,P09-1077,0,0.308125,"method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 2009) presented an implicit discourse relation classifier in PDTB with the use of contextual relations, constituent Parse Features, dependency parse features and cross-argument word pairs. Although both of two methods achieved the state of the art performance for automatical recognition of implicit discourse relations, due to lack"
W10-4326,P09-2004,0,0.0519904,"Missing"
W10-4326,W06-0305,0,0.0185618,"mplicit discourse connectives recognition. Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 2009) presented an implicit discourse relation classifier in PDTB with the use of contextual relations, constituent Parse Features, dependency parse featur"
W10-4326,prasad-etal-2008-penn,0,0.288617,"Missing"
W10-4326,N06-2034,0,0.103242,"Missing"
W10-4326,N03-1030,0,0.0457007,"ves predicted using a language model, for implicit relation recognition. Moreover, this method can be applied in both supervised and unsupervised ways by generating features on labeled and unlabeled training data and then performing implicit discourse connectives recognition. Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing"
W10-4326,W06-1317,0,0.310693,"Missing"
W10-4326,C08-2022,0,\N,Missing
