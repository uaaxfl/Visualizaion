2015.jeptalnrecital-court.29,F12-2024,0,0.0421149,"Missing"
2015.jeptalnrecital-court.29,P10-1052,1,0.856987,"Missing"
2015.jeptalnrecital-court.29,E12-2021,0,0.0931259,"Missing"
2015.jeptalnrecital-court.29,W09-1309,0,0.0698578,"Missing"
2015.jeptalnrecital-court.29,villemonte-de-la-clergerie-etal-2008-passage,0,0.06328,"Missing"
2015.jeptalnrecital-long.5,P10-1052,0,0.039442,"Missing"
2015.jeptalnrecital-long.5,E14-4026,0,0.0580521,"Missing"
2015.jeptalnrecital-long.5,llorens-etal-2012-timen,0,0.06874,"Missing"
2015.jeptalnrecital-long.5,W03-0430,0,0.010542,"Missing"
2015.jeptalnrecital-long.5,moriceau-tannier-2014-french,1,0.851424,"Missing"
2015.jeptalnrecital-long.5,pustejovsky-etal-2010-iso,0,0.245163,"Missing"
2015.jeptalnrecital-long.5,E12-2021,0,0.051131,"Missing"
2015.jeptalnrecital-long.5,strotgen-etal-2014-extending,0,0.0423585,"Missing"
2015.jeptalnrecital-long.5,strotgen-gertz-2012-temporal,0,0.0670179,"Missing"
2015.jeptalnrecital-long.5,W14-3413,0,0.0538301,"Missing"
2015.jeptalnrecital-long.5,S07-1014,0,0.0639399,"Missing"
2016.jeptalnrecital-poster.19,S15-2136,0,0.0366591,"Missing"
2016.jeptalnrecital-poster.19,S16-1165,0,0.0233445,"Missing"
2016.jeptalnrecital-poster.19,W06-1623,0,0.0677844,"Missing"
2016.jeptalnrecital-poster.19,P05-1022,0,0.112201,"Missing"
2016.jeptalnrecital-poster.19,deleger-etal-2014-annotation,1,0.887262,"Missing"
2016.jeptalnrecital-poster.19,W02-0109,0,0.0797351,"Missing"
2016.jeptalnrecital-poster.19,P14-5010,0,0.00491923,"Missing"
2016.jeptalnrecital-poster.19,N10-1004,0,0.0484883,"Missing"
2016.jeptalnrecital-poster.19,W11-0419,0,0.0708503,"Missing"
2016.jeptalnrecital-poster.19,Q14-1012,0,0.049267,"Missing"
2017.jeptalnrecital-court.29,E12-1058,0,0.0609025,"Missing"
2018.jeptalnrecital-court.2,F12-2024,0,0.039137,"Missing"
2018.jeptalnrecital-court.2,N09-2061,0,0.0460783,"Missing"
2018.jeptalnrecital-court.2,J06-4003,0,0.161561,"Missing"
2018.jeptalnrecital-court.2,P14-5010,0,0.0047238,"Missing"
2020.acl-tutorials.4,J08-1008,0,0.229852,"Missing"
2020.acl-tutorials.4,J07-4009,0,0.026953,"ot.mieskes@h-da.de neveol@limsi.fr 1 Tutorial Content considerable number of reviewers are junior researchers, who might lack the experience and expertise necessary for high-quality reviews. A tutorial on this topic might increase reviewers’ confidence, as well as the quality of the reviews. Given the importance of conferences in NLP, the reviewing standards should be as high as with journals in other fields. This tutorial will cover the goals, processes, and evaluation of reviewing research in natural language processing. As has been pointed out for years by leading figures in our community (Webber, 2007), researchers in the ACL community face a heavy—and growing—reviewing burden. Initiatives to lower this burden have been discussed at the recent ACL general assembly in Florence (ACL 2019)1 . Simultaneously, notable “false negatives”—rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences (Church, 2005)—has raised awareness of the fact that our reviewing practices leave something to be desired. . . and we do not often talk about “false positives” with respect to conference papers, but conversations in the hallways at *ACL mee"
2020.acl-tutorials.4,J05-4006,0,0.156389,"high as with journals in other fields. This tutorial will cover the goals, processes, and evaluation of reviewing research in natural language processing. As has been pointed out for years by leading figures in our community (Webber, 2007), researchers in the ACL community face a heavy—and growing—reviewing burden. Initiatives to lower this burden have been discussed at the recent ACL general assembly in Florence (ACL 2019)1 . Simultaneously, notable “false negatives”—rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences (Church, 2005)—has raised awareness of the fact that our reviewing practices leave something to be desired. . . and we do not often talk about “false positives” with respect to conference papers, but conversations in the hallways at *ACL meetings suggest that we have a publication bias towards papers that report high performance, with perhaps not much else of interest in them (Manning, 2015). It need not be this way. There is good reason to think that reviewing is a learnable (and teachable) skill (Basford, 1990; Paice, 2001; Benos et al., 2003; Koike et al., 2009; Shukla, 2010; Tandon, 2014; Spyns and Vida"
2020.jeptalnrecital-taln.35,D19-1588,0,0.0215614,"Missing"
2020.jeptalnrecital-taln.35,N16-1030,0,0.0893794,"Missing"
2020.jeptalnrecital-taln.35,D17-1018,0,0.0457912,"Missing"
2020.jeptalnrecital-taln.35,P14-2006,0,0.0308576,"Missing"
2020.jeptalnrecital-taln.35,W11-1901,0,0.0594136,"Missing"
2020.jeptalnrecital-taln.35,D17-1035,0,0.0201615,"Missing"
2020.jeptalnrecital-taln.35,Q14-1012,0,0.0453975,"Missing"
2020.jeptalnrecital-taln.35,M95-1002,0,0.272925,"Missing"
2020.jeptalnrecital-taln.35,P15-1137,0,0.0484465,"Missing"
2020.jeptalnrecital-taln.35,N16-1114,0,0.0388283,"Missing"
2020.jeptalnrecital-taln.35,P19-1083,0,0.0606537,"Missing"
2020.lrec-1.453,W19-5403,1,0.92318,"translation systems (Koehn and Knowles, 2017). The use of triangulation has been explored to compensate for the lack of parallel data in a specific language pair (Gispert and Mariño, 2006). Crowdsourcing has also been used for producing parallel corpus in rare language pairs (Zbib et al., 2012). In spite of these efforts, parallel corpus are still needed, especially in specialized domains. On this study, a parallel corpus of scientific abstracts was collected from MEDLINE for use in the biomedical task offered at the Workshop on Machine Translation (WMT) in 2018 and 2019 (Neves et al., 2018; Bawden et al., 2019). The abstracts in the corpus were produced by the authors of articles indexed in the MEDLINE database. Little is known about the writing practices used by the authors to create these texts. We have made the hypothesis that authors write their abstract in one language, and then translate it into one or more languages according to the journal requirements. Assuming this hypothesis is correct, there is no information on the language used to write the original abstracts. However, Machine Translation (MT) research has shown that features such as the translation direction have an impact on the perf"
2020.lrec-1.453,D08-1078,0,0.0124975,"lly available in the national or regional language of the territory where the patient is treated. Given the lack of natural language processing (NLP) tools available for languages other than English, translation can be envisaged as a preprocessing step (Campos et al., 2017). However, machine translation methods rely on the availability of large parallel corpus for training, tuning and evaluation (Koehn, 2009). Characteristics of language pairs, such as morphological complexity of the target language and relatedness of the two languages, are major indicators of success for translation systems (Birch et al., 2008). However, training corpus size is a bottleneck, especially for modern neural translation systems (Koehn and Knowles, 2017). The use of triangulation has been explored to compensate for the lack of parallel data in a specific language pair (Gispert and Mariño, 2006). Crowdsourcing has also been used for producing parallel corpus in rare language pairs (Zbib et al., 2012). In spite of these efforts, parallel corpus are still needed, especially in specialized domains. On this study, a parallel corpus of scientific abstracts was collected from MEDLINE for use in the biomedical task offered at the"
2020.lrec-1.453,W17-3204,0,0.0118389,"tural language processing (NLP) tools available for languages other than English, translation can be envisaged as a preprocessing step (Campos et al., 2017). However, machine translation methods rely on the availability of large parallel corpus for training, tuning and evaluation (Koehn, 2009). Characteristics of language pairs, such as morphological complexity of the target language and relatedness of the two languages, are major indicators of success for translation systems (Birch et al., 2008). However, training corpus size is a bottleneck, especially for modern neural translation systems (Koehn and Knowles, 2017). The use of triangulation has been explored to compensate for the lack of parallel data in a specific language pair (Gispert and Mariño, 2006). Crowdsourcing has also been used for producing parallel corpus in rare language pairs (Zbib et al., 2012). In spite of these efforts, parallel corpus are still needed, especially in specialized domains. On this study, a parallel corpus of scientific abstracts was collected from MEDLINE for use in the biomedical task offered at the Workshop on Machine Translation (WMT) in 2018 and 2019 (Neves et al., 2018; Bawden et al., 2019). The abstracts in the cor"
2020.lrec-1.453,P09-5002,0,0.0309542,"anguages other than English. Translation from a variety of languages into English is also important in biomedicine. For instance, it can support researchers working on clinical reports, which are usually available in the national or regional language of the territory where the patient is treated. Given the lack of natural language processing (NLP) tools available for languages other than English, translation can be envisaged as a preprocessing step (Campos et al., 2017). However, machine translation methods rely on the availability of large parallel corpus for training, tuning and evaluation (Koehn, 2009). Characteristics of language pairs, such as morphological complexity of the target language and relatedness of the two languages, are major indicators of success for translation systems (Birch et al., 2008). However, training corpus size is a bottleneck, especially for modern neural translation systems (Koehn and Knowles, 2017). The use of triangulation has been explored to compensate for the lack of parallel data in a specific language pair (Gispert and Mariño, 2006). Crowdsourcing has also been used for producing parallel corpus in rare language pairs (Zbib et al., 2012). In spite of these"
2020.lrec-1.453,2009.mtsummit-papers.9,0,0.012966,"les indexed in the MEDLINE database. Little is known about the writing practices used by the authors to create these texts. We have made the hypothesis that authors write their abstract in one language, and then translate it into one or more languages according to the journal requirements. Assuming this hypothesis is correct, there is no information on the language used to write the original abstracts. However, Machine Translation (MT) research has shown that features such as the translation direction have an impact on the performance of translation models trained or tuned on parallel corpus (Kurokawa et al., 2009; Lembersky et al., 2013; Stymne, 2017). Recent investigation of the test sets used in the WMT news translation tasks from 2016 to 2018 found that the use of mixed translation directions in test sets inflates the human scores for translation systems to the level that system rankings can be impacted (Zhang and Toral, 2019). While evaluations carried out on tests sets including documents with mixed translation directions can be re-interpreted in light of the translation direction information, it is now recommended to move away from the use of such test sets (Graham et al., 2019). Direct analysis"
2020.lrec-1.453,J13-4007,0,0.0206292,"INE database. Little is known about the writing practices used by the authors to create these texts. We have made the hypothesis that authors write their abstract in one language, and then translate it into one or more languages according to the journal requirements. Assuming this hypothesis is correct, there is no information on the language used to write the original abstracts. However, Machine Translation (MT) research has shown that features such as the translation direction have an impact on the performance of translation models trained or tuned on parallel corpus (Kurokawa et al., 2009; Lembersky et al., 2013; Stymne, 2017). Recent investigation of the test sets used in the WMT news translation tasks from 2016 to 2018 found that the use of mixed translation directions in test sets inflates the human scores for translation systems to the level that system rankings can be impacted (Zhang and Toral, 2019). While evaluations carried out on tests sets including documents with mixed translation directions can be re-interpreted in light of the translation direction information, it is now recommended to move away from the use of such test sets (Graham et al., 2019). Direct analysis of the MEDLINE datasets"
2020.lrec-1.453,L18-1043,1,0.773568,"d in the WMT news translation tasks from 2016 to 2018 found that the use of mixed translation directions in test sets inflates the human scores for translation systems to the level that system rankings can be impacted (Zhang and Toral, 2019). While evaluations carried out on tests sets including documents with mixed translation directions can be re-interpreted in light of the translation direction information, it is now recommended to move away from the use of such test sets (Graham et al., 2019). Direct analysis of the MEDLINE datasets shows that alignment and translation quality are uneven (Névéol et al., 2018). It has been hypothesized that the authors may have no translation training and sometimes lacking competence in some of the languages that they are required to use. Other conjectures include the possibility that authors may write the abstracts in different languages independently to leverage the language competence of different authors in a group, which would put an emphasis on maximizing lan3676 guage correctness and content accuracy rather than translation quality. This study aims to characterize MEDLINE authors’ language competence and writing practice of abstracts. It consisted of invitin"
2020.lrec-1.453,W18-6403,1,0.811523,"ly for modern neural translation systems (Koehn and Knowles, 2017). The use of triangulation has been explored to compensate for the lack of parallel data in a specific language pair (Gispert and Mariño, 2006). Crowdsourcing has also been used for producing parallel corpus in rare language pairs (Zbib et al., 2012). In spite of these efforts, parallel corpus are still needed, especially in specialized domains. On this study, a parallel corpus of scientific abstracts was collected from MEDLINE for use in the biomedical task offered at the Workshop on Machine Translation (WMT) in 2018 and 2019 (Neves et al., 2018; Bawden et al., 2019). The abstracts in the corpus were produced by the authors of articles indexed in the MEDLINE database. Little is known about the writing practices used by the authors to create these texts. We have made the hypothesis that authors write their abstract in one language, and then translate it into one or more languages according to the journal requirements. Assuming this hypothesis is correct, there is no information on the language used to write the original abstracts. However, Machine Translation (MT) research has shown that features such as the translation direction have"
2020.lrec-1.453,W17-0230,0,0.018539,"known about the writing practices used by the authors to create these texts. We have made the hypothesis that authors write their abstract in one language, and then translate it into one or more languages according to the journal requirements. Assuming this hypothesis is correct, there is no information on the language used to write the original abstracts. However, Machine Translation (MT) research has shown that features such as the translation direction have an impact on the performance of translation models trained or tuned on parallel corpus (Kurokawa et al., 2009; Lembersky et al., 2013; Stymne, 2017). Recent investigation of the test sets used in the WMT news translation tasks from 2016 to 2018 found that the use of mixed translation directions in test sets inflates the human scores for translation systems to the level that system rankings can be impacted (Zhang and Toral, 2019). While evaluations carried out on tests sets including documents with mixed translation directions can be re-interpreted in light of the translation direction information, it is now recommended to move away from the use of such test sets (Graham et al., 2019). Direct analysis of the MEDLINE datasets shows that ali"
2020.lrec-1.453,N12-1006,0,0.0206615,"g, tuning and evaluation (Koehn, 2009). Characteristics of language pairs, such as morphological complexity of the target language and relatedness of the two languages, are major indicators of success for translation systems (Birch et al., 2008). However, training corpus size is a bottleneck, especially for modern neural translation systems (Koehn and Knowles, 2017). The use of triangulation has been explored to compensate for the lack of parallel data in a specific language pair (Gispert and Mariño, 2006). Crowdsourcing has also been used for producing parallel corpus in rare language pairs (Zbib et al., 2012). In spite of these efforts, parallel corpus are still needed, especially in specialized domains. On this study, a parallel corpus of scientific abstracts was collected from MEDLINE for use in the biomedical task offered at the Workshop on Machine Translation (WMT) in 2018 and 2019 (Neves et al., 2018; Bawden et al., 2019). The abstracts in the corpus were produced by the authors of articles indexed in the MEDLINE database. Little is known about the writing practices used by the authors to create these texts. We have made the hypothesis that authors write their abstract in one language, and th"
2020.lrec-1.453,W19-5208,0,0.0164055,"rect, there is no information on the language used to write the original abstracts. However, Machine Translation (MT) research has shown that features such as the translation direction have an impact on the performance of translation models trained or tuned on parallel corpus (Kurokawa et al., 2009; Lembersky et al., 2013; Stymne, 2017). Recent investigation of the test sets used in the WMT news translation tasks from 2016 to 2018 found that the use of mixed translation directions in test sets inflates the human scores for translation systems to the level that system rankings can be impacted (Zhang and Toral, 2019). While evaluations carried out on tests sets including documents with mixed translation directions can be re-interpreted in light of the translation direction information, it is now recommended to move away from the use of such test sets (Graham et al., 2019). Direct analysis of the MEDLINE datasets shows that alignment and translation quality are uneven (Névéol et al., 2018). It has been hypothesized that the authors may have no translation training and sometimes lacking competence in some of the languages that they are required to use. Other conjectures include the possibility that authors"
2020.wmt-1.76,W19-5403,1,0.905677,"matic translation of a variety of biomedical texts. The first edition of the task (Bojar et al., 2016) focused on biomedical scientific abstracts in three language pairs. The second edition of the task offered ten language pairs and addressed scientific abstracts as well as patient-oriented health information (Jimeno Yepes et al., 2017). The third edition of the task offered six language pairs and addressed scientific abstracts (Neves et al., 2018). The fourth edition of the task offered ten language pairs. It addressed scientific abstracts and introduced the task of terminology translation (Bawden et al., 2019). This year’s edition of the task continues to address the translation of scientific abstracts and terminologies. It builds on previous tasks by offering a large range of training and test sets to support participants’ systems. The following language pairs are addressed this year: • During the construction of the test sets, and after the manual validation of the automatic alignment, we ran a pilot project for a couple of languages in which we manually finetuned the alignment of the test sets (cf. Section 2.2.3). • We ran a second pilot study in which we split the sentences according to the rep"
2020.wmt-1.76,federmann-2010-appraise,0,0.141593,"where it was sufficient to split sentences according to the Chinese punctuation (。) that marks the end of a sentence. Sentence alignment was carried out for all languages (except for zh/en) with the GMA tool using specific stopword lists for each language. For zh/en, we used the Champollion tool17 with the same configurations and stopword lists since 2018. 663 17 http://champollion.sourceforge.net/ We randomly retrieved a set of 100 abstracts for each language pair, and the automatic aligned sentences were manually validated by native speakers of the foreign languages using the Appraise tool (Federmann, 2010). Results of the validation are shown in Table 2. For the ru/en set, an additional set of 100 abstracts were randomly retrieved for a second round of manual validation. This was due to the low quality of the alignments that we obtained in the first round of validation. The official test set for ru/en was composed of the abstracts with better quality from the totality of 200 abstracts that were validated. As a pilot study this year, we performed a manual correction of the alignment which were identified as not being correct during the validation in the Appraise tool. This step was only carried"
2020.wmt-1.76,2020.wmt-1.88,0,0.0500914,"Missing"
2020.wmt-1.76,L18-1141,1,0.8847,"Missing"
2020.wmt-1.76,2020.wmt-1.89,1,0.806962,"Missing"
2020.wmt-1.76,P07-2045,0,0.00948999,"ei-Bot YerevaNN A3T3 A3T3 A3T3 A3T3 - A3 A3 A3 A3 A2 A3 A3 A1 A1 A3 - A2 A2 A1 A1 A1 - A2 A1 - A1 A2 - A2 A1 A1 A3 A3 A1 A1 A3 A1 A2 A3 A1 A2 - 6 3 1 1 9 12 9 2 3 1 1 2 5 6 4 6 2 6 2 5 Total 24 14 11 7 3 3 7 17 86 Table 4: Overview of the submissions from all teams and test sets translating from English. We identify submissions to the abstracts testsets with an “A” and to the terminology test set with a “T”. The value next to the letter indicates the number of runs for the corresponding test set, language pair, and team. using BLEU with the MULTI-EVAL v14 tool20 provided by the Moses package (Koehn et al., 2007). This means as well that we reused the tokenization approach used for Chinese. Results for MEDLINE BLEU are shown in Tables 10 and 11. 20 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ generic/mteval-v14.pl 5.2 News The test set of our challenge was included in the News challenge data set. We identified the translations in the News files and used the same evaluation procedure as applied to MEDLINE abstracts. Results of the systems are shown in Tables 12 and 13. 666 Teams de2en es2en fr2en it2en pt2en ru2en zh2en Total ai_not_intellegent Alibuba baidu_translation Huawei United"
2020.wmt-1.76,2020.wmt-1.90,0,0.0246879,"Missing"
2020.wmt-1.76,2020.acl-main.448,0,0.0154667,"ta, teams used the training data distributed by us and many of the sources described in (Névéol et al., 2018). Tables 7 and 8 provide details of the in-domain data used by the teams. For relevant language pairs, parallel data from other WMT tracks (e.g., News Task) was used. Interestingly, some teams used similarity measures based on biomedical corpora to extract additional biomedical sentences from out-of-domain corpora. Out-of-domain data was also used in the form of pre-trained base models. Table 9 shows details of the out-of-domain data used by the teams. 5 Automatic evaluation Following (Mathur et al., 2020), we used chrF (Popovi´c, 2015) as well as BLEU (Papineni et al., 2002) as automatic metrics. chrF scores are obtained using the nltk implementation.19 665 5.1 MEDLINE Similarly to previous years, we compared the submitted translations to the reference translations 19 https://www.nltk.org/_modules/nltk/ translate/chrf_score.html Team ID Institution ADAPT (Nayak et al., 2020) ai_not_intellegent Alibuba baidu_translation Elhuyar_NLP (Corral and Saralegi, 2020) Huawei United (Peng et al., 2020) Ixamed (Soto et al., 2020) LIMSI (Abdul Rauf et al., 2020) NLE nrpu-fjwu (Naz et al., 2020) one_connect"
2020.wmt-1.76,2020.wmt-1.91,0,0.05524,"Missing"
2020.wmt-1.76,W18-6403,1,0.794194,"ation for Computational Linguistics • We include a novel test set for the automatic translation of biomedical terminologies from English to Basque (cf. Section 2.2.1) matic translation of a variety of biomedical texts. The first edition of the task (Bojar et al., 2016) focused on biomedical scientific abstracts in three language pairs. The second edition of the task offered ten language pairs and addressed scientific abstracts as well as patient-oriented health information (Jimeno Yepes et al., 2017). The third edition of the task offered six language pairs and addressed scientific abstracts (Neves et al., 2018). The fourth edition of the task offered ten language pairs. It addressed scientific abstracts and introduced the task of terminology translation (Bawden et al., 2019). This year’s edition of the task continues to address the translation of scientific abstracts and terminologies. It builds on previous tasks by offering a large range of training and test sets to support participants’ systems. The following language pairs are addressed this year: • During the construction of the test sets, and after the manual validation of the automatic alignment, we ran a pilot project for a couple of language"
2020.wmt-1.76,L16-1470,1,0.865582,"Missing"
2020.wmt-1.76,W19-5333,0,0.047214,"Missing"
2020.wmt-1.76,L18-1043,1,0.851824,"d 35 seconds). All teams used transformerbased neural machine translation (except for team TRAMECAT, who used sequence2sequence) and mostly relied on existing implementations: 19 teams submitted runs using available libraries, one team submitted runs using a mix of libraries and inhouse implementations, one team submitted runs exclusively relying on their own implementation of NMT. Teams often used the same setup for a range of language pairs. Table 6 shows details about the teams methods. For in-domain data, teams used the training data distributed by us and many of the sources described in (Névéol et al., 2018). Tables 7 and 8 provide details of the in-domain data used by the teams. For relevant language pairs, parallel data from other WMT tracks (e.g., News Task) was used. Interestingly, some teams used similarity measures based on biomedical corpora to extract additional biomedical sentences from out-of-domain corpora. Out-of-domain data was also used in the form of pre-trained base models. Table 9 shows details of the out-of-domain data used by the teams. 5 Automatic evaluation Following (Mathur et al., 2020), we used chrF (Popovi´c, 2015) as well as BLEU (Papineni et al., 2002) as automatic metr"
2020.wmt-1.76,P02-1040,0,0.111539,"urces described in (Névéol et al., 2018). Tables 7 and 8 provide details of the in-domain data used by the teams. For relevant language pairs, parallel data from other WMT tracks (e.g., News Task) was used. Interestingly, some teams used similarity measures based on biomedical corpora to extract additional biomedical sentences from out-of-domain corpora. Out-of-domain data was also used in the form of pre-trained base models. Table 9 shows details of the out-of-domain data used by the teams. 5 Automatic evaluation Following (Mathur et al., 2020), we used chrF (Popovi´c, 2015) as well as BLEU (Papineni et al., 2002) as automatic metrics. chrF scores are obtained using the nltk implementation.19 665 5.1 MEDLINE Similarly to previous years, we compared the submitted translations to the reference translations 19 https://www.nltk.org/_modules/nltk/ translate/chrf_score.html Team ID Institution ADAPT (Nayak et al., 2020) ai_not_intellegent Alibuba baidu_translation Elhuyar_NLP (Corral and Saralegi, 2020) Huawei United (Peng et al., 2020) Ixamed (Soto et al., 2020) LIMSI (Abdul Rauf et al., 2020) NLE nrpu-fjwu (Naz et al., 2020) one_connect_000 OOM_20 Sheffield (Soares and Vaz, 2020) TMT (Wang et al., 2020) TR"
2020.wmt-1.76,2020.wmt-1.93,0,0.0298397,"Missing"
2020.wmt-1.76,W15-3049,0,0.0407701,"Missing"
2020.wmt-1.76,2020.wmt-1.94,0,0.0324894,"Missing"
2020.wmt-1.76,L18-1546,0,0.104812,": UFAL medical and MEDLINE abstracts corpus supplied by organizers. FINE - TUNING: MEDLINE abstracts UFAL medical and MEDLINE abstracts corpus supplied by organizers. MEDLINE abstracts corpus supplied by organizers; alignment was fixed using XLM-R 29 k 34,710 No No No - 2.5M UFAL (en) 5.4M 32,466 No - Elhuyar_NLP Scielo and corpora supplied by organizers. Ixamed MEDLINE corpus supplied by organizers and TAUS Corona Crisis Corpus UNICAM TRAINING : UFAL medical, Scielo (Neves et al., 2016), and MEDLINE abstracts corpus supplied by organizers. FINE - TUNING : MEDLINE abstracts BVS, EMEA, Scielo (Soares et al., 2018) and MEDLINE Sheffield corpus supplied by organizers as well as new crawled PubMed data. The data was checked against the official test set to avoid including test data during training. TRAMECAT Biomedical translation repository, EMEA, IBECS, ICD10, Kreshmoi, MEDLINE corpus supplied by organizers, in-house MEDLINE (dated 2018), Medem glossaries, MSDManuals, Portal Clinic corpus, Scielo, SNOMED 560k 1,290,201 No No - TRAINING : 1.3M FINE - TUNING: 67K No - 2.5M No - 7,232,784 No - ADAPT - Common Crawl selected by TermFinder SNOMED descriptions, hospital notes and wikipedia medical articles (en)"
2020.wmt-1.76,2020.lrec-1.465,0,0.0448156,"Missing"
2020.wmt-1.76,2020.wmt-1.95,0,0.0268683,"Missing"
2020.wmt-1.76,2020.wmt-1.96,1,0.796163,"Missing"
2020.wmt-1.76,2020.eamt-1.61,0,0.0113521,"f a single language pair: English to German. Each of the 10 models were trained for up to two days. The training was stopped when there were no improvements on the validation dataset for more than 10 epochs, as measured through cross-validation score. The corpora we used to train the models were the same as last year – when we had baselines generated using RNN-based sequence2sequence models: the UFAL medical corpus (UFA) without the “Subtitles” subset, and as validation we again used Khreshmoi (Dušek et al., 2017). For en/it and en/ru and en/eu we used the Helsinki-NLP/opus-mt-SRC-TRG models (Tiedemann and Thottingal, 2020) included in the huggingface transformers library 18 , trained with MarianNMT on the entirety of the OPUS corpora (Tiedemann, 2012). These models are not uniformly good; they performed very well for Italian, but fairly poor for Russian and Basque. Discussion. It is interesting that the models for English to/from Italian performed so well in the biomedical task, as they were trained on generic text, not targeting the biomedical domain. It is interesting in general to what extent models that excel on generic text (e.g. news) perform well on the biomedical texts as well. 4 Teams and systems This"
2020.wmt-1.76,tiedemann-2012-parallel,0,0.0512473,"provements on the validation dataset for more than 10 epochs, as measured through cross-validation score. The corpora we used to train the models were the same as last year – when we had baselines generated using RNN-based sequence2sequence models: the UFAL medical corpus (UFA) without the “Subtitles” subset, and as validation we again used Khreshmoi (Dušek et al., 2017). For en/it and en/ru and en/eu we used the Helsinki-NLP/opus-mt-SRC-TRG models (Tiedemann and Thottingal, 2020) included in the huggingface transformers library 18 , trained with MarianNMT on the entirety of the OPUS corpora (Tiedemann, 2012). These models are not uniformly good; they performed very well for Italian, but fairly poor for Russian and Basque. Discussion. It is interesting that the models for English to/from Italian performed so well in the biomedical task, as they were trained on generic text, not targeting the biomedical domain. It is interesting in general to what extent models that excel on generic text (e.g. news) perform well on the biomedical texts as well. 4 Teams and systems This year, 22 teams submitted a total of 151 runs. Two teams withdrew after submitting their runs. The remaining teams were from China ("
2020.wmt-1.76,2020.wmt-1.97,0,0.0847722,"Missing"
2021.eacl-tutorials.4,J05-4006,0,0.112871,"requisites: Proficiency in English Tutorial Content This tutorial will cover the theory and practice of reviewing research in natural language processing. As has been pointed out for years by leading figures in our community (Webber, 2007), researchers in the ACL community face a heavy—and growing— reviewing burden. Initiatives to lower this burden have been discussed at the recent ACL general assembly in Florence (ACL 2019)1 . Simultaneously, notable “false negatives”—rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences (Church, 2005)—have raised awareness of the fact that our reviewing practices leave something to be desired. . . and we do not often talk about “false positives” with respect to conference Table 1 presents a brief outline of the tutorial. Our aim is to provide enough options for hands-on experience and smaller-group activities in breakout rooms. 1.1 Reading List • Kenneth Church. 2005. Last words: Reviewing the reviewers. Computational Linguistics, 31(4):575–578 • Button K. S., Bal L., Clark A., and Shipley T. 2016. Preventing the ends from justifying the means: withholding results to address publication bi"
2021.eacl-tutorials.4,J08-1008,0,0.12302,"Missing"
2021.eacl-tutorials.4,J07-4009,0,0.0747782,"e context of widening the NLP community, researchers joining the field might not have the opportunity to practise reviewing. This tutorial fills in this gap by providing an opportunity to learn the basics of reviewing. Also more experienced researchers might find this tutorial interesting to revise their reviewing procedure. 1 Type: Introductory Structure: see Table 1 Prerequisites: Proficiency in English Tutorial Content This tutorial will cover the theory and practice of reviewing research in natural language processing. As has been pointed out for years by leading figures in our community (Webber, 2007), researchers in the ACL community face a heavy—and growing— reviewing burden. Initiatives to lower this burden have been discussed at the recent ACL general assembly in Florence (ACL 2019)1 . Simultaneously, notable “false negatives”—rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences (Church, 2005)—have raised awareness of the fact that our reviewing practices leave something to be desired. . . and we do not often talk about “false positives” with respect to conference Table 1 presents a brief outline of the tutorial. O"
2021.sustainlp-1.2,galibert-etal-2010-named,0,0.0167447,"Missing"
2021.sustainlp-1.2,N18-1131,0,0.0151111,"d in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 2016) and one that addresses both flat and nested entity recognition, introduced by (Yu et al., 2020). (Yu et al., 2020) adapt the biaffine dependency parsing model of (Dozat and Manning, 2017) to Named Entity Recognition by reformulating this task as the task of identifying start and en"
2021.sustainlp-1.2,2020.sustainlp-1.19,0,0.0322306,"count the number of experiments, conversion to interpretable numbers, comparison with other locations No, default value of PUE = 1.67 (2019) PC, local server, cloud No Yes, carbon intensity from carbonfootprint Dec 17, 2020 Yes Yes (online) No install needed Fair CC-BY-4.0 Generates a statement to report the results 2021 4 1 (Liu et al., 2021) Dynamic use Hardware only Asserting certain hardware Yes. Default PUE (1.58) can be adjusted Install dependent No Yes, carbon intensity from electricitymap April 29, 2021 Yes No Fair Fair MIT Generates a statement and graphs to report results 2020 33 3 (Cao et al., 2020; Prasanna et al., 2020; Peng et al., 2021) Experiment Impact Tracker (Henderson et al., 2020) Dynamic use Hardware only Partly, for Google, Amazon, Azure cloud providers. 3 specific providers, private infrastructure No Yes Yes, pointers supplied to user, including electricitymap May 4, 2021 Yes Yes (online) No install needed Fair MIT Generates text and LATEXcode to report the results 2019 35 4 (Sarti, 2020; Selby et al., 2021; Chaudhary et al., 2020; Gencoglu, 2020) (Lacoste et al., 2019) ML CO2 Impact Dynamic use Hardware only Year for the data, comparison with other locations No PUE used bu"
2021.sustainlp-1.2,N16-1030,0,0.0287126,"arbon footprint of computer use based on user supplied information including hardware, runtime, cloud provider and location of the computing facilities operated. We are aware that a new version of the tool is being developed under the umbrella of the Code Carbon 3 initiative. However, it is not yet described in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognitio"
2021.sustainlp-1.2,2020.findings-emnlp.152,0,0.0564346,"Missing"
2021.sustainlp-1.2,2021.findings-emnlp.71,0,0.0472671,"Missing"
2021.sustainlp-1.2,N18-1202,0,0.0207387,"user supplied information including hardware, runtime, cloud provider and location of the computing facilities operated. We are aware that a new version of the tool is being developed under the umbrella of the Code Carbon 3 initiative. However, it is not yet described in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 2016) and one that addre"
2021.sustainlp-1.2,D15-1102,0,0.0165078,"is not yet described in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 2016) and one that addresses both flat and nested entity recognition, introduced by (Yu et al., 2020). (Yu et al., 2020) adapt the biaffine dependency parsing model of (Dozat and Manning, 2017) to Named Entity Recognition by reformulating this task as the task of identif"
2021.sustainlp-1.2,2020.emnlp-main.259,0,0.0312547,"Missing"
2021.sustainlp-1.2,2020.coling-main.78,0,0.0179515,"ation including hardware, runtime, cloud provider and location of the computing facilities operated. We are aware that a new version of the tool is being developed under the umbrella of the Code Carbon 3 initiative. However, it is not yet described in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 2016) and one that addresses both flat and nested"
2021.sustainlp-1.2,P16-1101,0,0.489156,"mputer use based on user supplied information including hardware, runtime, cloud provider and location of the computing facilities operated. We are aware that a new version of the tool is being developed under the umbrella of the Code Carbon 3 initiative. However, it is not yet described in a scientific publication so we have decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 201"
2021.sustainlp-1.2,M93-1032,0,0.697737,"Missing"
2021.sustainlp-1.2,P19-1527,0,0.0273959,"Missing"
2021.sustainlp-1.2,P19-1355,0,0.0935406,"y the tools to assess the impact of named entity recognition experiments in order to compare the measurement obtained in two computational set-ups. Introduction Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the functional performance they offer for a variety of tasks, including text classification or named entity recognition (Tourille et al., 2018). Deep learning programs can have a high environmental impact in terms of Greenhouse Gas (GHG) emissions due in particular to the energy consumption of the computational facilities used to run them (Strubell et al., 2019). The impact has been increasing over the years (Schwartz et al., 2019) and is affecting populations that can be different from those generating the impact (Bender et al., 2021). In a recent medical imaging study, Selvan (2021) suggests that the increase of large model carbon footprint does not translate into proportional accuracy gains. Measuring this impact is a first step for raising awareness and controlling the impact of NLP experiments and operations. Some 2 Environmental impact due to deep learning programs As for any Information and Communication Technology (ICT) service, a deep learni"
2021.sustainlp-1.2,W18-5622,1,0.848153,"iments. 1 • We identify tools available for measuring the environmental impact of NLP experiments • We characterize impact measurement tools with respect to scope of the impact information provided and usability • We apply the tools to assess the impact of named entity recognition experiments in order to compare the measurement obtained in two computational set-ups. Introduction Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the functional performance they offer for a variety of tasks, including text classification or named entity recognition (Tourille et al., 2018). Deep learning programs can have a high environmental impact in terms of Greenhouse Gas (GHG) emissions due in particular to the energy consumption of the computational facilities used to run them (Strubell et al., 2019). The impact has been increasing over the years (Schwartz et al., 2019) and is affecting populations that can be different from those generating the impact (Bender et al., 2021). In a recent medical imaging study, Selvan (2021) suggests that the increase of large model carbon footprint does not translate into proportional accuracy gains. Measuring this impact is a first step f"
2021.sustainlp-1.2,2020.emnlp-demos.6,0,0.0207593,"Missing"
2021.sustainlp-1.2,2020.acl-main.577,0,0.114583,"decided to evaluate ML CO2 which has been used by the NLP research community. 4.1 Named Entity Recognition on QUAERO benchmark corpora NER methods. Many Named Entity Recognition (NER) models focus on identifying flat entities (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2018; Luoma and Pyysalo, 2020) based on a sequence labeling approach. However, to address the need for the extraction of nested entities, an increasing number of models do take nested entities into account as well (Alex et al., 2007; Lu and Roth, 2015; Ju et al., 2018; Straková et al., 2019; Yu et al., 2020; Li, 2021). Nested entities are embedded named entities included in other entities. To reflect current needs of entity extraction, we choose to evaluate the energy consumption of two deep learning neuronal models, one that addresses flat entity recognition (Ma and Hovy, 2016) and one that addresses both flat and nested entity recognition, introduced by (Yu et al., 2020). (Yu et al., 2020) adapt the biaffine dependency parsing model of (Dozat and Manning, 2017) to Named Entity Recognition by reformulating this task as the task of identifying start and end indices and associating a category to"
D15-1055,S15-2136,0,0.0455595,"Missing"
D15-1055,P11-2023,0,0.0312635,"d with normalized time expressions. We characterize temporal expression recognition in three domains and discuss how the development of an automated temporal expression identification tool may be impacted. For clinical text analysis, we experimented with standard tokenization (provided by TreeTagger (Schmid, 1994)) and a custom tokenization where punctuation marks are always considered as token separators, even in dates such as “10-022010” or “10.04.10”.2 2 For this study, we used two available French corpora with TIMEX3 annotations: the French TimeBank corpus (FTB called news in this paper) (Bittar et al., 2011) which covers the news domain and the AncientTimes corpus (ATC, called historical in this paper) (Strötgen et al., 2014b) which covers the historical domain. To cover a third domain, we developed a corpus using a set of clinical notes where personal identifying information (PII) had been marked and replaced by surrogates (Grouin and Névéol, 2014). This included marking some temporal expressions such as dates, which were replaced by surrogate dates obtained by substracting a fixed number of days to the original dates. Manual review ensured that there were no format or other errors in the reintr"
D15-1055,E12-2021,0,0.0825726,"Missing"
D15-1055,P13-1166,0,0.0682825,"Missing"
D15-1055,S10-1071,0,0.109988,"tations independently. This phase of the annotation process contributed to refining annotation guidelines and creating additional rules to improve on the pre-annotation. Subsequently, the rest of the corpus was divided between annotators, so that each document was annotated independently by two annotators. Final 3 Temporal Expression Extraction and Normalization Rule-based methods were shown to be very efficient for the extraction and normalization of time expressions from news narratives in several languages1 . In the latest SemEval campaign (UzZaman et al., 2013), the rule-based HeidelTime (Strötgen and Gertz, 2010) out performed machine-learning and hybrid counterparts by a large margin. However, statistical systems obtained promising results with respect to temporal entity extraction. Based on these results, we chose to use the state-of-the-art rule-based system HeidelTime as well as an in-house statistical tool relying on the Wapiti (Lavergne et al., 2010) implementation of Conditional Random Fields (CRFs) (Lafferty et al., 2001). Existing HeidelTime settings were used to customize it for the analysis of news and historical narratives in French. In addition, we developed a set of 14 rules to provide a"
D15-1055,strotgen-gertz-2012-temporal,0,0.0601929,"y the availability of the TimeBank corpus (Pustejovsky et al., 2003) used in evaluation campaigns such as TempEval (Verhagen et al., 2007). More recent efforts have extended the initial work on English and addressed other languages such as Chinese (Li et al., 2014), French (Moriceau and Tannier, 2014), Arabic, Italian, Spanish, and Vietnamese (Strötgen et al., 2014a). A study of three domain corpora in English in addition to the newswire domain (SMS, historical narratives and clinical trial abstracts) yielded interesting insight to extend the normalized representation of temporal expressions (Strötgen and Gertz, 2012). This work was then applied to cover historical narratives in an additional seven languages. One key finding was that domain specificity could differ between languages (Strötgen et al., 2014b). This prompts the need to study temporal analysis across domains in a variety of languages in order to adequately characterize each domain and language pairs. The prevalence of temporal references across all types of natural language utterances makes temporal analysis a key issue in Natural Language Processing. This work adresses three research questions: 1/is temporal expression recognition specific to"
D15-1055,P10-1052,0,0.0169474,"rmalization Rule-based methods were shown to be very efficient for the extraction and normalization of time expressions from news narratives in several languages1 . In the latest SemEval campaign (UzZaman et al., 2013), the rule-based HeidelTime (Strötgen and Gertz, 2010) out performed machine-learning and hybrid counterparts by a large margin. However, statistical systems obtained promising results with respect to temporal entity extraction. Based on these results, we chose to use the state-of-the-art rule-based system HeidelTime as well as an in-house statistical tool relying on the Wapiti (Lavergne et al., 2010) implementation of Conditional Random Fields (CRFs) (Lafferty et al., 2001). Existing HeidelTime settings were used to customize it for the analysis of news and historical narratives in French. In addition, we developed a set of 14 rules to provide additional customization for the analysis of clinical narratives in French. The CRF model was developed using part of the clinical corpus as a training set, with domain independant surface and lexical features for the text tokens: • The original token from the text (word form); • Surface features: capitalization of the token (all in upper/lower case"
D15-1055,E14-4026,0,0.05151,"Missing"
D15-1055,strotgen-etal-2014-extending,0,0.0343206,"Missing"
D15-1055,moriceau-tannier-2014-french,1,0.895531,"Missing"
D15-1055,S13-2001,0,0.0366908,"e annotators’ task was then to revise the pre-annotations independently. This phase of the annotation process contributed to refining annotation guidelines and creating additional rules to improve on the pre-annotation. Subsequently, the rest of the corpus was divided between annotators, so that each document was annotated independently by two annotators. Final 3 Temporal Expression Extraction and Normalization Rule-based methods were shown to be very efficient for the extraction and normalization of time expressions from news narratives in several languages1 . In the latest SemEval campaign (UzZaman et al., 2013), the rule-based HeidelTime (Strötgen and Gertz, 2010) out performed machine-learning and hybrid counterparts by a large margin. However, statistical systems obtained promising results with respect to temporal entity extraction. Based on these results, we chose to use the state-of-the-art rule-based system HeidelTime as well as an in-house statistical tool relying on the Wapiti (Lavergne et al., 2010) implementation of Conditional Random Fields (CRFs) (Lafferty et al., 2001). Existing HeidelTime settings were used to customize it for the analysis of news and historical narratives in French. In"
D15-1055,W14-3413,0,0.0238685,"can be done with limited efforts and should cover pre-processing as well as temporal specific tasks. 1 Aurélie Névéol LIMSI-CNRS UPR 3251 Rue John von Neuman 91403 Orsay, France neveol@limsi.fr The clinical domain has been addressed during the 2012 i2b2 challenge (Sun et al., 2013b), with a task on temporal relation extraction from clinical narratives. This task used a corpus of clinical notes in English annotated with temporal information (Sun et al., 2013a) based on ISOTimeML (Pustejovsky et al., 2010). It prompted further work in this domain in English (Jindal and Roth, 2013) and Swedish (Velupillai, 2014), including the release of detailed guidelines for creating temporal annotations of clinical text and a discussion of the clinical domain specificity related to temporal aspects (Styler IV et al., 2014). Finally, clinical TempEval 2015 brought the temporal information extraction tasks of past TempEval campaigns to the clinical domain (Bethard et al., 2015). Introduction References to phenomena occurring in the world and their temporal characterization can be found in natural language utterances across domains, genres and languages. Temporal analysis is a key issue in natural language processin"
D15-1055,S07-1014,0,0.108163,"Missing"
D15-1055,pustejovsky-etal-2010-iso,0,\N,Missing
D15-1055,Q14-1012,0,\N,Missing
deleger-etal-2014-annotation,W12-4304,0,\N,Missing
deleger-etal-2014-annotation,W12-2411,0,\N,Missing
deleger-etal-2014-annotation,E12-2021,0,\N,Missing
E17-2117,S15-2136,0,0.04979,"creation time from clinical notes. This work, based on a feature engineering approach, obtained competitive results with the current state-of-theart and led to two main conclusions. First, the use of word embeddings in place of lexical features tends to degrade performance. Second, our feature engineering approach can be applied with comparable results to two different languages, English and French in our case. To follow-up with the first conclusion, we would like to test a more integrated approach for using embeddings, either by turning all features into embeddings as in Yang and Eisenstein (2015) or by adopting a neural network architecture as in Chikka (2016). Concerning the CR task, results are separated by a 10 percent gap (0.65 for the MERLOT corpus and 0.53 for the THYME corpus). Results obtained for the THYME corpus are coherent with those obtained by Tourille et al. (2016) on the Clinical TempEval 2016 evaluation corpus2 . We increased the recall value in comparison to their results (from 0.436 to 0.47) but this measure is still the main point to improve. More globally, the best results of the Clinical TempEval shared task were 0.843 (accuracy) for the DR task and 0.573 (F1-Mea"
E17-2117,S16-1201,0,0.0612921,"Missing"
E17-2117,S16-1165,0,0.0337299,"ustejovsky and Stubbs, 2011) can be apprehended as temporal buckets in which several events may be included. These containers are anchored by temporal expressions, medical events or other concepts. Styler IV et al. (2014) argue that the use of narrative containers instead of classical temporal relations (Allen, 1983) yields better annotation while keeping most of the useful temporal information intact. The concept of narrative container is illustrated in Figure 1 and described further in Pustejovsky and Stubbs (2011). val task related to the topic for the past two years (Bethard et al., 2015; Bethard et al., 2016). Its first track focused on extracting clinical events and temporal expressions, while its second track included DR and CR tasks. Different approaches were implemented by the teams, among which SVM classifiers (Lee et al., 2016; Tourille et al., 2016; Cohan et al., 2016; AAl Abdulsalam et al., 2016) and CRF approaches (Caselli and Morante, 2016; AAl Abdulsalam et al., 2016) for the DR task, and CRF, Convolutional neural networks (Chikka, 2016) and SVM classifiers (Tourille et al., 2016; Lee et al., 2016; AAl Abdulsalam et al., 2016) for the CR task. 3 Corpus Presentation The MERLOT corpus is"
E17-2117,S16-1192,0,0.135986,"in Figure 1 and described further in Pustejovsky and Stubbs (2011). val task related to the topic for the past two years (Bethard et al., 2015; Bethard et al., 2016). Its first track focused on extracting clinical events and temporal expressions, while its second track included DR and CR tasks. Different approaches were implemented by the teams, among which SVM classifiers (Lee et al., 2016; Tourille et al., 2016; Cohan et al., 2016; AAl Abdulsalam et al., 2016) and CRF approaches (Caselli and Morante, 2016; AAl Abdulsalam et al., 2016) for the DR task, and CRF, Convolutional neural networks (Chikka, 2016) and SVM classifiers (Tourille et al., 2016; Lee et al., 2016; AAl Abdulsalam et al., 2016) for the CR task. 3 Corpus Presentation The MERLOT corpus is composed of clinical documents written in French from a Gastroenterology, Hepatology and Nutrition department. These documents have been de-identified (Grouin and Névéol, 2014) and annotated with entities, temporal expressions and relations (Deléger et al., 2014). The THYME corpus is a collection of clinical texts written in English from a cancer department that have been released during the Clinical TempEval campaigns. This corpus contains doc"
E17-2117,W11-0419,0,0.458059,"te, F-91191 France. olivier.ferret@cea.fr Xavier Tannier LIMSI, CNRS Univ. Paris-Sud Université Paris-Saclay xavier.tannier@limsi.fr Aurélie Névéol LIMSI, CNRS Université Paris-Saclay aurelie.neveol@limsi.fr Abstract In the DR task, the objective is to temporally locate EVENT entities according to the Document Creation Time of the document in which they occur. Possible tags are Before, Before-Overlap, Overlap and After. In the CR task, the objective is to identify temporal inclusion relations between pairs of entities (EVENT and/or TIMEX3) formalized as narrative container relations following Pustejovsky and Stubbs (2011). In this context, we build on Tourille et al. (2016) and show how this type of model can be applied for extracting temporal relations from clinical texts similarly in two languages. We experimented more specifically on two corpora: the THYME corpus (Styler IV et al., 2014), a corpus of de-identified clinical notes in English from the Mayo Clinic and the MERLOT corpus (Campillos et al., to appear), a comparable corpus in French from a group of French hospitals. In this paper, we present a method for temporal relation extraction from clinical narratives in French and in English. We experiment o"
E17-2117,S16-1194,0,0.117119,"f classical temporal relations (Allen, 1983) yields better annotation while keeping most of the useful temporal information intact. The concept of narrative container is illustrated in Figure 1 and described further in Pustejovsky and Stubbs (2011). val task related to the topic for the past two years (Bethard et al., 2015; Bethard et al., 2016). Its first track focused on extracting clinical events and temporal expressions, while its second track included DR and CR tasks. Different approaches were implemented by the teams, among which SVM classifiers (Lee et al., 2016; Tourille et al., 2016; Cohan et al., 2016; AAl Abdulsalam et al., 2016) and CRF approaches (Caselli and Morante, 2016; AAl Abdulsalam et al., 2016) for the DR task, and CRF, Convolutional neural networks (Chikka, 2016) and SVM classifiers (Tourille et al., 2016; Lee et al., 2016; AAl Abdulsalam et al., 2016) for the CR task. 3 Corpus Presentation The MERLOT corpus is composed of clinical documents written in French from a Gastroenterology, Hepatology and Nutrition department. These documents have been de-identified (Grouin and Névéol, 2014) and annotated with entities, temporal expressions and relations (Deléger et al., 2014). The TH"
E17-2117,deleger-etal-2014-annotation,1,0.896819,"Missing"
E17-2117,S16-1175,1,0.820831,"SI, CNRS Univ. Paris-Sud Université Paris-Saclay xavier.tannier@limsi.fr Aurélie Névéol LIMSI, CNRS Université Paris-Saclay aurelie.neveol@limsi.fr Abstract In the DR task, the objective is to temporally locate EVENT entities according to the Document Creation Time of the document in which they occur. Possible tags are Before, Before-Overlap, Overlap and After. In the CR task, the objective is to identify temporal inclusion relations between pairs of entities (EVENT and/or TIMEX3) formalized as narrative container relations following Pustejovsky and Stubbs (2011). In this context, we build on Tourille et al. (2016) and show how this type of model can be applied for extracting temporal relations from clinical texts similarly in two languages. We experimented more specifically on two corpora: the THYME corpus (Styler IV et al., 2014), a corpus of de-identified clinical notes in English from the Mayo Clinic and the MERLOT corpus (Campillos et al., to appear), a comparable corpus in French from a group of French hospitals. In this paper, we present a method for temporal relation extraction from clinical narratives in French and in English. We experiment on two comparable corpora, the MERLOT corpus for Frenc"
E17-2117,N15-1069,0,0.0152944,"essions and document creation time from clinical notes. This work, based on a feature engineering approach, obtained competitive results with the current state-of-theart and led to two main conclusions. First, the use of word embeddings in place of lexical features tends to degrade performance. Second, our feature engineering approach can be applied with comparable results to two different languages, English and French in our case. To follow-up with the first conclusion, we would like to test a more integrated approach for using embeddings, either by turning all features into embeddings as in Yang and Eisenstein (2015) or by adopting a neural network architecture as in Chikka (2016). Concerning the CR task, results are separated by a 10 percent gap (0.65 for the MERLOT corpus and 0.53 for the THYME corpus). Results obtained for the THYME corpus are coherent with those obtained by Tourille et al. (2016) on the Clinical TempEval 2016 evaluation corpus2 . We increased the recall value in comparison to their results (from 0.436 to 0.47) but this measure is still the main point to improve. More globally, the best results of the Clinical TempEval shared task were 0.843 (accuracy) for the DR task and 0.573 (F1-Mea"
E17-2117,P14-5010,0,\N,Missing
E17-2117,S16-1193,0,\N,Missing
E17-2117,S16-1195,0,\N,Missing
F14-2030,I08-1050,0,0.0465507,"Missing"
F14-2030,D09-1096,0,0.0284503,"Missing"
F14-2030,P10-1052,0,0.0302535,"Missing"
F14-2030,W06-3309,0,0.0600615,"Missing"
F14-2030,W09-3603,0,0.0477471,"Missing"
F14-2030,E12-2021,0,0.0807707,"Missing"
F14-2030,tepper-etal-2012-statistical,0,0.0895352,"Missing"
F14-2030,J02-4002,0,0.224647,"Missing"
L16-1470,W14-3302,0,0.108124,"Missing"
L16-1470,P07-2045,0,0.00954732,"rce>Target”, when the alignment is correct but the source contains more information than the target; (c) “Target>Source”, when the alignment is correct but the target contains more information than the source; (d) “Overlap”, when there is an overlap in the information content of both sentences but they cannot be considered aligned; (e) “No alignment”, when the sentences are unrelated and there is no alignment. 3.5. Machine Translation Evaluation We trained a statistical MT system on the parallel corpora to demonstrate the capabilities of the proposed corpus, For this purpose, we used Moses11 (Koehn et al., 2007) as the statistical MT tool. The BLEU score (Papineni et al., 2002) has been used as the translation evaluation measure. 9 http://nlp.cs.nyu.edu/GMA/ https://github.com/cfedermann/Appraise 11 http://www.statmt.org/moses 10 2944 Figure 2: Screen-shot of the Appraise tool during the validation of the alignment of a EN/PT pair of sentences. 4. Results and Evaluation In this section, we present the statistics on our corpus and its evaluation regarding two aspects: the quality of sentence alignment and the corpus suitability for training and evaluation of MT algorithms. 4.1. Corpus Statistics Table"
L16-1470,2005.mtsummit-papers.11,0,0.24186,"gy for constructing parallel corpus of scientific publication for biomedicine derived from the Scielo database. The work-flow is illustrated in Figure 1 and each phase is described in details below. 3.1. The development of parallel corpora as translation memories and use for training MT systems has been an active area of research. Previous work has addressed various types of documents, domains and language pairs. Popular parallel corpora in specialized domains include the News Commentary corpus5 , composed of news related documents and commonly used in the WMT challenges, the EuroParl corpus (Koehn, 2005), derived from the European Parliament proceedings, and the Acquis corpus (Steinberger et al., 2006), which contains legal documents for more than 20 European languages. 4 http://www.statmt.org/wmt16/ biomedical-translation-task.html 5 http://www.statmt.org/wmt16/ translation-task.html The OPUS corpus (Tiedemann, 2012) contains some subdomain data covering the biomedical domain with EMEA (European Medicines Agency) documents, which were later used, along with MEDLINE® titles, to produce parallel annotated data following the CLEF-ER 2013 challenge (Kors et al., 2015) and for named-entity recogn"
L16-1470,J10-4005,0,0.0216653,"ts. 6 http://hana.sap.com http://www.scielo.mec.pt/scielo. php?script=sci_abstract&pid= S0874-48902010000300006&lng=pt&nrm=iso& tlng=pt 8 https://opennlp.apache.org/ 7 Scielo contains many entries only available in one of the languages or in languages other than English, e.g., in both Portuguese and Spanish, given that the focus of the database is in the Latin American journals. These documents constitute our monolingual corpus, given that in-domain monolingual corpora are also a valuable resource for training and evaluation of language models, one of the components of statistical MT systems (Koehn, 2010). 3.3. Document alignment We automatically aligned sentences from titles and abstracts for the language pairs using the Geometric Mapping and Alignment (GMA) tool9 . No language-specific resources were provided while using the GMA tool, such as bilingual dictionaries. As discussed above, many articles in Scielo do not have both title and abstract available for a particular language, but just one of them. For this reason, we decided to align titles and abstracts separately. 3.4. Manual validation We manually checked the automatic alignment generated by the GMA tool to ensure the quality of the"
L16-1470,P02-1040,0,0.0995567,"ns more information than the target; (c) “Target>Source”, when the alignment is correct but the target contains more information than the source; (d) “Overlap”, when there is an overlap in the information content of both sentences but they cannot be considered aligned; (e) “No alignment”, when the sentences are unrelated and there is no alignment. 3.5. Machine Translation Evaluation We trained a statistical MT system on the parallel corpora to demonstrate the capabilities of the proposed corpus, For this purpose, we used Moses11 (Koehn et al., 2007) as the statistical MT tool. The BLEU score (Papineni et al., 2002) has been used as the translation evaluation measure. 9 http://nlp.cs.nyu.edu/GMA/ https://github.com/cfedermann/Appraise 11 http://www.statmt.org/moses 10 2944 Figure 2: Screen-shot of the Appraise tool during the validation of the alignment of a EN/PT pair of sentences. 4. Results and Evaluation In this section, we present the statistics on our corpus and its evaluation regarding two aspects: the quality of sentence alignment and the corpus suitability for training and evaluation of MT algorithms. 4.1. Corpus Statistics Table 1 presents statistics of the training data, including the proporti"
L16-1470,steinberger-etal-2006-jrc,0,0.0927572,"rom the Scielo database. The work-flow is illustrated in Figure 1 and each phase is described in details below. 3.1. The development of parallel corpora as translation memories and use for training MT systems has been an active area of research. Previous work has addressed various types of documents, domains and language pairs. Popular parallel corpora in specialized domains include the News Commentary corpus5 , composed of news related documents and commonly used in the WMT challenges, the EuroParl corpus (Koehn, 2005), derived from the European Parliament proceedings, and the Acquis corpus (Steinberger et al., 2006), which contains legal documents for more than 20 European languages. 4 http://www.statmt.org/wmt16/ biomedical-translation-task.html 5 http://www.statmt.org/wmt16/ translation-task.html The OPUS corpus (Tiedemann, 2012) contains some subdomain data covering the biomedical domain with EMEA (European Medicines Agency) documents, which were later used, along with MEDLINE® titles, to produce parallel annotated data following the CLEF-ER 2013 challenge (Kors et al., 2015) and for named-entity recognition and normalization for French in the QUAERO corpus (N´ev´eol et al., 2014). Previous attempts t"
L16-1470,tiedemann-2012-parallel,0,0.341453,"Missing"
L18-1025,D15-1301,0,0.0261746,"to reproducibility is a problem because without them, we cannot compare studies of reproducibility. A number of such studies have appeared very recently, and in general, the results have been depressing. Multiple studies over the course of the past two years have reported widespread failures of reproducibility (Collaboration and others, 2015; Collberg et al., 2015). They range from unusually large-scale studies in psychology (Collaboration and others, 2015), to surprisingly large ones in computer science (Collberg et al., 2015), to case studies in natural language processing (Schwartz, 2010; Borgholt et al., 2015; Cohen et al., 2016; Gomes et al., 2016; N´ev´eol et al., 2016; Cassidy and Estival, 2017; Kilicoglu, 2017; Mieskes, 2017). Yet, it is still quite difficult to get even a rough sense of the actual scale of the problem in natural language processing, because the lack of agreement about what exactly is being assessed makes it difficult to compare findings across papers on reproducibility issues. 156 To address this problem of a lack of consensus definitions, this paper proposes a set of dimensions of reproducibility. Perhaps counter-intuitively, we first give the definition of replicability or"
L18-1025,J92-1002,0,0.025332,"anguage-related value that stimulated an enormous amount of academic work, some of which has been evaluated with respect to the extent to which it does or does not reproduce the values reported in (Shannon, 1951). For example, (Cover and King, 1978) used a very different method from Shannon’s original one and found a value of 1.3 bits for the entropy of written English. The paper explicitly states that this value “agrees well with Shannon’s estimate,” suggesting that the authors considered their value to have reproduced Shannon’s original value in (Shannon, 1951)3 . In a very different tone, (Brown et al., 1992) reported an upper bound of exactly 1.75 bits, but did not explicitly compare that to previous findings, although it is clear from the paper that they considered it different from—and better than—previously reported values. As the authors put it: We see this paper as a gauntlet thrown down before the computational linguistics community. A relevant value from our papers that was not reproduced is the mean value for the frequency of negation. We reported this in our papers (Cohen et al., 2010) and (Cohen et al., 2017a). They were different by roughly a factor of 2, even though we used the same c"
L18-1025,daelemans-hoste-2002-evaluation,0,0.0820242,"as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics and natural language processing that integrates these strengths of our field and also explores the relationships between natural language processing; computational and corpus linguistics; artificia"
L18-1025,P13-1166,0,0.316125,"Missing"
L18-1025,H90-1013,0,0.482748,", it can be quite difficult to achieve (Fokkens et al., 2013; N´ev´eol et al., 2016), and the causes of reproducibility problems can be well-hidden—see (Johnson et al., 2007; Cohen et al., 2017b), as well as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computati"
L18-1025,H94-1017,0,0.0985049,"difficult to achieve (Fokkens et al., 2013; N´ev´eol et al., 2016), and the causes of reproducibility problems can be well-hidden—see (Johnson et al., 2007; Cohen et al., 2017b), as well as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics"
L18-1025,W16-6110,1,0.905358,"Missing"
L18-1025,J05-1004,0,0.0161754,"e processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics and natural language processing that integrates these strengths of our field and also explores the relationships between natural language processing; computational and corpus linguistics; artificial intelligence, theoretical linguistics, and cognitive science (Cori et al., 2002). (See also (Cori and L´eon, 2002) for a dis"
L18-1025,W10-1726,0,0.114498,"initions related to reproducibility is a problem because without them, we cannot compare studies of reproducibility. A number of such studies have appeared very recently, and in general, the results have been depressing. Multiple studies over the course of the past two years have reported widespread failures of reproducibility (Collaboration and others, 2015; Collberg et al., 2015). They range from unusually large-scale studies in psychology (Collaboration and others, 2015), to surprisingly large ones in computer science (Collberg et al., 2015), to case studies in natural language processing (Schwartz, 2010; Borgholt et al., 2015; Cohen et al., 2016; Gomes et al., 2016; N´ev´eol et al., 2016; Cassidy and Estival, 2017; Kilicoglu, 2017; Mieskes, 2017). Yet, it is still quite difficult to get even a rough sense of the actual scale of the problem in natural language processing, because the lack of agreement about what exactly is being assessed makes it difficult to compare findings across papers on reproducibility issues. 156 To address this problem of a lack of consensus definitions, this paper proposes a set of dimensions of reproducibility. Perhaps counter-intuitively, we first give the definiti"
L18-1043,W14-3302,0,0.0869344,"Missing"
L18-1043,federmann-2010-appraise,0,0.42498,"slation, others were created more freely and the content in one language could be structured differently in the other language. We made the hypothesis that documents could nonetheless be aligned at the sentence level and we relied on automatic tools for performing the alignment. We identified alignment tools based on an evaluation of alignment for literary texts which is a genre that also features fuzzy alignment(Xu et al., 2015). Quality checking. After automatically aligning the sentences of the documents, we manually checked a sample of our corpora. This was carried out using the Appraise (Federmann, 2010) tool, and we evaluated whether the aligned sentences where correct or whether more information was available in one language or the other. Native speakers of each foreign language were responsible for this task. 4. Application to Three Biomedical Corpora Here we describe the three corpora that we developed and highlight the differences regarding the particular tools that we used for the various steps above. EDP We identified five open access CC-BY journals, referenced EDP Sciences4 as having content in French and in English: the articles were originally written in French but the journals also"
L18-1043,hellrich-etal-2014-collaboratively,0,0.0539987,"Work One of the first efforts that involved the development of large-scale shareable parallel corpora for the biomedical domain was the OPUS collection that contained medical documents from the European Medicines Agency (EMEA) (Tiedemann, 2012)2 . A number of biomedical parallel (Widdows et al., 2002; Ozdowska et al., 2005; Deleger et al., 2009) and comparable corpora (Chiao and Zweigenbaum, 2004) have been used for terminology translation only. Similarly, the Mantra project (Kors et al., 2015; 1 2 286 http://github.com/biomedical-translation-corpora/corpora http://opus.lingfil.uu.se/EMEA.php Hellrich et al., 2014) provided corpora of biomedical articles automatically annotated for named-entity recognition for English, Spanish, French, German and Dutch. The corpora included MEDLINE titles, EMEA documents and patents in the biomedical field. While the goal of this project was to leverage annotation transfer from English to other languages to expand terminology coverage in languages other than English, to our knowledge, the corpus has not been used for machine translation. After general purpose machine translation systems were found to perform poorly on medical text (Zeng-Treitler et al., 2010), the use o"
L18-1043,2013.mtsummit-papers.10,0,0.0134367,"MT17). Then, we manually validated sentence segmentation in both languages in order to create a reference corpus that may be used to train and evaluate sentence segmentation tools. Therefore, Updated versions of the corpus reflect the manual sentence segmentation. Table 2 presents detailed statistics of the contents of the biomedical parallel corpora that we developped. Table 3 presents an overview of the corpora with the training and test set splits that were offered thoroughout the WMT campaigns. Sentence Alignment: GMA was used for Scielo and ReBEC. Due to difficulties to install GMA, Yasa(Lamraoui and Langlais, 2013) was used for EDP; however, Yasa may be limited to the language pair en/fr. We can refer readers to (Xu et al., 2015) for a discussion and evaluation of alignment tools for a specialized domain (literary texts). Nevertheless, both tools provided good automatic alignments (?). Additionally, GMA was used for two languages (es and pt) and two document types (scientific publications and clinical trials). 5.2. 6.2. 5. 5.1. Results Datasets Descriptive Statistics Quality Assessment We also provide a summary of the correct alignment rate for the various corpora, as shown in Table 4. The alignment was"
L18-1043,L16-1470,1,0.833842,"Missing"
L18-1043,W17-2507,1,0.921227,"ument across languages. However, despite its importance for the general population and researchers, there are very few parallel and comparable corpora specific for this domain. In this paper, we present an overview of the stateof-the-art on parallel and comparable corpora for the biomedical domain. In a scoping review of existing resources, we characterize the resources available by language pairs and document type and provide pointers to more in-depth descriptions of the resources. Additionally, we present the parallel corpora that we assembled and built, such as EDP (French/English), ReBEC (Neves, 2017) (Portuguese/English) and Scielo (Neves et al., 2016) (French/English, Portuguese/English and Spanish/English). For the latter, we provide details on the corpus construction, insights on the data and their utilization for the biomedical task (Bojar et al., 2016; Jimeno Yepes et al., 2017) of the Conference for Machine Translation (WMT). All corpora are available in our repository in GitHub1 . 2. Related Work One of the first efforts that involved the development of large-scale shareable parallel corpora for the biomedical domain was the OPUS collection that contained medical documents from the"
L18-1043,tiedemann-2012-parallel,0,0.541082,"eves et al., 2016) (French/English, Portuguese/English and Spanish/English). For the latter, we provide details on the corpus construction, insights on the data and their utilization for the biomedical task (Bojar et al., 2016; Jimeno Yepes et al., 2017) of the Conference for Machine Translation (WMT). All corpora are available in our repository in GitHub1 . 2. Related Work One of the first efforts that involved the development of large-scale shareable parallel corpora for the biomedical domain was the OPUS collection that contained medical documents from the European Medicines Agency (EMEA) (Tiedemann, 2012)2 . A number of biomedical parallel (Widdows et al., 2002; Ozdowska et al., 2005; Deleger et al., 2009) and comparable corpora (Chiao and Zweigenbaum, 2004) have been used for terminology translation only. Similarly, the Mantra project (Kors et al., 2015; 1 2 286 http://github.com/biomedical-translation-corpora/corpora http://opus.lingfil.uu.se/EMEA.php Hellrich et al., 2014) provided corpora of biomedical articles automatically annotated for named-entity recognition for English, Spanish, French, German and Dutch. The corpora included MEDLINE titles, EMEA documents and patents in the biomedica"
L18-1043,widdows-etal-2002-using,0,0.393247,"h and Spanish/English). For the latter, we provide details on the corpus construction, insights on the data and their utilization for the biomedical task (Bojar et al., 2016; Jimeno Yepes et al., 2017) of the Conference for Machine Translation (WMT). All corpora are available in our repository in GitHub1 . 2. Related Work One of the first efforts that involved the development of large-scale shareable parallel corpora for the biomedical domain was the OPUS collection that contained medical documents from the European Medicines Agency (EMEA) (Tiedemann, 2012)2 . A number of biomedical parallel (Widdows et al., 2002; Ozdowska et al., 2005; Deleger et al., 2009) and comparable corpora (Chiao and Zweigenbaum, 2004) have been used for terminology translation only. Similarly, the Mantra project (Kors et al., 2015; 1 2 286 http://github.com/biomedical-translation-corpora/corpora http://opus.lingfil.uu.se/EMEA.php Hellrich et al., 2014) provided corpora of biomedical articles automatically annotated for named-entity recognition for English, Spanish, French, German and Dutch. The corpora included MEDLINE titles, EMEA documents and patents in the biomedical field. While the goal of this project was to leverage a"
L18-1582,E12-1058,0,0.0318444,"dology choices for automating the literature screening, and to find ways to improve the quality of constructing datasets used to train such retrieval methods. Second, we experiment on an existing reference dataset and introduce a new, complementary dataset. 3681 2. Dataset Yearbook Related Work Methods for automation have been attempted with varying degrees of success in technology assisted review in several topics in biomedicine (O’Mara-Eves et al., 2015). Technology assisted review has also been implemented in other fields with similarly stringent recall requirements, such as patent search (Stein et al., 2012), and electronic discovery (Grossman and Cormack, 2011). Automated document discovery is typically cast as a ranking or classification problem (O’Mara-Eves et al., 2015). Common methods for automation include Support Vector Machines and variants of Naive Bayes, including Complement Naive Bayes (Matwin et al., 2010), and Multinomial Naive Bayes (Matwin and Sazonova, 2012). Other methods have been tried, including Voting Perceptrons (Cohen et al., 2006), Decision Trees (Bekhuis and DemnerFushman, 2010), Evolutional S VM (Bekhuis and DemnerFushman, 2010), WAODE (Bekhuis and Demner-Fushman, 2010),"
P17-2035,S16-1195,0,0.0149202,"has been offering a shared task related to temporal relation extraction from clinical narratives over the past two years (Bethard et al., 2015, 2016). Relying on the THYME corpus, the task challenged participants to extract EVENT and TIMEX 3 entities and then to extract narrative container relations and document creation time relations. Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations. Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhub"
P17-2035,E17-2118,0,0.5086,"have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract containment relations from the THYME corpus. From a more general perspective, relation extraction and classification is a task explored by many approaches, from fully unsupervised to fully supervised. Recent years have seen an increasing interest for the use of neural approaches. Introduction Temporal information extraction from clinical health records allows for a fine-grained analysis of patient health history. Providing medical staff with patient timelines could lead to"
P17-2035,P15-1061,0,0.0148876,"fy temporal relations between pairs of entities formalized as narrative container relations. 224 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: Descriptive statistics about the train and test parts of the THYME corp"
P17-2035,D13-1137,0,0.0126346,"entities and we focus on containment relation extraction where the objective is to identify temporal relations between pairs of entities formalized as narrative container relations. 224 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365"
P17-2035,S15-2136,0,0.16542,"Missing"
P17-2035,N16-1030,0,0.0615328,"Missing"
P17-2035,S16-1192,0,0.0751139,"n the THYME corpus, the task challenged participants to extract EVENT and TIMEX 3 entities and then to extract narrative container relations and document creation time relations. Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations. Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract containment relations from the THYME corpus. From a more general perspective, relation extraction and classification is a task exp"
P17-2035,S16-1201,0,0.0542222,"Missing"
P17-2035,S16-1194,0,0.0194281,"ed task related to temporal relation extraction from clinical narratives over the past two years (Bethard et al., 2015, 2016). Relying on the THYME corpus, the task challenged participants to extract EVENT and TIMEX 3 entities and then to extract narrative container relations and document creation time relations. Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations. Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract"
P17-2035,D15-1278,0,0.0130017,"n containment relation extraction where the objective is to identify temporal relations between pairs of entities formalized as narrative container relations. 224 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: De"
P17-2035,S16-1175,1,0.917526,"xtraction from clinical narratives over the past two years (Bethard et al., 2015, 2016). Relying on the THYME corpus, the task challenged participants to extract EVENT and TIMEX 3 entities and then to extract narrative container relations and document creation time relations. Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations. Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract containment relations from the THYME corp"
P17-2035,W16-2914,0,0.403651,"elations. Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations. Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract containment relations from the THYME corpus. From a more general perspective, relation extraction and classification is a task explored by many approaches, from fully unsupervised to fully supervised. Recent years have seen an increasing interest for the use of neural approaches. Introduction Temporal"
P17-2035,E17-2117,1,0.813839,"Missing"
P17-2035,D15-1206,0,0.0113271,"ort Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: Descriptive statistics about the train and test parts of the THYME corpus. Data 3.2 Corpus Presentation Preprocessing We preprocessed the corpus using cTAKES (Savova et al., 2010), an open-source natural language processing system for the extraction of in"
P17-2035,P16-1105,0,0.0225042,"ational Linguistics (Short Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: Descriptive statistics about the train and test parts of the THYME corpus. Data 3.2 Corpus Presentation Preprocessing We preprocessed the corpus using cTAKES (Savova et al., 2010), an open-source natural language processing system for the"
P17-2035,C14-1220,0,0.00961922,"between pairs of entities formalized as narrative container relations. 224 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: Descriptive statistics about the train and test parts of the THYME corpus. Data 3.2 Corpus"
P17-2035,W11-0419,0,0.048557,"poral expressions are assigned a Class attribute. Possible values for these attributes are presented in Table 2. 4 Narrative containers can be apprehended as temporal buckets in which several events may be included. These containers are anchored by temporal expressions, medical events or other concepts. Styler IV et al. (2014) argue that the use of narrative containers instead of classical temporal relations (Allen, 1983) yields better annotation while keeping most of the useful temporal information intact. The concept of narrative container is illustrated in Figure 1 and described further in Pustejovsky and Stubbs (2011). Task Description The container relation extraction task can be cast as a 3-class classification problem. For each combination of EVENT and/or TIMEX 3 from left to right, three cases are possible: • the first entity temporally contains the second entity, • the first entity is temporally contained by the second entity, • there is no temporal containment relation between the entities. Intra- and inter-sentence relation detection can be seen as two different tasks with specific features. Intra-sentence relations can benefit from intra-sentential clues such as adverbs (e.g. during) or pronouns (e"
P17-2035,P16-2034,0,0.00384149,"s 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: Descriptive statistics about the train and test parts of the THYME corpus. Data 3.2 Corpus Presentation Preprocessing We preprocessed the corpus using cTAKES (Savova et al., 2010), an open-source natural language processing system for the extraction of information from elect"
P17-2035,D11-1014,0,0.090597,"Missing"
P17-2035,D13-1170,0,0.00329242,"Missing"
P17-2035,E17-1108,0,\N,Missing
R19-1089,L18-1025,1,0.896139,"Missing"
R19-1089,dakota-kubler-2017-towards,0,0.0480764,"Missing"
R19-1089,Q17-1033,0,0.0541077,"Missing"
R19-1089,P13-1166,0,0.272847,"Missing"
R19-1089,D17-1076,0,0.0438822,"Missing"
R19-1089,E99-1046,0,0.432502,"Missing"
R19-1089,E17-4003,0,0.0452396,"Missing"
R19-1089,W17-1603,1,0.897332,"Missing"
R19-1089,C18-1097,0,0.0384908,"Missing"
R19-1089,W16-6110,1,0.903417,"Missing"
S16-1175,S16-1165,0,0.0431462,"relations. The latter consists of two subtasks. In the Document Creation Time Relation subtask (DR), participants are challenged to identify relations between the events and the document creation time. For the Container Relation subtask (CR), participants have to identity container relations between entities. Participants may submit either a complete system extracting entities and relations or focus on either the entity extraction or relation extraction (using the gold standard entities provided by the organizers). More details about the task and the definition of each subtask can be found in Bethard et al. (2016). In this paper, we present our submission for the CR and DR subtasks based on gold-standard entities (phase 2). Our global approach, which is illustrated in Figure 1, tackles the identification of temporal relations as a set of supervised classification tasks. We submitted two runs, one using plain lexical features and one using word embeddings computed on a large clinical corpus. We obtained scores well above the median scores in both subtasks. The remainder of this paper is organized as follows. Section 2 presents our system for the DR subtask while Section 3 describes our system for the CR"
S16-1175,P05-1022,0,0.0561315,"tistical feature selection. We used the Scikit-learn machine learning library (Pedregosa et al., 2011) for both implementing our classification models and performing statistical feature selection. 4.3 Corpus Preprocessing We applied a four-step preprocessing on the 440 texts that were provided for the subtasks. First, we used NLTK (Loper and Bird, 2002) to segment the texts into sentences with the Punkt Sentence Tokenizer pre-trained model for English provided within the framework. The second step consisted of parsing the resulting sentences. For this task, we used the BLLIP Reranking Parser (Charniak and Johnson, 2005) and a pre-trained biomedical parsing model (McClosky, 2010). In the third step, we lemmatized the corpus using BioLemmatizer (Liu et al., 2012), a tool built for processing the biomedical literature. We used the Part-Of-Speech tags from the previous step as parameters for the lemmatization. The last step consisted in using Metamap (Aronson and Lang, 2010) to detect biomedical events and linking them, after disambiguation, to their related UMLS® (Unified Medical Language System) concept. We chose to keep biomedical entities that had a 1140 span overlapping with at least one entity of the gold"
S16-1175,W02-0109,0,0.0472586,"mbeddings b Table 2: Machine learning algorithms and parameters used for the final submission The machine learning algorithms used for the final submission are presented in Table 2 together with their parameters and the percentage of the feature space kept after statistical feature selection. We used the Scikit-learn machine learning library (Pedregosa et al., 2011) for both implementing our classification models and performing statistical feature selection. 4.3 Corpus Preprocessing We applied a four-step preprocessing on the 440 texts that were provided for the subtasks. First, we used NLTK (Loper and Bird, 2002) to segment the texts into sentences with the Punkt Sentence Tokenizer pre-trained model for English provided within the framework. The second step consisted of parsing the resulting sentences. For this task, we used the BLLIP Reranking Parser (Charniak and Johnson, 2005) and a pre-trained biomedical parsing model (McClosky, 2010). In the third step, we lemmatized the corpus using BioLemmatizer (Liu et al., 2012), a tool built for processing the biomedical literature. We used the Part-Of-Speech tags from the previous step as parameters for the lemmatization. The last step consisted in using Me"
S16-1175,N10-1004,0,0.0122909,"brary (Pedregosa et al., 2011) for both implementing our classification models and performing statistical feature selection. 4.3 Corpus Preprocessing We applied a four-step preprocessing on the 440 texts that were provided for the subtasks. First, we used NLTK (Loper and Bird, 2002) to segment the texts into sentences with the Punkt Sentence Tokenizer pre-trained model for English provided within the framework. The second step consisted of parsing the resulting sentences. For this task, we used the BLLIP Reranking Parser (Charniak and Johnson, 2005) and a pre-trained biomedical parsing model (McClosky, 2010). In the third step, we lemmatized the corpus using BioLemmatizer (Liu et al., 2012), a tool built for processing the biomedical literature. We used the Part-Of-Speech tags from the previous step as parameters for the lemmatization. The last step consisted in using Metamap (Aronson and Lang, 2010) to detect biomedical events and linking them, after disambiguation, to their related UMLS® (Unified Medical Language System) concept. We chose to keep biomedical entities that had a 1140 span overlapping with at least one entity of the gold standard. 5 Results and Discussion In Table 3, we present th"
S16-1175,P06-4018,0,\N,Missing
S16-1175,Q14-1012,0,\N,Missing
S17-2098,S16-1195,0,0.0310096,"to extract containment (CONTAINS) relations between EVENT and/or TIMEX3 as well as Document Creation Time (DCT) relations between EVENT entities and documents in which they are embedded. The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 597–602, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics sentence relations that do not span over more than three sentences. By doing so, we ob"
S17-2098,S16-1194,0,0.0219105,"TIMEX3 as well as Document Creation Time (DCT) relations between EVENT entities and documents in which they are embedded. The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 597–602, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics sentence relations that do not span over more than three sentences. By doing so, we obtain a manageable training corpus size with less unbalanced c"
S17-2098,S16-1198,0,0.0296996,"n which they are embedded. The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 597–602, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics sentence relations that do not span over more than three sentences. By doing so, we obtain a manageable training corpus size with less unbalanced classes while keeping a good coverage. 3 Corpus Preprocessing We preprocessed the corpus"
S17-2098,S16-1190,0,0.0273623,"cument Creation Time (DCT) relations between EVENT entities and documents in which they are embedded. The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 597–602, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics sentence relations that do not span over more than three sentences. By doing so, we obtain a manageable training corpus size with less unbalanced classes while keeping a good"
S17-2098,S16-1200,0,0.0644888,"Missing"
S17-2098,S15-2136,0,0.0813531,"Missing"
S17-2098,S17-2093,0,0.0294291,"us editions of the challenge (Bethard et al., 2015, 2016), the first group of subtasks concerns medical event (EVENT) and temporal expression (TIMEX3) extraction from raw text. In a second group of subtasks, participants are challenged to extract containment (CONTAINS) relations between EVENT and/or TIMEX3 as well as Document Creation Time (DCT) relations between EVENT entities and documents in which they are embedded. The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International W"
S17-2098,N16-1030,0,0.0591663,"Missing"
S17-2098,S16-1201,0,0.0752453,"Missing"
S17-2098,S16-1197,0,0.0220144,"The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 597–602, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics sentence relations that do not span over more than three sentences. By doing so, we obtain a manageable training corpus size with less unbalanced classes while keeping a good coverage. 3 Corpus Preprocessing We preprocessed the corpus using cTAKES 3.2.2 (Savova et al.,"
S17-2098,D15-1063,0,0.0303953,"es while keeping a good coverage. 3 Corpus Preprocessing We preprocessed the corpus using cTAKES 3.2.2 (Savova et al., 2010), an open-source natural language processing system for the extraction of information from electronic health records. We extracted sentence and token boundaries, as well as token types and semantic types of the entities that have a span overlap with a least one gold standard EVENT entity of the THYME corpus. This information was added to the set of gold standard attributes available for EVENT entities in the corpus. We also preprocessed the corpus using HeidelTime 2.2.1 (Strötgen and Gertz, 2015), a multilingual domain-sensitive temporal tagger, and used the results to further extend our feature set. 4 4.1 Figure 1: Neural model for EVENT extraction. – – – – EVENT type attribute, EVENT plain lexical form, EVENT position within the document, POS tags of the verbs within the right and left contexts of the considered entity, – EVENT POS tag, – type or class of the other entities that are present within the left and right contexts, – token unigrams and bigrams within a window around the entity. Models Entity Extraction Our approach relies on Long Short-Term Memory Networks (LSTMs) (Hochre"
S17-2098,S16-1175,1,0.858559,"roblem. For each combination E1 – E2 of EVENT and/or TIMEX3 from left to right, three cases are possible: – E1 temporally contains E2, – E1 is temporally contained by E2, – there is no relation between E1 and E2. Intra- and inter-sentence relation detection can be seen as two different tasks with specific features. Intra-sentence relations can benefit from intra-sentential clues such as adverbs (e.g. during) or pronouns (e.g. which) which are not available at the inter-sentence level. Furthermore, past work on the topic seems to indicate that this differentiation improves overall performance (Tourille et al., 2016). We have adopted this approach by building two separate classifiers, one for intra-sentence relations and one for inter-sentence relations. If we were to consider all combinations of entities within documents for inter-sentence relations, it would result in a very large training corpus with very few positive examples. In order to cope with this issue, we limit our experiments to interIntroduction SemEval 2017 Task 12 offers 6 subtasks addressing medical event recognition and temporal reasoning in the clinical domain using the THYME corpus (Styler IV et al., 2014). Similarly to the two previou"
W09-1319,J08-4004,0,0.0298997,"aries, our phrase finding algorithm in pseudo-code is shown in Figure 1. The output of this algorithm may then be filtered by setting a threshold on the PMA values to accept. 5 5.1 Results Assessing the difficulty of the task To assess the difficulty of disease recognition, we computed the inter-annotator agreement (IAA) on the 300-query corpus. Agreement was computed at the disease mention level for all three annotators and at the disease concept level for the two annotators who produced UMLS annotations. Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. However, in our case, agreement was partly computed on a very large set of categories (UMLS concepts) so we decided to use Knowtator’s built-in feature, which computes IAA as the percentage of agreement and allows partial string matches. For example, in the query “dog model transient ischemic attacks”, annotator 1 selected “ischemic attacks” as a disorder while annotator 2 and 3 selected “transient ischemic attacks” as UMLS concept C0007787: Attacks, Transient Ischemic. In this case, at the subclass level (“disorder”) we have a match for"
W09-1319,W06-3305,1,0.781257,"model actually chooses the values of all the p and q quantities to optimize the prob values over all of D and N. For this work we have extended the approach to include a quantity qual q1 p1 k j 2 1 qj k i 2 qi2 pi k j i 1 1 qj prob (3) which represents a weighted average of all the quality numbers qi . We apply this formula to obtain qual as long as prob 0.5. If prob 0.5 we Table 3: Description of the training and test sets 4.1 (1) ph t1t2 tk and for each ti the corresponding numbers pi and qi we estimate the probability that ph D by Priority Model The priority model was first introduced in (Tanabe and Wilbur, 2006) and is adapted here to detect disease mentions in free text. Because our evaluation is performed at the concept level, the mentions extracted by the model are then mapped to UMLS using MetaMap. The priority model approach is based on two sets of phrases: one names of diseases, D, and one names of non-diseases, N. One trains the model to assign two numbers, p and q, to each token t that appears in a phrase in either D or N. Roughly, p is the probability that a phrase from D or N that has the token t in it is actually from D and q is the relative weight that should be assigned to t for this pur"
W09-1319,W07-1014,1,\N,Missing
W14-4907,J08-4004,0,0.11081,"tion quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotator modeling. Overall, past work shows that creating high-quality manual annotations is time-consuming and often requires the work of experts. The time burden is distributed between the sheer creation of the annotations, the act of producing multiple annotations for the same data and the subsequent analysis of multiple annotations to resolve conflicts, viz. the creation o"
W14-4907,J92-4003,0,0.100316,"Missing"
W14-4907,W11-0408,0,0.0224984,"s quality and affordable cost. Inter-annotator agreement (IAA) has been used as an indicator of annotation quality. Early work showed that the use of automatic preannotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006). Efforts have investigated methods to reduce the human workload while annotating corpora. In particular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and"
W14-4907,W10-1807,0,0.0164962,"r, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotat"
W14-4907,P10-1052,1,0.893732,"Missing"
W14-4907,J93-2004,0,0.0462052,"tasks such as part-of-speech tagging or named entity recognition by relying on large annotated text corpora. As a result, developping highquality annotated corpora representing natural language phenomena that can be processed by statistical tools has become a major challenge for the scientific community. Several aspects of the annotation task have been studied in order to ensure corpus quality and affordable cost. Inter-annotator agreement (IAA) has been used as an indicator of annotation quality. Early work showed that the use of automatic preannotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006). Efforts have investigated methods to reduce the human workload while annotating corpora. In particular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a c"
W14-4907,W13-2321,1,0.84682,"otation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotator modeling. Overall, past work shows that cr"
W14-4907,J11-2010,0,\N,Missing
W15-2603,max-wisniewski-2010-mining,0,0.0242559,"ure work will be to replicate this study on a larger and more diverse corpus of patient records with different disease profiles, so as to confirm our findings and see to what extent text is shared between patient records from different hospital departments. As a follow-up of this study we also plan to address two new main lines of research. First, we intend to develop a more precise surface redundancy measure which takes temporal expressions and terminological variation into account and which is more robust to small changes within large highly similar context. We will use the WiCoPaCo corpus (Max and Wisniewski, 2010) to train models that can automatically identify reformulations, and distinguish those from (error) corrections and updates. Second, we will study redundancy on the level of the patient’s records as a whole, not just on the document level. We intend to develop a measure that uses information on redundancy levels, the number of documents copied, the (temporal) distance of information that has been copied, ... to identify key documents within a patient’s record. To this end we will need a reference set of correctly identified key documents in a set of patient records. This will be carried out by"
W15-2604,J08-4004,0,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-5107,N13-1073,0,0.0169323,"), where the suicide must be coded by taking into account the specific circumstance that lead to it (here, strangulation). In such cases we kept the input statements separate. The most generic statement (e.g. suicide) was considered inconclusive and did not receive a code assignment while the ‘head’ statement (e.g. ligature strangulation, which provided the defining information for code assignment) was aligned with the output code. To align the statements, we used a model originally intended for bilingual word alignment in parallel sentences: a log-linear reparameterization of the IBM2 model (Dyer et al., 2013). The alignments were produced from the computed clauses without allowing for null alignment in order to satisfy our constraints, and with a Dirichlet prior to favor diagonal alignments. The model underperforms on multi-word segments as it relies on co-occurrence counts of raw and computed causes, which are very sparse. To overcome this problem, both causes were pre-processed by removing stopwords and applying stemming. Next, the Damerau-Levenshtein distance between two segments was linearly combined with the occurrence count to act as a prior on the alignment probabilities. 4 Results We appli"
W16-5107,W11-1801,0,0.0175283,". It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task. 1 Introduction Over the past decade, biomedical named entity recognition (NER) and concept normalization have been widely covered in NLP challenges. Different types of texts were explored: clinical texts were used in the CMC (Pestian et al., 2007) and the i2b2 NLP Challenges (Uzuner et al., 2007; Uzuner et al., 2011) while the biomedical literature provided material for the BioNLP-Shared Tasks (Kim et al., 2011; Nédellec et al., 2015). Few challenges offered datasets in more than one languages, such as the CLEF ER (RebholzSchuhmann et al., 2013) and CLEF eHealth Challenges (Goeuriot et al., 2015) The assignment of codes from the International Classification of Diseases (ICD) to clinical texts is primarily used for billing purposes but also has a wide range of applications including epidemiological studies (Woodfield et al., 2015), monitoring disease activity (Koopman et al., 2015a), or predicting cancer incidence through retrospective and prospective studies (Bedford et al., 2014). Nevertheless, use"
W16-5107,W07-1013,0,0.573981,"ed the coder’s decision for each code. The dataset comprises 93,694 death certificates totalling 276,103 statements and 377,677 ICD-10 code assignments (3,457 unique codes). It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task. 1 Introduction Over the past decade, biomedical named entity recognition (NER) and concept normalization have been widely covered in NLP challenges. Different types of texts were explored: clinical texts were used in the CMC (Pestian et al., 2007) and the i2b2 NLP Challenges (Uzuner et al., 2007; Uzuner et al., 2011) while the biomedical literature provided material for the BioNLP-Shared Tasks (Kim et al., 2011; Nédellec et al., 2015). Few challenges offered datasets in more than one languages, such as the CLEF ER (RebholzSchuhmann et al., 2013) and CLEF eHealth Challenges (Goeuriot et al., 2015) The assignment of codes from the International Classification of Diseases (ICD) to clinical texts is primarily used for billing purposes but also has a wide range of applications including epidemiological studies (Woodfield et al., 2015), moni"
W16-5112,S12-1008,0,0.0184955,"ssing will occur for that document pair and the information is effectively lost for the information extraction process. In this paper we present and evaluate the text reuse detection tool in isolation and discuss its strengths and weaknesses. 2 Background A traditional approach for the detection of verbatim copying1 is to compute the similarity between the source and target text as the proportion of substring sequences that the two texts have in common. These substring sequences can either be defined as character n-grams (Cohen et al., 2013), words (Wrenn et al., 2010), or word n-grams (Adeel Nawab et al., 2012). These methods are mainly based on fingerprinting and hashing techniques, i.e. the documents are represented as sets of unique digital signatures, and are highly precise but are not robust to much surface variation. Some methods, however, are adapted to deal with insertions and deletion of words or characters. For example, as an extension of the ‘longest common substring’ algorithm (Gusfield, 1997), which calculated text similarity as the length of the longest continuous sequence of characters normalized by the sum of the document lengths, Wise et al. (1996) developed the ‘Greedy String Tilin"
W16-5112,P02-1020,0,0.0151576,"f subsequent text mining processes, such as encoding errors, missing files, OCR errors, etc. One interesting issue in cumulatively constructed text corpora is the problem of ‘text reuse’. Text reuse is defined here as the intentional or unintentional reusing of existing text (fragments) to create a new text, for example, by copy-pasting text fragments from one document to fit into a new document; or by adapting a report and saving both the old and the new version as separate documents. Text reuse is a complex phenomenon which has been studied in multiple settings such as newspaper journalism (Clough et al., 2002), programming code (Ohno and Murao, 2009), the analysis of text reuse in blogs and web pages (Abdel Hamid et al., 2009), etc. It is quite prevalent in the medical domain (Wrenn et al., 2010) and often seen as a negative factor: Cohen et al. (2013) found that copy-pasting practices in US hospitals have a significant negative impact on the accuracy of the subsequent text mining systems on the clinical notes. However, when text reuse is considered as a diachronic phenomenon, it has some interesting aspects. By identifying which text (fragments) have been reused we can follow the flow of informati"
W16-5112,W15-2603,1,0.887071,"Missing"
W16-6110,P13-1166,0,0.22125,"Missing"
W17-4719,L16-1470,1,0.868974,"Missing"
W17-4719,C16-2064,0,0.0199749,"ach language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectur"
W17-4719,W16-4616,0,0.0207529,"training settings for each language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use o"
W17-4719,W17-4754,0,0.123112,"Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domai"
W17-4719,federmann-2010-appraise,0,0.0291939,"run2 LMU PJIIT run1 PJIIT run2 PJIIT run3 uedin-nmt run1 uedin-nmt run2 UHH run1 UHH run2 UHH run3 cs 15.93* 22.79* - de 20.45* 27.57* 26.79 29.46* 21.88* 33.06* 18.71 19.80 19.66* fr 22.99* 31.79 31.89 33.36* pl 14.09* 14.32 10.75 14.34* 23.15* 19.87 - es 40.97 41.20 41.22* ro 10.56* 18.10* 29.32* 27.32 - Table 7: Results for the NHS test sets. * indicates the primary run as informed by the participants. native speakers of the languages and were either members of the participating teams or colleagues from the research community. The validation task was carried out using the Appraise tool15 (Federmann, 2010). For each pairwise comparison, we validated a total of 100 randomly-chosen sentence pairs. The validation consisted of reading the two sentences (A and B), i.e., translations from two systems or from the reference, and choosing one of the options below: The manual validation for the Scielo test sets is presented in Table 8, for the comparison of the only participating team (UHH) to the reference translation. For en2es, the automatic translation scored lower than the reference one in 53 out of 100 pairs, but could still beat the reference translation in 23 pairs. For en2pt, the automatic trans"
W17-4719,W17-4730,0,0.0242129,"uilt BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectures, layer normalization, and more compact models due to weight-tying and improvements in BPE segmentations. Lilt (Lilt Inc.). The system from the Lilt Inc.13 uses an in-house implementation of a sequenceto-sequence model with Bahdanau-style attention. The final submissions are ensembles between models fine-tuned on different parts of the available data. LMU (Ludwig Maximilian University of Munich). LMU Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokeni"
W17-4719,W16-2337,0,0.097915,". All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domain data14 and out-ofdomain data. However, we did not perform SOUL re-scoring. and Kneser-Ney discounting were used to estimate 5-gram language models (LM). For word alignment, GIZA++ with the default grow-diag-finaland alignment symmetrization method was used. Tuning of the SMT systems was performed with MERT. Commoncrawl and Wikipedia were used as general domain data for all language pairs except for EN/PT, where no Commoncrawl data was provided by WMT. As for the in-domain corpo"
W17-4719,W17-4739,1,\N,Missing
W17-4719,W17-4743,0,\N,Missing
W18-5622,S15-2136,0,0.0739526,"Missing"
W18-5622,S17-2093,0,0.0512896,"Missing"
W18-5622,Q17-1010,0,0.193683,"#4, IOB format) and two categorical features (cols. #2 and #3), the partof-speech tag and a chunk label (IOB format). Tool Overview In this section, we present a general overview of YASET. First we describe input data formats (sequences and pre-trained word embeddings). Then we present the input pipeline, the network training phase, the implemented evaluation metrics and the management of its parameters. 4.1 4.2 Word Embeddings YASET supports embeddings in the word2vec (Mikolov et al., 2013b) or Genˇ uˇrek and Sojka, 2010) formats. Other sim (Reh˚ formats of embeddings, for instance FastText (Bojanowski et al., 2017) or Glove (Pennington et al., 2014), must first be converted to either accepted format. Input and Output Data YASET takes CoNLL-like10 formatted files as input. Sequences are separated by empty lines and there must be one token per line. For each token, 10 http://universaldependencies.org/ docs/format.html 195 the maximum number of iterations n and the patience criterion p. Training will stop if the maximum number n is reached or if there are p iterations without performance improvement on the development instances. Users can also set several parameters related to the learning algorithm such a"
W18-5622,W12-2411,0,0.0720674,"Missing"
W18-5622,N18-1131,0,0.0732005,"Missing"
W18-5622,N18-1079,0,0.0142593,"e of the total training set indicated on the first row). Standard errors appear between parentheses. We observe that the performance improves logarithmically with every chunk added in the training data as shown in Table 4. This finding is similar to the observation of Sun et al. (2017) for vision tasks. Further addition of data will slightly improve the performance as the maximum performance plateau is almost reached. 200 overlapping clinical entities at each layer. Ju et al. (2018) present a dynamic end-to-end neural network model capable of handling an undetermined number of nesting levels. Katiyar and Cardie (2018) model the task as an hypergraph whose structure is learned with an LSTM network. Future research will focus on the influence of word embedding models which were shown to significantly impact on performance. Specifically, models taking into account sub-token information (Bojanowski et al., 2017) or emphasizing context (Peters et al., 2018) should be further explored. Moreover, other neural network models for NER such as the ones proposed by Rei et al. (2016) and Ma and Hovy (2016) will be investigated and implemented in YASET. Having a centralized implementation of different NER models will al"
W18-5622,P17-1194,0,0.0178373,"put format and from the output format to the brat format. The tool produces several plots during training for performance analysis. It is implemented in Python and makes use of the TensorFlow library. Both implementations of the Bi-LSTM model suffer from a very long training time which makes them cumbersome to use. YASET offers a faster implementation of the model by allowing mini-batch training and by using the pipeline API of TensorFlow. Rei and Yannakoudakis (2016) released a Python implementation5 of different models presented in their works (Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). One major difference with YASET resides in the possibility to use a language modeling objective during training. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Networks (CNNs) for character level representation, is presented in Ma and Hovy (2016). The authors implemented the architecture with PyTorch."
W18-5622,C16-1030,0,0.112027,"., 2012) to the input format and from the output format to the brat format. The tool produces several plots during training for performance analysis. It is implemented in Python and makes use of the TensorFlow library. Both implementations of the Bi-LSTM model suffer from a very long training time which makes them cumbersome to use. YASET offers a faster implementation of the model by allowing mini-batch training and by using the pipeline API of TensorFlow. Rei and Yannakoudakis (2016) released a Python implementation5 of different models presented in their works (Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). One major difference with YASET resides in the possibility to use a language modeling objective during training. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Networks (CNNs) for character level representation, is presented in Ma and Hovy (2016). The authors implemented the architecture w"
W18-5622,N16-1030,0,0.193553,"e, we report distributions over 30 runs and different sizes of training datasets. YASET provides stateof-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts. 1 • a fast and accurate implementation of a state-of-the-art sequence tagging model based on Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997). The architecture is similar to the one described in Lample et al. (2016) and is able to process mini-batches for faster training. Furthermore, YASET supports the use of handcrafted features in combination with word and character embeddings; • an easy-to-use interface based on a central configuration file that is used to setup experiments, with default parameters that are suitable for most sequence tagging tasks; • an evaluation on various biomedical corpora and on the CoNLL 2003 corpus, studying the stability of our model and the effect of training data size. We compare YASET performance with state-of-the-art results published in the literature. Introduction Many"
W18-5622,P16-1112,0,0.0675502,"TM model. The authors intended to make the tool easy to use by providing automatic format conversion from the brat format (Stenetorp et al., 2012) to the input format and from the output format to the brat format. The tool produces several plots during training for performance analysis. It is implemented in Python and makes use of the TensorFlow library. Both implementations of the Bi-LSTM model suffer from a very long training time which makes them cumbersome to use. YASET offers a faster implementation of the model by allowing mini-batch training and by using the pipeline API of TensorFlow. Rei and Yannakoudakis (2016) released a Python implementation5 of different models presented in their works (Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). One major difference with YASET resides in the possibility to use a language modeling objective during training. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Netw"
W18-5622,D17-1035,0,0.0682807,"ning. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Networks (CNNs) for character level representation, is presented in Ma and Hovy (2016). The authors implemented the architecture with PyTorch. The code is freely available online8 . The same type of architecture is implemented in the tool released by Reimers and Gurevych (2017). It is implemented with Keras (Chollet et al., 2015) and is freely available online9 . Outside the peer-reviewed scientific environment, many other implementations are freely available online. However, we will not review them in this paper. 3 Neural Network Model There is currently one neural network model implemented in YASET. This model is mostly based on Lample et al. (2016). However, similar architectures are presented in other work (Collobert et al., 2011; Ma and Hovy, 2016; Rei and Yannakoudakis, 2016; Rei et al., 2016). Other network architectures will be implemented in the future. 3.1"
W18-5622,P16-1101,0,0.305832,"in their works (Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). One major difference with YASET resides in the possibility to use a language modeling objective during training. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Networks (CNNs) for character level representation, is presented in Ma and Hovy (2016). The authors implemented the architecture with PyTorch. The code is freely available online8 . The same type of architecture is implemented in the tool released by Reimers and Gurevych (2017). It is implemented with Keras (Chollet et al., 2015) and is freely available online9 . Outside the peer-reviewed scientific environment, many other implementations are freely available online. However, we will not review them in this paper. 3 Neural Network Model There is currently one neural network model implemented in YASET. This model is mostly based on Lample et al. (2016). However, similar architec"
W18-5622,E12-2021,0,0.11951,"Missing"
W18-5622,D14-1162,0,0.0823245,"features (cols. #2 and #3), the partof-speech tag and a chunk label (IOB format). Tool Overview In this section, we present a general overview of YASET. First we describe input data formats (sequences and pre-trained word embeddings). Then we present the input pipeline, the network training phase, the implemented evaluation metrics and the management of its parameters. 4.1 4.2 Word Embeddings YASET supports embeddings in the word2vec (Mikolov et al., 2013b) or Genˇ uˇrek and Sojka, 2010) formats. Other sim (Reh˚ formats of embeddings, for instance FastText (Bojanowski et al., 2017) or Glove (Pennington et al., 2014), must first be converted to either accepted format. Input and Output Data YASET takes CoNLL-like10 formatted files as input. Sequences are separated by empty lines and there must be one token per line. For each token, 10 http://universaldependencies.org/ docs/format.html 195 the maximum number of iterations n and the patience criterion p. Training will stop if the maximum number n is reached or if there are p iterations without performance improvement on the development instances. Users can also set several parameters related to the learning algorithm such as the initial learning rate, and gr"
W18-5622,N18-1202,0,0.0648615,"Missing"
W18-5622,P18-4013,0,0.0139859,"ented in Python and makes use of the TensorFlow library. Both implementations of the Bi-LSTM model suffer from a very long training time which makes them cumbersome to use. YASET offers a faster implementation of the model by allowing mini-batch training and by using the pipeline API of TensorFlow. Rei and Yannakoudakis (2016) released a Python implementation5 of different models presented in their works (Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). One major difference with YASET resides in the possibility to use a language modeling objective during training. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Networks (CNNs) for character level representation, is presented in Ma and Hovy (2016). The authors implemented the architecture with PyTorch. The code is freely available online8 . The same type of architecture is implemented in the tool released by Reimers and Gurevych (2017). It is im"
W18-6403,W18-6444,0,0.0369107,"a. Terminological resources such as the Unified Medical Language System (UMLS) (Bodenreider, 2004) were used as well. TFG TALP UPC. Each two submissions for language pairs es/en, fr/en and pt/en utilized either multi-source (run1, primary run) or the singlesource (run2) training. UFRGS. The two submissions from the UFRGS teams seem to have differed only on the MT tool that they used, i.e., either OpenNMT (run1, primary run) or Moses (run2). UHH-DS (University of Hamburg, Germany). The UHH-DS team utilized Moses (Koehn et al., 2007) trained on a variety of in-domain and general domain corpora (Duma and Menzel, 2018). The main feature of their system was the development of an unsupervised method to automatically under-sample sentences from the general domain collection that were better suited for the biomedical domain. Their under-sampling algorithm can be applied either on the source or target side of the corpora, as well as on both sides. 4 UHH-DS. The three submissions for each of the language pairs (en/es, en/pt, en/ro, es/en and pt/en) differed on whether the under-sampling algorithm was applied only on the English side (run1), on the non-English side (run2) or on both sides (run3, primary run). 4.2"
W18-6403,federmann-2010-appraise,0,0.736408,"de) • Portuguese-English (pt/en); Eng.-Port. (en/pt) • English-Romanian (en/ro) • Spanish-English (es/en); Eng.-Span. (en/es) Test sets Test sets were obtained from Medline and EDP. In these sources, text for both languages is readily available from the authors of the publications. Manual evaluation of the automatic alignment After compiling the Medline test sets, we manually checked the totality of the abstracts to assess the quality of the automatic alignment (cf. results shown in Table 2). We utilized a modified version of the Quality Checking task of our installation of the Appraise tool (Federmann, 2010, 2018) and one native speaker of each non-English language carried out the validation (cf. Figure 1). The only exception were the Chinese abstracts which were manually checked without the use of the Appraise tool. For each language pair, we checked the totality of the abstracts for both translation directions, e.g., en/de and de/en, which was later randomly EDP. This year’s test set was derived from last year’s processing of publications. We kept one extra test set for this year’s challenge. It can be noted that the sentence segmentation offered for the EDP corpus this year was performed manu"
W18-6403,C18-2019,0,0.042363,"Missing"
W18-6403,W18-6445,1,0.860902,"Missing"
W18-6403,W18-6446,0,0.0543218,"lti-source systems utilized a concatenation of training data from es/en, fr/en and pt/en. Hunter MT (Hunter College, USA). The Hunter team (Khan et al., 2018) used different transfer learning methods and trained different indomain biomedical data sets one after another. Their system was set up using parameters of previous training as the initialization of the following training. A News based model was used as pretraining. LMU (Ludwig Maximilian University of Munich, Germany). The LMU team implementated various neural network models and trained and tuned the models on parallel biomedical data (Huck et al., 2018). They experimented with implementations of the Transformer architecture (Sockeye implementation) and the encoderdecoder models (Nematus toolkit). The authors highlight that the word segmentation used on the German language for both translation directions were responsible for the good performance of the system in the human evaluation. Participating teams and systems We received submissions from six teams, as summarized in Table 3. The teams came from research and academic institutions of four countries (Brazil, Germany, Spain and USA) and from three continents. An overview of the teams and the"
W18-6403,ma-2006-champollion,0,0.0427762,"models for each languages, i.e., Chinese, French, German and Spanish (Manning et al., 2014).2 Since for Portuguese and Romanian no models are available in the Stanford CoreNLP tools, we used models for other similar Roman languages (Spanish for Portuguese and French for Romanian). The sentences were then automatically aligned using the GMA tool for which we provided a list of stopwords for each language.3 After a short analysis of the alignment of the Chinese/English abstracts, and given the bad alignments that we obtained, we carried out a new automatic alignment using the Champollion tool (Ma, 2006).4 The resulting aligned sentences were then manually checked for assessing their quality. 2 2.1 language pairs and from EDP for one language pair, as detailed below: • Chinese-English (zh/en); Eng.-Chinese (en/zh) • French-English (fr/en); Eng.-French (en/fr) • German-English (de/en); Eng.-German (en/de) • Portuguese-English (pt/en); Eng.-Port. (en/pt) • English-Romanian (en/ro) • Spanish-English (es/en); Eng.-Span. (en/es) Test sets Test sets were obtained from Medline and EDP. In these sources, text for both languages is readily available from the authors of the publications. Manual evaluat"
W18-6403,P14-5010,0,0.00380103,"d for which we have native speakers of the foreign languages. The text of the abstracts were extracted from the XML files and 120 abstracts were randomly selected, excepted for Romanian whose total of parallel documents in Medline was less than 50. The number 120 accounts for possible errors in the preprocessing of the abstract in order to have a final test set of 100 abstracts to be split into the two translation directions. The documents were automatically split using the Stanford CoreNLP tool and the respective available models for each languages, i.e., Chinese, French, German and Spanish (Manning et al., 2014).2 Since for Portuguese and Romanian no models are available in the Stanford CoreNLP tools, we used models for other similar Roman languages (Spanish for Portuguese and French for Romanian). The sentences were then automatically aligned using the GMA tool for which we provided a list of stopwords for each language.3 After a short analysis of the alignment of the Chinese/English abstracts, and given the bad alignments that we obtained, we carried out a new automatic alignment using the Champollion tool (Ma, 2006).4 The resulting aligned sentences were then manually checked for assessing their q"
W18-6403,L18-1191,0,0.135912,"(Prieto, 2018). Therefore, biomedicine is a domain for which suitable parallel corpora, official evaluation test sets and machine translation (MT) systems are in high demand. There is active development of parallel corpora in this domain (see the recent survey in (N´ev´eol et al., 2018)). In this year alone, three new corpora have been published in a single conference: a compilation of full texts from the Scielo database for English, Portuguese, and Spanish (Soares et al., 2018), medical documents and glossaries for Spanish/English (Villegas et al., 2018) and a biomedical corpus for Romanian (Mitrofan and Tufis, 2018). However, in spite of the growing number of parallel corpora and the many open source tools for MT (e.g., Moses (Koehn et al., 2007), OpenNMT (Klein et al., 2017) and Marian (Junczys-Dowmunt et al., 2018)), there is still no ready-to-use tool for automatic translation of biomedical publications for any language pair. With the aim of fostering advances in this field, we organized the third edition of the Biomedical Translation Task in the Conference for Machine Translation (WMT).1 It builds on the two previous editions (Bojar et al., 2016; Jimeno Yepes et al., 2017) by offering test sets from"
W18-6403,L18-1043,1,0.564419,"Missing"
W18-6403,P18-4020,0,0.0427834,"Missing"
W18-6403,L16-1470,1,0.656371,"Missing"
W18-6403,W18-6447,0,0.0591703,"nges in the Romanian language. 3 TFG TALP UPC (Technical University of Catalunya, Spain). For their system that provides translations into English, the TGF TALP UPC team participated with a Transformer architecture (Kaiser et al., 2017; Vaswani et al., 2017) using both single-language and multi-source systems (Tubay and Costa-Juss`a, 2018). The systems were trained on the Scielo and Medline titles made available by the shared task in the last years. The multi-source systems utilized a concatenation of training data from es/en, fr/en and pt/en. Hunter MT (Hunter College, USA). The Hunter team (Khan et al., 2018) used different transfer learning methods and trained different indomain biomedical data sets one after another. Their system was set up using parameters of previous training as the initialization of the following training. A News based model was used as pretraining. LMU (Ludwig Maximilian University of Munich, Germany). The LMU team implementated various neural network models and trained and tuned the models on parallel biomedical data (Huck et al., 2018). They experimented with implementations of the Transformer architecture (Sockeye implementation) and the encoderdecoder models (Nematus too"
W18-6403,W18-6448,0,0.167561,"FOKUS (Germany) Hunter College (USA) Ludwig Maximilian University of Munich (Germany) Technical University of Catalunya (Spain) Universidade Federal do Rio Grande do Sul (Brazil) University of Hamburg (Germany) Table 3: List of the participating teams. 327 LMU. The three en/de submissions from the LMU team were the following: a right-to-left reranked Transformer (run1, primary run), a Transformer ensemble without re-ranking (run2) and the encoder-decoder built with Nematus (run3). The only submission for de/en was a Transformer without ensemble. 2007) or OpenNMT (Klein et al., 2017) systems (Soares and Becker, 2018). Training data was prepared by concatenating several in-domain and outof-domain resources. The in-domain corpora included scientific articles (full texts) from Scielo, the UFAL medical corpus, the EMEA corpus and Brazilian theses and dissertations. Due to possible overlap with the test sets from Medline, the team applied some procedures to automatically exclude some publications from the Scielo training data. Terminological resources such as the Unified Medical Language System (UMLS) (Bodenreider, 2004) were used as well. TFG TALP UPC. Each two submissions for language pairs es/en, fr/en and"
W18-6403,L18-1546,0,0.166247,"ation of the best Chinese papers (Tao et al., 2018) and the development of automatic tools for the automatic translation of publications (Prieto, 2018). Therefore, biomedicine is a domain for which suitable parallel corpora, official evaluation test sets and machine translation (MT) systems are in high demand. There is active development of parallel corpora in this domain (see the recent survey in (N´ev´eol et al., 2018)). In this year alone, three new corpora have been published in a single conference: a compilation of full texts from the Scielo database for English, Portuguese, and Spanish (Soares et al., 2018), medical documents and glossaries for Spanish/English (Villegas et al., 2018) and a biomedical corpus for Romanian (Mitrofan and Tufis, 2018). However, in spite of the growing number of parallel corpora and the many open source tools for MT (e.g., Moses (Koehn et al., 2007), OpenNMT (Klein et al., 2017) and Marian (Junczys-Dowmunt et al., 2018)), there is still no ready-to-use tool for automatic translation of biomedical publications for any language pair. With the aim of fostering advances in this field, we organized the third edition of the Biomedical Translation Task in the Conference for"
W18-6403,P17-4012,0,0.124259,"mand. There is active development of parallel corpora in this domain (see the recent survey in (N´ev´eol et al., 2018)). In this year alone, three new corpora have been published in a single conference: a compilation of full texts from the Scielo database for English, Portuguese, and Spanish (Soares et al., 2018), medical documents and glossaries for Spanish/English (Villegas et al., 2018) and a biomedical corpus for Romanian (Mitrofan and Tufis, 2018). However, in spite of the growing number of parallel corpora and the many open source tools for MT (e.g., Moses (Koehn et al., 2007), OpenNMT (Klein et al., 2017) and Marian (Junczys-Dowmunt et al., 2018)), there is still no ready-to-use tool for automatic translation of biomedical publications for any language pair. With the aim of fostering advances in this field, we organized the third edition of the Biomedical Translation Task in the Conference for Machine Translation (WMT).1 It builds on the two previous editions (Bojar et al., 2016; Jimeno Yepes et al., 2017) by offering test sets from Medline for six Machine translation enables the automatic translation of textual documents between languages and can facilitate access to information only availabl"
W19-5012,P18-1019,0,0.129687,"is a deep learning model that is unsupervisedly pretrained on a large general language corpus, then supervisedly fine2. Train models to identify specific data items in full-text articles on diagnostic test accuracy 106 One of the main aims of our study is to determine how such a dataset should be constructed to allow for training well performing models. In particular, do we need directly supervised data, or can we build reliable models with distantly supervised data? If we do need directly supervised data, how much is necessary? 2 and classify these using sequence tagging (e.g. C RF, L STM) (Nye et al., 2018). Despite the body of previous work on automation, many data items relevant to systematic reviews have been overlooked. A 2015 systematic review of data extraction found 26 articles describing the attempted extraction of 52 different data items, but almost all focused on interventions (Jonnalagadda et al., 2015). No study considered any data item specific to diagnostic studies, except for general data items common to both interventions and diagnostic studies, such as age, sex, blinding, or the generation of random allocation sequences. The likely reason for this is that traditional data extrac"
W19-5403,W16-2331,0,0.0602264,"Missing"
W19-5403,federmann-2010-appraise,0,0.0998932,"primary runs that we considered from each team. We performed a total of 62 validations of pairwise datasets. We relied on human validators who were native speakers of the target languages and who were either members of the participating teams or colleagues from the research community. We also preferred to use validators who were familiar enough with the source language so that the original text could be consulted in case of questions about the translations, and for most language pairs this was the case. We carried out the so-called 3-way ranking task in our installation of the Appraise tool (Federmann, 2010).14 . For each pairwise dataset, we checked a total of 100 randomly-chosen sentence pairs. The validation consisted of reading the two translation sentences (A and B) and choosing one of the options listed below: en/pt. Results for en/pt from the BSC were almost 10 points higher than the ones for pt/en. The run from the BSC team based on OpenNMT outperfomed with some difference the baseline based on Marian NMT, maybe because of the many resources that the team trained its system on. Further, they were much superior to the baselines 2 and 3 also based on OpenNMT but only trained on the Medline"
W19-5403,P18-4020,0,0.0226832,"Missing"
W19-5403,P17-4012,0,0.0423458,"Missing"
W19-5403,P07-2045,0,0.0097248,"Terms 6,624 - Table 1: Number of documents, sentences, and terms in the training and test sets. the Appraise tool. We present statistics concerning the quality of the test set alignments in Table 2. than narrative. Two of the targeted languages, Portuguese and Chinese, are not present in UFAL. For Portuguese we therefore trained our model on the Scielo corpus (Neves et al., 2016) and tested on the Brazilian thesis corpus (Soares et al., 2018b). For Chinese we used the United Nations Parallel Corpus (Ziemski et al., 2016). The data was preprocessed using standard tools from the Moses toolkit (Koehn et al., 2007): tokenisation, cleaning of training data and truecasing. Subword segmentation (Sennrich et al., 2015) was then trained jointly over both source and target languages and applied using FastBPE.10 The number of merge operations for BPE was set to 85000. The models trained were shallow RNN encoderdecoders.11 They were trained on a GTX 1080 Ti with 8 GPUs. Validation using cross-entropy and BLEU was performed every 10,000 updates, and models were trained until there was no improvement on either metric for 5 consecutive updates. Training of a single model took approximately 2 days. Discussion. Comp"
W19-5403,W08-0336,0,0.0823727,"Missing"
W19-5403,W19-5418,0,0.251422,"Missing"
W19-5403,W19-5421,0,0.16707,"Missing"
W19-5403,L18-1043,1,0.820309,"Missing"
W19-5403,W17-2507,1,0.907231,"Missing"
W19-5403,W18-6403,1,0.767844,"Missing"
W19-5403,L18-1546,1,0.919338,"o improve patient-provider communication (Turner et al., 2019). However, the recurring conclusion of practical studies is that progress is still needed. The goal of this shared task is to bring machine translation of biomedical text to a level of performance that can help with these medical challenges. In recent years, many parallel corpora in the biomedical domain have been made available, which are valuable resources for training and evaluating MT systems. Examples of such corpora include Khresmoi (Duˇsek et al., 2017), Scielo (Neves et al., 2016), Full-Text Scientific Articles from Scielo (Soares et al., 2018a), MeSpEn (Villegas et al., 2018), thesis and dissertations (Soares et al., 2018b), and clinical trials (Neves, 2017). These corpora cover a variety of language pairs and document types, such as scientific articles, clinical trials, and academic dissertations. Many previous efforts have addressed MT for the biomedical domain. Interesting previous work includes a comparison of performance in biomedical MT to Google Translate for English, French, German, and Spanish (Wu et al., 2011). Pecina et al. applied MT for the task of multilingual information retrieval in the medical domain (Pecina et al"
W19-5403,L16-1470,1,0.901668,"Missing"
W19-5403,tiedemann-2012-parallel,0,0.252445,"Missing"
W19-5403,W18-1819,0,0.0299267,"summary paragraph. Below we provide a short description of the systems for which a corresponding paper is available or for which we received a description from the participants. Two teams (‘peace’ and ‘Radiant’) did not provide system descriptions. Table 5 provides an overview of the methods, implementations and training corpora used by the participants. While two teams used the statistical machine translation toolkit Moses (MT-UOCUPF and UHH-DS), the most popular translation 13 KU. The KU team’s systems were based on the Transformer-big architecture, trained using the Tensor2Tensor toolkit (Vaswani et al., 2018). Training data was carefully cleaned to remove encoding errors, bad translations, etc. They did not perform standard ensemble translation, but obtained a small BLEU improvement by taking a “majority vote” on the final translations for different checkpoints. MT-UOC-UPF. The MT-UOC-UPF team’s systems were deep RNN-based encoder-decoder models with attention, trained using Marian (and with layer normalisation, tied embeddings and 10.6084/m9.figshare.8094119 33 Team ID ARC BSC KU MT-UOC-UPF NRPU OOM peace Radiant Talp upc UCAM UHH-DS Institution Huawei Technologies (China), Barcelona Supercomputi"
W19-5403,W18-6429,0,0.02331,"to those from UCAM. Both teams used Transformer models but the ARC also used BERT multilingual embeddings. We observed no significant difference between the submissions from team ARC but runs based on the ensemble of models from team UCAM (i.e. runs 2 and 3) obtained a higher score than their single best systems. characters, achieving such an ideal tokenization requires a sophisticated dictionary (Chang et al., 2008) – including biomedical terms – and is beyond the scope of this shared task. Further, using character-level tokenization for BLEU purposes is in accordance with current practice (Wang et al., 2018; Xu and Carpuat, 2018). Table 6 shows BLEU scores for all language pairs when considering all sentences in our test sets. Table 7 only considers the sentences that have been manually classified as being correctly aligned (cf. Section 2). As expected, certain results improve considerably (by more than 10 BLEU points) when only considering the sentences that are correctly aligned. Most teams outperformed the three baselines, except the NRPU team’s submissions for en/fr and fr/en. Baseline1, trained using Marian NMT, obtained results not far behind the best performing team, while the two other b"
W19-5403,W19-5420,0,0.097457,"Missing"
W19-5403,W18-6431,0,0.0160294,". Both teams used Transformer models but the ARC also used BERT multilingual embeddings. We observed no significant difference between the submissions from team ARC but runs based on the ensemble of models from team UCAM (i.e. runs 2 and 3) obtained a higher score than their single best systems. characters, achieving such an ideal tokenization requires a sophisticated dictionary (Chang et al., 2008) – including biomedical terms – and is beyond the scope of this shared task. Further, using character-level tokenization for BLEU purposes is in accordance with current practice (Wang et al., 2018; Xu and Carpuat, 2018). Table 6 shows BLEU scores for all language pairs when considering all sentences in our test sets. Table 7 only considers the sentences that have been manually classified as being correctly aligned (cf. Section 2). As expected, certain results improve considerably (by more than 10 BLEU points) when only considering the sentences that are correctly aligned. Most teams outperformed the three baselines, except the NRPU team’s submissions for en/fr and fr/en. Baseline1, trained using Marian NMT, obtained results not far behind the best performing team, while the two other baselines were not very"
W19-5403,L16-1561,0,0.0276925,"ntences 50 589 50 719 50 526 50 599 50 486 50 593 50 491 50 589 50 283 50 351 Terminology test Terms 6,624 - Table 1: Number of documents, sentences, and terms in the training and test sets. the Appraise tool. We present statistics concerning the quality of the test set alignments in Table 2. than narrative. Two of the targeted languages, Portuguese and Chinese, are not present in UFAL. For Portuguese we therefore trained our model on the Scielo corpus (Neves et al., 2016) and tested on the Brazilian thesis corpus (Soares et al., 2018b). For Chinese we used the United Nations Parallel Corpus (Ziemski et al., 2016). The data was preprocessed using standard tools from the Moses toolkit (Koehn et al., 2007): tokenisation, cleaning of training data and truecasing. Subword segmentation (Sennrich et al., 2015) was then trained jointly over both source and target languages and applied using FastBPE.10 The number of merge operations for BPE was set to 85000. The models trained were shallow RNN encoderdecoders.11 They were trained on a GTX 1080 Ti with 8 GPUs. Validation using cross-entropy and BLEU was performed every 10,000 updates, and models were trained until there was no improvement on either metric for 5"
