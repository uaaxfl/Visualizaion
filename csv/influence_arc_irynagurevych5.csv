2018.gwc-1.20,baccianella-etal-2010-sentiwordnet,0,0.0247321,"improve the performance of the APC task. However, their impact on real world UGC data for APC had been relatively unexplored. Among lexical-semantic features, senselevel features were explored in previous works (Kehagias et al., 2003; Vossen et al., 2006) with varying conclusions. In this paper, we conduct extensive experiments, aiming at obtaining a more detailed understanding of whether or not the senses can be beneficial in certain cases compared to word-based fea1 Famous fiction/action movies. tures. Broadly, we explore the use of word senses, supersenses, and WordNet sentiment features (Baccianella et al., 2010) in personality classification. Our main contributions are: • Investigating the impact of different lexical-semantic features on APC task. • Revealing the accumulated benefit by combining word sense disambiguation (WSD) with semantic and sentiment features in APC. • Proposing and evaluating a feature selection method called Selective.WSD to improve WSD usage in APC. • Proposing a unified framework on top of the UIMA framework 2 to integrate different lexical-semantic resources for APC. The rest of this paper is organized as follows. Section 2 presents the related work and our novel contributio"
2018.gwc-1.20,D15-1208,1,0.867161,"rams. Our experimental results are comparable to state-of-the-art methods, while no personality-specific resources are required. 1 Introduction Automatic personality classification (APC) has been employed on user generated content (UGC), such as Tweets, to collect the user personality for various personalized intelligent applications, including recommender systems (Hu and Pu, 2011), mental health diagnosis (Uba, 2003), recruitment and career counseling (Gardner et al., 2012). Especially, the recommender applications benefit from knowing the personality of real as well as fictional characters (Flekova and Gurevych, 2015). For example, if a user is known to favor the personality traits displayed by the main ∗ *The research by the 1st and the 2nd authors has been done during their employment at the UKP Lab, Technische Universit¨at Darmstadt, Germany, and supported by the German Research Foundation under grant No. GU 798/14-1. characters of, say, Terminator 1 and Rambo1 , then the system should automatically recommend movies with similar characters. Currently, the performance of APC depends on how user personality is modeled and what types of personality features can be extracted. Regarding the first factor, one"
2018.gwc-1.20,E12-1059,1,0.885024,"Missing"
2018.gwc-1.20,P13-4007,1,0.81291,"Missing"
2018.gwc-1.20,W98-0705,0,0.0106702,"K dataset. Trait Majumder et al. (2017) Ours (Majority.Acc) cOPN cCON cEXT cAGR cNEU Avg 62.68 57.30 58.09 56.71 59.38 58.83 72.10 (70.40) 56.80 (52.00) 62.10 (38.40) 55.80 (53.60) 61.70 (39.60) 58.64 (50.80) Figure 4: A test on cEXT personal trait of ESSAYS dataset to compare between Selective.WSD and All.WSD. Impact of WSD on APC We found that the WSD does not generally lead to an improvement in classification results except arbitrary dataset-specific differences, which can be largely attributed to the lemmatization and POS tagging. However, in contrary to previous beliefs (Sanderson, 1994; Gonzalo et al., 1998), the performance WORD χ2 WN-WORD χ2 love boyfriend ’d me so people much we thinks .012 .008 .008 .007 .006 .006 .005 .005 .005 love music sleep assignment proud boyfriend worry people awkward .026 .010 .009 .009 .008 .007 .007 .007 .007 WN-MFS χ2 WN-S-LESK χ2 love1 v music1 n guy1 n good1 a proud1 a assignment1 n boyfriend1 n real1 a sleep1 v .016 .009 .009 .009 .008 .008 .008 .006 .006 love1 v assignment1 n sleep1 v street4 n love1 n sleep1 n music1 n good6 a proud3 a .017 .009 .008 .007 .006 .006 .005 .005 .004 Table 4: The highest ranked features for Extraversion on the ESSAYS dataset, ave"
2018.gwc-1.20,C16-1330,0,0.0514043,"Missing"
2018.gwc-1.20,rose-etal-2002-reuters,0,0.0292306,"rent lexical-semantic resources for APC. The rest of this paper is organized as follows. Section 2 presents the related work and our novel contributions, as well as background knowledge of the Five Factor Model. Section 3 describes the experimental datasets. Our proposed framework and methodology are presented in Section 4. Experimental results and discussion are in Section 5. Section 6 concludes this paper. 2 Related Work and Background Previous studies concerned the positive impact of sense-level features (i.e., using WordNet based WSD) on the performance of document classification systems (Rose et al., 2002; Kehagias et al., 2003; Moschitti and Basili, 2004; Vossen et al., 2006). Though they had different focuses, they suggest that word senses are not adequate to improve text classification accuracy. Vossen et al. (2006) report an improvement from 0.70 to 0.76 F-score while negative results have been reported by Kehagias et al. (2003). This is why supersenses, the coarse-grained semantic labels based on WordNet’s lexicographer files, have recently gained attention for text classification tasks. In this paper, we further explore the impact of these features in personality prediction. There have b"
2020.acl-main.115,N13-1073,0,0.0151844,"se models are denoted as Transformer+RoBERTa. 4 5 3.3 Train versus Test Splits Baselines We attemp to solve these puzzles with models of varying complexity, i.e. from random guessing to state-of-the-art neural machine translation systems. Random Words (RW): Since the vocabularies of source and target languages are quite small, we test what random word picking can accomplish. We simply tokenize the training sentence pairs and then randomly choose a word from the target language’s vocabulary for each token in the source sentence.5 FastAlign (FA): We use the translation alignment tool FastAlign (Dyer et al., 2013), to test 5 We don’t use frequency of the words, i.e., pick words that occur more often, since they are not that meaningful due to the tininess of the data. Experiments 5.1 Experimental Settings We first compile a subset from the puzzles that are diverse by means of languages and contain translation questions in both directions. During tuning, we use the test sentences on these puzzles to validate our models. Since our foreign languages are morphologically rich, we use BPE (Sennrich et al., 2016) to segment words into subwords. For the sentences in the foreign language, we learn the BPE from t"
2020.acl-main.115,N18-2017,0,0.0310852,"8). Our challenge distinguishes from previous benchmarks with some key properties. First, most of these reasoning tasks require external scientific or visual knowledge, which makes it hard to measure the actual reasoning performance. On the other hand, our challenge does not rely on any external, multimodal or expert-level information. Second, and more importantly, PuzzLing challenge consists of a minimal set of examples required for solution. That means, there exists no extra training data, ensuring that exploiting surface patterns would not be possible unlike in some of existing benchmarks (Gururangan et al., 2018). In summary, this paper introduces a unique challenge, PuzzLing Machines, made up of ∼100 Rosetta Stone, a.k.a translation puzzles covering 81 languages from 39 different language families based on the Linguistic Olympiads. The challenge requires System2 skills—sequential reasoning and abstraction of linguistic concepts, discussed in detail in §2. We discuss the dataset and the linguistic phenomena in the resulting dataset supported with statistics and examples in §3. In §4, we present the results of intuitive baseline methods and strong MT baselines such as Transformers encoder-decoder (Vasw"
2020.acl-main.115,2021.ccl-1.108,0,0.0854466,"Missing"
2020.acl-main.115,N19-4009,0,0.0158737,"eld out”. On the contrary, in some cases, vocabulary items attested in the input of foreign→English test instances may be crucial to the translation of English→foreign test instances, and vice versa. So it is only the targets of test instances that should be truly held out. This specificity is not ubiquitous across the puzzles, but it should be accounted for by any approach to their solution, for example by building the system vocabulary over the union of the train and input test data. We implement three different models based on Transformers (Vaswani et al., 2017) using the implementation of Ott et al. (2019). In the first scenario, we train an off-the-shelf Transformer encoder-decoder model for each direction, referred to as Transformer. Second, we use a strong pretrained English language model, RoBERTa (Liu et al., 2019), to initialize the encoder of the NMT model for English to foreign translation. Finally, for foreign to English translation, we concatenate the translation features extracted from the last Transformer decoder layer, with the language modeling features extracted from RoBERTa (Liu et al., 2019), before mapping the vectors to the output vocabulary. These models are denoted as Trans"
2020.acl-main.115,P07-2045,0,0.00747198,"For the sentences in the foreign language, we learn the BPE from the training data, while for English sentences we use the already available GPT2-BPE dictionary to exploit English language prior. For convenience, 6 We add all aligned target phrases of the source token to the dictionary. Hence, when one target phrase is seen multiple times, it is more likely to be chosen during inference. 1245 before we train the models, we lowercase the sentences, remove certain punctuations, remove pronoun tags and brackets, and augment training data with multiple reference translations. PBSMT: We use Moses (Koehn et al., 2007) with default settings. We employ wikitext-103 corpus to train a 5-gram English LM for the model with access to external data. The other model only uses training sentences for the LM. NMT: Following the suggestions for lowresource NMT systems by Sennrich and Zhang (2019), we use small and few layers and high dropout rates. Similarly we use the smallest available language model (RoBERTa Base) and freeze its parameters during training to reduce the number of trainable parameters. We tune the following hyper-parameters: BPE merge parameter, learning rate and number of epochs. 5.2 Evaluation Metri"
2020.acl-main.115,W16-2341,0,0.0186646,"on steps as an interesting future research direction. The first is the BLEU (Papineni et al., 2002) score since it is still the standard metric in MT. We use BLEU-2 to match the lower median of sentence lengths we observe across the English and the foreign data (see Fig 1). BLEU matches whole words rather than word pieces, which prevents us from assigning partial credit to subword matches, which could be especially relevant for foreign target languages with rich morphology. We therefore use three additional metrics that operate on the level of word pieces: CharacTER (Wang et al., 2016), ChrF (Popovic, 2016) and ChrF++ (Popovic, 2017). CharacTER is a measure derived from TER (Translation Edit Rate), where edit rate is calculated on character level, whereas shift rate is measured on the word level. It calculates the minimum number of character edits required to adjust a hypothesis, until the reference is matched, normalized by the length of the hypothesis sentence. For easier comparison, we report 1.0 − characT ER scores. ChrF is a simple F-measure reflecting precision and recall of the matching character n-grams. ChrF++ adds word unigrams and bi-grams to the standard ChrF for a higher human corre"
2020.acl-main.115,W17-3204,0,0.0200208,"ber of training instances varies greatly between the puzzles, which is related to a number of factors such as the difficulty and type of the task, as well as the linguistic properties of the foreign language. whether the puzzles can be solved by early lexical translation models (Brown et al., 1993). Since FA produces alignments for each training pair, we postprocess the output to create a translation dictionary separately for each direction. We then randomly choose from the translation entries for each token in source test sentence. 6 Phrase Based Statistical Machine Translation (PBSMT) Since Koehn and Knowles (2017) report that PBSMT models outperform vanilla NMT models in case of small parallel training data, we use PBSMT as one of the baselines. For the foreign→English direction, we implement two models—one using no external mono-lingual English data and one otherwise. 4.1 Neural Machine Translation One property of the data splits in linguistic puzzles, which diverges from the standard paradigm in machine learning, is that the input test data should not be considered “held out”. On the contrary, in some cases, vocabulary items attested in the input of foreign→English test instances may be crucial to th"
2020.acl-main.115,W17-4770,0,0.013022,"future research direction. The first is the BLEU (Papineni et al., 2002) score since it is still the standard metric in MT. We use BLEU-2 to match the lower median of sentence lengths we observe across the English and the foreign data (see Fig 1). BLEU matches whole words rather than word pieces, which prevents us from assigning partial credit to subword matches, which could be especially relevant for foreign target languages with rich morphology. We therefore use three additional metrics that operate on the level of word pieces: CharacTER (Wang et al., 2016), ChrF (Popovic, 2016) and ChrF++ (Popovic, 2017). CharacTER is a measure derived from TER (Translation Edit Rate), where edit rate is calculated on character level, whereas shift rate is measured on the word level. It calculates the minimum number of character edits required to adjust a hypothesis, until the reference is matched, normalized by the length of the hypothesis sentence. For easier comparison, we report 1.0 − characT ER scores. ChrF is a simple F-measure reflecting precision and recall of the matching character n-grams. ChrF++ adds word unigrams and bi-grams to the standard ChrF for a higher human correlation score. We experiment"
2020.acl-main.115,J93-2003,0,\N,Missing
2020.acl-main.115,P02-1040,0,\N,Missing
2020.acl-main.115,W13-3401,0,\N,Missing
2020.acl-main.115,W16-2342,0,\N,Missing
2020.acl-main.115,W18-5446,0,\N,Missing
2020.acl-main.115,P16-1162,0,\N,Missing
2020.acl-main.133,P05-1018,0,0.36157,"ogue Act (henceforth DA) gives a meaning to an utterance in a dialogue at the level of “illocutionary force”, and therefore, constitutes the basic unit of communication (Searle, 1969; Raheja and Tetreault, 2019). A DA captures what a speaker’s intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. Recent approaches to dialogue coherence modeling use the coherence features designed for monologue texts, e.g. entity transitions (Barzilay and Lapata, 2005), and augment them with dialoguerelevant features, e.g., DA labels (Cervone et al., 2018). These DA labels are provided by human annotators or DA prediction models. Such coherence models suffer from the following drawbacks: (a) they curb semantic representations of utterances to entities, which are sparse in dialogue because of short utterance lengths, and (b) their performance relies on the quality of their input DA labels. 1439 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1439–1450 c July 5 - 10, 2020. 2020 Association for Computational Lingu"
2020.acl-main.133,J08-1001,0,0.480876,"ues with respect to their coherence. (5) S-DiCoh: This is our coherence model, DiCoh, trained by only the supervision signal for coherence ranking, with the total loss L = Lφcoh (see Equation 11). This model does not benefit from DA information to enrich utterance vectors. (6) M-DiCoh: This is our full model trained by the proposed MTL using the supervision signals for both coherence ranking and DAP. The main advantage of this model is that it learns to focus on salient information of utterances for coherence assessment based on the given DAs for utterances. We follow former coherence papers (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Mesgar and Strube, 2018; Cervone et al., 2018) and use accuracy as the evaluation metric. In our experiments, this metric equals the frequency of correctly discriminated dialogue pairs in the test set of a problem-domain. # of correctly discriminated dialogue pairs . # of dialogue pairs (12) To reduce the risk of randomness in our experiments, we run each experiment five times with varying random seeds and report their average (Reimers and Gurevych, 2018). acc = Settings Each batch consists of 128 and 16 dialogue-pairs for the DailyDialog and SwitchBoard corpora,"
2020.acl-main.133,N10-1099,0,0.338436,"Missing"
2020.acl-main.133,P98-2241,0,0.426097,"act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code1 . 1 DA labels DiCoh model DAP model shared utterance encoder utterances Figure 1: A high-level view of our multi-task learning approach for dialogue coherence modeling. dialogue from a random sequence of dialogue utterances (Halliday and Hasan, 1976; Grosz and Sidner, 1986; Byron and Stent, 1998). Dialogue coherence deals with semantic relations between utterances considering their dialogue acts (Perrault and Allen, 1978; Cervone et al., 2018). Introduction Considering rapid progresses in developing open-domain dialogue agents (Serban et al., 2016; Ghazvininejad et al., 2018; Dinan et al., 2019; Li et al., 2019), the need for models that compare these agents in various dialogue aspects becomes extremely important (Liu et al., 2016; Dinan et al., 2019). Most available methods for dialogue evaluation rely on word-overlap metrics, e.g. BLEU, and manually collected human feedback. The for"
2020.acl-main.133,W19-3646,0,0.0227868,"models. Recent approaches to dialogue coherence modeling benefit from distributional representations of utterances. Zhang et al. (2018) quantify the coherence of dialogue using the semantic similarity between each utterance and its preceding utterances. This similarity is estimated, for example, by the cosine similarity between an utterance vector and a context vector where those vectors are the average of their pre-trained word embeddings. Vakulenko et al. (2018) measure dialogue coherence based on the consistency of new concepts introduced in a dialogue with background knowledge. Similarly, Dziri et al. (2019) utilize a natural language inference model to assess the content consistency among utterances as an indicator for dialogue coherence. However, these approaches lack dialogue-relevant information to measure coherence. Our MTL-based approach solves these issues: (i) it benefits from DAs and semantics of utterances to measure dialogue coherence by optimizing utterance vectors for both DAP and coherence assessment, and (ii) it uses DA labels to define an 1440 auxiliary task for training the DiCoh model using MTL, instead of utilizing them in a pipeline. Therefore, it efficiently mitigates the nee"
2020.acl-main.133,P19-1060,0,0.0127686,"ting the need for DA labels for dialogue coherence assessment during evaluations; (3) an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-theart coherence model on DailyDialog, and performs on par with it on SwitchBoard. 2 Related Work Early approaches to dialogue coherence modeling are built upon available models for monologue, such as the EntityGrid model (Barzilay and Lapata, 2005, 2008). EntityGrid and its extensions (Burstein et al., 2010; Guinaudeau and Strube, 2013; Mesgar and Strube, 2014; Tien Nguyen and Joty, 2017; Farag and Yannakoudakis, 2019) rely on entity transitions, as proxies of semantic connectivity, between utterances. These approaches are agnostic to discourse properties of dialogues (Purandare and Litman, 2008; Gandhe and Traum, 2008; Cervone et al., 2018). Utterance DA label This is my uncle, Charles. He looks strong. What does he do? He’s a captain. He must be very brave. Exactly! inform question inform inform inform incoherent utt1 :: This is my uncle, Charles. utt4 : He must be very brave. utt3 : He’s a captain. utt2 : He looks strong. What does he do? utt5 : Exactly! inform inform inform question inform coherent utt1"
2020.acl-main.133,W08-0127,0,0.017803,"t coherence model on DailyDialog, and performs on par with it on SwitchBoard. 2 Related Work Early approaches to dialogue coherence modeling are built upon available models for monologue, such as the EntityGrid model (Barzilay and Lapata, 2005, 2008). EntityGrid and its extensions (Burstein et al., 2010; Guinaudeau and Strube, 2013; Mesgar and Strube, 2014; Tien Nguyen and Joty, 2017; Farag and Yannakoudakis, 2019) rely on entity transitions, as proxies of semantic connectivity, between utterances. These approaches are agnostic to discourse properties of dialogues (Purandare and Litman, 2008; Gandhe and Traum, 2008; Cervone et al., 2018). Utterance DA label This is my uncle, Charles. He looks strong. What does he do? He’s a captain. He must be very brave. Exactly! inform question inform inform inform incoherent utt1 :: This is my uncle, Charles. utt4 : He must be very brave. utt3 : He’s a captain. utt2 : He looks strong. What does he do? utt5 : Exactly! inform inform inform question inform coherent utt1 : utt2 utt3 : utt4 : utt5 : Table 1: An example dialogue from DailyDialog (top) and its perturbation (bottom), which is generated by permuting the utterances said by one of the speakers (shown in boldfac"
2020.acl-main.133,D16-1230,0,0.0857759,"Missing"
2020.acl-main.133,J86-3001,0,0.725308,"ed for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code1 . 1 DA labels DiCoh model DAP model shared utterance encoder utterances Figure 1: A high-level view of our multi-task learning approach for dialogue coherence modeling. dialogue from a random sequence of dialogue utterances (Halliday and Hasan, 1976; Grosz and Sidner, 1986; Byron and Stent, 1998). Dialogue coherence deals with semantic relations between utterances considering their dialogue acts (Perrault and Allen, 1978; Cervone et al., 2018). Introduction Considering rapid progresses in developing open-domain dialogue agents (Serban et al., 2016; Ghazvininejad et al., 2018; Dinan et al., 2019; Li et al., 2019), the need for models that compare these agents in various dialogue aspects becomes extremely important (Liu et al., 2016; Dinan et al., 2019). Most available methods for dialogue evaluation rely on word-overlap metrics, e.g. BLEU, and manually collected"
2020.acl-main.133,P13-1010,0,0.562673,"g more informative utterance representations for coherence assessment; (2) alleviating the need for DA labels for dialogue coherence assessment during evaluations; (3) an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-theart coherence model on DailyDialog, and performs on par with it on SwitchBoard. 2 Related Work Early approaches to dialogue coherence modeling are built upon available models for monologue, such as the EntityGrid model (Barzilay and Lapata, 2005, 2008). EntityGrid and its extensions (Burstein et al., 2010; Guinaudeau and Strube, 2013; Mesgar and Strube, 2014; Tien Nguyen and Joty, 2017; Farag and Yannakoudakis, 2019) rely on entity transitions, as proxies of semantic connectivity, between utterances. These approaches are agnostic to discourse properties of dialogues (Purandare and Litman, 2008; Gandhe and Traum, 2008; Cervone et al., 2018). Utterance DA label This is my uncle, Charles. He looks strong. What does he do? He’s a captain. He must be very brave. Exactly! inform question inform inform inform incoherent utt1 :: This is my uncle, Charles. utt4 : He must be very brave. utt3 : He’s a captain. utt2 : He looks strong"
2020.acl-main.133,I17-1099,0,0.140958,"as an auxiliary task for training our coherence model in a multi-task learning (MTL) scenario (Figure 1). Our approach consists of three high-level components: an utterance encoder, a dialogue coherence model (DiCoh), and a Dialogue Act Prediction (DAP) model. The layers of the utterance encoder are shared between the DAP and the DiCoh model. This idea enables our DiCoh model to learn to focus on salient information presented in utterances considering their DAs and to alleviate the need for explicit DA labels during coherence assessment. We evaluate our MTL-based approach on the DailyDialog (Li et al., 2017) and SwitchBoard (Jurafsky and Shriberg, 1997) English dialogue corpora in several discriminating experiments, where our coherence model, DiCoh, is examined to discriminate a dialogue from its perturbations (see Table 1). We utilize perturbation methods, like utterance ordering and utterance insertion, inherited from coherence evaluation approaches for monologue texts, and also introduce two dialoguerelevant perturbations, named utterance replacement and even utterance ordering. Our core contributions are: (1) proposing an MTL-based approach for dialogue coherence assessment using DAP as an au"
2020.acl-main.133,W14-3701,1,0.812061,"representations for coherence assessment; (2) alleviating the need for DA labels for dialogue coherence assessment during evaluations; (3) an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-theart coherence model on DailyDialog, and performs on par with it on SwitchBoard. 2 Related Work Early approaches to dialogue coherence modeling are built upon available models for monologue, such as the EntityGrid model (Barzilay and Lapata, 2005, 2008). EntityGrid and its extensions (Burstein et al., 2010; Guinaudeau and Strube, 2013; Mesgar and Strube, 2014; Tien Nguyen and Joty, 2017; Farag and Yannakoudakis, 2019) rely on entity transitions, as proxies of semantic connectivity, between utterances. These approaches are agnostic to discourse properties of dialogues (Purandare and Litman, 2008; Gandhe and Traum, 2008; Cervone et al., 2018). Utterance DA label This is my uncle, Charles. He looks strong. What does he do? He’s a captain. He must be very brave. Exactly! inform question inform inform inform incoherent utt1 :: This is my uncle, Charles. utt4 : He must be very brave. utt3 : He’s a captain. utt2 : He looks strong. What does he do? utt5 :"
2020.acl-main.133,D18-1464,1,0.851473,"s our coherence model, DiCoh, trained by only the supervision signal for coherence ranking, with the total loss L = Lφcoh (see Equation 11). This model does not benefit from DA information to enrich utterance vectors. (6) M-DiCoh: This is our full model trained by the proposed MTL using the supervision signals for both coherence ranking and DAP. The main advantage of this model is that it learns to focus on salient information of utterances for coherence assessment based on the given DAs for utterances. We follow former coherence papers (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Mesgar and Strube, 2018; Cervone et al., 2018) and use accuracy as the evaluation metric. In our experiments, this metric equals the frequency of correctly discriminated dialogue pairs in the test set of a problem-domain. # of correctly discriminated dialogue pairs . # of dialogue pairs (12) To reduce the risk of randomness in our experiments, we run each experiment five times with varying random seeds and report their average (Reimers and Gurevych, 2018). acc = Settings Each batch consists of 128 and 16 dialogue-pairs for the DailyDialog and SwitchBoard corpora, respectively. Utterances are zero-padded and masked."
2020.acl-main.133,D14-1162,0,0.0833424,"racy as the evaluation metric. In our experiments, this metric equals the frequency of correctly discriminated dialogue pairs in the test set of a problem-domain. # of correctly discriminated dialogue pairs . # of dialogue pairs (12) To reduce the risk of randomness in our experiments, we run each experiment five times with varying random seeds and report their average (Reimers and Gurevych, 2018). acc = Settings Each batch consists of 128 and 16 dialogue-pairs for the DailyDialog and SwitchBoard corpora, respectively. Utterances are zero-padded and masked. We use pretrained GloVe embeddings (Pennington et al., 2014) of size 300 wherever word embeddings are required (i.e., in CoSim, S-DiCoh, and M-DiCoh). For the CoSim model, we use the SMART English stop word list (Salton, 1971) to eliminate all stop words. For the ASeq model, we use bi-grams of DA labels to define the coherence features (Cervone et al., 2018). All parameters of the EAGrid model have the same value as the best performing model proposed by Cervone et al. (2018). In DiCoh, the size of the hidden states in LSTMs of the utterance module is 128 and of the dialogue module is 256. The parameters of this model are optimized using the Adam optimi"
2020.acl-main.133,T78-1017,0,0.603282,") outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code1 . 1 DA labels DiCoh model DAP model shared utterance encoder utterances Figure 1: A high-level view of our multi-task learning approach for dialogue coherence modeling. dialogue from a random sequence of dialogue utterances (Halliday and Hasan, 1976; Grosz and Sidner, 1986; Byron and Stent, 1998). Dialogue coherence deals with semantic relations between utterances considering their dialogue acts (Perrault and Allen, 1978; Cervone et al., 2018). Introduction Considering rapid progresses in developing open-domain dialogue agents (Serban et al., 2016; Ghazvininejad et al., 2018; Dinan et al., 2019; Li et al., 2019), the need for models that compare these agents in various dialogue aspects becomes extremely important (Liu et al., 2016; Dinan et al., 2019). Most available methods for dialogue evaluation rely on word-overlap metrics, e.g. BLEU, and manually collected human feedback. The former does not strongly correlate with human judgments (Liu et al., 2016), and the latter is time-consuming and subjective. A fun"
2020.acl-main.133,N19-1373,0,0.023683,"ble methods for dialogue evaluation rely on word-overlap metrics, e.g. BLEU, and manually collected human feedback. The former does not strongly correlate with human judgments (Liu et al., 2016), and the latter is time-consuming and subjective. A fundamental aspect of dialogue is coherence – what discriminates a high-quality 1 https://github.com/UKPLab/ acl2020-dialogue-coherence-assessment coherence score A Dialogue Act (henceforth DA) gives a meaning to an utterance in a dialogue at the level of “illocutionary force”, and therefore, constitutes the basic unit of communication (Searle, 1969; Raheja and Tetreault, 2019). A DA captures what a speaker’s intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. Recent approaches to dialogue coherence modeling use the coherence features designed for monologue texts, e.g. entity transitions (Barzilay and Lapata, 2005), and augment them with dialoguerelevant features, e.g., DA labels (Cervone et al., 2018). These DA labels are provided by human annotators or DA prediction models. Such coherence model"
2020.acl-main.133,P17-1121,0,0.107647,"assessment; (2) alleviating the need for DA labels for dialogue coherence assessment during evaluations; (3) an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-theart coherence model on DailyDialog, and performs on par with it on SwitchBoard. 2 Related Work Early approaches to dialogue coherence modeling are built upon available models for monologue, such as the EntityGrid model (Barzilay and Lapata, 2005, 2008). EntityGrid and its extensions (Burstein et al., 2010; Guinaudeau and Strube, 2013; Mesgar and Strube, 2014; Tien Nguyen and Joty, 2017; Farag and Yannakoudakis, 2019) rely on entity transitions, as proxies of semantic connectivity, between utterances. These approaches are agnostic to discourse properties of dialogues (Purandare and Litman, 2008; Gandhe and Traum, 2008; Cervone et al., 2018). Utterance DA label This is my uncle, Charles. He looks strong. What does he do? He’s a captain. He must be very brave. Exactly! inform question inform inform inform incoherent utt1 :: This is my uncle, Charles. utt4 : He must be very brave. utt3 : He’s a captain. utt2 : He looks strong. What does he do? utt5 : Exactly! inform inform info"
2020.acl-main.133,D18-1432,0,0.056521,"Missing"
2020.acl-main.390,D11-1143,0,0.0825824,"Missing"
2020.acl-main.390,P19-1035,1,0.839361,"Solving C-tests requires orthographic, morphologic, syntactic, and semantic competencies as well as general vocabulary knowledge (Chapelle, 1994). C-tests can be easily created automatically by choosing an arbitrary text and introducing the gaps as described above. Because of the context and the kept word prefixes, C-test gaps typically only allow for a single solution (given by the original text) and therefore do not require manual correction. The biggest challenge, however, lies in controlling the difficulty of the text and the derived C-test with its gaps as we have shown in previous work (Lee et al., 2019). System objective. Given a large pool X of Ctests x ∈ X with n gaps gi ∈ x, 1 ≤ i ≤ n, the system objective is to learn a classifier d(g) ∈ LD to judge the gap difficulty of gaps g ∈ x with minimal training data. As the difficulty classes LD , we use the four labels very easy, easy, hard, and very hard proposed by Beinborn (2016). These four classes are based on the mean error rates e(g) of a gap g observed across all users. Figure 2 shows the mapping between the mean error rates e(g) and the four gap difficulty classes LD . Data. For our experiments, we obtained 3,408 solutions to English C-"
2020.acl-main.390,N19-1423,0,0.00649873,"ne of the sampling strategies introduced in the previous section. Then, we obtain labels based on how the user solved the test, which contributes (1) to the overall difficulty prediction for each gap and (2) to the representation of the current user’s proficiency. Our approach can be used with any underlying classifier d(g). In this paper, we train a multilayer perceptron (MLP) to predict the four difficulty classes for a C-test gap. To represent the input of the MLP, we use the 59 features previously proposed by Beinborn (2016). We furthermore introduce two novel features computed from BERT (Devlin et al., 2019): We hypothesize that the masking objective of BERT which masks individual words during training is very similar to a gap filling exercise and thus, a model trained in such a way may provide useful signals for assessing the difficulty of a gap. For each gap, we generate a sentence where only the gap is replaced by the masking token and fetch its predictions from the BERT model. From these predictions we take the prediction probability of the solution as the first feature and the entropy of the prediction probabilities of the top-50 predicted words as the second feature in concordance with find"
2020.acl-main.390,R19-1037,0,0.0138215,"othesize that the masking objective of BERT which masks individual words during training is very similar to a gap filling exercise and thus, a model trained in such a way may provide useful signals for assessing the difficulty of a gap. For each gap, we generate a sentence where only the gap is replaced by the masking token and fetch its predictions from the BERT model. From these predictions we take the prediction probability of the solution as the first feature and the entropy of the prediction probabilities of the top-50 predicted words as the second feature in concordance with findings by Felice and Buttery (2019) who show that entropy strongly correlates with the gap difficulty. Adding both features to the 59 features proposed by Beinborn (2016) increases the accuracy of our MLP from 0.33 to 0.37.4 While Beinborn successfully used support vector machines (SVM) in her work, we find that MLPs perform on par with SVMs (for the old and new features) and that they are more robust regarding the choice of the first sampled instance. Moreover, in our initial experiments with little training data, SVMs and Logistic Regression classifiers were only able to predict the majority class. Our MLP has a single hidden"
2020.acl-main.390,D18-1445,1,0.840951,"redictions. Active learning (Settles, 2012) is a frequently used technique to quickly maximize the prediction performance, as the system acquires user feedback in each iteration for 1 Our code and simulated learner models are available on Github: https://github.com/UKPLab/ acl2020-empowering-active-learning those instances that likely yield the highest performance improvement (e.g., because the system is yet uncertain about them). Active learning has been shown to reduce the amount of user feedback required while improving system performance for interactive NLP systems (P.V.S and Meyer, 2017; Gao et al., 2018) and to reduce the annotation costs in crowdsourcing scenarios (Fang et al., 2014). However, outside the typical annotation setup, it can be boring or frustrating for users to provide feedback on ill-predicted instances that hardly solve their needs. Consider a newly launched web application for learning a foreign language, which aims at suggesting exercises that match the user’s proficiency according to Vygotsky’s Zone of proximal development (Vygotsky, 1978). The underlying machine learning system starts without any data, but employs active learning to select an exercise the system cannot co"
2020.acl-main.390,W16-0535,0,0.0285332,"; Laws et al., 2011). Within the educational domain, active learning research is scarce.2 One example is the work by Rastogi et al. (2018), who propose a threshold-based sampling strategy utilizing the prediction probability and achieve a considerable speed-up without any significant performance drop. Hastings et al. (2018) find that ac2 Note, that in education, active learning often refers to a teaching paradigm which is unrelated to active learning in machine learning. tive learning can be used to efficiently train a system for providing feedback on student essays using teachers as oracles. Horbach and Palmer (2016) report mixed results for employing active learning in short-answer grading. While all of these works focus on improvements of the proposed system, users only benefit after training. In contrast, our work explicitly models the user objective, such that users already benefit while labeling training instances. Adaptive learning. Many systems provide user adaptation, and research has shifted from predefined sets of rules for adaptation to data-driven approaches. Several works investigate adaptive methods to provide exercises which are neither too hard nor too boring. For instance, Missura and G¨a"
2020.acl-main.390,P17-1124,1,0.860875,"Missing"
2020.acl-main.390,D08-1027,0,\N,Missing
2020.acl-main.390,D08-1112,0,\N,Missing
2020.acl-main.624,C16-2024,0,0.0606263,"Missing"
2020.acl-main.624,bartsch-2004-annotating,0,0.0778188,"umanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io https://github.com/UKPLab/ acl2020-interactive-entity-linking 2 2014), semantic enrichment (Schl¨ogl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018). These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004). In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation. For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. Th"
2020.acl-main.624,N19-1231,0,0.0613894,"Missing"
2020.acl-main.624,D18-1445,1,0.805525,"herefore, it is challenging to use state of the art systems. Human-in-the-loop annotation HITL machine learning describes an interactive scenario where a machine learning (ML) system and a human work together to improve their performance. The ML system gives predictions, and the human corrects if they are wrong and helps to spot things that have been overlooked by the machine. The system uses this feedback to improve, leading to better predictions and thereby reducing the effort of the human. In natural language processing, it has been applied in scenarios like interactive text summarization (Gao et al., 2018), parsing (He et al., 2016) or data generation (Wallace et al., 2019). Regarding machine-learning assisted annotation, Yimam et al. (2014) propose an annotation editor that during annotation, interactively trains a model using annotations made by the user. They use string matching and MIRA (Crammer and Singer, 2003) as recommenders, evaluate on POS and NER annotation and show improvement in annotation speed. TASTY (Arnold et al., 2016) is a system that is able to perform EL against Wikipedia on the fly while typing a document. A pretrained neural sequence tagger is being used that performs men"
2020.acl-main.624,N13-1122,0,0.0243513,"candidate with the highest score is returned as the final prediction. Existing systems rely on the availability of certain resources like a large Wikipedia as well as software tools and often are restricted in the knowledge base they can link to. Off-the-shelf systems like Dexter (Ceccarelli et al., 2013), DBPedia Spotlight (Daiber et al., 2013) and TagMe (Ferragina and Scaiella, 2010) most often can only link against Wikipedia or a related knowledge base like Wikidata or DBPedia. They require good Wikipedia coverage for computing frequency statistics like popularity, view count or PageRank (Guo et al., 2013). These features work very well for standard datasets due to their Zipfian distribution of entities, leading to high reported scores on stateof-the art datasets (Ilievski et al., 2018; Milne and Witten, 2008). However, these systems are rarely applied out-of-domain such as in digital humanities or classical studies. Compared to state-of-the-art approaches, only a limited amount of research has been performed on entity linking against domainspecific knowledge bases. AGDISTIS (Usbeck et al., 2014) developed a knowledge-base-agnostic approach based on the HITS algorithm. The mention detection rel"
2020.acl-main.624,D16-1258,0,0.0380006,"to use state of the art systems. Human-in-the-loop annotation HITL machine learning describes an interactive scenario where a machine learning (ML) system and a human work together to improve their performance. The ML system gives predictions, and the human corrects if they are wrong and helps to spot things that have been overlooked by the machine. The system uses this feedback to improve, leading to better predictions and thereby reducing the effort of the human. In natural language processing, it has been applied in scenarios like interactive text summarization (Gao et al., 2018), parsing (He et al., 2016) or data generation (Wallace et al., 2019). Regarding machine-learning assisted annotation, Yimam et al. (2014) propose an annotation editor that during annotation, interactively trains a model using annotations made by the user. They use string matching and MIRA (Crammer and Singer, 2003) as recommenders, evaluate on POS and NER annotation and show improvement in annotation speed. TASTY (Arnold et al., 2016) is a system that is able to perform EL against Wikipedia on the fly while typing a document. A pretrained neural sequence tagger is being used that performs mention detection. Candidates"
2020.acl-main.624,D11-1072,0,0.132539,"Missing"
2020.acl-main.624,C18-1056,0,0.0122089,"ols and often are restricted in the knowledge base they can link to. Off-the-shelf systems like Dexter (Ceccarelli et al., 2013), DBPedia Spotlight (Daiber et al., 2013) and TagMe (Ferragina and Scaiella, 2010) most often can only link against Wikipedia or a related knowledge base like Wikidata or DBPedia. They require good Wikipedia coverage for computing frequency statistics like popularity, view count or PageRank (Guo et al., 2013). These features work very well for standard datasets due to their Zipfian distribution of entities, leading to high reported scores on stateof-the art datasets (Ilievski et al., 2018; Milne and Witten, 2008). However, these systems are rarely applied out-of-domain such as in digital humanities or classical studies. Compared to state-of-the-art approaches, only a limited amount of research has been performed on entity linking against domainspecific knowledge bases. AGDISTIS (Usbeck et al., 2014) developed a knowledge-base-agnostic approach based on the HITS algorithm. The mention detection relies on gazetteers compiled from resources like Wikipedia and thereby performs string matching. Brando et al. (2016) propose REDEN, an approach based on graph centrality to link French"
2020.acl-main.624,C18-2002,1,0.914194,"Missing"
2020.acl-main.624,W18-5519,0,0.0130407,"roduction Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5. Earl of Cork, thereby disambiguating it. EL is highly beneficial in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io https://github.com/UKPLab/ acl2020-interactive-entity-linking 2 2014), semantic enrichment (Schl¨ogl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018). These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004). In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation. For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotat"
2020.acl-main.624,D19-1410,1,0.895958,"literature: the gradient boosted trees variant LightGBM (Ke et al., 2017), RankSVM (Joachims, 2002) and RankNet (Burges et al., 2005). Models are retrained in the background when new annotations are made, thus improving over time with an increasing number of annotations. We use a set of generic handcrafted features which are described in Table 1. These models were chosen as they can work with low data, train quickly and allow introspection. Using deep models or word embeddings as input features showed to be too slow to be inter6984 active. We also leverage pretrained Sentence-BERT embeddings (Reimers and Gurevych, 2019) trained on Natural Language Inference data written in simple English. These are not fine-tuned by us during training. Although they come from a different domain, we conjecture that the WordPiece tokenization of BERT helps with the spelling variance of our texts in contrast to traditional word embeddings which would have many out-of-vocabulary words. For specific tasks, custom features can easily be incorporated e.g. entity type information, time information for diachronic entity linking, location information or distance for annotating geographical entities. • • • • Mention exactly matches lab"
2020.acl-main.624,Q19-1029,0,0.0329165,"n-in-the-loop annotation HITL machine learning describes an interactive scenario where a machine learning (ML) system and a human work together to improve their performance. The ML system gives predictions, and the human corrects if they are wrong and helps to spot things that have been overlooked by the machine. The system uses this feedback to improve, leading to better predictions and thereby reducing the effort of the human. In natural language processing, it has been applied in scenarios like interactive text summarization (Gao et al., 2018), parsing (He et al., 2016) or data generation (Wallace et al., 2019). Regarding machine-learning assisted annotation, Yimam et al. (2014) propose an annotation editor that during annotation, interactively trains a model using annotations made by the user. They use string matching and MIRA (Crammer and Singer, 2003) as recommenders, evaluate on POS and NER annotation and show improvement in annotation speed. TASTY (Arnold et al., 2016) is a system that is able to perform EL against Wikipedia on the fly while typing a document. A pretrained neural sequence tagger is being used that performs mention detection. Candidates are precomputed and the candidate is chose"
2020.acl-main.624,P14-5016,1,0.905303,"Missing"
2020.acl-main.624,N10-1072,0,0.251576,"n We index the knowledge base and use full text search to retrieve candidates based on the surface form of the annotated mention. Besides, users can query this index during annotation. We use fuzzy search to help in cases where the mention and the knowledge base label are almost the same but not identical (e.g. Dublin vs. Dublyn). In the interactive setting, the user can also search the knowledge base during annotation, e.g. in cases when the gold entity is not ranked high enough or when the surface form and knowledge base label are not the same (Zeus vs. Jupiter). Candidate Ranking We follow Zheng et al. (2010) and model candidate ranking as a learningto-rank problem: given a mention and a list of candidates, sort the candidates so that the most relevant candidate is at the top. For training, we guarantee that the gold candidate is present in the candidate list. For evaluation, the gold candidate can be absent from the candidate list if the candidate search failed to find it. This interaction is the core Human-in-the-loop in our approach. For training, we rephrase the task as preference learning: By selecting an entity label from the candidate list, users express that the selected one was preferred"
2020.acl-main.770,D16-1203,0,0.0324748,"ch are carefully designed to be void of the biases found in the training data. Using such targeted evaluation, McCoy et al. (2019b) observe that models trained on MNLI dataset (Williams et al., 2018) leverage syntactic patterns involving word overlap to blindly predict entailment. Similarly, Schuster et al. (2019) show that the predictions of fact verification models trained for the FEVER task (Thorne et al., 2018) are largely driven by the presence of indicative words in the input claim sentences. Following similar observations across other tasks and domains, e.g., visual question-answering (Agrawal et al., 2016), paraphrase identification (Zhang et al., 2019), and argument reasoning comprehension (Niven and Kao, 2019), researchers proposed improved data collection techniques to reduce the artifacts that result in dataset biases. While these approaches are promising, only applying them without additional efforts in the modeling part may still deliver an unsatisfactory outcome. For instance, collecting new examples by asking human annotators to conform to specific rules may be costly and thus limit the scale and diversity of the resulting data (Kaushik et al., 2020). Recently proposed adversarial filte"
2020.acl-main.770,S19-1028,0,0.0852426,"Missing"
2020.acl-main.770,2020.acl-main.463,0,0.0247037,"ng: • We present a novel confidence regularization method to prevent models from utilizing bi• We provide insights on how the debiasing methods behave across different datasets with varying degrees of biases and show that our method is more optimal when enough biasfree examples are available in the dataset. 2 Related Work Biases in Datasets Researchers have recently studied more closely the success of large fine-tuned LMs in many NLU tasks and found that models are simply better in leveraging biased patterns instead of capturing a better notion of language understanding for the intended task (Bender and Koller, 2020). Models’ performance often drops to a random baseline when evaluated on out-of-distribution datasets which are carefully designed to be void of the biases found in the training data. Using such targeted evaluation, McCoy et al. (2019b) observe that models trained on MNLI dataset (Williams et al., 2018) leverage syntactic patterns involving word overlap to blindly predict entailment. Similarly, Schuster et al. (2019) show that the predictions of fact verification models trained for the FEVER task (Thorne et al., 2018) are largely driven by the presence of indicative words in the input claim se"
2020.acl-main.770,D19-1418,0,0.22065,"etween the surface features and the target labels are not present. This brittleness has, in turn, limited their practical applicability in some extrinsic use cases (Falke et al., 2019). This problem has sparked interest among researchers in building models that are robust against dataset biases. Proposed methods in this direction build on previous works, which have largely explored the format of several prominent labelrevealing biases on certain datasets (Belinkov et al., 2019). Two current prevailing methods, product-ofexpert (He et al., 2019; Mahabadi and Henderson, 2019) and learned-mixin (Clark et al., 2019a) introduce several strategies to overcome the known biases by correcting the conditional distribution of the target labels given the presence of biased features. They achieve this by reducing the importance of examples that can be predicted correctly by using only biased features. As a result, models are forced to learn from harder examples in which utilizing solely superficial features is not sufficient to make correct predictions. While these two state-of-the-art debiasing methods provide a remarkable improvement on the targeted out-of-distribution test sets, they do so at the cost of degr"
2020.acl-main.770,P19-1595,0,0.0381563,"Missing"
2020.acl-main.770,N19-1423,0,0.25874,"method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy.1 1 Introduction Despite the impressive performance on many natural language understanding (NLU) benchmarks (Wang et al., 2018), recent pre-trained language models (LM) such as BERT (Devlin et al., 2019) are shown to rely heavily on idiosyncratic biases of datasets (McCoy et al., 2019b; Schuster et al., 2019; Zhang et al., 2019). These biases are commonly characterized as surface features of input examples that are strongly associated with the target labels, e.g., occurrences of negation words in 1 The code is available at https://github.com/ UKPLab/acl2020-confidence-regularization natural language inference (NLI) datasets which are biased towards the contradiction label (Gururangan et al., 2018; Poliak et al., 2018). As a ramification of relying on biases, models break on the out-of-distrib"
2020.acl-main.770,P19-1213,1,0.901379,"Missing"
2020.acl-main.770,D18-1407,0,0.0601642,"Missing"
2020.acl-main.770,N18-2017,0,0.133194,"Missing"
2020.acl-main.770,D19-6115,0,0.48106,"the out-of-distribution data, in which such associative patterns between the surface features and the target labels are not present. This brittleness has, in turn, limited their practical applicability in some extrinsic use cases (Falke et al., 2019). This problem has sparked interest among researchers in building models that are robust against dataset biases. Proposed methods in this direction build on previous works, which have largely explored the format of several prominent labelrevealing biases on certain datasets (Belinkov et al., 2019). Two current prevailing methods, product-ofexpert (He et al., 2019; Mahabadi and Henderson, 2019) and learned-mixin (Clark et al., 2019a) introduce several strategies to overcome the known biases by correcting the conditional distribution of the target labels given the presence of biased features. They achieve this by reducing the importance of examples that can be predicted correctly by using only biased features. As a result, models are forced to learn from harder examples in which utilizing solely superficial features is not sufficient to make correct predictions. While these two state-of-the-art debiasing methods provide a remarkable improvement on the t"
2020.acl-main.770,P19-1459,0,0.0364325,", McCoy et al. (2019b) observe that models trained on MNLI dataset (Williams et al., 2018) leverage syntactic patterns involving word overlap to blindly predict entailment. Similarly, Schuster et al. (2019) show that the predictions of fact verification models trained for the FEVER task (Thorne et al., 2018) are largely driven by the presence of indicative words in the input claim sentences. Following similar observations across other tasks and domains, e.g., visual question-answering (Agrawal et al., 2016), paraphrase identification (Zhang et al., 2019), and argument reasoning comprehension (Niven and Kao, 2019), researchers proposed improved data collection techniques to reduce the artifacts that result in dataset biases. While these approaches are promising, only applying them without additional efforts in the modeling part may still deliver an unsatisfactory outcome. For instance, collecting new examples by asking human annotators to conform to specific rules may be costly and thus limit the scale and diversity of the resulting data (Kaushik et al., 2020). Recently proposed adversarial filtering methods (Zellers et al., 2019; Sakaguchi et al., 2019) are more cost effective but are not guaranteed t"
2020.acl-main.770,S18-2023,0,0.100767,"Missing"
2020.acl-main.770,W18-5446,0,0.069302,"Missing"
2020.acl-main.770,N18-1101,0,0.370376,"t. 2 Related Work Biases in Datasets Researchers have recently studied more closely the success of large fine-tuned LMs in many NLU tasks and found that models are simply better in leveraging biased patterns instead of capturing a better notion of language understanding for the intended task (Bender and Koller, 2020). Models’ performance often drops to a random baseline when evaluated on out-of-distribution datasets which are carefully designed to be void of the biases found in the training data. Using such targeted evaluation, McCoy et al. (2019b) observe that models trained on MNLI dataset (Williams et al., 2018) leverage syntactic patterns involving word overlap to blindly predict entailment. Similarly, Schuster et al. (2019) show that the predictions of fact verification models trained for the FEVER task (Thorne et al., 2018) are largely driven by the presence of indicative words in the input claim sentences. Following similar observations across other tasks and domains, e.g., visual question-answering (Agrawal et al., 2016), paraphrase identification (Zhang et al., 2019), and argument reasoning comprehension (Niven and Kao, 2019), researchers proposed improved data collection techniques to reduce t"
2020.acl-main.770,P19-1472,0,0.0230017,"dentification (Zhang et al., 2019), and argument reasoning comprehension (Niven and Kao, 2019), researchers proposed improved data collection techniques to reduce the artifacts that result in dataset biases. While these approaches are promising, only applying them without additional efforts in the modeling part may still deliver an unsatisfactory outcome. For instance, collecting new examples by asking human annotators to conform to specific rules may be costly and thus limit the scale and diversity of the resulting data (Kaushik et al., 2020). Recently proposed adversarial filtering methods (Zellers et al., 2019; Sakaguchi et al., 2019) are more cost effective but are not guaranteed to be artifacts-free. It is, 8718 therefore, crucial to develop learning methods that can overcome biases as a complement to the data collection efforts. Debiasing Models There exist several methods that aim to improve models’ robustness and generalization by leveraging the insights from previous work about the datasets’ artifacts. In the NLI task, Belinkov et al. (2019) make use of the finding that partial input information from the hypothesis sentence is sufficient to achieve reasonable accuracy. They then remove this h"
2020.acl-main.770,N19-1131,0,0.0692578,"Missing"
2020.acl-main.770,D19-1341,0,0.457712,"hem to receive enough incentive to learn from all the training examples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy.1 1 Introduction Despite the impressive performance on many natural language understanding (NLU) benchmarks (Wang et al., 2018), recent pre-trained language models (LM) such as BERT (Devlin et al., 2019) are shown to rely heavily on idiosyncratic biases of datasets (McCoy et al., 2019b; Schuster et al., 2019; Zhang et al., 2019). These biases are commonly characterized as surface features of input examples that are strongly associated with the target labels, e.g., occurrences of negation words in 1 The code is available at https://github.com/ UKPLab/acl2020-confidence-regularization natural language inference (NLI) datasets which are biased towards the contradiction label (Gururangan et al., 2018; Poliak et al., 2018). As a ramification of relying on biases, models break on the out-of-distribution data, in which such associative patterns between the surface features and the target labels are not"
2020.cl-2.4,P18-2049,0,0.110649,"ogy tasks has been proposed. Although these tasks seem to be intuitive, there are concerns regarding their consistency and correlation with downstream 336 ¨ S¸ahin et al. Gul Multilingual Probing Tasks for Word Representations task performance (Linzen 2016; Schnabel et al. 2015). Furthermore, such evaluation requires manually created test sets and these are usually only available for a small number of languages. Another option to assess the quality of word representation is through extrinsic evaluation, where the word vectors are used directly in downstream tasks, such as machine translation (Ataman and Federico 2018), semantic role labeling (SRL) (S¸ahin and Steedman 2018) or language modeling (Vania and Lopez 2017). Although this method provides more insightful information about the end task performance, it requires expensive human annotations, computational resources, and the results are sensitive to hyperparameter choice. To address the aforementioned problems, a few studies have introduced the idea ¨ of probing tasks (Kohn 2016; Shi, Padhi, and Knight 2016; Adi et al. 2017; Veldhoen, Hupkes, and Zuidema 2016; Conneau et al. 2018a); which are a set of multi-class classification problems that probe a le"
2020.cl-2.4,P17-1080,0,0.146812,"out the end task performance, it requires expensive human annotations, computational resources, and the results are sensitive to hyperparameter choice. To address the aforementioned problems, a few studies have introduced the idea ¨ of probing tasks (Kohn 2016; Shi, Padhi, and Knight 2016; Adi et al. 2017; Veldhoen, Hupkes, and Zuidema 2016; Conneau et al. 2018a); which are a set of multi-class classification problems that probe a learned word vector for a specific linguistic property, such as part-of-speech (POS), semantic, or morphological tag.1 Probing tasks have gained a lot of attention (Belinkov et al. 2017; Bisazza and Tump 2018, among others) due to their simplicity, low computational cost, and ability to provide some insights regarding the linguistic properties that are captured by the learned representations. The majority of the probing tests proposed so far are mostly designed for English language only, and operate on the sentence-level (e.g., tree depth, word count, top constituent by Conneau et al. 2018a). Although sentence-level probing may provide valuable insights for English sentence-level representations, we hypothesize that they would not be similarly beneficial in a multilingual se"
2020.cl-2.4,Q19-1004,0,0.0128313,"aining (e.g., machine translation typically needs a large amount parallel data for training). For a more general evaluation, Conneau et al. (2018a) and Tenney et al. (2019) each introduced a broad coverage evaluation suite to analyze representations on the sentence level with a focus on English. We build our methodology upon these recent works. However, unlike their methods, our evaluation suite is multilingual and takes language-specific features into account. Moreover, our tests are type-level, rather than sentence- (Conneau et al. 2018a) or sub-sentence-level (Tenney et al. 2019). Finally, Belinkov and Glass (2019) recently surveyed various analysis methods in NLP and mention three important aspects for model analysis: (1) the methods (classifiers, correlations, or similarity), (2) the linguistic phenomena (sentence length, word order, syntactic, or semantic information, etc), and (3) the neural network components (embeddings or hidden states). They have also provided a non-exhaustive list of previous work which use probing task (classifier) method for analyzing representations, including word representations. For a more comprehensive list of studies on what linguistic information is captured in neural"
2020.cl-2.4,D08-1031,0,0.105585,"Missing"
2020.cl-2.4,D18-1313,0,0.126479,"rmance, it requires expensive human annotations, computational resources, and the results are sensitive to hyperparameter choice. To address the aforementioned problems, a few studies have introduced the idea ¨ of probing tasks (Kohn 2016; Shi, Padhi, and Knight 2016; Adi et al. 2017; Veldhoen, Hupkes, and Zuidema 2016; Conneau et al. 2018a); which are a set of multi-class classification problems that probe a learned word vector for a specific linguistic property, such as part-of-speech (POS), semantic, or morphological tag.1 Probing tasks have gained a lot of attention (Belinkov et al. 2017; Bisazza and Tump 2018, among others) due to their simplicity, low computational cost, and ability to provide some insights regarding the linguistic properties that are captured by the learned representations. The majority of the probing tests proposed so far are mostly designed for English language only, and operate on the sentence-level (e.g., tree depth, word count, top constituent by Conneau et al. 2018a). Although sentence-level probing may provide valuable insights for English sentence-level representations, we hypothesize that they would not be similarly beneficial in a multilingual setup for several reasons"
2020.cl-2.4,Q17-1010,0,0.654042,"ta, thus eliminating the need for expensive manual feature engineering. The initial success of dense representations in NLP applications has led to the development of a multitude of embedding models, which differ in terms of design objective (monolingual [Mikolov et al. 2013b], crosslingual [Ruder, Vulic, and Søgaard 2019], contextualized [Peters et al. 2018], retrofitted [Faruqui et al. 2015], multi-sense [Pilehvar et al. 2017], cross-domain [Yang, Lu, and Zheng 2017], dependency-based [Levy and Goldberg 2014]), encoding architecture, (convolution [Kim et al. 2016], linear vector operations [Bojanowski et al. 2017], bidirectional LSTM [Ling et al. 2015]), as well as in terms of the target units (words, characters, character n-grams, morphemes, phonemes). While offering substantial benefits over the traditional feature-based representations of language, the performance of unsupervised embeddings may differ considerably depending on the language and the task. For instance, early embedding models such as word2vec (Mikolov et al. 2013b) and GloVe (Pennington, Socher, and Manning 2014) have been shown to suffer from out-of-vocabulary (OOV) issues for agglutinative languages like Turkish and Finnish (S¸ahin"
2020.cl-2.4,D15-1075,0,0.0795642,"Missing"
2020.cl-2.4,P12-1015,0,0.0416313,"n A standard approach to evaluate continuous word representations is by testing them on a variety of benchmarks that measure some linguistic properties of the word. These similarity benchmarks typically consist of a set of words or word pairs that are manually annotated for some notion of relatedness (semantic, syntactic, topical, etc.). For English, some of the widely used similarity benchmarks are WordSim-353 (Finkelstein et al. 2001), MC (Miller and Charles 1991), RG (Rubenstein and Goodenough 1965), SCWS (Huang et al. 2012), rare words data set (RW) (Luong, Socher, and Manning 2013), MEN (Bruni et al. 2012), and SimLex-999 (Hill, Reichart, and Korhonen 2015). Although these benchmarks have shown to be useful for evaluating English word representations, only very few word similarity data sets exist in other languages. Human-assessed translations of WordSim-353 and SimLex-999 on three languages, Italian, German, and Russian, have been collected.2 For the SemEval 2017 shared task, Camacho-Collados et al. (2017) introduced manually curated word-similarity data sets for English, Farsi, German, Italian, and Spanish. Another popular benchmark for evaluating word representations is the word analogy test"
2020.cl-2.4,S17-2002,0,0.0214118,"s are WordSim-353 (Finkelstein et al. 2001), MC (Miller and Charles 1991), RG (Rubenstein and Goodenough 1965), SCWS (Huang et al. 2012), rare words data set (RW) (Luong, Socher, and Manning 2013), MEN (Bruni et al. 2012), and SimLex-999 (Hill, Reichart, and Korhonen 2015). Although these benchmarks have shown to be useful for evaluating English word representations, only very few word similarity data sets exist in other languages. Human-assessed translations of WordSim-353 and SimLex-999 on three languages, Italian, German, and Russian, have been collected.2 For the SemEval 2017 shared task, Camacho-Collados et al. (2017) introduced manually curated word-similarity data sets for English, Farsi, German, Italian, and Spanish. Another popular benchmark for evaluating word representations is the word analogy test. This test was specifically introduced by Mikolov et al. (2013a) to evaluate word vectors trained using neural models. The main goal is to determine how syntactic and semantic relationships between words are reflected in the continuous space. Given a pair of words, man and woman, the task is to find a target word that shares the same relation with a given source word. For example, given a word king, one e"
2020.cl-2.4,K18-2005,0,0.0213678,"he same words would have different representations when used in different contexts. However, our probing tests are type-level (as opposed to token-level), thus we only use the representations generated independently per each token both for the intrinsic and extrinsic experiments. In the scope of this study, ELMo embeddings are treated as powerful pretrained character-level decontextualized vectors. To highlight this important detail, we further refer to our ELMo-derived embeddings as Decontextualized ELMo (D-ELMo). We use the multilingual pretrained ELMo embeddings distributed by the authors (Che et al. 2018; Fares et al. 2017),13 which are trained with the same hyperparameter settings as the original (Peters et al. 2018) for the bidirectional language model and the character CNN. They are trained on randomly sampled 20 million words from Wikipedia dump and Common Crawl data sets and have dimensionality 1,024. We use the three-layer averaged ELMo representation for each word. For all the experiments described in Section 4.4 and Section 4.3, we first created the vocabulary for all intrinsic and extrinsic data sets per language. Then, we generated the vectors using the embeddings that can handle OO"
2020.cl-2.4,P17-1152,0,0.0494826,"Missing"
2020.cl-2.4,P18-1198,0,0.108388,"are used directly in downstream tasks, such as machine translation (Ataman and Federico 2018), semantic role labeling (SRL) (S¸ahin and Steedman 2018) or language modeling (Vania and Lopez 2017). Although this method provides more insightful information about the end task performance, it requires expensive human annotations, computational resources, and the results are sensitive to hyperparameter choice. To address the aforementioned problems, a few studies have introduced the idea ¨ of probing tasks (Kohn 2016; Shi, Padhi, and Knight 2016; Adi et al. 2017; Veldhoen, Hupkes, and Zuidema 2016; Conneau et al. 2018a); which are a set of multi-class classification problems that probe a learned word vector for a specific linguistic property, such as part-of-speech (POS), semantic, or morphological tag.1 Probing tasks have gained a lot of attention (Belinkov et al. 2017; Bisazza and Tump 2018, among others) due to their simplicity, low computational cost, and ability to provide some insights regarding the linguistic properties that are captured by the learned representations. The majority of the probing tests proposed so far are mostly designed for English language only, and operate on the sentence-level ("
2020.cl-2.4,D18-1269,0,0.0431937,"Missing"
2020.cl-2.4,Q19-1021,0,0.324122,"lmost all tests, which could be due to the segmenting mechanism enabled by the BPE that can capture the morphological boundaries better than n-gram based fastText given the highly fusional nature of Russian morphological marking. Our intrinsic experiment results show that the probing task performance and the improvement compared to the majority baseline differs depending on the language and the task, signaling that not all languages and morphological categories are equally easy to model. There exist several ways to quantitatively capture morphological complexity, for example, a recent work by Cotterell et al. (2019) plots the morphological counting complexity (MCC) of the languages (defined as the number of cells in a language’s inflectional morphological paradigm) against a novel entropy-based irregularity measure to empirically demonstrate the hypothesized bound on the two complexity types: Although a language can have a large paradigm or be highly irregular, it’s never both. While paradigm-based counting complexity cannot be applied to the probing tests directly because of their categorical nature, one can use the number of unique values in a respective category as a rough approximation of the complex"
2020.cl-2.4,N18-2085,0,0.358703,"tasks: POS, DEP, and SRL, whereas XNLI correlated well with Case, Mood, OddFeat, Person, and Tense. We observe that the correlating tasks are similar to those of agglutinative languages in general (except from CharacterBin—explained below), however weaker. The weaker correlations may be the result of the highly ambiguous nature of German data (especially Case and POS), and less lexical diversity, which are both common among fusional languages, as shown in Table 5. In addition, the paradigm sizes for German nouns and verbs are only 29 and 8 (for reference Turkish has 120 and 100 respectively) (Cotterell et al. 2018). Russian. For Russian, we find that Case, Number, and TagCount have high correlations with a p-value of 0.2 for syntactic tasks, similar to German, whereas XNLI correlated better with the other features such as SameFeat, OddFeat, Person, and Tense. Russian is among fusional languages like German, with similar MCC and irregularity values. One exception is the high irregularity of Russian verbs (with a score of 1.67) compared with German verbs that have a score of 0.77 (Cotterell et al. 2018). This could explain the weaker correlations for verb-related probing tasks such as Person and Tense, as"
2020.cl-2.4,N13-4004,0,0.100329,"Missing"
2020.cl-2.4,erten-etal-2014-turkish,0,0.0424529,"Missing"
2020.cl-2.4,J08-3003,0,0.131419,"Missing"
2020.cl-2.4,W17-0237,0,0.0215423,"ld have different representations when used in different contexts. However, our probing tests are type-level (as opposed to token-level), thus we only use the representations generated independently per each token both for the intrinsic and extrinsic experiments. In the scope of this study, ELMo embeddings are treated as powerful pretrained character-level decontextualized vectors. To highlight this important detail, we further refer to our ELMo-derived embeddings as Decontextualized ELMo (D-ELMo). We use the multilingual pretrained ELMo embeddings distributed by the authors (Che et al. 2018; Fares et al. 2017),13 which are trained with the same hyperparameter settings as the original (Peters et al. 2018) for the bidirectional language model and the character CNN. They are trained on randomly sampled 20 million words from Wikipedia dump and Common Crawl data sets and have dimensionality 1,024. We use the three-layer averaged ELMo representation for each word. For all the experiments described in Section 4.4 and Section 4.3, we first created the vocabulary for all intrinsic and extrinsic data sets per language. Then, we generated the vectors using the embeddings that can handle OOV words, namely, fas"
2020.cl-2.4,N15-1184,0,0.0276499,"senting text with dense, low-dimensional vectors—or embeddings—has become the de facto approach, since these representations can encode complex relationships between the units of language and can be learned from unlabeled data, thus eliminating the need for expensive manual feature engineering. The initial success of dense representations in NLP applications has led to the development of a multitude of embedding models, which differ in terms of design objective (monolingual [Mikolov et al. 2013b], crosslingual [Ruder, Vulic, and Søgaard 2019], contextualized [Peters et al. 2018], retrofitted [Faruqui et al. 2015], multi-sense [Pilehvar et al. 2017], cross-domain [Yang, Lu, and Zheng 2017], dependency-based [Levy and Goldberg 2014]), encoding architecture, (convolution [Kim et al. 2016], linear vector operations [Bojanowski et al. 2017], bidirectional LSTM [Ling et al. 2015]), as well as in terms of the target units (words, characters, character n-grams, morphemes, phonemes). While offering substantial benefits over the traditional feature-based representations of language, the performance of unsupervised embeddings may differ considerably depending on the language and the task. For instance, early em"
2020.cl-2.4,W18-2501,0,0.0648441,"Missing"
2020.cl-2.4,Q18-1032,0,0.0464415,"Missing"
2020.cl-2.4,I17-1042,0,0.0394909,"Missing"
2020.cl-2.4,L18-1473,0,0.0329254,"hat have subword-level information learned from character n-grams. In simple terms, words are represented as a linear combination of the character n-gram embeddings of the token’s character n-grams. We use the embeddings distributed by fastText,10 which are trained on preprocessed Wikipedia using CBOW with position-weights, in dimension 300, with character n-grams of length 5 and a window of size 5. GloVe-BPE is another type of subword-level embedding that uses unsupervised morphological segments generated by a compression algorithm inspired by Gage (1994). We use the pretrained embeddings by Heinzerling and Strube (2018), which are trained on preprocessed Wikipedia using GloVe (Pennington, Socher, and Manning 2014). We use the Python wrapper open sourced by the authors11 with default dictionary size of 10K and dimension 300. Because the tool provides embeddings for each segment, in case of multiple segments per token, we used the averaged vector as the word representation. 9 https://code.google.com/archive/p/word2vec/. 10 https://fasttext.cc/. 11 https://github.com/bheinzerling/bpemb. 354 ¨ S¸ahin et al. Gul Multilingual Probing Tasks for Word Representations MUSE-supervised embeddings are crosslingual fastTe"
2020.cl-2.4,J15-4004,0,0.079853,"Missing"
2020.cl-2.4,N12-1032,0,0.0335734,"Missing"
2020.cl-2.4,P12-1092,0,0.0354999,") extrinsic, when they are evaluated on downstream NLP tasks. 2.1 Intrinsic Evaluation A standard approach to evaluate continuous word representations is by testing them on a variety of benchmarks that measure some linguistic properties of the word. These similarity benchmarks typically consist of a set of words or word pairs that are manually annotated for some notion of relatedness (semantic, syntactic, topical, etc.). For English, some of the widely used similarity benchmarks are WordSim-353 (Finkelstein et al. 2001), MC (Miller and Charles 1991), RG (Rubenstein and Goodenough 1965), SCWS (Huang et al. 2012), rare words data set (RW) (Luong, Socher, and Manning 2013), MEN (Bruni et al. 2012), and SimLex-999 (Hill, Reichart, and Korhonen 2015). Although these benchmarks have shown to be useful for evaluating English word representations, only very few word similarity data sets exist in other languages. Human-assessed translations of WordSim-353 and SimLex-999 on three languages, Italian, German, and Russian, have been collected.2 For the SemEval 2017 shared task, Camacho-Collados et al. (2017) introduced manually curated word-similarity data sets for English, Farsi, German, Italian, and Spanish. A"
2020.cl-2.4,L18-1293,0,0.0230723,"ntages: It’s compact and less prone to majority and domain shift effects. However, because downstream NLP tasks mostly operate on full-text data, decoupling evaluation from running text might result in less realistic performance estimates; besides, it limits the evaluation of contextualized word representations and black-box models. To investigate the limitations and the strengths of type-level tasks, we prepare a set of comparable token-level probing tasks using the modified Universal Dependency (UD) treebanks where the Morphosyntactic Descriptions have been converted to the UniMorph schema (McCarthy et al. 2018). Contrary to the typelevel tasks, we do not filter out any infrequent or ambiguous surface forms; and we do not introduce a “None” class for convenience. Because the data set is annotated with the same schema as in our type-level tasks, we simply adapt our existing source code that creates single form feature tests (e.g., Tense, Case) for token-sentence pairs. Similar to the single form type-level tasks, if the total number of samples for a certain feature is less than 10K; or if a feature (e.g., case marker) has only one value, we exclude that featurelanguage pair from the tests. The created"
2020.cl-2.4,D15-1246,0,0.0501669,"Missing"
2020.cl-2.4,W16-2512,0,0.121269,"s the quality of word representation is through extrinsic evaluation, where the word vectors are used directly in downstream tasks, such as machine translation (Ataman and Federico 2018), semantic role labeling (SRL) (S¸ahin and Steedman 2018) or language modeling (Vania and Lopez 2017). Although this method provides more insightful information about the end task performance, it requires expensive human annotations, computational resources, and the results are sensitive to hyperparameter choice. To address the aforementioned problems, a few studies have introduced the idea ¨ of probing tasks (Kohn 2016; Shi, Padhi, and Knight 2016; Adi et al. 2017; Veldhoen, Hupkes, and Zuidema 2016; Conneau et al. 2018a); which are a set of multi-class classification problems that probe a learned word vector for a specific linguistic property, such as part-of-speech (POS), semantic, or morphological tag.1 Probing tasks have gained a lot of attention (Belinkov et al. 2017; Bisazza and Tump 2018, among others) due to their simplicity, low computational cost, and ability to provide some insights regarding the linguistic properties that are captured by the learned representations. The majority of the probing t"
2020.cl-2.4,P14-2050,0,0.108505,"Missing"
2020.cl-2.4,D15-1176,0,0.0705479,"Missing"
2020.cl-2.4,W16-2503,0,0.112824,"vial. Scanning the large parameter space may be extremely time consuming and computationally expensive, which poses significant challenges, especially in the lowerresource non-English academic NLP communities. To simplify the search for a good representation, and estimate the “quality” of the representations, intrinsic evaluation via similarity and analogy tasks has been proposed. Although these tasks seem to be intuitive, there are concerns regarding their consistency and correlation with downstream 336 ¨ S¸ahin et al. Gul Multilingual Probing Tasks for Word Representations task performance (Linzen 2016; Schnabel et al. 2015). Furthermore, such evaluation requires manually created test sets and these are usually only available for a small number of languages. Another option to assess the quality of word representation is through extrinsic evaluation, where the word vectors are used directly in downstream tasks, such as machine translation (Ataman and Federico 2018), semantic role labeling (SRL) (S¸ahin and Steedman 2018) or language modeling (Vania and Lopez 2017). Although this method provides more insightful information about the end task performance, it requires expensive human annotation"
2020.cl-2.4,W13-3512,0,0.116497,"Missing"
2020.cl-2.4,W18-6011,0,0.0233356,"ntages: It’s compact and less prone to majority and domain shift effects. However, because downstream NLP tasks mostly operate on full-text data, decoupling evaluation from running text might result in less realistic performance estimates; besides, it limits the evaluation of contextualized word representations and black-box models. To investigate the limitations and the strengths of type-level tasks, we prepare a set of comparable token-level probing tasks using the modified Universal Dependency (UD) treebanks where the Morphosyntactic Descriptions have been converted to the UniMorph schema (McCarthy et al. 2018). Contrary to the typelevel tasks, we do not filter out any infrequent or ambiguous surface forms; and we do not introduce a “None” class for convenience. Because the data set is annotated with the same schema as in our type-level tasks, we simply adapt our existing source code that creates single form feature tests (e.g., Tense, Case) for token-sentence pairs. Similar to the single form type-level tasks, if the total number of samples for a certain feature is less than 10K; or if a feature (e.g., case marker) has only one value, we exclude that featurelanguage pair from the tests. The created"
2020.cl-2.4,P05-1012,0,0.0563746,"Missing"
2020.cl-2.4,W17-5301,0,0.0550772,"Missing"
2020.cl-2.4,W16-2504,0,0.0626383,"Missing"
2020.cl-2.4,D14-1162,0,0.0817963,"Missing"
2020.cl-2.4,N18-1202,0,0.648392,"ations with continuous ones. Representing text with dense, low-dimensional vectors—or embeddings—has become the de facto approach, since these representations can encode complex relationships between the units of language and can be learned from unlabeled data, thus eliminating the need for expensive manual feature engineering. The initial success of dense representations in NLP applications has led to the development of a multitude of embedding models, which differ in terms of design objective (monolingual [Mikolov et al. 2013b], crosslingual [Ruder, Vulic, and Søgaard 2019], contextualized [Peters et al. 2018], retrofitted [Faruqui et al. 2015], multi-sense [Pilehvar et al. 2017], cross-domain [Yang, Lu, and Zheng 2017], dependency-based [Levy and Goldberg 2014]), encoding architecture, (convolution [Kim et al. 2016], linear vector operations [Bojanowski et al. 2017], bidirectional LSTM [Ling et al. 2015]), as well as in terms of the target units (words, characters, character n-grams, morphemes, phonemes). While offering substantial benefits over the traditional feature-based representations of language, the performance of unsupervised embeddings may differ considerably depending on the language a"
2020.cl-2.4,P17-1170,0,0.0196078,"ional vectors—or embeddings—has become the de facto approach, since these representations can encode complex relationships between the units of language and can be learned from unlabeled data, thus eliminating the need for expensive manual feature engineering. The initial success of dense representations in NLP applications has led to the development of a multitude of embedding models, which differ in terms of design objective (monolingual [Mikolov et al. 2013b], crosslingual [Ruder, Vulic, and Søgaard 2019], contextualized [Peters et al. 2018], retrofitted [Faruqui et al. 2015], multi-sense [Pilehvar et al. 2017], cross-domain [Yang, Lu, and Zheng 2017], dependency-based [Levy and Goldberg 2014]), encoding architecture, (convolution [Kim et al. 2016], linear vector operations [Bojanowski et al. 2017], bidirectional LSTM [Ling et al. 2015]), as well as in terms of the target units (words, characters, character n-grams, morphemes, phonemes). While offering substantial benefits over the traditional feature-based representations of language, the performance of unsupervised embeddings may differ considerably depending on the language and the task. For instance, early embedding models such as word2vec (Mik"
2020.cl-2.4,P16-1140,0,0.0248194,"Missing"
2020.cl-2.4,C18-1228,0,0.0338959,"Missing"
2020.cl-2.4,S17-1017,0,0.0398526,"Missing"
2020.cl-2.4,W03-0419,0,0.485679,"Missing"
2020.cl-2.4,D15-1036,0,0.052783,"Missing"
2020.cl-2.4,P16-1162,0,0.0159801,"Missing"
2020.cl-2.4,D16-1159,0,0.0461589,"Missing"
2020.cl-2.4,P15-2111,0,0.155084,"cholinguistic ones like pseudowords (artificial words that are phonologically 1 We use the terms probing tasks and probing tests interchangeably throughout the article. 337 Computational Linguistics Volume 46, Number 2 well-formed but have no meaning). Although languages share a large set of common probing tasks, each has a list of its own, for example, Russian and Spanish are probed for gender, whereas Turkish is probed for polarity and possession. • We introduce a reusable, systematic methodology for creation and evaluation of such tests by utilizing the existing resources such as UniMorph (Sylak-Glassman et al. 2015; Sylak-Glassman 2016; Kirov et al. 2018), Wikipedia, and Wuggy (Keuleers and Brysbaert 2010). • We then use the proposed probing tasks to evaluate a set of diverse multilingual embedding models and to diagnose a neural end-to-end semantic role labeling model as a case study. We statistically assess the correlation between probing and downstream task performance for a variety of downstream tasks (POS tagging, dependency parsing [DEP], SRL, named entity recognition [NER], and natural language inference [NLI]) for a set of typologically diverse languages and find that a number of probing tests h"
2020.cl-2.4,W18-5400,0,0.0751506,"Missing"
2020.cl-2.4,D15-1243,0,0.0606531,"Missing"
2020.cl-2.4,D18-1278,1,0.908872,"Missing"
2020.cl-2.4,P17-1184,1,0.929586,"r consistency and correlation with downstream 336 ¨ S¸ahin et al. Gul Multilingual Probing Tasks for Word Representations task performance (Linzen 2016; Schnabel et al. 2015). Furthermore, such evaluation requires manually created test sets and these are usually only available for a small number of languages. Another option to assess the quality of word representation is through extrinsic evaluation, where the word vectors are used directly in downstream tasks, such as machine translation (Ataman and Federico 2018), semantic role labeling (SRL) (S¸ahin and Steedman 2018) or language modeling (Vania and Lopez 2017). Although this method provides more insightful information about the end task performance, it requires expensive human annotations, computational resources, and the results are sensitive to hyperparameter choice. To address the aforementioned problems, a few studies have introduced the idea ¨ of probing tasks (Kohn 2016; Shi, Padhi, and Knight 2016; Adi et al. 2017; Veldhoen, Hupkes, and Zuidema 2016; Conneau et al. 2018a); which are a set of multi-class classification problems that probe a learned word vector for a specific linguistic property, such as part-of-speech (POS), semantic, or morp"
2020.cl-2.4,N18-1101,0,0.0202248,"task that used the resources provided by RTE challenge tasks which had a small size.14 Later, a larger data set, also known as the Stanford Natural Language Inference (SNLI; Bowman et al. 2015) data set, which has been compiled from English image caption corpora and labeled via crowdsourcing, has been introduced. Some example pairs of sentences are shown in Table 7. As stated by Bowman et al. (2015) and also can be seen from Table 7, a high-performing NLI model should handle phenomena like tense, modality, and negation, which are mostly covered by our probing tasks. MultiGenre NLI (MultiNLI; Williams, Nangia, and Bowman, 2018) is a recent data set that covers a wider variety of text styles and topics. The Crosslingual NLI (XNLI; Conneau et al. 2018b) data set has been derived from MultiNLI and is used as a benchmark for evaluating crosslingual sentence representations. This evaluation benchmark originally aimed at testing the models trained for the source language (English), on the target language, and covers 15 languages including Spanish, Turkish, Russian, and German. It should be noted that the development and test splits for each language in XNLI have been translated by professional translators. The authors al"
2020.cl-2.4,D17-1312,0,0.056603,"Missing"
2020.conll-1.8,Q19-1038,0,0.0217242,"ular baseline, (ii) the concatenation of average, min and max pooling (pmeans) (R¨uckl´e et al., 2018); and Random LSTMs (Conneau et al., 2017; Wieting and Kiela, 2019), which feed word embeddings to randomly initialized LSTMs, then apply a pooling operation across time-steps. As parametric methods, we consider: InferSent (Conneau et al., 2017), which induces a sentence representation by learning a semantic entailment relationship between two sentences; QuickThought (Logeswaran and Lee, 2018) which reframes the popular SkipThought model (Kiros et al., 2015) in a classification context; LASER (Artetxe and Schwenk, 2019) derived from massively multilingual machine translation models, and BERT base (Devlin et al., 2019), where we average token embeddings of the last layer for a sentence representation. Dimensionalities of encoders are listed in the appendix. 3.2 Probing Tasks Following Conneau et al. (2018), we consider the following probing tasks: BigramShift (en, tr, ru, ka), TreeDepth (en), Length (en, tr, ru, ka), Subject Number (en, tr, ru), WordContent (en, tr, ru, ka), and TopConstituents (en). Approach In the absence of ground truth, our main interest is in a ‘stable’ structural setup for probing task"
2020.conll-1.8,J08-4004,0,0.0763376,"rpus using COMBO for dependency parsing (Rybak and Wr´oblewska, 2018). They find that en and pl probing results mostly agree, i.e., encoders store the same linguistic information across the two languages. 3 data size and classifier choice for probing tasks.3 For a selected set of points p0 , p1 , . . . in X , we evaluate all our encoders on pi , and determine the ‘outcomes’ Oi (e.g., ranking) of the encoders at pi . We consider a setup pi as stable if outcome Oi is shared by a majority of other settings pj . This can be considered a region of agreement, similarly to inter-annotator agreement (Artstein and Poesio, 2008). In other words, we identify ‘ideal’ test conditions by minimizing the influence of parameters pi on the outcome Oi . Below, we will approximate these intuitions using correlation. 3.1 We consider two types of sentence encoders, nonparametric methods which combine word embeddings in elementary ways, without training; and parametric methods, which tune parameters on top of word embeddings. As non-parametric methods, we consider: (i) average word embeddings as a popular baseline, (ii) the concatenation of average, min and max pooling (pmeans) (R¨uckl´e et al., 2018); and Random LSTMs (Conneau e"
2020.conll-1.8,C18-1133,0,0.0227386,"s only determines the opinion flavor of a statement. Since sentiment analysis is a very established NLP task, we did not machine translate en training data, but used original data for en, ru and tr and created a novel dataset for ka. For en, we use the US Airline Twitter Sentiment dataset, consisting of 14,148 tweets labeled in three sentiment classes8 . For tr, we took the Turkish Twitter Sentiment Dataset with 6,172 examples and three classes9 . For ru, we used the Russian Twitter Corpus (RuTweetCorp), which we reduced to 30,000 examples in two classes.10 For ka, we followed the approach by Choudhary et al. (2018) and crawled sentiment flavored tweets in a distant supervision manner. Emojis were used as distant signals to indicate sentiment on preselected tweets from the Twitter API. After post-processing, we were able to collect 11,513 Georgian tweets in three sentiment classes. The dataset will made available publicly, including more details on the creation process. TREC Question Type Detection Question type detection is an important part of QuestionAnswering systems. The Text Retrieval Conference (TREC) dataset consists of a set of questions labeled with their respective question types (six labels i"
2020.conll-1.8,L18-1269,0,0.489565,"probe for sentence-level linguistic knowledge encoded in sentence embeddings (Perone et al., 2018) in a multilingual setup which marginalizes out the effects of probing task design choices when comparing sentence representations. Sentence embeddings have become central for representing texts beyond the word level, e.g., in small data scenarios, where it is difficult to induce good higher-level text representations from word embeddings (Subramanian et al., 2018) or for clustering or text retrieval applications (Reimers and Gurevych, 2019). To standardize the comparison of sentence embeddings, Conneau and Kiela (2018) proposed the SentEval framework for evaluating the quality of sentence embeddings on a range of downstream and 10 probing tasks. Probing tasks are used to introspect embeddings for linguistic knowledge, by taking “probes” as dedicated syntactic or semantic micro tasks (K¨ohn, 2016). As opposed to an evaluation in downstream applications or benchmarks like GLUE (Wang et al., 2018), probing tasks target very specific linguistic knowledge which may otherwise be confounded in downstream applications. Since they are artificial tasks, they can also be better controlled for to avoid dataset biases a"
2020.conll-1.8,D17-1070,0,0.659799,"probing evaluation should thus be carried out on multiple languages in the future. High Mid Low (A,B,C) (A,C,B) (A,B,C) classifier MLP (A,B,C) (C,B,A) (B,A,C) NB RF (C,A,B) (A,B,C) (B,C,A) (C,B,A) (C,B,A) (A,B,C) Table 1: Schematic illustration of our concept of stability across two dimensions (classifier and training size). Here, three encoders, dubbed A,B,C, are ranked. The region of stability is given by those settings that support the majority ranking of encoders, which is ABC. Introduction Sentence embeddings (a.k.a. sentence encoders) have become ubiquitous in NLP (Kiros et al., 2015; Conneau et al., 2017), extending the concept of word embeddings to the sentence level. In the context of recent efforts to open the black box of deep learning models and representations (Linzen et al., 2019), it has also become fashionable to probe sentence embeddings for the linguistic information signals they contain (Perone et al., 2018), as this may not be clear from their performances in downstream tasks. Such probes are linguistic micro tasks—like detecting the length of a sentence or its dependency tree depth—that have to be solved by a classifier using given representations. The majority of approaches for"
2020.conll-1.8,P18-1198,0,0.477726,"h-quality dependency parsers, as required for standard probing tasks, exist only for a handful of languages. E.g., UDPipe (Straka, 2018) is available for only about 100 languages, and performance scores for some of these are considerably below those of English (Straka, 2018). 108 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 108–118 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 required for obtaining reliable probing task results. This question is also relevant for English: on the one hand, Conneau et al. (2018) claim that training data for a probing task should be plentiful, as otherwise (highly parametrized) classifiers on top of representations may be unable to extract the relevant information signals; on the other hand, Hewitt and Liang (2019) note that a sufficiently powerful classifier with enough training data can in principle learn any task, without this necessarily allowing to conclude that the representations adequately store the linguistic signal under scrutiny. Second, we ask how stable probing task results are across different classifiers (e.g., MLP vs. Naive Bayes). This question is clo"
2020.conll-1.8,N19-1423,0,0.0354004,"d Random LSTMs (Conneau et al., 2017; Wieting and Kiela, 2019), which feed word embeddings to randomly initialized LSTMs, then apply a pooling operation across time-steps. As parametric methods, we consider: InferSent (Conneau et al., 2017), which induces a sentence representation by learning a semantic entailment relationship between two sentences; QuickThought (Logeswaran and Lee, 2018) which reframes the popular SkipThought model (Kiros et al., 2015) in a classification context; LASER (Artetxe and Schwenk, 2019) derived from massively multilingual machine translation models, and BERT base (Devlin et al., 2019), where we average token embeddings of the last layer for a sentence representation. Dimensionalities of encoders are listed in the appendix. 3.2 Probing Tasks Following Conneau et al. (2018), we consider the following probing tasks: BigramShift (en, tr, ru, ka), TreeDepth (en), Length (en, tr, ru, ka), Subject Number (en, tr, ru), WordContent (en, tr, ru, ka), and TopConstituents (en). Approach In the absence of ground truth, our main interest is in a ‘stable’ structural setup for probing task design—with the end goal of applying this design to multilingual probing analyses (keeping their res"
2020.conll-1.8,W19-4308,1,0.755063,"Missing"
2020.conll-1.8,L18-1550,0,0.0282593,"ifier with a size of roughly 10k instances overall. Table 6 provides more details about the datasets. In line with SentEval (and partly supported by our results on dataset balance given in the appendix), we aim for as balanced label distributions as possible. Because of the small test sizes, we use inner 5-fold cross validation for all tasks except for SubjNumber, where we use pre-defined train/dev/test splits as in Conneau et al. (2018) to avoid leaking lexical information from train to test splits. We obtain average and pmeans embeddings through pooling over pre-trained FastText embeddings (Grave et al., 2018). The same embeddings Figure 3: Pearson correlations across languages for different encoders. (ii) Will encoder performances correlate across languages? For each encoder e, we correlate performances of e between en and the other languages on 5 (for ka) and 7 (for tr, ru) probing tasks (using 10k dataset size and LR for all involved languages, including en). In Figure 3, we observe that correlations between en and other languages are generally either zero or weakly positive. Only average embeddings have more than 1 positive correlation scores across the 3 language combinations with en. Among lo"
2020.conll-1.8,N18-1108,0,0.0227764,"to identify. We adopt Voice (en, tr, ru, ka) from Krasnowska-Kiera´s and Wr´oblewska (2019). For en, we additionally evaluate on TreeDepth and TopConstituents as hard syntactic tasks. We add two tasks not present in the canon of probing tasks given in SentEval: Subject-VerbAgreement (SV-Agree) (en, tr, ru, ka) and Subject-Verb-Distance (SV-Dist) (en, tr, ru). We probe representations for these properties because we suspect that agreement between subject and verb is a difficult task which requires inferring a relationship between pairs of words which may stand in a long-distance relationship (Gulordava et al., 2018). Moverover, we assume this task to be particularly hard in morphologically rich and word-order free languages, thus it could be a good predictor for performance in downstream tasks. To implement the probing tasks, for en, we use the probing tasks datasets defined in Conneau and Kiela (2018) and we apply spaCy4 to sentences extracted from Wikipedia for the newly added probing tasks Voice and SV-Agree. For tr, ru, and ka, we do not rely on dependency parsers because of quality issues and unavailability for ka. Instead, for trand ru, we use information from Universal Dependencies (UD) (Nivre et"
2020.conll-1.8,D19-1275,0,0.0216225,"below those of English (Straka, 2018). 108 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 108–118 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 required for obtaining reliable probing task results. This question is also relevant for English: on the one hand, Conneau et al. (2018) claim that training data for a probing task should be plentiful, as otherwise (highly parametrized) classifiers on top of representations may be unable to extract the relevant information signals; on the other hand, Hewitt and Liang (2019) note that a sufficiently powerful classifier with enough training data can in principle learn any task, without this necessarily allowing to conclude that the representations adequately store the linguistic signal under scrutiny. Second, we ask how stable probing task results are across different classifiers (e.g., MLP vs. Naive Bayes). This question is closely related to the question about size, since different classifiers have different sensitivities to data size; especially deep models are claimed to require more training data. We evaluate the sensitivity of probing task results to the two"
2020.conll-1.8,L18-1293,0,0.0286557,"Missing"
2020.conll-1.8,W16-2512,0,0.0531134,"Missing"
2020.conll-1.8,P19-1573,0,0.222202,"Missing"
2020.conll-1.8,W18-6011,0,0.0216002,"robing is typically either executed on type/token (word) (Tenney et al., 2019) or sentence level (Adi et al., 2017). For sentence level evaluation, SentEval thus far only includes en data. Each probing task in SentEval is balanced and has 100k train, 10k dev, and 10k test instances. The effects of these design choices are unclear, which is why our work addresses their influence systematically. In the multilingual setting, Sahin et al. (2019) propose 15 token and type level probing tasks. Their probing task data is sourced from UniMorph 2.0 (Kirov et al., 2018), Universal Dependency treebanks (McCarthy et al., 2018) and Wikipedia word frequency lists. To deal with lower-resourced languages, they only use 10K samples per probing task/language pair (7K/2K/1K for train/dev/test) and exclude task/language pairs for which this amount cannot be generated. Their final experi2 These questions are important because they indicate whether or not probing tasks (and their relation 109 Code and data are available from https://github. com/UKPLab/conll2020-multilingualsentence-probing. ments are carried out on five languages (Finnish, German, Spanish, ru, tr), for which enough training data is available. They find that"
2020.conll-1.8,L16-1262,0,0.0872553,"Missing"
2020.conll-1.8,W19-4318,0,0.06881,"e to probe sentence embeddings for the linguistic information signals they contain (Perone et al., 2018), as this may not be clear from their performances in downstream tasks. Such probes are linguistic micro tasks—like detecting the length of a sentence or its dependency tree depth—that have to be solved by a classifier using given representations. The majority of approaches for probing sentence embeddings target English, but recently some works have also addressed other languages such as Polish, Russian, or Spanish in a multiand cross-lingual setup (Krasnowska-Kiera´s and Wr´oblewska, 2019; Ravishankar et al., 2019). Motivations for considering a multi-lingual analysis include knowing whether findings from English transfer to other languages and determining a universal set of probing tasks that suits multiple languages, e.g., with richer morphology and freer word order. Our work is also inspired by probing sentence encoders in multiple (particularly low-resource) languages. We are especially interested in the formal structure of probing task design in this context. Namely, when designing probing tasks for lowresource languages, some questions arise naturally that are less critical in English. One of them"
2020.conll-1.8,D19-1410,1,0.836765,"have to be re-evaluated in languages other than en.2 2 Related work Our goal is to probe for sentence-level linguistic knowledge encoded in sentence embeddings (Perone et al., 2018) in a multilingual setup which marginalizes out the effects of probing task design choices when comparing sentence representations. Sentence embeddings have become central for representing texts beyond the word level, e.g., in small data scenarios, where it is difficult to induce good higher-level text representations from word embeddings (Subramanian et al., 2018) or for clustering or text retrieval applications (Reimers and Gurevych, 2019). To standardize the comparison of sentence embeddings, Conneau and Kiela (2018) proposed the SentEval framework for evaluating the quality of sentence embeddings on a range of downstream and 10 probing tasks. Probing tasks are used to introspect embeddings for linguistic knowledge, by taking “probes” as dedicated syntactic or semantic micro tasks (K¨ohn, 2016). As opposed to an evaluation in downstream applications or benchmarks like GLUE (Wang et al., 2018), probing tasks target very specific linguistic knowledge which may otherwise be confounded in downstream applications. Since they are ar"
2020.conll-1.8,K18-2004,0,0.0207424,"Missing"
2020.conll-1.8,D18-1402,1,0.765035,"ing and TREC.7 Statistics for all datasets are reported in Table 6. Argument Mining (AM) AM is an emergent NLP task requiring sophisticated reasoning capabilities. We reuse the sentence-level argument (stance) 5 https://clarino.uib.no/gnc http://translate.google.com 7 To estimate the quality of the machine translation, we measured its performance on parallel data. Details can be found in the appendix. While the machine translation is generally of acceptable quality, we cannot exclude the possibility that it may effect some of our downstream tasks results reported below. 6 detection dataset by Stab et al. (2018), which labels sentences extracted from web pages as pro-, con-, or non-arguments for eight different topics. A sentence only qualifies as pro or con argument when it both expresses a stance towards the topic and gives a reason for that stance. The classifier input is a concatenation of the sentence embedding and the topic encoding. In total, there are about 25,000 sentences. Sentiment Analysis As opposed to AM, sentiment analysis only determines the opinion flavor of a statement. Since sentiment analysis is a very established NLP task, we did not machine translate en training data, but used o"
2020.conll-1.8,W18-5446,0,0.060512,"Missing"
2020.deelio-1.5,S17-2001,0,0.0777077,"Missing"
2020.deelio-1.5,2021.ccl-1.108,0,0.158187,"Missing"
2020.deelio-1.5,N19-1423,0,0.14102,"ConceptNet and its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using adapter training. While overall results on the GLUE benchmark paint an inconclusive picture, a deeper analysis reveals that our adapter-based models substantially outperform BERT (up to 15-20 performance points) on inference tasks that require the type of conceptual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https://github.com/ wluper/retrograph. 1 Introduction Self-supervised neural models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019; Liu et al., 2019b), GPT (Radford et al., 2018, 2019), or XLNet (Yang et al., 2019) have rendered language modeling a very suitable pretraining task for learning language representations that are useful for a wide range of language understanding tasks (Wang et al., 2018, 2019). Although shown versatile w.r.t. the types of knowledge (Rogers et al., 2020) they encode, much like their predecessors – static word embedding models (Mikolov et al., 2013; Pennington et al., 2014) – neural LMs still only “consume” 43 Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Ext"
2020.deelio-1.5,P10-1023,0,0.0240196,"s Anne Lauscher♣ Olga Majewska♠ Leonardo F. R. Ribeiro♦ Iryna Gurevych♦ Nikolai Rozanov♠ Goran Glavaˇs♣ ♣ Data and Web Science Group, University of Mannheim, Germany ♠ Wluper, London, United Kingdom ♦ Ubiquitous Knowledge Processing (UKP) Lab, TU Darmstadt, Germany {anne,goran}@informatik.uni-mannheim.de {olga,nikolai}@wluper.com www.ukp.tu-darmstadt.de Abstract the distributional information from large corpora. Yet, a number of structured knowledge sources exist – knowledge bases (KBs) (Suchanek et al., 2007; Auer et al., 2007) and lexico-semantic networks (Miller, 1995; Liu and Singh, 2004; Navigli and Ponzetto, 2010) – encoding many types of knowledge that are underrepresented in text corpora. Starting from this observation, most recent efforts focused on injecting factual (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019) and linguistic knowledge (Lauscher et al., 2019; Peters et al., 2019) into pretrained LMs and demonstrated the usefulness of such knowledge in language understanding tasks (Wang et al., 2018, 2019). Joint pretraining models, on the one hand, augment distributional LM objectives with additional objectives based on external resources (Yu and Dredze, 2014; Nguyen et al., 2016; La"
2020.deelio-1.5,I05-5002,0,0.0284653,"Missing"
2020.deelio-1.5,P16-2074,0,0.029756,"gli and Ponzetto, 2010) – encoding many types of knowledge that are underrepresented in text corpora. Starting from this observation, most recent efforts focused on injecting factual (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019) and linguistic knowledge (Lauscher et al., 2019; Peters et al., 2019) into pretrained LMs and demonstrated the usefulness of such knowledge in language understanding tasks (Wang et al., 2018, 2019). Joint pretraining models, on the one hand, augment distributional LM objectives with additional objectives based on external resources (Yu and Dredze, 2014; Nguyen et al., 2016; Lauscher et al., 2019) and train the extended model from scratch. For models like BERT, this implies computationally expensive retraining from scratch of the encoding transformer network. Post-hoc fine-tuning models (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019), on the other hand, use the objectives based on external resources to fine-tune the encoder’s parameters, pretrained via distributional LM objectives. If the amount of fine-tuning data is substantial, however, this approach may lead to catastrophic forgetting of distributional knowledge obtained in pretraining (Goodfell"
2020.deelio-1.5,D14-1162,0,0.0985503,"der: https://github.com/ wluper/retrograph. 1 Introduction Self-supervised neural models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019; Liu et al., 2019b), GPT (Radford et al., 2018, 2019), or XLNet (Yang et al., 2019) have rendered language modeling a very suitable pretraining task for learning language representations that are useful for a wide range of language understanding tasks (Wang et al., 2018, 2019). Although shown versatile w.r.t. the types of knowledge (Rogers et al., 2020) they encode, much like their predecessors – static word embedding models (Mikolov et al., 2013; Pennington et al., 2014) – neural LMs still only “consume” 43 Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 43–49 c Online, November 19, 2020. 2020 Association for Computational Linguistics cause of this, adapter training preserves the distributional information obtained in LM pretraining, without the need for any distributional (re-)training. While (Wang et al., 2020) inject factual knowledge from Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) into BERT, in this work, we investigate two resources that are commonly assum"
2020.deelio-1.5,N18-1202,0,0.0352532,"th conceptual knowledge from ConceptNet and its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using adapter training. While overall results on the GLUE benchmark paint an inconclusive picture, a deeper analysis reveals that our adapter-based models substantially outperform BERT (up to 15-20 performance points) on inference tasks that require the type of conceptual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https://github.com/ wluper/retrograph. 1 Introduction Self-supervised neural models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019; Liu et al., 2019b), GPT (Radford et al., 2018, 2019), or XLNet (Yang et al., 2019) have rendered language modeling a very suitable pretraining task for learning language representations that are useful for a wide range of language understanding tasks (Wang et al., 2018, 2019). Although shown versatile w.r.t. the types of knowledge (Rogers et al., 2020) they encode, much like their predecessors – static word embedding models (Mikolov et al., 2013; Pennington et al., 2014) – neural LMs still only “consume” 43 Proceedings of Deep Learning Inside Out (DeeLIO): The Firs"
2020.deelio-1.5,D19-1005,0,0.0618754,"Missing"
2020.deelio-1.5,W18-5446,0,0.312602,"a number of structured knowledge sources exist – knowledge bases (KBs) (Suchanek et al., 2007; Auer et al., 2007) and lexico-semantic networks (Miller, 1995; Liu and Singh, 2004; Navigli and Ponzetto, 2010) – encoding many types of knowledge that are underrepresented in text corpora. Starting from this observation, most recent efforts focused on injecting factual (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019) and linguistic knowledge (Lauscher et al., 2019; Peters et al., 2019) into pretrained LMs and demonstrated the usefulness of such knowledge in language understanding tasks (Wang et al., 2018, 2019). Joint pretraining models, on the one hand, augment distributional LM objectives with additional objectives based on external resources (Yu and Dredze, 2014; Nguyen et al., 2016; Lauscher et al., 2019) and train the extended model from scratch. For models like BERT, this implies computationally expensive retraining from scratch of the encoding transformer network. Post-hoc fine-tuning models (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019), on the other hand, use the objectives based on external resources to fine-tune the encoder’s parameters, pretrained via distributional"
2020.deelio-1.5,D16-1264,0,0.0895152,"Missing"
2020.deelio-1.5,N18-1101,0,0.0640444,"Missing"
2020.deelio-1.5,2020.tacl-1.54,0,0.0251937,"ual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https://github.com/ wluper/retrograph. 1 Introduction Self-supervised neural models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019; Liu et al., 2019b), GPT (Radford et al., 2018, 2019), or XLNet (Yang et al., 2019) have rendered language modeling a very suitable pretraining task for learning language representations that are useful for a wide range of language understanding tasks (Wang et al., 2018, 2019). Although shown versatile w.r.t. the types of knowledge (Rogers et al., 2020) they encode, much like their predecessors – static word embedding models (Mikolov et al., 2013; Pennington et al., 2014) – neural LMs still only “consume” 43 Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 43–49 c Online, November 19, 2020. 2020 Association for Computational Linguistics cause of this, adapter training preserves the distributional information obtained in LM pretraining, without the need for any distributional (re-)training. While (Wang et al., 2020) inject factual knowledge from"
2020.deelio-1.5,P14-2089,0,0.0711671,"and Singh, 2004; Navigli and Ponzetto, 2010) – encoding many types of knowledge that are underrepresented in text corpora. Starting from this observation, most recent efforts focused on injecting factual (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019) and linguistic knowledge (Lauscher et al., 2019; Peters et al., 2019) into pretrained LMs and demonstrated the usefulness of such knowledge in language understanding tasks (Wang et al., 2018, 2019). Joint pretraining models, on the one hand, augment distributional LM objectives with additional objectives based on external resources (Yu and Dredze, 2014; Nguyen et al., 2016; Lauscher et al., 2019) and train the extended model from scratch. For models like BERT, this implies computationally expensive retraining from scratch of the encoding transformer network. Post-hoc fine-tuning models (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019), on the other hand, use the objectives based on external resources to fine-tune the encoder’s parameters, pretrained via distributional LM objectives. If the amount of fine-tuning data is substantial, however, this approach may lead to catastrophic forgetting of distributional knowledge obtained in"
2020.deelio-1.5,P19-1139,0,0.0442039,"ed Kingdom ♦ Ubiquitous Knowledge Processing (UKP) Lab, TU Darmstadt, Germany {anne,goran}@informatik.uni-mannheim.de {olga,nikolai}@wluper.com www.ukp.tu-darmstadt.de Abstract the distributional information from large corpora. Yet, a number of structured knowledge sources exist – knowledge bases (KBs) (Suchanek et al., 2007; Auer et al., 2007) and lexico-semantic networks (Miller, 1995; Liu and Singh, 2004; Navigli and Ponzetto, 2010) – encoding many types of knowledge that are underrepresented in text corpora. Starting from this observation, most recent efforts focused on injecting factual (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019) and linguistic knowledge (Lauscher et al., 2019; Peters et al., 2019) into pretrained LMs and demonstrated the usefulness of such knowledge in language understanding tasks (Wang et al., 2018, 2019). Joint pretraining models, on the one hand, augment distributional LM objectives with additional objectives based on external resources (Yu and Dredze, 2014; Nguyen et al., 2016; Lauscher et al., 2019) and train the extended model from scratch. For models like BERT, this implies computationally expensive retraining from scratch of the encoding transformer ne"
2020.deelio-1.5,D13-1170,0,0.00938693,"Missing"
2020.emnlp-demos.7,S17-2001,0,0.0526606,"Missing"
2020.emnlp-demos.7,2020.acl-main.747,0,0.143648,"Missing"
2020.emnlp-demos.7,N19-1423,0,0.0455398,"state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in lowresource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml. 1 Introduction Recent advances in NLP leverage transformerbased language models (Vaswani et al., 2017), pretrained on large amounts of text data (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020). These models are fine-tuned on a target task and achieve state-of-the-art (SotA) performance for most natural language understanding tasks. Their performance has been shown to scale with their size (Kaplan et al., 2020) and recent models have reached ∗ 1 *Equal contribution. https://github.com/huggingface/transformers 46 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 46–54 c November 16-20, 2020. 2020 Association for Computational Linguistics hance transformers with adapter modules that can be combined with existing SotA models with min"
2020.emnlp-demos.7,N16-1181,0,0.0279735,"l´ 2 Aishwarya Kamath , Ivan Vuli´c4 , Sebastian Ruder5 , Kyunghyun Cho2,3 , Iryna Gurevych1 1 Technical University of Darmstadt 2 New York University 3 CIFAR Associate Fellow 4 University of Cambridge 5 DeepMind AdapterHub.ml Abstract billions of parameters (Raffel et al., 2019; Brown et al., 2020). While fine-tuning large pre-trained models on target task data can be done fairly efficiently (Howard and Ruder, 2018), training them for multiple tasks and sharing trained models is often prohibitive. This precludes research on more modular architectures (Shazeer et al., 2017), task composition (Andreas et al., 2016), and injecting biases and external information (e.g., world or linguistic knowledge) into large models (Lauscher et al., 2019; Wang et al., 2020). Adapters (Houlsby et al., 2019) have been introduced as an alternative lightweight fine-tuning strategy that achieves on-par performance to full fine-tuning (Peters et al., 2019) on most tasks. They consist of a small set of additional newly initialized weights at every layer of the transformer. These weights are then trained during fine-tuning, while the pre-trained parameters of the large model are kept frozen/fixed. This enables efficient parame"
2020.emnlp-demos.7,I05-5002,0,0.0149582,"Missing"
2020.emnlp-demos.7,D19-1165,0,0.313909,"e been trained for particular tasks, domains, and languages. This opens up the possibility of building on and combining information from many more sources than was previously possible, and makes research such as intermediate task training (Pruksachatkun et al., 2020), composing information from many tasks (Pfeiffer et al., 2020a), and training models for very low-resource languages (Pfeiffer et al., 2020b) much more accessible. representations in intermediate layers of the pretrained model. Current work predominantly focuses on training adapters for each task separately (Houlsby et al., 2019; Bapna and Firat, 2019; Pfeiffer et al., 2020a,b), which enables parallel training and subsequent combination of the weights. In NLP, adapters have been mainly used within deep transformer-based architectures (Vaswani et al., 2017). At each transformer layer l, a set of adapter parameters Φl is introduced. The placement and architecture of adapter parameters Φ within a pre-trained model is non-trivial and may impact their efficacy: Houlsby et al. (2019) experiment with different adapter architectures, empirically validating that a two-layer feed-forward neural network with a bottleneck works well. While this down-"
2020.emnlp-demos.7,W07-1401,0,0.115017,"Missing"
2020.emnlp-demos.7,2020.acl-main.740,0,0.0195176,"no longer arises. By separating knowledge extraction and composition, adapters mitigate the two most common pitfalls of multi-task learning, catastrophic forgetting and catastrophic interference. Overcoming these problems together with the availability of readily available trained task-specific adapters enables researchers and practitioners to leverage information from specific tasks, domains, or languages that is often more relevant for a specific application—rather than more general pretrained counterparts. Recent work (Howard and Ruder, 2018; Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020) has shown the benefits of such information, which was previously only available by fully fine-tuning a model on the data of interest prior to task-specific fine-tuning. 3 & pr Loa e- di tra ng Θ ined mod ad el ap te Φ’ rs el ers od apt m d ng a di new Θ a Lo ing d Φ ad 1 & 5 Finding adapters 4 2 Training adapters 3 Φ’ Θ,Φ’ Extracting and uploading adapters 6 Inference Figure 1: The AdapterHub Process graph. Adapters Φ are introduced into a pre-trained transformer Θ (step ¬) and are trained (). They can then be extracted and open-sourced (®) and visualized (¯). Pre-trained adapters are downlo"
2020.emnlp-demos.7,2020.acl-main.467,0,0.0786619,"Missing"
2020.emnlp-demos.7,P18-1031,1,0.821346,"tely, the necessity of sampling heuristics due to skewed data set sizes no longer arises. By separating knowledge extraction and composition, adapters mitigate the two most common pitfalls of multi-task learning, catastrophic forgetting and catastrophic interference. Overcoming these problems together with the availability of readily available trained task-specific adapters enables researchers and practitioners to leverage information from specific tasks, domains, or languages that is often more relevant for a specific application—rather than more general pretrained counterparts. Recent work (Howard and Ruder, 2018; Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020) has shown the benefits of such information, which was previously only available by fully fine-tuning a model on the data of interest prior to task-specific fine-tuning. 3 & pr Loa e- di tra ng Θ ined mod ad el ap te Φ’ rs el ers od apt m d ng a di new Θ a Lo ing d Φ ad 1 & 5 Finding adapters 4 2 Training adapters 3 Φ’ Θ,Φ’ Extracting and uploading adapters 6 Inference Figure 1: The AdapterHub Process graph. Adapters Φ are introduced into a pre-trained transformer Θ (step ¬) and are trained (). They can then be extracte"
2020.emnlp-demos.7,D16-1264,0,0.0151158,"Missing"
2020.emnlp-demos.7,N19-5004,1,0.857724,"Missing"
2020.emnlp-demos.7,W19-4302,1,0.887479,"Missing"
2020.emnlp-demos.7,2020.emnlp-main.617,1,0.706042,"Missing"
2020.emnlp-demos.7,D13-1170,0,0.0449199,"020; Pfeiffer et al., 2020a,b). 2.1 Why Adapters? Adapter Architecture Adapters are neural modules with a small amount of additional newly introduced parameters Φ within a large pre-trained model with parameters Θ. The parameters Φ are learnt on a target task while keeping Θ fixed; Φ thus learn to encode task-specific 2 Layer normalization learns to normalize the inputs across the features. This is usually done by introducing a new set of features for mean and variance. 47 Full RTE (Wang et al., 2018) MRPC (Dolan and Brockett, 2005) STS-B (Cer et al., 2017) CoLA (Warstadt et al., 2019) SST-2 (Socher et al., 2013) QNLI (Rajpurkar et al., 2016) MNLI (Williams et al., 2018) QQP (Iyer et al., 2017) 66.2 90.5 88.8 59.5 92.6 91.3 84.1 91.4 Pfeif. Houl. 70.8 89.7 89.0 58.9 92.2 91.3 84.1 90.5 CRate 69.8 91.5 89.2 59.1 92.8 91.2 84.1 90.8 64 16 2 Base #Params Size Large #Params Size 0.2M 0.9M 7.1M 0.9Mb 3.5Mb 28Mb 0.8M 3.1M 25.2M 3.2Mb 13Mb 97Mb Table 2: Number of additional parameters and compressed storage space of the adapter of Pfeiffer et al. (2020a) in (Ro)BERT(a)-Base and Large transformer architectures. The adapter of Houlsby et al. (2019) requires roughly twice as much space. CRate refers to the adap"
2020.emnlp-demos.7,P19-1355,0,0.0848206,"Missing"
2020.emnlp-demos.7,W18-5446,0,0.202668,"scalability, modularity, and composition. We now provide a few use-cases for adapters to illustrate their usefulness in practice. Task-specific Layer-wise Representation Learning. Prior to the introduction of adapters, in order to achieve SotA performance on downstream tasks, the entire pre-trained transformer model needs to be fine-tuned (Peters et al., 2019). Adapters have been shown to work on-par with full fine-tuning, by adapting the representations at every layer. We present the results of fully fine-tuning the model compared to two different adapter architectures on the GLUE benchmark (Wang et al., 2018) in Table 1. The adapters of Houlsby et al. (2019, Figure 3c) and Pfeiffer et al. (2020a, Figure 3b) comprise two and one down- and up-projection Adapters While the predominant methodology for transfer learning is to fine-tune all weights of the pre-trained model, adapters have recently been introduced as an alternative approach, with applications in computer vision (Rebuffi et al., 2017) as well as the NLP domain (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020; Pfeiffer et al., 2020a,b). 2.1 Why Adapters? Adapter Architecture Adapters are neural modules with a small amount of"
2020.emnlp-demos.7,Q19-1040,0,0.0324564,"Missing"
2020.emnlp-demos.7,N18-1101,0,0.0345727,"Missing"
2020.emnlp-main.13,P98-1013,0,0.464337,"mation, having the same textual data annotated with multiple formalisms for the same task is rare. We focus on role semantics – a family of shallow semantic formalisms at the interface between syntax and propositional semantics that assign roles to the participants of natural language utterances, determining who did what to whom, where, when etc. Decades of research in theoretical linguistics have produced a range of rolesemantic frameworks that have been operationalized in NLP: syntax-driven PropBank (Palmer et al., 2005), coarse-grained VerbNet (Kipper-Schuler, 2005), fine-grained FrameNet (Baker et al., 1998), and, recently, decompositional Semantic ProtoRoles (SPR) (Reisinger et al., 2015; White et al., 2016). The SemLink project (Bonial et al., 2013) offers parallel annotation for PropBank, VerbNet and FrameNet for English. This allows us to isolate the object of our study: apart from the rolesemantic labels, the underlying data and conditions for the three formalisms are identical. SR3DE (M´ujdricza-Maydt et al., 2016) provides compatible annotation in three formalisms for German, enabling cross-lingual validation of our results. Combined, these factors make role semantics an ideal target for o"
2020.emnlp-main.13,P17-1080,0,0.0498208,"al., 2019; Lan et al., 2019; Raffel et al., 2019), we use the original BERT architecture, since it allows us to inherit the probing methodology and to build upon the related findings. 2.2 Probing Due to space limitations we omit high-level discussions on benchmarking (Wang et al., 2018) and sentence-level probing (Conneau et al., 2018a), and focus on the recent findings related to the representation of linguistic structure in BERT. Surface-level information generally tends to be represented in the lower layers of deep encoders, while higher layers store hierarchical and semantic information (Belinkov et al., 2017; Lin et al., 2019). Tenney et al. (2019a) show that the abstraction strategy applied by the English pre-trained BERT encoder follows the order of the classical NLP pipeline. Strengthening the claim about linguistic capabilities of BERT, Hewitt and Manning (2019) demonstrate that BERT implicitly learns syntax, and Reif et al. (2019) show that it encodes fine-grained lexicalsemantic distinctions. Rogers et al. (2020) provide a comprehensive overview of BERT’s properties discovered to date. While recent results indicate that BERT successfully represents lexical-semantic and grammatical informati"
2020.emnlp-main.13,W09-1206,0,0.177734,"Missing"
2020.emnlp-main.13,W13-5503,0,0.0250926,"Missing"
2020.emnlp-main.13,burchardt-etal-2006-salsa,0,0.0948319,"Missing"
2020.emnlp-main.13,P18-1198,0,0.168049,"n: the encoding of a sentence or a sentence pair is stored in a special token [CLS]. To facilitate multilingual experiments, we use the multilingual BERT-base (mBERT) published by Devlin et al. (2019). Although several recent encoders have outperformed BERT on benchmarks 172 (Liu et al., 2019; Lan et al., 2019; Raffel et al., 2019), we use the original BERT architecture, since it allows us to inherit the probing methodology and to build upon the related findings. 2.2 Probing Due to space limitations we omit high-level discussions on benchmarking (Wang et al., 2018) and sentence-level probing (Conneau et al., 2018a), and focus on the recent findings related to the representation of linguistic structure in BERT. Surface-level information generally tends to be represented in the lower layers of deep encoders, while higher layers store hierarchical and semantic information (Belinkov et al., 2017; Lin et al., 2019). Tenney et al. (2019a) show that the abstraction strategy applied by the English pre-trained BERT encoder follows the order of the classical NLP pipeline. Strengthening the claim about linguistic capabilities of BERT, Hewitt and Manning (2019) demonstrate that BERT implicitly learns syntax, and"
2020.emnlp-main.13,D18-1269,0,0.141836,"n: the encoding of a sentence or a sentence pair is stored in a special token [CLS]. To facilitate multilingual experiments, we use the multilingual BERT-base (mBERT) published by Devlin et al. (2019). Although several recent encoders have outperformed BERT on benchmarks 172 (Liu et al., 2019; Lan et al., 2019; Raffel et al., 2019), we use the original BERT architecture, since it allows us to inherit the probing methodology and to build upon the related findings. 2.2 Probing Due to space limitations we omit high-level discussions on benchmarking (Wang et al., 2018) and sentence-level probing (Conneau et al., 2018a), and focus on the recent findings related to the representation of linguistic structure in BERT. Surface-level information generally tends to be represented in the lower layers of deep encoders, while higher layers store hierarchical and semantic information (Belinkov et al., 2017; Lin et al., 2019). Tenney et al. (2019a) show that the abstraction strategy applied by the English pre-trained BERT encoder follows the order of the classical NLP pipeline. Strengthening the claim about linguistic capabilities of BERT, Hewitt and Manning (2019) demonstrate that BERT implicitly learns syntax, and"
2020.emnlp-main.13,D19-1275,0,0.0280959,"unified manner, facilitating the cross-task comparison. The original setup has several limitations that we address in our implementation. Regression tasks. The original edge probing setup only considers classification tasks. Many language phenomena - including positional information and semantic proto-roles, are naturally modeled as regression. We extend the architecture by Tenney et al. (2019b) and support both classification and regression: the former achieved via softmax, the latter via direct linear regression to the target value. Flat model. To decrease the models’ own expressive power (Hewitt and Liang, 2019), we keep the number of parameters in our probing model as low as possible. While Tenney et al. (2019b) utilize pooled self-attentional span representations and a projection layer to enable cross-model comparison, we directly feed the wordpiece encoding into the classifier, using the first wordpiece of a word. To further increase the selectivity of the model, we directly project the source and target wordpiece representations into the label space, opposed to the two-layer MLP classifier used in the original setup. Separate scalar mixes. To enable fine-grained analysis of probing results, we tr"
2020.emnlp-main.13,N19-1423,0,0.201618,"vestigated along with the commonly used cross-task and cross-lingual experimental settings. 1 L=0 L=8 L=11 Figure 1: Intra-sentence similarity by layer L of the multilingual BERT-base. Functional tokens are similar in L = 0, syntactic groups emerge at higher levels. Introduction The emergence of deep pre-trained contextualized encoders has had a major impact on the field of natural language processing. Boosted by the availability of general-purpose frameworks like AllenNLP (Gardner et al., 2018) and Transformers (Wolf et al., 2019), pre-trained models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have caused a shift towards simple architectures where a strong pre-trained encoder is paired with a shallow downstream model, often outperforming the intricate task-specific architectures of the past. The versatility of pre-trained representations implies that they encode some aspects of general linguistic knowledge (Reif et al., 2019). Indeed, even an informal inspection of layer-wise intrasentence similarities (Fig. 1) suggests that these models capture elements of linguistic structure, and those differ depending on the layer of the model. A grounded investigation of these regularities all"
2020.emnlp-main.13,N19-1419,0,0.0416661,"chmarking (Wang et al., 2018) and sentence-level probing (Conneau et al., 2018a), and focus on the recent findings related to the representation of linguistic structure in BERT. Surface-level information generally tends to be represented in the lower layers of deep encoders, while higher layers store hierarchical and semantic information (Belinkov et al., 2017; Lin et al., 2019). Tenney et al. (2019a) show that the abstraction strategy applied by the English pre-trained BERT encoder follows the order of the classical NLP pipeline. Strengthening the claim about linguistic capabilities of BERT, Hewitt and Manning (2019) demonstrate that BERT implicitly learns syntax, and Reif et al. (2019) show that it encodes fine-grained lexicalsemantic distinctions. Rogers et al. (2020) provide a comprehensive overview of BERT’s properties discovered to date. While recent results indicate that BERT successfully represents lexical-semantic and grammatical information, the evidence of its high-level semantic capabilities is inconclusive. Tenney et al. (2019a) show that the English PropBank semantics can be extracted from the encoder and follows syntax in the layer structure. However, out of all formalisms PropBank is most c"
2020.emnlp-main.13,W18-2501,0,0.024534,"ic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies and should be investigated along with the commonly used cross-task and cross-lingual experimental settings. 1 L=0 L=8 L=11 Figure 1: Intra-sentence similarity by layer L of the multilingual BERT-base. Functional tokens are similar in L = 0, syntactic groups emerge at higher levels. Introduction The emergence of deep pre-trained contextualized encoders has had a major impact on the field of natural language processing. Boosted by the availability of general-purpose frameworks like AllenNLP (Gardner et al., 2018) and Transformers (Wolf et al., 2019), pre-trained models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have caused a shift towards simple architectures where a strong pre-trained encoder is paired with a shallow downstream model, often outperforming the intricate task-specific architectures of the past. The versatility of pre-trained representations implies that they encode some aspects of general linguistic knowledge (Reif et al., 2019). Indeed, even an informal inspection of layer-wise intrasentence similarities (Fig. 1) suggests that these models capture elements of lingui"
2020.emnlp-main.13,P06-1117,0,0.0276991,"core and peripheral participants. SPR follows the work of Dowty (1991) and discards the notion of categorical semantic roles in favor of feature bundles. Instead of a fixed role label, each argument is assessed via a 11-dimensional cardinal feature set including Proto-Agent and Proto-Patient properties like volitional, sentient, destroyed, etc. The feature-based approach eliminates some of the theoretical issues associated with categorical role inventories and allows for more flexible modeling of role semantics. Each of the role labeling formalisms offers certain advantages and disadvantages (Giuglea and Moschitti, 2006; M´ujdricza-Maydt et al., 2016). While being close to syntax and thereby easier to predict, PropBank doesn’t contribute much semantics to the representation. On the opposite side of the spectrum, FrameNet offers rich predicatesemantic representations for verbs and nouns, but suffers from high granularity and coverage gaps (Hartmann et al., 2017). VerbNet takes a middle ground by following grammatical criteria while still encoding coarse-grained semantics, but only focuses on verbs and core (not modifier) roles. SPR avoids the granularity-generalization trade-off of the categorical inventories"
2020.emnlp-main.13,E17-1045,1,0.832327,"ature-based approach eliminates some of the theoretical issues associated with categorical role inventories and allows for more flexible modeling of role semantics. Each of the role labeling formalisms offers certain advantages and disadvantages (Giuglea and Moschitti, 2006; M´ujdricza-Maydt et al., 2016). While being close to syntax and thereby easier to predict, PropBank doesn’t contribute much semantics to the representation. On the opposite side of the spectrum, FrameNet offers rich predicatesemantic representations for verbs and nouns, but suffers from high granularity and coverage gaps (Hartmann et al., 2017). VerbNet takes a middle ground by following grammatical criteria while still encoding coarse-grained semantics, but only focuses on verbs and core (not modifier) roles. SPR avoids the granularity-generalization trade-off of the categorical inventories, but is yet to find its way into practical NLP applications. 3 Probing Methodology We take the edge probing setup by Tenney et al. (2019b) as our starting point. Edge probing aims to predict a label given a pair of contextualized span or word encodings. More formally, we encode a WP-tokenized sentence [wp1 , wp2 , ...wpk ] with a frozen pre-trai"
2020.emnlp-main.13,P19-1356,0,0.0196625,"we define the tasks in order of their complexity, and flat model architecture, the total runtime of Table 2 provides the probing task statistics, Table 3 the main experiments is under 8 hours on a sincompares the categorical role labeling formalisms gle Tesla V100 GPU. In addition to pre-trained in terms of granularity, and Table 4 provides exammBERT we report baseline performance using a ples. We evaluate the classification performance frozen untrained mBERT model obtained by ranusing Accuracy, while regression tasks are scored domizing the encoder weights post-initialization as via R2 . in Jawahar et al. (2019). Token type (ttype) predicts the type of a word. 5.1 General Trends This requires contextual processing since a word might consist of several wordpieces; While absolute performance is secondary to our Token position (token.ix) predicts the linear analysis, we report the probing task scores on reposition of a word, cast as a regression task over spective development sets in Table 5. We observe the first 20 words in the sentence. Again, the task that grammatical tasks score high, while core role is non-trivial since it requires the words to be as- labeling lags behind - in line with the finding"
2020.emnlp-main.13,D19-1279,0,0.0273936,"l tasks over 20 epochs, Acc for classification, R2 for regression; Baseline in parentheses. de [4.61] [5.2] [5.09] [5.75] [6.01] [5.99] [5.18] [5.24] [5.13] [6.12] [6.06] [5.75] [6.15] Layer Layer Figure 2: Layer probing results en 5.2 The Effect of Formalism Using separate scalar mixes for source and target tokens allows us to explore the cross-formalism encoding of role semantics by mBERT in detail. For both English and German role labeling, the probe’s layer utilization drastically differs for predicate and 4 Echoing the recent findings on mBERT’s multilingual capacity (Pires et al., 2019; Kondratyuk and Straka, 2019). ttype lex.unit pos deprel src fn src vn src pb src fn tgt vn tgt pb tgt fn src vn src deprel tgt pb src learning to locate relevant role-semantic information in mBERT’s layers. The untrained mBERT baseline expectedly underperforms; however, we note good baseline results on surface-level tasks for English, which we attribute to memorizing token identity and position: although the weights are set randomly, the frozen encoder still associates each wordpiece input with a fixed random vector. We have confirmed this assumption by scalar mix analysis of the untrained mBERT baseline: in our experime"
2020.emnlp-main.13,D19-1445,0,0.100637,"solate the object of our study: apart from the rolesemantic labels, the underlying data and conditions for the three formalisms are identical. SR3DE (M´ujdricza-Maydt et al., 2016) provides compatible annotation in three formalisms for German, enabling cross-lingual validation of our results. Combined, these factors make role semantics an ideal target for our cross-formalism probing study. A solid body of evidence suggests that encoders like BERT capture syntactic and lexical-semantic properties, but only few studies have considered probing for predicate-level semantics (Tenney et al., 2019b; Kovaleva et al., 2019). To the best of our knowledge we are the first to conduct a crossformalism probing study on role semantics, thereby contributing to the line of research on how and whether pre-trained BERT encodes higher-level semantic phenomena. Contributions. This work studies the effect of the linguistic formalism on probing results. We conduct cross-formalism experiments on PropBank, VerbNet and FrameNet role prediction in English and German, and show that the formalism can affect probing results in a linguistically meaningful way; in addition, we demonstrate that layer probing can detect subtle differenc"
2020.emnlp-main.13,W19-4825,0,0.0392193,"Missing"
2020.emnlp-main.13,2021.ccl-1.108,0,0.0655123,"Missing"
2020.emnlp-main.13,W08-1301,0,0.215796,"Missing"
2020.emnlp-main.13,L16-1484,1,0.903618,"Missing"
2020.emnlp-main.13,J05-1004,0,0.170801,"main and text type. While many linguistic corpora contain several layers of linguistic information, having the same textual data annotated with multiple formalisms for the same task is rare. We focus on role semantics – a family of shallow semantic formalisms at the interface between syntax and propositional semantics that assign roles to the participants of natural language utterances, determining who did what to whom, where, when etc. Decades of research in theoretical linguistics have produced a range of rolesemantic frameworks that have been operationalized in NLP: syntax-driven PropBank (Palmer et al., 2005), coarse-grained VerbNet (Kipper-Schuler, 2005), fine-grained FrameNet (Baker et al., 1998), and, recently, decompositional Semantic ProtoRoles (SPR) (Reisinger et al., 2015; White et al., 2016). The SemLink project (Bonial et al., 2013) offers parallel annotation for PropBank, VerbNet and FrameNet for English. This allows us to isolate the object of our study: apart from the rolesemantic labels, the underlying data and conditions for the three formalisms are identical. SR3DE (M´ujdricza-Maydt et al., 2016) provides compatible annotation in three formalisms for German, enabling cross-lingual v"
2020.emnlp-main.13,N18-1202,0,0.41464,"robing studies and should be investigated along with the commonly used cross-task and cross-lingual experimental settings. 1 L=0 L=8 L=11 Figure 1: Intra-sentence similarity by layer L of the multilingual BERT-base. Functional tokens are similar in L = 0, syntactic groups emerge at higher levels. Introduction The emergence of deep pre-trained contextualized encoders has had a major impact on the field of natural language processing. Boosted by the availability of general-purpose frameworks like AllenNLP (Gardner et al., 2018) and Transformers (Wolf et al., 2019), pre-trained models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have caused a shift towards simple architectures where a strong pre-trained encoder is paired with a shallow downstream model, often outperforming the intricate task-specific architectures of the past. The versatility of pre-trained representations implies that they encode some aspects of general linguistic knowledge (Reif et al., 2019). Indeed, even an informal inspection of layer-wise intrasentence similarities (Fig. 1) suggests that these models capture elements of linguistic structure, and those differ depending on the layer of the model. A grounded investig"
2020.emnlp-main.13,P19-1493,0,0.0213067,"score for word-level tasks over 20 epochs, Acc for classification, R2 for regression; Baseline in parentheses. de [4.61] [5.2] [5.09] [5.75] [6.01] [5.99] [5.18] [5.24] [5.13] [6.12] [6.06] [5.75] [6.15] Layer Layer Figure 2: Layer probing results en 5.2 The Effect of Formalism Using separate scalar mixes for source and target tokens allows us to explore the cross-formalism encoding of role semantics by mBERT in detail. For both English and German role labeling, the probe’s layer utilization drastically differs for predicate and 4 Echoing the recent findings on mBERT’s multilingual capacity (Pires et al., 2019; Kondratyuk and Straka, 2019). ttype lex.unit pos deprel src fn src vn src pb src fn tgt vn tgt pb tgt fn src vn src deprel tgt pb src learning to locate relevant role-semantic information in mBERT’s layers. The untrained mBERT baseline expectedly underperforms; however, we note good baseline results on surface-level tasks for English, which we attribute to memorizing token identity and position: although the weights are set randomly, the frozen encoder still associates each wordpiece input with a fixed random vector. We have confirmed this assumption by scalar mix analysis of the untrained m"
2020.emnlp-main.13,Q15-1034,0,0.168247,"Missing"
2020.emnlp-main.13,2020.tacl-1.54,0,0.0617573,"f general linguistic knowledge (Reif et al., 2019). Indeed, even an informal inspection of layer-wise intrasentence similarities (Fig. 1) suggests that these models capture elements of linguistic structure, and those differ depending on the layer of the model. A grounded investigation of these regularities allows to interpret the model’s behaviour, design better pre-trained encoders and inform the downstream model development. Such investigation is the main subject of probing, and recent studies confirm that BERT implicitly captures many aspects of language use, lexical semantics and grammar (Rogers et al., 2020). Most probing studies use linguistics as a theoretical scaffolding and operate on a task level. However, there often exist multiple ways to represent the same linguistic phenomenon: for example, English dependency syntax can be encoded using a variety of formalisms, incl. Universal (Schuster and Manning, 2016), Stanford (de Marneffe and Manning, 2008) and CoNLL-2009 dependencies (Hajiˇc et al., 2009), all using different label sets and syntactic head attachment rules. Any probing study inevitably commits to the specific theoretical framework used to produce the underlying data. The difference"
2020.emnlp-main.13,L16-1376,0,0.0560879,"Missing"
2020.emnlp-main.13,P19-1452,0,0.067151,"h. This allows us to isolate the object of our study: apart from the rolesemantic labels, the underlying data and conditions for the three formalisms are identical. SR3DE (M´ujdricza-Maydt et al., 2016) provides compatible annotation in three formalisms for German, enabling cross-lingual validation of our results. Combined, these factors make role semantics an ideal target for our cross-formalism probing study. A solid body of evidence suggests that encoders like BERT capture syntactic and lexical-semantic properties, but only few studies have considered probing for predicate-level semantics (Tenney et al., 2019b; Kovaleva et al., 2019). To the best of our knowledge we are the first to conduct a crossformalism probing study on role semantics, thereby contributing to the line of research on how and whether pre-trained BERT encodes higher-level semantic phenomena. Contributions. This work studies the effect of the linguistic formalism on probing results. We conduct cross-formalism experiments on PropBank, VerbNet and FrameNet role prediction in English and German, and show that the formalism can affect probing results in a linguistically meaningful way; in addition, we demonstrate that layer probing ca"
2020.emnlp-main.13,W18-5446,0,0.0835763,"Missing"
2020.emnlp-main.13,D16-1177,0,0.050208,"Missing"
2020.emnlp-main.13,C98-1013,0,\N,Missing
2020.emnlp-main.194,D19-5801,0,0.0427692,"they generalize to unseen settings. Previous work of Yang et al. (2020) investigates this on a smaller scale. They propose USE-QA, a sentence encoder for comparing questions and 2472 answers, and achieve promising zero-shot results in retrieval tasks. However, it is unclear how this model compares to the zero-shot performances of models trained on several different source domains and how best to combine the data from multiple domains. Other work addresses the generalization of models over several domains in different settings, e.g., for machine reading comprehension (Talmor and Berant, 2019; Fisch et al., 2019). More related to our work, Guo et al. (2020) propose a new evaluation suite with eight datasets for retrieval-based QA, in which they also study the effectiveness of USEQA. In contrast to them, our work (1) deals with re-ranking setups and uses cross-encoders, which is different to their bi-encoder scenario for retrieval; (2) we deal with question and answer passages instead of answer sentences; (3) we study a large number of 140 source domains and provide important insights on zero-shot transfer performances in relation to domain similarity and data size, and extensively analyze the training"
2020.emnlp-main.194,D19-1601,0,0.0106454,"76.8 WPQA SemEval MAP scores Σ Zero-Shot Transfer USE-QA MultiCQAB MultiCQAB-lg MultiCQARBa-lg 65.3 72.4 75.5 77.8 In-Domain Models Previous SoTA BERT BERT-lg RoBERTa-lg MultiCQARBa-lg 69.5† 68.7 72.5 70.9 80.5 Table 4: The results of zero-shot transfer and in-domain models. The first five columns are LAS-Travel, Cooking, Apple, Academia, and Aviation. AU is AskUbuntu, IQA is InsuranceQA, and WPQA is WikiPassageQA. Σ shows the average performance of benchmarks that use the same performance measure. † shows the scores of the best BERT models of (R¨uckl´e et al., 2019b), ‡ is the MICRON model (Han et al., 2019), ? is the BERT model in (Ma et al., 2019), and  is MV-DASE (Poerner and Sch¨utze, 2019). Query question: Passing parameters to the installer for 14.04? The installer for 14.04 gave me no chance (that I took notice of) to pass parameters [...] Most similar (MultiCQARBa-lg ): Which key combination would allow me to pass parameters to kernel? During boot I want to pass some parameters like the runlevel , nomodeset to kernel during the booting process [...] Ground truth: How can i customize the Ubuntu installer? I would like to know how can I customize the Ubuntu installer not customize Ubuntu ,"
2020.emnlp-main.194,D19-1410,1,0.831669,"1 SE Aviation 0.675 0.650 0.625 140 120 100 80 60 40 20 1 Included Models 0.480 140 120 100 80 60 40 20 1 Included Models Figure 3: The average performance scores (y-axis) of subsets of models (x-axis) selected by domain similarity or training size (scores are averaged over the included models). The oracle always selects the best models. performances. To simulate an optimal selection, we define an oracle that always identifies the best models. We present our findings in Figure 3. Domain similarity. To measure the domain similarity, we embed the questions of all datasets with Sentence-RoBERTa (Reimers and Gurevych, 2019). For each dataset, we obtain the mean over all embeddings and calculate the domain similarity to other datasets with cosine similarity. Domain similarity is most effective when selecting models for benchmarks of technical domains, e.g., AskUbuntu, LAS-Apple, and LAS-Aviation in Figure 3. However, this does not hold true for benchmarks of non-technical domains such as LASTravel or WikiPassageQA. In those cases, only considering the most similar source domains does not improve the average model performance. One reason might be that there do not exist many similar non-technical domains within St"
2020.emnlp-main.194,W17-6935,1,0.90552,"Missing"
2020.emnlp-main.194,D19-1171,1,0.890737,"Missing"
2020.emnlp-main.194,D18-1131,0,0.362456,"tups of non-factoid answer selection (Cohen et al., 2018; Tay et al., 2017; Feng et al., 2015; Verberne et al., 2010) and question similarity (Nakov et al., 2017; Lei et al., 2016). These tasks compare questions and answers, or two potentially related questions in community question answering (cQA) forums, FAQ pages, and general collections of text passages. In contrast to other text matching tasks in NLP, they compare texts of different lengths—e.g., answers can be long explanations or descriptions—and often deal with expert domains. This makes it difficult to transfer models across domains (Shah et al., 2018), and to apply common approaches such as universal sentence embeddings without further domain or task adaptations (Poerner and Sch¨utze, 2019). Non-factoid answer selection and question similarity are also particularly promising to study zeroshot transfer. Reasons are that (1) there exist a large number of domains, and (2) in-domain training data is often scarce. Previous work proposed domain adaptation techniques (Poerner and Sch¨utze, 2019; Shah et al., 2018), training with unlabeled data (R¨uckl´e et al., 2019b), and shallow architectures (R¨uckl´e et al., 2019a). However, these approaches"
2020.emnlp-main.194,P19-1485,0,0.0238043,"s, to understand how well they generalize to unseen settings. Previous work of Yang et al. (2020) investigates this on a smaller scale. They propose USE-QA, a sentence encoder for comparing questions and 2472 answers, and achieve promising zero-shot results in retrieval tasks. However, it is unclear how this model compares to the zero-shot performances of models trained on several different source domains and how best to combine the data from multiple domains. Other work addresses the generalization of models over several domains in different settings, e.g., for machine reading comprehension (Talmor and Berant, 2019; Fisch et al., 2019). More related to our work, Guo et al. (2020) propose a new evaluation suite with eight datasets for retrieval-based QA, in which they also study the effectiveness of USEQA. In contrast to them, our work (1) deals with re-ranking setups and uses cross-encoders, which is different to their bi-encoder scenario for retrieval; (2) we deal with question and answer passages instead of answer sentences; (3) we study a large number of 140 source domains and provide important insights on zero-shot transfer performances in relation to domain similarity and data size, and extensively"
2020.emnlp-main.194,P16-1044,0,0.0286078,"source code and the weights of our best multi-task model is publicly available.1 Additionally, all 140 source domain adapters are available 10 6 10 4 1 20 40 60 80 100 StackExchange forums 120 140 Figure 1: Number of questions in StackExchange forums (log scale) that can be used for self-supervision. at AdapterHub.ml (Pfeiffer et al., 2020b). 2 Related Work The predominant method for text matching tasks such as non-factoid answer selection and question similarity is to train a neural architecture on a large quantity of labeled in-domain data. This includes CNN and LSTM models with attention (Tan et al., 2016; Dos Santos et al., 2016; Wang et al., 2016; R¨uckl´e and Gurevych, 2017), compare-aggregate approaches (Wang and Jiang, 2017; R¨uckl´e et al., 2019a), and, more recently, transformer-based models (Hashemi et al., 2020; Yang et al., 2020; Mass et al., 2019). Fine-tuning of large pre-trained transformers such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) currently achieves stateof-the-art performances on many related benchmarks (Garg et al., 2020; Mass et al., 2019; Rochette et al., 2019; Nogueira and Cho, 2019). However, realistic scenarios often do not provide enough labeled d"
2020.emnlp-main.194,P16-1122,0,0.0312442,"lti-task model is publicly available.1 Additionally, all 140 source domain adapters are available 10 6 10 4 1 20 40 60 80 100 StackExchange forums 120 140 Figure 1: Number of questions in StackExchange forums (log scale) that can be used for self-supervision. at AdapterHub.ml (Pfeiffer et al., 2020b). 2 Related Work The predominant method for text matching tasks such as non-factoid answer selection and question similarity is to train a neural architecture on a large quantity of labeled in-domain data. This includes CNN and LSTM models with attention (Tan et al., 2016; Dos Santos et al., 2016; Wang et al., 2016; R¨uckl´e and Gurevych, 2017), compare-aggregate approaches (Wang and Jiang, 2017; R¨uckl´e et al., 2019a), and, more recently, transformer-based models (Hashemi et al., 2020; Yang et al., 2020; Mass et al., 2019). Fine-tuning of large pre-trained transformers such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) currently achieves stateof-the-art performances on many related benchmarks (Garg et al., 2020; Mass et al., 2019; Rochette et al., 2019; Nogueira and Cho, 2019). However, realistic scenarios often do not provide enough labeled data for supervised in-domain training. Thus,"
2020.emnlp-main.194,2020.acl-demos.12,0,0.577542,"e also particularly promising to study zeroshot transfer. Reasons are that (1) there exist a large number of domains, and (2) in-domain training data is often scarce. Previous work proposed domain adaptation techniques (Poerner and Sch¨utze, 2019; Shah et al., 2018), training with unlabeled data (R¨uckl´e et al., 2019b), and shallow architectures (R¨uckl´e et al., 2019a). However, these approaches result in entirely separate models that are specialized to individual target domains. One model that is re-usable and targets zero-shot transfer in similar settings is the question-answer encoder of Yang et al. (2020), which has recently been evaluated in cross-domain settings for efficient answer sentence retrieval (Guo et al., 2020). However, they do not study zero-shot transfer with a large number of source domains, and they do not assess how to best combine them. In this work, we address these limitations and are—to the best of our knowledge—the first to study the zero-shot transfer capabilities of re-usable text matching models with a large number of source domains in these challenging setups. In the first part, we investigate the zero-shot transfer capabilities of 140 domain-specific text matching mo"
2020.emnlp-main.365,D15-1075,0,0.362604,"languages. The presented approach has various advantages compared to other training approaches for multilingual sentence embeddings. LASER (Artetxe and Schwenk, 2019b) trains an encoder-decoder LSTM model using a translation task. The output of the encoder is used as sentence embedding. While LASER works well for identifying exact translations in different languages, it works less well for assessing the similarity of sentences that are not exact translations. Multilingual Universal Sentence Encoder (mUSE) (Chidambaram et al., 2019; Yang et al., 2019) was trained in a multi-task setup on SNLI (Bowman et al., 2015) and on over a billion question-answer pairs from popular online forums and QA websites. In order to align the crosslingual vector spaces, mUSE used a translation ranking task. Given a translation pair (si , ti ) and various alternative (incorrect) translations, identify the correct translation. First, multi-task learning is difficult since it can suffer from catastrophic forgetting and balancing multiple 4512 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4512–4525, c November 16–20, 2020. 2020 Association for Computational Linguistics Teacher EN"
2020.emnlp-main.365,S17-2001,0,0.345998,"tor space for English) as well as on the properties this source vector space has. Differences in performance can then be due to a better or worse alignment between the languages or due to different properties of the (source) vector space. We evaluate the following systems: SBERT-nli-stsb: The output of the BERT-base model is combined with mean pooling to create a fixed-sized sentence representation (Reimers and Gurevych, 2019). It was fine-tuned on the English AllNLI (SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018)) dataset and on the English training set of the STS benchmark (Cer et al., 2017) using a siamese network structure. 5 6 4514 bert-base-nli-stsb-mean-tokens model from our repository mBERT / XLM-R mean: Mean pooling of the outputs for the pre-trained multilingual BERT (mBERT) and XLM-R model. These models are pre-trained on multilingual data and have a multilingual vocabulary. However, no parallel data was used. mBERT- / XLM-R-nli-stsb: We fine-tuned XLM-R and mBERT on the (English) AllNLI and the (English) training set of the STS benchmark. LASER: LASER (Artetxe and Schwenk, 2019b) uses max-pooling over the output of a stacked LSTM-encoder. The encoder was trained in an e"
2020.emnlp-main.365,D18-2029,0,0.0650901,"Missing"
2020.emnlp-main.365,W19-4330,0,0.629976,"original source language from the teacher model M are adopted and transferred to other languages. The presented approach has various advantages compared to other training approaches for multilingual sentence embeddings. LASER (Artetxe and Schwenk, 2019b) trains an encoder-decoder LSTM model using a translation task. The output of the encoder is used as sentence embedding. While LASER works well for identifying exact translations in different languages, it works less well for assessing the similarity of sentences that are not exact translations. Multilingual Universal Sentence Encoder (mUSE) (Chidambaram et al., 2019; Yang et al., 2019) was trained in a multi-task setup on SNLI (Bowman et al., 2015) and on over a billion question-answer pairs from popular online forums and QA websites. In order to align the crosslingual vector spaces, mUSE used a translation ranking task. Given a translation pair (si , ti ) and various alternative (incorrect) translations, identify the correct translation. First, multi-task learning is difficult since it can suffer from catastrophic forgetting and balancing multiple 4512 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4512–452"
2020.emnlp-main.365,P19-4007,0,0.0545399,"Missing"
2020.emnlp-main.365,D17-1070,0,0.261798,"e European Parliament website (Koehn, 2005). • JW300: Mined, parallel sentences from the magazines Awake! and Watchtower (Agi´c and Vuli´c, 2019). • OpenSubtitles2018: Translated movie subtitles from opensubtitles.org (Lison and Tiedemann, 2016). • UNPC: Manually translated United Nations documents from 1994 - 2014 (Ziemski et al., 2016). Getting parallel sentence data can be challenging for some low-resource language pairs. Hence, we also experiment with bilingual dictionaries: • MUSE: MUSE5 provides 110 large-scale ground-truth bilingual dictionaries created by an internal translation tool (Conneau et al., 2017b). • Wikititles: We use the Wikipedia database dumps to extract the article titles from crosslanguage links between Wikipedia articles. For example, the page ”United States” links to the German page ”Vereinigte Staaten”. This gives a dictionary covering a wide range of topics. 4 https://tatoeba.org/ https://github.com/facebookresearch/ MUSE The data set sizes for English-German (EN-DE) and English-Arabic (EN-AR) are depicted in Table 5. For training, we balance the data set sizes by drawing for a mini batch roughly the same number of samples from each data set. Data from smaller data sets is"
2020.emnlp-main.365,D18-1269,0,0.106737,"l prefers one language or language pair over others. For example, a model would have a language bias if it maps sentences in the same language closer in vector space just because they are of the same language. Language bias can be an issue if the task involves a multilingual sentence pool: certain language pairs might get discriminated, potentially harming the overall performance for multilingual sentence pools. Figure 2 shows the plot of the first two principle components for different multi-lingual sentence embeddings methods. In the plot, we encoded the English premise sentences from XNLI (Conneau et al., 2018) with their Russian translation. The plot shows for the LaBSE model a drastic separation between the two languages, indicating that the language significantly impacts the resulting embedding vector. The experiments in Section 4 used so far monolingual sentence pools, i.e., all sentences in the source / target pool were of the same language. Hence, these benchmarks are not suited to measure a potential harmful effect from language bias. In order to measure a potential negative effect from language bias, we combine all sentence pairs from the multilingual STS dataset and compute similarity score"
2020.emnlp-main.365,W18-6317,0,0.117986,"Model 0.8 -0.2 0.3 MSE-Loss Parallel Data (EN-DE) Student EN sentence vector Hallo Welt 0.7 -0.1 0.3 Student Model MSE-Loss 0.9 -0.2 0.4 Student DE sentence vector Figure 1: Given parallel data (e.g. English and German), train the student model such that the produced vectors for the English and German sentences are close to the teacher English sentence vector. tasks is not straight-forward. Further, running the translation ranking task is complex and results in a huge computational overhead. Selecting random alternative translations usually leads to mediocre results. Instead, hard negatives (Guo et al., 2018) are required, i.e., alternative incorrect translations that have a high similarity to the correct translation. To get these hard negatives, mUSE was first trained with random negatives samples, then, this preliminary sentence encoder was used to identify hard negative examples. They then re-trained the network. In this work, we use Sentence-BERT (SBERT) (Reimers and Gurevych, 2019), which achieves state-of-the-art performance for various sentence embeddings task. SBERT is based on transformer models like BERT (Devlin et al., 2018) and applies mean pooling on the output. In our experiments we"
2020.emnlp-main.365,2020.findings-emnlp.39,0,0.0116461,"chieve a score of 76.7, while with the UNPC dataset (over 8 Million sentences), Dataset size XLM-R mean XLM-R-nli-stsb 1k 5k 10k 25k Full TED2020 EN-DE 21.3 59.5 71.5 74.5 77.0 80.0 80.4 EN-AR 17.4 44.0 48.4 59.6 69.5 70.2 78.0 Table 6: Performance on STS 2017 dataset when trained with reduced TED2020 dataset sizes. 6 Target Language Training In this section we evaluate whether it is better to transfer an English model to a certain target language or if training from-scratch on suitable datasets in the target language yields better results. For this, we use the KorNLI and KorSTS datasets from Ham et al. (2020). They translated the English SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018), and STSbenchmark (STSb) (Cer et al., 2017) datasets to Korean with an internal machine translation system. The dev and tests were postedited by professional translators. Ham et al. fine-tuned Korean RoBERTa and XLM-R on these datasets using the SBERT framework. We use the translated sentences they provide and tuned XLM-R using multilingual knowledge distillation. We use SBERT-nli-stsb as teacher model. Results are shown in Table 7. 4518 Model LASER mUSE Trained on KorNLI & KorSTS Korean RoBERTa-base Kor"
2020.emnlp-main.365,2005.mtsummit-papers.11,0,0.0267393,"ut 4,000 TED talks, available in over 100 languages. This dataset is available in our repository. • NewsCommentary: Political and economic commentary crawled from the web site Project Syndicate, provided by WMT. • WikiMatrix: Mined parallel sentences from Wikipedia in different languages (Schwenk et al., 2019). We only used pairs with scores above 1.05, as pairs below this threshold were often of bad quality. • Tatoeba: Tatoeba4 is a large database of example sentences and translations to support language learning. • Europarl: Parallel sentences extracted from the European Parliament website (Koehn, 2005). • JW300: Mined, parallel sentences from the magazines Awake! and Watchtower (Agi´c and Vuli´c, 2019). • OpenSubtitles2018: Translated movie subtitles from opensubtitles.org (Lison and Tiedemann, 2016). • UNPC: Manually translated United Nations documents from 1994 - 2014 (Ziemski et al., 2016). Getting parallel sentence data can be challenging for some low-resource language pairs. Hence, we also experiment with bilingual dictionaries: • MUSE: MUSE5 provides 110 large-scale ground-truth bilingual dictionaries created by an internal translation tool (Conneau et al., 2017b). • Wikititles: We us"
2020.emnlp-main.365,W18-3022,0,0.0131604,"istically insignificant language bias. There, the performance for the joined set only decreases by -0.19 and -0.11 compared to the evaluation on the individual sets. In summary, mUSE and the proposed multilingual knowledge distillation approach can be used on multilingual sentence pools without a negative performance impact from language bias, while LASER and LaBSE prefer certain language combinations over other, impacting the overall result. 8 Related Work Sentence embeddings are a well studied area with dozens of proposed methods (Kiros et al., 2015; Conneau et al., 2017a; Cer et al., 2018; Yang et al., 2018). Most of the methods have in common that they were only trained on English. Multilingual representations have attracted significant attention in recent times. Most of it focuses on cross-lingual word embeddings (Ruder, 2017). A common approach is to train word embeddings for each language separately and to learn a linear transformation that maps them to a shared space based on a bilingual dictionary (Artetxe et al., 2018). This mapping can also be learned without parallel data (Conneau et al., 2017b; Lample et al., 2018). A straightforward approach for creating crosslingual sentence embedding"
2020.emnlp-main.365,L16-1561,0,0.0333136,"s (Schwenk et al., 2019). We only used pairs with scores above 1.05, as pairs below this threshold were often of bad quality. • Tatoeba: Tatoeba4 is a large database of example sentences and translations to support language learning. • Europarl: Parallel sentences extracted from the European Parliament website (Koehn, 2005). • JW300: Mined, parallel sentences from the magazines Awake! and Watchtower (Agi´c and Vuli´c, 2019). • OpenSubtitles2018: Translated movie subtitles from opensubtitles.org (Lison and Tiedemann, 2016). • UNPC: Manually translated United Nations documents from 1994 - 2014 (Ziemski et al., 2016). Getting parallel sentence data can be challenging for some low-resource language pairs. Hence, we also experiment with bilingual dictionaries: • MUSE: MUSE5 provides 110 large-scale ground-truth bilingual dictionaries created by an internal translation tool (Conneau et al., 2017b). • Wikititles: We use the Wikipedia database dumps to extract the article titles from crosslanguage links between Wikipedia articles. For example, the page ”United States” links to the German page ”Vereinigte Staaten”. This gives a dictionary covering a wide range of topics. 4 https://tatoeba.org/ https://github.co"
2020.emnlp-main.365,W17-2512,0,0.0334495,"Missing"
2020.emnlp-main.365,tiedemann-2012-parallel,0,0.0323824,"d be suboptimal, as most words in tion for M other latin-based languages would be broken down to short character sequences, and words in nonlatin alphabets would be mapped to the UNK token. In contrast, XLM-R uses SentencePiece2 , which avoids language specific pre-processing. Further, it uses a vocabulary with 250k entries from 100 different languages. This makes XLM-R much more suitable for the initialization of the multilingual student model. 3 Training Data In this section, we evaluate the importance of training data for making the sentence embedding model multilingual. The OPUS website3 (Tiedemann, 2012) provides parallel data for hundreds of language pairs. In our experiments, we use the following datasets: • GlobalVoices: A parallel corpus of news stories from the web site Global Voices. • TED2020: We crawled the translated subti2 i https://github.com/google/ 1 Xh ˆ (tj ))2 sentencepiece ˆ (sj ))2 + (M (sj ) − M (M (sj ) − M 3 |B| http://opus.nlpl.eu/ j∈B 4513 tles for about 4,000 TED talks, available in over 100 languages. This dataset is available in our repository. • NewsCommentary: Political and economic commentary crawled from the web site Project Syndicate, provided by WMT. • WikiMatr"
2020.emnlp-main.365,L16-1147,0,\N,Missing
2020.emnlp-main.365,P19-1310,0,\N,Missing
2020.emnlp-main.365,N18-1101,0,\N,Missing
2020.emnlp-main.613,P19-1084,0,0.0803414,"Missing"
2020.emnlp-main.613,S19-1028,0,0.0356875,"Missing"
2020.emnlp-main.613,P17-2097,0,0.0199571,"2018; Poliak et al., 2018; Tsuchiya, 2018) or by basing their predictions on whether the inputs are 9 Although this may seem to be against the spirit of not using prior knowledge about the biases, we believe that this step is necessary to show the stability of the shallow models and to validate if they indeed capture the intended biases. highly overlapped (McCoy et al., 2019b; Dasgupta et al., 2018). Similar phenomena exist in various tasks, including argumentation mining (Niven and Kao, 2019), reading comprehension (Kaushik and Lipton, 2018), or story cloze completion (Schwartz et al., 2017; Cai et al., 2017). To allow a better evaluation of models’ reasoning capabilities, researchers constructed challenge test sets composed of “counterexamples” to the spurious shortcuts that models may adopt (Jia and Liang, 2017; Glockner et al., 2018; Zhang et al., 2019; Naik et al., 2018). Models evaluated on these sets often fall back to random baseline performance. There has been a flurry of work in dynamic dataset construction to systematically reduce dataset biases through adversarial filtering (Zellers et al., 2018; Sakaguchi et al., 2020; Bras et al., 2020) or human in the loop (Nie et al., 2019b; Kaushik"
2020.emnlp-main.613,D19-1418,0,0.309334,"are shown to perform better beyond training distribution. They improved the performance on challenge test sets that are designed such that relying on the spurious association leads to incorrect predictions. 1 The code is available at https://github.com/ UKPLab/emnlp2020-debiasing-unknown 2 E.g., in several textual entailment datasets, negation words such as “never” or “nobody” are highly associated with the contradiction label. Prevailing debiasing methods, e.g., example reweighting (Schuster et al., 2019), confidence regularization (Utama et al., 2020), and model ensembling (He et al., 2019; Clark et al., 2019; Mahabadi et al., 2020), are agnostic to model’s architecture as they operate by adjusting the training loss to account for biases. Namely, they first identify biased examples in the training data and down-weight their importance in the training loss so that models focus on learning from harder examples.3 While promising, these model agnostic methods rely on the assumption that the specific types of biased features (e.g., lexical overlap) are known a-priori. This assumption, however, is a limitation in various NLU tasks or datasets because it depends on researchers’ intuition and task-specifi"
2020.emnlp-main.613,N19-1423,0,0.0451039,"asing setup on a particular task. the resulting models perform the task by relying on lexical overlap biases. Fact verification We run debiasing experiments on the FEVER dataset (Thorne et al., 2018). It contains pairs of claim and evidence sentences labeled as either support, refutes, and not-enoughinformation. We evaluate on the FeverSymmetric test set (Schuster et al., 2019), which is collected to reduced claim-only biases (e.g., negative phrases such as “refused to” or “did not” are associated with the refutes label). 4.2 Main Model We apply our self-debiasing framework on the BERT model (Devlin et al., 2019), which performs very well on the three considered tasks.6 It also shows substantial improvements on the corresponding challenge datasets when trained through the existing debiasing methods (Clark et al., 2019; He et al., 2019). For each examined debiasing method, we show the comparison between the results when it is applied within our framework and when it is trained using prior knowledge to detect training examples with a specific bias. For the second scenario, MNLI and QQP models are debiased using a lexical overlap bias prior, whereas FEVER model is debiased using hand-crafted claim-only b"
2020.emnlp-main.613,D18-1002,0,0.0274539,"ing, though, is that Jobs has never really let this idea go. orig. hypo.: Jobs never held onto an idea for long. biased: 0 Jobs never held onto an idea for long. anti-biased: 1 Jobs never held onto an idea for long. label: 0 (contradiction) Figure 1: Synthetic bias datasets are created by appending an artificial feature to the input text that allows models to use it as a shortcut to the target label. For each example in MNLI, a number-coded label (contradiction: 0 , entailment: 1 , neutral: 2 ) is appended to the hypothesis sentences. relying on them for prediction may be harmful to fairness (Elazar and Goldberg, 2018) or generalization (McCoy et al., 2019b). The instances of these features may include protected socio-demographic attributes (gender, age, etc.) in automatic hiring decision systems; or surface-level patterns (negation words, lexical overlap, etc.) in NLU tasks. Further, we consider the label bias to be unknown when the information about the characteristics of its associated features is not precise enough for the existing debiasing strategies to identify potentially biased examples. 2 Motivation and Analysis Debiasing NLU models Recent NLU tasks are commonly formulated as multi-class classific"
2020.emnlp-main.613,W07-1401,0,0.0729783,"ask. Motivated by this, we perform similar evaluations for models trained on MNLI through the three debiasing setups: known-bias to target the HANS-specific bias, self-debiasing, and self-debiasing augmented with the proposed annealing mechanism. We do not tune the hyperparameters for each target dataset and use the models that we previously reported in the main results. As the target datasets, we use 4 NLI datasets: Scitail (Khot et al., 2018), SICK (Marelli et al., 2014), GLUE diagnostic set (Wang et al., 2018), and 3way version of RTE 1, 2, and 3 (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007).8 We present the results in Table 2. We observe that the debiasing with prior knowledge to target the specific lexical overlap bias (indicated by knownHANS ) can help models to perform better on SICK and Scitail. However, its resulting models under-perform the baseline in RTE sets and GLUE diagnostic, degrading the accuracy by 0.5 and 0.6pp. In contrast, the self-debiased models, with and without annealing mechanism, outperform the baseline on all target datasets, both achieving additional 1.1pp on average. The gains by the two self-debiasing suggest that while they are effective in mitigatin"
2020.emnlp-main.613,P18-2103,0,0.0218397,"ecessary to show the stability of the shallow models and to validate if they indeed capture the intended biases. highly overlapped (McCoy et al., 2019b; Dasgupta et al., 2018). Similar phenomena exist in various tasks, including argumentation mining (Niven and Kao, 2019), reading comprehension (Kaushik and Lipton, 2018), or story cloze completion (Schwartz et al., 2017; Cai et al., 2017). To allow a better evaluation of models’ reasoning capabilities, researchers constructed challenge test sets composed of “counterexamples” to the spurious shortcuts that models may adopt (Jia and Liang, 2017; Glockner et al., 2018; Zhang et al., 2019; Naik et al., 2018). Models evaluated on these sets often fall back to random baseline performance. There has been a flurry of work in dynamic dataset construction to systematically reduce dataset biases through adversarial filtering (Zellers et al., 2018; Sakaguchi et al., 2020; Bras et al., 2020) or human in the loop (Nie et al., 2019b; Kaushik et al., 2020; Gardner et al., 2020). While promising, researchers also show that newly constructed datasets may not be fully free of hidden biased patterns (Sharma et al., 2018). It is thus crucial to complement the data collectio"
2020.emnlp-main.613,N18-2017,0,0.136298,"Missing"
2020.emnlp-main.613,D19-6115,0,0.734486,"resulting models are shown to perform better beyond training distribution. They improved the performance on challenge test sets that are designed such that relying on the spurious association leads to incorrect predictions. 1 The code is available at https://github.com/ UKPLab/emnlp2020-debiasing-unknown 2 E.g., in several textual entailment datasets, negation words such as “never” or “nobody” are highly associated with the contradiction label. Prevailing debiasing methods, e.g., example reweighting (Schuster et al., 2019), confidence regularization (Utama et al., 2020), and model ensembling (He et al., 2019; Clark et al., 2019; Mahabadi et al., 2020), are agnostic to model’s architecture as they operate by adjusting the training loss to account for biases. Namely, they first identify biased examples in the training data and down-weight their importance in the training loss so that models focus on learning from harder examples.3 While promising, these model agnostic methods rely on the assumption that the specific types of biased features (e.g., lexical overlap) are known a-priori. This assumption, however, is a limitation in various NLU tasks or datasets because it depends on researchers’ intuit"
2020.emnlp-main.613,D17-1215,0,0.0468097,"e that this step is necessary to show the stability of the shallow models and to validate if they indeed capture the intended biases. highly overlapped (McCoy et al., 2019b; Dasgupta et al., 2018). Similar phenomena exist in various tasks, including argumentation mining (Niven and Kao, 2019), reading comprehension (Kaushik and Lipton, 2018), or story cloze completion (Schwartz et al., 2017; Cai et al., 2017). To allow a better evaluation of models’ reasoning capabilities, researchers constructed challenge test sets composed of “counterexamples” to the spurious shortcuts that models may adopt (Jia and Liang, 2017; Glockner et al., 2018; Zhang et al., 2019; Naik et al., 2018). Models evaluated on these sets often fall back to random baseline performance. There has been a flurry of work in dynamic dataset construction to systematically reduce dataset biases through adversarial filtering (Zellers et al., 2018; Sakaguchi et al., 2020; Bras et al., 2020) or human in the loop (Nie et al., 2019b; Kaushik et al., 2020; Gardner et al., 2020). While promising, researchers also show that newly constructed datasets may not be fully free of hidden biased patterns (Sharma et al., 2018). It is thus crucial to comple"
2020.emnlp-main.613,D18-1546,0,0.016714,"rform better than chance by only using the partial input (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) or by basing their predictions on whether the inputs are 9 Although this may seem to be against the spirit of not using prior knowledge about the biases, we believe that this step is necessary to show the stability of the shallow models and to validate if they indeed capture the intended biases. highly overlapped (McCoy et al., 2019b; Dasgupta et al., 2018). Similar phenomena exist in various tasks, including argumentation mining (Niven and Kao, 2019), reading comprehension (Kaushik and Lipton, 2018), or story cloze completion (Schwartz et al., 2017; Cai et al., 2017). To allow a better evaluation of models’ reasoning capabilities, researchers constructed challenge test sets composed of “counterexamples” to the spurious shortcuts that models may adopt (Jia and Liang, 2017; Glockner et al., 2018; Zhang et al., 2019; Naik et al., 2018). Models evaluated on these sets often fall back to random baseline performance. There has been a flurry of work in dynamic dataset construction to systematically reduce dataset biases through adversarial filtering (Zellers et al., 2018; Sakaguchi et al., 2020"
2020.emnlp-main.613,2020.acl-main.769,0,0.554577,"better beyond training distribution. They improved the performance on challenge test sets that are designed such that relying on the spurious association leads to incorrect predictions. 1 The code is available at https://github.com/ UKPLab/emnlp2020-debiasing-unknown 2 E.g., in several textual entailment datasets, negation words such as “never” or “nobody” are highly associated with the contradiction label. Prevailing debiasing methods, e.g., example reweighting (Schuster et al., 2019), confidence regularization (Utama et al., 2020), and model ensembling (He et al., 2019; Clark et al., 2019; Mahabadi et al., 2020), are agnostic to model’s architecture as they operate by adjusting the training loss to account for biases. Namely, they first identify biased examples in the training data and down-weight their importance in the training loss so that models focus on learning from harder examples.3 While promising, these model agnostic methods rely on the assumption that the specific types of biased features (e.g., lexical overlap) are known a-priori. This assumption, however, is a limitation in various NLU tasks or datasets because it depends on researchers’ intuition and task-specific insights to manually c"
2020.emnlp-main.613,marelli-etal-2014-sick,0,0.0357642,"ls based on only a single bias results in models that perform significantly worse upon cross-datasets evaluation for the reading comprehension task. Motivated by this, we perform similar evaluations for models trained on MNLI through the three debiasing setups: known-bias to target the HANS-specific bias, self-debiasing, and self-debiasing augmented with the proposed annealing mechanism. We do not tune the hyperparameters for each target dataset and use the models that we previously reported in the main results. As the target datasets, we use 4 NLI datasets: Scitail (Khot et al., 2018), SICK (Marelli et al., 2014), GLUE diagnostic set (Wang et al., 2018), and 3way version of RTE 1, 2, and 3 (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007).8 We present the results in Table 2. We observe that the debiasing with prior knowledge to target the specific lexical overlap bias (indicated by knownHANS ) can help models to perform better on SICK and Scitail. However, its resulting models under-perform the baseline in RTE sets and GLUE diagnostic, degrading the accuracy by 0.5 and 0.6pp. In contrast, the self-debiased models, with and without annealing mechanism, outperform the baseline on all"
2020.emnlp-main.613,P19-1334,0,0.230119,"asing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models’ reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.1 1 Introduction Neural models often achieve impressive performance on many natural language understanding tasks (NLU) by leveraging biased features, i.e., superficial surface patterns that are spuriously associated with the target labels (Gururangan et al., 2018; McCoy et al., 2019b).2 Recently proposed debiasing methods are effective in mitigating the impact of this tendency, and the resulting models are shown to perform better beyond training distribution. They improved the performance on challenge test sets that are designed such that relying on the spurious association leads to incorrect predictions. 1 The code is available at https://github.com/ UKPLab/emnlp2020-debiasing-unknown 2 E.g., in several textual entailment datasets, negation words such as “never” or “nobody” are highly associated with the contradiction label. Prevailing debiasing methods, e.g., example r"
2020.emnlp-main.613,C18-1198,0,0.0163303,"ow models and to validate if they indeed capture the intended biases. highly overlapped (McCoy et al., 2019b; Dasgupta et al., 2018). Similar phenomena exist in various tasks, including argumentation mining (Niven and Kao, 2019), reading comprehension (Kaushik and Lipton, 2018), or story cloze completion (Schwartz et al., 2017; Cai et al., 2017). To allow a better evaluation of models’ reasoning capabilities, researchers constructed challenge test sets composed of “counterexamples” to the spurious shortcuts that models may adopt (Jia and Liang, 2017; Glockner et al., 2018; Zhang et al., 2019; Naik et al., 2018). Models evaluated on these sets often fall back to random baseline performance. There has been a flurry of work in dynamic dataset construction to systematically reduce dataset biases through adversarial filtering (Zellers et al., 2018; Sakaguchi et al., 2020; Bras et al., 2020) or human in the loop (Nie et al., 2019b; Kaushik et al., 2020; Gardner et al., 2020). While promising, researchers also show that newly constructed datasets may not be fully free of hidden biased patterns (Sharma et al., 2018). It is thus crucial to complement the data collection efforts with learning algorithms that"
2020.emnlp-main.613,2020.acl-main.441,0,0.128965,"Missing"
2020.emnlp-main.613,P19-1459,0,0.0192457,"ended reasoning skills. In NLI, models can perform better than chance by only using the partial input (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) or by basing their predictions on whether the inputs are 9 Although this may seem to be against the spirit of not using prior knowledge about the biases, we believe that this step is necessary to show the stability of the shallow models and to validate if they indeed capture the intended biases. highly overlapped (McCoy et al., 2019b; Dasgupta et al., 2018). Similar phenomena exist in various tasks, including argumentation mining (Niven and Kao, 2019), reading comprehension (Kaushik and Lipton, 2018), or story cloze completion (Schwartz et al., 2017; Cai et al., 2017). To allow a better evaluation of models’ reasoning capabilities, researchers constructed challenge test sets composed of “counterexamples” to the spurious shortcuts that models may adopt (Jia and Liang, 2017; Glockner et al., 2018; Zhang et al., 2019; Naik et al., 2018). Models evaluated on these sets often fall back to random baseline performance. There has been a flurry of work in dynamic dataset construction to systematically reduce dataset biases through adversarial filte"
2020.emnlp-main.613,S18-2023,0,0.175096,"Missing"
2020.emnlp-main.613,L18-1239,0,0.0843662,"Missing"
2020.emnlp-main.613,D19-1341,0,0.573476,"y proposed debiasing methods are effective in mitigating the impact of this tendency, and the resulting models are shown to perform better beyond training distribution. They improved the performance on challenge test sets that are designed such that relying on the spurious association leads to incorrect predictions. 1 The code is available at https://github.com/ UKPLab/emnlp2020-debiasing-unknown 2 E.g., in several textual entailment datasets, negation words such as “never” or “nobody” are highly associated with the contradiction label. Prevailing debiasing methods, e.g., example reweighting (Schuster et al., 2019), confidence regularization (Utama et al., 2020), and model ensembling (He et al., 2019; Clark et al., 2019; Mahabadi et al., 2020), are agnostic to model’s architecture as they operate by adjusting the training loss to account for biases. Namely, they first identify biased examples in the training data and down-weight their importance in the training loss so that models focus on learning from harder examples.3 While promising, these model agnostic methods rely on the assumption that the specific types of biased features (e.g., lexical overlap) are known a-priori. This assumption, however, is"
2020.emnlp-main.613,2020.tacl-1.40,0,0.207091,"s et al., 2020) or human in the loop (Nie et al., 2019b; Kaushik et al., 2020; Gardner et al., 2020). While promising, researchers also show that newly constructed datasets may not be fully free of hidden biased patterns (Sharma et al., 2018). It is thus crucial to complement the data collection efforts with learning algorithms that are more robust to biases, such as the recently proposed product-ofexpert (Clark et al., 2019; He et al., 2019; Mahabadi et al., 2020), confidence regularization (Utama et al., 2020), or other training strategies (Belinkov et al., 2019b; Yaghoobzadeh et al., 2019; Tu et al., 2020). Despite their effectiveness, these methods are limited by their assumption on the availability of information about the task-specific biases. Our framework aims to alleviate this limitation and enable them to address unknown biases. 7 Conclusion We present a general self-debiasing framework to address the impact of unknown dataset biases by omitting the need for thorough dataset-specific analysis to discover the types of biases for each new dataset. We adopt the existing debiasing methods into our framework and enable them to obtain equally high improvements on several challenge test sets wi"
2020.emnlp-main.613,K17-1004,0,0.0218074,"ut (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) or by basing their predictions on whether the inputs are 9 Although this may seem to be against the spirit of not using prior knowledge about the biases, we believe that this step is necessary to show the stability of the shallow models and to validate if they indeed capture the intended biases. highly overlapped (McCoy et al., 2019b; Dasgupta et al., 2018). Similar phenomena exist in various tasks, including argumentation mining (Niven and Kao, 2019), reading comprehension (Kaushik and Lipton, 2018), or story cloze completion (Schwartz et al., 2017; Cai et al., 2017). To allow a better evaluation of models’ reasoning capabilities, researchers constructed challenge test sets composed of “counterexamples” to the spurious shortcuts that models may adopt (Jia and Liang, 2017; Glockner et al., 2018; Zhang et al., 2019; Naik et al., 2018). Models evaluated on these sets often fall back to random baseline performance. There has been a flurry of work in dynamic dataset construction to systematically reduce dataset biases through adversarial filtering (Zellers et al., 2018; Sakaguchi et al., 2020; Bras et al., 2020) or human in the loop (Nie et"
2020.emnlp-main.613,2020.acl-main.468,0,0.1314,"Missing"
2020.emnlp-main.613,P18-2119,0,0.0816975,"uition and task-specific insights to manually characterize the spurious biases, which may range from simple word/n-grams cooccurrence (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Schuster et al., 2019) to more complex stylistic and lexico-syntactic patterns (Zellers et al., 2019; Snow et al., 2006; Vanderwende and Dolan, 2006). The existing datasets or the newly created ones (Zellers et al., 2019; Sakaguchi et al., 2020; Nie et al., 2019b) are, therefore, still very likely to contain biased patterns that remain unknown without an in-depth analysis of each individual dataset (Sharma et al., 2018). In this paper, we propose a new strategy to enable the existing debiasing methods to be applicable in settings where there is minimum prior information about the biases. Specifically, models should automatically identify potentially biased examples without being pinpointed at a specific bias in advance. Our work makes the following novel contributions in this direction of automatic bias mitigation. First, we analyze the learning dynamics of a 3 We refer to biased examples as examples that can be solved using only biased features. 7597 Proceedings of the 2020 Conference on Empirical Methods i"
2020.emnlp-main.613,N06-1005,0,0.0665457,"learning from harder examples.3 While promising, these model agnostic methods rely on the assumption that the specific types of biased features (e.g., lexical overlap) are known a-priori. This assumption, however, is a limitation in various NLU tasks or datasets because it depends on researchers’ intuition and task-specific insights to manually characterize the spurious biases, which may range from simple word/n-grams cooccurrence (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Schuster et al., 2019) to more complex stylistic and lexico-syntactic patterns (Zellers et al., 2019; Snow et al., 2006; Vanderwende and Dolan, 2006). The existing datasets or the newly created ones (Zellers et al., 2019; Sakaguchi et al., 2020; Nie et al., 2019b) are, therefore, still very likely to contain biased patterns that remain unknown without an in-depth analysis of each individual dataset (Sharma et al., 2018). In this paper, we propose a new strategy to enable the existing debiasing methods to be applicable in settings where there is minimum prior information about the biases. Specifically, models should automatically identify potentially biased examples without being pinpointed at a specific bias i"
2020.emnlp-main.613,W18-5501,0,0.0127636,"their corresponding challenge test sets. The known-bias results for MNLI and FEVER are taken from Utama et al. (2020)([), Clark et al. (2019)(‡), Mahabadi et al. (2020)(†), and Schuster et al. (2019)(♣). The results of the proposed framework are indicated by self-debias. (♠) indicates the training with our proposed annealing mechanism. Boldface numbers indicate the highest challenge test set improvement for each debiasing setup on a particular task. the resulting models perform the task by relying on lexical overlap biases. Fact verification We run debiasing experiments on the FEVER dataset (Thorne et al., 2018). It contains pairs of claim and evidence sentences labeled as either support, refutes, and not-enoughinformation. We evaluate on the FeverSymmetric test set (Schuster et al., 2019), which is collected to reduced claim-only biases (e.g., negative phrases such as “refused to” or “did not” are associated with the refutes label). 4.2 Main Model We apply our self-debiasing framework on the BERT model (Devlin et al., 2019), which performs very well on the three considered tasks.6 It also shows substantial improvements on the corresponding challenge datasets when trained through the existing debiasi"
2020.emnlp-main.613,2020.acl-main.770,1,0.887293,"ating the impact of this tendency, and the resulting models are shown to perform better beyond training distribution. They improved the performance on challenge test sets that are designed such that relying on the spurious association leads to incorrect predictions. 1 The code is available at https://github.com/ UKPLab/emnlp2020-debiasing-unknown 2 E.g., in several textual entailment datasets, negation words such as “never” or “nobody” are highly associated with the contradiction label. Prevailing debiasing methods, e.g., example reweighting (Schuster et al., 2019), confidence regularization (Utama et al., 2020), and model ensembling (He et al., 2019; Clark et al., 2019; Mahabadi et al., 2020), are agnostic to model’s architecture as they operate by adjusting the training loss to account for biases. Namely, they first identify biased examples in the training data and down-weight their importance in the training loss so that models focus on learning from harder examples.3 While promising, these model agnostic methods rely on the assumption that the specific types of biased features (e.g., lexical overlap) are known a-priori. This assumption, however, is a limitation in various NLU tasks or datasets be"
2020.emnlp-main.613,W18-5446,0,0.0647738,"Missing"
2020.emnlp-main.613,N18-1101,0,0.0734977,"ve not enough information about the biases for training a debiased model, and thus biased examples should be identified automatically. Therefore, we only use the existing challenge test set for each examined task strictly for evaluation and do not use the information about their corresponding bias types during training. In the following, we briefly discuss the datasets used for training on each task as well as their corresponding challenge test sets to evaluate the impact of debiasing methods: Natural language inference We use the English Multi-Genre Natural Language Inference (MNLI) dataset (Williams et al., 2018) which consists of 392K pairs of premise and hypothesis sentences annotated with their textual entailment information. We test NLI models on lexical overlap bias using HANS evaluation set (McCoy et al., 2019b). It contains examples, in which premise and hypothesis sentences that consist of the same set of words may not hold an entailment relationship, e.g., “cat caught a mouse” vs. “mouse caught a cat”. Since word overlapping is biased towards entailment in MNLI, models trained on this dataset often perform close to a random baseline on HANS. Paraphrase identification We experiment with the Qu"
2020.emnlp-main.613,2020.findings-emnlp.74,1,0.753866,"Missing"
2020.emnlp-main.613,D18-1009,0,0.0405566,"ing comprehension (Kaushik and Lipton, 2018), or story cloze completion (Schwartz et al., 2017; Cai et al., 2017). To allow a better evaluation of models’ reasoning capabilities, researchers constructed challenge test sets composed of “counterexamples” to the spurious shortcuts that models may adopt (Jia and Liang, 2017; Glockner et al., 2018; Zhang et al., 2019; Naik et al., 2018). Models evaluated on these sets often fall back to random baseline performance. There has been a flurry of work in dynamic dataset construction to systematically reduce dataset biases through adversarial filtering (Zellers et al., 2018; Sakaguchi et al., 2020; Bras et al., 2020) or human in the loop (Nie et al., 2019b; Kaushik et al., 2020; Gardner et al., 2020). While promising, researchers also show that newly constructed datasets may not be fully free of hidden biased patterns (Sharma et al., 2018). It is thus crucial to complement the data collection efforts with learning algorithms that are more robust to biases, such as the recently proposed product-ofexpert (Clark et al., 2019; He et al., 2019; Mahabadi et al., 2020), confidence regularization (Utama et al., 2020), or other training strategies (Belinkov et al., 2019b"
2020.emnlp-main.613,P19-1472,0,0.099699,"that models focus on learning from harder examples.3 While promising, these model agnostic methods rely on the assumption that the specific types of biased features (e.g., lexical overlap) are known a-priori. This assumption, however, is a limitation in various NLU tasks or datasets because it depends on researchers’ intuition and task-specific insights to manually characterize the spurious biases, which may range from simple word/n-grams cooccurrence (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Schuster et al., 2019) to more complex stylistic and lexico-syntactic patterns (Zellers et al., 2019; Snow et al., 2006; Vanderwende and Dolan, 2006). The existing datasets or the newly created ones (Zellers et al., 2019; Sakaguchi et al., 2020; Nie et al., 2019b) are, therefore, still very likely to contain biased patterns that remain unknown without an in-depth analysis of each individual dataset (Sharma et al., 2018). In this paper, we propose a new strategy to enable the existing debiasing methods to be applicable in settings where there is minimum prior information about the biases. Specifically, models should automatically identify potentially biased examples without being pinpointed a"
2020.emnlp-main.613,N19-1131,0,0.0785494,"Missing"
2020.emnlp-main.613,2020.emnlp-main.659,0,0.0175433,"ty of losses throughout the training. This result suggests that overconfident predictions (unusually low loss examples) can be an indication of the model utilizing biases. This is in line with the finding of Utama et al. (2020), which shows that regularizing confidence on biased examples leads to improved robustness against biases. Bias identification stability Researchers have recently observed large variability in the generalization performance of fine-tuned BERT model (Mosbach et al., 2020; Zhang et al., 2020), especially in the out-of-distribution evaluation settings (McCoy et al., 2019a; Zhou et al., 2020). This may raise concerns on whether our shallow models, which are trained on the sub-sample of the training data, can consistently learn to rely mostly on biases. We, therefore, train 10 instances of shallow models on the MNLI dataset using different random seeds (for classifier’s weight initialization and training sub-sampling). For evaluation, we perform two different partitionings of MNLI dev set based on the output of two simple hand-crafted models, which 7604 100 80 60 40 random baseline 20 p d p -only -only verla verla hypo hypo lex-o lex-o hardeasyhardeasyatche dev-m Figure 6: Evaluati"
2020.emnlp-main.617,D16-1264,0,0.11385,"Missing"
2020.emnlp-main.617,D19-1454,0,0.0261635,"anguages, it achieves competitive performance even for high-resource languages and on more challenging tasks. These evaluations also hint at the modularity of the adapter-based MAD-X approach, which holds promise of quick adaptation to more tasks: we use exactly the same languagespecific adapters in NER, CCR, and QA for languages such as English and Mandarin Chinese that appear in all three evaluation language samples. 7 Further Analysis Impact of Invertible Adapters We also analyse the relative performance difference of MAD-X 80 60 F1 ting from Ponti et al. (2020a)—fine-tuning first on SIQA (Sap et al., 2019) and on the English COPA training set—and report other possible settings in the appendix. Target language adaptation outperforms XLM-RBase while MAD-XBase achieves the best scores. It shows gains in particular for the two unseen languages, Haitian Creole (ht) and Quechua (qu). Performance on the other languages is also generally competitive or better. 40 Language qu cdo ilo 20 0 0 20k 40k xmf mi mhr 60k Number of iterations Epochs 25 50 100 tk gn 80k 100k Figure 4: Cross-lingual NER performance of MAD-X transferring from English to the target languages with invertible and language adapters tra"
2020.emnlp-main.617,N19-1423,0,\N,Missing
2020.emnlp-main.617,D19-1572,1,\N,Missing
2020.emnlp-main.617,W19-4330,0,\N,Missing
2020.emnlp-main.617,D19-1165,0,\N,Missing
2020.findings-emnlp.74,N19-1246,0,0.0281059,"Missing"
2020.findings-emnlp.74,D19-5801,0,0.109658,"ds, we (1) concurrently model multiple biases without requiring any information about evaluation datasets, and (2) show that our debiasing framework achieves improvements in in-domain, as well as unseen out-of-domain datasets. Generalization in QA The ability to generalize models to unseen domains is important across a variety of QA tasks (R¨uckl´e et al., 2020; Guo et al., 2020; Talmor and Berant, 2019). In this work, we focus on extractive QA. In this context, the MRQA workshop held a shared task dedicated to evaluating the generalization capabilities of QA models to unseen target datasets (Fisch et al., 2019a). The winning team (Li et al., 2019) uses an ensemble of multiple pre-trained language models, which includes XLNet (Yang et al., 2019) and ERNIE (Sun et al., 2019). Other submissions outperform the baseline by using more complex models with more parameters and better pre-training. For example, 840 training example {context ci, question qi, answer ai} Teacher model Mt1 §3.1 • • Teacher model Mtn bias model B1 bias model B2 §3.2 §3.3 combine bias model Bk loss ℓ bias weight Student model M §3.4 teacher prediction pt,i Figure 1: An illustration of our debiasing framework. The teacher and bias"
2020.findings-emnlp.74,W18-2501,0,0.0128241,"ollows: L(xi ) =(1 − FB (xi )) × KL(log psi , pti ) 4 4.1 Experimental Setup Dataset wh. emp. lex. shal. one all SQuAD Hotpot Trivia News NQ 17.9 26.8 29.6 16.2 47.5 8.8 18.2 26.8 7.9 38.5 51.9 56.5 41.6 11.4 51.0 32.7 45.1 21.3 17.4 38.7 61.9 74.5 58.1 31.8 64.8 3.4 6.9 6.2 1.0 23.2 Base Model Table 1: The ratio of examples that are answered correctly by the bias models. ‘one’ shows the ratio of examples that contain at least one bias. ‘all’ shows the ratio for examples that contain all biases. We perform all experiments with BERT base uncased (Devlin et al., 2019) in the AllenNLP framework (Gardner et al., 2018). We use the MRQA multi-task implementation (Fisch et al., 2019b) of BERT for QA model as the baseline. 4.3 4.2 Examined Biases and Bias Models We incorporate four biases in our experiments. • Wh-word (Weissenborn et al., 2017): the corresponding model for detecting this bias only uses the interrogative adverbs from the question. • Lexical overlap (Jia and Liang, 2017): in many QA examples, the answer is in the sentence of the context that has a high similarity to the question. To recognize this bias, we train the bias model using only the sentence of the context that has the highest similarit"
2020.findings-emnlp.74,D19-1418,0,0.454355,"017; Das et al., 2019). Models tend to exploit these shallow patterns—which we refer to as biases in this paper– instead of learning general knowledge about solving the target task. Existing debiasing approaches weaken the impact of such biases by disregarding or down1 The code and data are available at https://github.com/UKPLab/ qa-generalization-concurrent-debiasing. weighting affected training examples. They are often evaluated using adversarial or synthetic sets that contain counterexamples, in which relying on the examined bias will result in incorrect predictions (Belinkov et al., 2019; Clark et al., 2019; He et al., 2019; Mahabadi et al., 2020). Importantly, the majority of existing debiasing approaches only deal with a single bias. They improve the performance scores on a targeted adversarial evaluation set, while typically resulting in performance decreases on the original datasets, or on adversarial datasets that contain different types of biases (Utama et al., 2020; Nie et al., 2019; He et al., 2019). In this paper, we show that modeling multiple biases is a key factor to benefit from debiasing methods for improving both in-domain performance and out-of-domain generalization, and propose"
2020.findings-emnlp.74,N18-2017,0,0.0620235,"Missing"
2020.findings-emnlp.74,D17-1215,0,0.634084,"y evaluate our framework on extractive question answering with training data from various domains with multiple biases of different strengths. We perform the evaluations in two different settings, in which the model is trained on a single domain or multiple domains simultaneously, and show its effectiveness in both settings compared to state-of-the-art debiasing methods.1 1 Introduction As a result of annotation artifacts, existing NLP datasets contain shallow patterns that correlate with target labels (Gururangan et al., 2018; McCoy et al., 2019; Schuster et al., 2019a; Le Bras et al., 2020; Jia and Liang, 2017; Das et al., 2019). Models tend to exploit these shallow patterns—which we refer to as biases in this paper– instead of learning general knowledge about solving the target task. Existing debiasing approaches weaken the impact of such biases by disregarding or down1 The code and data are available at https://github.com/UKPLab/ qa-generalization-concurrent-debiasing. weighting affected training examples. They are often evaluated using adversarial or synthetic sets that contain counterexamples, in which relying on the examined bias will result in incorrect predictions (Belinkov et al., 2019; Cla"
2020.findings-emnlp.74,P17-1147,0,0.0249962,"arity to the question. To recognize this bias, we train the bias model using only the sentence of the context that has the highest similarity to the question, if the answer lies in this sentence.3 Otherwise, we exclude the example during training. • Empty question (Sugawara et al., 2020): the answer can be found without the presence of a question, e.g., by selecting the most prominent entity of the context. The model for detecting this bias only uses contexts without questions. We use five training datasets. This includes SQuAD (Rajpurkar et al., 2016), HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (NQ) (Kwiatkowski et al., 2019). For evaluating the out-of-domain generalization of models, we use six datasets. This includes BioASQ (Wiese et al., 2017), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RACE (Lai et al., 2017), RelationExtraction (Levy et al., 2017), and TextbookQA (Kembhavi et al., 2017). For all training and evaluation datasets, we use the version that are provided by the MRQA shared task, in which all examples can be solved using extractive answer selection. Detailed statistics of all datasets are reported in the"
2020.findings-emnlp.74,D16-1264,0,0.0392276,"he answer is in the sentence of the context that has a high similarity to the question. To recognize this bias, we train the bias model using only the sentence of the context that has the highest similarity to the question, if the answer lies in this sentence.3 Otherwise, we exclude the example during training. • Empty question (Sugawara et al., 2020): the answer can be found without the presence of a question, e.g., by selecting the most prominent entity of the context. The model for detecting this bias only uses contexts without questions. We use five training datasets. This includes SQuAD (Rajpurkar et al., 2016), HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (NQ) (Kwiatkowski et al., 2019). For evaluating the out-of-domain generalization of models, we use six datasets. This includes BioASQ (Wiese et al., 2017), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RACE (Lai et al., 2017), RelationExtraction (Levy et al., 2017), and TextbookQA (Kembhavi et al., 2017). For all training and evaluation datasets, we use the version that are provided by the MRQA shared task, in which all examples can be solved using extractive answer selec"
2020.findings-emnlp.74,D19-1410,1,0.810405,"six datasets. This includes BioASQ (Wiese et al., 2017), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RACE (Lai et al., 2017), RelationExtraction (Levy et al., 2017), and TextbookQA (Kembhavi et al., 2017). For all training and evaluation datasets, we use the version that are provided by the MRQA shared task, in which all examples can be solved using extractive answer selection. Detailed statistics of all datasets are reported in the appendix. 4.4 2 The final loss is the average of the start and end losses, which are both computed using the same loss function L. 3 We use Sentence-BERT (Reimers and Gurevych, 2019) to determine the sentence similarity. Data Evaluation Settings We evaluate our proposed methods in two different settings: (1) single-domain (SD), and (2) multidomain (MD). In SD, the model is trained on a single dataset. For the MD setting, we use all the training datasets of §4.3. Our baseline within this set843 Dataset Dev. Baseline NQ Mb-WL Mb-CR Baseline 63.66 64.90 64.95 58.24 1.24 1.29 I-∆ TriviaQA Mb-WL Mb-CR 59.87 59.09 1.63 0.85 DROP RACE BioSQ TxtQA RelExt DuoRC 19.10 20.47 34.91 30.94 63.74 34.78 21.76 22.85 36.10 33.87 63.06 36.64 21.29 23.00 36.44 34.66 64.01 38.71 9.12 15.58 26"
2020.findings-emnlp.74,2020.emnlp-main.194,1,0.826967,"Missing"
2020.findings-emnlp.74,P18-1156,0,0.0898049,"(Sugawara et al., 2020): the answer can be found without the presence of a question, e.g., by selecting the most prominent entity of the context. The model for detecting this bias only uses contexts without questions. We use five training datasets. This includes SQuAD (Rajpurkar et al., 2016), HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (NQ) (Kwiatkowski et al., 2019). For evaluating the out-of-domain generalization of models, we use six datasets. This includes BioASQ (Wiese et al., 2017), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RACE (Lai et al., 2017), RelationExtraction (Levy et al., 2017), and TextbookQA (Kembhavi et al., 2017). For all training and evaluation datasets, we use the version that are provided by the MRQA shared task, in which all examples can be solved using extractive answer selection. Detailed statistics of all datasets are reported in the appendix. 4.4 2 The final loss is the average of the start and end losses, which are both computed using the same loss function L. 3 We use Sentence-BERT (Reimers and Gurevych, 2019) to determine the sentence similarity. Data Evaluation Settings We evaluate our"
2020.findings-emnlp.74,D19-1341,0,0.45047,"examples with high bias weights. We extensively evaluate our framework on extractive question answering with training data from various domains with multiple biases of different strengths. We perform the evaluations in two different settings, in which the model is trained on a single domain or multiple domains simultaneously, and show its effectiveness in both settings compared to state-of-the-art debiasing methods.1 1 Introduction As a result of annotation artifacts, existing NLP datasets contain shallow patterns that correlate with target labels (Gururangan et al., 2018; McCoy et al., 2019; Schuster et al., 2019a; Le Bras et al., 2020; Jia and Liang, 2017; Das et al., 2019). Models tend to exploit these shallow patterns—which we refer to as biases in this paper– instead of learning general knowledge about solving the target task. Existing debiasing approaches weaken the impact of such biases by disregarding or down1 The code and data are available at https://github.com/UKPLab/ qa-generalization-concurrent-debiasing. weighting affected training examples. They are often evaluated using adversarial or synthetic sets that contain counterexamples, in which relying on the examined bias will result in incor"
2020.findings-emnlp.74,D19-5827,0,0.0762617,"g (QA), for which a wide range of datasets from different domains exist—some contain crucial biases (Weissenborn et al., 2017; Sug839 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 839–853 c November 16 - 20, 2020. 2020 Association for Computational Linguistics awara et al., 2020; Jia and Liang, 2017). Existing approaches to improve generalization in QA either are only applicable when there exist multiple training domains (Talmor and Berant, 2019; Takahashi et al., 2019; Lee et al., 2019) or rely on models and ensembles with larger capacity (Longpre et al., 2019; Su et al., 2019; Li et al., 2019). In contrast, our novel debiasing approach can be applied to both single and multi-domain scenarios, and it improves the model generalization without requiring larger pre-trained language models. We compare our framework with the two stateof-the-art debiasing methods of Utama et al. (2020) and Mahabadi et al. (2020). We study its impact in two different scenarios where the model is trained on a single domain, or multiple domains simultaneously. Our results show the effectiveness of our framework compared to other debiasing methods, e.g., when the model is trained on a single"
2020.findings-emnlp.97,P19-1284,0,0.0191389,"hristopher , were acrimonious . [...] MultiRC Question What are we seeing when we see lightning ? Answer The discharge of electrons (TRUE) Document [...] Over time the differences increase . (R1) Eventually the electrons are discharged . This is what we see as lightning . You can watch an awesome slow - motion lightning strike below . [...] Figure 2: While the example from FEVER provides two alternative single-sentence rationales (R1 and R2), the MultiRC example requires considering two sentences at once for a single rationale (R1). to identify token–level rationales to avoid using REINFORCE (Bastings et al., 2019; Pfeiffer et al., 2019). Very recent work (Jain et al., 2020) aims similarly to us, to infer faithful rationales based on its impact on the target prediction without supervision, thereby relying on a dedicated explanation technique to identify rationales and an additional model for the prediction. This work is different in that we (a) rely on the same network weights for rationale selection and target prediction, and (b) provide quantitative analysis about the decision criteria of the models on the reasoning tasks. 3 3.1 Experimental Setup Datasets We conduct our experiments on three differen"
2020.findings-emnlp.97,D18-1407,0,0.0476448,"Missing"
2020.findings-emnlp.97,N18-2017,0,0.0414217,"Missing"
2020.findings-emnlp.97,2020.acl-main.386,0,0.129443,"ates. We show that this end–to–end trainable model (see Figure 1) can compete with a standard BERT on two reasoning tasks without rationale–supervision, and even slightly improve upon it, when supervised towards gold rationales. Our quantitative analysis shows how we can exploit these extracted rationales to identify the model’s decision boundaries and annotation artifacts of a multi–hop reasoning dataset. 2 Related Work Understanding the deep neural networks’ decisions has gained increasing interest in the research community (DeYoung et al., 2020; Alishahi et al., 2019; Wallace et al., 2019; Jacovi and Goldberg, 2020). Several works are concerned with post–hoc techniques to explain decisions of blackbox models (Ribeiro et al., 2016; Feng et al., 2018; Camburu et al., 2019). Visualizing attention weights has been heavily used, but is known to be insufficient (Jain and Wallace, 2019; Serrano and Smith, 2019). Other works focus on making the models themselves more interpretable via neural module networks (Jiang and Bansal, 2019; Gupta et al., 2020), graph–based networks (Tu et al., 2019; Qiu et al., 2019), pipeline models (Lehman et al., 2019), or by generating textual explanations (Camburu et al., 2018; Raja"
2020.findings-emnlp.97,N19-1357,0,0.025431,"exploit these extracted rationales to identify the model’s decision boundaries and annotation artifacts of a multi–hop reasoning dataset. 2 Related Work Understanding the deep neural networks’ decisions has gained increasing interest in the research community (DeYoung et al., 2020; Alishahi et al., 2019; Wallace et al., 2019; Jacovi and Goldberg, 2020). Several works are concerned with post–hoc techniques to explain decisions of blackbox models (Ribeiro et al., 2016; Feng et al., 2018; Camburu et al., 2019). Visualizing attention weights has been heavily used, but is known to be insufficient (Jain and Wallace, 2019; Serrano and Smith, 2019). Other works focus on making the models themselves more interpretable via neural module networks (Jiang and Bansal, 2019; Gupta et al., 2020), graph–based networks (Tu et al., 2019; Qiu et al., 2019), pipeline models (Lehman et al., 2019), or by generating textual explanations (Camburu et al., 2018; Rajani et al., 2019; Liu et al., 2019a). Rather than only producing this explanation as additional output, Latcinnik and Berant (2020) base the target prediction on this automatically created hypothesis. Some approaches jointly use rationales to explain the predictions an"
2020.findings-emnlp.97,D19-1401,0,0.0209971,"elves more interpretable via neural module networks (Jiang and Bansal, 2019; Gupta et al., 2020), graph–based networks (Tu et al., 2019; Qiu et al., 2019), pipeline models (Lehman et al., 2019), or by generating textual explanations (Camburu et al., 2018; Rajani et al., 2019; Liu et al., 2019a). Rather than only producing this explanation as additional output, Latcinnik and Berant (2020) base the target prediction on this automatically created hypothesis. Some approaches jointly use rationales to explain the predictions and boost performance without ensuring faithfulness (Zaidan et al., 2007; Melamud et al., 2019; Strout et al., 2019). Recent work use Gumbel Softmax (Maddison et al., 2016) FEVER Claim Joan Crawford has had four marriages. (SUPPORTS) Document [...] Following a public appearance in 1974 , after which unflattering photographs were published , Crawford withdrew from public life and became increasingly reclusive until her death in 1977 . (R1) Crawford married four times . (R2) Her first three marriages ended in divorce ; the last ended with the death of husband Alfred Steele . Crawford ’s relationships with her two older children , Christina and Christopher , were acrimonious . [...] Multi"
2020.findings-emnlp.97,P19-1416,0,0.132638,"plit into its sentences (1), each individually encoded via BERT (2) followed by a linear layer (3). The loss for each input part is calculated separately (4,5). The score is computed via max–pooling (6), normalized (7) to compute the weighted loss (8). The input part with the highest score (6) is used for prediction. solving the task based on each sub–sample individually. Similar to Clark and Gardner (2018), each sub–sample is associated with a learned score. Our model utilizes this score to jointly predict the target and the rationale. Instead of learning these scores via direct supervision (Min et al., 2019), our approach can derive them solely based on how useful each rationale is for solving the target task. Single–Sentence without Rationale Supervision Given a sample, the model must predict the label y based on a query q, i.e., the concatenation of the question and answer (MultiRC) or the claim (FEVER), and a document D. Instead of optimizing the objective given (q, D), we split D into segments and solve the overall task for each segment individually. We opt to split each document into sentences, as a trade-off between capturing enough semantic information within each segment while restricting"
2020.findings-emnlp.97,P19-1560,0,0.0788978,"ut labore et dolore magna aliqua. Robbie Collins is a British film critic. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquid ex ea ommodi consequat. Document Select Robbie Collins is a British film critic. Faithful explanation Predict SUPPORT Target label Figure 1: Example of the proposed rationale selecting process on one of the datasets (FEVER): Given a query and a document, our model selects the best rationale and predicts the label solely based on this selection. Introduction Large pre-trained language models, such as BERT (Devlin et al., 2018) or RoBERTa (Liu et al., 2019b) gain impressive results on a large variety of NLP tasks, including reasoning and inference (Rogers et al., 2020). Despite this success, research shows that their strong performance can rely, to some extent, on dataset–specific artifacts and not necessarily on the ability to solve the underlying task (Gururangan et al., 2018; Schuster et al., 2019; Gardner et al., 2020). Thus, these observations undermine the models’ trustworthiness and impede 1 Code available at https://github.com/UKPLab/ emnlp2020-faithful-rationales their deployment in situations where ‘blindly trusting’ the model is deem"
2020.findings-emnlp.97,N16-3020,0,0.263873,"hful explanations – the identification of the actual reason for the model’s prediction, which is essential for accountability, fairness, and credibility (Chakraborty et al., 2017; Wu and Mooney, 2019) to evaluate whether a model’s prediction is based on the correct evidence. The recently published ERASER benchmark (DeYoung et al., 2020) provides multiple datasets with annotated rationales, i.e., parts of the input document, which are essential for correct predictions of the target variable (Zaidan et al., 2007). By contrast to post-hoc techniques to identify relevant input parts such as LIME (Ribeiro et al., 2016) or input reduction (Feng et al., 2018), we focus on models that are faithful by design, in which the selected rationale matches the full underlying evidence used for the prediction. Existing strategies mostly rely on REINFORCE (Williams, 1992) style learning (Lei et al., 2016; Yu et al., 2019) or on 1080 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1080–1095 c November 16 - 20, 2020. 2020 Association for Computational Linguistics training two disjoint models (Lehman et al., 2019; DeYoung et al., 2020), in the latter case depending on rationale supervision. This"
2020.findings-emnlp.97,2021.ccl-1.108,0,0.161107,"Missing"
2020.findings-emnlp.97,D19-1341,0,0.249267,"cess on one of the datasets (FEVER): Given a query and a document, our model selects the best rationale and predicts the label solely based on this selection. Introduction Large pre-trained language models, such as BERT (Devlin et al., 2018) or RoBERTa (Liu et al., 2019b) gain impressive results on a large variety of NLP tasks, including reasoning and inference (Rogers et al., 2020). Despite this success, research shows that their strong performance can rely, to some extent, on dataset–specific artifacts and not necessarily on the ability to solve the underlying task (Gururangan et al., 2018; Schuster et al., 2019; Gardner et al., 2020). Thus, these observations undermine the models’ trustworthiness and impede 1 Code available at https://github.com/UKPLab/ emnlp2020-faithful-rationales their deployment in situations where ‘blindly trusting’ the model is deemed irresponsible (Sokol and Flach, 2020). Explainability has thus emerged as an increasingly popular field (Gilpin et al., 2018; Guidotti et al., 2018). We aim at faithful explanations – the identification of the actual reason for the model’s prediction, which is essential for accountability, fairness, and credibility (Chakraborty et al., 2017; Wu a"
2020.findings-emnlp.97,P19-1282,0,0.020869,"rationales to identify the model’s decision boundaries and annotation artifacts of a multi–hop reasoning dataset. 2 Related Work Understanding the deep neural networks’ decisions has gained increasing interest in the research community (DeYoung et al., 2020; Alishahi et al., 2019; Wallace et al., 2019; Jacovi and Goldberg, 2020). Several works are concerned with post–hoc techniques to explain decisions of blackbox models (Ribeiro et al., 2016; Feng et al., 2018; Camburu et al., 2019). Visualizing attention weights has been heavily used, but is known to be insufficient (Jain and Wallace, 2019; Serrano and Smith, 2019). Other works focus on making the models themselves more interpretable via neural module networks (Jiang and Bansal, 2019; Gupta et al., 2020), graph–based networks (Tu et al., 2019; Qiu et al., 2019), pipeline models (Lehman et al., 2019), or by generating textual explanations (Camburu et al., 2018; Rajani et al., 2019; Liu et al., 2019a). Rather than only producing this explanation as additional output, Latcinnik and Berant (2020) base the target prediction on this automatically created hypothesis. Some approaches jointly use rationales to explain the predictions and boost performance withou"
2020.findings-emnlp.97,2020.emnlp-main.665,0,0.0872006,"Missing"
2020.findings-emnlp.97,W19-4807,0,0.0354077,"Missing"
2020.findings-emnlp.97,N18-1074,0,0.0614371,"Missing"
2020.findings-emnlp.97,D19-3002,0,0.0273688,"Missing"
2020.findings-emnlp.97,K19-1065,0,0.0215918,"vision.. for the negative predictions. Whereas model U tends to select rationales for both labels based on similar criteria, the selected rationales for samples predicted False by model S almost entirely have lexical overlaps with the question only. This intuitively makes sense, as the same rationales are valid for each question. Negative rationales should therefore be relevant for the question, not for the answer. We show some examples in Appendix C. Are single sentences sufficient for MultiRC? It has been shown that noisy detection of evidence can already improve the performance on MultiRC (Wang et al., 2019), yet this should not be possible via single sentences. To see whether BERT exploits such biases, we follow Gururangan et al. (2018) and identify samples within the test–set that are solvable using a single–hop only, i.e., these which the single–sentence U model classified correctly. To limit the impact of lucky guesses, we group samples by the number of these models that could solve them in Table 5. As pointed out in Section 4, one of our single–sentence models U on MultiRC performed poorly due to its seed sensitivity . To exclude impacts from this specific model and group the test–split by m"
2020.findings-emnlp.97,W19-4812,0,0.0217749,"2019; Gardner et al., 2020). Thus, these observations undermine the models’ trustworthiness and impede 1 Code available at https://github.com/UKPLab/ emnlp2020-faithful-rationales their deployment in situations where ‘blindly trusting’ the model is deemed irresponsible (Sokol and Flach, 2020). Explainability has thus emerged as an increasingly popular field (Gilpin et al., 2018; Guidotti et al., 2018). We aim at faithful explanations – the identification of the actual reason for the model’s prediction, which is essential for accountability, fairness, and credibility (Chakraborty et al., 2017; Wu and Mooney, 2019) to evaluate whether a model’s prediction is based on the correct evidence. The recently published ERASER benchmark (DeYoung et al., 2020) provides multiple datasets with annotated rationales, i.e., parts of the input document, which are essential for correct predictions of the target variable (Zaidan et al., 2007). By contrast to post-hoc techniques to identify relevant input parts such as LIME (Ribeiro et al., 2016) or input reduction (Feng et al., 2018), we focus on models that are faithful by design, in which the selected rationale matches the full underlying evidence used for the predicti"
2020.findings-emnlp.97,D19-1260,0,0.0225394,"ion of both models U and S and observe a high correlation with word overlap of the question and the answer. Figure 7 shows KDE plots of the selected sentences based on the percentage of non–stopwords8 of the question and answer respectively, that are also contained within the selected sentence. We make multiple observations: Positive predictions mostly depend on a high overlap with the answer. The overlap with the question has a lower priority. Especially for the model S, a clear decision boundary between rationales for both labels can be seen based on the lexical overlap. Interestingly, also Yadav et al. (2019), to a large part, rely on similar lexical features for their unsupervised detection of justification sentences on MultiRC. In line with the previous section, rationale supervision only has a limited impact on positive predictions. A significant difference is shown 7 Compared to 54.5 (True) and 79.3 (False) We use spaCy to exclude punctuation and stopwords and seaborn (Waskom et al., 2017) with default parameters for plotting. 8 Figure 7: KDE plots for word overlaps between Question/Answer and the selected rationale of single– sentence models on MultiRC with (bottom) and without (top) rational"
2020.findings-emnlp.97,D18-1010,0,0.0218107,"the highest confidence from all sentences as the rationale rˆ, and the prediction based on rˆ as the target yˆ: rˆ = argmax(w); yˆ = argmax(zrˆ) Though the rationale is faithful on a sentence–level, we note that it does not indicate whether all information of rˆ is relevant to the model. Rationale supervision We believe that rationales without supervision provide more trustworthy explanations. They are not affected by an additional objective and solely are selected if they are useful for the target task. Nevertheless, we experimentally show how rationale–supervision can be applied by jointly (Yin and Roth, 2018) supervising on the target and rationale. To compute the rationale–loss as an additional objective, we treat slightly adapted confidence values c∗k as a multi– label problem via a sigmoid layer and binary cross– entropy loss.  wk,i lk,i (4) c∗k,i k=1 i=1 The rationale behind this is threefold: A right prediction, i.e., a low loss lk,i , is only possible for informative sentences from the model’s perspective. First, by allowing the model to distribute the weights for the losses amongst all candidates, it can neglect non–informative sentences when learning to assign low values (to high losses)."
2020.findings-emnlp.97,D19-1420,0,0.0400357,"Missing"
2020.findings-emnlp.97,N07-1033,0,0.0784151,"Missing"
2020.tacl-1.38,P18-1026,0,0.167178,"istical methods (Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2017). Recently, several neural graph-to-text models have exhibited success by leveraging encoder mechanisms based on LSTMs, GNNs, and Transformers. AMR-to-Text Generation. Various neural models have been proposed to generate sentences from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural approach for this task, by linearizing the input graph as a sequence of nodes and edges. Song et al. (2018) propose the graph recurrent network to directly encode the AMR nodes, whereas Beck et al. (2018) develop a model based on gated GNNs. 590 However, both approaches only use local node aggregation strategies. Damonte and Cohen (2019) combine graph convolutional networks and LSTMs in order to learn complementary node contexts. However, differently from Transformers and GNNs, LSTMs generate node representations that are influenced by the node order. Ribeiro et al. (2019) develop a model based on different GNNs that learns node representations which simultaneously encode a top–down and a bottom–up views of the AMR graphs, whereas Guo et al. (2019) leverage dense connectivity in GNNs. Recently"
2020.tacl-1.38,D19-1052,0,0.0837704,"Missing"
2020.tacl-1.38,D14-1179,0,0.026513,"Missing"
2020.tacl-1.38,N16-1087,0,0.0487873,"ing distant connections between all nodes, we allow for these missing links to be captured, as KGs are known to be highly incomplete (Dong et al., 2014; Schlichtkrull et al., 2018). In contrast, the local strategy refines the node representation with richer neighborhood information, as nodes that share the same neighborhood exhibit a strong homophily: Two similar entities are much more likely to be connected than at random. Consequently, the local context enriches the node representation with local information 2 Related Work Early efforts for graph-to-text generation used statistical methods (Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2017). Recently, several neural graph-to-text models have exhibited success by leveraging encoder mechanisms based on LSTMs, GNNs, and Transformers. AMR-to-Text Generation. Various neural models have been proposed to generate sentences from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural approach for this task, by linearizing the input graph as a sequence of nodes and edges. Song et al. (2018) propose the graph recurrent network to directly encode the AMR nodes, whereas Beck et al. (2018) develop a model bas"
2020.tacl-1.38,D18-1113,1,0.856445,"nerating text from KGs. In comparison to AMRs, which are rooted and connected graphs, KGs do not have a defined topology, which may vary widely among different datasets, making the generation process more demanding. KGs are sparse structures that potentially contain a large number of relations. Moreover, we are typically interested in generating multisentence texts from KGs, and this involves solving document planning issues (Konstas and Lapata, 2013). Recent neural approaches for KG-to-text generation simply linearize the KG triples, thereby loosing graph structure information. For instance, Colin and Gardent (2018), Moryossef et al. (2019), and Adapt (Gardent et al., 2017) utilize LSTM/ GRU to encode WebNLG graphs. Castro Ferreira et al. (2019) systematically compare pipeline and end-to-end models for text generation from WebNLG graphs. Trisedya et al. (2018) develop a graph encoder based on LSTMs that captures relationships within and between triples. Previous work has also studied how to explicitly encode the graph structure using GNNs or Transformers. Marcheggiani and Perez Beltrachini (2018) propose an encoder based on graph convolutional networks, that consider explicitly local node contexts, and s"
2020.tacl-1.38,W17-3518,1,0.256222,"F. R. Ribeiro† , Yue Zhang‡ , Claire Gardent§ and Iryna Gurevych† † Research Training Group AIPHES and UKP Lab, Technische Universit¨at Darmstadt ‡ School of Engineering, Westlake University, § CNRS/LORIA, Nancy, France ribeiro@aiphes.tu-darmstadt.de, yue.zhang@wias.org.cn claire.gardent@loria.fr, gurevych@ukp.informatik.tu-darmstadt.de Abstract be ordered and connected using appropriate discourse markers; and inter-sentential anaphora and ellipsis may need to be generated to avoid repetition. In this paper, we focus on generating texts rather than sentences where the output are short texts (Gardent et al., 2017) or paragraphs (KoncelKedziorski et al., 2019). A key issue in neural graph-to-text generation is how to encode the input graphs. The basic idea is to incrementally compute node representations by aggregating structural context information. To this end, two main approaches have been proposed: (i) models based on local node aggregation, usually built upon Graph Neural Networks (GNNs) (Kipf and Welling, 2017; Hamilton et al., 2017) and (ii) models that leverage global node aggregation. Systems that adopt global encoding strategies are typically based on Transformers (Vaswani et al. 2017), using"
2020.tacl-1.38,N19-1366,0,0.0564672,"Missing"
2020.tacl-1.38,W14-3348,0,0.0087213,"That is, we create a new relation node for each edge relation between two nodes. The new relation node is connected to the subject and object token entities by two binary relations, respectively. seeds, for the test sets, we report the averages over 4 training runs along with their standard deviation. We use byte pair encoding (Sennrich et al., 2016) to split entity words into smaller more frequent pieces. Therefore some nodes in the graph can be sub-words. We also obtain sub-words on the target side. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and CHRF++ (Popovi´c, 2015) automatic metrics and also perform a human evaluation (Section 5.6). For layer-wise models, the number of encoder layers are chosen from {2, 4, 6}, and for PGE and CGE, the global and local layers are chosen from and {2, 4, 6} and {1, 2, 3}, respectively. The hidden encoder dimensions are chosen from {256, 384, 448} (see Figure 3). Hyperparameters are tuned on the development set of both datasets. We report the test results when the BLEU score on dev set is optimal. 5 Experiments 5.1 Results on AGENDA We implemented all our models using PyTorch Geometric (PyG) (Fe"
2020.tacl-1.38,P17-4012,0,0.0669681,"Missing"
2020.tacl-1.38,P02-1040,0,0.109526,"n entities (Beck et al., 2018). That is, we create a new relation node for each edge relation between two nodes. The new relation node is connected to the subject and object token entities by two binary relations, respectively. seeds, for the test sets, we report the averages over 4 training runs along with their standard deviation. We use byte pair encoding (Sennrich et al., 2016) to split entity words into smaller more frequent pieces. Therefore some nodes in the graph can be sub-words. We also obtain sub-words on the target side. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and CHRF++ (Popovi´c, 2015) automatic metrics and also perform a human evaluation (Section 5.6). For layer-wise models, the number of encoder layers are chosen from {2, 4, 6}, and for PGE and CGE, the global and local layers are chosen from and {2, 4, 6} and {1, 2, 3}, respectively. The hidden encoder dimensions are chosen from {256, 384, 448} (see Figure 3). Hyperparameters are tuned on the development set of both datasets. We report the test results when the BLEU score on dev set is optimal. 5 Experiments 5.1 Results on AGENDA We implemented all our mode"
2020.tacl-1.38,N19-1238,0,0.214575,"Missing"
2020.tacl-1.38,W15-3049,0,0.0541798,"Missing"
2020.tacl-1.38,P17-1014,0,0.167206,"odels that encode an input graph combining both global and local node contexts, in order to learn better contextualized node embeddings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-totext datasets achieving BLEU scores of 18.01 on the AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.1 1 Introduction Graph-to-text generation refers to the task of generating natural language text from input graph structures, which can be semantic representations (Konstas et al., 2017) or knowledge graphs (KGs) (Gardent et al., 2017; Koncel-Kedziorski et al., 2019). Whereas most recent work (Song et al., 2018; Ribeiro et al., 2019; Guo et al., 2019) focuses on generating sentences, a more challenging and interesting scenario emerges when the goal is to generate multisentence texts. In this context, in addition to sentence generation, document planning needs to be handled: The input needs to be mapped into several sentences; sentences need to 1 Code is available at https://github.com/UKPLab/ kg2text. 589 Transactions of the Association for Computational Linguistics, vol. 8,"
2020.tacl-1.38,W16-6603,0,0.0488296,"between all nodes, we allow for these missing links to be captured, as KGs are known to be highly incomplete (Dong et al., 2014; Schlichtkrull et al., 2018). In contrast, the local strategy refines the node representation with richer neighborhood information, as nodes that share the same neighborhood exhibit a strong homophily: Two similar entities are much more likely to be connected than at random. Consequently, the local context enriches the node representation with local information 2 Related Work Early efforts for graph-to-text generation used statistical methods (Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2017). Recently, several neural graph-to-text models have exhibited success by leveraging encoder mechanisms based on LSTMs, GNNs, and Transformers. AMR-to-Text Generation. Various neural models have been proposed to generate sentences from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural approach for this task, by linearizing the input graph as a sequence of nodes and edges. Song et al. (2018) propose the graph recurrent network to directly encode the AMR nodes, whereas Beck et al. (2018) develop a model based on gated GNNs. 590 Howev"
2020.tacl-1.38,D13-1157,0,0.0618111,"ransformers, but learn globalized node representations, modeling graph paths in order to capture structural relations. KG-to-Text Generation. In this work, we focus on generating text from KGs. In comparison to AMRs, which are rooted and connected graphs, KGs do not have a defined topology, which may vary widely among different datasets, making the generation process more demanding. KGs are sparse structures that potentially contain a large number of relations. Moreover, we are typically interested in generating multisentence texts from KGs, and this involves solving document planning issues (Konstas and Lapata, 2013). Recent neural approaches for KG-to-text generation simply linearize the KG triples, thereby loosing graph structure information. For instance, Colin and Gardent (2018), Moryossef et al. (2019), and Adapt (Gardent et al., 2017) utilize LSTM/ GRU to encode WebNLG graphs. Castro Ferreira et al. (2019) systematically compare pipeline and end-to-end models for text generation from WebNLG graphs. Trisedya et al. (2018) develop a graph encoder based on LSTMs that captures relationships within and between triples. Previous work has also studied how to explicitly encode the graph structure using GNNs"
2020.tacl-1.38,D19-1314,1,0.905479,"iments, we demonstrate that our approaches lead to significant improvements on two graph-totext datasets achieving BLEU scores of 18.01 on the AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.1 1 Introduction Graph-to-text generation refers to the task of generating natural language text from input graph structures, which can be semantic representations (Konstas et al., 2017) or knowledge graphs (KGs) (Gardent et al., 2017; Koncel-Kedziorski et al., 2019). Whereas most recent work (Song et al., 2018; Ribeiro et al., 2019; Guo et al., 2019) focuses on generating sentences, a more challenging and interesting scenario emerges when the goal is to generate multisentence texts. In this context, in addition to sentence generation, document planning needs to be handled: The input needs to be mapped into several sentences; sentences need to 1 Code is available at https://github.com/UKPLab/ kg2text. 589 Transactions of the Association for Computational Linguistics, vol. 8, pp. 589–604, 2020. https://doi.org/10.1162/tacl a 00332 Action Editor: Alessandro Moschitti. Submission batch: 2/2019; Revision batch: 5/2020; Publi"
2020.tacl-1.38,W18-6501,0,0.122303,"Missing"
2020.tacl-1.38,P16-1162,0,0.0556469,"osion, we use regularization based on the basis function decomposition to define the model relation weights (Schlichtkrull et al., 2018). Also, as an alternative, we use the Levi Transformation to create nodes from relational edges between entities (Beck et al., 2018). That is, we create a new relation node for each edge relation between two nodes. The new relation node is connected to the subject and object token entities by two binary relations, respectively. seeds, for the test sets, we report the averages over 4 training runs along with their standard deviation. We use byte pair encoding (Sennrich et al., 2016) to split entity words into smaller more frequent pieces. Therefore some nodes in the graph can be sub-words. We also obtain sub-words on the target side. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and CHRF++ (Popovi´c, 2015) automatic metrics and also perform a human evaluation (Section 5.6). For layer-wise models, the number of encoder layers are chosen from {2, 4, 6}, and for PGE and CGE, the global and local layers are chosen from and {2, 4, 6} and {1, 2, 3}, respectively. The hidden encoder dimensions are chosen"
2020.tacl-1.38,P16-1000,0,0.208449,"Missing"
2020.tacl-1.38,P17-2002,1,0.873213,"w for these missing links to be captured, as KGs are known to be highly incomplete (Dong et al., 2014; Schlichtkrull et al., 2018). In contrast, the local strategy refines the node representation with richer neighborhood information, as nodes that share the same neighborhood exhibit a strong homophily: Two similar entities are much more likely to be connected than at random. Consequently, the local context enriches the node representation with local information 2 Related Work Early efforts for graph-to-text generation used statistical methods (Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2017). Recently, several neural graph-to-text models have exhibited success by leveraging encoder mechanisms based on LSTMs, GNNs, and Transformers. AMR-to-Text Generation. Various neural models have been proposed to generate sentences from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural approach for this task, by linearizing the input graph as a sequence of nodes and edges. Song et al. (2018) propose the graph recurrent network to directly encode the AMR nodes, whereas Beck et al. (2018) develop a model based on gated GNNs. 590 However, both approaches"
2020.tacl-1.38,2020.tacl-1.2,0,0.0329849,"evelop a model based on gated GNNs. 590 However, both approaches only use local node aggregation strategies. Damonte and Cohen (2019) combine graph convolutional networks and LSTMs in order to learn complementary node contexts. However, differently from Transformers and GNNs, LSTMs generate node representations that are influenced by the node order. Ribeiro et al. (2019) develop a model based on different GNNs that learns node representations which simultaneously encode a top–down and a bottom–up views of the AMR graphs, whereas Guo et al. (2019) leverage dense connectivity in GNNs. Recently, Wang et al. (2020) propose a local graph encoder based on Transformers using separated attentions for incoming and outgoing neighbors. Recent methods (Zhu et al., 2019; Cai and Lam, 2020) also use Transformers, but learn globalized node representations, modeling graph paths in order to capture structural relations. KG-to-Text Generation. In this work, we focus on generating text from KGs. In comparison to AMRs, which are rooted and connected graphs, KGs do not have a defined topology, which may vary widely among different datasets, making the generation process more demanding. KGs are sparse structures that pot"
2020.tacl-1.38,P18-1150,1,0.930784,"dings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-totext datasets achieving BLEU scores of 18.01 on the AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.1 1 Introduction Graph-to-text generation refers to the task of generating natural language text from input graph structures, which can be semantic representations (Konstas et al., 2017) or knowledge graphs (KGs) (Gardent et al., 2017; Koncel-Kedziorski et al., 2019). Whereas most recent work (Song et al., 2018; Ribeiro et al., 2019; Guo et al., 2019) focuses on generating sentences, a more challenging and interesting scenario emerges when the goal is to generate multisentence texts. In this context, in addition to sentence generation, document planning needs to be handled: The input needs to be mapped into several sentences; sentences need to 1 Code is available at https://github.com/UKPLab/ kg2text. 589 Transactions of the Association for Computational Linguistics, vol. 8, pp. 589–604, 2020. https://doi.org/10.1162/tacl a 00332 Action Editor: Alessandro Moschitti. Submission batch: 2/2019; Revisio"
2020.tacl-1.38,P18-1151,0,0.287663,"ntially contain a large number of relations. Moreover, we are typically interested in generating multisentence texts from KGs, and this involves solving document planning issues (Konstas and Lapata, 2013). Recent neural approaches for KG-to-text generation simply linearize the KG triples, thereby loosing graph structure information. For instance, Colin and Gardent (2018), Moryossef et al. (2019), and Adapt (Gardent et al., 2017) utilize LSTM/ GRU to encode WebNLG graphs. Castro Ferreira et al. (2019) systematically compare pipeline and end-to-end models for text generation from WebNLG graphs. Trisedya et al. (2018) develop a graph encoder based on LSTMs that captures relationships within and between triples. Previous work has also studied how to explicitly encode the graph structure using GNNs or Transformers. Marcheggiani and Perez Beltrachini (2018) propose an encoder based on graph convolutional networks, that consider explicitly local node contexts, and show superior performance compared with LSTMs. Recently, Koncel-Kedziorski et al. (2019) proposed a Transformer-based approach that computes the node representations by attending over node neighborhoods following a self-attention 591 strategy. In con"
2020.tacl-1.38,P18-1030,1,0.83059,"etter node representations in graph-to-text generation. To this end, existing methods use an artificial global node for message exchange with the other nodes. This strategy can be regarded as extending the graph structure but using similar message passing mechanisms. In particular, Koncel-Kedziorski et al. (2019) add a global node to the graph and use its representation to initialize the decoder. Recently, Guo et al. (2019) and Cai and Lam (2020) also utilized an artificial global node with direct edges to all other nodes to allow global message exchange for AMR-to-text generation. Similarly, Zhang et al. (2018) use a global node to a graph recurrent network model for sentence representation. Different from the above methods, we consider integrating global and local contexts at the node level, rather than the graph level, by investigating model alternatives rather than graph structure changes. In addition, we integrate GAT and Transformer architectures into a unified global-local model. 3 Graph-to-Text Model This section first describes (i) the graph transformation adopted to create a relational graph from the input (Section 3.1), and (ii) the graph encoders of our framework based on GAT (Veliˇckovi´"
2020.tacl-1.38,N19-1236,0,\N,Missing
2020.tacl-1.38,Q19-1019,0,\N,Missing
2020.tacl-1.38,D19-1548,0,\N,Missing
2020.tacl-1.38,2020.acl-main.640,0,\N,Missing
2020.tacl-1.49,W04-3202,0,0.040932,"im is to find the best candidate and those with low quality can simply be disregarded rather than ranked precisely. In this paper, we define two BO acquisition functions for interactive text ranking. While our approach is designed to adapt a model to a highly specialized task, generic models can provide hints to help us avoid low-quality candidates. Therefore, we learn the ranking function itself using a Bayesian approach, which integrates prior predictions from a generic model that is not tailored to the user. Previous interactive text ranking methods either do not exploit prior information (Baldridge and Osborne, 2004; P.V.S and Meyer, 2017; Lin and Parikh, 2017; Siddhant and Lipton, 2018), combine heuristics with user feedback after active learning is complete (Gao et al., 2018), or require expensive re-training of a non-Bayesian method (Peris and Casacuberta, 2018). Here, we show how BO can use prior information to expedite interactive text ranking. The interactive learning process is shown in Algorithm 1 and examples of our system outputs are shown in Figures 1 and 2. Our contributions are (1) a Bayesian optimization methodology for interactive text ranking that integrates prior predictions with user fe"
2020.tacl-1.49,D18-1454,0,0.0428691,"Missing"
2020.tacl-1.49,2020.acl-main.124,1,0.869799,"Missing"
2020.tacl-1.49,P13-1004,0,0.0149031,"aries for DUC’04 produced by RL (see Section 5.4) with a reward function learnt from 100 user interactions using (a) the BT, UNC method of Gao et al. (2018) and (b) our GPPL, IMP method. (c) is a model summary written by an expert. Each color indicates a particular news event or topic, showing where it occurs in each summary. Compared to (a), summary (b) covers more of the events discussed in the reference, (c). 2 Related Work BO for Preference Learning. Bayesian approaches using Gaussian processes (GPs) have previously been used to reduce errors in NLP tasks involving sparse or noisy labels (Cohn and Specia, 2013; Beck et al., 2014), making them well-suited to learning from user feedback. Gaussian process preference learning (GPPL) (Chu and Ghahramani 2005) enables GP inference with pairwise preference labels. Simpson and Gurevych (2018) introduced scalable inference for GPPL using stochastic variational inference (SVI) (Hoffman et al., 2013), which outperformed SVM and LSTM methods at ranking arguments by convincingness. They included a study on active learning with pairwise labels, but tested GPPL only with uncertainty sampling, not BO. Here, we adapt GPPL to summarization and cQA, show how to integ"
2020.tacl-1.49,N19-1423,0,0.0232484,"Missing"
2020.tacl-1.49,D18-1054,0,0.0630528,"Missing"
2020.tacl-1.49,P18-1169,0,0.0450431,"Missing"
2020.tacl-1.49,P12-1000,0,0.229746,"Missing"
2020.tacl-1.49,P08-2025,0,0.0468288,"e user to label them. The acquisition function implements one of many different strategies to minimize the number of interaction rounds, such as reducing uncertainty (Settles, 2012) by choosing informative labels that help learn the model more quickly. Introduction Many text ranking tasks are highly specific to an individual user’s topic of interest, which presents a challenge for NLP systems that have not been trained to solve that user’s problem. Consider ranking summaries or answers to non-factoid questions: A good solution requires understanding the topic and the user’s information needs (Liu and Agichtein, 2008; L´opez et al., 1999). We address this by proposing an interactive text ranking approach that efficiently gathers user feedback and combines it with predictions from pretrained, generic models. Many active learning strategies, such as the pairwise preference learning method of Gao et al. (2018), aim to learn a good ranking model for all candidates, for example, by querying the annotator about candidates whose rank is most uncertain. However, we often need to find and rank only a small set of good candidates to present to the user. For instance, in question answering, irrelevant answers should"
2020.tacl-1.49,K18-1015,0,0.0596526,"Missing"
2020.tacl-1.49,P17-1124,0,0.0584376,"Missing"
2020.tacl-1.49,D12-1024,0,0.625214,"Missing"
2021.acl-long.1,W14-2107,0,0.0262513,"al student annotations of our dataset to facilitate further research in this direction. 2 Related Work Label suggestions. In an early work, Rehbein et al. (2009) study the effects of label suggestions on the task of word sense disambiguation and observe a positive effect on annotation quality. With the introduction of annotation tools such as brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), or INCEpTION (Klie et al., 2018), the use of label suggestions became more feasible; leading to an increased investigation of label suggestions in the context of NLP. For instance, Yimam et al. (2014) investigate label suggestions for Amharic POS tagging and German named entity recognition and show with expert annotators 2 we made use of the Twitter Streaming API and gathered only those tweets which were classified as German by the Twitter language identifier. This resulted in a set of approximately 16.5 million tweets. We retained only tweets that contain key terms referring to measures related to the Covid-19 pandemic and removed all duplicates, retweets and all tweets with text length less than 30 characters. After filtering, 237,616 tweets remained and their daily temporal distribution"
2021.acl-long.1,N13-1132,0,0.0590394,"Missing"
2021.acl-long.1,L16-1259,0,0.0201975,"hence, are important for interactive label suggestions in non-expert annotation tasks. 3 Annotation Task Our task is inspired by social science research on analyzing public opinion using social media (Jungherr, 2015; McCormick et al., 2017). The goal is to identify opinions in German-speaking countries about governmental measures established to contain the spread of the Corona virus. We use Twitter due to its international and widespread usage that ensures a sufficient database and the several challenges for the automatic identification of opinions and stance it poses from an NLP perspective (Imran et al., 2016; Mohammad et al., 2016; Gorrell et al., 2019; Conforti et al., 2020). For example, the use of language varies from colloquial expressions to well-formed arguments and newsspreading statements due to its heterogeneous user base. Additionally, hashtags are used directly as part of text but also to embed the tweet itself in the broader discussion on the platform. Finally, the classification of a tweet is particularly challenging given the character limitation of the platform, i.e., at the date of writing Twitter allows for 280 characters per tweet. Unrelated: no measures related to the containme"
2021.acl-long.1,N19-1423,0,0.193863,"r Quiring2 and Iryna Gurevych1 1 Ubiquitous Knowledge Processing Lab, Technical University of Darmstadt 2 Institut f¨ur Publizistik, Johannes Gutenberg-University Mainz www.ukp.tu-darmstadt.de www.ifp.uni-mainz.de Abstract interviews or questionnaires. However, the publication of research results is often delayed or temporally transient due to limitations of traditional social science research, i.e. prolonged data gathering processes or opinion surveys being subject to reactivity. Given the increasing performance of language models trained on large amounts of data in a self-supervised manner (Devlin et al., 2019; Brown et al., 2020), one fundamental question that arises is how NLP systems can contribute to alleviate existing difficulties in studies for digital humanities and social sciences (Risch et al., 2019). One important approach to make data annotation more efficient is the use of automated label suggestions. In contrast to active learning, that aims to identify a subset of annotated data which leads to optimal model training, label suggestions alleviate the annotation process by providing annotators with pre-annotations (i.e., predictions) from a model (Ringger et al., 2008; Schulz et al., 201"
2021.acl-long.1,C18-2002,1,0.897035,"Missing"
2021.acl-long.1,W10-1807,0,0.473811,"estions alleviate the annotation process by providing annotators with pre-annotations (i.e., predictions) from a model (Ringger et al., 2008; Schulz et al., 2019). To enable the annotation of large amounts of data which are used for quantitative analysis by disciplines such as social sciences, label suggestions are a more viable solution than active learning. One major difficulty with label suggestions is the danger of biasing annotators towards (possibly erroneous) suggestions. So far, researchers have investigated automated label suggestions for tasks that require domain-specific knowledge (Fort and Sagot, 2010; Yimam et al., 2013; Schulz et al., 2019); and have shown that domain experts successfully identify erroneous suggestions and are more robust to potential biases. However, the limited availability of such expert annotators restricts the use of label suggestions to small, focused annotation studies. For tasks that do not require domain-specific knowledge and can be conducted with non-expert annotators – such as crowd workers or citizen science volunteers – on a large scale, label suggestions have not been considered yet. This leads to This work investigates the use of interactively updated lab"
2021.acl-long.1,2020.acl-main.624,1,0.840681,"Missing"
2021.acl-long.1,S19-2147,0,0.0204254,"suggestions in non-expert annotation tasks. 3 Annotation Task Our task is inspired by social science research on analyzing public opinion using social media (Jungherr, 2015; McCormick et al., 2017). The goal is to identify opinions in German-speaking countries about governmental measures established to contain the spread of the Corona virus. We use Twitter due to its international and widespread usage that ensures a sufficient database and the several challenges for the automatic identification of opinions and stance it poses from an NLP perspective (Imran et al., 2016; Mohammad et al., 2016; Gorrell et al., 2019; Conforti et al., 2020). For example, the use of language varies from colloquial expressions to well-formed arguments and newsspreading statements due to its heterogeneous user base. Additionally, hashtags are used directly as part of text but also to embed the tweet itself in the broader discussion on the platform. Finally, the classification of a tweet is particularly challenging given the character limitation of the platform, i.e., at the date of writing Twitter allows for 280 characters per tweet. Unrelated: no measures related to the containment of the pandemic are mentioned Comment: mea"
2021.acl-long.1,E12-2021,0,0.144026,"Missing"
2021.acl-long.1,L16-1623,0,0.0257616,"for interactive label suggestions in non-expert annotation tasks. 3 Annotation Task Our task is inspired by social science research on analyzing public opinion using social media (Jungherr, 2015; McCormick et al., 2017). The goal is to identify opinions in German-speaking countries about governmental measures established to contain the spread of the Corona virus. We use Twitter due to its international and widespread usage that ensures a sufficient database and the several challenges for the automatic identification of opinions and stance it poses from an NLP perspective (Imran et al., 2016; Mohammad et al., 2016; Gorrell et al., 2019; Conforti et al., 2020). For example, the use of language varies from colloquial expressions to well-formed arguments and newsspreading statements due to its heterogeneous user base. Additionally, hashtags are used directly as part of text but also to embed the tweet itself in the broader discussion on the platform. Finally, the classification of a tweet is particularly challenging given the character limitation of the platform, i.e., at the date of writing Twitter allows for 280 characters per tweet. Unrelated: no measures related to the containment of the pandemic are"
2021.acl-long.1,W09-3003,0,0.0551839,"ice annotators is common for sceC1: An evaluation of label suggestions in terms of annotation quality for non-expert annotators. C2: An investigation of label suggestion bias for both static and interactively updated suggestions. C3: A novel corpus of German Twitter posts that can be used by social science researchers to study the effects of governmental measures against Covid-19 on the public opinion. Finally, we also publish 200 expert and 2,785 individual student annotations of our dataset to facilitate further research in this direction. 2 Related Work Label suggestions. In an early work, Rehbein et al. (2009) study the effects of label suggestions on the task of word sense disambiguation and observe a positive effect on annotation quality. With the introduction of annotation tools such as brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), or INCEpTION (Klie et al., 2018), the use of label suggestions became more feasible; leading to an increased investigation of label suggestions in the context of NLP. For instance, Yimam et al. (2014) investigate label suggestions for Amharic POS tagging and German named entity recognition and show with expert annotators 2 we made use of the Twitter Str"
2021.acl-long.1,ringger-etal-2008-assessing,0,0.0379268,"f-supervised manner (Devlin et al., 2019; Brown et al., 2020), one fundamental question that arises is how NLP systems can contribute to alleviate existing difficulties in studies for digital humanities and social sciences (Risch et al., 2019). One important approach to make data annotation more efficient is the use of automated label suggestions. In contrast to active learning, that aims to identify a subset of annotated data which leads to optimal model training, label suggestions alleviate the annotation process by providing annotators with pre-annotations (i.e., predictions) from a model (Ringger et al., 2008; Schulz et al., 2019). To enable the annotation of large amounts of data which are used for quantitative analysis by disciplines such as social sciences, label suggestions are a more viable solution than active learning. One major difficulty with label suggestions is the danger of biasing annotators towards (possibly erroneous) suggestions. So far, researchers have investigated automated label suggestions for tasks that require domain-specific knowledge (Fort and Sagot, 2010; Yimam et al., 2013; Schulz et al., 2019); and have shown that domain experts successfully identify erroneous suggestio"
2021.acl-long.1,2020.emnlp-demos.6,0,0.0468919,"Missing"
2021.acl-long.1,W13-2321,0,0.0251069,"are conveyed to machine learning models (Gururangan et al., 2018). One possible source of bias may be due to the different decision making process triggered by label suggestions – namely, first deciding if the suggested label is correct and only if not, considering different labels (Turner and Schley, 2016). Hence, the key question that arises is to what extent annotators are influenced by such suggestions. Although Fort and Sagot (2010) identify an influence on annotation behaviour when providing pre-annotated data for POS-tagging, they do not measure any clear bias in the annotated labels. Rosset et al. (2013) come to a similar conclusion when investigating the bias introduced by label suggestions in a cross-domain setup, i.e., when using label suggestions from a model that is trained on data from a different domain than the annotated data. They conduct their experiments with eight annotators from varying levels of expertise and report considerable annotation performance gains while not finding considerable biases introduced by label suggestions. Most similar to our work is the setup from Schulz et al. (2019). The authors investigate interactive label suggestions for expert annotators across two do"
2021.acl-long.1,P14-5016,1,0.862016,"Missing"
2021.acl-long.1,2020.argmining-1.6,0,0.0548444,"Missing"
2021.acl-long.1,P13-4001,1,0.871717,"Missing"
2021.acl-long.243,2020.emnlp-main.367,0,0.649627,"e model capacity (Artetxe et al., 2020; Pfeiffer et al., 2020b; Chau et al., 2020) or through additional training for particular language pairs (Pfeiffer et al., 2020b; Ponti et al., 2020). Another observation concerns substantially reduced crosslingual and monolingual abilities of the models for resource-poor languages with smaller pretraining data (Wu and Dredze, 2020; Hu et al., 2020; Lauscher et al., 2020). Those languages remain underrepresented in the subword vocabulary and the model’s shared representation space despite oversampling. Despite recent efforts to mitigate this issue (e.g., Chung et al. (2020) propose to cluster and merge the vocabularies of similar languages, before defining a joint vocabulary across all languages), the multilingual LMs still struggle with balancing their parameters across many languages. Monolingual versus Multilingual LMs. New monolingual language-specific models also emerged for many languages, following BERT’s architecture and pretraining procedure. There are monolingual BERT variants for Arabic (Antoun et al., 2020), French (Martin et al., 2020), Finnish (Virtanen et al., 2019), Dutch (de Vries et al., 2019), to name only a few. Pyysalo et al. (2020) released"
2021.acl-long.243,2020.tacl-1.30,0,0.117373,"ng) criteria: C1) typological diversity; C2) availability of pretrained monolingual BERT models; C3) representation of the languages in standard evaluation benchmarks for a sufficient number of tasks. Regarding C1, most high-resource languages belong to the same language families, thus sharing a majority of their linguistic features. Neglecting typological diversity inevitably leads to poor generalizability and language-specific biases (Gerz et al., 2018; Ponti et al., 2019; Joshi et al., 2020). Following recent work in multilingual NLP that pays particular attention to typological diversity (Clark et al., 2020; Hu et al., 2020; Ponti et al., 2020, inter alia), we experiment with a language sample covering a broad spectrum of language properties. Regarding C2, for computational tractability, we only select languages with readily available BERT models. Unlike prior work, which typically lacks either language (R¨onnqvist et al., 2019; Zhang et al., 2020) or task diversity (Wu and Dredze, 2020; Vuli´c et al., 2020), we ensure that our experimental framework takes both into account, thus also satisfying C3. We achieve task diversity and generalizability by selecting a combination of tasks driven by lowe"
2021.acl-long.243,2020.acl-main.747,0,0.0639935,"Missing"
2021.acl-long.243,D18-1269,0,0.0249882,"9 languages from 8 language families, as listed in Table 1.3 We evaluate mBERT and monolingual BERT models on five downstream NLP tasks: named entity recognition (NER), sentiment analysis (SA), question answering (QA), universal dependency parsing (UDP), and part-of-speech tagging (POS).4 3 Note that, since we evaluate monolingual performance and not cross-lingual transfer performance, we require training data in the target language. Therefore, we are unable to leverage many of the available multilingual evaluation data such as XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), or XNLI (Conneau et al., 2018). These evaluation sets do not provide any training portions for languages other than English. Additional information regarding our selection of pretrained models is available in Appendix A.1. 4 Information on which datasets are associated with which language and the dataset sizes (examples per split) are provided in Appendix A.4. 3120 Language ISO Language Family Pretrained BERT Model Arabic English Finnish Indonesian Japanese Korean Russian Turkish Chinese AR Afroasiatic Indo-European Uralic Austronesian Japonic Koreanic Indo-European Turkic Sino-Tibetan AraBERT (Antoun et al., 2020) BERT (D"
2021.acl-long.243,D18-1029,1,0.883569,"Missing"
2021.acl-long.243,2021.eacl-main.270,1,0.685166,"Missing"
2021.acl-long.243,2020.acl-main.560,0,0.0242538,"ir comparisons. 3.1 Language and Task Selection Our selection of languages has been guided by several (sometimes competing) criteria: C1) typological diversity; C2) availability of pretrained monolingual BERT models; C3) representation of the languages in standard evaluation benchmarks for a sufficient number of tasks. Regarding C1, most high-resource languages belong to the same language families, thus sharing a majority of their linguistic features. Neglecting typological diversity inevitably leads to poor generalizability and language-specific biases (Gerz et al., 2018; Ponti et al., 2019; Joshi et al., 2020). Following recent work in multilingual NLP that pays particular attention to typological diversity (Clark et al., 2020; Hu et al., 2020; Ponti et al., 2020, inter alia), we experiment with a language sample covering a broad spectrum of language properties. Regarding C2, for computational tractability, we only select languages with readily available BERT models. Unlike prior work, which typically lacks either language (R¨onnqvist et al., 2019; Zhang et al., 2020) or task diversity (Wu and Dredze, 2020; Vuli´c et al., 2020), we ensure that our experimental framework takes both into account, thu"
2021.acl-long.243,2020.emnlp-main.363,1,0.857757,"Missing"
2021.acl-long.243,2020.acl-main.653,0,0.0385977,". Finally, we select a set of 9 languages from 8 language families, as listed in Table 1.3 We evaluate mBERT and monolingual BERT models on five downstream NLP tasks: named entity recognition (NER), sentiment analysis (SA), question answering (QA), universal dependency parsing (UDP), and part-of-speech tagging (POS).4 3 Note that, since we evaluate monolingual performance and not cross-lingual transfer performance, we require training data in the target language. Therefore, we are unable to leverage many of the available multilingual evaluation data such as XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), or XNLI (Conneau et al., 2018). These evaluation sets do not provide any training portions for languages other than English. Additional information regarding our selection of pretrained models is available in Appendix A.1. 4 Information on which datasets are associated with which language and the dataset sizes (examples per split) are provided in Appendix A.4. 3120 Language ISO Language Family Pretrained BERT Model Arabic English Finnish Indonesian Japanese Korean Russian Turkish Chinese AR Afroasiatic Indo-European Uralic Austronesian Japonic Koreanic Indo-European Turkic Sino-Tibetan AraBE"
2021.acl-long.243,N19-1392,0,0.0619148,"verified the scores, nor have they performed a controlled impartial comparison. Vuli´c et al. (2020) probed mBERT and monolingual BERT models across six typologically diverse languages for lexical semantics. They show that pretrained monolingual BERT models encode significantly more lexical information than mBERT. Zhang et al. (2020) investigated the role of pretraining data size with RoBERTa, finding that the model learns most syntactic and semantic features on corpora spanning 10M–100M word tokens, but still requires massive datasets to learn higher-level semantic and commonsense knowledge. Mulcaire et al. (2019) compared monolingual and bilingual ELMo (Peters et al., 2018) LMs across three downstream tasks, finding that contextualized representations from the bilingual models can improve monolingual task performance relative to their monolingual counterparts.2 However, it is unclear how their findings extend to massively multilingual LMs potentially suffering from the curse of multilinguality. R¨onnqvist et al. (2019) compared mBERT to monolingual BERT models for six languages (German, English, Swedish, Danish, Norwegian, Finnish) on three different tasks. They find that mBERT lags behind its monolin"
2021.acl-long.448,D19-6115,0,0.024124,"tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua et al., 2019; Chen et al., 2019; Nie et al., 2020). In the second approach, they first recognize examples that contain artifacts, and use this knowledge in the training objective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2019a; Utama et al., 2020a), i.e., datasets that contain counterexamples in which relying on the bias results in an incorrect prediction. In addition, it can also improve in-domain performances as well as generalization across various datasets that represent the same task (Wu et al., 2020a; Utama et al., 2020b). While there is an emerging tr"
2021.acl-long.448,D15-1162,0,0.0203263,"e answers lie in the sentence of the context that has the highest semantic similarity to the question. We use sentence-BERT (Reimers and Gurevych, 2019) to find the most similar sentence. • Short distance reasoning: for this bias, we train a model only using the sentence of the context that is the most similar to the question, instead of the whole context. We exclude the question-answer pairs in which the most similar sentence does not contain the answer. This model will not learn to perform coreference reasoning when the related coreferring pairs are not in the same sentence. 3 We use spaCy (Honnibal and Johnson, 2015) for NER. E.g., this can indicate the bias of the model to select the most frequent named entity in the context as the answer. 5770 4 For wh-word, empty question, and short distance reasoning, we use the TASE model (Segal et al., 2020) to learn the bias. Biased examples are then those that can be correctly solved by these models. We only change the training data for biased example detection, if necessary, and the development set is unchanged. The Quoref column in Table 1 reports the proportion of biased examples in the Quoref development set. Bias random named entity wh-word empty question sem"
2021.acl-long.448,2020.acl-main.132,0,0.024726,"as a span-prediction task by generating a query for each mention using the surrounding context, thus converting coreference resolution to a reading comprehension problem. They leverage the plethora of existing MRC datasets for data augmentation and improve the generalization of the coreference model. In parallel to Wu et al. (2020b), Aralikatte et al. (2019) also cast ellipsis and coreference resolution as reading comprehension tasks. They leverage the existing neural archi5769 tectures designed for MRC for ellipsis resolution and outperform the previous best results. In a similar direction, Hou (2020) propose to cast bridging anaphora resolution as question answering and present a question answering framework for this task. However, none of the above works investigate the impact of using coreference data on QA. Dua et al. (2020) use Amazon Mechanical Turkers to annotate the corresponding coreference chains of the answers in the passages of Quoref for 2,000 QA pairs. They then use this additional coreference annotation for training a model on Quoref. They show that including these additional coreference annotations improves the overall performance on Quoref. The proposed method by Dua et al"
2021.acl-long.448,D17-1215,0,0.0165097,"ggested by Tenney et al. (2019) and Clark et al. (2019b), or (3) coreference reasoning is not necessarily required for solving most examples. • Wh-word (Weissenborn et al., 2017): to recognize the QA pairs that can be answered by only using the interrogative adverbs from the question, we train a model on a variation of the training dataset in which questions only contain interrogative adverbs. • Empty question (Sugawara et al., 2020): to recognize QA pairs that are answerable without considering the question,4 we train a QA model only on the contexts and without questions. • Semantic overlap (Jia and Liang, 2017): for this artifact, we report the ratio of the QA pairs whose answers lie in the sentence of the context that has the highest semantic similarity to the question. We use sentence-BERT (Reimers and Gurevych, 2019) to find the most similar sentence. • Short distance reasoning: for this bias, we train a model only using the sentence of the context that is the most similar to the question, instead of the whole context. We exclude the question-answer pairs in which the most similar sentence does not contain the answer. This model will not learn to perform coreference reasoning when the related cor"
2021.acl-long.448,N18-1023,1,0.927993,"Comprehension Mingzhu Wu1 , Nafise Sadat Moosavi1 , Dan Roth2 , Iryna Gurevych1 1 2 UKP Lab, Technische Universitat Darmstadt Department of Computer and Information Science, UPenn 1 2 https://www.ukp.tu-darmstadt.de https://www.seas.upenn.edu/˜danroth/ Abstract improves the performance on some coreferencerelated datasets (Wu et al., 2020b; Aralikatte et al., 2019). There are also various datasets for the task of reading comprehension on which the model requires to perform coreference reasoning to answer some of the questions, e.g., DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), MultiRC (Khashabi et al., 2018), etc. Quoref (Dasigi et al., 2019) is a dataset that is particularly designed for evaluating coreference understanding of MRC models. Figure 1 shows a QA sample from Quoref in which the model needs to resolve the coreference relation between “his” and “John Motteux” to answer the question. Coreference resolution is essential for natural language understanding and has been long studied in NLP. In recent years, as the format of Question Answering (QA) became a standard for machine reading comprehension (MRC), there have been data collection efforts, e.g., Dasigi et al. (2019), that attempt to e"
2021.acl-long.448,J13-4004,0,0.0555938,"Missing"
2021.acl-long.448,2020.acl-main.703,0,0.148622,"tation artifacts, and its distribution of biases is closer to a coreference resolution dataset. The performance of state-of-the-art models on Quoref considerably drops on our evaluation set suggesting that (1) coreference reasoning is still an open problem for MRC models, and (2) our methodology opens a promising direction to create future challenging MRC datasets. Second, we propose to directly use coreference resolution datasets for training MRC models to improve their coreference reasoning. We automatically create a question whose answer is a coreferring expression m1 using the BART model (Lewis et al., 2020). We then consider this question, m1 ’s antecedent, and the corresponding document as a new (question, answer, context) tuple. This data helps the model learning to resolve the coreference relation between m1 and its antecedent to answer the question. We show that incorporating these additional data improves the performance of the state-of-the-art models on our new evaluation set. Our main contributions are as follows: • We show that Quoref does not reflect the natural challenges of coreference reasoning and propose a methodology for creating MRC datasets that better reflect this challenge. •"
2021.acl-long.448,2020.acl-main.441,0,0.228439,"are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models. There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua et al., 2019; Chen et al., 2019; Nie et al., 2020). In the second approach, they first recognize examples that contain artifacts, and use this knowledge in the training objective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2019a; Utama et al., 2020a), i.e., datasets that contain counterexamples in which relying on the bias results in an incorrect prediction. In addition, it"
2021.acl-long.448,W12-4501,0,0.572109,"velopment set is unchanged. The Quoref column in Table 1 reports the proportion of biased examples in the Quoref development set. Bias random named entity wh-word empty question semantic overlap short-distance reasoning Quoref CoNLLbart 9.39 22.99 21.51 28.66 50.70 1.52 13.12 11.60 21.38 9.86 Table 1: The proportion of examples in the Quoref development set and CoNLL-2012 coreference resolution dataset that contain each of the examined biases. We also investigate whether these biases have similar ratios in a coreference resolution dataset. We use the CoNLL-2012 coreference resolution dataset (Pradhan et al., 2012a) and convert it to a reading comprehension format, i.e., CoNLLbart in Section 5.5 This data contains question-answer pairs in which the question is created based on a coreferring expression in CoNLL-2012, and the answer is its closest antecedent. We split this data into training and test sets and train bias models on the training split. The CoNLLbart column in Table 1 shows the bias proportions on this data. 4 Creating an MRC Dataset that Better Reflects Coreference Reasoning There is a growing trend in using adversarial models for data creation to make the dataset more challenging or discar"
2021.acl-long.448,2020.acl-main.770,1,0.869579,"MRC models. We show that, while coreference resolution and MRC datasets are independent and belong to different domains, our approach improves the coreference reasoning of state-ofthe-art MRC models. 2 Related Work 2.1 Artifacts in NLP datasets One of the known drawbacks of many NLP datasets is that they contain artifacts.2 Models tend to ex2 I.e., the conditional distribution of the target label based on specific attributes of the training domain diverges while testing on other domains. ploit these easy-to-learn patterns in the early stages of training (Arpit et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models. There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo e"
2021.acl-long.448,2020.emnlp-main.613,1,0.893602,"MRC models. We show that, while coreference resolution and MRC datasets are independent and belong to different domains, our approach improves the coreference reasoning of state-ofthe-art MRC models. 2 Related Work 2.1 Artifacts in NLP datasets One of the known drawbacks of many NLP datasets is that they contain artifacts.2 Models tend to ex2 I.e., the conditional distribution of the target label based on specific attributes of the training domain diverges while testing on other domains. ploit these easy-to-learn patterns in the early stages of training (Arpit et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models. There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo e"
2021.acl-long.448,D16-1264,0,0.192798,"on simple lexical overlap without requiring coreference reasoning. We investigate five artifacts (biases) as follows: • Random named entity: the majority of answers in Quoref are person names. To evaluate this artifact, we randomly select a PERSON named entity from the context as the answer.3 How Well Quoref Presents Coreference Reasoning? For creating the Quoref dataset, annotators first identify coreferring expressions and then ask questions that connect the two coreferring expressions. Dasigi et al. (2019) use a BERT-base model (Devlin et al., 2019) that is fine-tuned on the SQuAD dataset (Rajpurkar et al., 2016) as an adversarial model to exclude QA samples that the adversarial model can already answer. The goal of using this adversarial model is to avoid including questionanswer pairs that can be solved using surface cues. They claim that most examples in Quoref cannot be answered without coreference reasoning. If we fine-tune a RoBERTa-large model on Quoref, it achieves 78 F1 score while the estimated human performance is around 93 F1 score (Dasigi et al., 2019). This high performance, given that RoBERTa can only predict continuous span answers while Quoref also contains discontinuous answers, indi"
2021.acl-long.448,D19-1410,1,0.830573,"that can be answered by only using the interrogative adverbs from the question, we train a model on a variation of the training dataset in which questions only contain interrogative adverbs. • Empty question (Sugawara et al., 2020): to recognize QA pairs that are answerable without considering the question,4 we train a QA model only on the contexts and without questions. • Semantic overlap (Jia and Liang, 2017): for this artifact, we report the ratio of the QA pairs whose answers lie in the sentence of the context that has the highest semantic similarity to the question. We use sentence-BERT (Reimers and Gurevych, 2019) to find the most similar sentence. • Short distance reasoning: for this bias, we train a model only using the sentence of the context that is the most similar to the question, instead of the whole context. We exclude the question-answer pairs in which the most similar sentence does not contain the answer. This model will not learn to perform coreference reasoning when the related coreferring pairs are not in the same sentence. 3 We use spaCy (Honnibal and Johnson, 2015) for NER. E.g., this can indicate the bias of the model to select the most frequent named entity in the context as the answer"
2021.acl-long.448,P18-1156,0,0.0175811,"Reasoning in Machine Reading Comprehension Mingzhu Wu1 , Nafise Sadat Moosavi1 , Dan Roth2 , Iryna Gurevych1 1 2 UKP Lab, Technische Universitat Darmstadt Department of Computer and Information Science, UPenn 1 2 https://www.ukp.tu-darmstadt.de https://www.seas.upenn.edu/˜danroth/ Abstract improves the performance on some coreferencerelated datasets (Wu et al., 2020b; Aralikatte et al., 2019). There are also various datasets for the task of reading comprehension on which the model requires to perform coreference reasoning to answer some of the questions, e.g., DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), MultiRC (Khashabi et al., 2018), etc. Quoref (Dasigi et al., 2019) is a dataset that is particularly designed for evaluating coreference understanding of MRC models. Figure 1 shows a QA sample from Quoref in which the model needs to resolve the coreference relation between “his” and “John Motteux” to answer the question. Coreference resolution is essential for natural language understanding and has been long studied in NLP. In recent years, as the format of Question Answering (QA) became a standard for machine reading comprehension (MRC), there have been data collection efforts, e.g., Dasigi"
2021.acl-long.448,2020.emnlp-main.248,0,0.401736,"odel only using the sentence of the context that is the most similar to the question, instead of the whole context. We exclude the question-answer pairs in which the most similar sentence does not contain the answer. This model will not learn to perform coreference reasoning when the related coreferring pairs are not in the same sentence. 3 We use spaCy (Honnibal and Johnson, 2015) for NER. E.g., this can indicate the bias of the model to select the most frequent named entity in the context as the answer. 5770 4 For wh-word, empty question, and short distance reasoning, we use the TASE model (Segal et al., 2020) to learn the bias. Biased examples are then those that can be correctly solved by these models. We only change the training data for biased example detection, if necessary, and the development set is unchanged. The Quoref column in Table 1 reports the proportion of biased examples in the Quoref development set. Bias random named entity wh-word empty question semantic overlap short-distance reasoning Quoref CoNLLbart 9.39 22.99 21.51 28.66 50.70 1.52 13.12 11.60 21.38 9.86 Table 1: The proportion of examples in the Quoref development set and CoNLL-2012 coreference resolution dataset that conta"
2021.acl-long.448,K17-1028,0,0.0202608,"d 93 F1 score (Dasigi et al., 2019). This high performance, given that RoBERTa can only predict continuous span answers while Quoref also contains discontinuous answers, indicates that either (1) Quoref presents coreference-aware QA very well so that the model can properly learn coreference reasoning from the training data, (2) pretrained transformer-based models have already learned coreference reasoning during their pre-training, e.g., as suggested by Tenney et al. (2019) and Clark et al. (2019b), or (3) coreference reasoning is not necessarily required for solving most examples. • Wh-word (Weissenborn et al., 2017): to recognize the QA pairs that can be answered by only using the interrogative adverbs from the question, we train a model on a variation of the training dataset in which questions only contain interrogative adverbs. • Empty question (Sugawara et al., 2020): to recognize QA pairs that are answerable without considering the question,4 we train a QA model only on the contexts and without questions. • Semantic overlap (Jia and Liang, 2017): for this artifact, we report the ratio of the QA pairs whose answers lie in the sentence of the context that has the highest semantic similarity to the ques"
2021.acl-long.448,2020.emnlp-demos.6,0,0.056864,"Missing"
2021.acl-long.448,2020.findings-emnlp.74,1,0.855048,"Missing"
2021.acl-long.448,2020.acl-main.622,0,0.214665,"jective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2019a; Utama et al., 2020a), i.e., datasets that contain counterexamples in which relying on the bias results in an incorrect prediction. In addition, it can also improve in-domain performances as well as generalization across various datasets that represent the same task (Wu et al., 2020a; Utama et al., 2020b). While there is an emerging trend of including adversarial models in data collection, their effectiveness is not yet compared with using debiasing methods, e.g., whether they are still beneficial when we use debiasing methods or vice versa. 2.2 Joint QA and Coreference Reasoning There are a few studies on the joint understanding of coreference relations and reading comprehension. Wu et al. (2020b) propose to formulate coreference resolution as a span-prediction task by generating a query for each mention using the surrounding context, thus converting coreference resolut"
2021.acl-long.448,D18-1259,0,0.111811,"et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models. There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua et al., 2019; Chen et al., 2019; Nie et al., 2020). In the second approach, they first recognize examples that contain artifacts, and use this knowledge in the training objective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2019a; Utama et al.,"
2021.acl-long.448,P19-1452,0,0.028621,"coreference reasoning. If we fine-tune a RoBERTa-large model on Quoref, it achieves 78 F1 score while the estimated human performance is around 93 F1 score (Dasigi et al., 2019). This high performance, given that RoBERTa can only predict continuous span answers while Quoref also contains discontinuous answers, indicates that either (1) Quoref presents coreference-aware QA very well so that the model can properly learn coreference reasoning from the training data, (2) pretrained transformer-based models have already learned coreference reasoning during their pre-training, e.g., as suggested by Tenney et al. (2019) and Clark et al. (2019b), or (3) coreference reasoning is not necessarily required for solving most examples. • Wh-word (Weissenborn et al., 2017): to recognize the QA pairs that can be answered by only using the interrogative adverbs from the question, we train a model on a variation of the training dataset in which questions only contain interrogative adverbs. • Empty question (Sugawara et al., 2020): to recognize QA pairs that are answerable without considering the question,4 we train a QA model only on the contexts and without questions. • Semantic overlap (Jia and Liang, 2017): for this"
2021.acl-long.448,D18-1009,0,0.171086,"ges of training (Arpit et al., 2017; Liu et al., 2020; Utama et al., 2020b), and therefore, they may not focus on learning harder patterns of the data that are useful for solving the underlying task. As a result, overfitting to dataset-specific artifacts limits the robustness and generalization of NLP models. There are two general approaches to tackle such artifacts: (1) adversarial filtering of biased examples, i.e., examples that contain artifacts, and (2) debiasing methods. In the first approach, potentially biased examples are discarded from the dataset, either after the dataset creation (Zellers et al., 2018; Yang et al., 2018a; Le Bras et al., 2020; Bartolo et al., 2020), or while creating the dataset (Dua et al., 2019; Chen et al., 2019; Nie et al., 2020). In the second approach, they first recognize examples that contain artifacts, and use this knowledge in the training objective to either skip or downweight biased examples (He et al., 2019; Clark et al., 2019a), or to regularize the confidence of the model on those examples (Utama et al., 2020a). The use of this information in the training objective improves the robustness of the model on adversarial datasets (He et al., 2019; Clark et al., 2"
2021.acl-long.524,W19-0606,0,0.0127048,"roxy for a conceptual metaphoric mapping. 6725 We first train FrameNet frame embeddings and employ evaluation metrics to ensure their quality. We then apply transformations between domains to literal verbs to generate metaphors grounded in conceptual metaphor theory. 3.1.1 Figure 2: Lexical generation process Learning Frame Embeddings In order to exploit FrameNet frames as conceptual domains, we will embed them in vector space. While lexical and contextualized embeddings have proven effective, the field of embedding concepts from lexical resources is less well explored (Sikos and Pad´o, 2018; Alhoshan et al., 2019). These methods involve tagging raw corpora using automatic FrameNet parsing and then inputting some combination of the original text and the FrameNet information into standard embedding algorithms. To train and evaluate frame embeddings, we use 211k sentences of Gold annotations used to train the Open-SESAME parser (Swayamdipta et al., 2017), along with a variety of other automatically tagged datasets: 250k individual sentence from the Gutenberg Poetry Corpus (Jacobs, 2018), 17k from various fiction section of the Brown Corpus (Francis and Kucera, 1979), and 80k sentences randomly selected fr"
2021.acl-long.524,P98-1013,0,0.356034,"ith metaphoric counterparts. This can be done by employing vector spaces, identifying the word most likely to fit in an appropriate context and subjecting them to some constraints of metaphoricity. We build on this paradigm by incorporating facets of conceptual metaphor theory. Our procedure is as follows: we learn a joint embedded representations for domains and lexical items. We then use the linear transformation between two domains as a mapping, which can be applied to input words from the target domain to generate a word from the source domain. As a proxy for domains, we utilize FrameNet (Baker et al., 1998), which contains semantic frames along with the set of lexical units that evoke them. Frames can be defined as related systems of concepts (Fillmore, 1982), which is exchangeable with the term “domain” used in conceptual metaphor theory (Cruse and Croft, 2004). Thus, we consider the transformation from one frame to another as a proxy for a conceptual metaphoric mapping. 6725 We first train FrameNet frame embeddings and employ evaluation metrics to ensure their quality. We then apply transformations between domains to literal verbs to generate metaphors grounded in conceptual metaphor theory. 3"
2021.acl-long.524,Q17-1010,0,0.0358182,"a hierarchy of connected frames. Starting with the assumption that frames connected in the structure should be more similar, we also calculate a structural similarity metric str. We follow the same process as above, taking the distance between the mean embedding of the local frames n ∈ N , where N is the immediate neighbors of f , to the mean embedding of a sample k of distant frames n ∈ / N. str(f ) = P cos(En ,Ef ) n∈N |N | k cos(E ,E ) P n f − n6∈N k We experiment with three lexical embeddings models: word2vec skip-gram (Mikolov et al., 2013), Glove (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). We experiment with 50, 100, and 300 dimensional representations; we find the 50 dimensional word2vec embeddings perform best for both evaluation metrics.4 3.1.2 Embedding Mappings To apply these embeddings to generate metaphors based on conceptual mappings, we learn mappings between frames and apply the mappings directly to lexical items to facilitate lexical replacement. We define a mapping m as the pointwise distance between the target frame embedding and the source frame embedding. Following the approach for learning connections between concrete and poetic themes of Gagliano et al. (2016)"
2021.acl-long.524,P19-1470,0,0.0569006,"horic verbs, and replacing them with infilling from a language model. We use a BERT-based metaphor classification model trained on the VUA metaphor corpus (Steen et al., 2010) to identify metaphoric verbs in a sentence (i.e “died” in The house where love had died). Then we convert it to a literal sentence (The house where love had ended) using infillings from pre-trained BERT (Devlin et al., 2019). To ensure the literal sentence with replacements convey the same semantic meaning as the metaphorical sentence they are then filtered using symbolic meaning (SymbolOf relation) obtained from COMET (Bosselut et al., 2019), a GPT based language model fine-tuned on ConceptNet (Speer et al., 2017). COMET returns top 5 symbolic beams of (loss, loneliness, despair, sadness and sorrow) for the sentence “The house where love had died” whereas it replaces sorrow with life for the literal version. While Chakrabarty et al. (2021) filter down to only those candidates with an exact match between the top 5 symbolic beams for the literal and metaphorical sentences returned by the COMET model, we ease the restriction to cases where at least four of five symbols are the same. In order to learn more direct metaphoric informati"
2021.acl-long.524,W15-1405,0,0.022996,"ctional preferences of verbs. Shaikh et al. (2014a) builds ”conceptual spaces” for source domains, using rule-based extraction of relations between lexical items. These conceptual spaces are then used to find new conceptual metaphors. This process is extended to build a repository of linguistic and conceptual metaphors (Shaikh et al., 2014b). Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metaphoric domains has been explored in the MetaNet project (Dodge et al., 2015). We however, restrict our work to FrameNet due to the coverage and availability of reliable automatic parsing. Metaphor Generation. Early work in metaphor generation was based in heuristics, learning to generate relatively simple ”A is like B” representations (Abe et al., 2006; Terai and Nakagawa, 2010). In a similar vein, Veale (2016) uses template-like structures to generate creative and metaphoric tweets. Other works focus on identifying metaphoric mappings using WordNet clustering and selectional preferences (Mason, 2004; Gandy et al., 2013), syntactic relations to build proposition datab"
2021.acl-long.524,P18-1082,0,0.0223628,"she left. where hEOT i and hV i are delimiters, DEATH is the source frame, and CAUSE TO END the target frame. The decoding target is the metaphoric text “The party died as soon as she left”, which evokes the CAUSE TO END IS DEATH mapping. Note that our training data differs only at the level of a single verb. We use the generative BART seq2seq model to generate metaphoric paraphrases, 6727 but due to the nature of the training data and the importance of verbs in metaphoric expressions, this is often realized in the output as lexical replacement. Post fine-tuning, we use top-k (k=5) sampling (Fan et al., 2018) to generate metaphors conditioned on the input literal sentence and source and target domains for the required metaphoric mapping.5 We evaluate the lexical model (CM-Lex) and the sequence-to-sequence model (CM-BART) under two experimental settings. 4 Experimental Setup We evaluate our metaphor generation methods against two previous approaches to metaphoric paraphrase generation: the MERMAID system (Chakrabarty et al., 2021) and the metaphor masking model (MetMask) (Stowe et al., 2020). We explore two tasks: generating against gold standard metaphoric expressions, and using rare and unseen me"
2021.acl-long.524,W16-0203,0,0.138294,"janowski et al., 2017). We experiment with 50, 100, and 300 dimensional representations; we find the 50 dimensional word2vec embeddings perform best for both evaluation metrics.4 3.1.2 Embedding Mappings To apply these embeddings to generate metaphors based on conceptual mappings, we learn mappings between frames and apply the mappings directly to lexical items to facilitate lexical replacement. We define a mapping m as the pointwise distance between the target frame embedding and the source frame embedding. Following the approach for learning connections between concrete and poetic themes of Gagliano et al. (2016), we sum the embedding of the target verb and the mapping m for the selected conceptual mapping, and select the most similar word to the resulting vector. This word is then delemmatized using fitbert (Havens and Stal, 2019) and inserted into the original sentence (Figure 2). Note that these resulting words are generated without context, as they rely only on the input word and the conceptual mappings. This approach has benefits: we require no labeled metaphor data, using only embeddings trained on FrameNet-tagged corpora. However, ignoring context is likely detrimental. In order to better use c"
2021.acl-long.524,2020.acl-main.703,0,0.22681,"6726 4 For full frame embedding evaluation, see Appendix A. Literal (filled from LM) That tyranny is destroyed The house where love had ended As the moments passed on What I learned my senses fraught Target Frame DESTRUCTION CAUSE TO END PROCESS END COMING TO BELIEVE Metaphoric (original) That tyranny is slain The house where love had died As the moments roll on What I bear my senses fraught Source Frame KILLING DEATH CAUSE MOTION BRINGING Table 1: Sample of extracted pairs from the data collection process. 3.2 CM-BART For sequence-to-sequence learning, we fine-tune a pre-trained BART model (Lewis et al., 2020), adding source and target information to guide generation towards the intended metaphors. We first outline a procedure for generating semi-supervised paired data, then detail the training and generation process. 3.2.1 Method for Creating Parallel Data In order to train sequence-to-sequence models for metaphor generation, we require large scale parallel corpora. We follow the approach of Chakrabarty et al. (2021) and build a corpus of literal/metaphoric paraphrases by starting with the Gutenberg Poetry corpus (Jacobs, 2018), identifying and masking metaphoric verbs, and replacing them with inf"
2021.acl-long.524,P18-1113,0,0.0555435,"Missing"
2021.acl-long.524,J04-1002,0,0.189424,"taphoricity, provide a strong signal for which domains to generate in. This highlights a possible benefit to the interaction between deep, pre-trained models such as BART and available lexical resources: by combining these, we are able to leverage the strength of each to build a powerful metaphor generation system. 6 Related Work We broadly cover two areas of related work: previous computational approaches to CMT, and previous approaches to metaphor generation. Computational Approaches to CMT. There are a variety of approaches to identifying conceptual metaphors themselves. The CorMet system (Mason, 2004) was built to extract conceptual metaphors based on selectional preferences of verbs. Shaikh et al. (2014a) builds ”conceptual spaces” for source domains, using rule-based extraction of relations between lexical items. These conceptual spaces are then used to find new conceptual metaphors. This process is extended to build a repository of linguistic and conceptual metaphors (Shaikh et al., 2014b). Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metap"
2021.acl-long.524,S16-2003,0,0.180733,"ings. For the former, we build a gold test set of metaphoric paraphrases that evoke a particular source/target mapping. For the latter, we apply a variety of source/target mappings to literal inputs for which we do not have gold outputs. 4.1 Building a Test Set For a test set, we use the same procedure as our data collection approach from Section 3.2.1. We apply this procedure to two datasets: a sample of the Gutenberg Poetry Corpus and a sample of fiction from the Brown Corpus (Francis and Kucera, 1979). This generates an initial set of literal/metaphoric pairs. We also tagged the pairs from Mohammad et al. (2016) with FrameNet tags, as these generally contain novel, well-formed metaphors. These three datasets each have different properties with regard to metaphor. The Gutenberg Poetry corpus has consistent, novel metaphors, but often unconventional syntactic constructions, due to the poetic nature of the text. The Mohammad 2016 corpus contains manually constructed metaphors which are novel, following relatively basic syntactic patterns. The Brown Corpus is standard fiction texts, so the metaphors within tend to be very conventional. From these sources, we draw pairs randomly, checking that they reflec"
2021.acl-long.524,C14-1165,0,0.0248913,"CMT, and previous approaches to metaphor generation. Computational Approaches to CMT. There are a variety of approaches to identifying conceptual metaphors themselves. The CorMet system (Mason, 2004) was built to extract conceptual metaphors based on selectional preferences of verbs. Shaikh et al. (2014a) builds ”conceptual spaces” for source domains, using rule-based extraction of relations between lexical items. These conceptual spaces are then used to find new conceptual metaphors. This process is extended to build a repository of linguistic and conceptual metaphors (Shaikh et al., 2014b). Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metaphoric domains has been explored in the MetaNet project (Dodge et al., 2015). We however, restrict our work to FrameNet due to the coverage and availability of reliable automatic parsing. Metaphor Generation. Early work in metaphor generation was based in heuristics, learning to generate relatively simple ”A is like B” representations (Abe et al., 2006; Terai and Nakagawa, 2010). In a similar vein, Veale (2016) uses te"
2021.acl-long.524,N19-4009,0,0.042058,"Missing"
2021.acl-long.524,D14-1162,0,0.0858506,"en frames (eg. used-by, uses), yielding a hierarchy of connected frames. Starting with the assumption that frames connected in the structure should be more similar, we also calculate a structural similarity metric str. We follow the same process as above, taking the distance between the mean embedding of the local frames n ∈ N , where N is the immediate neighbors of f , to the mean embedding of a sample k of distant frames n ∈ / N. str(f ) = P cos(En ,Ef ) n∈N |N | k cos(E ,E ) P n f − n6∈N k We experiment with three lexical embeddings models: word2vec skip-gram (Mikolov et al., 2013), Glove (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). We experiment with 50, 100, and 300 dimensional representations; we find the 50 dimensional word2vec embeddings perform best for both evaluation metrics.4 3.1.2 Embedding Mappings To apply these embeddings to generate metaphors based on conceptual mappings, we learn mappings between frames and apply the mappings directly to lexical items to facilitate lexical replacement. We define a mapping m as the pointwise distance between the target frame embedding and the source frame embedding. Following the approach for learning connections between concrete and"
2021.acl-long.524,D19-1410,1,0.846699,"Missing"
2021.acl-long.524,W14-4725,0,0.0741936,"Missing"
2021.acl-long.524,W18-3813,0,0.0520256,"Missing"
2021.acl-long.524,W16-1105,0,0.0462399,"Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metaphoric domains has been explored in the MetaNet project (Dodge et al., 2015). We however, restrict our work to FrameNet due to the coverage and availability of reliable automatic parsing. Metaphor Generation. Early work in metaphor generation was based in heuristics, learning to generate relatively simple ”A is like B” representations (Abe et al., 2006; Terai and Nakagawa, 2010). In a similar vein, Veale (2016) uses template-like structures to generate creative and metaphoric tweets. Other works focus on identifying metaphoric mappings using WordNet clustering and selectional preferences (Mason, 2004; Gandy et al., 2013), syntactic relations to build proposition databases (Ovchinnikova et al., 2014), and embedding based approaches to identify poetic relationships (Gagliano et al., 2016). However, the goal of these works is to generate mappings, rather than linguistic expressions that evoke them. Amongst deep learning approaches Yu and Wan (2019) identify literal and metaphoric words in corpora based"
2021.acl-long.524,D19-1221,0,0.0577942,"f-the-art performance in metaphor generation by both automatic and human evaluations. Future work can expand these models to go beyond verbs, incorporating nominal and other types of metaphors. The next necessary step is to go beyond lexicalized metaphors: good, consistent conceptual metaphors often span long stretches of text, and we need to design models that can learn and generate metaphors over larger texts. Ethical Considerations Although we use language models trained on data collected from the Web, which have been shown to have issues with bias and abusive language (Sheng et al., 2019; Wallace et al., 2019), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, BART is a conditional language model, which provides more control of the generated output. It should also be noted that our CM-BART model is fine-tuned on the poetry corpus which is devoid of harmful and toxic text especially targeted at marginalized communities Advances in generative AI inherently come with concerns about models’ ability to deceive, persuade, and misinform. Metaphorical language has been shown to express and elicit stronger emotion than literal language (Citron and"
2021.acl-long.524,N19-1092,0,0.214231,"domain from which we draw the metaphorical expressions, while the target domain is the conceptual domain that we try to understand. A classical mapping is ARGUMENT IS WAR, in which we conceptualize the target argumentation domain as the more concrete source domain of war: Introduction Recent neural models have led to important progress in natural language generation (NLG) tasks. While pre-trained models have facilitated advances in many areas of generation, the field of metaphor generation remains relatively unexplored. Moreover, the few existing deep learning models for metaphor generation (Yu and Wan, 2019; Stowe et al., 2020; Chakrabarty et al., 2020) lack any conceptualization of the meaning of the metaphors. This work proposes the first step towards metaphor generation informed by the conceptual metaphor theory (CMT) (Lakoff and Johnson, 1980; Lakoff, 1993; Reddy, 1979). CMT holds • They fought against the contract. • They defended their new proposal. We focus on verbs, as they are often the key component of metaphoric expressions (Steen et al., 2010; Martin, 2006). When used metaphorically, verbs typically evoke source domains (e.g. fought, defended in the above examples): they are concrete"
2021.acl-short.77,2020.emnlp-main.550,0,0.0213855,", 2020; Gao et al., 2020). Previous work showed the out-performance for fixed, rather small indexes. The largest dataset where it has been shown is the MS Marco (Bajaj et al., 2018) passage retrieval dataset, where retrieval is done over an index of 8.8 million text passages. However, in production scenarios, index sizes quickly reach 100 millions of documents. Related Work A common choice for dense retrieval is to finetune a transformer network like BERT (Devlin et al., 2018) on a given training corpus with queries and relevant documents (Guo et al., 2020; Guu et al., 2020; Gao et al., 2020; Karpukhin et al., 2020; Luan et al., 2020). Recent work showed that combining dense approaches with sparse, lexical approaches can further boost the performance (Luan et al., 2020; Gao et al., 2020). While the approaches have been tested on various information and question answering retrieval datasets, the performance was only evaluated on fixed, rather small indexes. Guo et al. (2020) evaluated approaches for eight different datasets having index sizes between 3k and 454k documents. We are not aware of previous work that compares sparse and dense approaches for increasing index sizes and the connection to the dime"
2021.acl-short.77,Q19-1026,0,0.0136913,", there is usually only a single passage labeled as relevant even though multiple passages would be considered as relevant by humans (Craswell et al., 2020). To avoid that the drop in performance is due to the retrieval of relevant, but unlabeled passages, we perform an experiment where we add random irrelevant noise to the index. Our index consists only of the relevant passages and a large fraction of irrelevant, randomly generated strings.3 We also evaluate the popular DPR system by Karpukhin et al. (2020), which is a BERT-based dense retriever trained on the Natural Questions (NQ) dataset (Kwiatkowski et al., 2019). We chose the NQ dev set, consisting of 1772 questions from Google search logs. DPR encodes the passage as Title [SEP] Paragraph. We create a random string for the paragraph and combine it with 1) a randomly generated string as title, 2) selecting randomly one of the over 6 Million real Wikipedia article titles, 3) selecting randomly one of the 1772 article titles found in the NQ dev set. We count for how many queries a random string is ranked higher than the relevant passage. The results are shown in Table 3. We observe that BM25 does not rank any randomly generated passage higher than the r"
2021.acl-short.77,2020.emnlp-main.733,0,0.022237,". However, with more indexed documents, the probability of false positives increases faster for low dimensional representations than for higher dimensional representations. Hence, at some index size, higher dimensional representations might outperform the lower-dimensional representation. 4 Empirical Investigation In the proof, we have assumed that vectors are independent and uniformly distributed over the space, which gives us a lower bound on the false positive rate. However, in practice, dense representations are neither independent nor uniformly distributed. As shown in (Ethayarajh, 2019; Li et al., 2020), dense representations derived from pre-trained Transformers like BERT map to an anisotropic space, i.e., the vectors occupy only a narrow cone in the vector space. This drastically increases the chance that an irrelevant document is closer to the query embedding than the relevant document. Hence, we study how actual dense models are impacted by increasing index sizes and lowerdimensional representations. 4.1 Dataset We conduct our experiments on the MS MARCO passage dataset (Bajaj et al., 2018). It consists of over 1 million unique real queries from the Bing search engine, together with 8.8"
2021.acl-short.77,D19-1006,0,0.0280443,"small index sizes. However, with more indexed documents, the probability of false positives increases faster for low dimensional representations than for higher dimensional representations. Hence, at some index size, higher dimensional representations might outperform the lower-dimensional representation. 4 Empirical Investigation In the proof, we have assumed that vectors are independent and uniformly distributed over the space, which gives us a lower bound on the false positive rate. However, in practice, dense representations are neither independent nor uniformly distributed. As shown in (Ethayarajh, 2019; Li et al., 2020), dense representations derived from pre-trained Transformers like BERT map to an anisotropic space, i.e., the vectors occupy only a narrow cone in the vector space. This drastically increases the chance that an irrelevant document is closer to the query embedding than the relevant document. Hence, we study how actual dense models are impacted by increasing index sizes and lowerdimensional representations. 4.1 Dataset We conduct our experiments on the MS MARCO passage dataset (Bajaj et al., 2018). It consists of over 1 million unique real queries from the Bing search engine,"
2021.acl-short.77,D19-1410,1,0.886565,"Missing"
2021.conll-1.26,P19-1602,0,0.0242386,"g our understanding of conceptual metaphor theory to produce stronger metaphoric expressions. Their scores for fluency and sentence similarity are lower than the free models, but are still on par with the original gold references. els such as the GPT family (Radford et al., 2019; Brown et al., 2020) are extremely effective at producing text. The perplexity of a given sentence under the language model can be a proxy for sentence fluency. This approach promotes common words over rare, perhaps penalizing creativity, but can used as an evaluation metrics for generation systems (Chen et al., 2020; Bao et al., 2019). Abstractness The notions of abstractness and concreteness have long been staples in metaphor detection systems (Dunn, 2013; Turney et al., 2011). We here use the abstract/concreteness ratings from Köper and Schulte im Walde (2017), which are 6 Automatic Evaluation based on Word2Vec embeddings (Mikolov et al., 2013). We evaluate the mean abstractness score, as Evaluation of metaphor generation is a difficult well as the standard deviation, under the assumptask, as traditional metrics for machine translation tion that one of the main aspects of metaphoricity and other tasks aim to enforce lexi"
2021.conll-1.26,W18-0906,0,0.0225228,"e of the corpus. Finally, the mappings learned are entirely reliant on the FrameNet frame tags: the mappings themselves are still detached from any theory, and may not reflect true metaphoric mappings. To obviate these difficulties, we here propose an alternative method for generating data which uses the MetaNet resource, which instead contains gold metaphoric mappings. A key bottleneck in metaphoric paraphrasing is the lack of high quality literal/metaphoric pairs for both training and evaluation. While two datasets of hand-crafted metaphoric paraphrases are available (Mohammad et al., 2016; Bizzoni and Lappin, 2018), they both contain less than 200 instances, and are thus too small for model training and/or fine-tuning. We explore two possible datasets for this purpose: a previously used dataset which 3.2 MetaNet Generation contains source and target domains via FrameNet frames (Stowe et al., 2021), and a new semi- MetaNet is a resource which contains a substansupervised dataset which contains source/target tial set of mappings between conceptual domains information based on the metaphor-based lexical (Dodge et al., 2015). These mappings consist of resource MetaNet. source and target domains which evoke"
2021.conll-1.26,P19-1470,0,0.0261508,"urce/Target pairs from FrameNet Tagging As a starting point, we use the data from Stowe et al. (2021). This dataset is a semi-supervised pairing of literal and metaphoric sentences. It is generated by taking sentences from the Gutenberg Poetry corpus (Jacobs, 2018), tagging them automatically with metaphor labels, then replacing metaphoric words using a language model to produce a literal paraphrase. This follows the assumption that the more likely replacement words from the language model will tend to be literal. The literal paraphrase and original metaphor are tagged using the COMET parser (Bosselut et al., 2019), and pairs that contain overlapping common-sense symbols are kept. They then use the open-SESAME parser to add FrameNet frame labels, which function as domains. The pairs are considered to reflect metaphoric mappings between their respective frame labels, which are then provided as control codes to a BART model to generate metaphors from specific mappings (Lewis et al., 2020). This method is effective for generating controlled metaphor pairs, but has a number of drawbacks. First, it relies heavily on a number of models (metaphor classification, FrameNet frame tagging, and COMET symbol extract"
2021.conll-1.26,N19-1423,0,0.0378604,"Missing"
2021.conll-1.26,D18-1171,1,0.842628,"icult well as the standard deviation, under the assumptask, as traditional metrics for machine translation tion that one of the main aspects of metaphoricity and other tasks aim to enforce lexical and semantic is the difference between abstractness levels of difsimilarity between the input and output sequences. ferent conceptual domains within a sentence. As these metrics often rely on word overlap, valid Novelty Classification We train a BERT regresmetaphoric paraphrases may be punished for besion model on the metaphoric novelty scores from ing overly creative. We thus implement a variety Do Dinh et al. (2018). We score the generated of standard evaluation metrics and evaluate their sentence as the mean metaphoricity score over all correlation with our gold standard annotations. words in the generated sentence. BLEU, METEOR, ROUGE Word and phrase Binary Metaphor Classification For binary overlap metrics from machine translation such as metaphor classification, we use the DeepMet sysBLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004) tem of Su et al. (2020), which achieved best performance on the metaphor shared task (Leong et al., are often used to evaluate paraph"
2021.conll-1.26,W15-1405,0,0.024593,"of hand-crafted metaphoric paraphrases are available (Mohammad et al., 2016; Bizzoni and Lappin, 2018), they both contain less than 200 instances, and are thus too small for model training and/or fine-tuning. We explore two possible datasets for this purpose: a previously used dataset which 3.2 MetaNet Generation contains source and target domains via FrameNet frames (Stowe et al., 2021), and a new semi- MetaNet is a resource which contains a substansupervised dataset which contains source/target tial set of mappings between conceptual domains information based on the metaphor-based lexical (Dodge et al., 2015). These mappings consist of resource MetaNet. source and target domains which evoke the concep325 NATIONS ARE MACHINES T: kingdom, country, land, state, ... NATIONS MACHINES POLITICAL LOCATION GIZMO I: He ruled the largest kingdom that ever existed He ruled the largest [MSK] that ever existed. MetaNet Domains FrameNet Frames Vocabulary Mask Filling Literal (Target) Context (T) Metaphoric Hypothesis 248k 8.5k 1k MetaNet Silver 360k 650 550 8.2 23.6 Gutenberg Corpus FrameNet corpus Stowe et al. (2021) S: machine, gear, apparatus, core, device, gadget, ... # Sentences Unique Mappings Unique Domai"
2021.conll-1.26,P19-1332,0,0.0192777,"irs based on MetaNet mappings, along with our gold test set and 1,250 samples annotated for fluency, sentence similarity, and metaphoricity to allow for better evaluation of metaphor generation systems. Related Work Paraphrase Generation The task of paraphrase generation has a rich background, including rule-based approaches (McKeown, 1983) and mono-lingual machine translation methods (Quirk et al., 2004; Wubben et al., 2010). Deep learning has driven the field in recent years, particularly autoencoder and LSTM networks (Gupta et al., 2018; Prakash et al., 2016) and transformer-based methods (Li et al., 2019; Egonmwan and Chali, 2019; Wang et al., 2019). Our task, however, diverges in many regards from the standard task of paraphrasing, which typically relies on syntactic and lexical transformations to generate sentences with exactly the same meaning as the input. We instead aim to generate sentences that contain additional meaning via a metaphoric mappings. In this regard, the entailment relations and semantics of generated metaphors will differ from traditional paraphrase generation: we expect the generated metaphor to entail the literal context, but as it adds something additional, the literal"
2021.conll-1.26,W04-1013,0,0.0673815,"es may be punished for besion model on the metaphoric novelty scores from ing overly creative. We thus implement a variety Do Dinh et al. (2018). We score the generated of standard evaluation metrics and evaluate their sentence as the mean metaphoricity score over all correlation with our gold standard annotations. words in the generated sentence. BLEU, METEOR, ROUGE Word and phrase Binary Metaphor Classification For binary overlap metrics from machine translation such as metaphor classification, we use the DeepMet sysBLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004) tem of Su et al. (2020), which achieved best performance on the metaphor shared task (Leong et al., are often used to evaluate paraphrasing, despite noted weaknesses (Reiter and Belz, 2009; Reiter, 2018). This model uses linguistics and contextual features in a siamese architecture, taking advantage 2018). We include them here to highlight their of local and distant context. DeepMet functions at performance on creative language, and to compare the word level; we score the generated sentence by them against human evaluations. Translation Error Rate TER is another com- taking the average number"
2021.conll-1.26,J83-1001,0,0.470453,"matic evaluation metrics, comparing them to crowdsourced annotations, showing conflict between fluency and metaphoricity evaluation metrics, indicating that individuals metrics are poor evaluators for metaphor generation. • We release our novel training dataset of 360k pairs based on MetaNet mappings, along with our gold test set and 1,250 samples annotated for fluency, sentence similarity, and metaphoricity to allow for better evaluation of metaphor generation systems. Related Work Paraphrase Generation The task of paraphrase generation has a rich background, including rule-based approaches (McKeown, 1983) and mono-lingual machine translation methods (Quirk et al., 2004; Wubben et al., 2010). Deep learning has driven the field in recent years, particularly autoencoder and LSTM networks (Gupta et al., 2018; Prakash et al., 2016) and transformer-based methods (Li et al., 2019; Egonmwan and Chali, 2019; Wang et al., 2019). Our task, however, diverges in many regards from the standard task of paraphrasing, which typically relies on syntactic and lexical transformations to generate sentences with exactly the same meaning as the input. We instead aim to generate sentences that contain additional mean"
2021.conll-1.26,J18-3002,0,0.0137986,"nd evaluate their sentence as the mean metaphoricity score over all correlation with our gold standard annotations. words in the generated sentence. BLEU, METEOR, ROUGE Word and phrase Binary Metaphor Classification For binary overlap metrics from machine translation such as metaphor classification, we use the DeepMet sysBLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004) tem of Su et al. (2020), which achieved best performance on the metaphor shared task (Leong et al., are often used to evaluate paraphrasing, despite noted weaknesses (Reiter and Belz, 2009; Reiter, 2018). This model uses linguistics and contextual features in a siamese architecture, taking advantage 2018). We include them here to highlight their of local and distant context. DeepMet functions at performance on creative language, and to compare the word level; we score the generated sentence by them against human evaluations. Translation Error Rate TER is another com- taking the average number of words in the sentence monly used metric in machine translation, measur- classified as metaphoric. We consider the generated output the hypotheing the amount of correction that is necessary for a sis,"
2021.conll-1.26,S16-2003,0,0.381949,"le not designed for metaphoricity, we include the SOW- REAP system of Goyal and Durrett (2020) as a comparison to highlight the disparity between metaphoric and non-metaphoric paraphrasing capabilities. 2.2 Metaphor Generation Early work in computational metaphor generation involves generating simple &quot;A is like B&quot; expressions, based on probabilistic relationships between words (Abe et al., 2006; Terai and Nakagawa, 2010). These methods are effective to a degree, but lack the flexibility necessary to instantiate natural language metaphors. 2 Consider the metaphor &quot;Her husband abuses alcohol.&quot; (Mohammad et al., 2016): it entails the literal paraphrase &quot;Her husband drinks alcohol&quot;, but the reverse is not necessarily true. 324 Metaphor generation has recently seen significant advances due to deep pretrained language models. Yu and Wan (2019) use neural models to generate metaphoric expressions in an unsupervised manner. They identify source and target verbs automatically from corpora, and use these to train a neural language model. However, they are generating metaphors without regard to reference texts from metaphorically trained language models, and the outputs bear no relation to the inputs. With regard"
2021.conll-1.26,P02-1040,0,0.109964,"alid Novelty Classification We train a BERT regresmetaphoric paraphrases may be punished for besion model on the metaphoric novelty scores from ing overly creative. We thus implement a variety Do Dinh et al. (2018). We score the generated of standard evaluation metrics and evaluate their sentence as the mean metaphoricity score over all correlation with our gold standard annotations. words in the generated sentence. BLEU, METEOR, ROUGE Word and phrase Binary Metaphor Classification For binary overlap metrics from machine translation such as metaphor classification, we use the DeepMet sysBLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004) tem of Su et al. (2020), which achieved best performance on the metaphor shared task (Leong et al., are often used to evaluate paraphrasing, despite noted weaknesses (Reiter and Belz, 2009; Reiter, 2018). This model uses linguistics and contextual features in a siamese architecture, taking advantage 2018). We include them here to highlight their of local and distant context. DeepMet functions at performance on creative language, and to compare the word level; we score the generated sentence by them against human evaluations. Translatio"
2021.conll-1.26,C16-1275,0,0.0187757,"n. • We release our novel training dataset of 360k pairs based on MetaNet mappings, along with our gold test set and 1,250 samples annotated for fluency, sentence similarity, and metaphoricity to allow for better evaluation of metaphor generation systems. Related Work Paraphrase Generation The task of paraphrase generation has a rich background, including rule-based approaches (McKeown, 1983) and mono-lingual machine translation methods (Quirk et al., 2004; Wubben et al., 2010). Deep learning has driven the field in recent years, particularly autoencoder and LSTM networks (Gupta et al., 2018; Prakash et al., 2016) and transformer-based methods (Li et al., 2019; Egonmwan and Chali, 2019; Wang et al., 2019). Our task, however, diverges in many regards from the standard task of paraphrasing, which typically relies on syntactic and lexical transformations to generate sentences with exactly the same meaning as the input. We instead aim to generate sentences that contain additional meaning via a metaphoric mappings. In this regard, the entailment relations and semantics of generated metaphors will differ from traditional paraphrase generation: we expect the generated metaphor to entail the literal context, b"
2021.conll-1.26,D19-1313,0,0.0188894,"sformations to generate sentences with exactly the same meaning as the input. We instead aim to generate sentences that contain additional meaning via a metaphoric mappings. In this regard, the entailment relations and semantics of generated metaphors will differ from traditional paraphrase generation: we expect the generated metaphor to entail the literal context, but as it adds something additional, the literal context may not entail the metaphoric output.2 Recent work in paraphrase generation has taken a step in this direction by focusing on generating diverse paraphrases (Xu et al., 2018; Qian et al., 2019; Yang et al., 2019; Goyal and Durrett, 2020). While not designed for metaphoricity, we include the SOW- REAP system of Goyal and Durrett (2020) as a comparison to highlight the disparity between metaphoric and non-metaphoric paraphrasing capabilities. 2.2 Metaphor Generation Early work in computational metaphor generation involves generating simple &quot;A is like B&quot; expressions, based on probabilistic relationships between words (Abe et al., 2006; Terai and Nakagawa, 2010). These methods are effective to a degree, but lack the flexibility necessary to instantiate natural language metaphors. 2 Con"
2021.conll-1.26,W04-3219,0,0.194648,"tations, showing conflict between fluency and metaphoricity evaluation metrics, indicating that individuals metrics are poor evaluators for metaphor generation. • We release our novel training dataset of 360k pairs based on MetaNet mappings, along with our gold test set and 1,250 samples annotated for fluency, sentence similarity, and metaphoricity to allow for better evaluation of metaphor generation systems. Related Work Paraphrase Generation The task of paraphrase generation has a rich background, including rule-based approaches (McKeown, 1983) and mono-lingual machine translation methods (Quirk et al., 2004; Wubben et al., 2010). Deep learning has driven the field in recent years, particularly autoencoder and LSTM networks (Gupta et al., 2018; Prakash et al., 2016) and transformer-based methods (Li et al., 2019; Egonmwan and Chali, 2019; Wang et al., 2019). Our task, however, diverges in many regards from the standard task of paraphrasing, which typically relies on syntactic and lexical transformations to generate sentences with exactly the same meaning as the input. We instead aim to generate sentences that contain additional meaning via a metaphoric mappings. In this regard, the entailment rel"
2021.conll-1.26,D19-1410,1,0.862776,"Missing"
2021.conll-1.26,2006.amta-papers.25,0,0.201948,"re, taking advantage 2018). We include them here to highlight their of local and distant context. DeepMet functions at performance on creative language, and to compare the word level; we score the generated sentence by them against human evaluations. Translation Error Rate TER is another com- taking the average number of words in the sentence monly used metric in machine translation, measur- classified as metaphoric. We consider the generated output the hypotheing the amount of correction that is necessary for a sis, the gold target metaphor as the reference, and generated output to be valid (Snover et al., 2006). SentBERT SentBERT (Reimers and Gurevych, the original literal input as the context. We compare each of these metrics to the crowdsourced an2019) provides sentence transformers to generate notations for all of our system outputs (n = 1458 sentential vectors. These can be then compared using cosine distance to find semantic similarity. unique hypotheses). For the similarity metrics, we evaluate using the hypothesis against the reference SentBERT has proven effective for a wide variety as well as against the context. For the others, they of similarity tasks, and should also be effective at can"
2021.conll-1.26,2020.figlang-1.4,0,0.0111859,"for besion model on the metaphoric novelty scores from ing overly creative. We thus implement a variety Do Dinh et al. (2018). We score the generated of standard evaluation metrics and evaluate their sentence as the mean metaphoricity score over all correlation with our gold standard annotations. words in the generated sentence. BLEU, METEOR, ROUGE Word and phrase Binary Metaphor Classification For binary overlap metrics from machine translation such as metaphor classification, we use the DeepMet sysBLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004) tem of Su et al. (2020), which achieved best performance on the metaphor shared task (Leong et al., are often used to evaluate paraphrasing, despite noted weaknesses (Reiter and Belz, 2009; Reiter, 2018). This model uses linguistics and contextual features in a siamese architecture, taking advantage 2018). We include them here to highlight their of local and distant context. DeepMet functions at performance on creative language, and to compare the word level; we score the generated sentence by them against human evaluations. Translation Error Rate TER is another com- taking the average number of words in the sentenc"
2021.conll-1.26,D11-1063,0,0.0300223,"are lower than the free models, but are still on par with the original gold references. els such as the GPT family (Radford et al., 2019; Brown et al., 2020) are extremely effective at producing text. The perplexity of a given sentence under the language model can be a proxy for sentence fluency. This approach promotes common words over rare, perhaps penalizing creativity, but can used as an evaluation metrics for generation systems (Chen et al., 2020; Bao et al., 2019). Abstractness The notions of abstractness and concreteness have long been staples in metaphor detection systems (Dunn, 2013; Turney et al., 2011). We here use the abstract/concreteness ratings from Köper and Schulte im Walde (2017), which are 6 Automatic Evaluation based on Word2Vec embeddings (Mikolov et al., 2013). We evaluate the mean abstractness score, as Evaluation of metaphor generation is a difficult well as the standard deviation, under the assumptask, as traditional metrics for machine translation tion that one of the main aspects of metaphoricity and other tasks aim to enforce lexical and semantic is the difference between abstractness levels of difsimilarity between the input and output sequences. ferent conceptual domains"
2021.conll-1.26,W10-4223,0,0.0289543,"Missing"
2021.conll-1.26,D19-1309,0,0.0163206,"rate sentences with exactly the same meaning as the input. We instead aim to generate sentences that contain additional meaning via a metaphoric mappings. In this regard, the entailment relations and semantics of generated metaphors will differ from traditional paraphrase generation: we expect the generated metaphor to entail the literal context, but as it adds something additional, the literal context may not entail the metaphoric output.2 Recent work in paraphrase generation has taken a step in this direction by focusing on generating diverse paraphrases (Xu et al., 2018; Qian et al., 2019; Yang et al., 2019; Goyal and Durrett, 2020). While not designed for metaphoricity, we include the SOW- REAP system of Goyal and Durrett (2020) as a comparison to highlight the disparity between metaphoric and non-metaphoric paraphrasing capabilities. 2.2 Metaphor Generation Early work in computational metaphor generation involves generating simple &quot;A is like B&quot; expressions, based on probabilistic relationships between words (Abe et al., 2006; Terai and Nakagawa, 2010). These methods are effective to a degree, but lack the flexibility necessary to instantiate natural language metaphors. 2 Consider the metaphor"
2021.conll-1.26,N19-1092,0,0.0226653,"work in computational metaphor generation involves generating simple &quot;A is like B&quot; expressions, based on probabilistic relationships between words (Abe et al., 2006; Terai and Nakagawa, 2010). These methods are effective to a degree, but lack the flexibility necessary to instantiate natural language metaphors. 2 Consider the metaphor &quot;Her husband abuses alcohol.&quot; (Mohammad et al., 2016): it entails the literal paraphrase &quot;Her husband drinks alcohol&quot;, but the reverse is not necessarily true. 324 Metaphor generation has recently seen significant advances due to deep pretrained language models. Yu and Wan (2019) use neural models to generate metaphoric expressions in an unsupervised manner. They identify source and target verbs automatically from corpora, and use these to train a neural language model. However, they are generating metaphors without regard to reference texts from metaphorically trained language models, and the outputs bear no relation to the inputs. With regard to paraphrasing, Stowe et al. (2020) use a metaphor masking process to generate parallel training data in which key metaphoric words are hidden, causing the resulting seq2seq model to generate metaphoric words. Chakrabarty et a"
2021.dash-1.6,D11-1072,0,0.0794692,"k 0.8 0.7 MFLE baseline LightGBM RankSVM 0.6 0.5 35k 0.4 0 2.5k 5k 7.5k 10k Number of annotations Accuracy@5 1.0 Accuracy@5 Accuracy@5 Accuracy@1 AIDA-CoNLL 12.5k 0 100 200 300 400 500 0.8 0.7 MFLE baseline LightGBM RankSVM 0.6 0.5 0.4 0 100 200 300 400 Number of annotations 500 Figure 1: Human-in-the-loop simulation results for our three datasets and models. One can see that the model achieves good Accuracy@5 with only a few annotations, especially for the RankSVM. Datasets We use the following three datasets for validating our approach: 1) the AIDA-YAGO stateof-the art dataset introduced by Hoffart et al. (2011). 2) Women Writers Online3 is a collection of texts by pre-Victorian women writers. It includes texts on a wide range of topics and from various genres including poems, plays, and novels. 3) The 1641 Depositions4 contain legal texts in form of court witness statements recorded after the Irish Rebellion of 1641. 3 User Study In order to validate the viability of our approach in a realistic scenario, we conduct a user study. For that, we augmented the already existing annotation tool INCEpTION (Klie et al., 2018) with our Human-In-The-Loop entity ranking and automatic suggestions. We let five us"
2021.dash-1.6,C18-2002,1,0.883547,"Missing"
2021.dash-1.6,W18-5519,0,0.014082,"ext and the candidate description as well as dense word and sentence embeddings of the descriptions. Introduction Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5th Earl of Cork, thereby disambiguating it. EL is highly relevant in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 2014), semantic enrichment (Schlögl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018). In these scenarios, the first crucial step is typically to annotate data. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we have developed a novel Human-In-The-Loop (HITL) entity linking approach. It helps annotators finding entity mentions in the text and linking them to the correct knowledge base entries. The more mentions get linked over time, the better the annotation support will be. 1 https://inception-project.github.io https://github.com/UKPLab/ acl2020-interactive-entity-linking 2 41 Proceedings of the 2nd Workshop on Data S"
2021.dash-1.6,N10-1072,0,0.0151912,"annotator selects a span that contains an entity. Then, they search for the correct entity in a KB. These search results are reranked to rank more suitable candidates higher. Each candidate from the knowledge base is assumed to have a label and a description. To speed up this annotation process, we support users twofold: To find suitable spans, we provide recommenders that suggest potential entity spans. They can also classify these entity spans (e.g. as person, location, etc.). These recommenders learn from new annotations and are retrained in the background. For candidate ranking, we follow Zheng et al. (2010) and model it as a learning-to-rank problem: given a marked span, search query and a list of candidates, sort the candidates so that the most relevant candidate is at the top. By selecting an entity label from the candidate list, users express that the selected one was preferred over all other candidates. These preferences are used to train state-of-the-art pairwise learning-to-rank models from the literature: the gradient boosted trees variant LightGBM (Ke et al., 2017) and RankSVM (Joachims, 2002). The continuously updated models improve over time with an increasing number of annotations. As"
2021.eacl-main.39,2020.deelio-1.5,1,0.647692,"Missing"
2021.eacl-main.39,Q17-1026,1,0.818565,"of each target task simultaneously. Using this approach, the network captures the common structure underlying all the target tasks. However, multi-task learning requires simul487 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 487–503 April 19 - 23, 2021. ©2021 Association for Computational Linguistics taneous access to all tasks during training. Adding new tasks thus requires complete joint retraining. Further, it is difficult to balance multiple tasks and train a model that solves each task equally well. As has been shown in Lee et al. (2017), these models often overfit on low resource tasks and underfit on high resource tasks. This makes it difficult to effectively transfer knowledge across tasks with all the tasks being solved equally well (Pfeiffer et al., 2020b), thus considerably limiting the applicability of multi-task learning in many scenarios. Recently, adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have emerged as an alternative training strategy. Adapters do not require fine-tuning of all parameters of the pretrained model, and instead introduce a small number of task specific parameters — while keeping the under"
2021.eacl-main.39,2021.acl-long.353,0,0.19641,"distributed across multiple tasks than adapters in earlier layers. The potential reason for this is that the last adapters are not encapsulated between frozen pretrained layers, and can thus be considered as an extension of the preContemporary Work In contemporaneous work, other approaches for parameter efficient fine-tuning have been proposed. Guo et al. (2020) train sparse “diff” vectors which are applied on top of pretrained frozen parameter vectors. Ravfogel and Goldberg (2021) only finetune bias terms of the pretrained language models, achieving similar results as full model finetuning. Li and Liang (2021) propose prefix-tuning for natural language generation tasks. Here, continuous task-specific vectors are trained while the remaining model is kept frozen. These alternative, parameter-efficient fine-tuning strategies all encapsulate the idiosyncratic task-specific information in designated parameters, creating the potential for new composition approaches of multiple tasks. R¨uckl´e et al. (2020a) analyse the training and inference efficiency of adapters and AdapterFusion. For AdapterFusion, they find that adding more tasks to the set of adapters results in a linear increase of computational co"
2021.eacl-main.39,P17-1001,0,0.0343179,"Missing"
2021.eacl-main.39,P19-1441,0,0.0245151,"terFusion Adapter Add & Norm Feed Forward Add & Norm Multi-Head Attention Figure 1: AdapterFusion architecture inside a transformer (Vaswani et al., 2017). The AdapterFusion component takes as input the representations of multiple adapters trained on different tasks and learns a parameterized mixer of the encoded information. Introduction The most commonly used method for solving NLU tasks is to leverage pretrained models, with the dominant architecture being a transformer (Vaswani et al., 2017), typically trained with a language modelling objective (Devlin et al., 2019; Radford et al., 2018; Liu et al., 2019b). Transfer to a task of interest is achieved by fine-tuning all the weights of the pretrained model on that single task, often yielding state-of-the-art results (Zhang and Yang, 2017; Ruder, 2017; Howard and Ruder, 2018; Peters et al., 2019). However, each task of interest requires all the parameters of the network to be fine-tuned, which results in a specialized model for each task. There are two approaches for sharing information across multiple tasks. The first consists of starting from the pretrained language model and sequentially fine-tuning on each of the tasks one by one (Phang et al"
2021.eacl-main.39,P11-1015,0,0.0269545,"oid introducing additional capacity. 4.2 Tasks and Datasets We briefly summarize the different types of tasks that we include in our experiments, and reference the related datasets accordingly. A detailed descriptions can be found in Appendix A.1. Commonsense reasoning is used to gauge whether the model can perform basic reasoning skills: Hellaswag (Zellers et al., 2018, 2019), Winogrande (Sakaguchi et al., 2020), CosmosQA (Huang et al., 2019), CSQA (Talmor et al., 2019), SocialIQA (Sap et al., 2019). Sentiment analysis predicts whether a given text has a positive or negative sentiment: IMDb (Maas et al., 2011), SST (Socher et al., 2013). Natural language inference predicts whether one sentence entails, contradicts, or is neutral to another: MNLI (Williams et al., 2018), SciTail (Khot et al., 2018), SICK (Marelli et al., 2014), RTE (as combined by Wang et al. (2018)), CB (De Marneffe et al., 2019). Sentence relatedness captures whether two sentences include similar content: MRPC (Dolan and Brockett, 2005), QQP5 . We also use an argument mining Argument (Stab et al., 2018) and reading comprehension BoolQ (Clark et al., 2019) dataset. 5 Results We present results for all 16 datasets in Table 1. For re"
2021.eacl-main.39,marelli-etal-2014-sick,0,0.0208113,"can be found in Appendix A.1. Commonsense reasoning is used to gauge whether the model can perform basic reasoning skills: Hellaswag (Zellers et al., 2018, 2019), Winogrande (Sakaguchi et al., 2020), CosmosQA (Huang et al., 2019), CSQA (Talmor et al., 2019), SocialIQA (Sap et al., 2019). Sentiment analysis predicts whether a given text has a positive or negative sentiment: IMDb (Maas et al., 2011), SST (Socher et al., 2013). Natural language inference predicts whether one sentence entails, contradicts, or is neutral to another: MNLI (Williams et al., 2018), SciTail (Khot et al., 2018), SICK (Marelli et al., 2014), RTE (as combined by Wang et al. (2018)), CB (De Marneffe et al., 2019). Sentence relatedness captures whether two sentences include similar content: MRPC (Dolan and Brockett, 2005), QQP5 . We also use an argument mining Argument (Stab et al., 2018) and reading comprehension BoolQ (Clark et al., 2019) dataset. 5 Results We present results for all 16 datasets in Table 1. For reference, we also include the adapter architecture of Houlsby et al. (2019), ST-AHoulsby , which has twice as many parameters compared to ST-A. To provide a fair comparison to Stickland and Murray (2019) we primarily expe"
2021.eacl-main.39,2020.emnlp-main.361,0,0.22089,"Missing"
2021.eacl-main.39,2020.acl-main.467,0,0.0195111,"for which we learn the optimal parameters Θm through minimizing the task’s loss on its training data. 2.1 Current Approaches to Transfer Learning There are two predominant approaches to achieve sharing of information from one task to another. 2.1.1 Sequential Fine-Tuning This involves sequentially updating all the weights of the model on each task. For a set of N tasks, the order of fine-tuning is defined and at each step the model is initialized with the parameters learned through the previous step. However, this approach does not perform well beyond two sequential tasks (Phang et al., 2018; Pruksachatkun et al., 2020) due to catastrophic forgetting. 2.1.2 Multi-Task Learning (MTL) All tasks are trained simultaneously with the aim of learning a shared representation that will enable the model to generalize better on each task (Caruana, 1997; Collobert and Weston, 2008; Nam et al., 2014; Liu et al., 2016, 2017; Zhang and Yang, 2017; Ruder, 2017; Ruder et al., 2019; Sanh et al., 2019; Pfeiffer et al., 2020b, inter alia). ! N X Θ0→{1,...,N } ← argmin Ln (Dn ; Θ0 ) Θ 2.2.1 Single-Task Adapters (ST-A) For each of the N tasks, the model is initialized with parameters Θ0 . In addition, a set of new and randomly in"
2021.eacl-main.39,W19-4302,0,0.0615418,"Missing"
2021.eacl-main.39,2020.emnlp-demos.7,1,0.852029,"Missing"
2021.eacl-main.39,D19-1410,1,0.848301,", and 12 and ST-A in Figure 4 (see Appendix Figure 6 for the remaining layers). We find that tasks which do not benefit from AdapterFusion tend to more strongly activate their own adapter at every layer (e.g. Argument, HellaSwag, MNLI, QQP, SciTail). This confirms that AdapterFusion only extracts information from adapters if they are beneficial for the target task m. We further find that MNLI is a useful intermediate task that benefits a large number of target tasks, e.g. BoolQ, SICK, CSQA, SST-2, CB, MRPC, RTE, which is in line with previous work (Phang et al., 2018; Conneau and Kiela, 2018; Reimers and Gurevych, 2019). Similarly, QQP is utilized by a large number of tasks, e.g. SICK, IMDB, RTE, CB, MRPC, SST-2. Most importantly, tasks with small datasets such as CB, RTE, and MRPC often strongly rely on adapters trained on large datasets such as MNLI and QQP. Interestingly, we find that the activations in layer 12 are considerably more distributed across multiple tasks than adapters in earlier layers. The potential reason for this is that the last adapters are not encapsulated between frozen pretrained layers, and can thus be considered as an extension of the preContemporary Work In contemporaneous work, ot"
2021.eacl-main.39,2020.emnlp-main.617,1,0.585903,"Missing"
2021.eacl-main.39,2020.emnlp-main.194,1,0.847232,"Missing"
2021.eacl-main.39,W18-5446,0,0.037738,"reasoning is used to gauge whether the model can perform basic reasoning skills: Hellaswag (Zellers et al., 2018, 2019), Winogrande (Sakaguchi et al., 2020), CosmosQA (Huang et al., 2019), CSQA (Talmor et al., 2019), SocialIQA (Sap et al., 2019). Sentiment analysis predicts whether a given text has a positive or negative sentiment: IMDb (Maas et al., 2011), SST (Socher et al., 2013). Natural language inference predicts whether one sentence entails, contradicts, or is neutral to another: MNLI (Williams et al., 2018), SciTail (Khot et al., 2018), SICK (Marelli et al., 2014), RTE (as combined by Wang et al. (2018)), CB (De Marneffe et al., 2019). Sentence relatedness captures whether two sentences include similar content: MRPC (Dolan and Brockett, 2005), QQP5 . We also use an argument mining Argument (Stab et al., 2018) and reading comprehension BoolQ (Clark et al., 2019) dataset. 5 Results We present results for all 16 datasets in Table 1. For reference, we also include the adapter architecture of Houlsby et al. (2019), ST-AHoulsby , which has twice as many parameters compared to ST-A. To provide a fair comparison to Stickland and Murray (2019) we primarily experiment with BERT-baseuncased. We additio"
2021.eacl-main.44,N19-1170,0,0.0283606,"ilment may acceptably be neutral, while contradictory responses are a serious consistency error. The sub-reward for the factual consistency with persona facts is not sufficient to generate a semantically plausible response. The agent can maximize 551 this sub-reward merely by repeating the persona’s facts and ignoring topical coherence (for an example, see Appendix A). To prevent such behavior, we assess the topical coherence and grammatical fluency of a response by the following sub-rewards. Topical coherence sub-reward (R2 ) Topical coherence is a crucial property of high-quality dialogues (See et al., 2019; Mesgar et al., 2020). We capture the topical coherence of response r to the last utterance uT 1 in dialogue history by representing them using an average pooling layer over their token representations obtained by BERT. Inspired by Baheti et al. (2018) and See et al. (2019), we use cosine similarity between ~r and ~uT 1 as a proxy for topical coherence: R2 = cos(~r, ~uT 1) . ↵ NLL(r) , ↵ (5) (6) where parameter ↵ is used to map any value of NLL that is greater than ↵ to ↵ so that the output of R3 will be between 0 and 1. To retain the language quality of responses similar to those of Transfer"
2021.eacl-main.44,P19-1363,0,0.378491,"and dialogue history. After generating the last token, i.e. ‘<EOS>’, or reaching the maximum length allowed for a response, a reward model assesses the quality of the response (Figure 1). The reward value is used to fine-tune the parameters of TransferTranfo towards the policy that generates a response that is factually consistent with persona facts and also semantically plausible. Action We consider generating each token of a response as an action performed by the TransferTransfo model: M Y k=2 P✓ (tk |t1..k 1 , s) 2 R2 + 3 R3 + 4 R4 , (2) Persona consistency sub-reward (R1 ) Recent studies (Welleck et al., 2019; Dziri et al., 2019) show that consistency with factual information, such as persona facts, can be characterized as a natural language inference (NLI) problem, where entailment labels can be taken as consistent labels and contradiction labels as inconsistent labels. Building on this, we use an NLI model to design this sub-reward. We define our NLI model using BERT as a bidirectional contextualized encoder: TransferTransfo P✓ (r|s) = P✓ (t1 |s) + where 1 + 2 + 3 + 4 = 1. These weights can be tuned as described below to prevent biasing the policy toward a particular sub-reward. ... p: persona f"
2021.emnlp-main.351,2020.emnlp-main.92,1,0.863934,"Missing"
2021.emnlp-main.351,W13-2322,0,0.119156,"StructAdapt More power to her for her achievements. fi Figure 1: (a) AMR for the sentence More power to her for her achievements. While in (b) the pretrained model gets as input the graph linearization, in (c) it additionally receives the graph connectivity information. Introduction Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Fla"
2021.emnlp-main.351,P18-1026,0,0.0789485,"nguage. As a result, knowledge from large-scale pretraining intuitively cannot be fully transferred, and finetuning a sentence representation using linearized graphs can lead to catastrophic forgetting of such distributional knowledge (Goodfellow et al., 2014; Kirkpatrick et al., 2017). Second, a linearized representation weakens structural information in the original graphs by diluting the explicit connectivity information (i.e., which nodes are connected to each other), and PLMs must infer how edge connections are specified in the sequence. This fact was also observed by Song et al. (2018), Beck et al. (2018) and Ribeiro et al. (2019), who show that GNN encoders outperform sequential encoders for AMR-to-text generation without pretraining. To mitigate the issues, we aim to explicitly encode the graph data into a PLM without contaminating its original distributional knowledge. To this end, we propose S TRUCTA DAPT, a novel structureaware adapter that effectively allows leveraging the input graph structure into PLMs (See Figure 1c). The main idea is to add layer-wise modules, which extract information from the pretrained layers and make use of it in a graph-structure encoding. As shown in Figure 2,"
2021.emnlp-main.351,2021.acl-long.324,1,0.759346,"ral language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2017) for representing the input graph. Pretrained language models"
2021.emnlp-main.351,W17-3518,0,0.0149925,"graph structure power :ARG1 she :mod more :purpose achieve :ARG0 she Pretrained Model with StructAdapt More power to her for her achievements. fi Figure 1: (a) AMR for the sentence More power to her for her achievements. While in (b) the pretrained model gets as input the graph linearization, in (c) it additionally receives the graph connectivity information. Introduction Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summar"
2021.emnlp-main.351,Q19-1019,0,0.0190683,"on Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2"
2021.emnlp-main.351,2021.acl-long.381,0,0.0669303,"Missing"
2021.emnlp-main.351,2020.coling-main.218,0,0.0422375,"Missing"
2021.emnlp-main.351,N13-1132,0,0.0213716,"Missing"
2021.emnlp-main.351,P18-1031,0,0.0678754,"Missing"
2021.emnlp-main.351,2021.findings-acl.82,0,0.0507666,"Missing"
2021.emnlp-main.351,2020.inlg-1.14,0,0.0438655,"Missing"
2021.emnlp-main.351,P17-1014,0,0.0285109,"Missing"
2021.emnlp-main.351,2020.deelio-1.5,1,0.701379,"e PLM, yielding a high degree of parameter sharing for graph-to-text tasks. 4.2 Graph Representation We convert each G0 into a bipartite graph G1 = (V1 , E1 ), replacing each labeled edge (u, r, v) ∈ E0 with two unlabeled edges e1 = (u, r) and e2 = (r, v). Similar to Beck et al. (2018), this process converts the graph into its unlabeled version. Figure 3 shows an (a) AMR subgraph and (b) its unlabeled representation. Note that PLMs typically use a vocabulary with subword units (Sennrich et al., 2016). This presents a challenge in how to represent such a graph using subword tokens. Inspired by Ribeiro et al. (2020b), we transform each G1 into a new token graph G = (V, E), where each token of a node in V1 becomes a node v ∈ V. We convert each edge (u1 , v1 ) ∈ E1 into a set of edges and connect every token of u1 to every token of v1 . That is, an edge (u, v) will belong to E if and only if there exists an edge (u1 , v1 ) ∈ E1 such that u ∈ u1 and v ∈ v1 , where u1 and v1 are seen as sets of tokens. Figure 3c shows an example of the token graph. 4.3 Method employs a two-layer architecture in order to re-purpose the PLM for the graph-to-text task using a small number of new parameters. Formally, for each"
2021.emnlp-main.351,2020.acl-main.703,0,0.0594932,"oints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2017) for representing the input graph. Pretrained language models (PLMs) (Devlin et al., 2019; Liu et al., 2020; Radford et al., 2019; Lewis et al., 2020) have been shown useful as a general text representation method, giving much improved results on a wide range of tasks (Wang et al., 2018, 2019). However, they cannot be directly leveraged to benefit AMR-to-text generation, and more generally graph-to-text generation, due to the structural nature of the input. One solution is to transform the structured input into a se4269 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4269–4282 c November 7–11, 2021. 2021 Association for Computational Linguistics quence, which can be directly fed into PLMs (See F"
2021.emnlp-main.351,2021.acl-long.353,0,0.0464137,"Missing"
2021.emnlp-main.351,C18-1101,0,0.288735,"Missing"
2021.emnlp-main.351,2021.eacl-main.129,0,0.0300749,"es the refined structural node representation zlv based on the local node context, using as input the global representation hlv generated by the current PLM encoder layer. In this way, the model is able to capture both the global context based on the PLM linguistic knowledge and the local context based on the graph knowledge. Finally, we employ A DAPT into the decoder in order to adapt the language model to the graph-to-text task. Evaluation. We evaluate the results with BLEU (Papineni et al., 2002) and chrF++ (Popovi´c, 2015) metrics. We also report the meaning (M) component of the MF-score (Opitz and Frank, 2021), which measures how well the source AMR graph can be reconstructed from the generated sentence. We use BERTScore (Zhang et al., 2020a) allowing a semantic evaluation that depends less on the surface forms. Finally, we also perform a human evaluation (§5.2). 5.1 Main Results We compare S TRUCTA DAPT with four methods: finetuning (F INE - TUNE), fine-tuning only the top or bottom 2 layers (FT- TOP 2, FT- BOTTOM 2) and A DAPT. All 5 Preliminary experiments with other architecture configurations led to worse or similar performance. 4273 6 Hyperparameter details are in the appendix A. Zhang et al."
2021.emnlp-main.351,2020.tacl-1.34,0,0.0136535,"aningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2017) for representing the input graph. Pre"
2021.emnlp-main.351,P02-1040,0,0.110337,"s under relation r ∈ R, and Wrl ∈ Rm×d encodes the edge type between the nodes u and v. Note that S TRUCTA DAPT computes the refined structural node representation zlv based on the local node context, using as input the global representation hlv generated by the current PLM encoder layer. In this way, the model is able to capture both the global context based on the PLM linguistic knowledge and the local context based on the graph knowledge. Finally, we employ A DAPT into the decoder in order to adapt the language model to the graph-to-text task. Evaluation. We evaluate the results with BLEU (Papineni et al., 2002) and chrF++ (Popovi´c, 2015) metrics. We also report the meaning (M) component of the MF-score (Opitz and Frank, 2021), which measures how well the source AMR graph can be reconstructed from the generated sentence. We use BERTScore (Zhang et al., 2020a) allowing a semantic evaluation that depends less on the surface forms. Finally, we also perform a human evaluation (§5.2). 5.1 Main Results We compare S TRUCTA DAPT with four methods: finetuning (F INE - TUNE), fine-tuning only the top or bottom 2 layers (FT- TOP 2, FT- BOTTOM 2) and A DAPT. All 5 Preliminary experiments with other architecture"
2021.emnlp-main.351,2020.emnlp-main.89,0,0.021665,"her to achieve. (c) Lightweight ne-tuning with graph structure power :ARG1 she :mod more :purpose achieve :ARG0 she Pretrained Model with StructAdapt More power to her for her achievements. fi Figure 1: (a) AMR for the sentence More power to her for her achievements. While in (b) the pretrained model gets as input the graph linearization, in (c) it additionally receives the graph connectivity information. Introduction Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/S"
2021.emnlp-main.351,2021.eacl-main.39,1,0.837333,"Missing"
2021.emnlp-main.351,2020.emnlp-demos.7,1,0.841136,"Missing"
2021.emnlp-main.351,2020.emnlp-main.617,1,0.83897,"Missing"
2021.emnlp-main.351,W15-3049,0,0.0193793,"Missing"
2021.emnlp-main.351,W16-6603,0,0.0583696,"Missing"
2021.emnlp-main.351,D19-1314,1,0.907719,"sks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2017) for representing"
2021.emnlp-main.351,2021.emnlp-main.57,1,0.829113,"Missing"
2021.emnlp-main.351,2020.tacl-1.38,1,0.845681,"e PLM, yielding a high degree of parameter sharing for graph-to-text tasks. 4.2 Graph Representation We convert each G0 into a bipartite graph G1 = (V1 , E1 ), replacing each labeled edge (u, r, v) ∈ E0 with two unlabeled edges e1 = (u, r) and e2 = (r, v). Similar to Beck et al. (2018), this process converts the graph into its unlabeled version. Figure 3 shows an (a) AMR subgraph and (b) its unlabeled representation. Note that PLMs typically use a vocabulary with subword units (Sennrich et al., 2016). This presents a challenge in how to represent such a graph using subword tokens. Inspired by Ribeiro et al. (2020b), we transform each G1 into a new token graph G = (V, E), where each token of a node in V1 becomes a node v ∈ V. We convert each edge (u1 , v1 ) ∈ E1 into a set of edges and connect every token of u1 to every token of v1 . That is, an edge (u, v) will belong to E if and only if there exists an edge (u1 , v1 ) ∈ E1 such that u ∈ u1 and v ∈ v1 , where u1 and v1 are seen as sets of tokens. Figure 3c shows an example of the token graph. 4.3 Method employs a two-layer architecture in order to re-purpose the PLM for the graph-to-text task using a small number of new parameters. Formally, for each"
2021.emnlp-main.351,2021.textgraphs-1.2,1,0.842111,"Missing"
2021.emnlp-main.351,P16-1162,0,0.0361553,"ration as we will show in §5 and §6. Moreover, different adapters for distinct graph domains can be used with the same PLM, yielding a high degree of parameter sharing for graph-to-text tasks. 4.2 Graph Representation We convert each G0 into a bipartite graph G1 = (V1 , E1 ), replacing each labeled edge (u, r, v) ∈ E0 with two unlabeled edges e1 = (u, r) and e2 = (r, v). Similar to Beck et al. (2018), this process converts the graph into its unlabeled version. Figure 3 shows an (a) AMR subgraph and (b) its unlabeled representation. Note that PLMs typically use a vocabulary with subword units (Sennrich et al., 2016). This presents a challenge in how to represent such a graph using subword tokens. Inspired by Ribeiro et al. (2020b), we transform each G1 into a new token graph G = (V, E), where each token of a node in V1 becomes a node v ∈ V. We convert each edge (u1 , v1 ) ∈ E1 into a set of edges and connect every token of u1 to every token of v1 . That is, an edge (u, v) will belong to E if and only if there exists an edge (u1 , v1 ) ∈ E1 such that u ∈ u1 and v ∈ v1 , where u1 and v1 are seen as sets of tokens. Figure 3c shows an example of the token graph. 4.3 Method employs a two-layer architecture in"
2021.emnlp-main.351,Q19-1002,1,0.914222,"Missing"
2021.emnlp-main.351,P18-1150,1,0.734082,"rmation. Introduction Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers"
2021.emnlp-main.351,2020.findings-emnlp.199,0,0.0917268,"Missing"
2021.emnlp-main.351,2020.emnlp-main.169,0,0.307718,"s on the LDC2017T10 test set. Mean (±s.d.) over 4 seeds. 5 Experiments Our models are initialized with pre-trained T5 (Raffel et al., 2019), but our approach can be combined with other PLMs such as BART (Lewis et al., 2020). Our implementation is based on Hugging Face Transformer models (Wolf et al., 2019). We use T5base for all experiments and report results with T5large for the test sets.6 We use the Adam optimizer (Kingma and Ba, 2015) and employ a linearly decreasing learning rate schedule without warm-up. BLEU is used for the stopping criterion. Following recent work (Mager et al., 2020; Zhang et al., 2020b), we evaluate our proposed models on LDC2017T10 and LDC2020T02 corpora. r∈R u∈Nr (v) where R denotes the set of relations, i.e., the edge types default and reverse, Nr (v) denotes the set of neighbors under relation r ∈ R, and Wrl ∈ Rm×d encodes the edge type between the nodes u and v. Note that S TRUCTA DAPT computes the refined structural node representation zlv based on the local node context, using as input the global representation hlv generated by the current PLM encoder layer. In this way, the model is able to capture both the global context based on the PLM linguistic knowledge and t"
2021.emnlp-main.351,W18-5446,0,0.0603749,"Missing"
2021.emnlp-main.38,P98-1012,0,0.773773,"Missing"
2021.emnlp-main.38,P19-1409,1,0.853304,"adhyay et al., 2016). We train δ on the gold development split of each respective corpus. that H YPER C OREF data is most helpful when it is used in large quantities. Using small subsets limits the diversity of training events and bears a greater risk of overfitting to noise. Error Analysis We sample 10 test clusters from each gold-standard corpus and manually compare the predictions of (1) both Sss models, (2) of the Ssg models optimized on the respective corpus and (3) of the respective in-domain Sgg model without augmentation. On ECB+, we make the common observation (Upadhyay et al., 2016; Barhom et al., 2019; Bugert et al., 2021) that the in-domain model primarily matches event actions with similar surface form. Sss and Sgg models are more liberal with merging paraphrases (such as “revealed” or “unveiled”) but overmerge unrelated mentions more frequently as a result. Compared to ECB+, FCC-T exhibits greater ambiguity of event actions (“win”, “draw”, “final” can refer to many different sports matches). The in-domain model rarely clusters such mentions, opposed to the Sss and Ssg models which merged such mentions if nearby participant mentions were compatible. Our GVC analysis mirrors these finding"
2021.emnlp-main.38,2020.emnlp-demos.27,0,0.0322295,"Missing"
2021.emnlp-main.38,2021.findings-emnlp.225,0,0.011908,"quires great effort since language-specific guidelines (Minard et al., 2016) and enough annotators with proficiency in the target language are required. As a consequence, CDCR corpora had to compromise on size, domain coverage, and the density of annotated mentions and coreference links, as well as language coverage. This data bottleneck is problematic for three reasons. Firstly, recent state-of-the-art CDCR systems are based on pretrained language models (Devlin et al., 2019; Liu et al., 2019) fine-tuned with supervised learning on human-annotated corpora (Zeng et al., 2020; Yu et al., 2020; Caciularu et al., 2021; Cattan et al., 2021). Achieving top results with 1 such models still requires high-quality labeled data Data and model available at https://github.com/ UKPLab/emnlp2021-hypercoref-cdcr for fine-tuning (Gururangan et al., 2020), yet the 471 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 471–491 c November 7–11, 2021. 2021 Association for Computational Linguistics size of current corpora is insufficient for training large systems. Secondly, because corpora need to make compromises on domain coverage, the domain coverage of all existing CDCR corpor"
2021.emnlp-main.38,2021.findings-acl.453,0,0.193168,"e language-specific guidelines (Minard et al., 2016) and enough annotators with proficiency in the target language are required. As a consequence, CDCR corpora had to compromise on size, domain coverage, and the density of annotated mentions and coreference links, as well as language coverage. This data bottleneck is problematic for three reasons. Firstly, recent state-of-the-art CDCR systems are based on pretrained language models (Devlin et al., 2019; Liu et al., 2019) fine-tuned with supervised learning on human-annotated corpora (Zeng et al., 2020; Yu et al., 2020; Caciularu et al., 2021; Cattan et al., 2021). Achieving top results with 1 such models still requires high-quality labeled data Data and model available at https://github.com/ UKPLab/emnlp2021-hypercoref-cdcr for fine-tuning (Gururangan et al., 2020), yet the 471 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 471–491 c November 7–11, 2021. 2021 Association for Computational Linguistics size of current corpora is insufficient for training large systems. Secondly, because corpora need to make compromises on domain coverage, the domain coverage of all existing CDCR corpora is limited even when"
2021.emnlp-main.38,P08-1090,0,0.211267,"Missing"
2021.emnlp-main.38,D17-1226,0,0.0563083,"Missing"
2021.emnlp-main.38,2021.eacl-main.101,0,0.0588822,"Missing"
2021.emnlp-main.38,cybulska-vossen-2014-using,0,0.370775,"checking. hyperlink reference describes event conviction of Roger Stone mentions event Mueller indicted Stone in January on five counts of lying to Congress [...] Stone is being prosecuted for allegedly lying, but in his case to Congress [...] implicit cross-document event coreference link Figure 1: Three online news articles interconnected by two hyperlinks. The relation between the underlined hyperlink anchors can be interpreted as cross-document event coreference. Annotation of CDCR is laborious and expensive, requiring expert annotation which can span weeks for several hundred documents (Cybulska and Vossen, 2014b; Vossen et al., 2018). Crowdsourcing annotation has been proposed, however this requires extensive training of annotators (Bornstein et al., 2020) or post-processing by experts (Bugert et al., 2020, 2021), precluding large-scale studies. Annotating CDCR data in a different language requires great effort since language-specific guidelines (Minard et al., 2016) and enough annotators with proficiency in the target language are required. As a consequence, CDCR corpora had to compromise on size, domain coverage, and the density of annotated mentions and coreference links, as well as language cove"
2021.emnlp-main.38,2020.lrec-1.8,0,0.0378679,"Missing"
2021.emnlp-main.38,N19-1423,0,0.00604918,"or post-processing by experts (Bugert et al., 2020, 2021), precluding large-scale studies. Annotating CDCR data in a different language requires great effort since language-specific guidelines (Minard et al., 2016) and enough annotators with proficiency in the target language are required. As a consequence, CDCR corpora had to compromise on size, domain coverage, and the density of annotated mentions and coreference links, as well as language coverage. This data bottleneck is problematic for three reasons. Firstly, recent state-of-the-art CDCR systems are based on pretrained language models (Devlin et al., 2019; Liu et al., 2019) fine-tuned with supervised learning on human-annotated corpora (Zeng et al., 2020; Yu et al., 2020; Caciularu et al., 2021; Cattan et al., 2021). Achieving top results with 1 such models still requires high-quality labeled data Data and model available at https://github.com/ UKPLab/emnlp2021-hypercoref-cdcr for fine-tuning (Gururangan et al., 2020), yet the 471 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 471–491 c November 7–11, 2021. 2021 Association for Computational Linguistics size of current corpora is insufficient for"
2021.emnlp-main.38,2021.naacl-main.198,0,0.0551052,"Missing"
2021.emnlp-main.38,2020.nuse-1.8,0,0.0630366,"Missing"
2021.emnlp-main.38,2020.acl-main.740,0,0.0436804,"Missing"
2021.emnlp-main.38,W13-1203,0,0.03783,"Missing"
2021.emnlp-main.38,D17-1018,0,0.0638314,"Missing"
2021.emnlp-main.38,H05-1004,0,0.352606,"Missing"
2021.emnlp-main.38,P14-5010,0,0.00472116,"Missing"
2021.emnlp-main.38,L16-1699,0,0.0573802,"Missing"
2021.emnlp-main.38,P16-1060,0,0.0214421,"pant mentions were compatible. Our GVC analysis mirrors these findings, with the BBC Sss and Ssg models performing noticeably better merges than other models, particularly for clusters with varied actions (“went off”, “shooting”, “discharged”) where a mention’s context matters. In summary, models trained on H YPER C OREF exhibit greater context sensitivity. Results Tables 4a and 4b show results of the state-of-the-art system and baseline results for each data scarcity scenario on each corpus. The complete results for all metrics, including the link-based entity-aware coreference metric (LEA) (Moosavi and Strube, 2016), are reported in Appendix A.4. In Sgg , the model trained on ECB+ generalizes best. This is due to the broad domain coverage of ECB+ which includes sports and gun violence – the two topics on which FCC-T and GVC specialize. The performance of Sgg models trained on an equal amount of gold and silver data from either ABC or BBC is mixed: test performance on individual corpora is at times higher, but aggregated performance across corpora declines. Looking at the most difficult scenario Sss , the performance of the models trained and optimized entirely on silver H YPER C OREF data is highly compe"
2021.emnlp-main.38,D16-1038,0,0.0594453,"Missing"
2021.emnlp-main.38,W12-4501,0,0.0491485,"sing gold event mention spans due to non-exhaustive event mention annotations in FCCT and GVC (Bugert et al., 2021). We therefore do not use the mention detection mechanism of Cattan et al. (2021) in this set of experiments. We evaluate 6 5 Please refer to original publication for further details. We report additional training and setup details in Appendix A.3. Coreference Resolution Experiments Using 1,2,3-gram TF-IDF vectors, pages with over 0.25 cosine similarity to gold-standard dev or test subtopics were discarded: 113 documents for ABC, and 89 for BBC. 475 CDCR with the CoNLL F1 metric (Pradhan et al., 2012). Baselines We report two common CDCR baselines. The lemma baseline clusters all event mentions with the same head lemma together. lemmaδ is a trainable variant of lemma which restricts merging to document pairs which exceed a TF-IDF cosine similarity of δ (Upadhyay et al., 2016). We train δ on the gold development split of each respective corpus. that H YPER C OREF data is most helpful when it is used in large quantities. Using small subsets limits the diversity of training events and bears a greater risk of overfitting to noise. Error Analysis We sample 10 test clusters from each gold-standa"
2021.emnlp-main.38,2020.acl-main.467,0,0.0287149,"Missing"
2021.emnlp-main.38,P14-6004,0,0.0757925,"Missing"
2021.emnlp-main.38,2021.hcinlp-1.8,0,0.0246997,"Missing"
2021.emnlp-main.38,2020.coling-main.275,0,0.015016,"CDCR data in a different language requires great effort since language-specific guidelines (Minard et al., 2016) and enough annotators with proficiency in the target language are required. As a consequence, CDCR corpora had to compromise on size, domain coverage, and the density of annotated mentions and coreference links, as well as language coverage. This data bottleneck is problematic for three reasons. Firstly, recent state-of-the-art CDCR systems are based on pretrained language models (Devlin et al., 2019; Liu et al., 2019) fine-tuned with supervised learning on human-annotated corpora (Zeng et al., 2020; Yu et al., 2020; Caciularu et al., 2021; Cattan et al., 2021). Achieving top results with 1 such models still requires high-quality labeled data Data and model available at https://github.com/ UKPLab/emnlp2021-hypercoref-cdcr for fine-tuning (Gururangan et al., 2020), yet the 471 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 471–491 c November 7–11, 2021. 2021 Association for Computational Linguistics size of current corpora is insufficient for training large systems. Secondly, because corpora need to make compromises on domain coverage, the do"
2021.emnlp-main.38,P19-1353,0,0.0587736,"Missing"
2021.emnlp-main.57,W13-2322,0,0.180259,"Missing"
2021.emnlp-main.57,P18-1026,0,0.0647295,"graphs. Overall, we find that a combination of both strategies further improves the performance, showing that they are complementary for this task. 2 Related Work Approaches for AMR-to-text generation predominantly focus on English, and typically employ an encoder-decoder architecture, employing a linearized representation of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual AMR-to-Text Genera"
2021.emnlp-main.57,2020.emnlp-main.195,0,0.0306526,"owing Fan and generation of text in many different languages (Da- Gardent (2020), we parse English sentences into monte and Cohen, 2018; Zhu et al., 2019). silver AMRs from parallel multilingual corpora While previous work has predominantly focused (S ILVER AMR), resulting in a dataset consisting of on monolingual English settings (Cai and Lam, grammatically correct sentences with noisy AMR 2020b; Bevilacqua et al., 2021), recent work has structures. (2) We leverage machine translation also studied multilinguality in meaning represen- (MT) and translate the English sentences from the tations (Blloshmi et al., 2020; Sheth et al., 2021). gold AMR-to-text corpus to the respective target Whereas Damonte and Cohen (2018) demonstrate languages (S ILVER S ENT), resulting in a dataset with 1 correct AMR structures but potentially unfaithful Our code and checkpoints are available at https://github.com/UKPLab/m-AMR2Text. or non-grammatical sentences. (3) We experiment 742 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 742–750 c November 7–11, 2021. 2021 Association for Computational Linguistics with utilizing the AMR-to-text corpus with both gold English AMR and sen"
2021.emnlp-main.57,2020.acl-main.119,0,0.0561986,"gies further improves the performance, showing that they are complementary for this task. 2 Related Work Approaches for AMR-to-text generation predominantly focus on English, and typically employ an encoder-decoder architecture, employing a linearized representation of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual AMR-to-Text Generation In AMR-to-text generation, we transduce an AMR graph G"
2021.emnlp-main.57,2020.acl-main.640,0,0.0416205,"gies further improves the performance, showing that they are complementary for this task. 2 Related Work Approaches for AMR-to-text generation predominantly focus on English, and typically employ an encoder-decoder architecture, employing a linearized representation of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual AMR-to-Text Generation In AMR-to-text generation, we transduce an AMR graph G"
2021.emnlp-main.57,2020.acl-main.747,0,0.0374779,"approaches. Training Strategies. We propose different training strategies under the setting of §3.2 in order to investigate which combination leads to stronger multilingual AMR-to-text generation. Besides training models using S ILVER AMR or S ILVER S ENT, we investigate different combinations of multi-source training also using G OLDAMR. Main Results. Table 1 shows our main results.8 First, S ILVER AMR substantially outperforms Fan and Gardent (2020) despite being trained on the same amount of silver AMR data. We believe this is because we utilize mT5, whereas Fan and Gardent (2020) use XLM (Conneau et al., 2020), and our parallel data may contain different domain data. S ILVER S ENT considerably outperforms S ILVER AMR in all metrics, despite S ILVER AMR consisting of two orders of magnitude more data. We believe the reasons are twofold: Firstly, the correct semantic structure of gold AMR annotations is necessary to learn a faithful realization; Secondly, S ILVER S ENT provides examples of the same domain as the evaluation test set. We observe similar performance to S ILVER S ENT when training on both G OLDAMR and S ILVER AMR, indicating that the combination of target domain data and gold AMR graphs"
2021.emnlp-main.57,N18-1104,0,0.0843336,"ntences into monte and Cohen, 2018; Zhu et al., 2019). silver AMRs from parallel multilingual corpora While previous work has predominantly focused (S ILVER AMR), resulting in a dataset consisting of on monolingual English settings (Cai and Lam, grammatically correct sentences with noisy AMR 2020b; Bevilacqua et al., 2021), recent work has structures. (2) We leverage machine translation also studied multilinguality in meaning represen- (MT) and translate the English sentences from the tations (Blloshmi et al., 2020; Sheth et al., 2021). gold AMR-to-text corpus to the respective target Whereas Damonte and Cohen (2018) demonstrate languages (S ILVER S ENT), resulting in a dataset with 1 correct AMR structures but potentially unfaithful Our code and checkpoints are available at https://github.com/UKPLab/m-AMR2Text. or non-grammatical sentences. (3) We experiment 742 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 742–750 c November 7–11, 2021. 2021 Association for Computational Linguistics with utilizing the AMR-to-text corpus with both gold English AMR and sentences in multi-source scenarios to enhance multilingual training. Our contributions and the organizatio"
2021.emnlp-main.57,N19-2003,0,0.0151877,"evious work (Fan and et al., 2013), and has recently received much re- Gardent, 2020) while discussing the feasibility of search interest (Ribeiro et al., 2019; Wang et al., multilingual AMR-to-text generation, has inves2020; Mager et al., 2020; Harkous et al., 2020; Fu tigated synthetically generated AMR as the only et al., 2021). AMR has applications to a range of source of silver training data. NLP tasks, including summarization (Hardy and In this paper, we aim to close this gap by providVlachos, 2018) and spoken language understand- ing an extensive analysis of different augmentation ing (Damonte et al., 2019), and has the potential techniques to cheaply acquire silver-standard mulpower of acting as an interlingua that allows the tilingual AMR-to-text data: (1) Following Fan and generation of text in many different languages (Da- Gardent (2020), we parse English sentences into monte and Cohen, 2018; Zhu et al., 2019). silver AMRs from parallel multilingual corpora While previous work has predominantly focused (S ILVER AMR), resulting in a dataset consisting of on monolingual English settings (Cai and Lam, grammatically correct sentences with noisy AMR 2020b; Bevilacqua et al., 2021), recent work ha"
2021.emnlp-main.57,W14-3348,0,0.0248803,"models using mT5base from HuggingFace (Wolf et al., 2020). We use the Adafactor optimizer (Shazeer and Stern, 2018) and employ a linearly decreasing learning rate schedule without warm-up. The hyperparameters we tune include the batch size, number of epochs and learning 4 https://tatoeba.org/ https://github.com/UKPLab/sentencetransformers/tree/master/docs/datasets 6 The English sentences of the parallel corpus are parsed using a state-of-the-art AMR parser (Cai and Lam, 2020a). rate.7 The models are evaluated in the multilingual LDC2020T07 test set, using BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), chrF++ (Popovi´c, 2015) and BERTscore (Zhang et al., 2020) metrics. We compare with a MT baseline – we generate the test set with an AMR-toEnglish model trained with T5 (Ribeiro et al., 2021) and translate the generated English sentences to the target language using MT. For a fair comparison, our MT model is based on mT5 and trained with the same data as the other approaches. Training Strategies. We propose different training strategies under the setting of §3.2 in order to investigate which combination leads to stronger multilingual AMR-to-text generation. Besides training models using S IL"
2021.emnlp-main.57,2020.emnlp-main.231,0,0.461391,"t combining both complementary sources of information further improves multilingual AMR-to-text generation. Our models surpass the previous state of the art for German, Italian, Spanish, and Chinese by a large margin.1 live-01 :ARG1 glorious :ARG0 they AMR-to En Es It Their life looks glorious. Su vida parece gloriosa. La loro vita sembra gloriosa. De Ihr Leben sieht herrlich aus. Zh 他们的⽇⼦看起来很光鲜. Figure 1: A generation example from English AMR to multiple different languages. that parsers can be effectively trained to transform multilingual text into English AMR, Mille et al. (2018, 2019) and Fan and Gardent (2020) discuss 1 Introduction the reverse task, turning meaning representations AMR-to-text generation is the task of recover- into multilingual text, as shown in Figure 1. Howing a text with the same meaning as a given Ab- ever, gold-standard multilingual AMR training data stract Meaning Representation (AMR) (Banarescu is currently scarce, and previous work (Fan and et al., 2013), and has recently received much re- Gardent, 2020) while discussing the feasibility of search interest (Ribeiro et al., 2019; Wang et al., multilingual AMR-to-text generation, has inves2020; Mager et al., 2020; Harkous et"
2021.emnlp-main.57,2021.acl-long.324,1,0.633831,"Missing"
2021.emnlp-main.57,Q19-1019,0,0.110809,"ion of both strategies further improves the performance, showing that they are complementary for this task. 2 Related Work Approaches for AMR-to-text generation predominantly focus on English, and typically employ an encoder-decoder architecture, employing a linearized representation of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual AMR-to-Text Generation In AMR-to-text generation, we trans"
2021.emnlp-main.57,W14-5808,0,0.0597511,"Missing"
2021.emnlp-main.57,D18-1086,0,0.0475649,"Missing"
2021.emnlp-main.57,2020.coling-main.218,0,0.183984,"Missing"
2021.emnlp-main.57,2005.mtsummit-papers.11,0,0.216957,"between English and target languages, we thus aim to identify the best augmentations strategies to achieve multilingual generation. As our monolingual AMR-to-text training dataset, we consider the LDC2017T10 dataset (G OLDAMR), containing English AMR graphs and sentences. We evaluate our different approaches on the multilingual LDC2020T07 test set by Damonte and Cohen (2018) consisting of gold annotations for Spanish (ES), Italian (IT), German (DE) and Chinese (ZH).3 For our multilingual parallel sentence corpus we consider data from different sources. For ES, IT and DE, we use: Europarl-v7 (Koehn, 2005), an aligned corpus of European Union parlia2 For example, for AMR-to-Spanish we use the prefix “translate AMR to Spanish:”. 3 This dataset was constructed by professional translators based on the LDC2017T10 test set. We employ mT5 (Xue et al., 2021), a Transformerbased encoder-decoder architecture (Vaswani et al., 743 BLEU BERTscore ES IT DE ZH All ES IT DE ZH All MT (Fan and Gardent, 2020) Multilingual model (Fan and Gardent, 2020) 21.6 21.7 19.6 19.8 15.7 15.3 - - - - - - - MT S ILVER AMR S ILVER S ENT S ILVER AMR + G OLDAMR S ILVER S ENT + G OLDAMR S ILVER AMR + S ILVER S ENT S ILVER AMR +"
2021.emnlp-main.57,2021.eacl-main.30,0,0.0591082,"Missing"
2021.emnlp-main.57,W19-4028,0,0.0228744,"of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual AMR-to-Text Generation In AMR-to-text generation, we transduce an AMR graph G to a surface realization as a sequence of tokens y = hy1 , . . . , y|y |i. As input we use an English-centric AMR graph where the output y can be realized in different languages (see Figure 1). 3.1 Approach 2017), motivated by prior work (Ribeiro et al., 2020a, 2021) that l"
2021.emnlp-main.57,P18-1150,1,0.867436,"r relatively larger graphs. Overall, we find that a combination of both strategies further improves the performance, showing that they are complementary for this task. 2 Related Work Approaches for AMR-to-text generation predominantly focus on English, and typically employ an encoder-decoder architecture, employing a linearized representation of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual"
2021.emnlp-main.57,tian-etal-2014-um,0,0.0456627,"Missing"
2021.emnlp-main.57,2020.tacl-1.2,0,0.0291043,"Missing"
2021.emnlp-main.57,2020.emnlp-demos.6,0,0.290837,"complementary sources of information further improves multilingual AMR-to-text generation. Our models surpass the previous state of the art for German, Italian, Spanish, and Chinese by a large margin.1 live-01 :ARG1 glorious :ARG0 they AMR-to En Es It Their life looks glorious. Su vida parece gloriosa. La loro vita sembra gloriosa. De Ihr Leben sieht herrlich aus. Zh 他们的⽇⼦看起来很光鲜. Figure 1: A generation example from English AMR to multiple different languages. that parsers can be effectively trained to transform multilingual text into English AMR, Mille et al. (2018, 2019) and Fan and Gardent (2020) discuss 1 Introduction the reverse task, turning meaning representations AMR-to-text generation is the task of recover- into multilingual text, as shown in Figure 1. Howing a text with the same meaning as a given Ab- ever, gold-standard multilingual AMR training data stract Meaning Representation (AMR) (Banarescu is currently scarce, and previous work (Fan and et al., 2013), and has recently received much re- Gardent, 2020) while discussing the feasibility of search interest (Ribeiro et al., 2019; Wang et al., multilingual AMR-to-text generation, has inves2020; Mager et al., 2020; Harkous et"
2021.emnlp-main.57,2021.naacl-main.41,0,0.0342624,"sh AMR graphs and sentences. We evaluate our different approaches on the multilingual LDC2020T07 test set by Damonte and Cohen (2018) consisting of gold annotations for Spanish (ES), Italian (IT), German (DE) and Chinese (ZH).3 For our multilingual parallel sentence corpus we consider data from different sources. For ES, IT and DE, we use: Europarl-v7 (Koehn, 2005), an aligned corpus of European Union parlia2 For example, for AMR-to-Spanish we use the prefix “translate AMR to Spanish:”. 3 This dataset was constructed by professional translators based on the LDC2017T10 test set. We employ mT5 (Xue et al., 2021), a Transformerbased encoder-decoder architecture (Vaswani et al., 743 BLEU BERTscore ES IT DE ZH All ES IT DE ZH All MT (Fan and Gardent, 2020) Multilingual model (Fan and Gardent, 2020) 21.6 21.7 19.6 19.8 15.7 15.3 - - - - - - - MT S ILVER AMR S ILVER S ENT S ILVER AMR + G OLDAMR S ILVER S ENT + G OLDAMR S ILVER AMR + S ILVER S ENT S ILVER AMR + S ILVER S ENT + G OLDAMR 27.6 23.3 28.3 28.2 28.5 30.7 30.4 24.2 21.2 24.3 24.9 24.6 26.4 26.1 19.4 16.9 18.9 19.4 19.2 20.6 20.5 23.3 20.1 22.2 22.9 22.3 24.2 23.4 23.6 20.4 23.4 23.9 23.7 25.5 25.1 87.1 84.5 87.3 87.6 87.3 87.8 88.0 85.7 83.7 85.7"
2021.emnlp-main.57,xue-etal-2014-interlingua,0,0.0620617,"Missing"
2021.emnlp-main.626,2021.findings-emnlp.410,1,0.829414,"Missing"
2021.emnlp-main.626,2021.acl-long.334,0,0.0937155,"Missing"
2021.emnlp-main.626,D19-1165,0,0.0436259,"Missing"
2021.emnlp-main.626,N19-1423,0,0.0725264,"Missing"
2021.emnlp-main.626,N18-1202,0,0.0604356,"Missing"
2021.emnlp-main.626,2021.eacl-main.39,1,0.864028,"Missing"
2021.emnlp-main.626,2020.emnlp-demos.7,1,0.921136,"Missing"
2021.emnlp-main.626,2020.emnlp-main.617,1,0.882046,"Missing"
2021.emnlp-main.626,P18-1031,0,0.0599069,"Missing"
2021.emnlp-main.626,2021.emnlp-main.800,1,0.70121,"Missing"
2021.emnlp-main.626,2020.deelio-1.5,1,0.863314,"Missing"
2021.emnlp-main.626,2021.emnlp-main.827,1,0.826642,"Missing"
2021.emnlp-main.626,2021.ccl-1.108,0,0.0635915,"Missing"
2021.emnlp-main.626,2020.emnlp-main.194,1,0.817594,"Missing"
2021.emnlp-main.626,N19-5004,0,0.0427409,"Missing"
2021.emnlp-main.626,2020.emnlp-main.21,0,0.0435443,"Missing"
2021.emnlp-main.626,P19-1355,0,0.0402569,"Missing"
2021.emnlp-main.626,2020.acl-main.195,0,0.0345865,"Missing"
2021.emnlp-main.713,2021.naacl-main.71,0,0.0595334,"Missing"
2021.emnlp-main.713,2021.acl-long.295,0,0.343647,"nd Schütze, 2021a; prompt-based model finetuned across three datasets Radford et al., 2019). This approach reformulates with varying data regimes. Our intriguing results downstream task instances as a language modeling reveal that: (i) zero-shot prompt-based models are input,2 allowing PLMs to make non-trivial task- more robust to using the lexical overlap heuristic specific predictions even in zero-shot settings. This during inference, indicated by their high perforin turn, provides a good initialization point for data mance on the corresponding challenge datasets; (ii) efficient finetuning (Gao et al., 2021), resulting in however, prompt-based finetuned models quickly adopt this heuristic as they learn from more labeled 1 The code is available at https://github.com/ data, which is indicated by gradual degradation of UKPLab/emnlp2021-prompt-ft-heuristics 2 E.g., appending a cloze prompt “It was [MASK]” to a the performance in challenge datasets. sentiment prediction input sentence “Delicious food!”, and We then show that regularizing prompt-based obtaining the sentiment label by comparing the probabilities assigned to the words “great” and “terrible”. finetuning, by penalizing the learning from up"
2021.emnlp-main.713,P18-2103,0,0.0213803,"task. For instance, models are shown to adopt heuristics based on the presence of certain indicative words or phrases in tasks such as reading comprehension (Kaushik and Lipton, 2018), story cloze completion (Schwartz et al., 2017; Cai et al., 2017), fact verification (Schuster et al., 2019), argumentation mining (Niven and Kao, 2019), and natural language inference (Gururangan et al., 2020). Heuristics in models are often investigated using constructed “challenge datasets” consisting of counter-examples to the spurious cues, which mostly result in incorrect predictions (Jia and Liang, 2017; Glockner et al., 2018; Naik et al., 2018; McCoy et al., 2019). Although the problem has been extensively studied, most works focus on models that are trained in standard settings where larger training datasets are available. Our work provides new insights in inference heuristics in models that are trained in zero- and few-shot settings. Efficiency and Robustness Prompting formulation enables language models to learn efficiently from a small number of training examples, which in turn reduces the computational cost for training (Le Scao and Rush, 2021). The efficiency benefit from prompting is very relevant to the l"
2021.emnlp-main.713,2020.acl-main.740,0,0.0491783,"Missing"
2021.emnlp-main.713,D17-1215,0,0.0298021,"learning the intended task. For instance, models are shown to adopt heuristics based on the presence of certain indicative words or phrases in tasks such as reading comprehension (Kaushik and Lipton, 2018), story cloze completion (Schwartz et al., 2017; Cai et al., 2017), fact verification (Schuster et al., 2019), argumentation mining (Niven and Kao, 2019), and natural language inference (Gururangan et al., 2020). Heuristics in models are often investigated using constructed “challenge datasets” consisting of counter-examples to the spurious cues, which mostly result in incorrect predictions (Jia and Liang, 2017; Glockner et al., 2018; Naik et al., 2018; McCoy et al., 2019). Although the problem has been extensively studied, most works focus on models that are trained in standard settings where larger training datasets are available. Our work provides new insights in inference heuristics in models that are trained in zero- and few-shot settings. Efficiency and Robustness Prompting formulation enables language models to learn efficiently from a small number of training examples, which in turn reduces the computational cost for training (Le Scao and Rush, 2021). The efficiency benefit from prompting is"
2021.emnlp-main.713,2020.acl-main.212,0,0.0205711,"ics in models by imOur experiments shed light on the negative impact proving the training dataset. Zellers et al. (2019); of low resource finetuning to the models’ overall Sakaguchi et al. (2020) propose to reduce artifacts performance that is previously obscured by stanin the training data by using adversarial filtering dard evaluation setup. The results indicate that methods; Nie et al. (2020); Kaushik et al. (2020) while finetuning helps prompt-based models to aim at a similar improvement via iterative data colrapidly gain the in-distribution improvement as lection using human-in-the-loop; Min et al. (2020); more labeled data are available, it also gradually Schuster et al. (2021); Liu et al. (2019a); Rozen increases models’ reliance on surface heuristics, et al. (2019) augment the training dataset with which we show to be less present in the zero-shot adversarial instances; and Moosavi et al. (2020a) evaluation. We further demonstrate that applying augment each training instances with their semanregularization that preserves pretrained weights durtic roles information. Complementary to this, reing finetuning mitigates the adoption of heuristics cent work introduces various learning algorithms t"
2021.emnlp-main.713,K19-1019,0,0.029504,"Missing"
2021.emnlp-main.713,2020.coling-main.488,0,0.0575617,"Missing"
2021.emnlp-main.713,2021.eacl-main.20,0,0.105644,"allenge datasets used to studies, standard finetuned models often overlook diagnose the inference heuristics.1 this information in the presence of lexical overlap (Nie et al., 2019; Dasgupta et al., 2018). We there1 Introduction fore question whether direct adaptation of PLMs usPrompt-based finetuning has emerged as a promis- ing prompts can better transfer the use of this inforing paradigm to adapt Pretrained Language Models mation during finetuning. We investigate this ques(PLM) for downstream tasks with limited number tion by systematically studying the heuristics in a of labeled examples (Schick and Schütze, 2021a; prompt-based model finetuned across three datasets Radford et al., 2019). This approach reformulates with varying data regimes. Our intriguing results downstream task instances as a language modeling reveal that: (i) zero-shot prompt-based models are input,2 allowing PLMs to make non-trivial task- more robust to using the lexical overlap heuristic specific predictions even in zero-shot settings. This during inference, indicated by their high perforin turn, provides a good initialization point for data mance on the corresponding challenge datasets; (ii) efficient finetuning (Gao et al., 2021"
2021.emnlp-main.713,2021.naacl-main.185,0,0.157777,"allenge datasets used to studies, standard finetuned models often overlook diagnose the inference heuristics.1 this information in the presence of lexical overlap (Nie et al., 2019; Dasgupta et al., 2018). We there1 Introduction fore question whether direct adaptation of PLMs usPrompt-based finetuning has emerged as a promis- ing prompts can better transfer the use of this inforing paradigm to adapt Pretrained Language Models mation during finetuning. We investigate this ques(PLM) for downstream tasks with limited number tion by systematically studying the heuristics in a of labeled examples (Schick and Schütze, 2021a; prompt-based model finetuned across three datasets Radford et al., 2019). This approach reformulates with varying data regimes. Our intriguing results downstream task instances as a language modeling reveal that: (i) zero-shot prompt-based models are input,2 allowing PLMs to make non-trivial task- more robust to using the lexical overlap heuristic specific predictions even in zero-shot settings. This during inference, indicated by their high perforin turn, provides a good initialization point for data mance on the corresponding challenge datasets; (ii) efficient finetuning (Gao et al., 2021"
2021.emnlp-main.713,2021.naacl-main.52,0,0.0707287,"Missing"
2021.emnlp-main.713,D19-1341,0,0.0182993,"ntage of rFT is the strongest on the lexical 5 overlap subset, which initially shows the highest See Appendix B for the detailed results. 9066 ous work shows that the artifacts of data annotations result in spurious surface cues, which gives away the labels, allowing models to perform well without properly learning the intended task. For instance, models are shown to adopt heuristics based on the presence of certain indicative words or phrases in tasks such as reading comprehension (Kaushik and Lipton, 2018), story cloze completion (Schwartz et al., 2017; Cai et al., 2017), fact verification (Schuster et al., 2019), argumentation mining (Niven and Kao, 2019), and natural language inference (Gururangan et al., 2020). Heuristics in models are often investigated using constructed “challenge datasets” consisting of counter-examples to the spurious cues, which mostly result in incorrect predictions (Jia and Liang, 2017; Glockner et al., 2018; Naik et al., 2018; McCoy et al., 2019). Although the problem has been extensively studied, most works focus on models that are trained in standard settings where larger training datasets are available. Our work provides new insights in inference heuristics in models tha"
2021.emnlp-main.713,2020.emnlp-main.613,1,0.806495,"present in the zero-shot adversarial instances; and Moosavi et al. (2020a) evaluation. We further demonstrate that applying augment each training instances with their semanregularization that preserves pretrained weights durtic roles information. Complementary to this, reing finetuning mitigates the adoption of heuristics cent work introduces various learning algorithms to while also maintains high in-distribution perforavoid adopting heuristics including by re-weighting mances. (He et al., 2019; Karimi Mahabadi et al., 2020; Clark et al., 2020) or regularizing the confidence Acknowledgement (Utama et al., 2020a; Du et al., 2021) on the training instances which exhibit certain biases. The type of We thank Michael Bugert, Tim Baumgärtner, Jan bias can be identified automatically (Yaghoobzadeh Buchman, and the anonymous reviewers for their et al., 2021; Utama et al., 2020b; Sanh et al., 2021; constructive feedback. This work is funded by Clark et al., 2020) or by hand-crafted models de- the German Research Foundation through the resigned based on prior knowledge about the bias. search training group AIPHES (GRK 1994/1) and Our finding suggests that prompted zero-shot mod- by the German Federal Ministr"
2021.emnlp-main.713,W18-5446,0,0.0586011,"Missing"
2021.emnlp-main.713,2020.emnlp-main.16,0,0.0344515,"e strong perbeen shown using common held-out evaluations, formances on few-shot finetuning by reformuwhich often conceal certain undesirable behaviors lating downstream tasks as a language modof models (Niven and Kao, 2019). eling problem. In this work, we demonstrate that, despite its advantages on low data One such behavior commonly reported in downregimes, finetuned prompt-based models for stream models is characterized by their preference sentence pair classification tasks still suffer to use surface features over general linguistic infrom a common pitfall of adopting inference formation (Warstadt et al., 2020). In the Natuheuristics based on lexical overlap, e.g., modral Language Inference (NLI) task, McCoy et al. els incorrectly assuming a sentence pair is of (2019) documented that models preferentially use the same meaning because they consist of the the lexical overlap feature between sentence pairs same set of words. Interestingly, we find that this particular inference heuristic is sigto blindly predict that one sentence entails the other. nificantly less present in the zero-shot evaluDespite models’ high in-distribution performance, ation of the prompt-based model, indicating they often fail"
2021.emnlp-main.713,N18-1101,0,0.030068,"data of size K to account for finetuning instability on small datasets (Dodge et al., 2020; Mosbach et al., 2021). We perform five data subsampling for each dataset and each data size K, where K ∈ {16, 32, 64, 128, 256, 512}. Note that K indicates the number of examples per label. We use the original development sets of each training dataset for testing the in-distribution performance. We perTask and Datasets We evaluate on three English language datasets included in the GLUE benchmark (Wang et al., 2018) for which there are challenge datasets to evaluate the lexical overlap heuristic: MNLI (Williams et al., 2018), SNLI (Bowman et al., 2015), and Quora Question Pairs (QQP). In MNLI and SNLI, the task is to determine whether premise sentence s1 entails, contradicts, or is neutral to the hypothesis sentence s2 . In QQP, s1 and 3 s2 are a pair of questions that are labeled as either See appendix A for details of HANS, PAWS, and Scramduplicate or non-duplicate. ble Test test sets. 9064 SNLI PAWS Scramble Test 80 80 80 60 60 60 60 40 40 40 40 20 20 20 20 0 0 0 0 0 16 32 64 128 256 512 ACC 100 ACC F1 QQP 80 duplicate F1 non-duplicate F1 0 16 32 64 128 256 512 MNLI 100 HANS (lex.) 100 HANS (subseq.) 0 16 32 6"
2021.emnlp-main.713,2020.sustainlp-1.11,0,0.0427071,"iency and Robustness Prompting formulation enables language models to learn efficiently from a small number of training examples, which in turn reduces the computational cost for training (Le Scao and Rush, 2021). The efficiency benefit from prompting is very relevant to the larger efforts towards sustainable and green NLP models (Moosavi et al., 2020b; Schwartz et al., 2020a) which encompass a flurry of techniques including knowledge distillation (Hinton et al., 2015; Sanh et al., 2019), pruning (Han et al., 2015), quantization (Jacob et al., 2018), and early exiting (Schwartz et al., 2020b; Xin et al., 2020). Recently, Hooker et al. (2020) show that methods improving compute and memory efficiency using pruning and quantization may be at odds with robustness and fairness. They report that while performance on standard test sets is largely unchanged, the performance of efficient models on certain underrepresented subsets of the data is disproportionately reduced, suggesting the importance of a more comprehensive evaluation to estimate overall changes in performance. Heuristics Mitigation Significant prior work at- 5 Conclusion tempt to mitigate the heuristics in models by imOur experiments shed lig"
2021.emnlp-main.713,2021.eacl-main.291,0,0.061405,"Missing"
2021.emnlp-main.713,P19-1472,0,0.0188215,"ving compute and memory efficiency using pruning and quantization may be at odds with robustness and fairness. They report that while performance on standard test sets is largely unchanged, the performance of efficient models on certain underrepresented subsets of the data is disproportionately reduced, suggesting the importance of a more comprehensive evaluation to estimate overall changes in performance. Heuristics Mitigation Significant prior work at- 5 Conclusion tempt to mitigate the heuristics in models by imOur experiments shed light on the negative impact proving the training dataset. Zellers et al. (2019); of low resource finetuning to the models’ overall Sakaguchi et al. (2020) propose to reduce artifacts performance that is previously obscured by stanin the training data by using adversarial filtering dard evaluation setup. The results indicate that methods; Nie et al. (2020); Kaushik et al. (2020) while finetuning helps prompt-based models to aim at a similar improvement via iterative data colrapidly gain the in-distribution improvement as lection using human-in-the-loop; Min et al. (2020); more labeled data are available, it also gradually Schuster et al. (2021); Liu et al. (2019a); Rozen"
2021.emnlp-main.713,N19-1131,0,0.0459935,"Missing"
2021.emnlp-main.800,2021.findings-emnlp.410,1,0.87244,"Missing"
2021.emnlp-main.800,2020.acl-main.421,1,0.820989,"The most extreme challenge is dealing with unseen languages with unseen በእግዚአብሔር བྷྱཿཤཱནྟིཾ ެވެންނުތާރަފްތާނަކ scripts (i.e., the scripts are notުޖrepresented in the እግዚአብሔርን བཻཌཱུརྱའི ެވެ ކެއާޔްއިރޫހްމ pretraining data; see Figure 1), where the pretrained ከእግዚአብሔር ེདྷརྨཱཧེཏ ިވެންނެގިއަޑަވި ބިލ ེཏུནྟེ entirely ކްނާޔަބ ް ީނަވިއަފare used models የመጀመሪያውን are bound toཝཱཧfail ifޮށthey ትምህርታቸውን ྲེའུའི further model އަޑަވެވްނަފުއadaptation. ިަގ off-the-shelf withoutསྤany Existing work focuses on the embedding layer and learns either a new embedding matrix for the target language (Artetxe et al., 2020) or adds new tokens to the pretrained vocabulary. While the former has only been applied to high-resource languages, the latter approaches have been limited to languages with seen scripts (Chau et al., 2020; Müller et al., 2021) and large pretraining corpora (Wang et al., 2020). Another line of work adapts the embedding layer as well as other layers of the model via adapters (Pfeiffer et al., 2020b; Üstün et al., 2020). Such methods, however, cannot be directly applied to languages with unseen scripts. In this work, we first empirically verify that the original tokenizer and the original embed"
2021.emnlp-main.800,D19-1165,0,0.0261306,"ained multilingual model such as mBERT or XLM-R is 1) fine-tuning it on labelled data of a downstream task in a source language and then 2) applying it directly to perform inference in a target language (Hu et al., 2020). However, as the model must balance between many languages in its representation space, it is not suited to excel at a specific language at inference time without further adaptation (Pfeiffer et al., 2020b). Adapters for Cross-lingual Transfer. Adapterbased approaches have been proposed as a remedy (Rebuffi et al., 2017, 2018; Houlsby et al., 2019; Stickland and Murray, 2019; Bapna and Firat, 2019; Pfeiffer et al., 2020a, 2021). In the crosslingual setups, the idea is to increase the multilingual model capacity by storing language-specific knowledge of each language in dedicated parameters (Pfeiffer et al., 2020b; Vidoni et al., 2020). We start from MAD-X (Pfeiffer et al., 2020b), a state-of-the-art adapter-based framework for crosslingual transfer. For completeness, we provide a brief overview of the framework in what follows. MAD-X comprises three adapter types: language, task, and invertible adapters; this enables learning language and task-specific transformations in a modular and"
2021.emnlp-main.800,2020.findings-emnlp.118,0,0.510962,"Figure 2. Pearson’s ρ correlation scores between the lexical overlap and 2 An alternative approach based on transliteration (Müller proportion of UNKs (see Table 1) and NER perforet al., 2021) side-steps script adaptation but relies on languagespecific heuristics, which are not available for most languages. mance are 0.443 and −0.798, respectively. 10188 Method EL- RAND EL- LEX MFC ∗ - RAND MFC ∗ - LEX Special tokens X X X X Lexical overlap Latent semantic concepts Language clusters New params # of new params Reference X X X0 X0 F0 , I0 F0 , I0 7.68M 7.68M 1M + C·10k 1M + C·10k Artetxe et al. (2020) Chau et al. (2020); Wang et al. (2020) Ours Ours X X X X Table 2: Overview of our methods and related approaches together with the pretrained knowledge they utilize. We calculate the number of new parameters per language with V 0 = 10k, D = 768, and D0 = 100. We do not include up-projection matrices G as these are learned only once and make up a comparatively small number of parameters. Recent approaches such as invertible adapters (Pfeiffer et al., 2020b) that adapt embeddings in the pretrained multilingual vocabulary may be able to deal with lesser degrees of lexical overlap. Still, they ca"
2021.emnlp-main.800,2020.emnlp-main.367,0,0.255907,"embedding matrix X whereas F stores token-specific information. G only needs to be pretrained once and can be used and fine-tuned for every new language. To this end, we simply learn new low-dimensional embeddings 0 0 F0 ∈ R|V |×D with the pretraining task, which are up-projected with G and fed to the model. MFC KM EANS -∗. When C > 1, each token is associated with one of C up-projection matrices. Grouping tokens and using a separate up-projection matrix per group may help balance sharing information across typologically similar languages with learning a robust representation for each token (Chung et al., 2020). We propose two approaches to automatically learn such a clustering. In our first, pipeline-based approach, we first cluster X into C clusters using KMeans. For each cluster, we then factorize the subset of embeddings Xc associated with the c-th cluster separately using Semi-NMF equivalently as for MF1 -∗. For a new language, we learn new low-dim em0 0 beddings F0 ∈ R|V |×D and a randomly initialized 0 matrix Z ∈ R|V |×C , which allows us to compute 0 the cluster assignment matrix I0 ∈ R|V |×C . Specifically, for token v, we obtain its cluster assignment as arg max of z0v,· . As arg max is no"
2021.emnlp-main.800,2020.emnlp-main.358,0,0.0272702,"ddings X0 ∈ R|V |×D for all V 0 vocabulary items where D is the dimensionality of the existing embeddings X ∈ R|V |×D , and only initialize special tokens (e.g. [CLS], [SEP]) with their pretrained representations. We train the new embeddings of the X0 with the pretraining task. This approach, termed EL- RAND, was proposed by Artetxe et al. (2020): they show that it allows learning aligned representations for a new language but only evaluate on high-resource languages. The shared special tokens allow the model to access a minimum amount of lexical information, which can be useful for transfer (Dufter and Schütze, 2020). Beyond this, this approach leverages knowledge from the existing embedding matrix only implicitly to the extent that the higher-level hidden representations are aligned to the lexical representations. 3.2 Initialization with Lexical Overlap 0 , and us denote this vocabulary subset with Vlex 0 0 0 Vrand = V  Vlex . In particular, we initialize the embeddings of all lexically overlapping tokens 0 with their pretrained representaX0lex from Vlex tions from the original matrix X, while the tokens 0 from Vrand receive randomly initialized embeddings X0rand . We then fine-tune all target language"
2021.emnlp-main.800,P19-1070,1,0.848463,"able parameters and yield more efficient model adaptation. Our approach, based on matrix factorization and language clusters, extracts relevant information from the pretrained embedding matrix. 4) We show that our methods outperform previous approaches with both resourcerich and resource-poor languages. They substantially reduce the gap between random and lexicallyoverlapping initialization, enabling better model adaption to unseen scripts. The code for this work is released at github.com/ Adapter-Hub/UNKs_everywhere. surpassed (static) cross-lingual word embedding spaces (Ruder et al., 2019; Glavas et al., 2019) as the state-of-the-art paradigm for cross-lingual transfer in NLP (Pires et al., 2019; Wu and Dredze, 2019; Wu et al., 2020; Hu et al., 2020; K et al., 2020). However, recent studies have also indicated that even current state-of-the-art models such as XLM-R (Large) still do not yield reasonable transfer performance across a large number of target languages (Hu et al., 2020). The largest drops are reported for resource-poor target languages (Lauscher et al., 2020), and (even more dramatically) for languages not covered at all during pretraining (Pfeiffer et al., 2020b). Standard Cross-Lingua"
2021.emnlp-main.800,2021.eacl-main.270,1,0.788219,"default implementation provided by Bauckhage et al. (2011).9 We train for 3,000 update steps and leverage the corresponding matrices F and G as initialization for the new vocabulary. We choose the reduced embedding dimensionality D0 = 100. F is only used when initializing the (lower-dimensional) embedding matrix with lexically overlapping representations. transfer. We train all the models with a batch size of 16 on high resource languages. For NER we use learning rates 2e − 5 and 1e − 4 for full fine-tuning and adapter-based training, respectively. For DP, we use a transformer-based variant (Glavas and Vulic, 2021) of the standard deep biaffine attention dependency parser (Dozat and Manning, 2017) and train with learning rates 2e − 5 and 5e − 4 for full fine-tuning and adapter-based training respectively. Masked Language Modeling. For MLM pretraining we leverage the entire Wikipedia corpus of the respective language. We train for 200 epochs or ∼100k update steps, depending on the corpus size. The batch size is 64; the learning rate is 1e − 4. 5 Results and Discussion The main results are summarised in Table 3a for NER, and in Table 3b for DP. First, our novel MAD-X 2.0 considerably outperforms the MADX"
2021.emnlp-main.800,2020.emnlp-main.363,1,0.903259,"Missing"
2021.emnlp-main.800,2021.naacl-main.38,0,0.0340559,"nguage vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model. Amharic የእግዚአብሔርን የሚሰጡዋቸውን የእግዚአብሔር ቤተክርስቲያን ኢትዮጵያውያን Tibetan Divehi ཉྩཡོནིརོདྷཨེཝ ެގާޔްއިރޫހްމުޖްލުސީއަރ བཱདཱིམཧཱཤྲམཎ ެވެއަވަންނަގިއަޑަވެނެދ བཱདཱིམཧཱཤྲམཎ އޓްސޮއ ި ޯރތާއ ު ަްސިޓިއ ཧཱུཕཊསྭཱཧཱ ލސީއަރ ު ްާޔްއިރޫހްމުޖ ཧཱུཧཱུཕཊ ާވިއަފްނެގިއަ ަޑވި ބިލ Figure 1: Example tokens of unseen scripts. training corpora (Pfeiffer et al., 2020b; Müller et al., 2021; Ansell et al., 2021). The most extreme challenge is dealing with unseen languages with unseen በእግዚአብሔር བྷྱཿཤཱནྟིཾ ެވެންނުތާރަފްތާނަކ scripts (i.e., the scripts are notުޖrepresented in the እግዚአብሔርን བཻཌཱུརྱའི ެވެ ކެއާޔްއިރޫހްމ pretraining data; see Figure 1), where the pretrained ከእግዚአብሔር ེདྷརྨཱཧེཏ ިވެންނެގިއަޑަވި ބިލ ེཏུནྟེ entirely ކްނާޔަބ ް ީނަވިއަފare used models የመጀመሪያውን are bound toཝཱཧfail ifޮށthey ትምህርታቸውን ྲེའུའི further model އަޑަވެވްނަފުއadaptation. ިަގ off-the-shelf withoutསྤany Existing work focuses on the embedding layer and learns either a new embedding matrix fo"
2021.emnlp-main.800,L16-1262,0,0.0926263,"Missing"
2021.emnlp-main.800,2020.lrec-1.497,0,0.0513677,"Missing"
2021.emnlp-main.800,P17-1178,0,0.0893718,"Missing"
2021.emnlp-main.800,2020.emnlp-demos.7,1,0.921743,"Figure 2. Pearson’s ρ correlation scores between the lexical overlap and 2 An alternative approach based on transliteration (Müller proportion of UNKs (see Table 1) and NER perforet al., 2021) side-steps script adaptation but relies on languagespecific heuristics, which are not available for most languages. mance are 0.443 and −0.798, respectively. 10188 Method EL- RAND EL- LEX MFC ∗ - RAND MFC ∗ - LEX Special tokens X X X X Lexical overlap Latent semantic concepts Language clusters New params # of new params Reference X X X0 X0 F0 , I0 F0 , I0 7.68M 7.68M 1M + C·10k 1M + C·10k Artetxe et al. (2020) Chau et al. (2020); Wang et al. (2020) Ours Ours X X X X Table 2: Overview of our methods and related approaches together with the pretrained knowledge they utilize. We calculate the number of new parameters per language with V 0 = 10k, D = 768, and D0 = 100. We do not include up-projection matrices G as these are learned only once and make up a comparatively small number of parameters. Recent approaches such as invertible adapters (Pfeiffer et al., 2020b) that adapt embeddings in the pretrained multilingual vocabulary may be able to deal with lesser degrees of lexical overlap. Still, they ca"
2021.emnlp-main.800,2020.emnlp-main.617,1,0.343661,"en mBERT’s and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model. Amharic የእግዚአብሔርን የሚሰጡዋቸውን የእግዚአብሔር ቤተክርስቲያን ኢትዮጵያውያን Tibetan Divehi ཉྩཡོནིརོདྷཨེཝ ެގާޔްއިރޫހްމުޖްލުސީއަރ བཱདཱིམཧཱཤྲམཎ ެވެއަވަންނަގިއަޑަވެނެދ བཱདཱིམཧཱཤྲམཎ އޓްސޮއ ި ޯރތާއ ު ަްސިޓިއ ཧཱུཕཊསྭཱཧཱ ލސީއަރ ު ްާޔްއިރޫހްމުޖ ཧཱུཧཱུཕཊ ާވިއަފްނެގިއަ ަޑވި ބިލ Figure 1: Example tokens of unseen scripts. training corpora (Pfeiffer et al., 2020b; Müller et al., 2021; Ansell et al., 2021). The most extreme challenge is dealing with unseen languages with unseen በእግዚአብሔር བྷྱཿཤཱནྟིཾ ެވެންނުތާރަފްތާނަކ scripts (i.e., the scripts are notުޖrepresented in the እግዚአብሔርን བཻཌཱུརྱའི ެވެ ކެއާޔްއިރޫހްމ pretraining data; see Figure 1), where the pretrained ከእግዚአብሔር ེདྷརྨཱཧེཏ ިވެންނެގިއަޑަވި ބިލ ེཏུནྟེ entirely ކްނާޔަބ ް ީނަވިއަފare used models የመጀመሪያውን are bound toཝཱཧfail ifޮށthey ትምህርታቸውን ྲེའུའི further model އަޑަވެވްނަފުއadaptation. ިަގ off-the-shelf withoutསྤany Existing work focuses on the embedding layer and learns either a n"
2021.emnlp-main.800,P19-1493,0,0.0619527,"Missing"
2021.emnlp-main.800,P19-1015,0,0.109665,"Missing"
2021.emnlp-main.800,2021.acl-long.243,1,0.644693,"Massachusetts Encyclopedia Pennsylvania Jacksonville Turkmenistan Melastomataceae munisipalidad Internasional internasional International establecimiento vicepresidente Internacional internacional Independencia University Therefore suffering existence practice languages language formula disease control government Chinese govern system ation International Bangladesh wikipedia Australia Zimbabwe Table 5: Longest lexically overlapping (sub)words. model (e.g. Georgian (ka), Urdu (ur), and Hindi (hi)), the proposed methods outperform MAD-X 2.0 for all tasks. This is in line with contemporary work (Rust et al., 2021), which emphasizes the importance of tokenizer quality for the downstream task. Consequently, for unseen languages with under-represented scripts, the performance gains are even larger, e.g., we see large improvements for Min Dong (cdo), Mingrelian (xmf), and Sindhi (sd). For unseen languages with the Latin script, our methods perform competitively (e.g. Maori (mi), Ilokano (ilo), Guarani (gn), and Wolof (wo)): this empirically confirms that the Latin script is adequately represented in the original vocabulary. The largest gains are achieved for languages with unseen scripts (e.g. Amharic (am)"
2021.emnlp-main.800,P18-1072,1,0.88992,"Missing"
2021.emnlp-main.800,2020.emnlp-main.180,0,0.115408,"Missing"
2021.emnlp-main.827,E17-2026,0,0.175409,"re-trained transformer weights are frozen and only the newly introduced adapter weights are trained. 10585 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10585–10605 c November 7–11, 2021. 2021 Association for Computational Linguistics models (Wolf et al., 2020; Pfeiffer et al., 2020a), the chances of finding pre-trained models that yield positive transfer gains are high. However, it is infeasible to brute-force the identification of the best intermediate task. Existing approaches have focused on beneficial task selection for multi-task learning (Bingel and Søgaard, 2017), full fine-tuning of intermediate and target transformer-based LMs for NLP tasks (Vu et al., 2020), adapter-based models for vision tasks (Puigcerver et al., 2021) and unsupervised approaches for zero-shot transfer for community question answering (Rücklé et al., 2020). Each of these works require different types of data, such as intermediate task data and/or intermediate model weights, which, depending on the scenario, are potentially not accessible.3 In this work we thus aim to address the efficiency aspect of transfer learning in NLP from multiple different angles, resulting in the followi"
2021.emnlp-main.827,D15-1075,0,0.05605,"Missing"
2021.emnlp-main.827,S19-2005,0,0.029668,"Missing"
2021.emnlp-main.827,N19-1300,0,0.0353981,"Missing"
2021.emnlp-main.827,N19-1361,0,0.0500077,"Missing"
2021.emnlp-main.827,N19-1112,0,0.133366,"diate task for transfer learning, without the necessity of computational expensive, explicit training on all potential candidates. We compare different selection techniques, consolidating previously proposed and new methods; 4) We provide a thorough analysis of the different techniques, available data scenarios, and task-, and model types, thus presenting deeper insights into the best approach for each respective setting; 5) We provide computational cost estimates, enabling informed decision making for trade-offs between expense and downstream task performance. 2019a; Talmor and Berant, 2019; Liu et al., 2019a; Sap et al., 2019; Pruksachatkun et al., 2020; Vu et al., 2020). Wang et al. (2019a), Yogatama et al. (2019), and Pruksachatkun et al. (2020) emphasizes the risks of catastrophic forgetting and negative transfer results, finding that the success of sequential transfer varies largely when considering different intermediate tasks. While previous work has shown that intermediate task training improves the performance on the target task in full fine-tuning setups, we establish that the same holds true for adapter-based training. 2.2 Predicting Beneficial Transfer Sources Automatically selecting"
2021.emnlp-main.827,P11-1015,0,0.0571287,"Missing"
2021.emnlp-main.827,W00-0726,0,0.0153672,"Missing"
2021.emnlp-main.827,D13-1170,0,0.0050593,"Missing"
2021.emnlp-main.827,P19-1355,0,0.087661,"Missing"
2021.emnlp-main.827,D19-1608,0,0.0210983,"Missing"
2021.emnlp-main.827,P19-1485,0,0.110922,"identify the best intermediate task for transfer learning, without the necessity of computational expensive, explicit training on all potential candidates. We compare different selection techniques, consolidating previously proposed and new methods; 4) We provide a thorough analysis of the different techniques, available data scenarios, and task-, and model types, thus presenting deeper insights into the best approach for each respective setting; 5) We provide computational cost estimates, enabling informed decision making for trade-offs between expense and downstream task performance. 2019a; Talmor and Berant, 2019; Liu et al., 2019a; Sap et al., 2019; Pruksachatkun et al., 2020; Vu et al., 2020). Wang et al. (2019a), Yogatama et al. (2019), and Pruksachatkun et al. (2020) emphasizes the risks of catastrophic forgetting and negative transfer results, finding that the success of sequential transfer varies largely when considering different intermediate tasks. While previous work has shown that intermediate task training improves the performance on the target task in full fine-tuning setups, we establish that the same holds true for adapter-based training. 2.2 Predicting Beneficial Transfer Sources Automa"
2021.emnlp-main.827,N19-1421,0,0.0524115,"Missing"
2021.emnlp-main.827,W17-2623,0,0.0540193,"Missing"
2021.emnlp-main.827,2020.emnlp-main.180,0,0.0381188,"Missing"
2021.emnlp-main.827,2020.emnlp-main.635,0,0.134182,"roceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10585–10605 c November 7–11, 2021. 2021 Association for Computational Linguistics models (Wolf et al., 2020; Pfeiffer et al., 2020a), the chances of finding pre-trained models that yield positive transfer gains are high. However, it is infeasible to brute-force the identification of the best intermediate task. Existing approaches have focused on beneficial task selection for multi-task learning (Bingel and Søgaard, 2017), full fine-tuning of intermediate and target transformer-based LMs for NLP tasks (Vu et al., 2020), adapter-based models for vision tasks (Puigcerver et al., 2021) and unsupervised approaches for zero-shot transfer for community question answering (Rücklé et al., 2020). Each of these works require different types of data, such as intermediate task data and/or intermediate model weights, which, depending on the scenario, are potentially not accessible.3 In this work we thus aim to address the efficiency aspect of transfer learning in NLP from multiple different angles, resulting in the following contributions: 1) We focus on adapter-based transfer learning which is considerably more paramet"
2021.emnlp-main.827,P19-1439,0,0.0429132,"Missing"
2021.emnlp-main.827,W18-5446,0,0.0261767,"2017) and Vu et al. (2020) require Vu et al. (2020) compute task embeddings based on access to both intermediate task data and models, Puigcerver the Fisher Information Matrix of a probe network. et al. (2021) require access to only the intermediate model, and Rücklé et al. (2020) only to the intermediate task data. While many different methods have been pro10586 3 Adapter-Based Sequential Transfer Tasks We select QA tasks from the MultiQA repository (Talmor and Berant, 2019) and sequence tagging tasks from Liu et al. (2019a). Most of our classification tasks are available in the (Super)GLUE (Wang et al., 2018, 2019b) benchmarks. We experiment with multiple choice commonsense reasoning tasks to cover a broader range of different types, and domains. In total, we experiment with 53 tasks, divided into 42 intermediate and 11 target tasks.4 3.2 80 70 60 RTE COPA R. Tomatoes CQ CoNLL 2003 CS QA STS-B DROP Quail BoolQ DepRel-EWT 50 40 30 We present a large-scale study on adapter-based sequential fine-tuning, finding that around half of the task combinations yield no positive gains. This demonstrates the importance of finding approaches that efficiently identify suitable intermediate tasks. 3.1 90 Perform"
2021.emnlp-main.827,Q19-1040,0,0.0360056,"Missing"
2021.emnlp-main.827,D18-1259,0,0.0597865,"Missing"
2021.emnlp-main.827,D18-1009,0,0.0549264,"Missing"
2021.findings-emnlp.199,J08-1001,0,0.233695,"ons in a text. Such a graph contains tinguishes a high-quality text from a random se- two types of edges: (1) Edges that capture entitybased relations between sentences, and (2) edges quence of sentences. Modeling local coherence is crucial for various downstream NLP applications, that capture the linear order of sentences in the text. e.g., summary evaluation and generation (Barzilay To encode such graphs, we adapt Relational Graph and Lapata, 2008; Parveen et al., 2016), readabil- Convolutional Networks (RGCNs) (Schlichtkrull et al., 2018). RGCNs encode nodes of a graph into ity assessment (Barzilay and Lapata, 2008; Mesgar and Strube, 2014), essay scoring (Burstein et al., vectors using the graph’s connectivity structure and 2010; Mesgar and Strube, 2016), dialogue evalu- any feature information captured in the graph, such ation and generation (Mesgar et al., 2020, 2021), as edge types. We then apply a self-attention layer to these node vectors to capture to what extent each and machine translation (Born et al., 2017; Kuang sentence of the text is crucial for estimating the coet al., 2018). herence of the entire text. We finally use an output Motivated by the Centering theory (Joshi and Weinstein, 1981)"
2021.findings-emnlp.199,W17-4803,1,0.898504,"Missing"
2021.findings-emnlp.199,N10-1099,0,0.0863748,"Missing"
2021.findings-emnlp.199,N19-1423,0,0.0324553,"Missing"
2021.findings-emnlp.199,P13-1010,0,0.0173648,"Missing"
2021.findings-emnlp.199,P18-1052,0,0.0299933,"en proposed representations to encode coherence. The benefits of convolutional neural models for exto enrich these representations and also to extract tracting informative features from entity grids features from these representations to model local have been recently studied. In this work, we coherence. Recent work shows the effectiveness of study the benefits of Relational Graph Convoconvolutional neural networks (CNNs) for extractlutional Networks (RGCN) to encode entity ing features from entity grids to encode coherence graphs for measuring local coherence. We (Tien Nguyen and Joty, 2017; Joty et al., 2018). evaluate our neural graph-based model for Pre-trained transformer-based encoders can also two benchmark coherence evaluation tasks: capture relations between tokens in a text (Devlin sentence ordering (SO) and summary coherence rating (SCR). The results show that our et al., 2019). However, these encoders are potenneural graph-based model consistently outpertially incapable of capturing long-distance relations forms the neural grid-based model for both (Martins et al., 2021), specifically where the text tasks. Our model performs competitively with length is greater than the maximum input len"
2021.findings-emnlp.199,C18-1050,0,0.0476563,"Missing"
2021.findings-emnlp.199,2020.acl-main.133,1,0.752408,"eam NLP applications, that capture the linear order of sentences in the text. e.g., summary evaluation and generation (Barzilay To encode such graphs, we adapt Relational Graph and Lapata, 2008; Parveen et al., 2016), readabil- Convolutional Networks (RGCNs) (Schlichtkrull et al., 2018). RGCNs encode nodes of a graph into ity assessment (Barzilay and Lapata, 2008; Mesgar and Strube, 2014), essay scoring (Burstein et al., vectors using the graph’s connectivity structure and 2010; Mesgar and Strube, 2016), dialogue evalu- any feature information captured in the graph, such ation and generation (Mesgar et al., 2020, 2021), as edge types. We then apply a self-attention layer to these node vectors to capture to what extent each and machine translation (Born et al., 2017; Kuang sentence of the text is crucial for estimating the coet al., 2018). herence of the entire text. We finally use an output Motivated by the Centering theory (Joshi and Weinstein, 1981), many approaches to local coher- layer to transform the outputs of the self-attention ence modeling rely on entity relations between sen- layer to a score, which estimates the coherence detences. The entity grid (Barzilay and Lapata, 2005, gree of the t"
2021.findings-emnlp.199,2021.eacl-main.44,1,0.812042,"Missing"
2021.findings-emnlp.199,P17-1121,0,0.0275005,"Several methods have been proposed representations to encode coherence. The benefits of convolutional neural models for exto enrich these representations and also to extract tracting informative features from entity grids features from these representations to model local have been recently studied. In this work, we coherence. Recent work shows the effectiveness of study the benefits of Relational Graph Convoconvolutional neural networks (CNNs) for extractlutional Networks (RGCN) to encode entity ing features from entity grids to encode coherence graphs for measuring local coherence. We (Tien Nguyen and Joty, 2017; Joty et al., 2018). evaluate our neural graph-based model for Pre-trained transformer-based encoders can also two benchmark coherence evaluation tasks: capture relations between tokens in a text (Devlin sentence ordering (SO) and summary coherence rating (SCR). The results show that our et al., 2019). However, these encoders are potenneural graph-based model consistently outpertially incapable of capturing long-distance relations forms the neural grid-based model for both (Martins et al., 2021), specifically where the text tasks. Our model performs competitively with length is greater than t"
2021.findings-emnlp.199,W14-3701,1,0.630574,"contains tinguishes a high-quality text from a random se- two types of edges: (1) Edges that capture entitybased relations between sentences, and (2) edges quence of sentences. Modeling local coherence is crucial for various downstream NLP applications, that capture the linear order of sentences in the text. e.g., summary evaluation and generation (Barzilay To encode such graphs, we adapt Relational Graph and Lapata, 2008; Parveen et al., 2016), readabil- Convolutional Networks (RGCNs) (Schlichtkrull et al., 2018). RGCNs encode nodes of a graph into ity assessment (Barzilay and Lapata, 2008; Mesgar and Strube, 2014), essay scoring (Burstein et al., vectors using the graph’s connectivity structure and 2010; Mesgar and Strube, 2016), dialogue evalu- any feature information captured in the graph, such ation and generation (Mesgar et al., 2020, 2021), as edge types. We then apply a self-attention layer to these node vectors to capture to what extent each and machine translation (Born et al., 2017; Kuang sentence of the text is crucial for estimating the coet al., 2018). herence of the entire text. We finally use an output Motivated by the Centering theory (Joshi and Weinstein, 1981), many approaches to local"
2021.findings-emnlp.199,N16-1167,1,0.809899,"ations between sentences, and (2) edges quence of sentences. Modeling local coherence is crucial for various downstream NLP applications, that capture the linear order of sentences in the text. e.g., summary evaluation and generation (Barzilay To encode such graphs, we adapt Relational Graph and Lapata, 2008; Parveen et al., 2016), readabil- Convolutional Networks (RGCNs) (Schlichtkrull et al., 2018). RGCNs encode nodes of a graph into ity assessment (Barzilay and Lapata, 2008; Mesgar and Strube, 2014), essay scoring (Burstein et al., vectors using the graph’s connectivity structure and 2010; Mesgar and Strube, 2016), dialogue evalu- any feature information captured in the graph, such ation and generation (Mesgar et al., 2020, 2021), as edge types. We then apply a self-attention layer to these node vectors to capture to what extent each and machine translation (Born et al., 2017; Kuang sentence of the text is crucial for estimating the coet al., 2018). herence of the entire text. We finally use an output Motivated by the Centering theory (Joshi and Weinstein, 1981), many approaches to local coher- layer to transform the outputs of the self-attention ence modeling rely on entity relations between sen- laye"
2021.findings-emnlp.199,2021.eacl-main.308,0,0.0682734,"Missing"
2021.findings-emnlp.199,D19-1231,0,0.0693776,"contain edges for capturing linear order of sentences as well as entity-based relations. Moreover, our model adapts RGCN to extract features for estimating coherence. Our model also outperforms the examined entity grid-based models. The Neural EntGrid and Lex. Neural EntGrid models represent entity relations in text by entity grids and then apply CNNs to these grids to extract features for modeling the text coherence. Differently, our model uses graphs to represent relations between sentences and applies RGCN to learn features from graphs. Our model slightly outperforms the model proposed by Moon et al. (2019). We note that the best results for M&M are 92.93 for SO and 83.8 for SCR, achieved with ELMo as word embeddings. We compare with their Word2Vec setting to study the influence of our models, not word embeddings. Moon et al. (2019)’s model uses no explicit representations of text structure (neither graphs nor grids). It captures linear relations between adjacent sentences using a neural bilinear layer, and their relations with a global representation of a text using a CNN-based module. This model is trained by a language model loss together with a ranking loss specifically designed for SO. Our"
2021.findings-emnlp.199,D16-1074,1,0.790199,"sent a text via 1 Introduction a graph (Figure 1) since a graph can capture longLocal coherence is a discourse property that dis- distance relations in a text. Such a graph contains tinguishes a high-quality text from a random se- two types of edges: (1) Edges that capture entitybased relations between sentences, and (2) edges quence of sentences. Modeling local coherence is crucial for various downstream NLP applications, that capture the linear order of sentences in the text. e.g., summary evaluation and generation (Barzilay To encode such graphs, we adapt Relational Graph and Lapata, 2008; Parveen et al., 2016), readabil- Convolutional Networks (RGCNs) (Schlichtkrull et al., 2018). RGCNs encode nodes of a graph into ity assessment (Barzilay and Lapata, 2008; Mesgar and Strube, 2014), essay scoring (Burstein et al., vectors using the graph’s connectivity structure and 2010; Mesgar and Strube, 2016), dialogue evalu- any feature information captured in the graph, such ation and generation (Mesgar et al., 2020, 2021), as edge types. We then apply a self-attention layer to these node vectors to capture to what extent each and machine translation (Born et al., 2017; Kuang sentence of the text is crucial f"
2021.findings-emnlp.59,S17-2001,0,0.0420774,"d rectly from the specific task or from other (similar) where H ∈R is the decoder hidden states tasks. A good approach should also work if some within t decoding steps at the k-th layer, d is the T 1×d labeled data is available. size of the sentence embedding, [s ] ∈ R is Hence, we propose to evaluate unsupervised sena one-row matrix including the sentence embedtence embedding approaches in following three ding vector and Q, K and V are the query, key and value, respectively. By exploring different configu- setups: Unsupervised Learning: We assume we just rations on the STS benchmark dataset (Cer et al., 2017), we discover that the best combination is: have unlabeled sentences from the target task and tune our approaches based on these sentences. (1) adopting deletion as the input noise and setting the deletion ratio to 0.6, (2) using the output of the Domain Adaptation: We assume we have unla[CLS] token as fixed-sized sentence representa- beled sentences from the target task and labeled sention (3) tying the encoder and decoder parameters tences from NLI (Bowman et al., 2015; Williams during training. For the detailed tuning process, et al., 2018) and STS benchmark (Cer et al., 2017) please refer"
2021.findings-emnlp.59,N16-1162,0,0.044333,"Missing"
2021.findings-emnlp.59,2020.acl-main.703,0,0.0396265,"grant GU 798/29-1), by the European Regional Development Fund (ERDF) and the Hessian State Chancellery – Hessian Minister of Digital Strategy and Development under the promotional reference 20005482 (TexPrax), and by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE. We mainly experiment with pre-trained Transformer encoders in this work. Besides single encoders, there are also pre-trained encoder-decoder models like BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). However, they are already extensively pre-trained with variants of auto-encoder loss on the general domain and they are suspected of overfitting the reconstruction behavior. To verify this idea, we also further train BART-base and T5-base models with TSDAE on the 4 domainspecific datasets. The results are shown in Table 4. References We observe that BART and T5 can achieve much lower training loss (1.7 and 1.3 on average, resp.) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anthan from scratch (3.4) or BERT (2.7), bu"
2021.findings-emnlp.59,2020.emnlp-main.733,0,0.426493,"ial Denoising Auto-Encoder (SDAE) (Vincent et al., 2010; Goodfellow et al., ferent training objectives to achieve state-of-the-art 2016; Hill et al., 2016) is a popular unsupervised results on STS tasks. Among them, Contrastive method in machine learning, how to combine it Tension (CT) (Giorgi et al., 2021) simply views with pre-trained Transformers for learning sentence the identical and different sentences as positive and embeddings remains unclear. In this section, we negative examples, resp. and train two independent first introduce the training objective of TSDAE and encoders; BERT-flow (Li et al., 2020) trains model via debiasing embedding distribution towards Gaus- then give the optimal configuration of TSDAE. sian; SimCSE (Gao et al., 2021) is based on contrastive learning (Hadsell et al., 2006; Chen et al., 3.1 Training Objective 2020) and views the identical sentences with dif- Figure 1 illustrates the architecture of TSDAE. TSferent dropout mask as the positive examples. For DAE train sentence embeddings by adding a certain more details, please refer to Section 5. All of them type of noise (e.g. deleting or swapping words) to 672 input sentences, encoding the damaged sentences into fixe"
2021.findings-emnlp.59,P14-5010,0,0.00332233,"n, we record the POS tag for w ˆ and compute the bedding methods, i.e., which words (part-of-speech distribution of POS tags across all sentence pairs. 678 Checkpoint BERT-base Scratch BART-base T5-base AskU. 59.4/2.2 56.6/2.6 58.5/1.4 45.6/1.0 CQADup. 14.5/3.4 8.4/4.2 9.5/2.0 2.2/1.4 TwitterP. 73.0/2.4 69.8/3.3 60.3/1.5 48.2/1.5 SciDocs 74.0/2.8 67.2/3.5 62.0/1.7 30.8/1.1 Avg. 55.2/2.7 50.5/3.4 47.6/1.7 31.7/1.3 Table 4: Test performance/training loss of TSDAE models starting from different checkpoints. The results for BERT-base are copied from Table 2. POS-tags are determined using CoreNLP (Manning et al., 2014). The result averaged over the four datasets is shown in Figure 4. For the result on each dataset, please refer to Appendix H. Comparing the indomain supervised model (SBERT-sup.) and the prior distribution of the POS tags, we find that nouns (NN) are by far the most relevant content words in a sentence, while function words such as prepositions (IN) and determinators (DT) have little influence on the model prediction. Surprisingly, we do not perceive significant differences between all the approaches. This is good news for the unsupervised methods (TSDAE, CT, SimCSE and BERT-flow) and show th"
2021.findings-emnlp.59,N18-1049,0,0.0309899,"arts and out-ofthe-box supervised pre-trained models on the above mentioned tasks. For comparison, we include three recent state-of-the-art unsupervised approaches: CT, SimCSE, and BERT-flow. We use the proposed hyper-parameters from the respective paper. Without other specification, BERT-base-uncased3 is used as the base Transformer model. To eliminate the influence of randomness, we report the scores averaged over 5 random seeds. For other details, please refer to Appendix B. 5.1 Baseline Methods We compare the approaches against avg. GloVe embeddings (Pennington et al., 2014) and Sent2Vec (Pagliardini et al., 2018). The former generates sentence embeddings by averaging word embeddings trained on a large corpus from the general domain; the latter is also a bag-of-words model but trained on the in-domain unlabeled corpus. The unsupervised baseline of BERT-base-uncased with mean pooling is also in comparison. We further compare against existent pre-trained models: Universial Sentence Embedding (USE) (Yang et al., 2020), which was trained on multiple supervised datasets including NLI and community question answering. From the Sentence-Transformers package, we use SBERT-base-nli-v2 and SBERT-basenli-stsb-v2:"
2021.findings-emnlp.59,D14-1162,0,0.110401,"Missing"
2021.findings-emnlp.59,C16-1009,1,0.891203,"Missing"
2021.findings-emnlp.59,D19-1410,1,0.845798,"heterogeneous domains. Supervised sentence embeddings utilize labels for sentence pairs which provide the information about the relation between the sentences. Since sentence embeddings are usually applied to measure the similarity of a sentence pair, the most direct way is to label this similarity for supervised training (Henderson et al., 2017). Many studies also find that natural language inference (NLI), question answering and conversational context datasets can successfully be used to train sentence embeddings (Conneau et al., 2017; Cer et al., 2018). The recently proposed Sentence-BERT (Reimers and Gurevych, 2019) introduced pre-trained Transformers to the field of sentence embeddings. Although high-quality sentence embeddings can be derived via supervised training, the labeling cost is a major obstacle for practical usage, especially for specialized domains. Unsupervised sentence embeddings utilize 3 Sequential Denoising Auto-Encoder only an unlabeled corpus during training. Recent work combined pre-trained Transformers with dif- Although Sequential Denoising Auto-Encoder (SDAE) (Vincent et al., 2010; Goodfellow et al., ferent training objectives to achieve state-of-the-art 2016; Hill et al., 2016) is"
2021.findings-emnlp.59,D18-1481,0,0.0418511,"Missing"
2021.naacl-main.28,S14-2010,0,0.0277071,"use two binary sentence pair classification tasks: Duplicate question detection and news paraphrase identification. Examples for all datasets are given in Table 2. SemEval Spanish STS: Semantic Textual Similarity (STS)5 is the task of assessing the degree of similarity between two sentences over a scale ranging from [0, 5] with 0 indicating no semantic overlap and 5 indicating identical content (Agirre et al., 2016). We choose Spanish STS data to test our methods for a different language than English. For our training and development dataset, we use the datasets provided by SemEval STS 2014 (Agirre et al., 2014) and SemEval STS 2015 (Agirre et al., 2015). These consist of annotated sentence pairs from news articles and from Wikipedia. As test set, we use SemEval STS 2017 (Cer et al., 2017), which annotated image caption pairs from SNLI (Bowman et al., 2015). For all our experiments, we normalise the original similarity scores to [0, 1] by dividing the score by 5. 5 https://github.com/facebookresearch/faiss 299 https://ixa2.si.ehu.es/stswiki Dataset Spanish-STS BWS (cross-topic) BWS (in-topic) Quora-QP MRPC # training-samples # development-samples # testing-samples 1,400 220 250 2125 425 850 2471 478"
2021.naacl-main.28,2020.findings-emnlp.264,0,0.0253899,"context. A given candidate is represented by one vector, while the context is jointly encoded with the candidates (similar to cross-encoders). Unlike cross-encoder’s full self attention technique, polyencoders apply attention between two inputs only at the top layer. Poly-encoders have the drawback that they are only practical for certain applications: The score function is not symmetric, i.e., they cannot be applied for tasks with a symmetric similarity relation. Further, poly-encoder representations cannot be efficiently indexed, causing issues for retrieval tasks with large corpora sizes. Chen et al. (2020) propose the DiPair architecture which, similar to our work, also uses a crossencoder model to annotate unlabeled pairs for finetuning a bi-encoder model. DiPair focuses on inference speed and provides a detailed ablation for optimal bi-encoder architectures for performance versus speed trade-offs. The focus of our work are sampling techniques, which we find crucial for performance boosts in the bi-encoder model while keeping its architecture constant. Our proposed data augmentation approach is based on semi-supervision (Blum and Mitchell, 1998) for in-domain tasks, which has been applied 2 Re"
2021.naacl-main.28,S16-1081,0,0.0642896,"Missing"
2021.naacl-main.28,D17-1070,0,0.0260247,"ed data augmentation approach is based on semi-supervision (Blum and Mitchell, 1998) for in-domain tasks, which has been applied 2 Related Work successfully for a wide range of tasks. Uva et al. Sentence embeddings are a well studied area in (2018) train a SVM model with few gold samples recent literature. Earlier techniques included un- and apply semi-supervision with pre-training neusupervised methods such as Skip-thought vectors ral networks. Another common strategy is to gener(Kiros et al., 2015) and supervised methods such as ate paraphrases of existent sentences, for example, InferSent (Conneau et al., 2017) or USE (Cer et al., by replacing words with synonyms (Wei and Zou, 2018). For pairwise scoring tasks, more recent sen- 2019), by using round-trip translation (Yu et al., tence embedding techniques are also able to encode 2018; Xie et al., 2020), or with seq2seq-models a pair of sentences jointly. Among these, BERT (Kumar et al., 2019). Other approaches generate (Devlin et al., 2018) can be used as a cross-encoder. synthetic data by using generative adversarial netBoth inputs are separated by a special SEP token works (Tanaka and Aranha, 2019), by using a lanand multi-head attention is applied"
2021.naacl-main.28,C04-1051,0,0.249339,"tes. Quora released a dataset7 containing 404,290 question pairs. We start with the same dataset partitions from Wang et al. (2017)8 . We remove all overlaps and ensure that a question in one split of the dataset does not appear in any other split to mitigate the transductive classification problem (Ji et al., 2010). As we observe performance differences between cross- and bi-encoders mainly for small datasets, we randomly downsample the training set to 10,000 pairs while preserving the original balance of non-duplicate to duplicate question pairs. Microsoft Research Paraphrase Corpus (MRPC): Dolan et al. (2004) presented a paraphrase identification dataset consisting of sentence pairs automatically extracted from online news sources. Each pair was manually annotated by 7 https://www.quora.com/q/quoradata/First-QuoraDataset-Release-Question-Pairs 8 https://drive.google.com/file/d/0B0PlTAo– BnaQWlsZl9FZ3l1c28 Train / Dev / Test (Total Pairs) Train (Ratio) Dev / Test (Ratio) 919706 / 101k / 101k 254142 / 10k / 10k 919100 / 101k / 101k 919706 / 101k / 101k 1 : 100 3.71 : 100 1 : 100 1 : 100 1 : 100 1:1 1 : 100 1 : 100 Table 4: Summary of multi-domain datasets originally proposed by Shah et al. (2018) an"
2021.naacl-main.28,D15-1075,0,0.029767,"degree of similarity between two sentences over a scale ranging from [0, 5] with 0 indicating no semantic overlap and 5 indicating identical content (Agirre et al., 2016). We choose Spanish STS data to test our methods for a different language than English. For our training and development dataset, we use the datasets provided by SemEval STS 2014 (Agirre et al., 2014) and SemEval STS 2015 (Agirre et al., 2015). These consist of annotated sentence pairs from news articles and from Wikipedia. As test set, we use SemEval STS 2017 (Cer et al., 2017), which annotated image caption pairs from SNLI (Bowman et al., 2015). For all our experiments, we normalise the original similarity scores to [0, 1] by dividing the score by 5. 5 https://github.com/facebookresearch/faiss 299 https://ixa2.si.ehu.es/stswiki Dataset Spanish-STS BWS (cross-topic) BWS (in-topic) Quora-QP MRPC # training-samples # development-samples # testing-samples 1,400 220 250 2125 425 850 2471 478 451 10,000 3,000 3,000 4,340 731 730 # total-samples 1,870 3,400 3,400 16,000 5,801 Table 1: Summary of all datasets being used for diverse in-domain sentence pair tasks in this paper. Dataset Sentence 1 Sentence 2 Score BWS Cloning treats children a"
2021.naacl-main.28,D18-2029,0,0.0411307,"Missing"
2021.naacl-main.28,P18-2046,0,0.0607977,"Missing"
2021.naacl-main.28,D19-1670,0,0.0649743,"Missing"
2021.naacl-main.34,C18-1287,0,0.0256452,"ld be used to mitigate this issue by, for instance, automatically providing topic- and aspectspecific counter-arguments for all arguments of a given text (this has been shown for single arguments in Section 7.2). We believe that working on and providing access to such models is of major importance and, overall, a benefit to society. Open-sourcing such language models also encourages the work on counter-measures to detect malicious use: While many works have been published on the topic of automatic fake news detection in texts (Kaliyar et al., 2020; Reis et al., 2019; Hanselowski et al., 2018; Pérez-Rosas et al., 2018), the recent emergence of large-scale language models has also encouraged research to focus on detecting the creator of these texts (Varshney et al., 2020; Zellers et al., 2019). The former approaches are aimed at detecting fake news in general, i.e. independent of who (or what) composed a text, whereas the latter approaches are designed to recognize if a text was written by a human or generated by a language model. We encourage the work on both types of methods. Ideally, social networks and news platforms would indicate if a statement was automatically generated in addition to its factual cor"
2021.naacl-main.34,P15-4019,0,0.0273857,"les and online discussions with frames (Hartmann et al., 2019; Naderi and Hirst, 2017). These methods are, however, limited to a small set of predefined frames that represent 2 Related Work high-level concepts. Contrarily, we operate on a Argument Aspect Detection Early work by Fujii fine-grained span-level to detect aspects that are and Ishikawa (2006) focuses mainly on Japanese explicitly mentioned in arguments. 381 Argument Generation Early approaches rely on rules from argumentation theory and user preference models (Carenini and Moore, 2006; Zukerman et al., 1998). In a more recent work, Sato et al. (2015) construct rules to find arguments in a large data source, which are then filtered and ordered with a neural network based ranker. Baff et al. (2019) use a clustering and regression approach to assemble discourse units (major claims, pro and con statements) to argumentative texts. However, most of these approaches rely on hand-crafted features and do not generalize well. Moreover, they all require permanent access to large data sources and are not able to generate new arguments. Recently, research on generating arguments with language models gained more attention. Hua and Wang (2019) use a seq"
2021.naacl-main.34,P16-1162,0,0.050076,"Missing"
2021.naacl-main.34,N18-5005,1,0.860092,"lely on a given topic, stance, and argument aspect. For instance, to enforce focus on the aspect of cancer for the topic of nuclear energy, we input a control code “Nuclear Energy CON cancer” that creates a contra argument discussing this aspect, for instance: “Studies show that people living next to nuclear power plants have a higher risk of developing cancer.”. To obtain control codes from training data, we pre-define a set of topics to retrieve documents for and rely on an existing stance detection model to classify whether a sentence argues in favor (pro) or against (con) the given topic (Stab et al., 2018a). Regarding argument aspect detection, however, past work has two drawbacks: it either uses simple rule-based extraction of verb- and noun-phrases (Fujii and Ishikawa, 2006) or the definition of aspects is based on target-concepts located within the same sentence (Gemechu and Reed, 2019). Aspects as we require and define them are not bound to any part-of-speech tag and (1) hold the core reason upon which the conclusion/evidence is built and (2) encode the stance towards a general but not necessarily explicitly mentioned topic the argument discusses. For instance: Topic: Nuclear Energy Argume"
2021.naacl-main.34,D18-1402,1,0.922912,"lely on a given topic, stance, and argument aspect. For instance, to enforce focus on the aspect of cancer for the topic of nuclear energy, we input a control code “Nuclear Energy CON cancer” that creates a contra argument discussing this aspect, for instance: “Studies show that people living next to nuclear power plants have a higher risk of developing cancer.”. To obtain control codes from training data, we pre-define a set of topics to retrieve documents for and rely on an existing stance detection model to classify whether a sentence argues in favor (pro) or against (con) the given topic (Stab et al., 2018a). Regarding argument aspect detection, however, past work has two drawbacks: it either uses simple rule-based extraction of verb- and noun-phrases (Fujii and Ishikawa, 2006) or the definition of aspects is based on target-concepts located within the same sentence (Gemechu and Reed, 2019). Aspects as we require and define them are not bound to any part-of-speech tag and (1) hold the core reason upon which the conclusion/evidence is built and (2) encode the stance towards a general but not necessarily explicitly mentioned topic the argument discusses. For instance: Topic: Nuclear Energy Argume"
2021.naacl-main.34,P18-1023,0,0.0284037,"el method to evaluate generated arguments based on the argument quality detection approach proposed by Gretz et al. (2020b). They create an argument qual- 7.2 Extrinsic Evaluation: Counter-Arguments ity dataset that contains around 30,000 arguments over 71 topics. For each argument, annotators were Drafting counter-arguments is an important skill asked whether or not they would recommend a for debating, to provide constructive feedback, and friend to use the displayed argument in a speech. to foster critical thinking. We lean onto the work The quality scores for each argument result from a of Wachsmuth et al. (2018) who describe a counterweighted average (WA) or MACE Probability func- argument as discussing the same aspect as an inition of all annotations and range between 0 (low- tial argument, but with a switched stance. Hence, est quality) and 1.0 (highest quality). We use the given our defined control codes, our model is esWA-score as label, the same model (BERTBASE ) pecially fit for counter-argument generation. Unand hyperparameters as given in the original pa- like current models for this task, we do not reper, and reproduce the reported correlations of .52 quire a specific dataset with argument a"
2021.naacl-main.34,N16-1007,0,0.0975349,"shows that the ArgCTRL is able to generate high-quality, aspectspecific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL.1 1 Introduction Language models (Bengio et al., 2003) allow to generate text through learned distributions of a language and have been applied to a variety of areas like machine translation (Bahdanau et al., 2015), summarization (Paulus et al., 2018), or dialogue systems (Wen et al., 2017). A rather new field for these models is the task of producing text with argumentative content (Wang and Ling, 2016). We believe this technology can support humans in the challenging task of finding and formulating arguments. A politician might use this to prepare for a debate with a political opponent or for a press conference. It may be used to support students in writing argumentative essays or to enrich one-sided discussions with counter-arguments. In contrast to retrieval methods, generation allows to combine and stylistically adapt text (e.g. arguments) based on a given input (usually the beginning of a sentence). Current argument generation models, however, produce lengthy texts and allow the user li"
2021.naacl-main.34,E17-1042,0,0.0152842,"this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the ArgCTRL is able to generate high-quality, aspectspecific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL.1 1 Introduction Language models (Bengio et al., 2003) allow to generate text through learned distributions of a language and have been applied to a variety of areas like machine translation (Bahdanau et al., 2015), summarization (Paulus et al., 2018), or dialogue systems (Wen et al., 2017). A rather new field for these models is the task of producing text with argumentative content (Wang and Ling, 2016). We believe this technology can support humans in the challenging task of finding and formulating arguments. A politician might use this to prepare for a debate with a political opponent or for a press conference. It may be used to support students in writing argumentative essays or to enrich one-sided discussions with counter-arguments. In contrast to retrieval methods, generation allows to combine and stylistically adapt text (e.g. arguments) based on a given input (usually th"
2021.nlp4convai-1.20,N18-3011,0,0.0526414,"Missing"
2021.nlp4convai-1.20,2021.acl-long.342,0,0.0150715,"tween entities (Gardent et al., 2017; Vougiouklis et al., 2018; Koncel-Kedziorski et al., 2019). Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017). Recently, it has been shown that structured meaning representation, such as AMR or KG, can store the internal state of a dialog system, providing core semantic knowledge (Bonial et al., 2020; Bai et al., 2021) or can be the result of a database query for conversational QA (Yu et al., 2019). Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators (Cheng et al., 2020). Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures (Vaswani et al., 2017) have considerably outperformed prior state of the art in various downstream tasks (Devlin et al., 2019; Yang et al., 2019a; Liu et al., 2020; Radford et al., 2019). In this paper, we analyze the applicability of two recen"
2021.nlp4convai-1.20,W13-2322,0,0.0364976,"more fluent texts than human references. In an extensive analysis, we identify possible reasons for the PLMs’ success on graph-totext tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.1 1 Introduction Graphs are important data structures in NLP as they represent complex relations within a set of objects. For example, semantic and syntactic structures of sentences can be represented using different graph representations (e.g., AMRs, Banarescu et al., 2013; semantic-role labeling, Surdeanu et al., 2008; syntactic and semantic graphs, Belz et al., 2011) and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities (Gardent et al., 2017; Vougiouklis et al., 2018; Koncel-Kedziorski et al., 2019). Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017)."
2021.nlp4convai-1.20,P18-1026,0,0.0725876,"possible reasons for such a good performance. 2 Related Work Graph-to-text Learning. Various neural models have been proposed to generate sentences from graphs from different domains. Konstas et al. (2017) propose the first neural approach for AMRto-text generation that uses a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input (Trisedya et al., 2018; Moryossef et al., 2019; Castro Ferreira et al., 2019; Ribeiro et al., 2021a). Recent approaches (Marcheggiani and Perez Beltrachini, 2018; Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhao et al., 2020a; Schmitt et al., 2021; Ribeiro et al., 2021b) propose architectures based on GNNs to directly encode the graph structure, whereas other efforts (Ribeiro et al., 2020; Schmitt et al., 2020; Yao et al., 2020; Wang et al., 2020) inject the graph structure information into Transformer-based architectures. The success of those approaches suggests that imposing a strong relational inductive bias into the graph-to-text model can assist the generation. Pretrained Language Models. Pretrained Transformer-based models, such as BERT (Devl"
2021.nlp4convai-1.20,W11-2832,0,0.0408327,"e PLMs’ success on graph-totext tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.1 1 Introduction Graphs are important data structures in NLP as they represent complex relations within a set of objects. For example, semantic and syntactic structures of sentences can be represented using different graph representations (e.g., AMRs, Banarescu et al., 2013; semantic-role labeling, Surdeanu et al., 2008; syntactic and semantic graphs, Belz et al., 2011) and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities (Gardent et al., 2017; Vougiouklis et al., 2018; Koncel-Kedziorski et al., 2019). Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017). Recently, it has been shown that structured meaning representation, such as AMR or KG, can store t"
2021.nlp4convai-1.20,2020.lrec-1.86,0,0.01309,"form of relations between entities (Gardent et al., 2017; Vougiouklis et al., 2018; Koncel-Kedziorski et al., 2019). Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017). Recently, it has been shown that structured meaning representation, such as AMR or KG, can store the internal state of a dialog system, providing core semantic knowledge (Bonial et al., 2020; Bai et al., 2021) or can be the result of a database query for conversational QA (Yu et al., 2019). Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators (Cheng et al., 2020). Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures (Vaswani et al., 2017) have considerably outperformed prior state of the art in various downstream tasks (Devlin et al., 2019; Yang et al., 2019a; Liu et al., 2020; Radford et al., 2019). In this paper, we analyze the applica"
2021.nlp4convai-1.20,P19-1470,0,0.041308,"Missing"
2021.nlp4convai-1.20,2020.acl-main.119,0,0.021947,"ditional Task-specific Data In order to evaluate the proposed task-adaptive pretraining strategies for graph-to-text generation, we collect task-specific data for two graph domains: meaning representations (like AMR) and scientific data (like AGENDA). We did not attempt collecting additional data like WebNLG because the texts in this benchmark do not stem from a corpus but were specifically written by annotators. AMR Silver Data. In order to generate additional data for AMR, we sample two sentence collections of size 200K and 2M from the Gigaword5 corpus and use a state-of-the-art AMR parser (Cai and Lam, 2020a) to parse them into AMR graphs.6 For supervised pretraining, we condition a model on the AMR silver graphs to generate the corresponding sentences before fine-tuning it on gold AMR graphs. For self-supervised pretraining, we only use the sentences.7 4 BLEU 27.87 31.82 32.46 33.90 34.10 Details of the preprocessing procedure of AMRs are provided in Appendix A. 5 https://catalog.ldc.upenn.edu/LDC2003T05 6 We filter out sentences that do not yield well-formed AMR graphs. 7 Gigaword and AMR datasets share similar data sources. Semantic Scholar AI Data. We collect titles and abstracts of around 1"
2021.nlp4convai-1.20,2020.acl-main.740,0,0.0449382,"Missing"
2021.nlp4convai-1.20,2020.coling-main.218,0,0.0564303,"Missing"
2021.nlp4convai-1.20,P17-1014,0,0.0962897,"ly better fluency than existing works and the human references. • We discover that PLMs perform well even when trained on a shuffled linearized graph representation without any information about connectivity (bag of node and edge labels), which is surprising since prior studies showed that explicitly encoding the graph structure improves models trained from scratch (e.g., Zhao et al., 2020a); and investigate the possible reasons for such a good performance. 2 Related Work Graph-to-text Learning. Various neural models have been proposed to generate sentences from graphs from different domains. Konstas et al. (2017) propose the first neural approach for AMRto-text generation that uses a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input (Trisedya et al., 2018; Moryossef et al., 2019; Castro Ferreira et al., 2019; Ribeiro et al., 2021a). Recent approaches (Marcheggiani and Perez Beltrachini, 2018; Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhao et al., 2020a; Schmitt et al., 2021; Ribeiro et al., 2021b) propose architectures based on GNNs to directly encode the graph structure,"
2021.nlp4convai-1.20,P02-1040,0,0.111203,"77.57 78.46 78.40 78.29 62.76 66.53 67.69 70.92 72.25 Table 2: Results on WebNLG. A, S and U stand for all, seen, and unseen partitions of the test set, respectively. models for AMR. Following Wolf et al. (2019), we use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 3 · 10−5 . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from {2,4,8} and {1,3,5}, respectively, based on the respective development set. Dev BLEU is used for model selection. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and chrF++ (Popovi´c, 2015) metrics. We also use MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020) metrics, as they employ contextual and semantic knowledge and thus depend less on the surface symbols. Additionally, we perform a human evaluation (cf. §5.4) quantifying the fluency, semantic adequacy and meaning similarity of the generated texts. 5.1 Results on AMR-to-Text other metrics follow similar trends. See Table 13 in Appendix for evaluation with more metrics. The strong performance of both BART and T5 in"
2021.nlp4convai-1.20,2020.acl-main.703,0,0.21309,"onal QA (Yu et al., 2019). Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators (Cheng et al., 2020). Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures (Vaswani et al., 2017) have considerably outperformed prior state of the art in various downstream tasks (Devlin et al., 2019; Yang et al., 2019a; Liu et al., 2020; Radford et al., 2019). In this paper, we analyze the applicability of two recent text-to-text pretrained language models (PLMs), BART (Lewis et al., 2020) and T5 (Raffel et al., 2019), for graph-to-text generation. We choose these models because of their encoderdecoder architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We further introduce task-adaptive graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin. While recent works have shown the benefit of explicitly encoding the graph structure in graph-totext generation (Song et al."
2021.nlp4convai-1.20,D19-1250,0,0.0604649,"Missing"
2021.nlp4convai-1.20,W15-3049,0,0.0449003,"Missing"
2021.nlp4convai-1.20,D18-1360,0,0.0246621,"Appendix A. 5 https://catalog.ldc.upenn.edu/LDC2003T05 6 We filter out sentences that do not yield well-formed AMR graphs. 7 Gigaword and AMR datasets share similar data sources. Semantic Scholar AI Data. We collect titles and abstracts of around 190K scientific papers from the Semantic Scholar (Ammar et al., 2018) taken from the proceedings of 36 top Computer Science/AI conferences. We construct KGs from the paper abstracts employing DyGIE++ (Wadden et al., 2019), an information extraction system for scientific texts. Note that the AGENDA dataset was constructed using the older SciIE system (Luan et al., 2018), which also extracts KGs from AI scientific papers. A second difference is that in our new dataset, the domain is broader as we collected data from 36 conferences compared to 12 from AGENDA. Furthermore, to prevent data leakage, all AGENDA samples used for performance evaluation are removed from our dataset. We will call the new dataset KGAIA (KGs from AI Abstracts).8 Table 11 in Appendix shows relevant dataset statistics. 5 Experiments We modify the BART and T5 implementations released by Hugging Face (Wolf et al., 2019) in order to adapt them to graph-to-text generation. For the KG datasets"
2021.nlp4convai-1.20,W18-6501,0,0.0308433,"Missing"
2021.nlp4convai-1.20,P19-1081,0,0.026448,"erent graph representations (e.g., AMRs, Banarescu et al., 2013; semantic-role labeling, Surdeanu et al., 2008; syntactic and semantic graphs, Belz et al., 2011) and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities (Gardent et al., 2017; Vougiouklis et al., 2018; Koncel-Kedziorski et al., 2019). Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017). Recently, it has been shown that structured meaning representation, such as AMR or KG, can store the internal state of a dialog system, providing core semantic knowledge (Bonial et al., 2020; Bai et al., 2021) or can be the result of a database query for conversational QA (Yu et al., 2019). Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators (Cheng et al., 2020). Transfer learning has become ubiquitous in NLP and pretrained Transformer-based arch"
2021.nlp4convai-1.20,2021.textgraphs-1.2,1,0.789375,"Various neural models have been proposed to generate sentences from graphs from different domains. Konstas et al. (2017) propose the first neural approach for AMRto-text generation that uses a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input (Trisedya et al., 2018; Moryossef et al., 2019; Castro Ferreira et al., 2019; Ribeiro et al., 2021a). Recent approaches (Marcheggiani and Perez Beltrachini, 2018; Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhao et al., 2020a; Schmitt et al., 2021; Ribeiro et al., 2021b) propose architectures based on GNNs to directly encode the graph structure, whereas other efforts (Ribeiro et al., 2020; Schmitt et al., 2020; Yao et al., 2020; Wang et al., 2020) inject the graph structure information into Transformer-based architectures. The success of those approaches suggests that imposing a strong relational inductive bias into the graph-to-text model can assist the generation. Pretrained Language Models. Pretrained Transformer-based models, such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), or RoBERTa (Liu et al., 2020), have establi"
2021.nlp4convai-1.20,2020.acl-main.704,0,0.0228128,"llowing Wolf et al. (2019), we use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 3 · 10−5 . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from {2,4,8} and {1,3,5}, respectively, based on the respective development set. Dev BLEU is used for model selection. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and chrF++ (Popovi´c, 2015) metrics. We also use MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020) metrics, as they employ contextual and semantic knowledge and thus depend less on the surface symbols. Additionally, we perform a human evaluation (cf. §5.4) quantifying the fluency, semantic adequacy and meaning similarity of the generated texts. 5.1 Results on AMR-to-Text other metrics follow similar trends. See Table 13 in Appendix for evaluation with more metrics. The strong performance of both BART and T5 in the AMR dataset suggests that PLMs can infer the AMR structure by a simple linear sequence of the graph, in contrast to GNN-based models that explicitly consider the graph structure"
2021.nlp4convai-1.20,P18-1150,0,0.0752798,"al., 2020) and T5 (Raffel et al., 2019), for graph-to-text generation. We choose these models because of their encoderdecoder architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We further introduce task-adaptive graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin. While recent works have shown the benefit of explicitly encoding the graph structure in graph-totext generation (Song et al., 2018; Ribeiro et al., 2019, 2020; Schmitt et al., 2020; Zhao et al., 2020a, to name a few), our approaches based on PLMs 1 consistently outperform these models, even though Our code is available at https://github.com/UKPLab/plmsgraph2text. PLMs – as sequence models – do not exhibit any 211 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 211–227 November 10, 2021. ©2021 Association for Computational Linguistics a) ARG0 ARG1 terrible-01 degree r he we u ck ba Alan ber Bean Mem de an ARG1 time crew Alfred Worden now tion upa occ Test Pilot m ARG2 Nasa fee"
2021.nlp4convai-1.20,2020.acl-main.640,0,0.0208796,"es a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input (Trisedya et al., 2018; Moryossef et al., 2019; Castro Ferreira et al., 2019; Ribeiro et al., 2021a). Recent approaches (Marcheggiani and Perez Beltrachini, 2018; Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhao et al., 2020a; Schmitt et al., 2021; Ribeiro et al., 2021b) propose architectures based on GNNs to directly encode the graph structure, whereas other efforts (Ribeiro et al., 2020; Schmitt et al., 2020; Yao et al., 2020; Wang et al., 2020) inject the graph structure information into Transformer-based architectures. The success of those approaches suggests that imposing a strong relational inductive bias into the graph-to-text model can assist the generation. Pretrained Language Models. Pretrained Transformer-based models, such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), or RoBERTa (Liu et al., 2020), have established a qualitatively new level of baseline performance for many widely used natural language understanding (NLU) benchmarks. Generative pretrained 2 The model architecture does not exp"
2021.nlp4convai-1.20,2020.acl-main.224,0,0.364129,"n. We choose these models because of their encoderdecoder architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We further introduce task-adaptive graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin. While recent works have shown the benefit of explicitly encoding the graph structure in graph-totext generation (Song et al., 2018; Ribeiro et al., 2019, 2020; Schmitt et al., 2020; Zhao et al., 2020a, to name a few), our approaches based on PLMs 1 consistently outperform these models, even though Our code is available at https://github.com/UKPLab/plmsgraph2text. PLMs – as sequence models – do not exhibit any 211 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 211–227 November 10, 2021. ©2021 Association for Computational Linguistics a) ARG0 ARG1 terrible-01 degree r he we u ck ba Alan ber Bean Mem de an ARG1 time crew Alfred Worden now tion upa occ Test Pilot m ARG2 Nasa feel-01 ARG0 t lo i pP m ARG1 have-rel-role-91 child tor opera co ARG0 A"
2021.nlp4convai-1.20,D19-1053,0,0.0238115,"een partitions of the test set, respectively. models for AMR. Following Wolf et al. (2019), we use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 3 · 10−5 . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from {2,4,8} and {1,3,5}, respectively, based on the respective development set. Dev BLEU is used for model selection. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and chrF++ (Popovi´c, 2015) metrics. We also use MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020) metrics, as they employ contextual and semantic knowledge and thus depend less on the surface symbols. Additionally, we perform a human evaluation (cf. §5.4) quantifying the fluency, semantic adequacy and meaning similarity of the generated texts. 5.1 Results on AMR-to-Text other metrics follow similar trends. See Table 13 in Appendix for evaluation with more metrics. The strong performance of both BART and T5 in the AMR dataset suggests that PLMs can infer the AMR structure by a simple linear sequence of the graph, in contrast"
2021.textgraphs-1.2,P18-1081,0,0.046667,"Missing"
2021.textgraphs-1.2,2020.acl-main.640,0,0.203716,"edge Graphs Martin Schmitt1 Leonardo F. R. Ribeiro2 Philipp Dufter1 Iryna Gurevych2 Hinrich Schütze1 1 2 Center for Information and Language Processing (CIS), LMU Munich Research Training Group AIPHES and UKP Lab, Technische Universität Darmstadt martin@cis.lmu.de Abstract et al., 2018) or variants of Transformer (Vaswani et al., 2017) that apply self-attention on all nodes together, including those that are not directly connected. To avoid losing information, the latter approaches use edge or node labels from the shortest path when computing the attention between two nodes (Zhu et al., 2019; Cai and Lam, 2020). Assuming the existence of a path between any two nodes is particularly problematic for KGs: a set of KG facts often does not form a connected graph. We propose a flexible alternative that neither needs such an assumption nor uses label information to model graph structure: a Transformerbased encoder that interprets the lengths of shortest paths in a graph as relative position information and thus, by means of multi-head attention, dynamically learns different structural views of the input graph with differently weighted connection patterns. We call this new architecture Graformer. Following"
2021.textgraphs-1.2,D19-1052,0,0.0340134,"Missing"
2021.textgraphs-1.2,P18-1008,0,0.0181307,") is the one-hotencoding of the ith node’s label. To compute the node representation H (L) in the Lth layer, we follow Vaswani et al. (2017), i.e., we first normalize the input from the previous layer H (L−1) via layer normalization LN , followed by multi-head graph self-attention SelfAtt g (see § 3.3 for details), which – after dropout regularization Dr and a residual connection – yields the intermediate representation I (cf. Eq. (1)). A feedforward layer FF with one hidden layer and GeLU (Hendrycks and Gimpel, 2016) activation computes the final layer output (cf. Eq. (2)). As recommended by Chen et al. (2018), we apply an additional layer normalization step to the output H (LE ) of the last encoder layer LE . singular −1 −2 +1 value −1 comparison +2 +1 decomposition used-for word2vec embedding −1 +1 Graformer encoder used-for learning (c) Incidence graph with SAMEp edges (dashed green) Figure 1: Different representations of the same KG (types are omitted for clarity). I (L) = Dr (SelfAtt g (LN (H (L−1) ))) + H (L−1) (1) ality of node labels in the graph structure by splitting each node into as many nodes as there are tokens in its label. We thus obtain a directed hypergraph GT = (VT , A, sT , tT ,"
2021.textgraphs-1.2,N18-3011,0,0.196784,"Missing"
2021.textgraphs-1.2,W17-3518,0,0.507841,"(LD ) of the last decoder layer LD is used to compute the probability distribution Pi ∈ [0, 1]|Σ| over all words in the vocabulary Σ at time step i:   (L ) Pi = σ Zi D E &gt; (14) Note that E ∈ R|Σ|×d is the same matrix that is also used to embed node labels and text tokens. 3.5 Training We train Graformer by minimizing the standard negative log-likelihood loss based on the likelihood estimations described in the previous section. 4 4.1 Experiments Datasets We evaluate our new architecture on two popular benchmarks for KG-to-text generation, AGENDA (Koncel-Kedziorski et al., 2019) and WebNLG (Gardent et al., 2017). While the latter contains crowd-sourced texts corresponding to subgraphs from various DBPedia categories, the former was automatically created by applying an information extraction tool (Luan et al., 2018) on a corpus of scientific abstracts (Ammar et al., 2018). As this process is noisy, we corrected 7 train instances where an entity name was erroneously split on a special character and, for the same reason, deleted 1 train instance entirely. Otherwise, we use the data as is, including the train/dev/test split. We list the number of instances per data split, as well as general statistics ab"
2021.textgraphs-1.2,Q19-1019,0,0.0329763,"xtreme versions of the graph: direct neighbors and full connection. Our approach is more flexible and dynamically learns a different structural view per attention head. It is also more parameter-efficient as our multi-view encoder does not need a separate set of parameters for each view. Related Work Most recent approaches to graph-to-text generation employ a graph neural network (GNN) based on message passing through the input graph’s topology as the encoder in their encoder-decoder architectures (Marcheggiani and Perez-Beltrachini, 2018; Koncel-Kedziorski et al., 2019; Ribeiro et al., 2019; Guo et al., 2019). As one layer of these encoders only considers immediate neighbors, a large number of stacked layers can be necessary to learn about distant nodes, which in turn also increases the risk of propagating noise (Li et al., 2018). Other approaches (Zhu et al., 2019; Cai and Lam, 2020) base their encoder on the Transformer architecture (Vaswani et al., 2017) and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, facilitating the information flow between distant nodes. Like Graformer, these approaches incorporate information about the graph topology with some varian"
2021.textgraphs-1.2,W05-0909,0,0.0857936,"LG, we follow previous work (Gardent et al., 2017) by replacing underscores in entity names with whitespace and breaking apart camelcased relations. We furthermore follow the evaluation protocol of the original challenge by converting all characters to lowercased ASCII and separating all punctuation from alphanumeric characters during tokenization. For both datasets, we train a BPE vocabulary using sentencepiece (Kudo and Richardson, 2018) on 5 5.1 Results and Discussion Overall performance Table 2 shows the results of our evaluation on AGENDA in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CHRF++ (Popovi´c, 2017). Like the models we compare with, we report the average and standard deviation of 4 runs with different random seeds. Our model outperforms previous Transformerbased models that only consider first-order neighborhoods per encoder layer (Koncel-Kedziorski et al., 2019; An et al., 2019). Compared to the very 15 BLEU Ours METEOR CHRF++ µc #P 17.80 ±0.31 22.07 ±0.23 45.43 ±0.39 36.3 GT 14.30 ±1.01 18.80 ±0.28 – – GT+RBS 15.1 ±0.97 19.5 ±0.29 – – CGE-LW 18.01 ±0.14 22.34 ±0.07 46.69 ±0.17 69.8 Table 2: Experimental results on AGENDA. GT (Graph Transformer) from (Koncel"
2021.textgraphs-1.2,N19-1119,0,0.0562463,"Missing"
2021.textgraphs-1.2,W17-4770,0,0.0247789,"Missing"
2021.textgraphs-1.2,D18-2012,0,0.051661,"Missing"
2021.textgraphs-1.2,D19-1314,1,0.858326,"ach can only see two extreme versions of the graph: direct neighbors and full connection. Our approach is more flexible and dynamically learns a different structural view per attention head. It is also more parameter-efficient as our multi-view encoder does not need a separate set of parameters for each view. Related Work Most recent approaches to graph-to-text generation employ a graph neural network (GNN) based on message passing through the input graph’s topology as the encoder in their encoder-decoder architectures (Marcheggiani and Perez-Beltrachini, 2018; Koncel-Kedziorski et al., 2019; Ribeiro et al., 2019; Guo et al., 2019). As one layer of these encoders only considers immediate neighbors, a large number of stacked layers can be necessary to learn about distant nodes, which in turn also increases the risk of propagating noise (Li et al., 2018). Other approaches (Zhu et al., 2019; Cai and Lam, 2020) base their encoder on the Transformer architecture (Vaswani et al., 2017) and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, facilitating the information flow between distant nodes. Like Graformer, these approaches incorporate information about the graph topolo"
2021.textgraphs-1.2,D18-1360,0,0.060737,"me matrix that is also used to embed node labels and text tokens. 3.5 Training We train Graformer by minimizing the standard negative log-likelihood loss based on the likelihood estimations described in the previous section. 4 4.1 Experiments Datasets We evaluate our new architecture on two popular benchmarks for KG-to-text generation, AGENDA (Koncel-Kedziorski et al., 2019) and WebNLG (Gardent et al., 2017). While the latter contains crowd-sourced texts corresponding to subgraphs from various DBPedia categories, the former was automatically created by applying an information extraction tool (Luan et al., 2018) on a corpus of scientific abstracts (Ammar et al., 2018). As this process is noisy, we corrected 7 train instances where an entity name was erroneously split on a special character and, for the same reason, deleted 1 train instance entirely. Otherwise, we use the data as is, including the train/dev/test split. We list the number of instances per data split, as well as general statistics about the graphs in Table 1. Note that the graphs in WebNLG are humanauthored subgraphs of DBpedia while the graphs Graformer decoder Our decoder follows closely the standard Transformer decoder (Vaswani et al"
2021.textgraphs-1.2,2020.tacl-1.38,1,0.885365,"2. Our graph encoder efficiently handles dependencies between much more distant nodes. Pei et al. (2020) define an additional neighborhood based on Euclidean distance in a continuous node embedding space. Similar to our work, a node can thus receive information from distant nodes, given their embeddings are close enough. However, Pei et al. (2020) compute these embeddings only once before training whereas in our approach node similarity is based on the learned representation in each encoder layer. This allows Graformer to dynamically change node interaction patterns during training. Recently, Ribeiro et al. (2020) use two GNN encoders – one using the original topology and one with a fully connected version of the graph – and combine their output in various ways for graph-totext generation. This approach can only see two extreme versions of the graph: direct neighbors and full connection. Our approach is more flexible and dynamically learns a different structural view per attention head. It is also more parameter-efficient as our multi-view encoder does not need a separate set of parameters for each view. Related Work Most recent approaches to graph-to-text generation employ a graph neural network (GNN)"
2021.textgraphs-1.2,W18-6501,0,0.16164,"mbine their output in various ways for graph-totext generation. This approach can only see two extreme versions of the graph: direct neighbors and full connection. Our approach is more flexible and dynamically learns a different structural view per attention head. It is also more parameter-efficient as our multi-view encoder does not need a separate set of parameters for each view. Related Work Most recent approaches to graph-to-text generation employ a graph neural network (GNN) based on message passing through the input graph’s topology as the encoder in their encoder-decoder architectures (Marcheggiani and Perez-Beltrachini, 2018; Koncel-Kedziorski et al., 2019; Ribeiro et al., 2019; Guo et al., 2019). As one layer of these encoders only considers immediate neighbors, a large number of stacked layers can be necessary to learn about distant nodes, which in turn also increases the risk of propagating noise (Li et al., 2018). Other approaches (Zhu et al., 2019; Cai and Lam, 2020) base their encoder on the Transformer architecture (Vaswani et al., 2017) and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, facilitating the information flow between distant nodes. Like Graformer, these app"
2021.textgraphs-1.2,2020.emnlp-main.577,1,0.8342,"ion A knowledge graph (KG) is a flexible data structure commonly used to store both general world knowledge (Auer et al., 2008) and specialized information, e.g., in biomedicine (Wishart et al., 2018) and computer vision (Krishna et al., 2017). Generating a natural language description of such a graph (KG→text) makes the stored information accessible to a broader audience of end users. It is therefore important for KG-based question answering (Bhowmik and de Melo, 2018), datato-document generation (Moryossef et al., 2019; Koncel-Kedziorski et al., 2019) and interpretability of KGs in general (Schmitt et al., 2020). Recent approaches to KG→text employ encoderdecoder architectures: the encoder first computes vector representations of the graph’s nodes, the decoder then uses them to predict the text sequence. Typical encoder choices are graph neural networks based on message passing between direct neighbors in the graph (Kipf and Welling, 2017; Veliˇckovi´c 1 Our code is publicly available: https://github. com/mnschmit/graformer 10 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 10–21 June 11, 2021. ©2021 Association for Computational Lin"
2021.textgraphs-1.2,N19-1236,0,0.0399162,"Missing"
2021.textgraphs-1.2,N18-2074,0,0.460794,"ly considers immediate neighbors, a large number of stacked layers can be necessary to learn about distant nodes, which in turn also increases the risk of propagating noise (Li et al., 2018). Other approaches (Zhu et al., 2019; Cai and Lam, 2020) base their encoder on the Transformer architecture (Vaswani et al., 2017) and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, facilitating the information flow between distant nodes. Like Graformer, these approaches incorporate information about the graph topology with some variant of relative position embeddings (Shaw et al., 2018). They, however, assume that there is always a path between any pair of nodes, i.e., there are no unreachable nodes or disconnected subgraphs. Thus they use an LSTM (Hochreiter and Schmidhuber, 1997) to compute a relation embedding from the labels along this path. However, in contrast to the AMR2 graphs used for their evaluation, KGs are frequently disconnected. Graformer is more flexible and makes no assumption about connectivity. Furthermore, its relative position embeddings only depend on the lengths of shortest paths i.e., purely structural information, not labels. It thus effectively lear"
2021.textgraphs-1.2,P02-1040,0,0.109913,"ts are also lowercased. For WebNLG, we follow previous work (Gardent et al., 2017) by replacing underscores in entity names with whitespace and breaking apart camelcased relations. We furthermore follow the evaluation protocol of the original challenge by converting all characters to lowercased ASCII and separating all punctuation from alphanumeric characters during tokenization. For both datasets, we train a BPE vocabulary using sentencepiece (Kudo and Richardson, 2018) on 5 5.1 Results and Discussion Overall performance Table 2 shows the results of our evaluation on AGENDA in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CHRF++ (Popovi´c, 2017). Like the models we compare with, we report the average and standard deviation of 4 runs with different random seeds. Our model outperforms previous Transformerbased models that only consider first-order neighborhoods per encoder layer (Koncel-Kedziorski et al., 2019; An et al., 2019). Compared to the very 15 BLEU Ours METEOR CHRF++ µc #P 17.80 ±0.31 22.07 ±0.23 45.43 ±0.39 36.3 GT 14.30 ±1.01 18.80 ±0.28 – – GT+RBS 15.1 ±0.97 19.5 ±0.29 – – CGE-LW 18.01 ±0.14 22.34 ±0.07 46.69 ±0.17 69.8 Table 2: Experimental results on AGENDA."
2021.textgraphs-1.2,D19-1548,0,0.0729804,"eration from Knowledge Graphs Martin Schmitt1 Leonardo F. R. Ribeiro2 Philipp Dufter1 Iryna Gurevych2 Hinrich Schütze1 1 2 Center for Information and Language Processing (CIS), LMU Munich Research Training Group AIPHES and UKP Lab, Technische Universität Darmstadt martin@cis.lmu.de Abstract et al., 2018) or variants of Transformer (Vaswani et al., 2017) that apply self-attention on all nodes together, including those that are not directly connected. To avoid losing information, the latter approaches use edge or node labels from the shortest path when computing the attention between two nodes (Zhu et al., 2019; Cai and Lam, 2020). Assuming the existence of a path between any two nodes is particularly problematic for KGs: a set of KG facts often does not form a connected graph. We propose a flexible alternative that neither needs such an assumption nor uses label information to model graph structure: a Transformerbased encoder that interprets the lengths of shortest paths in a graph as relative position information and thus, by means of multi-head attention, dynamically learns different structural views of the input graph with differently weighted connection patterns. We call this new architecture G"
C04-1110,P03-1048,0,0.0146477,"of semantic similarity metrics using the noun portion of WordNet as a knowledge source. So far, the noun senses have been disambiguated manually. The algorithm aims to extract utterances carrying the essential content of dialogues. We evaluate the system on 20 Switchboard dialogues. The results show that our system outperforms LEAD, RANDOM and TF*IDF baselines. 1 Introduction Research in automatic text summarization began in the late 1950s and has been receiving more attention again over the last decade. The maturity of this research area is indicated by recent large-scale evaluation efforts (Radev et al., 2003). In comparison, speech summarization is a rather new research area which emerged only a few years ago. However, the demand for speech summarization is growing because of the increasing availability of (digitally encoded) speech databases (e.g. spoken news, political speeches). Our research is concerned with the development of a system for automatically generating summaries of conversational speech. As a potential application we envision the automatic generation of meeting minutes. The approach to spoken dialogue summarization presented herein unifies corpus- and knowledge-based approaches to"
C04-1110,P00-1040,0,0.0266536,"n speech summarization focused mainly on single-speaker, written-to-be-spoken text (e.g. spoken news, political speeches, etc.). The methods were mostly derived from work on text summarization, but extended it by exploiting particular characteristics of spoken language, e.g. acoustic confidence scores or intonation. Difficulties arise because speech recognition systems are not perfect. Therefore, spoken dialogue summarization systems have to deal with errors in the input. There are no sentence boundaries in spoken language either. Work on spoken dialogue summarization is still in its infancy (Reithinger et al., 2000; Zechner, 2002). Multiparty dialogue is much more difficult to process than written text. In addition to the difficulties speech summarization has to face, spoken dialogue contains a whole range of dialogue phenomena as disfluencies, hesitations, interruptions, etc. Also, the information to be summarized may be contributed by different speakers (e.g. in questionanswer pairs). Finally, the language used in spoken dialogue differs from language used in texts. Because discourse participants are able to immediately clarify misunderstandings, the language used does not have to be that explicit. 3"
C04-1110,J02-4002,0,0.01784,"ion 4 provides information about the data used in our experiments, while Section 5 describes the experiments and the results together with their statistical significance. 2 Text, Speech and Dialogue Summarization Most research on automatic summarization dealt with written text. This work was based either on corpus-based, statistical methods or on knowledgebased techniques (for an overview over both strands of research see Mani & Maybury (1999)). Recent advances in text summarization are mostly due to statistical techniques with some additional usage of linguistic knowledge, e.g. (Marcu, 2000; Teufel & Moens, 2002), which can be applied to unrestricted input. Research on speech summarization focused mainly on single-speaker, written-to-be-spoken text (e.g. spoken news, political speeches, etc.). The methods were mostly derived from work on text summarization, but extended it by exploiting particular characteristics of spoken language, e.g. acoustic confidence scores or intonation. Difficulties arise because speech recognition systems are not perfect. Therefore, spoken dialogue summarization systems have to deal with errors in the input. There are no sentence boundaries in spoken language either. Work on"
C04-1110,J02-4003,0,0.0123953,"cused mainly on single-speaker, written-to-be-spoken text (e.g. spoken news, political speeches, etc.). The methods were mostly derived from work on text summarization, but extended it by exploiting particular characteristics of spoken language, e.g. acoustic confidence scores or intonation. Difficulties arise because speech recognition systems are not perfect. Therefore, spoken dialogue summarization systems have to deal with errors in the input. There are no sentence boundaries in spoken language either. Work on spoken dialogue summarization is still in its infancy (Reithinger et al., 2000; Zechner, 2002). Multiparty dialogue is much more difficult to process than written text. In addition to the difficulties speech summarization has to face, spoken dialogue contains a whole range of dialogue phenomena as disfluencies, hesitations, interruptions, etc. Also, the information to be summarized may be contributed by different speakers (e.g. in questionanswer pairs). Finally, the language used in spoken dialogue differs from language used in texts. Because discourse participants are able to immediately clarify misunderstandings, the language used does not have to be that explicit. 3 Semantic Similar"
C04-1110,O97-1002,0,\N,Missing
C08-4002,H05-1103,0,0.064591,"Missing"
C08-4002,W06-1416,0,0.04396,"Missing"
C08-4002,P06-4001,0,0.0219699,"Missing"
C08-4002,W05-0203,0,0.0602349,"Missing"
C08-4002,W05-0201,0,0.0603001,"Missing"
C08-4002,P98-1032,0,0.0234241,"Missing"
C08-4002,W08-0913,0,0.0224401,"Missing"
C08-4002,N06-1028,0,0.0608849,"Missing"
C08-4002,W08-0912,0,0.0310858,"Missing"
C08-4002,E99-1042,0,0.0589513,"Missing"
C08-4002,W03-1602,0,0.0301108,"Missing"
C08-4002,zesch-etal-2008-extracting,1,0.790896,"Missing"
C08-4002,P08-1021,0,0.0470054,"Missing"
C08-4002,P07-1011,0,0.0375383,"Missing"
C08-4002,D07-1012,0,0.026906,"Missing"
C08-4002,P06-1036,0,0.0441154,"Missing"
C08-4002,W08-0907,0,0.0332319,"Missing"
C08-4002,2005.sigdial-1.10,0,0.0614936,"Missing"
C08-4002,N04-3002,0,0.0608916,"Missing"
C08-4002,W06-1650,0,0.0730046,"Missing"
C08-4002,P07-2032,1,0.807216,"Missing"
C08-4002,P07-1130,1,0.883425,"Missing"
C10-1152,W03-1004,0,0.54068,"Missing"
C10-1152,E99-1042,0,0.966587,"tting, dropping, reordering and substitution integrally. We also describe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. 1 Introduction Sentence simplification transforms long and difficult sentences into shorter and more readable ones. This helps humans read texts more easily and faster. Reading assistance is thus an important application of sentence simplification, especially for people with reading disabilities (Carroll et al., 1999; Inui et al., 2003), low-literacy readers (Watanabe et al., 2009), or non-native speakers (Siddharthan, 2002). Not only human readers but also NLP applications can benefit from sentence simplification. The original motivation for sentence simplification is using it as a preprocessor to facilitate parsing or translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of"
C10-1152,C96-2183,0,0.821322,"ms long and difficult sentences into shorter and more readable ones. This helps humans read texts more easily and faster. Reading assistance is thus an important application of sentence simplification, especially for people with reading disabilities (Carroll et al., 1999; Inui et al., 2003), low-literacy readers (Watanabe et al., 2009), or non-native speakers (Siddharthan, 2002). Not only human readers but also NLP applications can benefit from sentence simplification. The original motivation for sentence simplification is using it as a preprocessor to facilitate parsing or translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No. GU 798/3-1, and by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under the grant No. I/82806. Iryna Gurevych1 Department of Computer Science Technische Universit¨at Darmstadt 2 delphine.bernhard@limsi.fr sentence fusion (Filippova and Strube, 2008b), semantic role lab"
C10-1152,W08-1105,0,0.612298,"r translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No. GU 798/3-1, and by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under the grant No. I/82806. Iryna Gurevych1 Department of Computer Science Technische Universit¨at Darmstadt 2 delphine.bernhard@limsi.fr sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al., 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). At sentence level, reading difficulty stems either from lexical or syntactic complexity. Sentence simplification can therefore be classified into two types: lexical simplification and syntactic simplification (Carroll et al., 1999). These two types of simplification can be further implemented by a set of simplification operations. Splitting, dropping, reordering, and substitution"
C10-1152,D08-1019,0,0.264012,"r translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No. GU 798/3-1, and by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under the grant No. I/82806. Iryna Gurevych1 Department of Computer Science Technische Universit¨at Darmstadt 2 delphine.bernhard@limsi.fr sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al., 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). At sentence level, reading difficulty stems either from lexical or syntactic complexity. Sentence simplification can therefore be classified into two types: lexical simplification and syntactic simplification (Carroll et al., 1999). These two types of simplification can be further implemented by a set of simplification operations. Splitting, dropping, reordering, and substitution"
C10-1152,J08-3004,0,0.0123846,"ce compression systems (Filippova and Strube, 2008a) mainly use the dropping operation. As far as lexical simplification is concerned, word substitution is usually done by selecting simpler synonyms from Wordnet based on word frequency (Carroll et al., 1999). In this paper, we propose a sentence simplification model by tree transformation which is based 1353 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353–1361, Beijing, August 2010 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada and Knight, 2002; Graehl et al., 2008). Our model integrally covers splitting, dropping, reordering and phrase/word substitution. The parameters of our model can be efficiently learned from complexsimple parallel datasets. The transformation from a complex sentence to a simple sentence is conducted by applying a sequence of simplification operations. An expectation maximization (EM) algorithm is used to iteratively train our model. We also propose a method based on monolingual word mapping which speeds up the training process significantly. Finally, a decoder is designed to generate the simplified sentences using a greedy strategy"
C10-1152,W03-1602,0,0.724115,"ering and substitution integrally. We also describe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. 1 Introduction Sentence simplification transforms long and difficult sentences into shorter and more readable ones. This helps humans read texts more easily and faster. Reading assistance is thus an important application of sentence simplification, especially for people with reading disabilities (Carroll et al., 1999; Inui et al., 2003), low-literacy readers (Watanabe et al., 2009), or non-native speakers (Siddharthan, 2002). Not only human readers but also NLP applications can benefit from sentence simplification. The original motivation for sentence simplification is using it as a preprocessor to facilitate parsing or translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research"
C10-1152,E06-1021,0,0.0526913,"Missing"
C10-1152,P05-1065,0,0.0122847,"e. This implies that good monolingual translation is not necessarily good simplification. OOV is the percentage of words that are not in the Basic English BE850 list.10 TSM is ranked as the second best system for this criterion. The perplexity (PPL) is a score of text probability measured by a language model and normalized by the number of words in the text (Equ. 6). 10 http://simple.wikipedia.org/wiki/ Wikipedia:Basic_English_alphabetical_ wordlist PPL can be used to measure how tight the language model fits the text. Language models constitute an important feature for assessing readability (Schwarm and Ostendorf, 2005). We train a trigram LM using the simple sentences in PWKP and calculate the PPL with SRILM. TSM gets the best PPL score. From this table, we can conclude that TSM achieves better overall readability than the baseline systems. 1 P P L(text) = P (w1 w2 ...wN )− N (6) There are still some important issues to be considered in future. Based on our observations, the current model performs well for word substitution and segmentation. But the completion of the new sentences is still problematic. For example, we copy the dependent NP to the new sentences. This may break the coherence between sentences"
C10-1152,P08-1040,0,0.0347455,"ex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No. GU 798/3-1, and by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under the grant No. I/82806. Iryna Gurevych1 Department of Computer Science Technische Universit¨at Darmstadt 2 delphine.bernhard@limsi.fr sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al., 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). At sentence level, reading difficulty stems either from lexical or syntactic complexity. Sentence simplification can therefore be classified into two types: lexical simplification and syntactic simplification (Carroll et al., 1999). These two types of simplification can be further implemented by a set of simplification operations. Splitting, dropping, reordering, and substitution are widely accepted as important simplification opera"
C10-1152,P01-1067,0,0.025569,"f (2007) focus on sentence splitting, while sentence compression systems (Filippova and Strube, 2008a) mainly use the dropping operation. As far as lexical simplification is concerned, word substitution is usually done by selecting simpler synonyms from Wordnet based on word frequency (Carroll et al., 1999). In this paper, we propose a sentence simplification model by tree transformation which is based 1353 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353–1361, Beijing, August 2010 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada and Knight, 2002; Graehl et al., 2008). Our model integrally covers splitting, dropping, reordering and phrase/word substitution. The parameters of our model can be efficiently learned from complexsimple parallel datasets. The transformation from a complex sentence to a simple sentence is conducted by applying a sequence of simplification operations. An expectation maximization (EM) algorithm is used to iteratively train our model. We also propose a method based on monolingual word mapping which speeds up the training process significantly. Finally, a decoder is designed to generate t"
C10-1152,P02-1039,0,0.00549496,"e splitting, while sentence compression systems (Filippova and Strube, 2008a) mainly use the dropping operation. As far as lexical simplification is concerned, word substitution is usually done by selecting simpler synonyms from Wordnet based on word frequency (Carroll et al., 1999). In this paper, we propose a sentence simplification model by tree transformation which is based 1353 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353–1361, Beijing, August 2010 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada and Knight, 2002; Graehl et al., 2008). Our model integrally covers splitting, dropping, reordering and phrase/word substitution. The parameters of our model can be efficiently learned from complexsimple parallel datasets. The transformation from a complex sentence to a simple sentence is conducted by applying a sequence of simplification operations. An expectation maximization (EM) algorithm is used to iteratively train our model. We also propose a method based on monolingual word mapping which speeds up the training process significantly. Finally, a decoder is designed to generate the simplified sentences u"
C10-1152,zesch-etal-2008-extracting,1,0.7627,"Missing"
C10-1152,P09-1094,0,0.008128,"ication has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No. GU 798/3-1, and by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under the grant No. I/82806. Iryna Gurevych1 Department of Computer Science Technische Universit¨at Darmstadt 2 delphine.bernhard@limsi.fr sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al., 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). At sentence level, reading difficulty stems either from lexical or syntactic complexity. Sentence simplification can therefore be classified into two types: lexical simplification and syntactic simplification (Carroll et al., 1999). These two types of simplification can be further implemented by a set of simplification operations. Splitting, dropping, reordering, and substitution are widely accepted as important simplification operations. The splitting operation splits a long sentence into several shorter sentences to de"
C12-1011,J08-4004,0,0.0735347,"mentation of the system by Clough and Stevenson (2011) achieves F¯1 = .788. Our system again outperforms all other systems with F¯1 = .859. In our envisioned semi-supervised application scenario, potentially reused texts are presented to users in an informative manner. Here, fine-grained distinctions are not necessary, and we decided to go even one step further and fold all potential cases of text reuse. This variant of the dataset results in a binary classification of plagiarized/non-plagiarized texts. We present 10 11 12 An exhaustive discussion of inter-rater agreement measures is given by Artstein and Poesio (2008). http://www.ukp.tu-darmstadt.de/data/text-similarity/text-reuse-annotations Strength of agreement for κ values according to Landis and Koch (1977) We report the results for our re-implementation of the system by Clough and Stevenson (2011). In their original work, they did not evaluate on this dataset. 13 175 System Acc. F¯1 Majority Class Baseline Ferret Baseline .715 .684 .417 .535 Clough and Stevenson (2011)13 Sánchez-Vega et al. (2010) .692 .783 .680 .705 Our Approach .802 .768 exp. class. reuse no reuse reuse 151 30 no reuse 20 52 Table 6: Results and confusion matrix for the best classi"
C12-1011,R11-1071,1,0.849433,"ihalcea et al., 2006; Landauer et al., 1998; Gabrilovich and Markovitch, 2007). However, existing similarity measures typically exhibit a major limitation: They compute similarity only on features which can be derived from the content of the given texts. By following this approach, they inherently imply that the similarity computation process does not need to take any other text characteristics into account. In contrast, we propose that text reuse detection indeed benefits from also assessing similarity along other text characteristics (dimensions, henceforth). We follow empirical evidence by Bär et al. (2011) and focus on three characteristic similarity dimensions inherent to texts: content, structure, and style. Figure 1 shows an example of text reuse taken from the Wikipedia Rewrite Corpus (see Section 3.1) where parts of a given source text have been reused either verbatim or by using similar words or phrases. As the example illustrates, the process of creating reused text includes a revision step in which the editor has a certain degree of freedom on how to reuse the source text. This kind of similarity is detectable by content-centric text similarity measures. However, the editor has further"
C12-1011,C10-1005,0,0.0135997,"as occurred. After compiling two sets of n-grams, we compared them using the Jaccard coefficient, following Lyon et al. (2001), as well as using the containment measure (Broder, 1997). We tested n-gram sizes for n = 1, 2, . . . , 15, and will use the original system name Ferret (Lyon et al., 2004) to refer to the variant with n = 3 using the Jaccard coefficient, henceforth. Following the idea of comparing lexical patterns, we also used a measure which has not yet been considered for assessing content similarity: character n-gram profiles (Keselj et al., 2003).3 We follow the implementation by Barrón-Cedeño et al. (2010) and discard all characters (case insensitive) which are not in the alphabet Σ = {a, . . . , z, 0, . . . , 9}, then generate all n-grams on character level, weight them by a tfidf scheme, and finally compare the feature vectors of both the rewritten and the source text using the cosine measure. While in the original implementation only n = 3 was used, we generalize the measure to n = 2, 3, . . . , 15. In cases where the editor replaced content words by synonyms, string measures typically fail due to the vocabulary gap. We thus used similarity measures which are capable of measuring semantic si"
C12-1011,P04-2006,0,0.037944,"Missing"
C12-1011,W10-0701,0,0.0183427,"Missing"
C12-1011,J96-2004,0,0.0493822,"Missing"
C12-1011,P02-1020,0,0.0325053,"ve knowledge of what content is already present in the wiki, and what is not. As wikis are traditionally growing fast, this is hardly feasible, though. To remedy this issue, we aim at supporting authors of collaborative text collections by means of automatic text reuse detection. We envision a semi-supervised system that informs a content author of potentially pre-existing instances of text reuse, and then lets the author decide how to proceed, e.g. to merge both texts. Detecting text reuse has been studied in a variety of tasks and applications, e.g. the detection of journalistic text reuse (Clough et al., 2002), the identification of rewrite sources for ancient literary texts (Lee, 2007), or the analysis of text reuse in blogs and web pages (Abdel-Hamid et al., 2009). Another common instance of text reuse is plagiarism, with the additional constraint that the reuse needs to be unacknowledged. Near-duplicate detection is also a broad field of related work where the detection of text reuse is crucial, e.g. in the context of web search and crawling (Hoad and Zobel, 2003; Henzinger, 2006; Manku et al., 2007). Prior work, however, mainly utilizes fingerprinting and hashing techniques (Charikar, 2002) for"
C12-1011,W99-0625,0,0.0219255,"ing only stopwords. All n-grams of both texts are then compared using the containment measure (Broder, 1997). We tested n-gram sizes for n = 2, 3, . . . , 15. For the same reason, we also included part-of-speech n-grams in our feature set. Disregarding the actual words that appear in two given texts, computing n-grams along part-of-speech tags allows to detect syntactic similarities between these texts. Again, we tested n-gram sizes for n = 2, 3, . . . , 15, and compared the two sets using the containment measure (Broder, 1997). We also employed two similarity measures between pairs of words (Hatzivassiloglou et al., 1999). The word pair order measure assumes that a similar syntactical structure in reused texts may cause two words to occur in the same order in both texts (with any number of words in between). The complementary word pair distance measure counts the number of words which lie between those of a given pair. For each measure, we computed feature vectors for both texts along all shared word pairs and compared the vectors using Pearson’s correlation. 2.3 Stylistic Similarity Measures of stylistic similarity adopt ideas from authorship attribution (Mosteller and Wallace, 1964) or use statistical proper"
C12-1011,O97-1002,0,0.0394863,". , 9}, then generate all n-grams on character level, weight them by a tfidf scheme, and finally compare the feature vectors of both the rewritten and the source text using the cosine measure. While in the original implementation only n = 3 was used, we generalize the measure to n = 2, 3, . . . , 15. In cases where the editor replaced content words by synonyms, string measures typically fail due to the vocabulary gap. We thus used similarity measures which are capable of measuring semantic similarity between words. We used the following word similarity measures with WordNet (Fellbaum, 1998): Jiang and Conrath (1997), Lin (1998), and Resnik (1995). In order to scale these pairwise word similarity scores to the document level, we follow the aggregation strategy by Mihalcea et al. (2006): First, a directional similarity score simd (Ti , T j ) is computed from a text Ti to a second text T j (Eq. 1). Therefore, for each word w i in Ti , its best-matching counterpart in T j is sought (ma xSim(w i , T j )). The similarity scores of all these matches are summed up and weighted according to their inverse document frequency idf (Spärck Jones, 1972), then normalized. The final document-level similarity figure is th"
C12-1011,2005.mtsummit-papers.11,0,0.0165224,"ores of all these matches are summed up and weighted according to their inverse document frequency idf (Spärck Jones, 1972), then normalized. The final document-level similarity figure is the average of applying this strategy in both directions, from Ti to T j and vice-versa (Eq. 2). P simd (Ti , T j ) = wi maxSim(w i , T j ) · id f (w i ) P wi id f (w i ) (1) sim(Ti , T j ) =  1 simd (Ti , T j ) + simd (T j , Ti ) (2) 2 We also tested text expansion mechanisms with the semantic word similarity measures described above: We used the Moses SMT system (Koehn et al., 2007), trained on Europarl (Koehn, 2005), to translate the original English texts via a bridge language (Dutch) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. We computed pairwise word similarity with the measures described above and aggregated according to Mihalcea et al. (2006). Furthermore, we used the statistical technique Latent Semantic Analysis (LSA) (Landauer et al., 3 Traditionally, character n-gram profiles have rather been shown successful for authorship attribution. However, the similarity scores of word n-grams and those of"
C12-1011,P07-2045,0,0.012772,"t (ma xSim(w i , T j )). The similarity scores of all these matches are summed up and weighted according to their inverse document frequency idf (Spärck Jones, 1972), then normalized. The final document-level similarity figure is the average of applying this strategy in both directions, from Ti to T j and vice-versa (Eq. 2). P simd (Ti , T j ) = wi maxSim(w i , T j ) · id f (w i ) P wi id f (w i ) (1) sim(Ti , T j ) =  1 simd (Ti , T j ) + simd (T j , Ti ) (2) 2 We also tested text expansion mechanisms with the semantic word similarity measures described above: We used the Moses SMT system (Koehn et al., 2007), trained on Europarl (Koehn, 2005), to translate the original English texts via a bridge language (Dutch) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. We computed pairwise word similarity with the measures described above and aggregated according to Mihalcea et al. (2006). Furthermore, we used the statistical technique Latent Semantic Analysis (LSA) (Landauer et al., 3 Traditionally, character n-gram profiles have rather been shown successful for authorship attribution. However, the similarity"
C12-1011,P07-1060,0,0.0987682,"re traditionally growing fast, this is hardly feasible, though. To remedy this issue, we aim at supporting authors of collaborative text collections by means of automatic text reuse detection. We envision a semi-supervised system that informs a content author of potentially pre-existing instances of text reuse, and then lets the author decide how to proceed, e.g. to merge both texts. Detecting text reuse has been studied in a variety of tasks and applications, e.g. the detection of journalistic text reuse (Clough et al., 2002), the identification of rewrite sources for ancient literary texts (Lee, 2007), or the analysis of text reuse in blogs and web pages (Abdel-Hamid et al., 2009). Another common instance of text reuse is plagiarism, with the additional constraint that the reuse needs to be unacknowledged. Near-duplicate detection is also a broad field of related work where the detection of text reuse is crucial, e.g. in the context of web search and crawling (Hoad and Zobel, 2003; Henzinger, 2006; Manku et al., 2007). Prior work, however, mainly utilizes fingerprinting and hashing techniques (Charikar, 2002) for text comparison rather than methods from natural language processing. A commo"
C12-1011,W01-0515,0,0.129313,"discussed measures in order to stimulate the development of novel measures: http://code.google.com/p/dkpro-similarity-asl 169 distance according to a given metric. We used the following measures in our experiments: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Starting from the observation that not all words in a document are of equal importance, we further employed a similarity measure which weights all words by a tfidf scheme (Salton and McGill, 1983) and computes text similarity as the cosine between two document vectors. Comparing word n-grams (Lyon et al., 2001) is a popular means for comparing lexical patterns between two texts. The more similar the patterns, the more likely is it that text reuse has occurred. After compiling two sets of n-grams, we compared them using the Jaccard coefficient, following Lyon et al. (2001), as well as using the containment measure (Broder, 1997). We tested n-gram sizes for n = 1, 2, . . . , 15, and will use the original system name Ferret (Lyon et al., 2004) to refer to the variant with n = 3 using the Jaccard coefficient, henceforth. Following the idea of comparing lexical patterns, we also used a measure which has"
C12-1044,E12-1036,0,0.170926,"es, format refers to HTML/CSS and templates. Fong and Biuk-Aghai (2010) present a system to automatically calculate and categorize edits. Similar to our system, their system computes a list of basic edit actions on the unparsed source text (i.e. including markup). Edits are calculated with granularity at the sentence and token level. The authors suggest a set of rules and categories to label the basic edit actions calculated before. Examples of their categories are (De)Wikify, Content Modification or Spelling Correction. The implementation and evaluation of their system is rather preliminary. Bronner and Monz (2012) distinguish between Factual and Fluency edits. They segment adjacent revisions into edits and classify them in a supervised machine learning system. Their system successfully classifies edits into Factual and Fluency edits with a maximum accuracy of 0.88. Except for Bronner and Monz (2012), all of the above presented annotation studies label pairs of adjacent revisions, not edits. Hence, even if multi-labeling is applied, it is not possible to k reassign each local edit e v−1,v with a category from the set or list of categories assigned to the (r v−1 , r v )-pair. No manual annotation study w"
C12-1044,P11-4017,1,0.77822,"izes of the groups result from the fact that we had to choose adjacent revisions for each article and stage. These revisions contain diverging numbers of edits which did not always sum up to precisely 50. The corpus has been selected to reflect the entire range of possible edits in Wikipedia, including bot edits, vandalism and reverts. Hence, no further filtering is done. Edit segmentation The raw data for our corpus is extracted from the English Wikipedia Revision History, from the dump as of April 2011. We process the revision content (text with markup) using the Wikipedia Revision Toolkit (Ferschke et al., 2011). We do not parse the revision text, as we want to include both edits affecting the content and edits affecting the k layout into one taxonomy. For each (r v−1 , r v )-pair, we calculate all of the n changes e v−1,v that have been made to the current revision via an adapted version of the diff comparison algorithm by Heckel (1978). The algorithm splits each revision into its lines and numbers them. Then, it compares each line in r v−1 with each line in r v to find differences in terms of inserted, deleted, modified and relocated lines. Although we only work with data from the English Wikipedia"
C12-1044,P12-2049,0,0.0486399,"Online collaboration software supports project management, version control systems enable the collaborative development of source code, and recent developments in cloud computing have generated new ways of collaborating on single files. A lot of research has been devoted to the development of userfriendly tools and editors for collaborative writing (Noel and Robert, 2004). Free tools include web-based software such as Zoho Writer, Google Drive or Etherpad, as well as Wikis such as Twiki, Foswiki and MediaWiki. Corpora for analyzing the writing process mostly come from the educational domain (Lee and Webster, 2012). An exception is the Digital Variants Archive2 , which contains contemporary texts by Spanish and Italian authors including various revisions of those texts. However, these corpora only consist of textual revisions by one author. Although there are many tools enabling users to collaboratively write texts, little work has been done to analyze the underlying collaboration process of the data that is created with these tools. One possible reason is that few corpora for analyzing collaborative writing are available. Since their invention in the mid 90s, Wikis have become one of the most important"
C12-1044,passonneau-2006-measuring,0,0.0204064,"Missing"
C12-1044,D11-1038,0,0.0353539,"possible reason is that few corpora for analyzing collaborative writing are available. Since their invention in the mid 90s, Wikis have become one of the most important tools for creating and sharing contents. They enable a detailed tracking of changes, as they usually implement a revision control system which saves every change to a page. At the time of writing, the number of revisions in the English online encyclopedia Wikipedia kept growing by 3.2 million revisions each month.3 Various studies have processed parts of that data for different tasks such as extracting sentence simplification (Woodsend and Lapata, 2011) or spelling error correction (Zesch, 2012). Whenever an editor of a page in Wikipedia saves changes, a new revision is created. As one revision may contain a set of distinct local changes, we distinguish between revisions and edits. We define an edit as a coherent local change, usually perceived by a human reader as one single editing action. For a pair of adjacent revisions, we denote the previous revision with r v−1 and k the newer revision with r v . For each (r v−1 , r v )-pair, we calculate a set of n edits e v−1,v (where k = {0, 1, ...n − 1}) that have been made to transform r v−1 into"
C12-1044,E12-1054,0,0.153751,"rative writing are available. Since their invention in the mid 90s, Wikis have become one of the most important tools for creating and sharing contents. They enable a detailed tracking of changes, as they usually implement a revision control system which saves every change to a page. At the time of writing, the number of revisions in the English online encyclopedia Wikipedia kept growing by 3.2 million revisions each month.3 Various studies have processed parts of that data for different tasks such as extracting sentence simplification (Woodsend and Lapata, 2011) or spelling error correction (Zesch, 2012). Whenever an editor of a page in Wikipedia saves changes, a new revision is created. As one revision may contain a set of distinct local changes, we distinguish between revisions and edits. We define an edit as a coherent local change, usually perceived by a human reader as one single editing action. For a pair of adjacent revisions, we denote the previous revision with r v−1 and k the newer revision with r v . For each (r v−1 , r v )-pair, we calculate a set of n edits e v−1,v (where k = {0, 1, ...n − 1}) that have been made to transform r v−1 into r v (see Section 3.2). We label edits with"
C12-1108,atserias-etal-2004-spanish,0,0.0804833,"Missing"
C12-1108,J06-1003,0,0.237255,"tural language processing applications, however, cannot disambiguate such relations easily. The same applies to translations: The German ausstellen has, for instance, a meaning of (1) exhibiting an object, (2) certificating a document and (3) turning off smth. Figure 1 illustrates this kind of underspecification. In Section 3, we propose a solution to this issue by automatically disambiguating the semantic relations and translations in Wiktionary. Sense-disambiguated relations are a necessary precondition for many applications, such as computing semantic relatedness by measuring path lengths (Budanitsky and Hirst, 2006): if undisambiguated relations were used, then exhibit and loiter would be highly related as they both have a relation to hang. Besides the information inherently found in Wiktionary, we infer new semantic relations based on our disambiguated translations. This is particularly useful for the English Wiktionary, which encodes only about 26,000 semantic relations (compared to 290,000 in the German edition). With our inference method, we are able to increase the number of semantic relations for the English language by almost ten times. Section 4 describes our inference method and provides statist"
C12-1108,E12-1059,1,0.814223,"novel German and two cross-lingual verb similarity datasets. Our resource competes with expert-built wordnets in the monolingual setting. Since Wiktionary is available in many languages, this allows for computing verb similarity also for languages lacking large expert-built resources. In the cross-lingual setting, our sense-disambiguated resource outperforms both Wikipedia and the expert-built wordnets. The former suffers from the small amount of knowledge about verbs and the latter lack coverage of the inter-lingual index. In future work, we plan to combine our resource with BabelNet or UBY (Gurevych et al., 2012) in order to benefit from the heterogeneous knowledge found in WordNet, Wikipedia, and our resource. Extending our resource to other languages and exploring alternative disambiguation algorithms such as CQC are further promising options. We will also consider providing our inferred semantic relations to the Wiktionary community to contribute to the harmonization of Wiktionary data. Besides verb similarity, our sense-disambiguated resource has the potential to improve other natural language processing tasks as well, for instance, question answering. Acknowledgments This work has been supported"
C12-1108,W99-0501,0,0.264549,"The task of disambiguating semantic relations (also called sense linking and relation anchoring) has been previously described in the context of machine-readable dictionaries (Krovetz, 1992) and ontology learning (Pantel and Pennacchiotti, 2008). Meyer and Gurevych (2010) discussed relation disambiguation for the German Wiktionary using a disambiguation method based on textual similarity. In Section 3.3, we will compare this approach to our system. The disambiguation of all words in a sense definition (i.e. gloss disambiguation), as it has been done in the WordNet 2/eXtendend WordNet project (Harabagiu et al., 1999; Mihalcea and Moldovan, 2001), is very similar to the disambiguation of semantic relations. Therefore, many of the features defined in Section 3.1 are similar to those proposed by Moldovan and Novischi (2004). Note however that we use explicitly defined semantic relations rather than sense definitions as our disambiguation subjects. In addition to that, we adapt our method to Wiktionary instead of using WordNet-specific features and also extend this work to a crosslingual setting. Very recently, Flati and Navigli (2012) proposed a graph-based method to gloss disambiguation outperforming previ"
C12-1108,W02-1106,0,0.0914461,"Missing"
C12-1108,W99-0905,0,0.0543253,"adapt our method to Wiktionary instead of using WordNet-specific features and also extend this work to a crosslingual setting. Very recently, Flati and Navigli (2012) proposed a graph-based method to gloss disambiguation outperforming previous approaches. While this method could in general be adapted to disambiguating relations, we observe that the graph induced by Wiktionary’s semantic relations is very sparse. This would hinder finding the cycles and quasi-cycles required by the method. The disambiguation of translations has been studied in the context of bilingual dictionaries and corpora (Kikui, 1999; Tsunakawa and Kaji, 2010). Mausam et al. (2009) discovered new translations in Wiktionary using a graph-based inference algorithm for Wiktionary translations. Although this also involves a disambiguation of translations, their work is not directly comparable to ours, since they do not strictly use the word senses encoded in Wiktionary but define them based on the translations shared across multiple languages. In contrast to that, we aim at exploiting a wide range of lexical-semantic knowledge and therefore need to rely on the word senses actually encoded in Wiktionary. 3 http://www.illc.uva."
C12-1108,P92-1054,0,0.857613,"from WordNet. BabelNet aligns WordNet and Wikipedia at the level of word senses. Although this yields a large resource, the additional information from Wikipedia is almost entirely about nouns – there are hence no translations for verbs, adjectives, or the like. Our work provides a viable option towards closing this gap, as it makes use of lexical-semantic knowledge covering any part of speech. Relation disambiguation. The task of disambiguating semantic relations (also called sense linking and relation anchoring) has been previously described in the context of machine-readable dictionaries (Krovetz, 1992) and ontology learning (Pantel and Pennacchiotti, 2008). Meyer and Gurevych (2010) discussed relation disambiguation for the German Wiktionary using a disambiguation method based on textual similarity. In Section 3.3, we will compare this approach to our system. The disambiguation of all words in a sense definition (i.e. gloss disambiguation), as it has been done in the WordNet 2/eXtendend WordNet project (Harabagiu et al., 1999; Mihalcea and Moldovan, 2001), is very similar to the disambiguation of semantic relations. Therefore, many of the features defined in Section 3.1 are similar to those"
C12-1108,S10-1003,0,0.0609704,"Missing"
C12-1108,P09-1030,0,0.0298201,"f using WordNet-specific features and also extend this work to a crosslingual setting. Very recently, Flati and Navigli (2012) proposed a graph-based method to gloss disambiguation outperforming previous approaches. While this method could in general be adapted to disambiguating relations, we observe that the graph induced by Wiktionary’s semantic relations is very sparse. This would hinder finding the cycles and quasi-cycles required by the method. The disambiguation of translations has been studied in the context of bilingual dictionaries and corpora (Kikui, 1999; Tsunakawa and Kaji, 2010). Mausam et al. (2009) discovered new translations in Wiktionary using a graph-based inference algorithm for Wiktionary translations. Although this also involves a disambiguation of translations, their work is not directly comparable to ours, since they do not strictly use the word senses encoded in Wiktionary but define them based on the translations shared across multiple languages. In contrast to that, we aim at exploiting a wide range of lexical-semantic knowledge and therefore need to rely on the word senses actually encoded in Wiktionary. 3 http://www.illc.uva.nl/EuroWordNet/finalresults-ewn.html (accessed 20"
C12-1108,S10-1002,0,0.0481075,"Missing"
C12-1108,W04-0804,0,0.448551,"learning (Pantel and Pennacchiotti, 2008). Meyer and Gurevych (2010) discussed relation disambiguation for the German Wiktionary using a disambiguation method based on textual similarity. In Section 3.3, we will compare this approach to our system. The disambiguation of all words in a sense definition (i.e. gloss disambiguation), as it has been done in the WordNet 2/eXtendend WordNet project (Harabagiu et al., 1999; Mihalcea and Moldovan, 2001), is very similar to the disambiguation of semantic relations. Therefore, many of the features defined in Section 3.1 are similar to those proposed by Moldovan and Novischi (2004). Note however that we use explicitly defined semantic relations rather than sense definitions as our disambiguation subjects. In addition to that, we adapt our method to Wiktionary instead of using WordNet-specific features and also extend this work to a crosslingual setting. Very recently, Flati and Navigli (2012) proposed a graph-based method to gloss disambiguation outperforming previous approaches. While this method could in general be adapted to disambiguating relations, we observe that the graph induced by Wiktionary’s semantic relations is very sparse. This would hinder finding the cyc"
C12-1108,nastase-etal-2010-wikinet,0,0.0282585,"02). All of them are professionally crafted and provide a well-structured network of word senses and relations. Despite their high quality, the sizes vary largely. For English and German, there are, for instance, only 16,347 shared word senses in EuroWordNet (the only one encoding this language pair).3 Another drawback of these wordnets is their high development cost which hinders the large-scale manual extension of their contents. Wikipedia-based multilingual resources is another strand of research. Well-known works are Yago (Suchanek et al., 2007), DBpedia (Bizer et al., 2009), and WikiNet (Nastase et al., 2010), which mostly differ in their structure and the way they extract the data. The bulk of knowledge in Wikipedia is, however, of encyclopedic nature, whereas our work aims at lexical-semantic knowledge. The two most closely related research efforts to ours are Universal WordNet (de Melo and Weikum, 2009) and BabelNet (Navigli and Ponzetto, 2010). The former uses WordNet for bootstrapping a multilingual resource based on combined evidence found in existing wordnets, parallel corpora, and machine-readable dictionaries. It incorporates (undisambiguated) Wiktionary translations, but solely relies on"
C12-1108,P10-1023,0,0.0437222,"is their high development cost which hinders the large-scale manual extension of their contents. Wikipedia-based multilingual resources is another strand of research. Well-known works are Yago (Suchanek et al., 2007), DBpedia (Bizer et al., 2009), and WikiNet (Nastase et al., 2010), which mostly differ in their structure and the way they extract the data. The bulk of knowledge in Wikipedia is, however, of encyclopedic nature, whereas our work aims at lexical-semantic knowledge. The two most closely related research efforts to ours are Universal WordNet (de Melo and Weikum, 2009) and BabelNet (Navigli and Ponzetto, 2010). The former uses WordNet for bootstrapping a multilingual resource based on combined evidence found in existing wordnets, parallel corpora, and machine-readable dictionaries. It incorporates (undisambiguated) Wiktionary translations, but solely relies on semantic relations taken from WordNet. BabelNet aligns WordNet and Wikipedia at the level of word senses. Although this yields a large resource, the additional information from Wikipedia is almost entirely about nouns – there are hence no translations for verbs, adjectives, or the like. Our work provides a viable option towards closing this g"
C12-1108,2007.tmi-papers.18,0,0.0164851,"cing globalization and the permeation of the internet in our daily lives raises a strong demand for multilingual applications, such as machine translation, crosslingual question answering, or information retrieval. Traditional multilingual approaches are knowledge-based using bilingual dictionaries (Neff and McCord, 1990) or multilingual wordnets (Tufi¸s et al., 2004). To date, these approaches are getting more and more replaced by statistical translation models, although it has been found that multilingual resources have the ability to substantially contribute to the performance of a system (Oepen et al., 2007; Herbert et al., 2011). One reason for the knowledge-based approaches being rarely employed is the challenging construction process of multilingual resources. They are either manually compiled by professional translators or lexicographers or automatically generated from large amounts of unstructured data. The former usually results in small resources due to the time and cost intensive work, whereas the latter often reaches only a limited quality. Although Wikipedia has been found as a promising alternative for obtaining multilingual knowledge (Medelyan et al., 2009), it is almost entirely res"
C12-1108,W09-3420,0,0.0237478,"Missing"
C12-1108,W10-3205,0,0.0287871,"hod to Wiktionary instead of using WordNet-specific features and also extend this work to a crosslingual setting. Very recently, Flati and Navigli (2012) proposed a graph-based method to gloss disambiguation outperforming previous approaches. While this method could in general be adapted to disambiguating relations, we observe that the graph induced by Wiktionary’s semantic relations is very sparse. This would hinder finding the cycles and quasi-cycles required by the method. The disambiguation of translations has been studied in the context of bilingual dictionaries and corpora (Kikui, 1999; Tsunakawa and Kaji, 2010). Mausam et al. (2009) discovered new translations in Wiktionary using a graph-based inference algorithm for Wiktionary translations. Although this also involves a disambiguation of translations, their work is not directly comparable to ours, since they do not strictly use the word senses encoded in Wiktionary but define them based on the translations shared across multiple languages. In contrast to that, we aim at exploiting a wide range of lexical-semantic knowledge and therefore need to rely on the word senses actually encoded in Wiktionary. 3 http://www.illc.uva.nl/EuroWordNet/finalresults"
C12-1108,zesch-etal-2008-extracting,1,0.829449,"s. The eighth word sense is defined as “to exhibit (an object)” with synonymy relations targeting at exhibit and show and translations into German ausstellen, French exposer, Dutch ophangen, and other languages. The target of a relation is encoded using word forms. Thus, it remains underspecified which word sense a relation is pointing to. The synonym exhibit of the eighth word sense of hang can, for example, refer to the meaning of displaying something (e.g., exhibiting a drawing) or 1 2 http://www.wiktionary.org All statistics are based on Wiktionary data of April 2011 accessed using JWKTL (Zesch et al., 2008a). 1764 Figure 1: The synonym (to) exhibit of the English Wiktionary entry (to) hang and its German translation ausstellen have multiple possible target word senses. demonstrating a skill (e.g., exhibiting a talent in acting). For humans, it is easy to recognize that hang is synonymous to the first word sense of exhibit, but not to the second. Natural language processing applications, however, cannot disambiguate such relations easily. The same applies to translations: The German ausstellen has, for instance, a meaning of (1) exhibiting an object, (2) certificating a document and (3) turning"
C12-1108,W09-2413,0,\N,Missing
C12-1108,J08-4004,0,\N,Missing
C12-1108,kunze-lemnitzer-2002-germanet,0,\N,Missing
C12-1109,W06-3814,0,0.0243746,"Missing"
C12-1109,S07-1070,0,0.0660507,"Missing"
C12-1109,S12-1059,1,0.0274292,"cal gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with additional terms. On this expanded representation, we are able to apply the well-known overlap-based methods to text similarity without any modification. Lexical expansion has already proven useful in semantic text similarity evaluations (Bär et al., 2012), which is a task related to matching sense definitions to contexts. The intuition behind our approach is depicted in Figure 1: say we wish to disambiguate the word interest in the sentence, “The loan interest is paid monthly.” The correct sense definition from our MRD (“a fixed charge for borrowing money”) has no words in common with the context, and thus would not be selected by an overlap-based WSD algorithm. But with the addition of ten lexical expansions per content word (shown in smaller text), we increase the number of overlapping word pairs (shown in boldface) to seven. Observe also th"
C12-1109,D07-1108,0,0.022479,"algorithm was found to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with"
C12-1109,de-marneffe-etal-2006-generating,0,0.00424528,"Missing"
C12-1109,J93-1003,0,0.37538,"orpus from the Leipzig Corpora Collection3 (Biemann et al., 2007) with the Stanford parser (de Marneffe et al., 2006) and used collapsed dependencies to extract features for words: each dependency triple (w1 , r, w2 ) denoting a directed dependency of type r between words w1 and w2 results in a feature (r, w2 ) characterizing w1 , and a feature (w1 , r) characterizing w2 . Words are thereby represented by the concatenation of the surface form and the POS as assigned by the parser. After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (Dunning, 1993)), rank features per word according to their significance, and prune the data, keeping only the 300 most salient features per word. The similarity of two words is given by the number of their common features (which we will shortly illustrate with an example). The pruning operation greatly reduces run time at thesaurus construction, rendering memory reduction techniques like Goyal et al. (2012) unnecessary. Despite its simplicity and the basic count of feature overlap, we found this setting to be equal to or better than more complex weighting schemes in word similarity evaluations. Across all p"
C12-1109,P05-1050,0,0.0337179,"onyms); their “extended” Lesk algorithm was found to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context"
C12-1109,D12-1100,0,0.0137786,"by represented by the concatenation of the surface form and the POS as assigned by the parser. After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (Dunning, 1993)), rank features per word according to their significance, and prune the data, keeping only the 300 most salient features per word. The similarity of two words is given by the number of their common features (which we will shortly illustrate with an example). The pruning operation greatly reduces run time at thesaurus construction, rendering memory reduction techniques like Goyal et al. (2012) unnecessary. Despite its simplicity and the basic count of feature overlap, we found this setting to be equal to or better than more complex weighting schemes in word similarity evaluations. Across all parts of speech, the DT contains five or more similar terms for a vocabulary of over 150 000 words. To illustrate the DT, Table 1 shows the top three most similar words to the noun paper, together with the features which determine the similarities. Amongst their 300 most salient features as determined by the significance measure, newspaper and paper share 45, book and paper share 33, and articl"
C12-1109,E12-1059,1,0.0654393,"tion— but they might well result in assigning incorrect senses. A straightforward improvement would alter the lexical expansion mechanism as to be sensitive to the context—something that is captured, for example, by LDA sampling (Blei et al., 2003). A further extension would be to have the number of lexical expansions depend on the DT similarity score (be it static or 1793 contextualized) instead of the fixed number we used here. In the future, we would like to examine the interplay of lexical expansion methods in WSD systems with richer knowledge resources (e.g., Navigli and Ponzetto (2010); Gurevych et al. (2012)) and apply our approach to other languages with fewer lexical resources. Also, it seems promising to apply lexical expansion techniques to text similarity, text segmentation, machine translation, and semantic indexing. Acknowledgments We thank Richard Steuer for computing and providing us access to the distributional thesaurus. This work has been supported by the Hessian research excellence program Landes-Offensive zur Entwicklung Wissenschaftlich-ökonomischer Exzellenz (LOEWE) as part of the research center Digital Humanities, and also by the Volkswagen Foundation as part of the Lichtenberg"
C12-1109,E12-1039,0,0.0310549,"ted from the gloss, synonyms, and example sentences provided by WordNet, plus the 1784 same information for all senses in a direct semantic relation. This setup specifically targets situations where such a resource serves as the sense inventory but no large sense-annotated corpus is available for supervised WSD (thus precluding use of the most frequent sense backoff). This is the case for many languages, where wordnets but not manually tagged corpora are available, and also for domain-specific WSD using the English WordNet. Whereas other approaches in this setting (Ponzetto and Navigli, 2010; Henrich et al., 2012) aim at improving WSD accuracy through the combination of several lexical resources, we restrict ourselves to WordNet and bridge the lexical gap with non-supervised, data-driven methods. How one computes the overlap between two strings was left unspecified by Lesk; we therefore adopt the simple approach of removing occurrences of the target word, treating both strings as bags of case-insensitive word tokens, and taking the cardinality of their intersection. We do not preprocess the texts by lemmatization or stop word filtering, since the terms in the distributional thesaurus are likewise unpro"
C12-1109,S01-1004,0,0.0130406,"g error that is inevitable when representing a large vocabulary with a small fixed number of dimensions or topics. On the other hand, while vector-space models do a good job at ranking candidates according to their similarity,2 they fail to efficiently generate a top-ranked list of possible expansions: due to its size, it is infeasible to rank the full vocabulary every time. Lexical expansion methods based on distributional similarity, however, generate a short list of highly similar candidates. 1 Distributional information was also used in a much older, semi-automatic approach by Tugwell and Kilgarriff (2001). In their technique, “word sketches” consisting of common patterns of usage of a word were extracted from a large POS-tagged corpus and presented to a human operator for manual sense annotation. The pattern–sense associations were then used as input to bootstrapping WSD algorithm. 2 See Rapp (2004) for an early success of vector-space models on a semantic task. 1783 The interest : loan mortgage loans debt financing mortgages credit lease bond grant funding a fixed interest solved hefty resolved monthly additional existing reduced done current substantial is charge charges counts charging cost"
C12-1109,P10-1116,0,0.026512,"d to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with additional terms."
C12-1109,P98-2127,0,0.555994,"mensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with additional terms. On this expanded representation, we are able to apply the well-known overlap-based methods to text similarity without any modification. Lexical expansion has already proven useful in semantic text similarity evaluations (Bär et al., 2012), which is a task related to matching sense definitions to contexts. The intuition behind our approach is depicted in Figure 1: say we wish to disambiguate the word interest in the sentence, “The loan interest is paid monthly.” The correct sense definition from o"
C12-1109,W03-2408,0,0.0264845,"h problem in computational linguistics. Approaches to WSD can be classified according to what lexical resources are used: knowledgebased techniques rely only on machine-readable dictionaries (MRDs), lexical semantic resources (LSRs), and untagged corpora, whereas supervised approaches instead or additionally use manually annotated training examples. Though supervised systems generally perform better, their use is restricted to scenarios where a sufficient amount of hand-crafted training data is available. Estimates for the amount of time required to produce such training data are pessimistic (Mihalcea and Chklovski, 2003); this knowledge acquisition bottleneck is the principal motivation behind research into semi-supervised and knowledge-based WSD. The latter have the advantage that, unlike manually annotated corpora, MRDs and LSRs do exist for many languages and domains. In the past, however, knowledge-based approaches have suffered from a variant of the lexical gap problem: when matching a sense description to a given context of a disambiguation target, it is often the case that the description and context do not have much vocabulary in common. We propose a new method to bridge this lexical gap which is base"
C12-1109,S07-1006,0,0.211195,"Missing"
C12-1109,P10-1023,0,0.0215006,"ith the correct sense description— but they might well result in assigning incorrect senses. A straightforward improvement would alter the lexical expansion mechanism as to be sensitive to the context—something that is captured, for example, by LDA sampling (Blei et al., 2003). A further extension would be to have the number of lexical expansions depend on the DT similarity score (be it static or 1793 contextualized) instead of the fixed number we used here. In the future, we would like to examine the interplay of lexical expansion methods in WSD systems with richer knowledge resources (e.g., Navigli and Ponzetto (2010); Gurevych et al. (2012)) and apply our approach to other languages with fewer lexical resources. Also, it seems promising to apply lexical expansion techniques to text similarity, text segmentation, machine translation, and semantic indexing. Acknowledgments We thank Richard Steuer for computing and providing us access to the distributional thesaurus. This work has been supported by the Hessian research excellence program Landes-Offensive zur Entwicklung Wissenschaftlich-ökonomischer Exzellenz (LOEWE) as part of the research center Digital Humanities, and also by the Volkswagen Foundation as"
C12-1109,S01-1005,0,0.140023,"Missing"
C12-1109,P10-1154,0,0.556073,"entences provided by some dictionaries; Kilgarriff and Rosenzweig (2000) found that including them (for simplified Lesk) led to significantly better performance than using the definitions alone. Banerjee and Pedersen (2002) observed that, where there exists a lexical resource like WordNet (Fellbaum, 1998) which also provides semantic relations between senses, 1782 these can be used to augment definitions with those from related senses (such as hypernyms and hyponyms); their “extended” Lesk algorithm was found to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), a"
C12-1109,rapp-2004-freely,0,0.0251874,"e to its size, it is infeasible to rank the full vocabulary every time. Lexical expansion methods based on distributional similarity, however, generate a short list of highly similar candidates. 1 Distributional information was also used in a much older, semi-automatic approach by Tugwell and Kilgarriff (2001). In their technique, “word sketches” consisting of common patterns of usage of a word were extracted from a large POS-tagged corpus and presented to a human operator for manual sense annotation. The pattern–sense associations were then used as input to bootstrapping WSD algorithm. 2 See Rapp (2004) for an early success of vector-space models on a semantic task. 1783 The interest : loan mortgage loans debt financing mortgages credit lease bond grant funding a fixed interest solved hefty resolved monthly additional existing reduced done current substantial is charge charges counts charging cost conviction allegation pay suspicion count part paid paying pay pays owed generated invested spent collected raised reimbursed for monthly. annual weekly yearly quarterly hefty daily regular additional substantial recent borrowing spending borrow lending borrowed debt investment raising inflows inve"
C12-1109,W04-0811,0,0.111045,"Missing"
C12-1109,S01-1037,0,0.158399,"d of sampling error that is inevitable when representing a large vocabulary with a small fixed number of dimensions or topics. On the other hand, while vector-space models do a good job at ranking candidates according to their similarity,2 they fail to efficiently generate a top-ranked list of possible expansions: due to its size, it is infeasible to rank the full vocabulary every time. Lexical expansion methods based on distributional similarity, however, generate a short list of highly similar candidates. 1 Distributional information was also used in a much older, semi-automatic approach by Tugwell and Kilgarriff (2001). In their technique, “word sketches” consisting of common patterns of usage of a word were extracted from a large POS-tagged corpus and presented to a human operator for manual sense annotation. The pattern–sense associations were then used as input to bootstrapping WSD algorithm. 2 See Rapp (2004) for an early success of vector-space models on a semantic task. 1783 The interest : loan mortgage loans debt financing mortgages credit lease bond grant funding a fixed interest solved hefty resolved monthly additional existing reduced done current substantial is charge charges counts charging cost"
C12-1109,vasilescu-etal-2004-evaluating,0,0.521787,"hich the word is found. This variant avoids the combinatorial explosion of word sense combinations the original version suffers from when trying to disambiguate multiple words in a text. Both the original and simplified versions of the Lesk algorithm suffer from low coverage due to the lexical gap problem: because the context and definitions are usually quite short, it is often the case that there are no overlapping content words at all. Various solutions to the problem have been proposed, with varying degrees of success. Lesk himself proposed increasing the size of the context window, though Vasilescu et al. (2004) found that performance was generally better for smaller contexts. Lesk also proposed augmenting the definitions with example sentences provided by some dictionaries; Kilgarriff and Rosenzweig (2000) found that including them (for simplified Lesk) led to significantly better performance than using the definitions alone. Banerjee and Pedersen (2002) observed that, where there exists a lexical resource like WordNet (Fellbaum, 1998) which also provides semantic relations between senses, 1782 these can be used to augment definitions with those from related senses (such as hypernyms and hyponyms);"
C12-1109,S07-1053,0,\N,Missing
C12-1109,C98-2122,0,\N,Missing
C12-2057,D09-1092,0,0.0260182,"Missing"
C12-2057,D10-1025,0,0.0179604,"Missing"
C14-1025,E09-1005,0,0.401245,"re two main approaches to WSA which have been applied: Similarity-based and graph-based ones. To our knowledge, there exists no previous work which effectively combines both approaches in a unified framework, and only few works which combine both kinds of features for different purposes. 2.1 Similarity-based Approaches WordNet was aligned to Wikipedia (Niemann and Gurevych, 2011) and Wiktionary (Meyer and Gurevych, 2011) using a framework based on gloss similarity, in spirit of the earliest work in WSD presented by Lesk (1986). In both cases, cosine and personalized PageRank (PPR) similarity (Agirre and Soroa, 2009) were calculated, and a simple machine learning approach was used to classify each pair of senses (see Section 3.3). This idea was also applied to cross-lingual alignment between WordNet and the German part of OmegaWiki (Gurevych et al., 2012), using machine translation as an intermediate component. Henrich et al. (2011) use a similar approach for aligning GermaNet and Wiktionary, but with word overlap as the similarity measure. De Melo and Weikum (2010) report an alignment of WordNet synsets to Wikipedia articles which is also based on word overlap. We later report results based on gloss simi"
C14-1025,C12-1011,1,0.86444,"Missing"
C14-1025,P13-1133,0,0.0228211,"of a disconnected graph), we assume infinite distance. 3.2.3 Other Features We also experimented with other features which were accessible directly from the resources, i.e. without the need for external knowledge or extensive computational effort; these were usually not available for every resource pair. Features we tried were the part of speech (Wiktionary, OmegaWiki, WordNet), the sense index, i.e. the position in the sense list for a lexeme (WordNet, Wiktionary), similarity of example sentences (WordNet, Wiktionary), overlap of translations into other languages (Wikipedia, Wiktionary, cf. (Bond and Foster, 2013)) and overlap of domain labels (Wikipedia, Wiktionary, WordNet, OmegaWiki). However, for none of these features we could observe any significant1 impact on the results, mostly due to sparsity of the respective features. Thus, we do not report them, but on the other hand we consider this an indicator that gloss similarity and distance in the resource graph already sufficiently capture the similarity between senses. 3.3 Machine Learning Classifiers We experimented with different machine learning classifiers using WEKA (Hall et al., 2009). While a detailed discussion of these classifiers is beyon"
C14-1025,E14-1008,1,0.606286,"usage challenging. These observations have led to the insight that word sense alignment (WSA), i.e. linking at the level of word senses, is key for the efficient exploitation of LSRs, and it was shown that the usage of linked resources can indeed yield performance improvements. Examples include WSD using aligned WordNet and Wikipedia (Navigli and Ponzetto, 2012a), semantic role labeling using PropBank, VerbNet and FrameNet (Palmer, 2009), the construction of a semantic parser using FrameNet, WordNet, and VerbNet (Shi and Mihalcea, 2005) and IE using WordNet and Wikipedia (Moro et al., 2013). Cholakov et al. (2014) address the special task of verb sense disambiguation. They use the large-scale resource UBY (Gurevych et al., 2012) which contains nine resources in two languages, mapped to a uniform representation using the LMF standard for interoperability (Eckle-Kohler et al., 2012), and also (among others) sense alignments between WordNet, FrameNet, VerbNet and Wiktionary which are exploited in their approach. However, WSA is challenging because of word ambiguities, different sense granularities and information types (Navigli, 2006), so that past efforts mostly focused on specific resources or applicati"
C14-1025,W02-1001,0,0.0352194,"Missing"
C14-1025,de-melo-weikum-2010-providing,0,0.0607466,"Missing"
C14-1025,eckle-kohler-etal-2012-uby,1,0.860024,"provements. Examples include WSD using aligned WordNet and Wikipedia (Navigli and Ponzetto, 2012a), semantic role labeling using PropBank, VerbNet and FrameNet (Palmer, 2009), the construction of a semantic parser using FrameNet, WordNet, and VerbNet (Shi and Mihalcea, 2005) and IE using WordNet and Wikipedia (Moro et al., 2013). Cholakov et al. (2014) address the special task of verb sense disambiguation. They use the large-scale resource UBY (Gurevych et al., 2012) which contains nine resources in two languages, mapped to a uniform representation using the LMF standard for interoperability (Eckle-Kohler et al., 2012), and also (among others) sense alignments between WordNet, FrameNet, VerbNet and Wiktionary which are exploited in their approach. However, WSA is challenging because of word ambiguities, different sense granularities and information types (Navigli, 2006), so that past efforts mostly focused on specific resources or applications, where expert-built resources such as WordNet played a central role in most cases. Approaches which aim at being more generic (i.e. applicable to a wider range of LSRs) usually focused on only one information source for the alignment (e.g. glosses or graph structures)"
C14-1025,E12-1059,1,0.86136,"evel of word senses, is key for the efficient exploitation of LSRs, and it was shown that the usage of linked resources can indeed yield performance improvements. Examples include WSD using aligned WordNet and Wikipedia (Navigli and Ponzetto, 2012a), semantic role labeling using PropBank, VerbNet and FrameNet (Palmer, 2009), the construction of a semantic parser using FrameNet, WordNet, and VerbNet (Shi and Mihalcea, 2005) and IE using WordNet and Wikipedia (Moro et al., 2013). Cholakov et al. (2014) address the special task of verb sense disambiguation. They use the large-scale resource UBY (Gurevych et al., 2012) which contains nine resources in two languages, mapped to a uniform representation using the LMF standard for interoperability (Eckle-Kohler et al., 2012), and also (among others) sense alignments between WordNet, FrameNet, VerbNet and Wiktionary which are exploited in their approach. However, WSA is challenging because of word ambiguities, different sense granularities and information types (Navigli, 2006), so that past efforts mostly focused on specific resources or applications, where expert-built resources such as WordNet played a central role in most cases. Approaches which aim at being"
C14-1025,P13-1134,1,0.848473,"’s size and quality, and how negative examples (i.e. non-alignments) can be more reliably derived. We also plan to find out if such datasets could be created for other Wiktionary language editions. The fact that the achieved results are close to the human agreement suggests that, for the datasets considered, there is not much room for improvement. Thus, we plan to apply and adapt the algorithm to LSRs with different properties than the ones considered here, such as the more syntax-focused FrameNet (Ruppenhofer et al., 2010) which only recently has received research attention in automatic WSA (Hartmann and Gurevych, 2013). The usage of syntactic features to express sense similarity has not been thoroughly explored yet, and it seems a promising direction to make further progress in WSA. Usage of more elaborate textual similarity features (e.g. covering semantic similarity or using lexical expansion) as it was suggested for text reuse detection (B¨ar et al., 2012) would be another direction worth exploring. Inspired by the semi-automatic construction of the Wiktionary-Wikipedia gold standard for English from existing datasets, we also want to investigate whether an alignment of more than two resources at once (n"
C14-1025,I08-1073,0,0.027503,"very close in the resource graph, i.e. both similarity measures fail at once, they are likely not aligned (false negatives). On the other hand, false positives occur for examples such as Brand, which is the name of districts in two different German cities (Aachen and Zwickau). The sense descriptions are very much alike, and the senses are also located in similar regions of the resource graphs (roughly speaking, German geography), which makes the distinction hard. Addressing these issues might be possible by computing more sophisticated gloss similarity measures (e.g. using lexical expansion (Iida et al., 2008)) or enhancing the graph construction process. In general, however, there are no discernible systematic errors made by our system. 5 Conclusions and future work We have shown that through joint modeling of different similarity measures for WSA the overall alignment quality in terms of F-measure can be significantly improved over the state of the art for each and every of the considered four datasets. This proves that such a joint usage of global structure as well as the content of the LSRs is indeed preferable over using either of them in isolation or combining them in a simple backoff approac"
C14-1025,Q13-1013,1,0.184874,"from Wiktionary in a novel approach, making it the first crowdsourced WSA dataset. ii) Also for the first time, we jointly model different aspects of sense similarity by applying machine learning techniques to WSA. However, unlike previous approaches, we do not engineer our features towards a specific resource pair, rendering the approach powerful but proprietary. Instead, we aim to combine generic features which are applicable to a variety of resources, and we show that combining them leads to state-of-the-art WSA performance. In particular, we employ distances calculated with Dijkstra-WSA (Matuschek and Gurevych, 2013), an algorithm which works on graph representations of resources, as well as gloss similarity values. This lets us take advantage of both (orthogonal) ways of identifying equivalent senses and yields a very robust and flexible WSA framework. The rest of this paper is structured as follows: In Section 2 we discuss related work, in Section 3 we describe our approach and introduce the resources and datasets we use in our experiments, in Section 4 we evaluate our results, and we conclude in Section 5 with some directions for future work. 2 Related Work There are two main approaches to WSA which ha"
C14-1025,I11-1099,1,0.905636,"roach and introduce the resources and datasets we use in our experiments, in Section 4 we evaluate our results, and we conclude in Section 5 with some directions for future work. 2 Related Work There are two main approaches to WSA which have been applied: Similarity-based and graph-based ones. To our knowledge, there exists no previous work which effectively combines both approaches in a unified framework, and only few works which combine both kinds of features for different purposes. 2.1 Similarity-based Approaches WordNet was aligned to Wikipedia (Niemann and Gurevych, 2011) and Wiktionary (Meyer and Gurevych, 2011) using a framework based on gloss similarity, in spirit of the earliest work in WSD presented by Lesk (1986). In both cases, cosine and personalized PageRank (PPR) similarity (Agirre and Soroa, 2009) were calculated, and a simple machine learning approach was used to classify each pair of senses (see Section 3.3). This idea was also applied to cross-lingual alignment between WordNet and the German part of OmegaWiki (Gurevych et al., 2012), using machine translation as an intermediate component. Henrich et al. (2011) use a similar approach for aligning GermaNet and Wiktionary, but with word ove"
C14-1025,P06-1014,0,0.126167,", 2005) and IE using WordNet and Wikipedia (Moro et al., 2013). Cholakov et al. (2014) address the special task of verb sense disambiguation. They use the large-scale resource UBY (Gurevych et al., 2012) which contains nine resources in two languages, mapped to a uniform representation using the LMF standard for interoperability (Eckle-Kohler et al., 2012), and also (among others) sense alignments between WordNet, FrameNet, VerbNet and Wiktionary which are exploited in their approach. However, WSA is challenging because of word ambiguities, different sense granularities and information types (Navigli, 2006), so that past efforts mostly focused on specific resources or applications, where expert-built resources such as WordNet played a central role in most cases. Approaches which aim at being more generic (i.e. applicable to a wider range of LSRs) usually focused on only one information source for the alignment (e.g. glosses or graph structures) without combining them in an elaborate way. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/"
C14-1025,E09-1068,0,0.0274203,"f OmegaWiki (Gurevych et al., 2012), using machine translation as an intermediate component. Henrich et al. (2011) use a similar approach for aligning GermaNet and Wiktionary, but with word overlap as the similarity measure. De Melo and Weikum (2010) report an alignment of WordNet synsets to Wikipedia articles which is also based on word overlap. We later report results based on gloss similarity as one of our baselines (Tables 2 and 3). 2.2 Graph-based Approaches In one of the earliest structure-based works, Daud´e et al. (2003) map different versions of WordNet based on the synset hierarchy. Navigli (2009) disambiguates WordNet glosses, i.e. sense markers are assigned to all non-stopwords in each WordNet gloss. The approach is based on finding circles in the WordNet relation graph to identify disambiguations. In later work, this idea was applied to the disambiguation of translations in a bilingual dictionary (Flati and Navigli, 2012). While this “alignment” of dictionary entries is related to our problem, it was not discussed how this idea could be applied to word sense alignment of two resources. Laparra et al. (2010) use a shortest path algorithm (SSI-Dijkstra+) to align FrameNet lexical unit"
C14-1025,W11-0122,1,0.871615,"lated work, in Section 3 we describe our approach and introduce the resources and datasets we use in our experiments, in Section 4 we evaluate our results, and we conclude in Section 5 with some directions for future work. 2 Related Work There are two main approaches to WSA which have been applied: Similarity-based and graph-based ones. To our knowledge, there exists no previous work which effectively combines both approaches in a unified framework, and only few works which combine both kinds of features for different purposes. 2.1 Similarity-based Approaches WordNet was aligned to Wikipedia (Niemann and Gurevych, 2011) and Wiktionary (Meyer and Gurevych, 2011) using a framework based on gloss similarity, in spirit of the earliest work in WSD presented by Lesk (1986). In both cases, cosine and personalized PageRank (PPR) similarity (Agirre and Soroa, 2009) were calculated, and a simple machine learning approach was used to classify each pair of senses (see Section 3.3). This idea was also applied to cross-lingual alignment between WordNet and the German part of OmegaWiki (Gurevych et al., 2012), using machine translation as an intermediate component. Henrich et al. (2011) use a similar approach for aligning"
C14-1142,J08-4004,0,0.108987,"ent component as a statement covering an entire sentence or less. We consolidate the annotations of all annotators before continuing with the next step (section 5.4). 3. Annotation of argumentative relations: Finally, the claims and premises are linked within each paragraph, and the claims are linked to the major claim either with a support or attack relation. 6 Although the coefficient was introduced by Fleiss as a generalization of Cohen’s κ (Cohen, 1960), it is actually a generalization of Scott’s π (Scott, 1955), since it assumes a cumulative distribution of annotations by all annotators (Artstein and Poesio, 2008). We follow the naming proposed by Artstein and Poesio and refer to the measure as multi-π. 1505 5 Annotation Study Three annotators participate in the study and annotate the essays independently using our described annotation scheme. We conduct several training sessions after each annotator has read the annotation guidelines. In these sessions, annotators collaboratively annotate 8 example essays for resolving disagreements and obtaining a common understanding of the annotation guidelines. For the actual annotation task, we used the brat annotation tool that is freely available.7 It allows th"
C14-1142,W13-0902,0,0.0271196,"s which they call shell expressions. They annotate 200 essays and estimate an inter-rater agreement of κ = 0.699 and F1 = 0.726 on a subset of 50 essays annotated by two annotators. However, their annotation scheme is limited to shell expressions and compared to our work it does not include argument components or argumentative relations. Additional annotation studies on persuasive essays focus on identifying style criteria (Burstein and Wolska, 2003), factual information (Beigman Klebanov and Higgins, 2012), holistic scores for argumentation quality (Attali et al., 2013) or metaphors (Beigman Klebanov and Flor, 2013). We are not aware of an annotation study including argument components and argumentative relations in persuasive essays. 3 Annotation Scheme The goal of our proposed annotation scheme is to model argument components as well as argumentative relations that constitute the argumentative discourse structure in persuasive essays. We propose an annotation scheme including three argument components and two argumentative relations (figure 1). Figure 1: Argument annotation scheme including argument components and argumentative relations indicated by arrows below the components. 3.1 Argument Components"
C14-1142,W12-2007,0,0.030102,"persuasive discourse. Further, they refer to organizational elements as claim and premise indicating word sequences which they call shell expressions. They annotate 200 essays and estimate an inter-rater agreement of κ = 0.699 and F1 = 0.726 on a subset of 50 essays annotated by two annotators. However, their annotation scheme is limited to shell expressions and compared to our work it does not include argument components or argumentative relations. Additional annotation studies on persuasive essays focus on identifying style criteria (Burstein and Wolska, 2003), factual information (Beigman Klebanov and Higgins, 2012), holistic scores for argumentation quality (Attali et al., 2013) or metaphors (Beigman Klebanov and Flor, 2013). We are not aware of an annotation study including argument components and argumentative relations in persuasive essays. 3 Annotation Scheme The goal of our proposed annotation scheme is to model argument components as well as argumentative relations that constitute the argumentative discourse structure in persuasive essays. We propose an annotation scheme including three argument components and two argumentative relations (figure 1). Figure 1: Argument annotation scheme including a"
C14-1142,W98-0303,0,0.24907,"ports or attacks a claim enable the collection of additional evidence from other resources to recommend argument improvement. In addition, we provide a detailed analysis of the inter-rater agreement and an analysis of disagreements. 1 http://www.ukp.tu-darmstadt.de/data/argumentation-mining 1502 2.2 Persuasive Essays Persuasive essays are extensively studied in the context of automated essay grading (Shermis and Burstein, 2013), which aims at automatically assigning a grade to a student’s essay by means of several features. Since the argument structure is crucial for evaluating essay quality, Burstein et al. (1998) propose an approach for identifying the argumentative discourse structure by means of discourse marking. They utilize a surface cue word and phrase lexicon to identify the boundaries of arguments at the sentence level in order to evaluate the content of individual arguments and to enrich their feature set for determining precise grades. Although the identification of argument boundaries is important for argument recognition, our work allows a more fine-grained analysis of arguments since it also includes argument components and argumentative relations. Madnani et al. (2012) studied persuasive"
C14-1142,J96-2004,0,0.521668,"Missing"
C14-1142,E12-1085,0,0.0450212,"Missing"
C14-1142,P11-1099,0,0.310064,"the student’s writing skills. An argument consists of several components (i.e. claims and premises) and exhibits a certain structure constituted by argumentative relations between components (Peldszus and Stede, 2013). Hence, recognizing arguments in textual documents includes several subtasks: (1) separating argumentative from non-argumentative text units, (2) identifying claims and premises, and (3) identifying relations between argument components. There exist a great demand for reliably annotated corpora including argument components as well as argumentative relations (Reed et al., 2008; Feng and Hirst, 2011) since they are required for supervised machine learning approaches for extracting arguments. Previous argument annotated corpora are limited to specific domains including legal documents (Mochales-Palau and Moens, 2008), newspapers and court cases (Reed et al., 2008), product reviews (Villalba and Saint-Dizier, 2012) and online debates (Cabrio and Villata, 2012). To the best of our knowledge, no work has been carried out to annotate argument components and argumentative relations in persuasive essays (section 2). In addition, the reliability of the corpora is unknown, since only few of these"
C14-1142,N12-1003,0,0.0442732,"ng essay quality, Burstein et al. (1998) propose an approach for identifying the argumentative discourse structure by means of discourse marking. They utilize a surface cue word and phrase lexicon to identify the boundaries of arguments at the sentence level in order to evaluate the content of individual arguments and to enrich their feature set for determining precise grades. Although the identification of argument boundaries is important for argument recognition, our work allows a more fine-grained analysis of arguments since it also includes argument components and argumentative relations. Madnani et al. (2012) studied persuasive essays for separating organizational elements from content. They argue that the detection of organizational elements is a step towards argument recognition and inferring the structure of persuasive discourse. Further, they refer to organizational elements as claim and premise indicating word sequences which they call shell expressions. They annotate 200 essays and estimate an inter-rater agreement of κ = 0.699 and F1 = 0.726 on a subset of 50 essays annotated by two annotators. However, their annotation scheme is limited to shell expressions and compared to our work it does"
C14-1142,reed-etal-2008-language,0,0.0518501,"quality as well as the student’s writing skills. An argument consists of several components (i.e. claims and premises) and exhibits a certain structure constituted by argumentative relations between components (Peldszus and Stede, 2013). Hence, recognizing arguments in textual documents includes several subtasks: (1) separating argumentative from non-argumentative text units, (2) identifying claims and premises, and (3) identifying relations between argument components. There exist a great demand for reliably annotated corpora including argument components as well as argumentative relations (Reed et al., 2008; Feng and Hirst, 2011) since they are required for supervised machine learning approaches for extracting arguments. Previous argument annotated corpora are limited to specific domains including legal documents (Mochales-Palau and Moens, 2008), newspapers and court cases (Reed et al., 2008), product reviews (Villalba and Saint-Dizier, 2012) and online debates (Cabrio and Villata, 2012). To the best of our knowledge, no work has been carried out to annotate argument components and argumentative relations in persuasive essays (section 2). In addition, the reliability of the corpora is unknown, s"
C14-2023,J08-4004,0,0.150504,"Missing"
C14-2023,J96-2004,0,0.348403,"Missing"
C14-2023,P12-3015,1,0.879236,"Missing"
C14-2023,passonneau-2006-measuring,0,0.0142748,"Missing"
C14-2023,P13-4001,1,0.500567,"Missing"
C16-1009,S12-1051,0,0.714684,"tional NLP task of determining the degree of semantic similarity between two texts. Most STS systems compute the similarity score between two texts on a fixed scale, for example a scale between 0 and 5, with 0 indicating the semantics are completely independent and 5 indicating semantic equivalence. In recent years, the number and quality of systems that rate the STS between texts have increased, as has the number of tasks where such systems are used. Textual similarity is an active research field and was part of several shared tasks. In 2012, the pilot Semantic Textual Similarity (STS) Task (Agirre et al., 2012) was established at the Semantic Evaluation (SemEval) workshop. Further shared tasks on text similarity were part of SemEval 2013 (Agirre et al., 2013), SemEval 2014 (Agirre et al., 2014), SemEval 2015 (Agirre et al., 2015), and SemEval 2016 (Agirre et al., 2016). For the latest shared task on semantic textual similarity at SemEval 2016, 43 teams were submitting 119 different systems, depicting the large interest in this field. STS is a foundational NLP technique, however, STS systems are seldom used for the sole purpose of measuring the similarity of two texts. Often they are used in a larger"
C16-1009,S13-1004,0,0.448708,"a fixed scale, for example a scale between 0 and 5, with 0 indicating the semantics are completely independent and 5 indicating semantic equivalence. In recent years, the number and quality of systems that rate the STS between texts have increased, as has the number of tasks where such systems are used. Textual similarity is an active research field and was part of several shared tasks. In 2012, the pilot Semantic Textual Similarity (STS) Task (Agirre et al., 2012) was established at the Semantic Evaluation (SemEval) workshop. Further shared tasks on text similarity were part of SemEval 2013 (Agirre et al., 2013), SemEval 2014 (Agirre et al., 2014), SemEval 2015 (Agirre et al., 2015), and SemEval 2016 (Agirre et al., 2016). For the latest shared task on semantic textual similarity at SemEval 2016, 43 teams were submitting 119 different systems, depicting the large interest in this field. STS is a foundational NLP technique, however, STS systems are seldom used for the sole purpose of measuring the similarity of two texts. Often they are used in a larger context. Examples for such tasks can be found in the field of Automatic Essay Grading (Attali et al., 2006), Plagiarism Detection (Potthast et al., 20"
C16-1009,S15-2045,0,0.0226875,"ingle evaluation measure consistently produced the best predictions. The requirements on the STS systems for different tasks are too distinct, that a single evaluation measure could cope with all those. We thus claim that understanding the properties of the task and mapping them to the desired properties of the evaluation measure is crucial when selecting a measure for an intrinsic evaluation. Therefore, we propose in section 4 a new framework on the intrinsic evaluation of STS systems by taking the requirements of the target task into account. This publication is based on the thesis of Beyer (2015). Some details in this paper are ommited for brevity and can be found online.1 2 Limitations of the Pearson Correlation and Alternative Evaluation Measures Figure 1 depicts the output of four hypothetical STS systems in comparison to the gold standard derived from human judgment. These four distributions, also known as Anscombe’s quartet, all have the same Pearson correlation coefficient of 0.816. By comparing only the Pearson correlation, all systems would be judged as equally good. Pearson correlation is especially sensitive to non-linear relations, for example as depicted in the upperright"
C16-1009,W97-0703,0,0.0416733,"), SemEval 2015 (Agirre et al., 2015), and SemEval 2016 (Agirre et al., 2016). For the latest shared task on semantic textual similarity at SemEval 2016, 43 teams were submitting 119 different systems, depicting the large interest in this field. STS is a foundational NLP technique, however, STS systems are seldom used for the sole purpose of measuring the similarity of two texts. Often they are used in a larger context. Examples for such tasks can be found in the field of Automatic Essay Grading (Attali et al., 2006), Plagiarism Detection (Potthast et al., 2012), Automated Text Summarization (Barzilay and Elhadad, 1997), Question Answering (Lin and Pantel, 2001), or Link Discovery (He, 2009). In this paper we call a task that heavily depends on the output of an STS system an STS based task. These tasks are often strongly dependent on the quality of the STS system they use, but they might apply further steps as well. Given this large number of different STS systems, it is hard for an NLP system designer to decide which STS system should be implemented and used for a specific task. As such tasks often strongly depend on the quality of the STS system, the NLP system designer likes to use the most suitable syste"
C16-1009,S14-2010,0,\N,Missing
C16-1099,J08-4004,0,0.189848,"the Nugget Identification Step To ensure the reproducibility of our corpus creation procedure, we first assess how well the annotators agree on the results of the nugget identification step. The agreement scores reveal to what extent the annotation experiment can be repeated with different annotators or data. Furthermore, they yield insight into the degree of subjectivity of the task. Besides the average proportion of observed agreement (AO ) of nuggets jointly identified by two annotators, we use the standard metrics from content analysis, namely Fleiss’ κ and Krippendorff’s α, as defined by Artstein and Poesio (2008). We first model the nugget identification step as a coding task (i.e., assigning categories to predefined units) by aggregating each annotator’s nuggets to the levels of sentences and paragraphs. That is, a sentence or paragraph is considered important, if it contains at least one nugget. This setup allows us to compute the usual AO , κ and α agreement scores. Although κ has been used in some previous works, it is not well-suited for this setup, as it has no explicit notion of missing values. Rather, it assumes that all annotators processed the entire corpus, which is neither true for our wor"
C16-1099,W00-0405,0,0.709235,"ed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 http://duc.nist.gov 2 http://www.nist.gov/tac 3 http://multiling.iit.demokritos.gr 1039 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1039–1050, Osaka, Japan, December 11-17 2016. Corpus Summary type Genre Lang. Topics Doc/Topic DUC (2001–2003) DUC (2004) DUC (2005–2007) TAC (2008–2011) TAC (2014) MultiLing (2011; 2013) MultiLing (2015) Loupy et al. (2010) Hirao et al. (2004) Ulrich et al. (2008) Goldstein et al. (2000a) Zechner (2002) Carenini et al. (2007) Nakano et al. (2010) Lloret et al. (2013) Our work Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts & Extracts Extracts Extracts Extracts Extracts Extracts Coherent Extracts News News News News, blogs Scientific News News News News e-Mails News Trans. speech e-Mails Heterogeneous Heterogeneous Heterogeneous en en, ar en en en 7 10 fr ja en en en en en en de 30–60 50 50 44 50 10 15 20 N/A 30 25 23 20 24 310 10 10 10 ≥ 25 10 10 10 10 20 N/A ≈ 11 10 N/A ≥4 352 10 4–14 Summary size 50–400 words 10–100 words"
C16-1099,L16-1146,1,0.822756,"in each topic, the source documents are highly heterogeneous, covering brief introductory texts, syllabi, term definitions, presentations of interest groups, and many other text genres. Once we identify the source documents, we shorten those with more than 40 pages and topics with more than 30,000 tokens in order to reduce the annotation effort. We do not remove documents in order to conserve the full diversity of subtopics and genres. Conversely, we define a minimum of 3 documents and 1,500 tokens to ensure that there is enough information to summarize. We use the boilerplate removal tool by Habernal et al. (2016) to remove HTML markup and non-content (e.g., advertisement, navigation menu). 3.2 Creating Coherent Extracts We divide the task of creating coherent extracts into three subsequent steps, which in turn consist of further substeps which we explain in an extensive, publicly available annotation guidebook. These detailed instructions ensure a reliable, reproducible annotation workflow and allow us to produce informative, non-redundant, grammatically correct coherent extracts. Step 1: Identification of nuggets We first instruct the annotators to read each text before annotating, to ensure that the"
C16-1099,W03-0508,0,0.13857,"Missing"
C16-1099,C04-1077,0,0.681006,"largely unconsidered. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 http://duc.nist.gov 2 http://www.nist.gov/tac 3 http://multiling.iit.demokritos.gr 1039 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1039–1050, Osaka, Japan, December 11-17 2016. Corpus Summary type Genre Lang. Topics Doc/Topic DUC (2001–2003) DUC (2004) DUC (2005–2007) TAC (2008–2011) TAC (2014) MultiLing (2011; 2013) MultiLing (2015) Loupy et al. (2010) Hirao et al. (2004) Ulrich et al. (2008) Goldstein et al. (2000a) Zechner (2002) Carenini et al. (2007) Nakano et al. (2010) Lloret et al. (2013) Our work Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts & Extracts Extracts Extracts Extracts Extracts Extracts Coherent Extracts News News News News, blogs Scientific News News News News e-Mails News Trans. speech e-Mails Heterogeneous Heterogeneous Heterogeneous en en, ar en en en 7 10 fr ja en en en en en en de 30–60 50 50 44 50 10 15 20 N/A 30 25 23 20 24 310 10 10 10 ≥ 25 10 10 10 10 20 N/A ≈ 11 10 N/A ≥4 352 10"
C16-1099,W04-1013,0,0.0961277,"of occurrence. Lloret et al. (2013) uses the overall quality for evaluation, based on a five-point Likert-type scale. Later on, Nenkova et al. (2007) introduced the Pyramid method. So-called Summary Content Units (SCUs), which are semantically motivated, subsentential units, serve as the basis for the evaluation. They are variable in length, but not bigger than a sentential clause (Nenkova et al., 2007), and are closely related to model units. In both cases, system summaries are evaluated based on the content overlap rankings. A widely used quantitative, automatic evaluation metric is ROUGE (Lin, 2004), which has also been used to evaluate manual summaries, for instance, by Nakano et al. (2010), Lloret et al. (2013), and in DUC evaluations (up to 2007). In our work, we focus on manual qualitative evaluation using various aspects of text quality, similar to 1041 Figure 1: Process of creation and evaluation of coherent extracts and the intermediate processing steps. the evaluation approach taken in the early DUC evaluations (Over, 2001; Over and Liggett, 2002; Over and Yen, 2003). More specifically, we focus on coherence in summary evaluation and statistically prove that this feature, which i"
C16-1099,C14-2023,1,0.89956,"Missing"
C16-1099,P16-4017,1,0.921219,"ning paper. First, we introduce the Input in form of the source document collection and topic selection procedure (Section 3.1). Our MDS process consists of three steps: In the first step the most important units are selected (so-called Identification of Nuggets). Next, the selected nuggets are clustered into groups with similar content and for each cluster the best nugget is selected (Selection of best nuggets). Finally, during the Formulation of Summaries, the best nuggets are co-reference resolved, grammaticalized and sorted coherently. Details for each step are given in Section 3.2 and in Meyer et al. (2016). In Section 4, we describe the Evaluation of our work using inter-annotator agreement measures, automatic summarization scoring with ROUGE, and human judgments based on Likert scales. 3.1 Heterogeneous Document Collection Deutscher Bildungsserver5 (DBS) is a large web portal that collects links to educational resources in German. As a publicly funded project, it fulfills an important information need of different stakeholders, including teachers, students, parents, politicians, and educational researchers. DBS contains links to highly heterogeneous text types, such as interviews, book reviews"
C16-1099,nakano-etal-2010-construction,0,0.152572,"se. License details: http://creativecommons.org/licenses/by/4.0/ 1 http://duc.nist.gov 2 http://www.nist.gov/tac 3 http://multiling.iit.demokritos.gr 1039 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1039–1050, Osaka, Japan, December 11-17 2016. Corpus Summary type Genre Lang. Topics Doc/Topic DUC (2001–2003) DUC (2004) DUC (2005–2007) TAC (2008–2011) TAC (2014) MultiLing (2011; 2013) MultiLing (2015) Loupy et al. (2010) Hirao et al. (2004) Ulrich et al. (2008) Goldstein et al. (2000a) Zechner (2002) Carenini et al. (2007) Nakano et al. (2010) Lloret et al. (2013) Our work Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts Abstracts & Extracts Extracts Extracts Extracts Extracts Extracts Coherent Extracts News News News News, blogs Scientific News News News News e-Mails News Trans. speech e-Mails Heterogeneous Heterogeneous Heterogeneous en en, ar en en en 7 10 fr ja en en en en en en de 30–60 50 50 44 50 10 15 20 N/A 30 25 23 20 24 310 10 10 10 ≥ 25 10 10 10 10 20 N/A ≈ 11 10 N/A ≥4 352 10 4–14 Summary size 50–400 words 10–100 words 250 words 100 words 250 words 240–250 words 240–250 words ≈"
C16-1099,W04-3252,0,\N,Missing
C16-1099,J02-4003,0,\N,Missing
C16-1099,J13-3001,0,\N,Missing
C16-1160,E14-1060,0,0.0742904,"prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over different alphabets. Lemmatization is the task of deriving the lemma from an inflected word form such as atmest→atmen. The problem is relatively simple for morphologically poor languages like English, but much harder for languages like Finnish. The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag. 2 Our implementation of PCRF-Seq2Seq is available at: https://github.com/UKPLab/coling2016-pcrf-seq2seq 1704 3 Data Here we detail the data sets used in our experiments; examples are provided in Table 1. These datasets reflect the different Seq2Seq tasks we aim to investigate. The Text+Berg corpus (Bubenhofer et al., 2015) contains historic proceedings of the Schweizer Alpenclub (“Swiss Alpine Club”) from the years 1864–1899 in Swiss German and French. The data has been digi"
C16-1160,P00-1037,0,0.44826,"report on “par or better” performance of their inflection generation neural architecture. However, a closer inspection of their results suggests that their system is sometimes worse and sometimes better than traditional approaches. Here, we aim for a more balanced comparison on three exemplary monotone1 Seq2Seq tasks, namely spelling correction, G2P conversion, and lemmatization. Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field. Their simplicity vis-`a-vis non-monotonic problems such as machine translation renders them as particularly tractable testbeds of technological progress. Unlike previous work, which has typically focussed on only one specific subproblem of monotone Seq2Seq tasks at a time, we consider model performances on three such tasks simultaneously. This leads to a more balanced view on the relative performance of different models. We compare three variants of encoder-decoder models — includin"
C16-1160,P05-1022,0,0.062135,"orkhorse for many sequence labeling tasks such as part-of-speech tagging and named entity recognition during the 2000s. Unfortunately, training and decoding time depend polynomially on the tag set size and exponentially on the order of the CRF. Here, order refers to the dependencies on the label side. This makes higher-order CRFs impractical for large training data sizes, which is the reason why virtually only first-order (linear chain) CRFs were used until recently. M¨uller et al. (2013) introduced pruned CRFs (PCRFs) that approximate the CRF objective function using coarse-to-fine decoding (Charniak and Johnson, 2005). PCRFs require much shorter runtime and are thus able to make use of higher orders. Higher orders, in turn, have been shown to be highly beneficial for coarse and fine-grained part-of-speech tagging, outperforming first-order models. For our tasks, we have adapted the implementation from M¨uller et al. (2013) — originally designed for sequence labeling — to general monotone Seq2Seq tasks. Sequence labeling assumes that an input sequence of length N is mapped to an output sequence of identical length N , while in Seq2Seq tasks, input string lengths may be shorter, longer, or equal to output st"
C16-1160,P14-2111,0,0.0282944,"ling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over different alphabets. Lemmatization is the task of deriving the lemma from an inflected word form such as atmest→atmen. The problem is relatively simple for morphologically poor lan"
C16-1160,J81-4005,0,0.748202,"Missing"
C16-1160,W04-3238,0,0.0480566,"ng paper and scan quality, typeface, and OCR engine, OCR error rate can be extraordinarily high (Reynaert, 2014). OCR post-correction is of particular practical importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and “unusual” typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech"
C16-1160,N13-1138,0,0.161001,"representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over different alphabets. Lemmatization is the task of deriving the lemma from an inflected word form such as atmest→atmen. The problem is relatively simple for morphologically poor languages like English, but much harder for languages like Finnish. The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag. 2 Our implementation of PCRF-Seq2Seq is available at: https://github.com/UKPLab/coling2016-pcrf-seq2seq 1704 3 Data Here we detail the data sets used in our experiments; examples are provided in Table 1. These datasets reflect the different Seq2Seq tasks we aim to investigate. The Text+Berg corpus (Bubenhofer et al., 2015) contains historic proceedings of the Schweizer Alpenclub (“Swiss Alpine Club”) from the years 1864–1899 in Swiss German and French."
C16-1160,P14-2027,0,0.0261239,"al importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and “unusual” typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over"
C16-1160,N16-1077,0,0.134148,"correction (Schmaltz et al., 2016; Xie et al., 2016). We have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer. Similarly, Faruqui et al. (2016) report on “par or better” performance of their inflection generation neural architecture. However, a closer inspection of their results suggests that their system is sometimes worse and sometimes better than traditional approaches. Here, we aim for a more balanced comparison on three exemplary monotone1 Seq2Seq tasks, namely spelling correction, G2P conversion, and lemmatization. Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction ("
C16-1160,P16-1154,0,0.0423358,"he decoder (top) generates the output sequence ~y . The attention-based mechanism (shown here) enables the decoder to “peek” into the input at every decoding step through multiple input representations at . Illustration from Bahdanau et al. (2014). • morph-trans: Faruqui et al. (2016) present a new encoder-decoder model designed for morphological inflection, proposing to feed the input sequence directly into the decoder. This approach is motivated by the observation that input and output are usually very similar in problems such as morphological inflection. Similar ideas have been proposed in Gu et al. (2016) in their so-called “CopyNet” encoder-decoder model (which they apply to text summarization) that allows for portions of the input sequence to be simply copied to the output sequence, without modifications. A priori, this observation seems to apply to our tasks too: at least in spelling correction, the output usually differs only marginally from the input. For the tested neural models, we follow the same overall approach as Faruqui et al. (2016): we perform decoding and evaluation of the test data using an ensemble of k = 5 independently trained models in order to deal with the non-convex natu"
C16-1160,P14-2028,0,0.0142773,"typeface, and OCR engine, OCR error rate can be extraordinarily high (Reynaert, 2014). OCR post-correction is of particular practical importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and “unusual” typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related"
C16-1160,N10-1103,0,0.360765,"ion 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 We call our tasks, described below, monotone because relationships between input and output sequence characters typically obey monotonicity. That is, unlike in machine translation, there are no ‘crossing edges’ in corresponding alignments. 1703 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1703–1714, Osaka, Japan, December 11-17 2016. well-established baselines for monotone Seq2Seq, namely Sequitur (Bisani and Ney, 2008), DirecTL+ (Jiampojamarn et al., 2010), and Phonetisaurus (Novak et al., 2012). We also offer our own contribution2 , which may be considered a variation of the principles underlying DirecTL+. For that purpose, we have adapted higher-order pruned conditional random fields (PCRFs) (M¨uller et al., 2013; Lafferty et al., 2001) to handle generic monotone Seq2Seq tasks. We find that traditional models appear to still be on par with or better than encoder-decoder models in most cases, depending on factors such as training data size and the complexity of the task at hand. We show that neural models unfold their strengths as soon as more"
C16-1160,D15-1166,0,0.0610405,"language processing (NLP) ever since the origins of the field. Their simplicity vis-`a-vis non-monotonic problems such as machine translation renders them as particularly tractable testbeds of technological progress. Unlike previous work, which has typically focussed on only one specific subproblem of monotone Seq2Seq tasks at a time, we consider model performances on three such tasks simultaneously. This leads to a more balanced view on the relative performance of different models. We compare three variants of encoder-decoder models — including attention-based models (Bahdanau et al., 2014; Luong et al., 2015) and the model proposed by Faruqui et al. (2016) — to three very This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 We call our tasks, described below, monotone because relationships between input and output sequence characters typically obey monotonicity. That is, unlike in machine translation, there are no ‘crossing edges’ in corresponding alignments. 1703 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1703–1714, Osaka, Japan,"
C16-1160,D13-1032,0,0.0649511,"Missing"
C16-1160,N15-1093,0,0.247831,"notone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over different alphabets. Lemmatization is the task of deriving the lemma from an inflected word form such as atmest→atmen. The problem is relatively simple for morphologically poor languages like English, but much harder for languages like Finnish. The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag. 2 Our implementation of PCRF-Seq2Seq is available at: https://github.com/UKPLab/coling2016-pcrf-seq2seq 1704 3 Data Here we detail the data sets used in our experiments; examples are provided in Table 1. These datasets reflect the different Seq2Seq tasks we aim to investigate. The Text+Berg corpus (Bubenhofer et al., 2015) contains historic proceedings of the Schweizer Alpenclub (“Swiss Alpine Club”) from the years 1864–1899 in Swiss German and French. The data has been digitized and OCR errors h"
C16-1160,W12-6208,0,0.321437,"http://creativecommons.org/licenses/by/4.0/ 1 We call our tasks, described below, monotone because relationships between input and output sequence characters typically obey monotonicity. That is, unlike in machine translation, there are no ‘crossing edges’ in corresponding alignments. 1703 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1703–1714, Osaka, Japan, December 11-17 2016. well-established baselines for monotone Seq2Seq, namely Sequitur (Bisani and Ney, 2008), DirecTL+ (Jiampojamarn et al., 2010), and Phonetisaurus (Novak et al., 2012). We also offer our own contribution2 , which may be considered a variation of the principles underlying DirecTL+. For that purpose, we have adapted higher-order pruned conditional random fields (PCRFs) (M¨uller et al., 2013; Lafferty et al., 2001) to handle generic monotone Seq2Seq tasks. We find that traditional models appear to still be on par with or better than encoder-decoder models in most cases, depending on factors such as training data size and the complexity of the task at hand. We show that neural models unfold their strengths as soon as more complex phenomena need to be learned. T"
C16-1160,D08-1047,0,0.0170633,"of particular practical importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and “unusual” typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output stri"
C16-1160,W16-0528,0,0.0260809,"s, including our own adaptation of pruned CRFs. 1 Introduction Encoder-decoder neural models (Sutskever et al., 2014) are a generic deep-learning approach to sequence-to-sequence translation (Seq2Seq) tasks. They encode an input sequence into a vector representation from which the decoder generates an output. These models have shown to achieve state-ofthe-art or at least highly competitive results for various NLP tasks including machine translation (Cho et al., 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al., 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016). We have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer. Similarly, Faruqui et al. (2016) report on “"
C16-1160,P07-1119,0,0.0392891,"e state transducer. Similarly, Faruqui et al. (2016) report on “par or better” performance of their inflection generation neural architecture. However, a closer inspection of their results suggests that their system is sometimes worse and sometimes better than traditional approaches. Here, we aim for a more balanced comparison on three exemplary monotone1 Seq2Seq tasks, namely spelling correction, G2P conversion, and lemmatization. Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field. Their simplicity vis-`a-vis non-monotonic problems such as machine translation renders them as particularly tractable testbeds of technological progress. Unlike previous work, which has typically focussed on only one specific subproblem of monotone Seq2Seq tasks at a time, we consider model performances on three such tasks simultaneously. This leads to a more balanced view on the relative performance of different models. We compare th"
C16-1160,P02-1019,0,0.0682067,"on various factors including paper and scan quality, typeface, and OCR engine, OCR error rate can be extraordinarily high (Reynaert, 2014). OCR post-correction is of particular practical importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and “unusual” typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for s"
C16-1160,W16-0103,0,0.0604195,"Missing"
C16-1160,P98-2246,0,\N,Missing
C16-1160,C98-2241,0,\N,Missing
C16-1195,J08-4004,0,0.0174246,"Missing"
C16-1195,banea-etal-2008-bootstrapping,0,0.0274325,"because they are specific for a certain language pair and do not make any assumptions on the (negative) connotations of a trade name. Kondrak and Dorr (2004) adapt orthographic and phonetic similarity methods to find confusable drug names. Unlike our tool which retrieves pragmatically marked sense descriptions, their work is limited to identifying similar forms. The recognition of words and phrases associated with opinions and emotions is the goal of sentiment analysis. There have been multiple attempts to construct large sentiment lexicons, such as SentiWordNet (Esuli and Sebastiani, 2006). Banea et al. (2008) propose a bootstrapping approach to induce such lexicons from a small, manually defined seed list. Our approach is similar, since we propagate pragmatic information based on lexical relations. However, we do not require a seed list and we consider relations across many languages. While monolingual resources are not suitable for our task at all, the existing multilingual sentiment lexicons are severely limited in size. The NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2013) is among the largest and available in about 40 languages, but since it has been automatically translated, it"
C16-1195,I13-1112,1,0.839534,"012) describe an automatic approach to generate neologisms that can be used as product names or slogans. Both works are focused on the English language and on the identification or generation of good names, whereas our work aims at the detection of problematic names without focusing on a particular language or target market. Our task is similar to automatically distinguishing cognates and false friends. Inkpen et al. (2005) use orthographic similarity metrics for this task including Soundex, which we also propose for our method. Follow-up works by Mitkov et al. (2007), Gomes and Lopes (2011), Beinborn et al. (2013), and Ciobanu and Dinu (2014) propose different edit distance, machine translation, and seman2072 tic similarity methods for identifying cognates. While false friends play a crucial role for the detection of marketing blunders, the existing approaches cannot be used directly, because they are specific for a certain language pair and do not make any assumptions on the (negative) connotations of a trade name. Kondrak and Dorr (2004) adapt orthographic and phonetic similarity methods to find confusable drug names. Unlike our tool which retrieves pragmatically marked sense descriptions, their work"
C16-1195,P14-2017,0,0.0130419,"pproach to generate neologisms that can be used as product names or slogans. Both works are focused on the English language and on the identification or generation of good names, whereas our work aims at the detection of problematic names without focusing on a particular language or target market. Our task is similar to automatically distinguishing cognates and false friends. Inkpen et al. (2005) use orthographic similarity metrics for this task including Soundex, which we also propose for our method. Follow-up works by Mitkov et al. (2007), Gomes and Lopes (2011), Beinborn et al. (2013), and Ciobanu and Dinu (2014) propose different edit distance, machine translation, and seman2072 tic similarity methods for identifying cognates. While false friends play a crucial role for the detection of marketing blunders, the existing approaches cannot be used directly, because they are specific for a certain language pair and do not make any assumptions on the (negative) connotations of a trade name. Kondrak and Dorr (2004) adapt orthographic and phonetic similarity methods to find confusable drug names. Unlike our tool which retrieves pragmatically marked sense descriptions, their work is limited to identifying si"
C16-1195,P16-1038,0,0.0133813,"nnoying person) and the French crev´e (extremely fatigued). While the former is relevant for detecting this blunder, the latter might have a somewhat similar pronunciation in English (e.g., [kôi:v]), but definitely not in French ([kK@.ve]). The same is true for Lada and the Swedish L˚ada ([&quot;lo:.da]). Such problems are due to Soundex being designed for English. Since Wiktionary also contains pronunciation information, a future method could lookup index entries with similar or equal IPA representation, given that they are available at a large-scale and for a large number of languages. Recently, Deri and Knight (2016) introduced a new grapheme-to-phoneme model for almost any language, which we consider highly relevant for indexing language-independent pseudo-phonetic representations. Ricks (2006) also notes the English form crap as a blunder cause for creap, since it has a similar spelling. None of our methods, however, returns crap as a clue for this name. An obvious solution would be the use of string similarity metrics, such as Levenshtein’s edit distance. We indeed tried this metric, but found that it returns a huge number of irrelevant clues. We therefore suggest to develop a modified edit distance me"
C16-1195,esuli-sebastiani-2006-sentiwordnet,0,0.00832116,"ches cannot be used directly, because they are specific for a certain language pair and do not make any assumptions on the (negative) connotations of a trade name. Kondrak and Dorr (2004) adapt orthographic and phonetic similarity methods to find confusable drug names. Unlike our tool which retrieves pragmatically marked sense descriptions, their work is limited to identifying similar forms. The recognition of words and phrases associated with opinions and emotions is the goal of sentiment analysis. There have been multiple attempts to construct large sentiment lexicons, such as SentiWordNet (Esuli and Sebastiani, 2006). Banea et al. (2008) propose a bootstrapping approach to induce such lexicons from a small, manually defined seed list. Our approach is similar, since we propagate pragmatic information based on lexical relations. However, we do not require a seed list and we consider relations across many languages. While monolingual resources are not suitable for our task at all, the existing multilingual sentiment lexicons are severely limited in size. The NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2013) is among the largest and available in about 40 languages, but since it has been automat"
C16-1195,C04-1137,0,0.0291544,"rthographic similarity metrics for this task including Soundex, which we also propose for our method. Follow-up works by Mitkov et al. (2007), Gomes and Lopes (2011), Beinborn et al. (2013), and Ciobanu and Dinu (2014) propose different edit distance, machine translation, and seman2072 tic similarity methods for identifying cognates. While false friends play a crucial role for the detection of marketing blunders, the existing approaches cannot be used directly, because they are specific for a certain language pair and do not make any assumptions on the (negative) connotations of a trade name. Kondrak and Dorr (2004) adapt orthographic and phonetic similarity methods to find confusable drug names. Unlike our tool which retrieves pragmatically marked sense descriptions, their work is limited to identifying similar forms. The recognition of words and phrases associated with opinions and emotions is the goal of sentiment analysis. There have been multiple attempts to construct large sentiment lexicons, such as SentiWordNet (Esuli and Sebastiani, 2006). Banea et al. (2008) propose a bootstrapping approach to induce such lexicons from a small, manually defined seed list. Our approach is similar, since we propa"
C16-1195,P12-1074,0,0.0319555,"Missing"
C16-1195,ozbal-etal-2012-brand,0,0.0691888,"Missing"
C16-1195,P11-1029,0,0.0236527,"sions like the ones by Spears (2007) or K¨upper (1984), as well as specialized dictionaries on trade names, such as Room (1982). Slang dictionaries are limited in their up-to-dateness (as indicated by the old publication years), word coverage, and range of available languages. Specialized name dictionaries are generally of little use for this task, as it is often the essence of a marketing campaign to create new, previously unused product or brand names. In natural language processing, there are previous works which address the automatic generation and retrieval of slogans and creative names. Veale (2011) presents a search engine for creative text retrieval, ¨ which assists copywriters to find metaphors and unusual word combinations. Ozbal and Strapparava (2012) describe an automatic approach to generate neologisms that can be used as product names or slogans. Both works are focused on the English language and on the identification or generation of good names, whereas our work aims at the detection of problematic names without focusing on a particular language or target market. Our task is similar to automatically distinguishing cognates and false friends. Inkpen et al. (2005) use orthographic"
C16-1195,P16-2036,0,0.0147389,"defined seed list. Our approach is similar, since we propagate pragmatic information based on lexical relations. However, we do not require a seed list and we consider relations across many languages. While monolingual resources are not suitable for our task at all, the existing multilingual sentiment lexicons are severely limited in size. The NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2013) is among the largest and available in about 40 languages, but since it has been automatically translated, it does not distinguish word senses and lacks slang and dialects. In recent work, Vo and Zhang (2016) propose a neural network architecture to build sentiment lexicons relying on emoticons as distant supervision signals. Although they currently publish only English and Arabic lexicons, such (almost) unsupervised methods are promising for building multilingual lexicons. 3 Task Formalization Let T denote a product, brand, or company name. Our goal is to develop a method M retrieving a set of clues C = M(T ) for a given T , which can be used by copywriters to decide whether T should be accepted or rejected as a name. We consider each clue c = (w, `, d) ∈ C as a tuple of a word form w of language"
C16-1258,D16-1084,0,0.0221261,"Missing"
C16-1258,E12-1004,0,0.0109603,"representation of the contexts yield broad topical similarities, while using dependency-based contexts yields more functional similarities (Levy et al., 2015). In addition, with word2vec (E) embeddings, we use linguistically motivated pre-trained dependency embeddings (D) and task-specific factual embeddings (F) for capturing syntactic and functional regularities encoded in the propositions, in order to better distinguish different types of claims. To incorporate these linguistic embeddings at word level into the learning process, we extend the network as illustrated in Figure 1a. Inspired by Baroni et al. (2012)’s supervised distributional concatenation method and a linguistically informed CNN (Ebert et al., 2015), we concatenate word2vec (E), dependency (D), and factual (F) word embeddings corresponding to the ith input word into a merged vector #» c i ∈ Rk+m+n : #» #» #» c i = [ #» e i , d i , f i ], (2) #» #» e i , d i , and f i represent, respectively, the concatenated word2vec, dependency, and factual where #» embeddings corresponding to ith word in the sentence. In the final representation, every input claim from the data set is represented using combined word2vec and linguistic embeddings in t"
C16-1258,J81-4005,0,0.723203,"Missing"
C16-1258,P14-5011,1,0.38724,"Missing"
C16-1258,W15-2915,0,0.033601,"Missing"
C16-1258,P11-1099,0,0.0404464,"factual vs. emotional), and performance comparable to the state of the art on the other data set (which categorizes propositions according to their verifiability). Our approach has the advantages of using a generalized, simple, and effective methodology that works for claim categorization on different data sets and tasks. 1 Introduction Argumentation mining is a relatively new subfield in natural language processing that aims to automatically identify and extract arguments, and their underlying structures, from textual documents (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013; Stab and Gurevych, 2014). Some such documents are written by professionals and contain well-formed, explicit arguments—i.e., propositions supported by evidence and connected through reasoning. However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014) and classifica"
C16-1258,N16-1138,0,0.0125456,"Missing"
C16-1258,D14-1181,0,0.226125,"ropositions, Park and Cardie (2014) followed previous work such as Reed et al. (2008) and Palau and Moens (2009), employing supervised learning methods. Despite using a rich set of linguistic features, these approaches suffer from low accuracy. Moreover, generating these features can be a tedious and complex process. In this paper, we show that state-of-the-art performance in claim classification for online user comments can be achieved without the need for expensive features. Our work, which employs CNN- and LSTM-based deep neural networks, is inspired by advances in sentence classification (Kim, 2014) and sequence classification (Hochreiter and Schmidhuber, 1997) using distributional word representations and deep learning. In particular, our approach leverages word2vec1 distributional embeddings, dependency context–based embeddings (Levy and Goldberg, 2014), and factuality/certainty-indicating embeddings for improving claim classification. (We refer to these embeddings as linguistic embeddings, as these are compiled from linguistic annotations such as dependency relations, verb modalities, and actuality information.) In this paper, we separately evaluate the usefulness of word and linguist"
C16-1258,N16-1175,0,0.00460711,"ebank. (Merely using clause tags without capturing dependencies for important clauses may not help much in distinguishing objective verifiable claims from unverifiable subjective ones.) Park and Cardie (2014) also used tense and person counts for distinguishing verifiable claims from unverifiable claims. We hypothesize that word2vec and dependency context–based embeddings can inherently capture these linguistic characteristics and can replace these features. Dependency context based embeddings capture functional similarities across the words using different contexts (Levy and Goldberg, 2014). Komninos and Manandhar (2016) have shown that dependency-based models produce word embeddings that better capture functional properties of words for question type classification and relation detection. Task-specific Embeddings. Compiling embeddings for the specific vocabulary present in the task data can also be helpful in a classification task. Tang et al. (2014) use enriched task-specific word embeddings and show improvement in a Twitter sentiment classification task. Park and Cardie (2014) compiled a speech-event lexicon containing the most frequent speech anchors (predicates such as “said” and “wrote”) from MPQA 2.0,"
C16-1258,P14-2050,0,0.0931832,"reover, generating these features can be a tedious and complex process. In this paper, we show that state-of-the-art performance in claim classification for online user comments can be achieved without the need for expensive features. Our work, which employs CNN- and LSTM-based deep neural networks, is inspired by advances in sentence classification (Kim, 2014) and sequence classification (Hochreiter and Schmidhuber, 1997) using distributional word representations and deep learning. In particular, our approach leverages word2vec1 distributional embeddings, dependency context–based embeddings (Levy and Goldberg, 2014), and factuality/certainty-indicating embeddings for improving claim classification. (We refer to these embeddings as linguistic embeddings, as these are compiled from linguistic annotations such as dependency relations, verb modalities, and actuality information.) In this paper, we separately evaluate the usefulness of word and linguistic embeddings in the claim classification task on both the aforementioned data sets. We also concatenate (stack) these embeddings and show how these stacked embeddings, as well as tuning of the hyper-parameters, further improves claim classification performance"
C16-1258,N15-1098,0,0.0133337,"tion layer, a max pooling layer, and a fully connected softmax layer. Each claim in the input layer is represented as a sentence comprised of distributional word embeddings. Let #» v i ∈ Rk be the k-dimensional word vector corresponding to the ith word in the sentence. Then a sentence S of length ` is represented as the concatenation of its word vectors: S = #» v 1 ⊕ #» v 2 ⊕ · · · ⊕ #» v `. (1) Word2vec embeddings which are learned using the bag-of-words representation of the contexts yield broad topical similarities, while using dependency-based contexts yields more functional similarities (Levy et al., 2015). In addition, with word2vec (E) embeddings, we use linguistically motivated pre-trained dependency embeddings (D) and task-specific factual embeddings (F) for capturing syntactic and functional regularities encoded in the propositions, in order to better distinguish different types of claims. To incorporate these linguistic embeddings at word level into the learning process, we extend the network as illustrated in Figure 1a. Inspired by Baroni et al. (2012)’s supervised distributional concatenation method and a linguistically informed CNN (Ebert et al., 2015), we concatenate word2vec (E), dep"
C16-1258,N13-1090,0,0.0211161,"ocused on stance classification but the claims in the data set are related to the data sets used in our work. 2.2 Distributional Word Embeddings Traditional supervised learning approaches to NLP tasks depend heavily on manual annotation, and often suffer from data sparseness. Distributional representations of words, also known as word embeddings, can be learned from large, unlabelled corpora using neural networks, and encode both syntactic and semantic properties of words. Studies have found the learned word vectors to capture linguistic regularities and to collapse similar words into groups (Mikolov et al., 2013b). Their utility in tasks such as sentiment classification (Kim, 2014) is well attested. 1 https://code.google.com/archive/p/word2vec/ 2741 Dependency-based Embeddings. Claims containing multiple clauses or propositions might be better distinguished with the help of dependency embeddings inferred from the respective proposition contexts. Consider the following claim from one of our data sets: “The Governor said that he enjoyed it.” In this claim, the main clause, “The Governor said”, is the core proposition, which excludes consideration of the remainder. The reason is that “said” is a reporti"
C16-1258,N15-1046,0,0.0152382,"al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013; Stab and Gurevych, 2014). Some such documents are written by professionals and contain well-formed, explicit arguments—i.e., propositions supported by evidence and connected through reasoning. However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014) and classification of propositions and arguments (Park and Cardie, 2014; Park et al., 2015; Oraby et al., 2015). Of particular interest is the fact that arguments in online user comments, unlike those written by professionals, often have inappropriate or missing justifications. Recognizing such propositions and determining the appropriate types of support can be useful for assessing the strength of the supporting information and, in turn, the strength of the whole argument. To this end, two previous studies have produced data sets and methods for classi"
C16-1258,W15-0515,0,0.141118,"ritten by professionals and contain well-formed, explicit arguments—i.e., propositions supported by evidence and connected through reasoning. However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014) and classification of propositions and arguments (Park and Cardie, 2014; Park et al., 2015; Oraby et al., 2015). Of particular interest is the fact that arguments in online user comments, unlike those written by professionals, often have inappropriate or missing justifications. Recognizing such propositions and determining the appropriate types of support can be useful for assessing the strength of the supporting information and, in turn, the strength of the whole argument. To this end, two previous studies have produced data sets and methods for classifying propositions in online argumentative discourse. The first of these studies (Park and Cardie, 2014) compiled online user comments from a discussion"
C16-1258,W14-2105,0,0.0824102,"Gurevych, 2014). Some such documents are written by professionals and contain well-formed, explicit arguments—i.e., propositions supported by evidence and connected through reasoning. However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014) and classification of propositions and arguments (Park and Cardie, 2014; Park et al., 2015; Oraby et al., 2015). Of particular interest is the fact that arguments in online user comments, unlike those written by professionals, often have inappropriate or missing justifications. Recognizing such propositions and determining the appropriate types of support can be useful for assessing the strength of the supporting information and, in turn, the strength of the whole argument. To this end, two previous studies have produced data sets and methods for classifying propositions in online argumentative discourse. The first of these studies (Park and Cardie, 2014) compile"
C16-1258,W15-0506,0,0.304352,"uch documents are written by professionals and contain well-formed, explicit arguments—i.e., propositions supported by evidence and connected through reasoning. However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014) and classification of propositions and arguments (Park and Cardie, 2014; Park et al., 2015; Oraby et al., 2015). Of particular interest is the fact that arguments in online user comments, unlike those written by professionals, often have inappropriate or missing justifications. Recognizing such propositions and determining the appropriate types of support can be useful for assessing the strength of the supporting information and, in turn, the strength of the whole argument. To this end, two previous studies have produced data sets and methods for classifying propositions in online argumentative discourse. The first of these studies (Park and Cardie, 2014) compiled online user comme"
C16-1258,reed-etal-2008-language,0,0.0082597,"his paper, we use the term “claim” loosely to refer to an individual proposition (a sentence or independent clause) in an argument, or to a short argumentative text containing one or more propositions. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: https://creativecommons.org/licenses/by/4.0/ 2740 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2740–2751, Osaka, Japan, December 11-17 2016. In classifying propositions, Park and Cardie (2014) followed previous work such as Reed et al. (2008) and Palau and Moens (2009), employing supervised learning methods. Despite using a rich set of linguistic features, these approaches suffer from low accuracy. Moreover, generating these features can be a tedious and complex process. In this paper, we show that state-of-the-art performance in claim classification for online user comments can be achieved without the need for expensive features. Our work, which employs CNN- and LSTM-based deep neural networks, is inspired by advances in sentence classification (Kim, 2014) and sequence classification (Hochreiter and Schmidhuber, 1997) using distr"
C16-1258,P09-1026,0,0.0190345,"rguments, and their underlying structures, from textual documents (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013; Stab and Gurevych, 2014). Some such documents are written by professionals and contain well-formed, explicit arguments—i.e., propositions supported by evidence and connected through reasoning. However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014) and classification of propositions and arguments (Park and Cardie, 2014; Park et al., 2015; Oraby et al., 2015). Of particular interest is the fact that arguments in online user comments, unlike those written by professionals, often have inappropriate or missing justifications. Recognizing such propositions and determining the appropriate types of support can be useful for assessing the strength of the supporting information and, in turn, the strength of the whole argument. To this end,"
C16-1258,D14-1006,1,0.526602,"able to the state of the art on the other data set (which categorizes propositions according to their verifiability). Our approach has the advantages of using a generalized, simple, and effective methodology that works for claim categorization on different data sets and tasks. 1 Introduction Argumentation mining is a relatively new subfield in natural language processing that aims to automatically identify and extract arguments, and their underlying structures, from textual documents (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013; Stab and Gurevych, 2014). Some such documents are written by professionals and contain well-formed, explicit arguments—i.e., propositions supported by evidence and connected through reasoning. However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014) and classification of propositions and arguments (Park and Cardie"
C16-1258,P14-1146,0,0.00968581,"y context–based embeddings can inherently capture these linguistic characteristics and can replace these features. Dependency context based embeddings capture functional similarities across the words using different contexts (Levy and Goldberg, 2014). Komninos and Manandhar (2016) have shown that dependency-based models produce word embeddings that better capture functional properties of words for question type classification and relation detection. Task-specific Embeddings. Compiling embeddings for the specific vocabulary present in the task data can also be helpful in a classification task. Tang et al. (2014) use enriched task-specific word embeddings and show improvement in a Twitter sentiment classification task. Park and Cardie (2014) compiled a speech-event lexicon containing the most frequent speech anchors (predicates such as “said” and “wrote”) from MPQA 2.0, a corpus manually annotated for opinions and other private states. These anchors can help in correctly distinguishing verifiable claims from unverifiable ones when the propositions contain both objective and subjective expressions. In our work, we use factual embeddings learned from the labelled FactBank corpus (Saur´ı and Pustejovsky,"
C16-1258,D15-1167,0,0.00329224,"pproaches when applied to various sentence- and document-level classification tasks. Kim (2014) have shown that CNNs outperform traditional machine learning–based approaches on several tasks, such as sentiment classification, question type classification, and subjectivity classification, using simple static word embeddings and tuning of hyper-parameters. Zhang et al. (2015) proposed character-level CNNs for text classification. Lai et al. (2015) and Visin et al. (2015) proposed recurrent CNNs, while Johnson and Zhang (2015) proposed semi-supervised CNNs for solving a text classification task. Tang et al. (2015) used a document classification approach based on recurrent neural networks (RNNs) and showed an improvement on a sentiment classification task. Palangi et al. (2016) proposed sentence embedding using an LSTM network for an information retrieval task. Zhou et al. (2016) proposed attention-based, bidirectional LSTM networks for a relation classification task. Augenstein et al. (2016) employed a weakly supervised conditional LSTM encoding approach to stance detection for unseen targets on Twitter stance detection data, and presented improved results. RNNs model text sequences effectively by capt"
C16-1258,walker-etal-2012-corpus,0,0.0454691,"structures, from textual documents (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013; Stab and Gurevych, 2014). Some such documents are written by professionals and contain well-formed, explicit arguments—i.e., propositions supported by evidence and connected through reasoning. However, informal arguments in online argumentative discourses can exhibit different styles. Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014) and classification of propositions and arguments (Park and Cardie, 2014; Park et al., 2015; Oraby et al., 2015). Of particular interest is the fact that arguments in online user comments, unlike those written by professionals, often have inappropriate or missing justifications. Recognizing such propositions and determining the appropriate types of support can be useful for assessing the strength of the supporting information and, in turn, the strength of the whole argument. To this end, two previous studies h"
C16-1258,P16-2034,0,0.00446291,"ty classification, using simple static word embeddings and tuning of hyper-parameters. Zhang et al. (2015) proposed character-level CNNs for text classification. Lai et al. (2015) and Visin et al. (2015) proposed recurrent CNNs, while Johnson and Zhang (2015) proposed semi-supervised CNNs for solving a text classification task. Tang et al. (2015) used a document classification approach based on recurrent neural networks (RNNs) and showed an improvement on a sentiment classification task. Palangi et al. (2016) proposed sentence embedding using an LSTM network for an information retrieval task. Zhou et al. (2016) proposed attention-based, bidirectional LSTM networks for a relation classification task. Augenstein et al. (2016) employed a weakly supervised conditional LSTM encoding approach to stance detection for unseen targets on Twitter stance detection data, and presented improved results. RNNs model text sequences effectively by capturing long-range dependencies among the words. LSTM-based approaches based on RNNs effectively capture the sequences in the sentences when compared to the CNN and SVM-based approaches. 3 Claim Classification Here we present two deep learning–based methods for claim clas"
C16-1272,S12-1051,0,0.158401,"rase detection (Dolan et al., 2004), for example, tries to identify whether two texts express the same information. It cannot, however, capture cases where there is only partial information overlap. One paradigm that addresses this issue is textual entailment (Dagan et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which"
C16-1272,J05-3002,0,0.134834,"Missing"
C16-1272,P99-1071,0,0.315577,"Missing"
C16-1272,W04-1016,0,0.0794161,"Missing"
C16-1272,C04-1051,0,0.749669,"e it is difficult for non-experts to annotate. We analyze 200 pairs of similar sentences and identify several underlying properties of sentence intersection. We leverage these insights to design an algorithm that decomposes the sentence intersection task into several simpler annotation tasks, facilitating the construction of a high quality dataset via crowdsourcing. We implement this approach and provide an annotated dataset of 1,764 sentence intersections. 1 Introduction Various paradigms exist for comparing the meanings of two texts and modeling their semantic overlap. Paraphrase detection (Dolan et al., 2004), for example, tries to identify whether two texts express the same information. It cannot, however, capture cases where there is only partial information overlap. One paradigm that addresses this issue is textual entailment (Dagan et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for th"
C16-1272,D08-1019,0,0.068328,"Missing"
C16-1272,P08-2049,0,0.0504803,"Missing"
C16-1272,E14-1057,0,0.0558176,"Missing"
C16-1272,P16-2041,1,0.833208,"sist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree within that sentence t,"
C16-1272,P13-2080,1,0.857695,"d in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount of shared information as in semantic text similarity, sentence intersection captures what this shared information is. Although sentence intersection has existed for over a decade, it has received little attention due to a lack of annotated data. Previous annotation attempts have either used experts, which did not scale, or crowdsourcing, which yielded unreliable annotations (McKeown et al., 2010). We also observe that annotating sentence intersection is difficult for non-experts. We hypothesize that this difficulty stems from the task’s require"
C16-1272,W04-1013,0,0.0536885,"ria – output set and extractiveness – we define extractive sentence intersection as the set of all sentences that each contains all the information common to the input sentences, while being composed only of words that appeared in the input sentences and placeholders.2 The entire set of extractive intersections allows for more accurate automatic evaluation, since the evaluation mechanism does not need to overcome issues in lexical variability; instead, it can simply select the most similar expert-crafted sentence from the set using simple metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004). Having multiple possible solutions is not a foreign concept to NLP, and is widely used in translation and summarization. 2 While this paper discusses intersections between two input sentences, one can theoretically extend this setting to multiple input sentences by consecutively applying the intersection operation. For example, if we have three sentences s1 , s2 , s3 , we could first find the intersection between s1 and s2 , and then for each sentence s0 in s1 ∩ s2 , intersect S that with s3 . We would essentially take the union of the latter set of intersection sets, i.e. s1 ∩ s2 ∩ s3 = (s1"
C16-1272,W05-1612,0,0.420831,"et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than q"
C16-1272,S07-1009,0,0.0447691,"in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree within that sentence t, and another subtree t0 , which is not necessarily part of s. It creates a new sentence s0 by"
C16-1272,N10-1044,0,0.285035,"s whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount"
C16-1272,P13-1131,1,0.858605,"context, which will assist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree"
C16-1272,N04-1019,0,0.0377941,"Missing"
C16-1272,P02-1040,0,0.0954815,"scope. Combining both these criteria – output set and extractiveness – we define extractive sentence intersection as the set of all sentences that each contains all the information common to the input sentences, while being composed only of words that appeared in the input sentences and placeholders.2 The entire set of extractive intersections allows for more accurate automatic evaluation, since the evaluation mechanism does not need to overcome issues in lexical variability; instead, it can simply select the most similar expert-crafted sentence from the set using simple metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004). Having multiple possible solutions is not a foreign concept to NLP, and is widely used in translation and summarization. 2 While this paper discusses intersections between two input sentences, one can theoretically extend this setting to multiple input sentences by consecutively applying the intersection operation. For example, if we have three sentences s1 , s2 , s3 , we could first find the intersection between s1 and s2 , and then for each sentence s0 in s1 ∩ s2 , intersect S that with s3 . We would essentially take the union of the latter set of intersection sets, i."
C16-1272,P15-2070,0,0.0224273,"Missing"
C16-1272,P13-1051,0,0.0304221,"Missing"
C16-1272,W11-1606,0,0.0173189,"be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount of shared information as in"
C16-1272,I13-1198,0,0.0305508,"Missing"
C16-1272,P12-2031,1,0.836148,"subtree entailment in context, which will assist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sente"
C16-1272,N07-1051,0,\N,Missing
C18-1020,P17-1151,0,0.0146261,"lead to significant improvements. Their approach is similar to our lemma-w2 setup (Section 3). Another recent attempt to address this problem is the study by Vuli´c et al. (2017b), which uses a small set of morphological rules to specialize the vector space bringing the forms of the same word closer together while setting the derivational antonyms further apart. We relate to this approach in Section 4. Finally, type-based VSMs neglect the problem of polysemy: all senses of a word are encoded as a single entry in the VSM. This issue has been recently approached by multi-modal word embeddings (Athiwaratkun and Wilson, 2017) and Gaussian embeddings (Vilnis and McCallum, 2014). Partially disambiguating the source data via POS tagging has been employed in (Trask et al., 2015). Here, instead of constructing vectors for word forms, POS information is integrated into the vector space as well. This is similar to our type.POS-w2 setup (Section 3) which, as we show, introduces additional sparsity and ignores morphological inflection. An alternative line of research aims to learn embeddings for lexical units by using an external WSD tool to preprocess the corpus and applying standard word embedding machinery to induce dis"
C18-1020,P98-1013,0,0.0937485,", it introduces an additional level of complexity (clustering algorithm and its parameters) which might obscure the VSM performance details. In this paper we propose an alternative, suggestion-based evaluation approach: we use the source VSM directly to generate word class suggestions (WCS) for a given input term. Many lexical resources group words into intersecting word classes, providing a compact way to describe word properties on the class level. For example, in VerbNet (Schuler, 2005) verbs can belong to one or more Levin classes (Levin, 1993) based on their syntactic behavior, FrameNet (Baker et al., 1998) groups its entries by the semantic frames they can evoke, and WordNet (Miller, 1995) provides coarse-grained supersense groupings. Suggestion-based evaluation can be seen as flexible alternative to clustering-based evaluation, which intrinsically takes ambiguity into account and does not require an additional clustering layer. The following section describes the suggestion-based evaluation in more detail4 . 5.2 Task formulation Abstracting away from the resource specifics, a lexicon L defines a mapping from a set of members m1 , m2 , ...mi ∈ M to a set of word classes c1 , c2 ...cj ∈ C. We fu"
C18-1020,W03-1022,0,0.067377,"tasks. While prior work shows that lemmatization and POS-typing of targets in isolation are beneficial for downstream tasks, it does not provide a detailed investigation on why it is the case and does not study the effects of combining the two preprocessing techniques. This paper aims to close this gap. We evaluate the effects of lemmatization and POS disambiguation separately and combined on similarity benchmarks, and further refine our results using a novel resource-based word class suggestion scenario which measures how well a VSM represents VerbNet (Schuler, 2005) and WordNet supersense (Ciaramita and Johnson, 2003) class membership. We find that POS-typing and lemmatization have complementary qualitative and vocabulary-level effects and are best used in combination. We observe that English verb similarity is harder to model and show that using lemmatized and disambiguated embeddings implicitly targets some of the verb-specific issues. In summary, the contributions of this paper are as follows: • We suggest using lemmatized and POS disambiguated targets as a conceptually plausible alternative to type, word form and lemma-based VSMs; • We introduce the suggestion-based evaluation scenario applicable to a"
C18-1020,D16-1071,0,0.211794,"Missing"
C18-1020,P16-1191,1,0.776459,"ting the source data via POS tagging has been employed in (Trask et al., 2015). Here, instead of constructing vectors for word forms, POS information is integrated into the vector space as well. This is similar to our type.POS-w2 setup (Section 3) which, as we show, introduces additional sparsity and ignores morphological inflection. An alternative line of research aims to learn embeddings for lexical units by using an external WSD tool to preprocess the corpus and applying standard word embedding machinery to induce distributed representations for lexical units, e.g. (Iacobacci et al., 2015; Flekova and Gurevych, 2016). Such approaches require an external WSD tool which introduces additional bias and might not be available for lower-resourced languages. Moreover, to query such VSMs it is necessary to either apply WSD to the input, or to align the inputs with the senses in some other way, which is not always feasible. From the evaluation perspective, a popular method to assess the performance of a particular vector space model is similarity benchmarking. A similarity benchmark consists of word pairs along with human-assessed similarity scores, which are compared to cosine similarities returned by VSMs via ra"
C18-1020,D16-1235,0,0.022965,"Missing"
C18-1020,P06-1117,0,0.0196857,"es. For example, the verb buy belongs to the class get-13.5.1. The class specifies a set of available roles, e.g. an animate Agent (buyer), an Asset (price paid) and a Theme (thing bought), and lists available syntactic constructions, e.g. the Asset V Theme construction (“$50 won’t buy a dress“). A verb might appear in several classes, indicating different verb senses. For example, the verb hit allows several readings: as hurt (John hit his leg), as throw (John hit Mary the ball) and as bump (The cart hit against the wall). VerbNet has been successfully used to support semantic role labeling (Giuglea and Moschitti, 2006), information extraction (Mausam et al., 2012) and semantic parsing (Shi and Mihalcea, 2005). WordNet, besides providing a dense network of lexical relations, groups its entries into coarsegrained supersense classes, e.g. noun.animal (aardvark, koala), noun.location (park, senegal), noun.time (forties, nanosecond). WordNet supersense tags have been applied to a range of downstream tasks, e.g. metaphor identification and sentiment polarity classification (Flekova and Gurevych, 2016). WordNet differs from VerbNet in terms of granularity, member-class distribution and part of speech coverage, and"
C18-1020,J15-4004,0,0.212436,"ed (V.Fin.Past) e-mailing (V.Ger) form type token LAMB e-mail (e-mail.N.Sg) e-mails (e-mail.N.Pl) e-mails (e-mail.V.Fin.Pres.3Sg.) e-mails e-mails (e-mail.N.Pl) sense2vec word2vec, GloVe He1 e-mails2 them3 every4 day5. They6 receive7 his8 e-mails9. Figure 1: Hierarchy of word-related concepts for emails, with VerbNet as lexicon example. latter aims to classify lexemes (e.g. word clustering, thesaurus construction, lexicon completion). Lexical resources mostly operate on lexeme or lexical unit level. This includes most of the similarity benchmarks (Finkelstein et al., 2001; Bruni et al., 2014; Hill et al., 2015; Gerz et al., 2016) that implicitly provide similarity scores for lexemes and not word types. Traditional word embeddings approaches (Mikolov et al., 2013; Pennington et al., 2014) induce representations on word type level. As our example in Figure 1 shows, this leads to a conceptual gap between the lexicon and the VSM, which has practical consequences. Consider the standard scenario when a type-based VSM is evaluated against a lexeme-based benchmark. One of the types contained in the VSM vocabulary corresponds to the base form (lemma), and the vector for the base word form is used for evalua"
C18-1020,P15-1010,0,0.0228003,"4). Partially disambiguating the source data via POS tagging has been employed in (Trask et al., 2015). Here, instead of constructing vectors for word forms, POS information is integrated into the vector space as well. This is similar to our type.POS-w2 setup (Section 3) which, as we show, introduces additional sparsity and ignores morphological inflection. An alternative line of research aims to learn embeddings for lexical units by using an external WSD tool to preprocess the corpus and applying standard word embedding machinery to induce distributed representations for lexical units, e.g. (Iacobacci et al., 2015; Flekova and Gurevych, 2016). Such approaches require an external WSD tool which introduces additional bias and might not be available for lower-resourced languages. Moreover, to query such VSMs it is necessary to either apply WSD to the input, or to align the inputs with the senses in some other way, which is not always feasible. From the evaluation perspective, a popular method to assess the performance of a particular vector space model is similarity benchmarking. A similarity benchmark consists of word pairs along with human-assessed similarity scores, which are compared to cosine similar"
C18-1020,P14-2050,0,0.568026,"ed targets as a conceptually plausible alternative to type, word form and lemma-based VSMs; • We introduce the suggestion-based evaluation scenario applicable to a wide range of lexical resources; • We show that lemmatization and POS disambiguation improve both benchmark and resource-based performance by implicitly targeting some of the grammar-level issues. 234 2 Related work The starting point for our study are the results from the lemma-based word embedding approach by Ebert et al. (2016), the POS-disambiguated sense2vec embeddings by Trask et al. (2015) and the dependencybased contexts by Levy and Goldberg (2014). Our primary focus is the effect of word embedding targets on vocabulary-based task performance. However, to put our work in context, we provide an extended overview of the issues related to traditional type-based approaches to VSM construction, along with relevant solutions. Type-based VSMs (Mikolov et al., 2013; Pennington et al., 2014) have several limitations that have been extensively studied. They have restricted vocabularies and hence lack the ability to represent out-of-vocabulary words. Several recent approaches treat this issue by learning vector representations for sub-word units,"
C18-1020,P14-5010,0,0.00872553,"Missing"
C18-1020,D12-1048,0,0.00989799,"-13.5.1. The class specifies a set of available roles, e.g. an animate Agent (buyer), an Asset (price paid) and a Theme (thing bought), and lists available syntactic constructions, e.g. the Asset V Theme construction (“$50 won’t buy a dress“). A verb might appear in several classes, indicating different verb senses. For example, the verb hit allows several readings: as hurt (John hit his leg), as throw (John hit Mary the ball) and as bump (The cart hit against the wall). VerbNet has been successfully used to support semantic role labeling (Giuglea and Moschitti, 2006), information extraction (Mausam et al., 2012) and semantic parsing (Shi and Mihalcea, 2005). WordNet, besides providing a dense network of lexical relations, groups its entries into coarsegrained supersense classes, e.g. noun.animal (aardvark, koala), noun.location (park, senegal), noun.time (forties, nanosecond). WordNet supersense tags have been applied to a range of downstream tasks, e.g. metaphor identification and sentiment polarity classification (Flekova and Gurevych, 2016). WordNet differs from VerbNet in terms of granularity, member-class distribution and part of speech coverage, and allows us to estimate VSM performance on nomi"
C18-1020,D14-1162,0,0.0916809,"d2vec, GloVe He1 e-mails2 them3 every4 day5. They6 receive7 his8 e-mails9. Figure 1: Hierarchy of word-related concepts for emails, with VerbNet as lexicon example. latter aims to classify lexemes (e.g. word clustering, thesaurus construction, lexicon completion). Lexical resources mostly operate on lexeme or lexical unit level. This includes most of the similarity benchmarks (Finkelstein et al., 2001; Bruni et al., 2014; Hill et al., 2015; Gerz et al., 2016) that implicitly provide similarity scores for lexemes and not word types. Traditional word embeddings approaches (Mikolov et al., 2013; Pennington et al., 2014) induce representations on word type level. As our example in Figure 1 shows, this leads to a conceptual gap between the lexicon and the VSM, which has practical consequences. Consider the standard scenario when a type-based VSM is evaluated against a lexeme-based benchmark. One of the types contained in the VSM vocabulary corresponds to the base form (lemma), and the vector for the base word form is used for evaluation. This particular form, however, is selected based on grammatical considerations and, as we later demonstrate, is neither the most frequent nor the most representative in terms"
C18-1020,D17-1270,0,0.0301887,"Missing"
C18-1020,P17-1006,0,0.0357784,"Missing"
C18-1020,D16-1157,0,0.117102,"gets on vocabulary-based task performance. However, to put our work in context, we provide an extended overview of the issues related to traditional type-based approaches to VSM construction, along with relevant solutions. Type-based VSMs (Mikolov et al., 2013; Pennington et al., 2014) have several limitations that have been extensively studied. They have restricted vocabularies and hence lack the ability to represent out-of-vocabulary words. Several recent approaches treat this issue by learning vector representations for sub-word units, e.g. fastText (Bojanowski et al., 2016) and charagram (Wieting et al., 2016). Type-based VSMs do not abstract away from word inflection: different forms of the same word are assigned different representations in the VSM. Ebert et al. (2016) introduces lemmatized LAMB embeddings as a way to address this issue and shows that lemmatization is highly beneficial for word similarity and lexical modeling tasks even for morphologically poor English, while using stems doesn’t lead to significant improvements. Their approach is similar to our lemma-w2 setup (Section 3). Another recent attempt to address this problem is the study by Vuli´c et al. (2017b), which uses a small set"
C18-1071,Q16-1022,0,0.0291622,"result, acquiring (high-quality) datasets for new languages comes at a high cost—be it in terms of training and/or hiring expert annotators or querying large crowds in crowd-sourcing experiments. It is thus of utmost importance to train NLP systems in AM that are capable of going cross-language, so that annotation efforts do not have to be multiplied by the number of languages of interest. This is in line with current trends in NLP, which increasingly recognize the possibility and the necessity to work cross-lingually, be it in part-of-speech tagging (Zhang et al., 2016), dependency parsing (Agic et al., 2016), sentiment mining (Chen et al., 2016; Zhou et al., 2016), or other fields. In this work, we address the problem of cross-lingual (token-level) AM for the first time. We initially experiment with available resources in English, German, and Chinese. We show that the existing datasets for analyzing argumentation are not suitable for assessing cross-lingual component extraction due to their heterogeneity or lack of complexity. Given this scarcity of homogeneously annotated high-quality largescale datasets across different languages, our first contribution is to (1) provide a fully parallel (en-de"
C18-1071,W17-5115,0,0.0231203,", which is very cheap to obtain for dozens of high-resource languages. Our findings imply that current neural MT has reached a level where it can act as a substitute for costly (non-expert) HT even for problems that operate on the fine-grained token-level. 2 Related Work In what follows, we briefly summarize the works that most closely relate to our current research. Argumentation Mining AM seeks to automatically identify argument structures in text and has received a lot of attention in NLP lately. Existing approaches focus, for instance, on specific subtasks like argument unit segmentation (Ajjour et al., 2017), identifying different types of argument components (Mochales-Palau and Moens, 2009; Habernal and Gurevych, 2017), recognizing argumentative discourse relations (Nguyen and Litman, 2016) or extracting argument components and relations endto-end (Eger et al., 2017). However, most of these approaches are specifically designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dat"
C18-1071,W17-5108,0,0.0258307,"cally designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dataset with claims and premises and translated it to English subsequently. There are very few works studying AM in other languages, e.g. Basile et al. (2016) for Italian, Li et al. (2017) for Chinese and Sardianos et al. (2015) for Greek. There are also two recent papers addressing some form of cross-linguality: Aker and Zhang (2017) map argumentative sentences from English to Mandarin using machine translation in comparable Wikipedia articles. Sliwa et al. (2018) create corpora in Balkan languages and Arabic by labeling the English side of corresponding parallel corpora on the sentence level and then using the same label for the target sentences. In contrast to these works, we work on the more challenging token-level. Moreover, we actually train classifiers for language transfer rather than only creating annotated data in other languages based on parallel data. As mentioned, we focus on cross-lingual component extraction"
C18-1071,P17-2037,0,0.0140282,"ell as translations with humancreated and projected annotations. Major claims in bold, claims underlined, premises in italics. HT/MT =human/machine translation. language adaptation, because it is the most realistic cross-lingual scenario for AM, as it may be costly to even produce small amounts of training data in many different languages. Most cross-lingual sequence tagging approaches address POS tagging, and only few are devoted to NER (Mayhew et al., 2017; Tsai et al., 2016), aspect-based sentiment classification (Lambert, 2015), or even more challenging problems such as discourse parsing (Braud et al., 2017). While POS tagging and NER are in some sense very similar to AM, namely, insofar as both can be modeled as sequential tagging of tokens, there are also important differences. For example, in POS tagging and NER, the label for a current token usually strongly depends on the token itself plus some local context. This strong association between label, token and local context is largely absent in AM, causing some models that perform well on POS and NER to fail blatantly in AM.1 Cross-lingual Word Embeddings are the (modern) basis of the direct transfer method. As with monolingual embeddings, ther"
C18-1071,D17-1070,0,0.0612602,"Missing"
C18-1071,D17-1078,0,0.0598659,"Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations in L2, there are also supervised cross-lingual approaches based on multi-task learning (Cotterell and Heigold, 2017; Yang et al., 2017; Kim et al., 2017). These assume small training sets in L2, and a system trained on them is regularized by a larger amount of training data in L1. In our work, we only consider unsupervised 832 Name Docum. Tokens Sentences Major Cl. Cl. Prem. Genre Lang. MTX CRC PE 112 315 402 8,865 (en) 21,858 148,186 (en) 449 957 7,141 135 751 112 1,415 1,506 464 684 3,832 short texts reviews persuasive essays en, de zh [en] en [de, fr, es, zh] Table 1: Statistics for datasets used in this work. Languages in brackets added by the current work. Orig-EN HT-DE-HumanAnno HT-DE-ProjAnno MT-DE-"
C18-1071,P11-1061,0,0.025871,"ken with a BIO label plus its respective component type. The BIO label marks the start, continuation and end of specific argument components. Examples are given in Tables 2 and 3. Cross-lingual sequence tagging POS tagging and named-entity recognition (NER) are standard tasks in NLP. In recent years, there is increased interest not only in evaluating POS and NER models within multiple individual languages (Plank et al., 2016), but also cross-lingually (Zhang et al., 2016; Tsai et al., 2016; Mayhew et al., 2017; Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations in L2, there are also supervised cross-lingual approaches based on multi-task learning (Cotterell and Heigold, 2017; Yang et al., 2017; Kim et al., 2017). These assume small training sets in L"
C18-1071,D17-1218,1,0.898743,"Missing"
C18-1071,N13-1073,0,0.0299452,"., 2015) on >2 million aligned sentences from the Europarl corpus (Koehn, 2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shuffles each concatenation and trains a standard monolingual word embedding technique on the result; here, we use the word2vec skipgram model (Mikolov et al., 2013). BIVCD was shown to be competitive to more challenging approaches in Upadhyay et al. (2016). BISKIP is a variant of the standard skip-gram model which predicts monoand cross-lingual contexts. It requires word alignments between parallel sentences and we use fast-align for this (Dyer et al., 2013). For EN↔ZH we train the same models on the UN corpus (Ziemski et al., 2016), which comprises >11 million parallel sentences. We train embeddings of sizes 100 and 200. Projection To implement projection for the problem of token-level AM, we proceed as follows. We take our human-labeled L1 data and align it with its corresponding parallel L2 data using fast-align. Once we have word level alignment information, we consider for each argument component c(s) in L1 of type a (e.g., MajorClaim, Claim, Premise) with consecutive words s1 , . . . , sN : the word t1 with smallest index in the correspondi"
C18-1071,D15-1267,1,0.933711,"alysis, summarization, debate modeling, and law, among others (Peldszus and Stede, 2013a). Recent studies have successfully applied computational methods to analyze monological argumentation (Wachsmuth et al., 2017; Eger et al., 2017). Most of these studies view arguments as consisting of (at least) claims and premises—and so do we in this work. Thereby, our focus is on token-level argument component extraction, that is, the segmentation and typing of argument components. AM has thus far almost exclusively been performed monolingually, e.g. in English (Mochales-Palau and Moens, 2009), German (Eckle-Kohler et al., 2015), or Chinese (Li et al., 2017). Working only monolingually is problematic, however, because AM is a difficult task even for humans due to its dependence on background knowledge and parsing of complex pragmatic relations (Moens, 2017). As a result, acquiring (high-quality) datasets for new languages comes at a high cost—be it in terms of training and/or hiring expert annotators or querying large crowds in crowd-sourcing experiments. It is thus of utmost importance to train NLP systems in AM that are capable of going cross-language, so that annotation efforts do not have to be multiplied by the"
C18-1071,P17-1002,1,0.921063,"almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/ coling2018-xling_argument_mining. 1 Introduction Argumentation mining (AM) is a fast-growing research field with applications in discourse analysis, summarization, debate modeling, and law, among others (Peldszus and Stede, 2013a). Recent studies have successfully applied computational methods to analyze monological argumentation (Wachsmuth et al., 2017; Eger et al., 2017). Most of these studies view arguments as consisting of (at least) claims and premises—and so do we in this work. Thereby, our focus is on token-level argument component extraction, that is, the segmentation and typing of argument components. AM has thus far almost exclusively been performed monolingually, e.g. in English (Mochales-Palau and Moens, 2009), German (Eckle-Kohler et al., 2015), or Chinese (Li et al., 2017). Working only monolingually is problematic, however, because AM is a difficult task even for humans due to its dependence on background knowledge and parsing of complex pragmati"
C18-1071,J17-1004,1,0.846653,"ral MT has reached a level where it can act as a substitute for costly (non-expert) HT even for problems that operate on the fine-grained token-level. 2 Related Work In what follows, we briefly summarize the works that most closely relate to our current research. Argumentation Mining AM seeks to automatically identify argument structures in text and has received a lot of attention in NLP lately. Existing approaches focus, for instance, on specific subtasks like argument unit segmentation (Ajjour et al., 2017), identifying different types of argument components (Mochales-Palau and Moens, 2009; Habernal and Gurevych, 2017), recognizing argumentative discourse relations (Nguyen and Litman, 2016) or extracting argument components and relations endto-end (Eger et al., 2017). However, most of these approaches are specifically designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dataset with claims and premises and translated it to English subsequently. There are very few works studying AM in o"
C18-1071,D17-1302,0,0.0862285,"projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations in L2, there are also supervised cross-lingual approaches based on multi-task learning (Cotterell and Heigold, 2017; Yang et al., 2017; Kim et al., 2017). These assume small training sets in L2, and a system trained on them is regularized by a larger amount of training data in L1. In our work, we only consider unsupervised 832 Name Docum. Tokens Sentences Major Cl. Cl. Prem. Genre Lang. MTX CRC PE 112 315 402 8,865 (en) 21,858 148,186 (en) 449 957 7,141 135 751 112 1,415 1,506 464 684 3,832 short texts reviews persuasive essays en, de zh [en] en [de, fr, es, zh] Table 1: Statistics for datasets used in this work. Languages in brackets added by the current work. Orig-EN HT-DE-HumanAnno HT-DE-ProjAnno MT-DE-ProjAnno MT-ES-ProjAnno MT-FR-ProjAnno"
C18-1071,2005.mtsummit-papers.11,0,0.0193789,"s induced via machine translation, then a second source of noise for projection is the ‘unreliable’ L2 input training data. Direct Transfer Here, we directly train a system on bilingual representations, which in our case come in the form of bilingual word embeddings. To retain some freedom over the choice and parameters of our word embeddings, we choose to train them ourselves instead of using pre-trained ones. For EN↔DE we induce bilingual word embeddings by training BIVCD (Vulic and Moens, 2015) and BISKIP models (Luong et al., 2015) on >2 million aligned sentences from the Europarl corpus (Koehn, 2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shuffles each concatenation and trains a standard monolingual word embedding technique on the result; here, we use the word2vec skipgram model (Mikolov et al., 2013). BIVCD was shown to be competitive to more challenging approaches in Upadhyay et al. (2016). BISKIP is a variant of the standard skip-gram model which predicts monoand cross-lingual contexts. It requires word alignments between parallel sentences and we use fast-align for this (Dyer et al., 2013). For EN↔ZH we train the same models on the UN corpus (Ziemsk"
C18-1071,P15-2128,0,0.0195247,"不能算计划中的运气] 。 Table 2: Human-annotated English sentence in the PE dataset as well as translations with humancreated and projected annotations. Major claims in bold, claims underlined, premises in italics. HT/MT =human/machine translation. language adaptation, because it is the most realistic cross-lingual scenario for AM, as it may be costly to even produce small amounts of training data in many different languages. Most cross-lingual sequence tagging approaches address POS tagging, and only few are devoted to NER (Mayhew et al., 2017; Tsai et al., 2016), aspect-based sentiment classification (Lambert, 2015), or even more challenging problems such as discourse parsing (Braud et al., 2017). While POS tagging and NER are in some sense very similar to AM, namely, insofar as both can be modeled as sequential tagging of tokens, there are also important differences. For example, in POS tagging and NER, the label for a current token usually strongly depends on the token itself plus some local context. This strong association between label, token and local context is largely absent in AM, causing some models that perform well on POS and NER to fail blatantly in AM.1 Cross-lingual Word Embeddings are the"
C18-1071,N16-1030,0,0.0187963,"Missing"
C18-1071,W16-2817,0,0.0139484,", for instance, on specific subtasks like argument unit segmentation (Ajjour et al., 2017), identifying different types of argument components (Mochales-Palau and Moens, 2009; Habernal and Gurevych, 2017), recognizing argumentative discourse relations (Nguyen and Litman, 2016) or extracting argument components and relations endto-end (Eger et al., 2017). However, most of these approaches are specifically designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dataset with claims and premises and translated it to English subsequently. There are very few works studying AM in other languages, e.g. Basile et al. (2016) for Italian, Li et al. (2017) for Chinese and Sardianos et al. (2015) for Greek. There are also two recent papers addressing some form of cross-linguality: Aker and Zhang (2017) map argumentative sentences from English to Mandarin using machine translation in comparable Wikipedia articles. Sliwa et al. (2018) create corpora in Balkan languages and Arabic by labeling the En"
C18-1071,W15-1521,0,0.0294941,"ons than direct transfer: it requires parallel data.3 When the parallel data is induced via machine translation, then a second source of noise for projection is the ‘unreliable’ L2 input training data. Direct Transfer Here, we directly train a system on bilingual representations, which in our case come in the form of bilingual word embeddings. To retain some freedom over the choice and parameters of our word embeddings, we choose to train them ourselves instead of using pre-trained ones. For EN↔DE we induce bilingual word embeddings by training BIVCD (Vulic and Moens, 2015) and BISKIP models (Luong et al., 2015) on >2 million aligned sentences from the Europarl corpus (Koehn, 2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shuffles each concatenation and trains a standard monolingual word embedding technique on the result; here, we use the word2vec skipgram model (Mikolov et al., 2013). BIVCD was shown to be competitive to more challenging approaches in Upadhyay et al. (2016). BISKIP is a variant of the standard skip-gram model which predicts monoand cross-lingual contexts. It requires word alignments between parallel sentences and we use fast-align for this (Dyer et"
C18-1071,P16-1101,0,0.124359,"Missing"
C18-1071,D17-1269,0,0.149769,"2), while having only annotated source language (L1) data. We operate on token-level by labeling each token with a BIO label plus its respective component type. The BIO label marks the start, continuation and end of specific argument components. Examples are given in Tables 2 and 3. Cross-lingual sequence tagging POS tagging and named-entity recognition (NER) are standard tasks in NLP. In recent years, there is increased interest not only in evaluating POS and NER models within multiple individual languages (Plank et al., 2016), but also cross-lingually (Zhang et al., 2016; Tsai et al., 2016; Mayhew et al., 2017; Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations in L2, there are also supervised cross-lingual approaches based on multi-task learning"
C18-1071,D11-1006,0,0.102983,"20-26, 2018. We then (2) machine translate the 402 student essays into German, Spanish, French, and Chinese. Both our human and machine translations contain argumentation annotations, in the form of either human annotations or automatically projected annotations. Our experiments indicate that both the translations and the projected annotations are of very high quality, cf. examples in Table 2. Besides contributing new datasets, (3) we perform the first evaluations of cross-lingual (token-level) AM, based on suitable adaptations of two popular cross-lingual techniques, namely, direct transfer (McDonald et al., 2011) and projection (Yarowsky et al., 2001). We find that projection works considerably better than direct transfer and almost closes the cross-lingual gap, i.e., cross-lingual performance is almost on par with in-language performance when we use parallel data and project annotations to the target language. This holds both for human (translated, HT) parallel data, which is costly to obtain, and machine translated (MT) parallel data, which is very cheap to obtain for dozens of high-resource languages. Our findings imply that current neural MT has reached a level where it can act as a substitute for"
C18-1071,D13-1032,0,0.0277835,"Missing"
C18-1071,P16-1107,0,0.0496735,"xpert) HT even for problems that operate on the fine-grained token-level. 2 Related Work In what follows, we briefly summarize the works that most closely relate to our current research. Argumentation Mining AM seeks to automatically identify argument structures in text and has received a lot of attention in NLP lately. Existing approaches focus, for instance, on specific subtasks like argument unit segmentation (Ajjour et al., 2017), identifying different types of argument components (Mochales-Palau and Moens, 2009; Habernal and Gurevych, 2017), recognizing argumentative discourse relations (Nguyen and Litman, 2016) or extracting argument components and relations endto-end (Eger et al., 2017). However, most of these approaches are specifically designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dataset with claims and premises and translated it to English subsequently. There are very few works studying AM in other languages, e.g. Basile et al. (2016) for Italian, Li et al. (2017) f"
C18-1071,W13-2324,0,0.136121,"ction and (ii) bilingual word embeddings based direct transfer strategies for cross-lingual AM, finding that the former performs considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/ coling2018-xling_argument_mining. 1 Introduction Argumentation mining (AM) is a fast-growing research field with applications in discourse analysis, summarization, debate modeling, and law, among others (Peldszus and Stede, 2013a). Recent studies have successfully applied computational methods to analyze monological argumentation (Wachsmuth et al., 2017; Eger et al., 2017). Most of these studies view arguments as consisting of (at least) claims and premises—and so do we in this work. Thereby, our focus is on token-level argument component extraction, that is, the segmentation and typing of argument components. AM has thus far almost exclusively been performed monolingually, e.g. in English (Mochales-Palau and Moens, 2009), German (Eckle-Kohler et al., 2015), or Chinese (Li et al., 2017). Working only monolingually is"
C18-1071,P16-2067,0,0.0153708,"n, that is, the segmentation and typing of argument components in a target language (L2), while having only annotated source language (L1) data. We operate on token-level by labeling each token with a BIO label plus its respective component type. The BIO label marks the start, continuation and end of specific argument components. Examples are given in Tables 2 and 3. Cross-lingual sequence tagging POS tagging and named-entity recognition (NER) are standard tasks in NLP. In recent years, there is increased interest not only in evaluating POS and NER models within multiple individual languages (Plank et al., 2016), but also cross-lingually (Zhang et al., 2016; Tsai et al., 2016; Mayhew et al., 2017; Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations i"
C18-1071,P17-1049,0,0.0252093,"hours to translate the whole PE corpus into German, and the resulting overall cost was roughly 3,000 USD. The motivations to ask translators to translate argument components contiguously were that (i) all monolingual AM datasets we know of have contiguous components, (ii) transfer would have been naturally hampered had components in the source language been contiguous but not in the target language, at least for methods such as direct transfer.2 2 We note that even professional translations typically differ from original, non-translated texts because they retain traces of the source language (Rabinovich et al., 2017). We thus speculate that our reported results are probably slightly upward biased compared to a situation where the test data consists of original German student essays. This latter situation would have been much more costly to produce, in any way: it would have required retrieval (and, if necessary, creation) of original student essays in German as well as induction of all subsequent annotation mark-up. 834 To obtain further parallel versions of the PE data, we also automatically translated them into German, French, Spanish, and Chinese using Google Translate. Of course, we cannot make any de"
C18-1071,N18-2006,1,0.855743,"hen this is possible, and then project labels to the translated text. This eliminates the (particular) “OOV” and “ordering” problems inherent to direct transfer. Prerequiste to this approach is high quality MT, which, with the advent of neural techniques, appears to be now available. We hope our new datasets fuel AM research in languages other than English. In this work, we did not consider cross-lingual argumentative relation identification, although relations are available in the newly created parallel PE and CRC datasets. Future work should explore cross-lingual multi-task learning for AM (Schulz et al., 2018) with the source language as main task and small amounts of labeled target language data, as well as adversarial training techniques (Yasunaga et al., 2018), which promise to be beneficial for the particular OOV problem that direct transfer is prone to (though not for the ordering problem). We also want to combine projection with direct transfer by training on the union of projected L2 data as well as the original L1 data using shared representations. Acknowledgements This work has been supported by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 01"
C18-1071,J17-3005,1,0.940333,"for the first time. We initially experiment with available resources in English, German, and Chinese. We show that the existing datasets for analyzing argumentation are not suitable for assessing cross-lingual component extraction due to their heterogeneity or lack of complexity. Given this scarcity of homogeneously annotated high-quality largescale datasets across different languages, our first contribution is to (1) provide a fully parallel (en-de), human-translated version of one of the most popular current AM datasets, namely, the English dataset of persuasive student essays published by Stab and Gurevych (2017). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 831 Proceedings of the 27th International Conference on Computational Linguistics, pages 831–844 Santa Fe, New Mexico, USA, August 20-26, 2018. We then (2) machine translate the 402 student essays into German, Spanish, French, and Chinese. Both our human and machine translations contain argumentation annotations, in the form of either human annotations or automatically projected annotations. Our experiments indicate that both the translations and"
C18-1071,N12-1052,0,0.0561356,"Missing"
C18-1071,Q13-1001,0,0.0489631,"Missing"
C18-1071,K16-1022,0,0.0960127,"target language (L2), while having only annotated source language (L1) data. We operate on token-level by labeling each token with a BIO label plus its respective component type. The BIO label marks the start, continuation and end of specific argument components. Examples are given in Tables 2 and 3. Cross-lingual sequence tagging POS tagging and named-entity recognition (NER) are standard tasks in NLP. In recent years, there is increased interest not only in evaluating POS and NER models within multiple individual languages (Plank et al., 2016), but also cross-lingually (Zhang et al., 2016; Tsai et al., 2016; Mayhew et al., 2017; Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations in L2, there are also supervised cross-lingual approaches based on"
C18-1071,I05-3027,0,0.00813628,"mise scheme (Stab and Gurevych, 2017). We thus chose to include this dataset in our experiments, despite differences in the domain of the annotated texts. Li et al. (2017) used crowdsourcing to annotate Chinese hotel reviews from tripadvisor.com with four component types (major claim, claim, premise, premise supporting an implicit claim). We consider only those components with direct overlap with the components used by Stab and Gurevych (2017), thus considering components labeled as “premise supporting an implicit claim” as non-argumentative. We applied the CRF-based Chinese word segmenter by Tseng et al. (2005) to split Chinese character streams into tokens. Furthermore, we only use the “Easy Reviews Corpus” from Li et al. (2017). The remaining part of the corpus are isolated sentences from reviews with low overall inter-annotator agreement, which we ignored. An example from CRC can be found in Table 3. 3.3 A Large-Scale Parallel Dataset of Persuasive Essays (PE) Stab and Gurevych (2017) created a dataset of persuasive essays written by students on essaysforum.com. These are about controversial topics such as “competition or cooperation—which is better?”. To obtain a human-translated parallel versio"
C18-1071,P16-1157,0,0.0586659,"as sequential tagging of tokens, there are also important differences. For example, in POS tagging and NER, the label for a current token usually strongly depends on the token itself plus some local context. This strong association between label, token and local context is largely absent in AM, causing some models that perform well on POS and NER to fail blatantly in AM.1 Cross-lingual Word Embeddings are the (modern) basis of the direct transfer method. As with monolingual embeddings, there exists a veritable zoo of different approaches, but they often perform very similarly in applications (Upadhyay et al., 2016) and seemingly very different approaches are oftentimes also equivalent on a theoretical level (Ruder et al., 2017). 3 Data We chose three freely available datasets: a small parallel German-English dataset, and considerably larger English and Chinese datasets using (almost) the same inventory of argument types, which we therefore assumed to be adequate for cross-lingual experiments. We translated the two last named monolingual datasets in other languages, described below. Statistics for all datasets are given in Table 1. 1 E.g., we had tried out a word embedding based HMM model (Zhang et al.,"
C18-1071,P15-2118,0,0.0417021,"r step. Projection makes stronger assumptions than direct transfer: it requires parallel data.3 When the parallel data is induced via machine translation, then a second source of noise for projection is the ‘unreliable’ L2 input training data. Direct Transfer Here, we directly train a system on bilingual representations, which in our case come in the form of bilingual word embeddings. To retain some freedom over the choice and parameters of our word embeddings, we choose to train them ourselves instead of using pre-trained ones. For EN↔DE we induce bilingual word embeddings by training BIVCD (Vulic and Moens, 2015) and BISKIP models (Luong et al., 2015) on >2 million aligned sentences from the Europarl corpus (Koehn, 2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shuffles each concatenation and trains a standard monolingual word embedding technique on the result; here, we use the word2vec skipgram model (Mikolov et al., 2013). BIVCD was shown to be competitive to more challenging approaches in Upadhyay et al. (2016). BISKIP is a variant of the standard skip-gram model which predicts monoand cross-lingual contexts. It requires word alignments between parallel sentences a"
C18-1071,D17-1253,0,0.0205808,"considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/ coling2018-xling_argument_mining. 1 Introduction Argumentation mining (AM) is a fast-growing research field with applications in discourse analysis, summarization, debate modeling, and law, among others (Peldszus and Stede, 2013a). Recent studies have successfully applied computational methods to analyze monological argumentation (Wachsmuth et al., 2017; Eger et al., 2017). Most of these studies view arguments as consisting of (at least) claims and premises—and so do we in this work. Thereby, our focus is on token-level argument component extraction, that is, the segmentation and typing of argument components. AM has thus far almost exclusively been performed monolingually, e.g. in English (Mochales-Palau and Moens, 2009), German (Eckle-Kohler et al., 2015), or Chinese (Li et al., 2017). Working only monolingually is problematic, however, because AM is a difficult task even for humans due to its dependence on background knowledge and parsing"
C18-1071,H01-1035,0,0.738347,"ate the 402 student essays into German, Spanish, French, and Chinese. Both our human and machine translations contain argumentation annotations, in the form of either human annotations or automatically projected annotations. Our experiments indicate that both the translations and the projected annotations are of very high quality, cf. examples in Table 2. Besides contributing new datasets, (3) we perform the first evaluations of cross-lingual (token-level) AM, based on suitable adaptations of two popular cross-lingual techniques, namely, direct transfer (McDonald et al., 2011) and projection (Yarowsky et al., 2001). We find that projection works considerably better than direct transfer and almost closes the cross-lingual gap, i.e., cross-lingual performance is almost on par with in-language performance when we use parallel data and project annotations to the target language. This holds both for human (translated, HT) parallel data, which is costly to obtain, and machine translated (MT) parallel data, which is very cheap to obtain for dozens of high-resource languages. Our findings imply that current neural MT has reached a level where it can act as a substitute for costly (non-expert) HT even for proble"
C18-1071,N18-1089,0,0.0240798,"Missing"
C18-1071,N16-1156,0,0.477629,"x pragmatic relations (Moens, 2017). As a result, acquiring (high-quality) datasets for new languages comes at a high cost—be it in terms of training and/or hiring expert annotators or querying large crowds in crowd-sourcing experiments. It is thus of utmost importance to train NLP systems in AM that are capable of going cross-language, so that annotation efforts do not have to be multiplied by the number of languages of interest. This is in line with current trends in NLP, which increasingly recognize the possibility and the necessity to work cross-lingually, be it in part-of-speech tagging (Zhang et al., 2016), dependency parsing (Agic et al., 2016), sentiment mining (Chen et al., 2016; Zhou et al., 2016), or other fields. In this work, we address the problem of cross-lingual (token-level) AM for the first time. We initially experiment with available resources in English, German, and Chinese. We show that the existing datasets for analyzing argumentation are not suitable for assessing cross-lingual component extraction due to their heterogeneity or lack of complexity. Given this scarcity of homogeneously annotated high-quality largescale datasets across different languages, our first contribution i"
C18-1071,P16-1133,0,0.0251809,"ges comes at a high cost—be it in terms of training and/or hiring expert annotators or querying large crowds in crowd-sourcing experiments. It is thus of utmost importance to train NLP systems in AM that are capable of going cross-language, so that annotation efforts do not have to be multiplied by the number of languages of interest. This is in line with current trends in NLP, which increasingly recognize the possibility and the necessity to work cross-lingually, be it in part-of-speech tagging (Zhang et al., 2016), dependency parsing (Agic et al., 2016), sentiment mining (Chen et al., 2016; Zhou et al., 2016), or other fields. In this work, we address the problem of cross-lingual (token-level) AM for the first time. We initially experiment with available resources in English, German, and Chinese. We show that the existing datasets for analyzing argumentation are not suitable for assessing cross-lingual component extraction due to their heterogeneity or lack of complexity. Given this scarcity of homogeneously annotated high-quality largescale datasets across different languages, our first contribution is to (1) provide a fully parallel (en-de), human-translated version of one of the most popular cu"
C18-1071,L16-1561,0,0.0419904,"2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shuffles each concatenation and trains a standard monolingual word embedding technique on the result; here, we use the word2vec skipgram model (Mikolov et al., 2013). BIVCD was shown to be competitive to more challenging approaches in Upadhyay et al. (2016). BISKIP is a variant of the standard skip-gram model which predicts monoand cross-lingual contexts. It requires word alignments between parallel sentences and we use fast-align for this (Dyer et al., 2013). For EN↔ZH we train the same models on the UN corpus (Ziemski et al., 2016), which comprises >11 million parallel sentences. We train embeddings of sizes 100 and 200. Projection To implement projection for the problem of token-level AM, we proceed as follows. We take our human-labeled L1 data and align it with its corresponding parallel L2 data using fast-align. Once we have word level alignment information, we consider for each argument component c(s) in L1 of type a (e.g., MajorClaim, Claim, Premise) with consecutive words s1 , . . . , sN : the word t1 with smallest index in the corresponding L2 sentence that is aligned to some word in s1 , . . . , sN , and the ana"
C18-1132,E17-1005,0,0.144668,"antic tasks (NER, supersense tagging) on the higher levels show no improvement, they conclude that tasks need to be “sufficiently similar, e.g., all of syntactic nature” for multi-task learning to increase performance. Their conclusion is challenged by Hashimoto et al. (2017), who create a similar multi-task learning network in which lower layers predict syntactic tasks, while higher layers predict sentential relatedness and entailment. Their semantic tasks also improve when introducing shortcut connections, i.e., feeding the word representations into all layers of their network. In contrast, Alonso and Plank (2017) find generally mixed performance of MTL for semantic tasks. They also use syntactic tasks as low-level auxiliary tasks, but cannot improve performance over a single-task learning baseline for three out of five investigated semantic tasks (NER, supersense classification, frame identification). Schulz et al. (2018) apply MTL to another semantic task, argumentation mining. Instead of syntactic tasks as auxiliaries, they use diverse argumentation mining datasets from different domains. Similar to our tasks, annotation and labels vary across their different tasks due to inherent subjectivity and v"
C18-1132,W14-2302,0,0.181282,"eral language classification is the violation of selectional preferences (Wilks, 1978). It is used, e.g., in met* (Fass, 1991) to classify metaphors and metonyms. While the system distinguishes between both phenomena, it does so only after using the selectional preference information. In a related task, Horbach et al. (2016) employ this information for classifying idiomatic uses of infinitive verb compounds. Another feature used across different non-literal language detection tasks is topic information. While the work by Horbach et al. (2016) includes this feature for idiom detection, Beigman Klebanov et al. (2014) utilize it to classify metaphorically used tokens. Additionally, they make use of concreteness ratings, grounded in the Conceptual Metaphor Theory (Lakoff and Johnson, 1980). However, as argued in our introduction, concreteness is also useful for the detection of other kinds of non-literal language. For example, Zhang and Gelernter (2015) utilize such ratings to detect 1559 metonymy. Further, supersenses are employed to detect metaphors (Tsvetkov et al., 2014) or non-literal language in general (K¨oper and Schulte im Walde, 2017). One more feature that is often integrated is textual cohesion,"
C18-1132,C16-1013,0,0.10852,"are common ground in the first place or are many of them using arbitrary and mutually exclusive definitions? If the answer to this question is “yes”, then this would call for a re-thinking of a whole computational research area, and a return to safe grounds. To address our research questions, we apply multi-task learning (MTL) to four non-literal language datasets. Introduced by Caruana (1993), MTL has recently been used to improve a variety of NLP tasks with the help of other, auxiliary tasks. This includes chunking (Søgaard and Goldberg, 2016), using POS tagging, and spelling normalization (Bollmann and Søgaard, 2016), using orthographic data from other domains, but also semantic tasks like entailment (Hashimoto et al., 2017) or argument mining (Schulz et al., 2018). The underlying idea of MTL is that it is beneficial to learn several tasks jointly because of “spill-over” effects. Some researchers have claimed that a requirement for success in MTL is task relatedness. If this is true, then MTL is a formidable testbed for both of our research questions. We aim at four birds, namely: We consider detection of non-literal phenomena in four different datasets, regarding each as a separate task. These are (A) me"
C18-1132,W16-1104,1,0.884829,"test splits or usage of cross validation. The latter was too costly in terms of computation time for our tested architectures. Nonetheless, we include reference numbers for an approximate comparison. adj-noun-met is the only dataset for which we have the original test split. Here, the original feature based implementation of Tsvetkov et al. (2014) (F1 = 0.85) outperforms our approach (MTL: 0.63) by a large margin. We attribute this to heavy featureengineering on their part, using supersenses and concreteness information. In contrast, we perform on par with the state-of-the-art on tok-met (Do Dinh and Gurevych (2016): F1 = 0.56, our system: 1566 F1 = 0.56). They implement a simple MLP, incorporating also POS tags and concreteness features. Their test set is similarly large as ours. Horbach et al. (2016) report only accuracy (A = 0.86) for their cross validation experiments on inf-verb. Our system results are comparable to their approach, albeit in a different setup using a dedicated test set (A = 0.85 for MTL-de). However, as described in Section 2, Horbach et al. (2016) employ a multitude of semantic features, including selectional preferences and topic information. For part-verb, we do not reach the res"
C18-1132,J91-1003,0,0.140484,"andom forests and a variety of semantic features, including supersenses and concreteness values. The models are then used to classify metaphors in similarly annotated Spanish, Russian, and Farsi test sets. To the best of our knowledge, a combined detection of multiple non-literal phenomena has not been conducted before. This is surprising because common semantic features have already been used to classify different kinds of non-literal language. One such typical feature in non-literal language classification is the violation of selectional preferences (Wilks, 1978). It is used, e.g., in met* (Fass, 1991) to classify metaphors and metonyms. While the system distinguishes between both phenomena, it does so only after using the selectional preference information. In a related task, Horbach et al. (2016) employ this information for classifying idiomatic uses of infinitive verb compounds. Another feature used across different non-literal language detection tasks is topic information. While the work by Horbach et al. (2016) includes this feature for idiom detection, Beigman Klebanov et al. (2014) utilize it to classify metaphorically used tokens. Additionally, they make use of concreteness ratings,"
C18-1132,D17-1206,0,0.137069,"e answer to this question is “yes”, then this would call for a re-thinking of a whole computational research area, and a return to safe grounds. To address our research questions, we apply multi-task learning (MTL) to four non-literal language datasets. Introduced by Caruana (1993), MTL has recently been used to improve a variety of NLP tasks with the help of other, auxiliary tasks. This includes chunking (Søgaard and Goldberg, 2016), using POS tagging, and spelling normalization (Bollmann and Søgaard, 2016), using orthographic data from other domains, but also semantic tasks like entailment (Hashimoto et al., 2017) or argument mining (Schulz et al., 2018). The underlying idea of MTL is that it is beneficial to learn several tasks jointly because of “spill-over” effects. Some researchers have claimed that a requirement for success in MTL is task relatedness. If this is true, then MTL is a formidable testbed for both of our research questions. We aim at four birds, namely: We consider detection of non-literal phenomena in four different datasets, regarding each as a separate task. These are (A) metaphor detection in content tokens, (B) classification of metaphorical adjective-noun constructions, (C) detec"
C18-1132,L16-1135,0,0.421661,"arsi test sets. To the best of our knowledge, a combined detection of multiple non-literal phenomena has not been conducted before. This is surprising because common semantic features have already been used to classify different kinds of non-literal language. One such typical feature in non-literal language classification is the violation of selectional preferences (Wilks, 1978). It is used, e.g., in met* (Fass, 1991) to classify metaphors and metonyms. While the system distinguishes between both phenomena, it does so only after using the selectional preference information. In a related task, Horbach et al. (2016) employ this information for classifying idiomatic uses of infinitive verb compounds. Another feature used across different non-literal language detection tasks is topic information. While the work by Horbach et al. (2016) includes this feature for idiom detection, Beigman Klebanov et al. (2014) utilize it to classify metaphorically used tokens. Additionally, they make use of concreteness ratings, grounded in the Conceptual Metaphor Theory (Lakoff and Johnson, 1980). However, as argued in our introduction, concreteness is also useful for the detection of other kinds of non-literal language. Fo"
C18-1132,2005.mtsummit-papers.11,0,0.0702443,"Missing"
C18-1132,N16-1039,0,0.180947,"Missing"
C18-1132,E17-2086,0,0.169409,"Missing"
C18-1132,W15-1521,0,0.0483256,"Missing"
C18-1132,S16-2003,0,0.0378134,"d expression with lexicalized figurative sense). Much effort has been spent on the detection of metaphors, idioms, and general non-literal language use (Shutova, 2015). However, because of the named vague and subjective nature of these phenomena, a multitude of datasets using differing definitions has emerged in the process. Even when datasets address the same aspect of non-literality, they may use diverging or underspecified definitions; compare, e.g., the guidelines for annotating metaphors of Tsvetkov et al. (2014) (“[...] all words that, in your opinion, are used non-literally [...]”) and Mohammad et al. (2016) (“more complex; more distant from our senses; more abstract; more vague; ...”). The fuzziness of non-literality has two natural consequences: (1) training data is sparse, because different researchers may use diverging definitions, and hence may annotate different things rather than extend “the same story”; (2) high-quality training data is costly to obtain because there may be considerable disagreement among crowd-workers and even trained experts regarding the labels for different instances of (non-)literality. In this work, we address two research questions: This work is licensed under a Cr"
C18-1132,P17-1186,0,0.021685,"y Kahse (2017), a framework for multi-task learning sequence tagging generalizing the model of Søgaard and Goldberg (2016). An example of a two-task setup is shown in Figure 1. It uses English metaphor classification on a token level as the main task, and English metaphor detection in adjective-noun constructions as an auxiliary task (both described in Section 4). After an input layer that reads in word embeddings, there are multiple shared, bi-directional LSTM layers, thus implementing hard parameter sharing. The shared layers are followed by a number of fully connected task-specific layers (Peng et al., 2017), storing private information for each task. At their end, a softmax classifier predicts labels for each input token. In addition to using pre-trained word embeddings, the architecture incorporates character-level information to improve handling of out-of-vocabulary words. The framework can further be configured to terminate different tasks at different layers. We set this option to randomly use one of two scenarios: either all tasks use all BiLSTM layers, or all auxiliary tasks terminate one layer before the main task. 1560 Figure 1: Example setup for the sequence tagging MTL framework (Kahse"
C18-1132,W14-2303,0,0.0269138,"Missing"
C18-1132,N18-2006,1,0.879652,"s would call for a re-thinking of a whole computational research area, and a return to safe grounds. To address our research questions, we apply multi-task learning (MTL) to four non-literal language datasets. Introduced by Caruana (1993), MTL has recently been used to improve a variety of NLP tasks with the help of other, auxiliary tasks. This includes chunking (Søgaard and Goldberg, 2016), using POS tagging, and spelling normalization (Bollmann and Søgaard, 2016), using orthographic data from other domains, but also semantic tasks like entailment (Hashimoto et al., 2017) or argument mining (Schulz et al., 2018). The underlying idea of MTL is that it is beneficial to learn several tasks jointly because of “spill-over” effects. Some researchers have claimed that a requirement for success in MTL is task relatedness. If this is true, then MTL is a formidable testbed for both of our research questions. We aim at four birds, namely: We consider detection of non-literal phenomena in four different datasets, regarding each as a separate task. These are (A) metaphor detection in content tokens, (B) classification of metaphorical adjective-noun constructions, (C) detection of idiomatic use of infinitiveverb c"
C18-1132,schafer-bildhauer-2012-building,0,0.0395964,"Missing"
C18-1132,J15-4002,0,0.0199261,"ause of its non-lexicalized figurative meaning. Few would dispute the idiomaticity of “kicking the bucket”, as the non-literal meaning in this multi-word expression is largely conventionalized. However, in the sentence “One approach would be to draw the line by reference [...]” the expression “draw the line” could be classified as either metaphorical (because it still evokes the literal senses of its constituents) or idiomatic (as it is a fixed expression with lexicalized figurative sense). Much effort has been spent on the detection of metaphors, idioms, and general non-literal language use (Shutova, 2015). However, because of the named vague and subjective nature of these phenomena, a multitude of datasets using differing definitions has emerged in the process. Even when datasets address the same aspect of non-literality, they may use diverging or underspecified definitions; compare, e.g., the guidelines for annotating metaphors of Tsvetkov et al. (2014) (“[...] all words that, in your opinion, are used non-literally [...]”) and Mohammad et al. (2016) (“more complex; more distant from our senses; more abstract; more vague; ...”). The fuzziness of non-literality has two natural consequences: (1"
C18-1132,P16-2038,0,0.453474,"ction due to problem (2) above; • do existing datasets for non-literality share common ground in the first place or are many of them using arbitrary and mutually exclusive definitions? If the answer to this question is “yes”, then this would call for a re-thinking of a whole computational research area, and a return to safe grounds. To address our research questions, we apply multi-task learning (MTL) to four non-literal language datasets. Introduced by Caruana (1993), MTL has recently been used to improve a variety of NLP tasks with the help of other, auxiliary tasks. This includes chunking (Søgaard and Goldberg, 2016), using POS tagging, and spelling normalization (Bollmann and Søgaard, 2016), using orthographic data from other domains, but also semantic tasks like entailment (Hashimoto et al., 2017) or argument mining (Schulz et al., 2018). The underlying idea of MTL is that it is beneficial to learn several tasks jointly because of “spill-over” effects. Some researchers have claimed that a requirement for success in MTL is task relatedness. If this is true, then MTL is a formidable testbed for both of our research questions. We aim at four birds, namely: We consider detection of non-literal phenomena in"
C18-1132,E09-1086,0,0.0123563,"they make use of concreteness ratings, grounded in the Conceptual Metaphor Theory (Lakoff and Johnson, 1980). However, as argued in our introduction, concreteness is also useful for the detection of other kinds of non-literal language. For example, Zhang and Gelernter (2015) utilize such ratings to detect 1559 metonymy. Further, supersenses are employed to detect metaphors (Tsvetkov et al., 2014) or non-literal language in general (K¨oper and Schulte im Walde, 2017). One more feature that is often integrated is textual cohesion, e.g., in metaphor (Schulder and Hovy, 2014) and idiom detection (Sporleder and Li, 2009). The use of such common features suggests that different aspects of non-literality require similar information and that representation sharing may thus turn out beneficial. Introduced in the early nineties (Caruana, 1993), multi-task learning (MTL) has been more widely and successfully used in NLP recently (Ruder, 2017). MTL denotes a machine learning technique in which multiple tasks are trained in parallel in the same system, using a shared representation. The goal is to take advantage of commonalities between the different tasks. Bollmann and Søgaard (2016) use multi-task learning for hist"
C18-1132,P14-1024,0,0.248767,"ical (because it still evokes the literal senses of its constituents) or idiomatic (as it is a fixed expression with lexicalized figurative sense). Much effort has been spent on the detection of metaphors, idioms, and general non-literal language use (Shutova, 2015). However, because of the named vague and subjective nature of these phenomena, a multitude of datasets using differing definitions has emerged in the process. Even when datasets address the same aspect of non-literality, they may use diverging or underspecified definitions; compare, e.g., the guidelines for annotating metaphors of Tsvetkov et al. (2014) (“[...] all words that, in your opinion, are used non-literally [...]”) and Mohammad et al. (2016) (“more complex; more distant from our senses; more abstract; more vague; ...”). The fuzziness of non-literality has two natural consequences: (1) training data is sparse, because different researchers may use diverging definitions, and hence may annotate different things rather than extend “the same story”; (2) high-quality training data is costly to obtain because there may be considerable disagreement among crowd-workers and even trained experts regarding the labels for different instances of"
C18-1158,D16-1084,0,0.0341281,"y is crucial for the method’s future impact, we finally introduce a new evaluation dataset and evaluate how well the FNC-1 models generalize to unseen data from a different domain. In addition to in-domain experiments, we also conduct cross-domain experiments in order to analyze the transfer potential of a method. 2 Related Work Previous works in stance detection mostly considered target-specific stance prediction, whereby the stance of a text entity with respect to a topic or a named entity is determined. Target-specific stance prediction has been performed for tweets (Mohammad et al., 2016; Augenstein et al., 2016; Zarrella and Marsh, 2016) and online debates (Walker et al., 2012; Somasundaran and Wiebe, 2010; Sridhar et al., 2015). Such target-specific approaches are based on structural (Walker et al., 2012), linguistic and lexical features (Somasundaran and Wiebe, 2010) and they jointly model disagreement only and collective stance using probabilistic soft logic (Sridhar et al., 2015) or neural models (Zarrella and Marsh, 2016; Du et al., 2017) with conditional encoding (Augenstein et al., 2016). Stance prediction in tweets (Mohammad et al., 2016; Augenstein et al., 2016; Du et al., 2017) and in onli"
C18-1158,W17-4215,0,0.0532172,"Missing"
C18-1158,N16-1138,0,0.195845,"is different from that of stance detection in a news article, which – while similar – is concerned with stance detection of a news article relative to a statement in natural language. To the best of our knowledge, there is yet no overview or analysis paper on FNC-1 similar to the shared task on detecting stance in twitter (Mohammad et al., 2016; Derczynski et al., 2017; Taulé et al., 2017). To demonstrate the best scientific practices and achieve research transparency, we close this gap by systematically reviewing the top-ranked systems at FNC-1. The FNC-1 stance detection task is inspired by Ferreira and Vlachos (2016), who classify the stance of a single sentence of a news headline towards a specific claim. In FNC-1, however, the task is documentlevel stance detection, which requires the classification of an entire news article relative to a headline. The top performing system in FNC-1 is called SOLAT in the SWEN (Sean et al., 2017) by Talos Intelligence (henceforth: Talos). They use a combination of deep convolutional neural networks and gradient-boosted decision trees with lexical features. Team Athene (Hanselowski et al., 2017) won the second place with 1860 Dataset headlines documents tokens instances"
C18-1158,N18-1175,1,0.883098,"Missing"
C18-1158,I13-1191,0,0.105865,"nd Marsh, 2016) and online debates (Walker et al., 2012; Somasundaran and Wiebe, 2010; Sridhar et al., 2015). Such target-specific approaches are based on structural (Walker et al., 2012), linguistic and lexical features (Somasundaran and Wiebe, 2010) and they jointly model disagreement only and collective stance using probabilistic soft logic (Sridhar et al., 2015) or neural models (Zarrella and Marsh, 2016; Du et al., 2017) with conditional encoding (Augenstein et al., 2016). Stance prediction in tweets (Mohammad et al., 2016; Augenstein et al., 2016; Du et al., 2017) and in online debates (Hasan and Ng, 2013) is different from that of stance detection in a news article, which – while similar – is concerned with stance detection of a news article relative to a statement in natural language. To the best of our knowledge, there is yet no overview or analysis paper on FNC-1 similar to the shared task on detecting stance in twitter (Mohammad et al., 2016; Derczynski et al., 2017; Taulé et al., 2017). To demonstrate the best scientific practices and achieve research transparency, we close this gap by systematically reviewing the top-ranked systems at FNC-1. The FNC-1 stance detection task is inspired by"
C18-1158,N13-1132,0,0.0245125,"h regard to human performance. Therefore, we ask five human raters to manually label 200 instances. The raters reach an overall inter-annotator agreement of Fleiss’ κ = .686 (Fleiss, 1971), which is substantial and allows drawing tentative conclusions (Artstein and Poesio, 2008). However, when ignoring the UNR class, the inter-annotator agreement dramatically drops to κ = .218. This indicates that differentiating between the three related classes AGR, DSG, and DSC is difficult even for humans. On the basis of the annotation, we also determine the most probable stance labels according to MACE (Hovy et al., 2013), and compare them to the ground truth from the Emergent project. The agreement of the labels in this case is better, reaching a Fleiss’ κ of .807 overall and .552 for the three related classes. The MACE-based most probable label allows us to compute the human upper bound as F1 m = .754, which we include in Table 9 along with the upper bound per class F1 scores UNR = .997 AGR = .588, DSG =.667, and DSC = .765. 5 Analysis of models and features In this section, we first perform an error analysis in order to be able to find out what the three best performing models are learning and in which case"
C18-1158,P16-2064,0,0.0175329,"mmunity: 50 teams from both academia and industry participated. The goal of the FNC-1 challenge is to determine the perspective (or stance) of a news article relative to a given headline. An article’s stance can either agree or disagree with the headline, discuss the same topic, or it is completely unrelated. Table 1 shows four example documents illustrating these classes. Stance detection is a crucial building block for a variety of tasks, such as analyzing online debates (Walker et al., 2012; Sridhar et al., 2015; Somasundaran and Wiebe, 2010), determining the veracity of rumors on twitter (Lukasik et al., 2016; Derczynski et al., 2017), or understanding the argumentative structure of persuasive essays (Stab and Gurevych, 2017). While stance detection has been previously focused on individual sentences or phrases, the systems participating in FNC-1 have to detect the stance of an entire document, which raises many new challenges. Although the disagreeing article of Table 1 clearly leans against the headline’s claim, the fourth sentence would agree to it if considered in isolation. To properly learn from a scientific shared task, there are typically overview and analysis papers that compare the archi"
C18-1158,S16-1003,0,0.480975,"Missing"
C18-1158,S13-2053,0,0.0154881,"based on small word lists. Figure 1 indicates that COOC performs well, whereas both lexicon-based features are on par with the majority vote baseline. Challenge features. The three analyzed FNC-1 systems rely on combinations of the following features: Bag-of-words (BoW) unigram features, topic model features based on non-negative matrix factorization (NMF-300, NMF-cos) (Lin, 2007), Latent Dirichlet Allocation (LDA-cos) (Blei et al., 2001), Latent Semantic Indexing (LSI-300) (Deerwester et al., 1990), two lexicon-based features using NRC Hashtag Sentiment (NRC-Lex) and Sentiment140 (Sent140) (Mohammad et al., 2013), and word similarity features which measure the cosine similarity of pre-trained word2vec embeddings of nouns and verbs in the headlines and the documents (WSim). The topic models use 300 topics. Besides the concatenated topic vectors, we also consider the cosine similarity between the topics of document and headline (NMF-cos, LDA-cos). The BoW features perform best in terms of F1 m. While LSI-300, NMF-300 and NMF-cos topic models yield high scores, LDA-cos and WSim fall behind. Novel features. We also analyze a number of novel features for the FNC-1 task which have not been used in the chall"
C18-1158,W10-0204,0,0.0280756,"012), Coleman-Liau index (Mari and Ta Lin, 1975), automated readability index (Senter and Smith, 1967), LIX and RIX (Jonathan, 1983), McAlpine EFLAW Readability Score (McAlpine, 1997), and Strain Index (Solomon, 2006). However, in the present problem setting these features show only a low performance. The same is true for the lexical diversity (LexDiv) metrics, type-token ratio, and the measure of textual diversity (MTLD) (McCarthy, 2005). We finally analyze the performance of features based on the following lexicons: MPQA (Wilson et al., 2005), MaxDiff (Kiritchenko et al., 2014), and EmoLex (Mohammad and Turney, 2010). These features are based on the sentiment, polarity, and emotion expressed by headlines and documents, which might be good indicators of an author’s opinion. However, our results show that these lexicon-based features are not helpful. Even though the considered lexicons are important for fake-news detection (Shu et al. (2017), Horne and Adali (2017)), for stance detection, the properties captured by the lexicon-based features are not very useful. 1864 0.8 Majority vote FNC-1 baseline FNC-1 features Challenge Novel 0.9 0.8 0.7 0.7 0.6 0.6 0.5 0.5 F1 m FNC score Majority vote FNC-1 baseline FN"
C18-1158,D14-1162,0,0.0922311,"but when. ... Table 5: A correctly classified DSG instance by the stackLSTM on word embeddings and sequential encoding. Sequential processing of information is important in order to get the meaning of the whole sentence, e.g. ""It wasn’t long ago that Gary Bettman was ready to expand NHL."" VS. ""It was long ago that Gary Bettman wasn’t ready to expand NHL."" In Figure 2, we introduce this stackLSTM model, which combines the best feature set found in the ablation test with a stacked long short-term memory (LSTM) network (Hermans and Schrauwen, 2013). We use 50-dimensional GloVe word embeddings7 (Pennington et al., 2014) in order to generate sequences of word vectors of a headline–document pair. For this, we concatenate a maximum of 100 tokens of the headline and the document. These embedded word sequences v1 , v2 , . . . , vn are fed through two stacked LSTMs with a hidden state size of 100 with a dropout of 0.2 each. The last hidden state of the second LSTM is concatenated with the feature set and fed into a 3-layer neural network with 600 neurons each. Finally, we add a dense layer with four neurons and softmax activation function in order to retrieve the class probabilities. v1 v2 Embedded v3 token sequen"
C18-1158,S15-2078,0,0.0353994,"Missing"
C18-1158,W10-0214,0,0.382473,"atically detect fake news. The challenge received much attention in the NLP community: 50 teams from both academia and industry participated. The goal of the FNC-1 challenge is to determine the perspective (or stance) of a news article relative to a given headline. An article’s stance can either agree or disagree with the headline, discuss the same topic, or it is completely unrelated. Table 1 shows four example documents illustrating these classes. Stance detection is a crucial building block for a variety of tasks, such as analyzing online debates (Walker et al., 2012; Sridhar et al., 2015; Somasundaran and Wiebe, 2010), determining the veracity of rumors on twitter (Lukasik et al., 2016; Derczynski et al., 2017), or understanding the argumentative structure of persuasive essays (Stab and Gurevych, 2017). While stance detection has been previously focused on individual sentences or phrases, the systems participating in FNC-1 have to detect the stance of an entire document, which raises many new challenges. Although the disagreeing article of Table 1 clearly leans against the headline’s claim, the fourth sentence would agree to it if considered in isolation. To properly learn from a scientific shared task, th"
C18-1158,P15-1012,0,0.323412,"AI technology to automatically detect fake news. The challenge received much attention in the NLP community: 50 teams from both academia and industry participated. The goal of the FNC-1 challenge is to determine the perspective (or stance) of a news article relative to a given headline. An article’s stance can either agree or disagree with the headline, discuss the same topic, or it is completely unrelated. Table 1 shows four example documents illustrating these classes. Stance detection is a crucial building block for a variety of tasks, such as analyzing online debates (Walker et al., 2012; Sridhar et al., 2015; Somasundaran and Wiebe, 2010), determining the veracity of rumors on twitter (Lukasik et al., 2016; Derczynski et al., 2017), or understanding the argumentative structure of persuasive essays (Stab and Gurevych, 2017). While stance detection has been previously focused on individual sentences or phrases, the systems participating in FNC-1 have to detect the stance of an entire document, which raises many new challenges. Although the disagreeing article of Table 1 clearly leans against the headline’s claim, the fourth sentence would agree to it if considered in isolation. To properly learn fr"
C18-1158,J17-3005,1,0.784064,"perspective (or stance) of a news article relative to a given headline. An article’s stance can either agree or disagree with the headline, discuss the same topic, or it is completely unrelated. Table 1 shows four example documents illustrating these classes. Stance detection is a crucial building block for a variety of tasks, such as analyzing online debates (Walker et al., 2012; Sridhar et al., 2015; Somasundaran and Wiebe, 2010), determining the veracity of rumors on twitter (Lukasik et al., 2016; Derczynski et al., 2017), or understanding the argumentative structure of persuasive essays (Stab and Gurevych, 2017). While stance detection has been previously focused on individual sentences or phrases, the systems participating in FNC-1 have to detect the stance of an entire document, which raises many new challenges. Although the disagreeing article of Table 1 clearly leans against the headline’s claim, the fourth sentence would agree to it if considered in isolation. To properly learn from a scientific shared task, there are typically overview and analysis papers that compare the architectures, features, and results of the participating systems. To date, there is, however, no such paper for FNC-1, whic"
C18-1158,W17-4214,0,0.106651,"Missing"
C18-1158,N12-1072,0,0.258957,"r the development of AI technology to automatically detect fake news. The challenge received much attention in the NLP community: 50 teams from both academia and industry participated. The goal of the FNC-1 challenge is to determine the perspective (or stance) of a news article relative to a given headline. An article’s stance can either agree or disagree with the headline, discuss the same topic, or it is completely unrelated. Table 1 shows four example documents illustrating these classes. Stance detection is a crucial building block for a variety of tasks, such as analyzing online debates (Walker et al., 2012; Sridhar et al., 2015; Somasundaran and Wiebe, 2010), determining the veracity of rumors on twitter (Lukasik et al., 2016; Derczynski et al., 2017), or understanding the argumentative structure of persuasive essays (Stab and Gurevych, 2017). While stance detection has been previously focused on individual sentences or phrases, the systems participating in FNC-1 have to detect the stance of an entire document, which raises many new challenges. Although the disagreeing article of Table 1 clearly leans against the headline’s claim, the fourth sentence would agree to it if considered in isolation"
C18-1158,H05-1044,0,0.0435824,"ding ease (Kincaid et al., 1975), Gunning fog index (Štajner et al., 2012), Coleman-Liau index (Mari and Ta Lin, 1975), automated readability index (Senter and Smith, 1967), LIX and RIX (Jonathan, 1983), McAlpine EFLAW Readability Score (McAlpine, 1997), and Strain Index (Solomon, 2006). However, in the present problem setting these features show only a low performance. The same is true for the lexical diversity (LexDiv) metrics, type-token ratio, and the measure of textual diversity (MTLD) (McCarthy, 2005). We finally analyze the performance of features based on the following lexicons: MPQA (Wilson et al., 2005), MaxDiff (Kiritchenko et al., 2014), and EmoLex (Mohammad and Turney, 2010). These features are based on the sentiment, polarity, and emotion expressed by headlines and documents, which might be good indicators of an author’s opinion. However, our results show that these lexicon-based features are not helpful. Even though the considered lexicons are important for fake-news detection (Shu et al. (2017), Horne and Adali (2017)), for stance detection, the properties captured by the lexicon-based features are not very useful. 1864 0.8 Majority vote FNC-1 baseline FNC-1 features Challenge Novel 0."
C18-1158,S16-1074,0,0.0195571,"od’s future impact, we finally introduce a new evaluation dataset and evaluate how well the FNC-1 models generalize to unseen data from a different domain. In addition to in-domain experiments, we also conduct cross-domain experiments in order to analyze the transfer potential of a method. 2 Related Work Previous works in stance detection mostly considered target-specific stance prediction, whereby the stance of a text entity with respect to a topic or a named entity is determined. Target-specific stance prediction has been performed for tweets (Mohammad et al., 2016; Augenstein et al., 2016; Zarrella and Marsh, 2016) and online debates (Walker et al., 2012; Somasundaran and Wiebe, 2010; Sridhar et al., 2015). Such target-specific approaches are based on structural (Walker et al., 2012), linguistic and lexical features (Somasundaran and Wiebe, 2010) and they jointly model disagreement only and collective stance using probabilistic soft logic (Sridhar et al., 2015) or neural models (Zarrella and Marsh, 2016; Du et al., 2017) with conditional encoding (Augenstein et al., 2016). Stance prediction in tweets (Mohammad et al., 2016; Augenstein et al., 2016; Du et al., 2017) and in online debates (Hasan and Ng, 2"
C18-1158,S14-2077,0,0.0349565,"Missing"
C18-1197,D16-1235,0,0.0533128,"Missing"
C18-1197,W17-6809,0,0.0239639,"Missing"
C18-1197,P14-2065,0,0.0252912,"tion to which extent automatically derived image representations can contribute complementary information when combined with textual representations. Most multimodal research to date focuses on the representation of individual concepts (nouns) and their properties (adjectives). The benefit of multimodal representations for language tasks going beyond concept similarity needs to be examined in more detail from both, engineering and theoretical perspectives. Multimodal grounding of verbs Verbs play a fundamental role for expressing relations between concepts and their situational functionality (Hartshorne et al., 2014). The dynamic nature of verbs poses a challenge for multimodal grounding. To our best knowledge, only Hill et al. (2014) and Collell et al. (2017) consider verbs in their evaluation. They report that results for verbs are significantly worse, but 2332 Figure 3: Illustration for the quality of verb representations indicated as Spearman correlation between the cosine similarity of verbs and their corresponding similarity rating in the SimVerb dataset. do not elaborate on this finding. We present first steps towards an investigation of verb grounding.1 Figure 3 illustrate the quality of verb repr"
C18-1197,N18-1199,0,0.0186171,"rior performance is mainly due to a better representation of concepts. This raises the assumption that multimodal grounding should only be performed on selected words. Glavaˇs et al. (2017) propose to condition the inclusion of visual information on the prototypicality of a concept as measured by the image dispersion score (Kiela et al., 2014). This measure calculates the average pairwise cosine distance in a set of images to model the assumption that an image collection for an abstract concept like happiness is more diverse than for a concrete concept like ladder. Lazaridou et al. (2015) and Hessel et al. (2018) propose alternative concreteness measures based on the same idea. Unfortunately, these measures are highly dependent on the image retrieval algorithm which might be optimized towards obtaining a diverse range of images. Nevertheless, we assume that selective multimodal grounding constitutes a more plausible approach to sentence processing. Some functional words (e.g., locative expressions) might benefit from multimodal information, but it currently remains unclear how words with syntactic functions (e.g., coordinating expressions) should be represented visually. 6 Conclusion We analyzed how m"
C18-1197,D14-1032,0,0.0924497,"ations (cm1 , cm2 ) and minimizes the similarity for pairs with random target representations (cm1 , randomm2 ) has been shown to be a good choice for image labeling (Frome et al., 2013). In this task, the mapping approach is applied in the image-to-text direction to classify unknown objects in images based on their semantic similarity to known objects (Socher et al., 2013) . Lazaridou et al. (2014) and Collell et al. (2017) proceed in the reverse text-to-image direction to ground words in the visual world. Similar propagation approaches had already been examined by Johns and Jones (2012) and Hill and Korhonen (2014), but they used human-elicited perceptual features from the McRae dataset (McRae et al., 2005) instead of automatically derived image representations. Joint learning The mapping approaches assume a directed transformation from one modality to the other. Joint estimation approaches aim to learn shared representations instead (Figure 2b). An approach inspired by topic modeling interprets aligned data as a multimodal document and uses Latent Dirichlet Allocation to derive multimodal topics (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte Im Walde, 2013)."
C18-1197,Q14-1023,0,0.337397,"neficial for modeling concrete words, whereas Anderson et al. (2017) conclude that textual models sufficiently integrate visual properties. Further interdisciplinary research involving computer science, neuroscience, and psycholinguistics is required to obtain a deeper understanding of cognitively plausible language processing (Embick and Poeppel, 2015). 4.2 Grounding phrases Most experiments for conceptual grounding indicate that providing a multimodal representation for abstract concepts is significantly more challenging due to the lack of perceptual patterns associated with abstract words (Hill et al., 2014). For grounding phrases, the meaning for concrete and abstract concepts 2330 need to be combined (see Section 3.3). Bruni et al. (2012) examine the compositional meaning of color adjectives and find that multimodal representations are superior in modeling color. However, they fail to distinguish between literal and non-literal usage of color adjectives (e.g., green cup vs green future). Vivid imagery and synaesthetic associations play an important role in the interpretation of figurative language. In their influential theory of metaphor, Lakoff and Johnson (1980) argue that abstract concepts c"
C18-1197,J15-4004,0,0.0379944,"tention for conceptual grounding, but perceptual information from the auditory and the olfactory channel have also been used for dedicated tasks (Kiela et al., 2015; Kiela and Clark, 2017). In order to provide a more concrete discussion, we focus on the combination of textual and visual cues for the remainder of the survey. The quality of concept representations is commonly evaluated by their ability to model semantic properties. Different approaches to learning conceptual models are compared by their performance on similarity datasets, e.g., WordSim353 (Finkelstein et al., 2002), SimLex-999 (Hill et al., 2015), MEN (Bruni et al., 2012), SemSim, and VisSim (Silberer and Lapata, 2014)). These datasets contain pairs of words that have been annotated with similarity scores for the two concepts. Several evaluations of semantic models have shown that multimodal concept representations outperform unimodal ones (Feng and Lapata, 2010; Silberer and Lapata, 2012; Bruni et al., 2014; Kiela et al., 2014). Kiela et al. (2016) perform a comparison of different image sources and architectures and their ability to model semantic similarity. Despite the advantages of multimodal models in capturing semantic relation"
C18-1197,D14-1005,0,0.0167493,"d visual features (Silberer and Lapata, 2012; Roller and Schulte Im Walde, 2013). Conveniently, integrating knowledge from different modalities has been facilitated due to the now common low-level representations of the input (also known as embeddings). Images are represented as groups of pixels, videos as series of image frames, audio data as windows of waveform samples, and language as sequences of distributional word representations. These values are then fed into a neural network that learns to compress and normalize the representation such that it better generalizes across input samples (Kiela and Bottou, 2014). A concept representation is usually obtained by averaging over many different samples for the concept (e.g., the concept bird is represented by averaging over the representation for n images showing a bird). The representations are expressed as high-dimensional matrices which can be projected into a common space. This approach facilitates a joint information flow between different modalities and has contributed to the growing success of multimodal processing. 2328 Fusion The most intuitive approach is multimodal fusion (Figure 2a). Assuming that a unimodal vector representation v for the con"
C18-1197,P14-2135,0,0.123833,"their ability to model semantic properties. Different approaches to learning conceptual models are compared by their performance on similarity datasets, e.g., WordSim353 (Finkelstein et al., 2002), SimLex-999 (Hill et al., 2015), MEN (Bruni et al., 2012), SemSim, and VisSim (Silberer and Lapata, 2014)). These datasets contain pairs of words that have been annotated with similarity scores for the two concepts. Several evaluations of semantic models have shown that multimodal concept representations outperform unimodal ones (Feng and Lapata, 2010; Silberer and Lapata, 2012; Bruni et al., 2014; Kiela et al., 2014). Kiela et al. (2016) perform a comparison of different image sources and architectures and their ability to model semantic similarity. Despite the advantages of multimodal models in capturing semantic relations, it remains an open question whether they contribute to a cognitively more plausible approximation of human conceptual grounding. Bulat et al. (2017b) and Anderson et al. (2017) conduct experiments to label brain activity scans by human subjects with the corresponding concepts that elicited the brain activity. They compare different distributional semantic models and obtain mixed resul"
C18-1197,P15-2038,0,0.0237562,"ion. The challenges that arise from these efforts are discussed in Section 5. 4.1 Grounding concepts Multimodal concept representations are motivated by the idea that semantic relations between words are grounded in perception. Being able to assess semantic relations between concepts is an important prerequisite for modeling generalization capabilities in language processing. The combination of the textual and the visual modality has received most attention for conceptual grounding, but perceptual information from the auditory and the olfactory channel have also been used for dedicated tasks (Kiela et al., 2015; Kiela and Clark, 2017). In order to provide a more concrete discussion, we focus on the combination of textual and visual cues for the remainder of the survey. The quality of concept representations is commonly evaluated by their ability to model semantic properties. Different approaches to learning conceptual models are compared by their performance on similarity datasets, e.g., WordSim353 (Finkelstein et al., 2002), SimLex-999 (Hill et al., 2015), MEN (Bruni et al., 2012), SemSim, and VisSim (Silberer and Lapata, 2014)). These datasets contain pairs of words that have been annotated with s"
C18-1197,D16-1043,0,0.168878,"Missing"
C18-1197,P14-1132,0,0.0214948,"sentation in m1 : cm2 ∼ f (cm1 ). The choice of the similarity and the loss measures for learning the mapping function vary. A max-margin optimization which maximizes the similarity between true pairs of concept representations (cm1 , cm2 ) and minimizes the similarity for pairs with random target representations (cm1 , randomm2 ) has been shown to be a good choice for image labeling (Frome et al., 2013). In this task, the mapping approach is applied in the image-to-text direction to classify unknown objects in images based on their semantic similarity to known objects (Socher et al., 2013) . Lazaridou et al. (2014) and Collell et al. (2017) proceed in the reverse text-to-image direction to ground words in the visual world. Similar propagation approaches had already been examined by Johns and Jones (2012) and Hill and Korhonen (2014), but they used human-elicited perceptual features from the McRae dataset (McRae et al., 2005) instead of automatically derived image representations. Joint learning The mapping approaches assume a directed transformation from one modality to the other. Joint estimation approaches aim to learn shared representations instead (Figure 2b). An approach inspired by topic modeling"
C18-1197,N15-1016,0,0.23457,"cRae et al., 2005) instead of automatically derived image representations. Joint learning The mapping approaches assume a directed transformation from one modality to the other. Joint estimation approaches aim to learn shared representations instead (Figure 2b). An approach inspired by topic modeling interprets aligned data as a multimodal document and uses Latent Dirichlet Allocation to derive multimodal topics (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte Im Walde, 2013). Unfortunately, this approach cannot be easily used for zero shot learning. Lazaridou et al. (2015) enrich the skip-gram model by Mikolov et al. (2013) with visual features. Their model optimizes two constraints: the representation of concepts c with respect to their textual contexts (unsupervised skip-gram objective in m1 ) and the similarity between word representations and their visual counterparts (supervised max-margin objective for (cm1 , cm2 )). In their approach, the visual representations remain fixed, but the textual representations are learned from scratch. Silberer and Lapata (2014) go one step further and use stacked multimodal autoencoders to simultaneously learn good represen"
C18-1197,N16-1043,0,0.04676,"Missing"
C18-1197,I11-1162,0,0.0337907,"is approach facilitates a joint information flow between different modalities and has contributed to the growing success of multimodal processing. 2328 Fusion The most intuitive approach is multimodal fusion (Figure 2a). Assuming that a unimodal vector representation v for the concept c and the modalities m1 and m2 exists, the multimodal representation vmm consists of the concatenation a of the two vectors weighted by a tunable parameter α: vmm (c) = α · vm1 (c) a (1 − α) · vm2 (c). The concatenation occurs directly on the concept level and is thus called feature-level fusion or early fusion (Leong and Mihalcea, 2011; Bruni et al., 2011). In the case of pure concatenation, the unimodal representations reside in separate conceptual spaces. The concatenated representation for cat could give us the information that cat is visually similar to panther and textually similar to dog, but it is not possible to determine cross-modal similarity. In order to smooth the concatenated representations while maintaining multimodal correlations, dimensionality reduction techniques such as singular value decomposition (Bruni et al., 2014) or canonical correlation analysis (Silberer and Lapata, 2012) have been applied. 3.2 P"
C18-1197,N18-1078,0,0.0215739,"t to associate natural language descriptions of actions with perceptual input from its sensors. For experiments on grounded language understanding, the situational environment is usually artificially restricted to a very small domain. This confined setting facilitates the analysis of compositional expressions and their referential interpretation as complex object descriptions or action sequences. In open-domain language understanding, semantic disambiguation is even more challenging. Approaches using multimodal information for the disambiguation of concepts (Xie et al., 2017), named entities (Moon et al., 2018), and sentences (Botschen et al., 2018; Shutova et al., 2017) show promising tendencies, but the underlying compositional principles are not yet understood. 5 Challenges for grounded language processing Multimodal grounding of language has been a longstanding goal of language researchers. The discussion has gained new momentum due to the recent developments in learning distributed multimodal representations. Most evaluations indicate that multimodal representations are beneficial for a variety of tasks, but explanatory analyses of this effect are still in a developing phase. In this section, w"
C18-1197,D14-1162,0,0.0916264,"in besonderer Schwerpunkt wird dabei auf die multimodale konzeptuelle Verankerung von Verben gelegt, da diese eine wichtige kompositorische Funktion erf¨ullen. 1 Introduction Natural languages are continually developing constructs that include numerous variations and irregularities. Modeling the subtleties of language in a formal, processable way has driven computational linguistics for decades. In recent years, distributional approaches have become the most widely accepted solution to model the associative character of word meaning (Harris, 1954; Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014). These approaches learn word representations in a high-dimensional vector space based on context patterns in large text collections. Machine learning researchers aim at reducing external knowledge to an absolute minimum and simply interpret language as a continuous stream of characters. From an engineering perspective, these data-driven approaches are highly attractive because they reduce the need of domain experts. From a cognitive perspective, processing language in isolation without information on situational context seems to be an overly artificial setup. Human acquisition of semantic rep"
C18-1197,Q13-1003,0,0.0222283,"dou et al., 2017a). Psycholinguistic research indicates that conceptual mapping modulated by visual properties is not only relevant for first language acquisition, but is also used as a means to establish cross-lingual links in foreign language learning (Beinborn et al., 2014). Bergsma and Van Durme (2011) and Vuli´c et al. (2016) take advantage of this observation and use multimodal representations to induce multilingual representations. Grounding sequences in actions Situational grounding of action descriptions requires the representation of sequences and their compositional interpretation. Regneri et al. (2013) build a corpus that grounds descriptions of actions in videos showing these actions. For the interpretation of sequences, evaluating verbs and their arguments plays a fundamental role. Yatskar et al. (2016) developed the imSitu dataset which consists of images depicting verbs and annotations which link the verb arguments to visual referents. This dataset can be used for the multimodal task of situation recognition (Mallya and Lazebnik, 2017; Zellers and Choi, 2017), and it serves as a multimodal resource for verb processing. Grounding verbs is particularly challenging because of the variety o"
C18-1197,D13-1115,0,0.0579217,"Missing"
C18-1197,W16-4318,1,0.743331,"nt multimodal processing (Figure 1c). For 2327 (a) Multimodal fusion. Concatenate known representations from modality A and B and apply dimensionality reduction. (b) Mapping. Learn a mapping function f from modality A to B that can be applied on unknown concepts cn . (c) Joint learning. Optimize two objectives simultaneously: quality of unimodal representations and crossmodal alignment. Figure 2: Methods for learning multimodal representations. Blue and yellow shapes indicate the representation space of modality A and B. emotion recognition (Morency et al., 2011) or persuasiveness prediction (Santos et al., 2016), the actual content of an utterance and paraverbal cues (e.g., pitch, facial expression) need to be jointly evaluated. An ironic tone of voice might reverse the conceptual interpretation of the language content. Recent work from the vision community goes one step further and tackles tasks that imperatively require an interactive flow of information. In visual question answering, a human user can ask questions about an image that the system should answer (Malinowski et al., 2015). This requires several steps: understanding the question, determining the salient elements in the image, interpreti"
C18-1197,N16-1020,0,0.110709,"t learning of multimodal representations is very popular in the vision community (Karpathy et al., 2014; Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011). 3.3 Compositional representations For tasks that require representing longer sequences, a na¨ıve approach is sequence-level fusion. In this setting, the unimodal sequence representation is obtained by performing an arithmetic operation (e.g., 2329 average, max) over the concept representations for each word in the sequence. Multimodal fusion is then performed on this averaged representation (Glavaˇs et al., 2017; Bruni et al., 2014). Shutova et al. (2016) work with short phrases consisting of two words and directly learn phrase representations. Missing concept representations in one modality can be obtained by mapping functions (Botschen et al., 2018). For image captioning approaches, representations for a pair of an image and the corresponding caption are learned jointly (Kiros et al., 2014; Socher et al., 2014). Pre-trained unimodal representations are fed into a neural network which is trained with the max-margin objective to distinguish between true and false captions for an image. The multimodal sequence representation can be obtained fro"
C18-1197,S17-1018,0,0.160584,"th perceptual input from its sensors. For experiments on grounded language understanding, the situational environment is usually artificially restricted to a very small domain. This confined setting facilitates the analysis of compositional expressions and their referential interpretation as complex object descriptions or action sequences. In open-domain language understanding, semantic disambiguation is even more challenging. Approaches using multimodal information for the disambiguation of concepts (Xie et al., 2017), named entities (Moon et al., 2018), and sentences (Botschen et al., 2018; Shutova et al., 2017) show promising tendencies, but the underlying compositional principles are not yet understood. 5 Challenges for grounded language processing Multimodal grounding of language has been a longstanding goal of language researchers. The discussion has gained new momentum due to the recent developments in learning distributed multimodal representations. Most evaluations indicate that multimodal representations are beneficial for a variety of tasks, but explanatory analyses of this effect are still in a developing phase. In this section, we discuss open challenges that arise from existing work. For"
C18-1197,D12-1130,0,0.540208,"ethods for obtaining joint representations computationally. 3 Multimodal representation learning Multimodal representations combine information from separate modalities. We discuss methods for representing known concepts, projecting information to represent unknown concepts, and for combining concept representations into compositional representations for sequences. 3.1 Concept representations Even in unimodal tasks, researchers experiment with many different variations of representing concepts and their relations. Earlier work on multimodal representations used human-elicited visual features (Silberer and Lapata, 2012; Roller and Schulte Im Walde, 2013). Conveniently, integrating knowledge from different modalities has been facilitated due to the now common low-level representations of the input (also known as embeddings). Images are represented as groups of pixels, videos as series of image frames, audio data as windows of waveform samples, and language as sequences of distributional word representations. These values are then fed into a neural network that learns to compress and normalize the representation such that it better generalizes across input samples (Kiela and Bottou, 2014). A concept represent"
C18-1197,P14-1068,0,0.276771,"Schulte Im Walde, 2013). Unfortunately, this approach cannot be easily used for zero shot learning. Lazaridou et al. (2015) enrich the skip-gram model by Mikolov et al. (2013) with visual features. Their model optimizes two constraints: the representation of concepts c with respect to their textual contexts (unsupervised skip-gram objective in m1 ) and the similarity between word representations and their visual counterparts (supervised max-margin objective for (cm1 , cm2 )). In their approach, the visual representations remain fixed, but the textual representations are learned from scratch. Silberer and Lapata (2014) go one step further and use stacked multimodal autoencoders to simultaneously learn good representations for each modality (unsupervised reconstruction objective for m1 and m2 ) and their optimal multimodal combination (supervised classification objective for (cm1 , cm2 )). Both approaches implicitly also learn a mapping between the two modalities and can be adjusted to induce a directional projection for zero shot learning. Joint learning of multimodal representations is very popular in the vision community (Karpathy et al., 2014; Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011). 3.3"
C18-1197,Q14-1017,0,0.043579,"forming an arithmetic operation (e.g., 2329 average, max) over the concept representations for each word in the sequence. Multimodal fusion is then performed on this averaged representation (Glavaˇs et al., 2017; Bruni et al., 2014). Shutova et al. (2016) work with short phrases consisting of two words and directly learn phrase representations. Missing concept representations in one modality can be obtained by mapping functions (Botschen et al., 2018). For image captioning approaches, representations for a pair of an image and the corresponding caption are learned jointly (Kiros et al., 2014; Socher et al., 2014). Pre-trained unimodal representations are fed into a neural network which is trained with the max-margin objective to distinguish between true and false captions for an image. The multimodal sequence representation can be obtained from the last hidden layer of the network. The introduction of attention variables can function as a mediator between the visual and the textual modality (see Section 2.2). For a more detailed overview of multimodal sequence representations in the vision community, the interested reader is referred to Wu et al. (2017). The approaches for compositional representation"
C18-1197,D11-1063,0,0.0187781,". However, they fail to distinguish between literal and non-literal usage of color adjectives (e.g., green cup vs green future). Vivid imagery and synaesthetic associations play an important role in the interpretation of figurative language. In their influential theory of metaphor, Lakoff and Johnson (1980) argue that abstract concepts can be grounded metaphorically in embodied and situated knowledge. They assume that metaphors can be understood as a mapping from a concrete source domain to a more abstract target domain. For example, time is often viewed as a stream that flows in a direction. Turney et al. (2011) operationalize this theoretical account by identifying metaphoric phrases based on the discrepancy in concreteness of source and target term. Shutova et al. (2016) and Bulat et al. (2017a) build on their approach and use multimodal models for identifying metaphoric word usage in adjective-noun combinations. They show that words used in a metaphorical combination (dry wit) exhibit less similarity than words in non-metaphorical phrases (dry skin). We strongly believe that progress in multimodal compositional grounding will pave the way for a more holistic understanding of figurative language pr"
C18-1197,P16-2031,0,0.0461309,"Missing"
C18-1197,D17-1099,0,0.0221579,"tions Situational grounding of action descriptions requires the representation of sequences and their compositional interpretation. Regneri et al. (2013) build a corpus that grounds descriptions of actions in videos showing these actions. For the interpretation of sequences, evaluating verbs and their arguments plays a fundamental role. Yatskar et al. (2016) developed the imSitu dataset which consists of images depicting verbs and annotations which link the verb arguments to visual referents. This dataset can be used for the multimodal task of situation recognition (Mallya and Lazebnik, 2017; Zellers and Choi, 2017), and it serves as a multimodal resource for verb processing. Grounding verbs is particularly challenging because of the variety of their possible visual instantiations. For example, an image of an adult drinking beer has very little in common with a zebra drinking water. Multimodal interpretation of sequences is highly relevant for robotics research (Chaplot et al., 2017). Mordatch and Abbeel (2017) examine the emergence of compositionality in grounded multi-agent communication. The language learned by artificial agents is not necessarily interpretable by humans. Lazaridou et al. (2017b) show"
C18-1280,C16-1236,0,0.104831,"ing Wikidata. The depicted graph structure consists of entities and relations from the KB and the special q-node. Any entity in the KB that can take the place of the q-node will be a part of the answer. In this paper, we describe a semantic parsing approach to the problem of KB QA. That is, for each input question, we construct an explicit structural semantic parse (semantic graph), as in Figure 1. Semantic parses can be deterministically converted to a query to extract the answers from the KB. Similar graphical representations were used in previous work (Yih et al., 2015; Reddy et al., 2016; Bao et al., 2016). However, the modern semantic parsing approaches usually focus either on the syntactic analysis of the input question (Reddy et al., 2016) or on detecting individual KB relations (Yu et al., 2017), whereas the structure of the semantic parse is ignored or only approximately modeled. Reddy et al. (2016) use the syntactic structure of the question to build all possible semantic parses and then apply a linear model with manually defined features to choose the correct parse. A subset of their features encodes basic information about the graph structure of the semantic parse (e.g. number of nodes)"
C18-1280,P14-1133,0,0.143916,"Missing"
C18-1280,D13-1160,0,0.549103,"r model with manually defined features to choose the correct parse. A subset of their features encodes basic information about the graph structure of the semantic parse (e.g. number of nodes). The state-of-the-art approach of Yih et al. (2015) and Bao et al. (2016) restricts all semantic parses to a single core relation and a small set of constraints that can be added to it. Their system uses manual features for the constraints and a similarity score between the core relation and the question to model the semantic parses. The abovementioned systems were evaluated on the WebQuestions data set (Berant et al., 2013). Figure 2 plots results for the state-of-the-art systems by the number of relations that needs to be identified to get the correct answer to a question.2 For example, the question in Figure 1 requires two relations This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.wikidata.org/ 2 We include results for the most recent systems that have published the output on the WebQuestions data set. We compute the number of needed relations from the manual semantic parses provided by Yih et al. ("
C18-1280,D14-1179,0,0.0639551,"Missing"
C18-1280,P15-1026,0,0.40621,"n the ple question opposite direction (add argmax/argmin) adds a new edge with either argmax or argmin sorting constraint to the semantic graph: am (E, F) = {E ∪ r(q, argmax), F; E ∪ r(q, argmin), F |r ∈ Rd }, where Rd is a set of KB relation types that allow dates as values. Our semantic graph construction process allows to effectively search the space of possible graphs for a given question through an iterative application of the defined actions A on the last state S t (see, for example, Figure 4). 3 Representation learning We follow the state-of-the-art approaches for QA (Yih et al., 2015; Dong et al., 2015; Bao et al., 2016) and learn representations for the question and every possible semantic graph. Then we use a simple reward function γ to judge if a semantic graph is a correct semantic parse of the input question. Below we describe the architectures that we use to learn the representations for questions and graphs. 3.1 Deep Convolutional Networks We use Deep Convolutional Neural Networks (DCNN) to learn a representation for a question. DCNNs have been proven effective for constructing sentence-level representations on a variety of NLP tasks, including named entity recognition (Strubell et a"
C18-1280,W17-1804,0,0.0226027,"odel. 6 Related work We have focused on the problem of the increasing error rates for complex questions and the encoding of the semantic graph structure. In this paper, we describe a semantic parsing system for KB QA and follow the approach of Yih et al. (2015) who do not rely on syntactic parsing to construct semantic parses. Our semantic graphs do not cover some aspects of the first-order logic, such as negation. Reddy et al. (2016) define a semantic parser that builds first-order logic representations from syntactic dependencies. They further specify how it can be extended with negation in Fancellu et al. (2017). We only train on the WebQSP-WD data set and we note that more data might be necessary to effectively train the gated graph architecture. Reddy et al. (2014) suggest an unsupervised learning method to learn a model from a large web corpus, while Su et al. (2016) use patterns and crowdsourcing to create new data with specific properties. These techniques can be used to further improve the performance of our model. An alternative solution to semantic parsing is to build an information extraction pipeline that views the question as a query and the KB as a source of relevant information (Yao et a"
C18-1280,N16-2016,0,0.126646,"gure 1 requires two relations This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.wikidata.org/ 2 We include results for the most recent systems that have published the output on the WebQuestions data set. We compute the number of needed relations from the manual semantic parses provided by Yih et al. (2016). 3306 Proceedings of the 27th International Conference on Computational Linguistics, pages 3306–3317 Santa Fe, New Mexico, USA, August 20-26, 2018. Princess Leia Yih et al. (2015) Jain (2016) Reddy et al. (2016) Berant and Liang (2014) 0.6 F-score HOME WORLD q INSTANCE OF 0.5 0.4 0.3 planet 1 Figure 1: Semantic graph for an example question “What is Princess Leia’s home planet?” 2 3 4 Figure 2: F-score QA results from previous work by the number of relations needed to find the correct answer to find the answer. It can be clearly seen in Figure 2 that the lack of structure modeling in the modern approaches results in a worse performance on more complex questions that require more than one relation. We claim that one needs to explicitly model the semantic structure to be able to fin"
C18-1280,D17-1159,0,0.0347524,"rom the KB. However, such approaches are hard to analyze for errors or to modify with explicit constraints. For example, it is not directly possible to implement the temporal sorting constraint (argmax). We apply GGNNs to the problem of semantic parsing. Li et al. (2016) have developed the gated architecture based on the graph neural network formulation of Scarselli et al. (2009). Recently, a slightly different design of Graph Convolutional Networks was proven effective on a KB completion task (Schlichtkrull et al., 2018). Kipf and Welling (2017) introduced Graph Convolutional Networks, while Marcheggiani and Titov (2017) employed them for natural language processing for the first time and compared them to other formulations. Graph Convolutional Networks have a similar gated architecture and share most of the same properties with the Gated Graph Neural Networks used. 7 Conclusions In this work, we have used Gated Graph Neural Networks to encode the structure of the target semantic parse for KB QA. We have shown that disregarding the semantic structure leads to a falling performance on questions that require complex semantic parses to get the correct answers. Our GGNN architecture was able to successfully model"
C18-1280,D17-1252,0,0.272542,"s between a known entity and the q-node. Thus, the semantic graphs defined here are a superset of the query graphs in Yih et al. (2015) and allow us to model more complex meanings. Consequently, they also correspond to the formalized representations used by Reddy et al. (2014) and the simple λ-DCS (Berant et al., 2013), since those were the foundation for the query graphs (Yih et al., 2015). 2.2 Semantic graph construction Yih et al. (2015) defined a step-by-step staged graph generation that does not need full syntactic parses and was also adopted in subsequent publications (Bao et al., 2016; Peng et al., 2017). We use the same procedure as Yih et al. (2015) to construct semantic graphs with a set of modifications that allow us to build more expressive and complex graphs. We define a set of states and a set of actions that can be applied at each state to extend the current semantic graph. A state is a tuple of a graph and a set of free entities: S = (E, F), where the graph is E, as defined above, and F = {e|e ∈ E}. The set of free entities F is the entities that were identified in the question but were not yet added to the graph.4 The first state is a tuple of an empty graph with no edges and a set"
C18-1280,D14-1162,0,0.0936364,"named entity recognition (Strubell et al., 2017) and KB QA (Yih et al., 2015; Dong et al., 2015). The DCNN architecture is depicted in Figure 5, where it is used to map an input question to a fixed-size vector representation. The input question is first tokenized and the special start and end tokens are added to the sequence: x = {hsi, x1 , x2 . . . xn , hf i}. Next, we map the tokens in x to dw -dimensional pre-trained word embeddings, using a matrix Wglove ∈ R|Vw |×dw , where |Vw |is the size of the vocabulary. We use 50-dimensional GloVe embeddings that were trained on a 6 billion corpus (Pennington et al., 2014). The sequence of word embeddings is further processed by an array of CNN layers. We apply a pooling operation after the last CNN layer and transform the output with a fully connected layer H and a ReLU non-linearity. We take the resulting vector vq as the representation of the question. 3.2 Gated Graph Neural Networks Gate Graph Neural Networks (GGNN) process graphs by iteratively updating representations of graph nodes based on the neighboring nodes and relations (Li et al., 2016). We adopt GGNNs for semantic parsing to learn a vector representation of a semantic graph. Li et al. (2016) give"
C18-1280,Q14-1030,0,0.278701,"he query graphs of Yih et al. (2015) and their extension in Bao et al. (2016), the key difference being that we do not differentiate between the core relation and modifiers, but rather allow graphs that have multiple independent relations. We also allow relations to attach to other nodes rather than the q-node, enabling longer relation paths between a known entity and the q-node. Thus, the semantic graphs defined here are a superset of the query graphs in Yih et al. (2015) and allow us to model more complex meanings. Consequently, they also correspond to the formalized representations used by Reddy et al. (2014) and the simple λ-DCS (Berant et al., 2013), since those were the foundation for the query graphs (Yih et al., 2015). 2.2 Semantic graph construction Yih et al. (2015) defined a step-by-step staged graph generation that does not need full syntactic parses and was also adopted in subsequent publications (Bao et al., 2016; Peng et al., 2017). We use the same procedure as Yih et al. (2015) to construct semantic graphs with a set of modifications that allow us to build more expressive and complex graphs. We define a set of states and a set of actions that can be applied at each state to extend the"
C18-1280,D17-1283,0,0.0214064,"et al., 2015; Bao et al., 2016) and learn representations for the question and every possible semantic graph. Then we use a simple reward function γ to judge if a semantic graph is a correct semantic parse of the input question. Below we describe the architectures that we use to learn the representations for questions and graphs. 3.1 Deep Convolutional Networks We use Deep Convolutional Neural Networks (DCNN) to learn a representation for a question. DCNNs have been proven effective for constructing sentence-level representations on a variety of NLP tasks, including named entity recognition (Strubell et al., 2017) and KB QA (Yih et al., 2015; Dong et al., 2015). The DCNN architecture is depicted in Figure 5, where it is used to map an input question to a fixed-size vector representation. The input question is first tokenized and the special start and end tokens are added to the sequence: x = {hsi, x1 , x2 . . . xn , hf i}. Next, we map the tokens in x to dw -dimensional pre-trained word embeddings, using a matrix Wglove ∈ R|Vw |×dw , where |Vw |is the size of the vocabulary. We use 50-dimensional GloVe embeddings that were trained on a 6 billion corpus (Pennington et al., 2014). The sequence of word em"
C18-1280,D16-1054,0,0.0269416,"not rely on syntactic parsing to construct semantic parses. Our semantic graphs do not cover some aspects of the first-order logic, such as negation. Reddy et al. (2016) define a semantic parser that builds first-order logic representations from syntactic dependencies. They further specify how it can be extended with negation in Fancellu et al. (2017). We only train on the WebQSP-WD data set and we note that more data might be necessary to effectively train the gated graph architecture. Reddy et al. (2014) suggest an unsupervised learning method to learn a model from a large web corpus, while Su et al. (2016) use patterns and crowdsourcing to create new data with specific properties. These techniques can be used to further improve the performance of our model. An alternative solution to semantic parsing is to build an information extraction pipeline that views the question as a query and the KB as a source of relevant information (Yao et al., 2014). Dong et al. (2015) and Jain (2016) construct a vector representation for the question and use it to directly score candidate answers from the KB. However, such approaches are hard to analyze for errors or to modify with explicit constraints. For exampl"
C18-1280,P15-1049,0,0.263743,"ase and are not compatible with Wikidata. We use the more recent STAGG approach to position the graph models against the previous work and the feature-based models. 4.3 Training the model To train the model, we need positive pairs of questions and semantic graphs. Since WebQSP does not contain semantic parses for Wikidata, we use weak supervision as suggested in Berant et al. (2013). Specifically, we follow Yih et al. (2015) and run our semantic graph construction procedure to create training instances (see Section 2.2). We use the state-of-the-art S-MART entity linking system for noisy data (Yang and Chang, 2015) to extract a set of entities F from each question.6 Instead of scoring the semantic graphs with the model, we evaluate each graph against Wikidata and compare the extracted answers to the manual answers in the data set. The semantic graphs that result in a correct answer are stored as positive training instances and the rest of the graphs generated during the same process are used as negative instances. Due to the differences between Freebase and Wikidata, some question can not be answered exactly using Wikidata. We generate positive semantic graphs for 1945 questions out of 2880 (see Table 1"
C18-1280,W14-2416,0,0.032146,"Missing"
C18-1280,P15-1128,0,0.0725357,"e example question could be modeled using Wikidata. The depicted graph structure consists of entities and relations from the KB and the special q-node. Any entity in the KB that can take the place of the q-node will be a part of the answer. In this paper, we describe a semantic parsing approach to the problem of KB QA. That is, for each input question, we construct an explicit structural semantic parse (semantic graph), as in Figure 1. Semantic parses can be deterministically converted to a query to extract the answers from the KB. Similar graphical representations were used in previous work (Yih et al., 2015; Reddy et al., 2016; Bao et al., 2016). However, the modern semantic parsing approaches usually focus either on the syntactic analysis of the input question (Reddy et al., 2016) or on detecting individual KB relations (Yu et al., 2017), whereas the structure of the semantic parse is ignored or only approximately modeled. Reddy et al. (2016) use the syntactic structure of the question to build all possible semantic parses and then apply a linear model with manually defined features to choose the correct parse. A subset of their features encodes basic information about the graph structure of th"
C18-1280,P16-2033,0,0.166696,"t al., 2013). Figure 2 plots results for the state-of-the-art systems by the number of relations that needs to be identified to get the correct answer to a question.2 For example, the question in Figure 1 requires two relations This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.wikidata.org/ 2 We include results for the most recent systems that have published the output on the WebQuestions data set. We compute the number of needed relations from the manual semantic parses provided by Yih et al. (2016). 3306 Proceedings of the 27th International Conference on Computational Linguistics, pages 3306–3317 Santa Fe, New Mexico, USA, August 20-26, 2018. Princess Leia Yih et al. (2015) Jain (2016) Reddy et al. (2016) Berant and Liang (2014) 0.6 F-score HOME WORLD q INSTANCE OF 0.5 0.4 0.3 planet 1 Figure 1: Semantic graph for an example question “What is Princess Leia’s home planet?” 2 3 4 Figure 2: F-score QA results from previous work by the number of relations needed to find the correct answer to find the answer. It can be clearly seen in Figure 2 that the lack of structure modeling in the mode"
C18-1280,P17-1053,0,0.0525325,"wer. In this paper, we describe a semantic parsing approach to the problem of KB QA. That is, for each input question, we construct an explicit structural semantic parse (semantic graph), as in Figure 1. Semantic parses can be deterministically converted to a query to extract the answers from the KB. Similar graphical representations were used in previous work (Yih et al., 2015; Reddy et al., 2016; Bao et al., 2016). However, the modern semantic parsing approaches usually focus either on the syntactic analysis of the input question (Reddy et al., 2016) or on detecting individual KB relations (Yu et al., 2017), whereas the structure of the semantic parse is ignored or only approximately modeled. Reddy et al. (2016) use the syntactic structure of the question to build all possible semantic parses and then apply a linear model with manually defined features to choose the correct parse. A subset of their features encodes basic information about the graph structure of the semantic parse (e.g. number of nodes). The state-of-the-art approach of Yih et al. (2015) and Bao et al. (2016) restricts all semantic parses to a single core relation and a small set of constraints that can be added to it. Their syst"
C18-2002,pianta-etal-2008-textpro,0,0.0490422,", rejections, etc.). WebAnno (Yimam et al., 2014) integrates an automation mode in which the system can learn from annotations made by the user and provide suggestions. However, retraining has to be triggered manually by an administrator. Also, it uses a non-modular backend that provides only one machine learning algorithm and does not support active learning. WebAnno presents the document to be annotated and the recommended annotations separately in a split-screen mode which makes it tedious to relate recommendations to already existing annotations. The general approach described by Emanuele Pianta and Zanoli (2008) who integrate an active learning process with an existing annotation tool and the ability to call out to different machine learning backends for recommendations as well as Prodi.gy2 are similar to our approach. However, they focus strongly on the active learning aspect and force the user to follow the lead of the active learning module, restricting the user’s workflow. In INCEpTION, the active learning algorithm highlights a particular recommendation to be judged by the user, but does not prevent the user from making other annotations. The web-based tool AlvisAE (Papazian et al., 2012) suppor"
C18-2002,N06-4006,0,0.0181285,"s Prodi.gy2 are similar to our approach. However, they focus strongly on the active learning aspect and force the user to follow the lead of the active learning module, restricting the user’s workflow. In INCEpTION, the active learning algorithm highlights a particular recommendation to be judged by the user, but does not prevent the user from making other annotations. The web-based tool AlvisAE (Papazian et al., 2012) supports both linking entity mentions to a knowledge base and editing knowledge bases (with limitations), but it does not support recommendations or active learning. Knowtator (Ogren, 2006) is another instance of a desktop application which ships as an annotation plugin for an ontology building tool. However, single-user tools like the ones above do not meet today’s demand for collaboration-oriented annotation tools. 3 INCEpTION – System Overview INCEpTION offers a number of functionalities expected from a generic annotation platform: a versatile and yet intuitive user interface, flexible configuration of the annotation schema, the ability to run multiple annotation projects concurrently for multiple users and workflow-support with annotation and adjudication stages, etc. With r"
C18-2002,W12-3621,0,0.0374643,"Missing"
C18-2002,S18-2007,1,0.795254,"tors. A pre-trained deep learning model is integrated as an external recommender and is used during annotation. The annotators that use INCEpTION in conjunction with the recommenders report the usefulness and improvement in annotation speed and quality. EDoHa. Stahlhut et al. (2018) have created a hypothesis validation tool using INCEpTION. It features a hypothesis/evidence editor which allows users to create hypotheses and link evidence in the form of text paragraphs to it. Knowledge-driven rntity ranking. In order to support users during entity linking, the re-ranking approach described in (Sorokin and Gurevych, 2018) was adapted and integrated into INCEpTION. It is used as a recommender and in the auto-suggestion box for the named entity layer. As part of the collaborations with the above use-cases, INCEpTION logs the users’ actions in order to investigate for instance which assistive features (i.e. recommenders) work best for the respective tasks, whether they introduce a bias in the annotator’s results, and how to improve the user interface for an improved user experience. 3 Spring Boot: https://projects.spring.io/spring-boot/ 8 5 Conclusion and Future Work In this paper, we have presented INCEpTION whi"
C18-2002,P14-5016,1,0.890445,"Missing"
chiarcos-etal-2012-open,rehm-etal-2008-ontology,1,\N,Missing
chiarcos-etal-2012-open,kemps-snijders-etal-2008-isocat,0,\N,Missing
chiarcos-etal-2012-open,W11-0122,1,\N,Missing
chiarcos-etal-2012-open,ide-etal-2008-masc,0,\N,Missing
chiarcos-etal-2012-open,J08-3010,0,\N,Missing
chiarcos-etal-2012-open,W07-1501,0,\N,Missing
chiarcos-etal-2012-open,C04-1074,0,\N,Missing
chiarcos-etal-2012-open,P98-1013,0,\N,Missing
chiarcos-etal-2012-open,C98-1013,0,\N,Missing
chiarcos-etal-2012-open,W09-3021,0,\N,Missing
chiarcos-etal-2012-open,2005.mtsummit-papers.11,0,\N,Missing
chiarcos-etal-2012-open,chiarcos-2012-ontologies,1,\N,Missing
chiarcos-etal-2012-open,I11-1099,1,\N,Missing
chiarcos-etal-2012-open,vossen-etal-2008-integrating,0,\N,Missing
cholakov-etal-2014-lexical,E12-1039,0,\N,Missing
cholakov-etal-2014-lexical,W02-0816,0,\N,Missing
cholakov-etal-2014-lexical,P06-1057,0,\N,Missing
cholakov-etal-2014-lexical,N13-1133,1,\N,Missing
D07-1060,W02-2006,0,0.0324426,"Missing"
D07-1060,P06-1046,0,0.0283785,"ombining written text with a published thesaurus to measure distance between concepts (or word senses) using distributional measures, thereby eliminating sense-conflation and achieving results better than the simple word-distance measures and indeed also most of the WordNet-based semantic measures. We called these measures distributional measures of concept-distance. Concept-distance 2 LSA is especially expensive as singular value decomposition, a key component for dimensionality reduction, requires computationally intensive matrix operations; making it less scalable to large amounts of text (Gorman and Curran, 2006). 572 measures can be used to measure distance between a word pair by choosing the distance between their closest senses. Thus, even though ‘children’s recreation’ is the predominant sense of play, the ‘drama’ sense is much closer to actor and so their distance will be chosen. These distributional conceptdistance approaches need to create only V × C cooccurrence and C × C distance matrices, where C is the number of categories or senses (usually about 1000). It should also be noted that unlike the best WordNet-based measures, distributional measures (both word- and concept-distance ones) can be"
D07-1060,I05-1067,1,0.955571,"ke resources that these methods require do not exist for most of the 3000–6000 languages in existence today and they are costly to create. In this paper, we introduce cross-lingual distributional measures of concept-distance, or simply cross-lingual measures, that determine the distance between a word pair belonging to a resource-poor language using a knowledge source in a resourcerich language and a bilingual lexicon3 . We will use the cross-lingual measures to calculate distances between German words using an English thesaurus and a German corpus. Although German is not resourcepoor per se, Gurevych (2005) has observed that the German wordnet GermaNet (Kunze, 2004) (about 60,000 synsets) is less developed than the English WordNet (Fellbaum, 1998) (about 117,000 synsets) with respect to the coverage of lexical items and lexical semantic relations represented therein. On the other hand, substantial raw corpora are available for the German language. Crucially for our evaluation, the existence of GermaNet allows comparison of our cross-lingual approach with monolingual ones. 2 Monolingual Distributional Measures In order to set the context for cross-lingual conceptdistance measures (Section 3), we"
D07-1060,O97-1002,0,0.740037,"guistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an English thesaurus to create English– German distributional profiles of concepts, which in turn will be used to measure the semantic distance between German words. Two classes of methods have been used in determining semantic distance. Semantic measures of concept-distance, such as those of Jiang and Conrath (1997) and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it (see Budanitsky and Hirst (2006) for a survey). Distributional measures of word-distance1 , such as cosine and α-skew divergence (Lee, 2001), deem 1 Many distributional approaches represent the sets of contexts of the target words as points in multidimensional cooccurrence space or as co-occurrence distributions. A measure, such as cosine, that captures vector distance or a measure, such as α-skew divergence, that captures distance between distributions"
D07-1060,S07-1004,0,0.0788107,"Missing"
D07-1060,W04-2607,1,0.78437,"e synset and in synsets close to it in the network. 576 (2005) and Zesch et al. (2007) asked native German speakers to mark two different sets of German word pairs with distance values. Set 1 (Gur65) consists of a German translation of the English Rubenstein and Goodenough (1965) dataset. It has 65 noun– noun word pairs. Set 2 (Gur350) is a larger dataset containing 350 word pairs made up of nouns, verbs, and adjectives. The semantically close word pairs in Gur65 are mostly synonyms or hypernyms (hyponyms) of each other, whereas those in Gur350 have both classical and non-classical relations (Morris and Hirst, 2004) with each other. Details of these semantic distance benchmarks13 are summarized in Table 2. Inter-subject correlations are indicative of the degree of ease in annotating the datasets. 4.1.2 Results and Discussion Word-pair distances determined using different distance measures are compared in two ways with the two human-created benchmarks. The rank ordering of the pairs from closest to most distant is evaluated with Spearman’s rank order correlation ρ; the distance judgments themselves are evaluated with Pearson’s correlation coefficient r. The higher the correlation, the more accurate the me"
D07-1060,2003.mtsummit-papers.42,0,0.0345828,"n raw text and possibly some shallow syntactic processing. They do not require any other manually-created resource, and tend to have a higher coverage. However, by themselves they perform poorly when compared to semantic measures (Mohammad and Hirst, 2006b) because when given a target word pair we usually need the distance between their closest senses, but distributional measures of word-distance tend to conflate the distances between all possible sense pairs. Latent semantic analysis (LSA) (Landauer et al., 1998) has also been used to measure distributional distance with encouraging results (Rapp, 2003). However, it too measures the distance between words and not senses. Further, the dimensionality reduction inherent to LSA has the effect of making the predominant sense more dominant while de-emphasizing the other senses. Therefore, an LSA-based approach will also conflate information from the different senses, and even more emphasis will be placed on the predominant senses. Given the semantically close target nouns play and actor, for example, a distributional measure will give a score that is some sort of a dominance-based average of the distances between their senses. The noun play has th"
D07-1060,P98-2127,0,0.234235,"Missing"
D07-1060,P04-1036,0,0.211979,"cooccurs with any word. A statistic such as PMI can then give the strength of association between w and c. with each of its senses is summed. The sense that has the highest cumulative association is chosen as the intended sense. A new bootstrapped WCCM is created such that each cell mi j , corresponding to en word wen i and concept c j , is populated with the en number of times wi co-occurs with any word used in sense cen j . Mohammad and Hirst (2006a) used the DPCs created from the bootstrapped WCCM to attain nearupper-bound results in the task of determining word sense dominance. Unlike the McCarthy et al. (2004) dominance system, our approach can be applied to much smaller target texts (a few hundred sentences) without the need for a large similarly-sensedistributed text5 . In Mohammad and Hirst (2006a), the DPC-based monolingual distributional measures of concept-distance were used to rank word pairs by their semantic similarity and to correct realword spelling errors, attaining markedly better results than monolingual distributional measures of word-distance. In the spelling correction task, the distributional concept-distance measures performed better than all WordNet-based measures as well, excep"
D07-1060,P06-1040,0,0.0290057,". Nachbildung (replica) b. Doppelkinn (double chin) d. Zweitschrift (copy) Our approach to evaluating distance measures fol14 In Table 3, all values are statistically significant at the 0.01 level (2-tailed), except for the one in italic (0.212), which is significant at the 0.05 level (2-tailed). 15 English translations are in parentheses. 577 lows that of Jarmasz and Szpakowicz (2003), who evaluated semantic similarity measures through their ability to solve synonym problems (80 TOEFL (Landauer and Dumais, 1997), 50 ESL (Turney, 2001), and 300 (English) Reader’s Digest Word Power questions). Turney (2006) used a similar approach to evaluate the identification of semantic relations, with 374 college-level multiple-choice word analogy questions. The Reader’s Digest Word Power (RDWP) benchmark for German consists of 1072 of these word-choice problems collected from the January 2001 to December 2005 issues of the Germanlanguage edition (Wallace and Wallace, 2005). We discarded 44 problems that had more than one correct answer, and 20 problems that used a phrase instead of a single term as the target. The remaining 1008 problems form our evaluation dataset, which is significantly larger than any of"
D07-1060,E06-1016,1,0.581452,"all performance is again better than the best monolingual measures. 1 Introduction Accurately estimating the semantic distance between concepts or between words in context has pervasive applications in computational linguistics, including machine translation, information retrieval, speech recognition, spelling correction, and text categorization (see Budanitsky and Hirst (2006) for discussion), and it is becoming clear that basing such measures on a combination of corpus statistics with a knowledge source, such as a dictionary, published thesaurus, or WordNet, can result in higher accuracies (Mohammad and Hirst, 2006b). This is because such knowledge sources capture semantic information about concepts and, to some extent, world knowledge. They also act as sense inventories for the words in a language. However, applying algorithms for semantic distance to most languages is hindered by the lack of linguistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an Eng"
D07-1060,W06-1605,1,0.605764,"all performance is again better than the best monolingual measures. 1 Introduction Accurately estimating the semantic distance between concepts or between words in context has pervasive applications in computational linguistics, including machine translation, information retrieval, speech recognition, spelling correction, and text categorization (see Budanitsky and Hirst (2006) for discussion), and it is becoming clear that basing such measures on a combination of corpus statistics with a knowledge source, such as a dictionary, published thesaurus, or WordNet, can result in higher accuracies (Mohammad and Hirst, 2006b). This is because such knowledge sources capture semantic information about concepts and, to some extent, world knowledge. They also act as sense inventories for the words in a language. However, applying algorithms for semantic distance to most languages is hindered by the lack of linguistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an Eng"
D07-1060,N07-2052,1,0.781373,"Missing"
D07-1060,C92-2070,0,\N,Missing
D07-1060,J06-1003,1,\N,Missing
D09-1139,P09-1082,1,0.810838,"econd CKB we use is Wiktionary which is a multilingual dictionary and an affiliated project of Wikipedia. It resembles WordNet by containing synonym and hyponym information. It also contains information usually not found in LKBs like abbreviations, compounds, contractions, and the etymology of words. The 171 language-specific 1339 editions of Wiktionary contain more than 5 million entries. Note that each language-specific edition contains not only entries for words of that particular language, but also for words of foreign languages. Wiktionary has been used in IR (M¨uller and Gurevych, 2008; Bernhard and Gurevych, 2009) and other tasks like sentiment analysis (Chesley et al., 2006) or ontology learning (Weber and Buitelaar, 2006). In our experiments we used the Wiktionary dump of Oct 16, 2007. 2.3 For WordNet, the glosses and example sentences of the synsets are used. Wiktionary does not contain glosses for all entries due to instance incompleteness. Therefore, a concatenation of selected information from each entry is used. See Zesch et al. (2008) for details. 319115 400194 256.23 50 117 2.44 Table 1: Statistics of the test data (after preprocessing). 3 Semantic Relatedness Measure A wide range of methods f"
D09-1139,P07-1130,1,0.884493,"Missing"
D09-1139,W98-0704,0,0.0460717,"ets impossible for the user to anticipate the terms that occur in all relevant documents, but not in non-relevant ones. The use of semantic knowledge for improving IR by compensating non-optimal queries has been a field of study for a long time. First experiments on query expansion by Voorhees (1994) using lexical-semantic relations extracted from a linguistic knowledge base (LKB), namely WordNet (Fellbaum, 1998), showed sginificant improvements in performance only for manually selected expansion terms. The combination of WordNet with thesauri built from the underlying document collections by Mandala et al. (1998) improved the performance on several test collections. Mandala et al. (1998) identified missing relations, especially cross part of speech relations and insufficient lexical coverage as reasons for the low performance improvement when using only WordNet. In recent work, collaborative knowledge bases (CKB) like Wikipedia have been used in IR for judging the document relevance by computing the semantic relatedness (SR) of queries and documents (Gurevych et al., 2007; Egozi et al., 2008; M¨uller and Gurevych, 2008) and have shown promising results. These resources have a high coverage of general"
D09-1139,N07-2052,1,0.900314,"Missing"
D09-1139,J06-1003,0,\N,Missing
D10-1101,P07-1056,0,0.911815,"Conference on Empirical Methods in Natural Language Processing, pages 1035–1045, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics Random Fields (CRF) (Lafferty et al., 2001) have been successfully applied to several IE tasks in the past (Peng and McCallum, 2006). A recurring problem, which arises when working with supervised approaches, concerns the domain portability. In the opinion mining context this question has been prominently investigated with respect to opinion polarity analysis (sentiment analysis) in previous research (Aue and Gamon, 2005; Blitzer et al., 2007). Terms as “unpredictable” can express a positive opinion when uttered about the storyline of a movie but a negative opinion when the handling of a car is described. Hence the effects of training and testing a machine learning algorithm for sentiment analysis on data from different domains have been analyzed in previous research. However to the best of our knowledge, these effects have not been investigated regarding the extraction of opinion targets. The contribution of this paper is a CRF-based approach for opinion targets extraction which tackles the problem of domain portability. We first"
D10-1101,N07-1039,0,0.0100056,"488 in the task of extracting opinion target - opinion expression pairs. Kessler and Nicolov (2009) solely focus on identifying which opinion expression is linked to which opinion target in a sentence. They present a dataset of car and camera reviews in which opinion expressions and opinion targets are annotated. Starting with this information, they train a machine learning classifier for identifying related opinion expressions and targets. Their algorithm receives the opinion expression and opinion target annotations as input during runtime. The classifier is evaluated using the algorithm by Bloom et al. (2007) as a baseline. The support vector machine based approach by Kessler and Nicolov (2009) yields an F-Measure of 0.698, outperforming the baseline which yields an F-Measure of 0.445. 2.3 Domain Adaptation in Opinion Mining The task of creating a supervised algorithm, which when trained on data from domain A, also performs well on data from another domain B, is a domain adaptation problem (Daum´e III and Marcu, 2006; Jiang and Zhai, 2007). Aue and Gamon (2005) have investigated this challenge very early in the task of document level sentiment classification (positive / 1037 negative). They observ"
D10-1101,H05-1045,0,0.0157753,"mparison to our approach. Since three of the features we employed in 1044 Rec 0.277 0.307 0.384 0.394 0.381 0.327 0.381 Rec 0.339 0.382 0.408 0.395 0.396 0.435 0.405 F-Me 0.422 0.479 0.499 0.489 0.492 0.518 0.496 cameras Rec 0.330 0.303 0.369 0.339 0.376 0.376 0.378 - F-Me 0.399 0.391 0.475 0.421 0.472 0.483 0.474 - our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse. Acknowledgments The project was funded by means of the German Federal Ministry of Economy and Technology under the promotional reference “01MQ07012”. The authors take the responsibility for the con"
D10-1101,P07-1034,0,0.359679,"argets. Their algorithm receives the opinion expression and opinion target annotations as input during runtime. The classifier is evaluated using the algorithm by Bloom et al. (2007) as a baseline. The support vector machine based approach by Kessler and Nicolov (2009) yields an F-Measure of 0.698, outperforming the baseline which yields an F-Measure of 0.445. 2.3 Domain Adaptation in Opinion Mining The task of creating a supervised algorithm, which when trained on data from domain A, also performs well on data from another domain B, is a domain adaptation problem (Daum´e III and Marcu, 2006; Jiang and Zhai, 2007). Aue and Gamon (2005) have investigated this challenge very early in the task of document level sentiment classification (positive / 1037 negative). They observe that increasing the amount of training data raises the classification accuracy, but only if the training data is from one source domain. Increasing the training data by mixing domains does not yield any consistent improvements. Blitzer et al. (2007) introduce an extension to a structural correspondence learning algorithm, which was specifically designed to address the task of domain adaptation. Their enhancement aims at identifying p"
D10-1101,W06-0301,0,0.0231272,"Missing"
D10-1101,H05-1043,0,0.203779,"Missing"
D10-1101,P10-1059,1,0.426407,"ments, we use the CRF implementation from the Mallet toolkit3 . 4 Experimental Setup 4.1 In our experiments, we employ datasets from three different sources, which span four domains in total (see Table 1). All of them consist of reviews collected from Web 2.0 sites. The first dataset consists of reviews for 20 different movies collected from the Internet Movie Database. It was presented in Zhuang et al. (2006) and annotated regarding opinion target - opinion expression pairs. The second dataset consists of 234 reviews for two different web-services collected from epinions.com, as described in Toprak et al. (2010). The third dataset is an extended version of the data presented in Kessler and Nicolov (2009). The authors have provided us with additional documents, which have been annotated in the meantime. The version of the dataset used in our experiments consists of 179 blog postings regarding different digital cameras and 336 reviews of different cars. In the description of their annotation guidelines, Kessler and Nicolov (2009) refer to opinion targets as mentions. Mentions are all aspects of the review topic, which can be targets of expressed opinions. However, not only mentions which occur as opini"
D10-1101,H05-2017,0,\N,Missing
D13-1055,S12-1059,1,0.767277,"Missing"
D13-1055,E12-1036,0,0.413161,"of the same text. This scenario implies certain characteristics for a well-designed feature set as we will demonstrate in this study. The main contributions of this paper are: First, we introduce a novel feature set for edit category classification. Second, we evaluate the performance of this feature set on different tasks within a corpus of Wikipedia edits. We propose the new task of edit category classification and show that our model is able to classify edits from a 21-category taxonomy. Furthermore, our model achieves state-of-theart performance in a fluency edit classification task 579 (Bronner and Monz, 2012). Third, we analyze collaboration patterns based on edit categories on two subsets of Wikipedia articles, namely featured and non-featured articles. We detect correlations between collaboration patterns and high-quality articles. This is demonstrated by the fact that featured articles have a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles. The rest of this paper is structured as follows. In Section 2, we motivate our experiments based on previous work. Section 3 explains our training data and the features we use for the machine learning"
D13-1055,W12-4006,0,0.071313,"manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lanFigure 1: An example edit from WPEC labeled with R EFERENCE -M, as displayed by Wikimedia’s diff page tool. guage model (word n-grams). In their experiments, character-level features and named entity features show the highest improvement o"
D13-1055,N13-1055,0,0.286819,"ewly created articles in the online encyclopedia declined in the last few years (Suh et al., 2009), the number of edits in existing articles is rather stable.1 It is reasonable to assume that the latter will not change in the near future. One of the major reasons for the popularity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activity. Wikipedia’s revision history stores all changes made to any page in the encyclopedia in separate revisions. Previous studies have exploited revision history data in tasks such as preposition error correction (Cahill et al., 2013), spelling error correction (Zesch, 2012) or paraphrasing (Max and Wisniewski, 2010). However, they all use different approaches to extract the information needed for their task. Ferschke et al. (2013) outline several applications benefiting from revision history data. They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy. In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classificat"
D13-1055,C12-1044,1,0.818265,"or paraphrasing (Max and Wisniewski, 2010). However, they all use different approaches to extract the information needed for their task. Ferschke et al. (2013) outline several applications benefiting from revision history data. They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy. In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classification taxonomy developed in previous work (Daxenberger and Gurevych, 2012). This taxonomy enables a finegrained analysis of edit activity in revision histories. We present the results from an automatic classification experiment, based on an annotated corpus of edits in the English Wikipedia. Additional information necessary to reproduce our results, including word lists and training, development and test data, is released online.2 To the best of our knowledge, this is the first approach allowing to classify each single edit in Wikipedia into one or more of 21 different edit categories using a supervised machine learning 1 http://stats.wikimedia.org/EN/ TablesDatabas"
D13-1055,P11-4017,1,0.393937,"and markup is added. There are four basic types of edits, namely Insertions, Deletions, Modifications and Relocations. These are calculated via a line-based diff comparison on the source text (including wiki markup). As previously suggested (Daxenberger and Gurevych, 2012), inside modified lines, only the span of text which has actually been changed is marked as edit (either Insertion, Deletion or Modification), not the entire line. We extracted the data which is not contained in WPEC (meta data and plain text of rv−1 and rv ) using the Java Wikipedia Library (JWPL) with the Revision Toolkit (Ferschke et al., 2011). In Daxenberger and Gurevych (2012), we divide the 21-category taxonomy into text-base (meaningchanging edits), surface (non meaning-changing edits) and Wikipedia policy (VANDALISM and R E VERT ) edits. Among the text-base edits, we include categories for templates, references (internal and external links), files and information, each of which is further divided into an insertion (I), deletion (D) and modification (M) category. Surface edits consist of paraphrases, spelling and grammar corrections, relocations and markup edits. The latter category contains all edits which affect markup elemen"
D13-1055,N12-1019,0,0.0360855,"(Liu and Ram, 2011). Second, automatic classification of edits generates huge amounts of training data for the above mentioned NLP systems. Edit category classification is related to the better known task of document pair classification. In document pair classification, a pair of documents has to be assigned to one or more categories (e.g. paraphrase/non-paraphrase, plagiarism/nonplagiarism). Here, the document may be a very short text, such as a sentence or a single word. Applications of document pair classification include plagiarism detection (Potthast et al., 2012), paraphrase detection (Madnani et al., 2012) or text similarity detection (B¨ar et al., 2012). In edit category classification, we also have two documents. However, these documents are different versions of the same text. This scenario implies certain characteristics for a well-designed feature set as we will demonstrate in this study. The main contributions of this paper are: First, we introduce a novel feature set for edit category classification. Second, we evaluate the performance of this feature set on different tasks within a corpus of Wikipedia edits. We propose the new task of edit category classification and show that our model"
D13-1055,max-wisniewski-2010-mining,0,0.0613179,"(Suh et al., 2009), the number of edits in existing articles is rather stable.1 It is reasonable to assume that the latter will not change in the near future. One of the major reasons for the popularity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activity. Wikipedia’s revision history stores all changes made to any page in the encyclopedia in separate revisions. Previous studies have exploited revision history data in tasks such as preposition error correction (Cahill et al., 2013), spelling error correction (Zesch, 2012) or paraphrasing (Max and Wisniewski, 2010). However, they all use different approaches to extract the information needed for their task. Ferschke et al. (2013) outline several applications benefiting from revision history data. They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy. In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classification taxonomy developed in previous work (Daxenberger and Gurevych, 2012). This taxon"
D13-1055,P13-1162,0,0.0472839,"ations such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lanFigure 1: An example edit from WPEC labeled with R EFERENCE -M, as displayed by Wikimedia’s diff page tool. guage model (word n-grams). In their experiments, character-level features and named entity features show the highest improvement over the baseline. Vandalism detection in Wikipedia has mostly been defined as a binary machine learning t"
D13-1055,D11-1038,0,0.0163116,"ored in its revision history. Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lanFigure 1: An example edit from WPEC labeled with R EFERENCE -M, as displayed by Wikimedia’s diff page"
D13-1055,P08-2035,0,0.070412,"ining data for edit category classification, as all previous versions of each page in the encyclopedia are stored in its revision history. Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, a"
D13-1055,N10-1056,0,0.0144371,"he encyclopedia are stored in its revision history. Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lanFigure 1: An example edit from WPEC labeled with R EFERENCE -M, as displa"
D13-1055,E12-1054,0,0.488945,"declined in the last few years (Suh et al., 2009), the number of edits in existing articles is rather stable.1 It is reasonable to assume that the latter will not change in the near future. One of the major reasons for the popularity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activity. Wikipedia’s revision history stores all changes made to any page in the encyclopedia in separate revisions. Previous studies have exploited revision history data in tasks such as preposition error correction (Cahill et al., 2013), spelling error correction (Zesch, 2012) or paraphrasing (Max and Wisniewski, 2010). However, they all use different approaches to extract the information needed for their task. Ferschke et al. (2013) outline several applications benefiting from revision history data. They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy. In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classification taxonomy developed in previous work ("
D13-1055,W10-3504,0,\N,Missing
D14-1006,P03-1054,0,0.050797,"Missing"
D14-1006,D09-1036,0,0.0696402,"Missing"
D14-1006,W01-1605,0,0.323313,"Missing"
D14-1006,W10-4310,0,0.0430584,"Missing"
D14-1006,P14-5011,1,0.787211,"Missing"
D14-1006,P02-1047,0,0.0292845,"of the covering sentences in the essay, four Boolean features indicate if the argument components are present in the first or last sentence of a paragraph, one Boolean feature for representing if the target component occurs before the source component, the sentence distance between the covering sentences, and a Boolean feature which indicates if both argument components are in the same sentence. Lexical features: We define lexical features based on word pairs, first words and modals. It has been shown in previous work that word pairs are effective for identifying implicit discourse relations (Marcu and Echihabi, 2002). We define each pair of words between the source and target components as a Boolean feature and investigate word pairs containing stop words as well as stop word filtered word pairs. In addition, we adopt the first word features proposed by Pitler et al. (2009). We extract the first word either from the argument component or from non-annotated tokens preceding the argument component in the covering sentence if present. So, the first word of an argument component is either the first word of the sentence containing the argument component, the first word following a preceding argument component"
D14-1006,P11-1099,0,0.216877,"Missing"
D14-1006,W13-2707,0,0.456291,"Missing"
D14-1006,P09-1077,0,0.145307,"istance between the covering sentences, and a Boolean feature which indicates if both argument components are in the same sentence. Lexical features: We define lexical features based on word pairs, first words and modals. It has been shown in previous work that word pairs are effective for identifying implicit discourse relations (Marcu and Echihabi, 2002). We define each pair of words between the source and target components as a Boolean feature and investigate word pairs containing stop words as well as stop word filtered word pairs. In addition, we adopt the first word features proposed by Pitler et al. (2009). We extract the first word either from the argument component or from non-annotated tokens preceding the argument component in the covering sentence if present. So, the first word of an argument component is either the first word of the sentence containing the argument component, the first word following a preceding argument component in the same sentence or the first word of the actual argument component if it commences the sentence or directly follows another argument component. Table 4: Confusion matrix (SVM) for argument component classification (MC = Major Claim; Cl = Claim; Pr = Premise"
D14-1006,prasad-etal-2008-penn,0,0.254104,"Missing"
D14-1006,reed-etal-2008-language,0,0.018153,"Missing"
D14-1006,C14-1142,1,0.306604,"cted from parse trees yield good results, whereas Louis et al. (2010) found that features based on namedentities do not perform as well as lexical features. However, current approaches to discourse analysis like the RST or the PDTB are designed to analyze general discourse structures, and thus include a large set of generic discourse relations, whereas only a subset of those relations is relevant for argumentative discourse analysis. For instance, the argumentation scheme proposed by Peldszus and Stede (2013) includes three argumentative relations (support, attack and counter-attack), whereas Stab and Gurevych (2014) propose a scheme including only two relations (support and attack). The difference between argumentative relations and those included in general tagsets like RST and PDTB is best illustrated by the work of Biran and Rambow (2011), which is to the best of our knowledge the only work that focuses on the identification of argumentative relations. They argue that existing definitions of discourse relations are only relevant as a building block for identifying argumentative discourse and that existing approaches do not contain a single relation that corresponds to Discourse Relations Identifying a"
D14-1006,N03-1033,0,0.0404417,"Missing"
D15-1035,P09-1032,0,0.0615952,"Missing"
D15-1035,P14-2064,0,0.378135,"s, and no systematic benefit from soft labeling. 1 Text: India’s Taj Mahal gets facelift Sadness Rating (0-100): 8.0 α Agreement (-1.0 – 1.0): 0.7 Figure 1: Affect Recognition Easy Case. Text: After Iraq trip, Clinton proposes war limits Sadness Rating (0-100): 12.5 α Agreement (-1.0 – 1.0): -0.1 Figure 2: Affect Recognition Hard Case. Introduction In Figure 1, annotators mostly agreed that the headline expresses little sadness. But in Figure 2, the low item agreement may be caused by instance difficulty (i.e., Is a war zone sad or just bad?): a Hard Case (Zeman, 2010). Previous work (Beigman Klebanov and Beigman, 2014; Beigman and Beigman Klebanov, 2009) has shown that training strategy may affect Hard and Easy Case test instances differently. In this work, for five natural language tasks, we examine the impact of passing crowdsource item agreement on to the task classifier, by means of training instance filtering and soft labeling. We construct classifiers for Biased Text Detection, Stemming Classification, Recognizing Textual Entailment, Twitter POS Tagging, and Affect Recognition, and evaluate the effect of our different training strategies on the accuracy of each Crowdsourcing is a cheap and increasing"
D15-1035,N15-1152,0,0.0683195,"Missing"
D15-1035,P14-5011,1,0.849778,"d Strapparava and Mihalcea (2007) in using mean as gold standard. Although another aggregation such as as median might be more representative, such discussion is beyond the scope of this paper. github.com/EmilyKJamison/crowdsourcing 292 and Easy Cases, the training instances were unaffected, but the test instances were filtered by α item agreement. Hard/Easy Case parameters were chosen to divide the corpus by item agreement into roughly equal portions3 , relative to the corpus, for post-hoc error analysis. All systems except Affect Recognition were constructed using DKPro Text Classification (Daxenberger et al., 2014), and used Weka’s SMO (Platt, 1999) or SMOreg (Shevade et al., 2000) implementations with default parameters, with 10fold (or 5-fold, for computationally-intensive POS Tagging) cross-validation. More details are available in the Supplemental Notes document. cutoffs were <-.21 and &gt;.20. Of 1041 total instances, 161 were Hard Cases (<-.21) and 499 were Easy Cases (&gt;.20). We built an SVM regression task using unigrams, to predict the numerical amount of bias. The gold standard was the integrated labels. Itemspecific agreement was calculated with Ordinal Distance Function (Krippendorff, 1980). We"
D15-1035,C14-2023,1,0.831355,"Missing"
D15-1035,P11-2008,0,0.124929,"Missing"
D15-1035,petrov-etal-2012-universal,0,0.0406924,"n number of tokens, shared named entities, and subtask names. The gold standard was the original labels from Dagan et al. (2006). Hard/Easy Case cutoffs were <0.0 and &gt;.3. Training strategies are from Biased Language (VeryHigh) and Stem (others) experiments, except the HighAgree cutoff was 0.0 and the VeryHigh cutoff was 0.3. Of 800 total instances, 230 were Hard Cases (<0.0) and 207 were Easy Cases (&gt;.30). 3.4 Affect Recognition POS tagging We built a POS-tagger for Twitter posts. We used the training section of the dataset from Gimpel et al. (2011). The POS tagset was the universal tag set (Petrov et al., 2012); we converted Gimpel et al. (2011)’s tags to the universal tagset using Hovy et al. (2014)’s mapping. Crowdsource labels for this data came from Hovy et al. (2014)8 , who obtained 5 labels for each tweet. After aligning and cleaning, our dataset consisted of 953 tweets of 14,439 tokens. We followed Hovy et al. (2014) in constructing a CRF classifier (Lafferty et al., 2001), using a list of English affixes, Hovy et al. (2014)’s set of orthographic features, and word clusters (Owoputi et al., 2013). In the cross-validation division, individual tweets were assigned to folds. The gold standard wa"
D15-1035,D08-1027,0,0.588025,"Missing"
D15-1035,E14-1078,0,0.280753,"Missing"
D15-1035,S07-1013,0,0.0261423,"Missing"
D15-1035,P14-2083,0,0.231208,"Missing"
D15-1035,P14-2021,0,0.0282452,"e or ten labels are collected for an instance, and are aggregated together into an integrated label. The high number of labels is used to compensate for worker bias, task misunderstanding, lack of interest, incompetance, and malicious intent (Wauthier and Jordan, 2011). Majority voting for label aggregation has been found effective in filtering noisy labels (Nowak and R¨uger, 2010). Labels can be aggregated under weighted conditions reflecting the reliability of the annotator (Whitehill et al., 2009; Welinder et al., 2010). Certain classifiers are also robust to random (unbiased) label noise (Tibshirani and Manning, 2014; Beigman and Beigman Klebanov, 2009). However, minority label information is discarded by aggregation, and when the labels were 291 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 291–297, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. task. These tasks represent a wide range of machine learning tasks typical in NLP: sentence-level SVM regression using n-grams; word pairs with character-based features and binary SVM classification; pairwise sentence binary SVM classification with similarity score features"
D15-1035,D07-1002,0,0.112033,"Missing"
D15-1035,W10-0723,0,0.0201839,"Missing"
D15-1035,S07-1067,0,\N,Missing
D15-1035,P14-2062,0,\N,Missing
D15-1035,N13-1039,0,\N,Missing
D15-1208,P14-5011,1,0.75011,"s of 175 characters - 80 extraverts (E) and 95 introverts (I) - containing 289 274 words in 21 857 utterances (on average 111 utterances for E and 136 for I, as I are often central in books).4 4.2 Classification approach for direct speech All speech utterances of one book character are represented as one instance in our system. We use the leave-one-out classification setup due to the relatively small dataset size, using the support vector machines (SVM-SMO) classifier, which performs well on comparable tasks (Celli et al., 2013). The classification is performed through the DKPro TC Framework (Daxenberger et al., 2014). Lexical features As a bottom-up approach we use the 1000 most frequent word uni-, bi- and trigrams, 1000 dependency word pairs, 1000 character trigrams and 500 most frequent verbs, adverbs, adjectives and interjections as binary features. Semantic features Since the top-down approach, i.e. not focusing on individual words, has 4 The data set size is comparable to ongoing personality profiling challenges - see http://pan.webis.de 1808 been found more suitable for the personality profiling task on smaller data sets (Celli et al., 2013), we aim on capturing additional phenomena on a higher leve"
D15-1208,P14-1035,0,0.256828,"Missing"
D15-1208,D13-1185,0,0.0408572,"Missing"
D15-1208,P10-1015,0,0.136288,"Missing"
D15-1208,esuli-sebastiani-2006-sentiwordnet,0,0.0257131,"Missing"
D15-1208,P05-1045,0,0.00756178,"Missing"
D15-1208,E12-1059,1,0.906374,"Missing"
D15-1208,P13-1129,0,0.0265398,"Missing"
D15-1208,W14-0906,0,0.348376,"Missing"
D15-1208,W10-0204,0,0.0776968,"Missing"
D15-1208,D14-1162,0,0.0791784,"Missing"
D15-1208,P13-1035,0,0.217758,"Missing"
D15-1208,P06-2081,0,0.110225,"Missing"
D15-1208,D12-1072,0,0.0335131,"Missing"
D15-1255,P12-2041,0,0.0933985,"d approaches are the following. First, the argumentation models are simplified to either claims or a few types of premises/propositions. Second, the segmentation of discourse into argument components is ignored (except the work of Goudas et al. ˇ (2014)). Recently, Boltuˇzi´c and Snajder (2015) employed hierarchical clustering to cluster arguments in online debates using embeddings projection, but in contrast to our work they performed only intrinsic evaluation of the clusters. Debate portals have been used in a related body of research, such as classifying support and attack between posts by Cabrio and Villata (2012), or stance detection by Hasan and Ng (2013) or Gottipati et al. (2013). These approaches consider the complete documents (posts) but do not analyze the micro-level argumentation (e.g., claims or premises). 2128 Doc #2823 (article comment, public-private-schools): [claim: I agree - Kids can do great in the public school system and parents DO need to be involved.] The more people leave, the worse its going to become. [premise: The public school system lets them deal with real life too, unfortunate that it may be but that is what’s out there in college and the work force too.] [premise: There ar"
D15-1255,W14-5201,1,0.885501,"Missing"
D15-1255,P11-1099,0,0.0267559,"opic in numerous research areas, such as philosophy (van Eemeren et al., 2014), communication studies (Mercier and Sperber, 2011), and informal logic (Blair, 2004), among others. In this section, we will focus on the most related works on argumentation mining techniques in NLP in the first part, with an emphasis on Web data in the second part. Mochales and Moens (2011) based their work on argumentation schemes (Walton et al., 2008) and experimented with Araucaria and ECHR datasets using supervised models to classify argumentative and non-argumentative sentences (≈ 0.7F1 ) and their structure. Feng and Hirst (2011) classified argument schemes on the Araucaria dataset, reaching 0.6-0.9 accuracy. Experiments on this dataset were also conducted by Rooney et al. (2012), who classified sentences to four categories (conclusion, premise, conclusion-premise, 2 3 For instance createdebate.com or debate.org https://github.com/habernal/emnlp2015 and none) and achieved 0.65 accuracy. These approaches assume the text is already segmented into argument components. Stab and Gurevych (2014b) examined argumentation in persuasive essays and classified argument components into four categories (premise, claim, major claim,"
D15-1255,D13-1191,0,0.0456064,"ified to either claims or a few types of premises/propositions. Second, the segmentation of discourse into argument components is ignored (except the work of Goudas et al. ˇ (2014)). Recently, Boltuˇzi´c and Snajder (2015) employed hierarchical clustering to cluster arguments in online debates using embeddings projection, but in contrast to our work they performed only intrinsic evaluation of the clusters. Debate portals have been used in a related body of research, such as classifying support and attack between posts by Cabrio and Villata (2012), or stance detection by Hasan and Ng (2013) or Gottipati et al. (2013). These approaches consider the complete documents (posts) but do not analyze the micro-level argumentation (e.g., claims or premises). 2128 Doc #2823 (article comment, public-private-schools): [claim: I agree - Kids can do great in the public school system and parents DO need to be involved.] The more people leave, the worse its going to become. [premise: The public school system lets them deal with real life too, unfortunate that it may be but that is what’s out there in college and the work force too.] [premise: There are still great teachers in the public schools - lets stand behind them.]"
D15-1255,P14-5011,1,0.790127,"the corpus. Argument components were annotated on the token level as non-overlapping annotation spans. We therefore represent the argument annotations using BIO encoding. Each token is labeled with one of the 11 categories (5 argument component types × B or I tag + one O category for non-argumentative text). 4 Method We cast the task of identifying argument components as a sequence tagging problem and employ SVMhmm (Joachims et al., 2009).6 For linguistic annotations and feature engineering, we rely on two UIMA-based frameworks – DKProCore (Eckart de Castilho and Gurevych, 2014) and DKProTC (Daxenberger et al., 2014). Although the argument component annotations in the corpus are aligned to the token boundaries (token-level annotations), the minimal classification unit in our sequence tagging approach is set to the sentence level. First, this allows us to capture rich features that are available for entire sentences as opposed to the token level. Second, by modeling sequences on the token level we would lose the advantage of SVMhmm to estimate dependencies between labels, as the label context is limited due to computational feasibility. On the token level, the label sequences are rather static (long sequen"
D15-1255,I13-1191,0,0.0335283,"tation models are simplified to either claims or a few types of premises/propositions. Second, the segmentation of discourse into argument components is ignored (except the work of Goudas et al. ˇ (2014)). Recently, Boltuˇzi´c and Snajder (2015) employed hierarchical clustering to cluster arguments in online debates using embeddings projection, but in contrast to our work they performed only intrinsic evaluation of the clusters. Debate portals have been used in a related body of research, such as classifying support and attack between posts by Cabrio and Villata (2012), or stance detection by Hasan and Ng (2013) or Gottipati et al. (2013). These approaches consider the complete documents (posts) but do not analyze the micro-level argumentation (e.g., claims or premises). 2128 Doc #2823 (article comment, public-private-schools): [claim: I agree - Kids can do great in the public school system and parents DO need to be involved.] The more people leave, the worse its going to become. [premise: The public school system lets them deal with real life too, unfortunate that it may be but that is what’s out there in college and the work force too.] [premise: There are still great teachers in the public schools"
D15-1255,J13-4004,0,0.0152379,"sults into 30 realvalued features. NLP Semantic Role Labeler (Choi, 2012), extract various semantic information (agent, predicate + agent, predicate + agent + patient + (optional) negation, argument type + argument value) and discourse markers. Discourse relations also play an important role in argumentation analysis (Cabrio et al., 2013). We thus employ binary features (such as the presence of the sentence in a chain, the transition type, the distance to previous/next sentences in the chain, or the number of inter-sentence coreference links) obtained from Stanford Coreference Chain Resolver (Lee et al., 2013). Furthermore, we include features resulting from a PTDB-style discourse parser (Li et al., 2012), such as the type of discourse relation (explicit, implicit), the presence of discourse connectives, and attributions. 4.2 Unsupervised features We enrich the above-mentioned features by utilizing external large unlabeled resources – debate portals. They fulfill several criteria, namely (a) they are ‘argumentative’ (meant as opposed to, for example, prose or encyclopedic genres), (b) they are comprised of user-generated content and (c) and there is at least some overlap with topics from our experi"
D15-1255,P09-1040,0,0.00867701,"presence of discourse markers has been shown to be helpful in argument component analysis (e.g, “therefore” and “since” for premises or “think” and “believe” for claims), we encode the first and last three words as binary features. Furthermore, we capture the relative position of the sentence in the paragraph and the document, the number of part of speech 1-3 grams, maximum dependency tree depth, constituency tree production rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002) with topics trained on unlabeled data provided as a part of the corpu"
D15-1255,N13-1039,0,0.0752159,"Missing"
D15-1255,W14-2105,0,0.327459,"by the need of 1 Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation of propositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, including all writing and speaking which is persuasive in form (Ketcham, 1917). retrieving and understanding ordinary people’s arguments to various contentious topics on the large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in education (Habernal et al., 2014). Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the prevalent view in argumentation mining treats arguments as discourse structures consisting of several argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argumentation usually follow the fully supervised machinelearning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. O"
D15-1255,W13-2324,0,0.0224999,"ncluding all writing and speaking which is persuasive in form (Ketcham, 1917). retrieving and understanding ordinary people’s arguments to various contentious topics on the large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in education (Habernal et al., 2014). Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the prevalent view in argumentation mining treats arguments as discourse structures consisting of several argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argumentation usually follow the fully supervised machinelearning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. Only few publicly available argumentation corpora exist, as annotations are costly, error-prone, and require skilled human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014). To overcome the limited scope and size of the existing annotated corpora, semi-supervised methods can be adopted, as they gain performance by exploiting large unlabeled datasets ("
D15-1255,P06-1055,0,0.0113892,"and syntactic features (FS1) Since the presence of discourse markers has been shown to be helpful in argument component analysis (e.g, “therefore” and “since” for premises or “think” and “believe” for claims), we encode the first and last three words as binary features. Furthermore, we capture the relative position of the sentence in the paragraph and the document, the number of part of speech 1-3 grams, maximum dependency tree depth, constituency tree production rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002) with topics trained on unlabeled data"
D15-1255,D13-1170,0,0.00384266,"words as binary features. Furthermore, we capture the relative position of the sentence in the paragraph and the document, the number of part of speech 1-3 grams, maximum dependency tree depth, constituency tree production rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002) with topics trained on unlabeled data provided as a part of the corpus.8 Semantic and discourse features (FS3) Features based on semantic frames has been introduced in relevant works on stance recognition (Hasan and Ng, 2013). Our features, based on PropBank semantic role labels an"
D15-1255,C14-1142,1,0.801219,"can boost the performance of argument component identification and to which extent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we significantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios. 1 Introduction Argumentation mining, an evolving sub-field of NLP, deals with analyzing argumentation1 in various genres, such as legal cases (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014a), and medical and scientific articles (Green, 2014; Teufel and Moens, 2002). Recently, the focus of argumentation mining has also shifted to the Web registers (such as comments to articles, forum posts, or blogs) which is motivated by the need of 1 Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation of propositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, includi"
D15-1255,N03-1033,0,0.0112977,"s ‘one-hot’ (binary) features. Structural and syntactic features (FS1) Since the presence of discourse markers has been shown to be helpful in argument component analysis (e.g, “therefore” and “since” for premises or “think” and “believe” for claims), we encode the first and last three words as binary features. Furthermore, we capture the relative position of the sentence in the paragraph and the document, the number of part of speech 1-3 grams, maximum dependency tree depth, constituency tree production rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002)"
D15-1255,P10-1040,0,0.0227832,"features. We assume that phrases (sentences or documents) can be projected into a latent vector space, using, typically, a sum or a weighted average of all the word embeddings vectors in the phrase; see for example (Le and Mikolov, 2014). Neighboring vectors in the latent vector space exhibit some interesting properties, such as semantic similarity (thoroughly studied within the distributional semantics area). If the latent vector space is clustered, each n-dimensional vector gets reduced to a single cluster number; such clusters have been used directly as features in many tasks, such as NER (Turian et al., 2010), POS tagging (Owoputi 2130 et al., 2013), or sentiment analysis (Habernal and Brychc´ın, 2013). We build upon the above-mentioned approach (described by Søgaard (2013) as ‘clusters-asfeatures’ semi-supervised paradigm) and extend it further. We take both sentences and posts from the unlabeled debate portals, project them into a latent space using word embeddings and cluster them. The motivation is that these clusters will contain similar phrases or (similar ‘arguments’). Centroids of these clusters would then represent a ‘prototypical argument’ (note that the centroids exist only in the laten"
D15-1255,D14-1006,1,0.735835,"can boost the performance of argument component identification and to which extent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we significantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios. 1 Introduction Argumentation mining, an evolving sub-field of NLP, deals with analyzing argumentation1 in various genres, such as legal cases (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014a), and medical and scientific articles (Green, 2014; Teufel and Moens, 2002). Recently, the focus of argumentation mining has also shifted to the Web registers (such as comments to articles, forum posts, or blogs) which is motivated by the need of 1 Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation of propositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, includi"
D15-1255,J02-4002,0,0.0258662,"xtent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we significantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios. 1 Introduction Argumentation mining, an evolving sub-field of NLP, deals with analyzing argumentation1 in various genres, such as legal cases (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014a), and medical and scientific articles (Green, 2014; Teufel and Moens, 2002). Recently, the focus of argumentation mining has also shifted to the Web registers (such as comments to articles, forum posts, or blogs) which is motivated by the need of 1 Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation of propositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, including all writing and speaking which is persuasive in form (Ketcham, 1917). retr"
D15-1255,W15-0514,0,\N,Missing
D15-1267,W13-2610,0,0.0356755,"Previous work on the relation between DMs and DRs is mostly based on corpora annotated with DRs (Wolf et al., 2004; Taboada, 2006; Prasad et al., 2008), most notably the Penn Discourse Treebank (PDTB) for English (Prasad et al., 2008). The PDTB is annotated with DRs, such as contrast or result, and the corresponding DMs, even if they were not realized in the text. For instance, the contrast relation can be expressed by the DMs however and but. DRs that are lexically signaled by DMs in the text are called explicit DRs. Some DMs are highly polysemous, e.g., while appears in 12 DRs in the PDTB. Asr and Demberg (2013) analyzed the DMs and their corresponding DRs annotated in the PDTB and addressed the question, which information is conveyed by discourse connectives in the context of human sentence processing, i.e., how they contribute in the process of inferring a particular DR. Taboada (2006) performed a corpus-based analysis of DRs annotated in the Rhetorical Structure Theory (RST) Discourse Treebank (Carlson et al., 2003). The most frequent relation in the RST Discourse Treebank is concession, and this relation also received particular attention in the corpus linguistics literature: Taboada and G´omez-G"
D15-1267,P14-5011,1,0.367212,"2004), a German newspaper corpus: one set containing the 350 most frequent conjunctions and adverbs (DMtiger), and another set containing the 300 most frequent non-open-class words (NOCtiger, excluding the word classes of nouns, main verbs and adjectives).7 In addition, we considered the top 1 800 unigrams (with minimum frequency 5) as baseline features. We trained three Machine Learning algorithms (Naive Bayes (NB), Random Forests (RF) and Support Vector Machine (SVM)) on the three datasets annotated by the annotators A1, A2 and A3, using 10-fold cross-validation and the DKPro TC framework (Daxenberger et al., 2014). For the classification experiments, we used all 88 documents in the annotated corpus (including the documents from the pre-study). Table 2 summarizes the results. All classifiers show significant improvement compared to the majority class baseline (MC), indicating that DMs might be useful as predictive features for discriminating claims and premises. The DMs extracted from Tiger did not improve the performance consistently for all three datasets, showing that the coverage of the manually compiled DMs is good. Using the NOCtiger set, however, improved the performance by up to 4 pp., compared"
D15-1267,W14-5201,1,0.660732,"Missing"
D15-1267,W13-2707,0,0.0604737,"point of view. (2) As the students get frustrated, their performance generally does not improve. Also, the point of repeating all courses because of only one or two bad grades is arguable. Introduction The growing field of argumentation mining in NLP develops methods to automatically analyze argumentative discourse for enhanced Information Extraction. Terminology: We understand argumentation as a rhetorical arrangement of arguments with the intent to convince or persuade the reader of a particular state of affairs. Following previous work in argumentation mining (e.g., (Palau and Moens, 2009; Florou et al., 2013; Peldszus and Stede, 2013a; Stab and Gurevych, 2014)), we define an argument as the pairing of a single claim (an arguable text unit) and a (possibly empty) set of premises, which each either support or attack the claim (Besnard and Hunter, 2008). We subsume claims and premises under the term argument unit (AU). Discourse markers in argumentative discourse: Since an argumentation line can only be captured in the context of a coherent text, argumentation mining is closely related to automated discourse analysis (Cabrio et al., 2013), which aims at identifying discourse functions of text segmen"
D15-1267,C14-2023,1,0.86009,"Missing"
D15-1267,W04-2703,0,0.0948982,"Missing"
D15-1267,C12-1113,0,0.0148239,"Ms expressing concession are often used to introduce counter-arguments in an argumentation line. 2.2 DMs in Classification There is previous work in sentiment classification and argumentation mining using DMs as features, as well as work in predicting DMs for natural language generation tasks, such as abstractive summarization. Taboada et al. (2011) successfully employed discourse particles as features for the calculation of polarity scores in automated sentiment analysis. They focused on particles acting as intensifiers, which modify the semantic intensity of the lexical item they refer to.1 Mukherjee and Bhattacharyya (2012) demonstrated that using discourse connectives as features in a system for sentiment classification of Twitter posts significantly improves classification accuracy over state-of-the art systems not considering DMs as features. In argumentation mining, Stab and Gurevych (2014) experimented with different types of features, including DMs from the PDTB annotation guidelines, to classify text units into the classes non-argumentative, major claim, claim, and premise. While the PDTB DMs appeared to be not helpful for discriminating between argumentative and non-argumentative text units, they were us"
D15-1267,D13-1094,0,0.0466389,"Missing"
D15-1267,W13-2324,0,0.0535681,"the students get frustrated, their performance generally does not improve. Also, the point of repeating all courses because of only one or two bad grades is arguable. Introduction The growing field of argumentation mining in NLP develops methods to automatically analyze argumentative discourse for enhanced Information Extraction. Terminology: We understand argumentation as a rhetorical arrangement of arguments with the intent to convince or persuade the reader of a particular state of affairs. Following previous work in argumentation mining (e.g., (Palau and Moens, 2009; Florou et al., 2013; Peldszus and Stede, 2013a; Stab and Gurevych, 2014)), we define an argument as the pairing of a single claim (an arguable text unit) and a (possibly empty) set of premises, which each either support or attack the claim (Besnard and Hunter, 2008). We subsume claims and premises under the term argument unit (AU). Discourse markers in argumentative discourse: Since an argumentation line can only be captured in the context of a coherent text, argumentation mining is closely related to automated discourse analysis (Cabrio et al., 2013), which aims at identifying discourse functions of text segments, and discourse relation"
D15-1267,prasad-etal-2008-penn,0,0.71517,", argumentation mining is closely related to automated discourse analysis (Cabrio et al., 2013), which aims at identifying discourse functions of text segments, and discourse relations (DRs) between adjacent text DMs belong to the word classes of conjunctions and adverbs (also called discourse particles) and are semantically characterized in traditional grammar books. The correspondence between DM semantics and DR semantics has received considerable attention in previous research in linguistics, most of which is based on corpora annotated with DRs (Carlson et al., 2003; Wolf and Gibson, 2005; Prasad et al., 2008). In contrast, the role of DMs as potential lexical signals in argumentative discourse is not well-understood, yet. While Stab and Gurevych (2014) used DMs as features for classifying AUs into different types, they did not analyze the semantics of DMs with respect to AU classification or considered different DM resources. As far as we are aware, there is no prior work performing a detailed investigation on the role of DMs as lexical signals for discriminating between the two fundamental argumentative roles of AUs, i.e., between claims and premises. Our contribution: In this paper, we address t"
D15-1267,D14-1006,1,0.559058,"d, their performance generally does not improve. Also, the point of repeating all courses because of only one or two bad grades is arguable. Introduction The growing field of argumentation mining in NLP develops methods to automatically analyze argumentative discourse for enhanced Information Extraction. Terminology: We understand argumentation as a rhetorical arrangement of arguments with the intent to convince or persuade the reader of a particular state of affairs. Following previous work in argumentation mining (e.g., (Palau and Moens, 2009; Florou et al., 2013; Peldszus and Stede, 2013a; Stab and Gurevych, 2014)), we define an argument as the pairing of a single claim (an arguable text unit) and a (possibly empty) set of premises, which each either support or attack the claim (Besnard and Hunter, 2008). We subsume claims and premises under the term argument unit (AU). Discourse markers in argumentative discourse: Since an argumentation line can only be captured in the context of a coherent text, argumentation mining is closely related to automated discourse analysis (Cabrio et al., 2013), which aims at identifying discourse functions of text segments, and discourse relations (DRs) between adjacent te"
D15-1267,J11-2001,0,0.00291836,"tudy of DMs that express concession across English and Spanish in different genres. A classification of DMs signaling concession across English and German is presented by Grote et al. (1997). They also point out the importance of concession in argumentative discourse: DMs expressing concession are often used to introduce counter-arguments in an argumentation line. 2.2 DMs in Classification There is previous work in sentiment classification and argumentation mining using DMs as features, as well as work in predicting DMs for natural language generation tasks, such as abstractive summarization. Taboada et al. (2011) successfully employed discourse particles as features for the calculation of polarity scores in automated sentiment analysis. They focused on particles acting as intensifiers, which modify the semantic intensity of the lexical item they refer to.1 Mukherjee and Bhattacharyya (2012) demonstrated that using discourse connectives as features in a system for sentiment classification of Twitter posts significantly improves classification accuracy over state-of-the art systems not considering DMs as features. In argumentation mining, Stab and Gurevych (2014) experimented with different types of fea"
D15-1267,W03-2102,0,0.114078,"Missing"
D15-1267,J05-2005,0,0.0185725,"text of a coherent text, argumentation mining is closely related to automated discourse analysis (Cabrio et al., 2013), which aims at identifying discourse functions of text segments, and discourse relations (DRs) between adjacent text DMs belong to the word classes of conjunctions and adverbs (also called discourse particles) and are semantically characterized in traditional grammar books. The correspondence between DM semantics and DR semantics has received considerable attention in previous research in linguistics, most of which is based on corpora annotated with DRs (Carlson et al., 2003; Wolf and Gibson, 2005; Prasad et al., 2008). In contrast, the role of DMs as potential lexical signals in argumentative discourse is not well-understood, yet. While Stab and Gurevych (2014) used DMs as features for classifying AUs into different types, they did not analyze the semantics of DMs with respect to AU classification or considered different DM resources. As far as we are aware, there is no prior work performing a detailed investigation on the role of DMs as lexical signals for discriminating between the two fundamental argumentative roles of AUs, i.e., between claims and premises. Our contribution: In th"
D15-1267,W01-1605,0,\N,Missing
D16-1086,P15-1034,0,0.306749,"(Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from scratch, such a transfer approach would simplify the rule creation, making it possible to build Open IE systems for o"
D16-1086,Q13-1034,0,0.0531775,"Missing"
D16-1086,N13-1136,0,0.0459881,"Missing"
D16-1086,W08-1301,0,0.0581554,"Missing"
D16-1086,D11-1142,0,0.111728,"ly usable in downstream applications.1 1 Introduction The goal of Open Information Extraction (Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from scratch, such a trans"
D16-1086,L16-1146,1,0.879747,"Missing"
D16-1086,D12-1048,0,0.116203,"roduction The goal of Open Information Extraction (Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from scratch, such a transfer approach would simplify the rule creation"
D16-1086,D13-1043,0,0.470644,"Missing"
D16-1086,moriceau-tannier-2014-french,0,0.0589965,"Missing"
D16-1086,L16-1262,0,0.0678305,"Missing"
D16-1086,seeker-kuhn-2012-making,0,0.0610596,"Missing"
D16-1086,P15-2050,1,0.887842,"Missing"
D16-1086,D15-1063,0,0.0553119,"Missing"
D16-1086,P10-1013,0,0.140857,"r English and readily usable in downstream applications.1 1 Introduction The goal of Open Information Extraction (Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from s"
D16-1129,W15-0608,0,0.199907,"istribution and (2) classifying types of flaws in less convincing arguments. Our experiments with feature-rich SVM learners and Bidirectional LSTM neural networks with convolution and attention mechanism reveal that such a novel fine-grained analysis of Web argument convincingness is a very challenging task. We release the new corpus UKPConvArg2 and the accompanying software under permissive licenses to the research community. 1 Addressing qualitative aspects of arguments has recently started gaining attention in the field of computational argumentation. Scoring strength of persuasive essays (Farra et al., 2015; Persing and Ng, 2015), exploring interaction in persuasive dialogues on Reddit (Tan et al., 2016), or detecting convincing arguments (Habernal and Gurevych, 2016) are among recent attempts to tackle the quality of argumentation. However, these approaches are holistic and do not necessarily explain why a given argument is strong or convincing. Introduction People engage in argumentation in various contexts, both online and in the real life. Existing definitions of argumentation do not solely focus on giving reasons and laying out a logical framework of premises and conclusions, but also highl"
D16-1129,D16-1166,0,0.0133034,"Govier, 2010; Johnson and Blair, 2006) or categorizing fallacies (Hamblin, 1970; Tindale, 2007). However, the nature of these normative approaches causes a gap between the ‘ideal’ models and empirically encountered real-world arguments, such as those on the Web (van Eemeren et al., 2014; Walton, 2012). Regarding the methodology utilized later in this paper, deep (recursive) neural networks have gained extreme popularity in NLP in recent years. Long Short-Term Memory networks (LSTM) with Attention mechanism have been applied on textual entailment (Rockt¨aschel et al., 2016), QuestionAnswering (Golub and He, 2016), or source-code summarization (Allamanis et al., 2016). 3 Data As our source data set, we took the publicly available UKPConvArg1 corpus.2 It is based on arguments originated from 16 debates from Web debate platforms createdebate.com and convinceme.net, each debate has two sides (usually pro and con). Arguments from each of the 32 debate sides are connected into a set of argument pairs, and each argument pair is annotated with a binary relation (argument A is more/less convincing than argument B), resulting in total into 11,650 argument pairs. Annotations performed by Habernal and Gurevych (2"
D16-1129,D15-1255,1,0.444691,"led with 17 categories which is improved by local and global filtering techniques. (3) We experiment with several computational models, both traditional and neu1215 ral network-based, and evaluate their performance quantitatively and qualitatively. The newly created data set UKPConvArg2 is available under CC-BY-SA license along with the experimental software for full reproducibility at GitHub.1 2 Related Work The growing field of computational argumentation has been traditionally devoted to structural tasks, such as argument component detection and classification (Habernal and Gurevych, 2017; Habernal and Gurevych, 2015), argument structure parsing (Peldszus and Stede, 2015; Stab and Gurevych, 2014), or argument schema classification (Lawrence and Reed, 2015), leaving the issues of argument evaluation or quality assessment as an open future work. There are only few attempts to tackle the qualitative aspects of arguments, especially in the Web discourse. Park and Cardie (2014) classified propositions in Web arguments into four classes with respect to their level of verifiability. Focusing on convincingness of Web arguments, Habernal and Gurevych (2016) annotated 16k pairs of arguments with a binary relation “i"
D16-1129,P16-1150,1,0.81458,"networks with convolution and attention mechanism reveal that such a novel fine-grained analysis of Web argument convincingness is a very challenging task. We release the new corpus UKPConvArg2 and the accompanying software under permissive licenses to the research community. 1 Addressing qualitative aspects of arguments has recently started gaining attention in the field of computational argumentation. Scoring strength of persuasive essays (Farra et al., 2015; Persing and Ng, 2015), exploring interaction in persuasive dialogues on Reddit (Tan et al., 2016), or detecting convincing arguments (Habernal and Gurevych, 2016) are among recent attempts to tackle the quality of argumentation. However, these approaches are holistic and do not necessarily explain why a given argument is strong or convincing. Introduction People engage in argumentation in various contexts, both online and in the real life. Existing definitions of argumentation do not solely focus on giving reasons and laying out a logical framework of premises and conclusions, but also highlight its social purpose which is to convince or to persuade (O’Keefe, We asked the following research questions. First, can we assess what makes an argument convinc"
D16-1129,J17-1004,1,0.172087,"111 argument pairs multi-labeled with 17 categories which is improved by local and global filtering techniques. (3) We experiment with several computational models, both traditional and neu1215 ral network-based, and evaluate their performance quantitatively and qualitatively. The newly created data set UKPConvArg2 is available under CC-BY-SA license along with the experimental software for full reproducibility at GitHub.1 2 Related Work The growing field of computational argumentation has been traditionally devoted to structural tasks, such as argument component detection and classification (Habernal and Gurevych, 2017; Habernal and Gurevych, 2015), argument structure parsing (Peldszus and Stede, 2015; Stab and Gurevych, 2014), or argument schema classification (Lawrence and Reed, 2015), leaving the issues of argument evaluation or quality assessment as an open future work. There are only few attempts to tackle the qualitative aspects of arguments, especially in the Web discourse. Park and Cardie (2014) classified propositions in Web arguments into four classes with respect to their level of verifiability. Focusing on convincingness of Web arguments, Habernal and Gurevych (2016) annotated 16k pairs of argum"
D16-1129,N13-1132,0,0.0656131,"dependency parsing. For example, each reason from Figure 1 would be transformed into two reason units.3 Overall, we obtained about 70k reason units from the entire UKPConvArg1 corpus. 3.1 Annotation scheme In order to develop a code book for assigning a label to each reason unit, we ran several pilot expert annotation studies (each with 200-300 reason units). Having a set of ≈ 25 distinct labels, we ran two larger studies on Amazon Mechanical Turk (AMT), each with 500 reason units and 10 workers. The workers were split into two groups; we then estimated gold labels for each group using MACE (Hovy et al., 2013) and compared both groups’ results in order to find systematic discrepancies. Finally, we ended up with a set of 19 distinct labels (classes). As the number of classes is too big for non-expert crowd workers, we developed a hierarchical annotation process guided by questions that narrow down the final class decision. The scheme is depicted in Figure 2.4 Workers were shown only the reason units without seeing the original arguments. 3 We picked this example for its simplicity, in reality the texts are much more fuzzy. 4 It might seem that some labels are missing, such as C8-2 and C8-3; these be"
D16-1129,W15-0516,0,0.0195702,"itional and neu1215 ral network-based, and evaluate their performance quantitatively and qualitatively. The newly created data set UKPConvArg2 is available under CC-BY-SA license along with the experimental software for full reproducibility at GitHub.1 2 Related Work The growing field of computational argumentation has been traditionally devoted to structural tasks, such as argument component detection and classification (Habernal and Gurevych, 2017; Habernal and Gurevych, 2015), argument structure parsing (Peldszus and Stede, 2015; Stab and Gurevych, 2014), or argument schema classification (Lawrence and Reed, 2015), leaving the issues of argument evaluation or quality assessment as an open future work. There are only few attempts to tackle the qualitative aspects of arguments, especially in the Web discourse. Park and Cardie (2014) classified propositions in Web arguments into four classes with respect to their level of verifiability. Focusing on convincingness of Web arguments, Habernal and Gurevych (2016) annotated 16k pairs of arguments with a binary relation “is more convincing” and also elicited explanation for the annotators’ decisions. Recently, research in persuasive essay scoring has started co"
D16-1129,P14-5010,0,0.00295754,"asize its main flaws or strengths. This approach is also known as knowledge elicitation – acquiring appropriate information from experts by asking ”why?” (Reed and Rowe, 2004). We therefore used the reasons as a proxy for developing a scheme for labeling argument quality attributes. This was done in a purely bottom-up empirical manner, as opposed to using ‘standard’ evaluation criteria known from argumentation literature (Johnson and Blair, 2006; Schiappa and Nordin, 2013). In particular, we split all reasons into several reason units by simple preprocessing (splitting using Stanford CoreNLP (Manning et al., 2014), segmentation into Elementary Discourse Units by RST tools (Surdeanu et al., 2015)) and identified the referenced arguments (A1 or A2) by pattern matching and dependency parsing. For example, each reason from Figure 1 would be transformed into two reason units.3 Overall, we obtained about 70k reason units from the entire UKPConvArg1 corpus. 3.1 Annotation scheme In order to develop a code book for assigning a label to each reason unit, we ran several pilot expert annotation studies (each with 200-300 reason units). Having a set of ≈ 25 distinct labels, we ran two larger studies on Amazon Mech"
D16-1129,W14-2105,0,0.014167,"l reproducibility at GitHub.1 2 Related Work The growing field of computational argumentation has been traditionally devoted to structural tasks, such as argument component detection and classification (Habernal and Gurevych, 2017; Habernal and Gurevych, 2015), argument structure parsing (Peldszus and Stede, 2015; Stab and Gurevych, 2014), or argument schema classification (Lawrence and Reed, 2015), leaving the issues of argument evaluation or quality assessment as an open future work. There are only few attempts to tackle the qualitative aspects of arguments, especially in the Web discourse. Park and Cardie (2014) classified propositions in Web arguments into four classes with respect to their level of verifiability. Focusing on convincingness of Web arguments, Habernal and Gurevych (2016) annotated 16k pairs of arguments with a binary relation “is more convincing” and also elicited explanation for the annotators’ decisions. Recently, research in persuasive essay scoring has started combining holistic approaches based on rubrics for several dimensions typical to this genre with explicit argument detection. Persing and Ng (2015) manually labeled 1,000 student persuasive essays with a single score on the"
D16-1129,D15-1110,0,0.0178965,"al filtering techniques. (3) We experiment with several computational models, both traditional and neu1215 ral network-based, and evaluate their performance quantitatively and qualitatively. The newly created data set UKPConvArg2 is available under CC-BY-SA license along with the experimental software for full reproducibility at GitHub.1 2 Related Work The growing field of computational argumentation has been traditionally devoted to structural tasks, such as argument component detection and classification (Habernal and Gurevych, 2017; Habernal and Gurevych, 2015), argument structure parsing (Peldszus and Stede, 2015; Stab and Gurevych, 2014), or argument schema classification (Lawrence and Reed, 2015), leaving the issues of argument evaluation or quality assessment as an open future work. There are only few attempts to tackle the qualitative aspects of arguments, especially in the Web discourse. Park and Cardie (2014) classified propositions in Web arguments into four classes with respect to their level of verifiability. Focusing on convincingness of Web arguments, Habernal and Gurevych (2016) annotated 16k pairs of arguments with a binary relation “is more convincing” and also elicited explanation for t"
D16-1129,D14-1162,0,0.116247,"notated with several labels, the goal is to predict all these labels. We use two deep learning models. Our first model, Bidirectional Long Short-Term Memory (BLSTM) network contains two LSTM blocks (forward and backward), each with 64 hidden units on the output. The output is concatenated into a single vector and pushed through sigmoid layer with 17 output units (corresponding to 17 labels). We use cross entropy loss function in order to minimize distance of label distributions in training and test data (Nam et al., 2014). In the input layer, we rely on pre-trained word embeddings from Glove (Pennington et al., 2014) whose weights are updated during training the network. The second models is BLSTM extended with an attention mechanism (Rockt¨aschel et al., 2016; Golub and He, 2016) combined with convolution layers over the input. In particular, the input em1219 Debate Ban plastic water bottles? Christianity or Atheism Evolution vs. Creation Firefox vs. Internet Explorer Gay marriage: right or wrong? Should parents use spanking? If your spouse committed murder... India has the potential to lead the world Is it better to have a lousy father or to be fatherless? Is porn wrong? Is the school uniform a good or"
D16-1129,P15-1053,0,0.12527,"classifying types of flaws in less convincing arguments. Our experiments with feature-rich SVM learners and Bidirectional LSTM neural networks with convolution and attention mechanism reveal that such a novel fine-grained analysis of Web argument convincingness is a very challenging task. We release the new corpus UKPConvArg2 and the accompanying software under permissive licenses to the research community. 1 Addressing qualitative aspects of arguments has recently started gaining attention in the field of computational argumentation. Scoring strength of persuasive essays (Farra et al., 2015; Persing and Ng, 2015), exploring interaction in persuasive dialogues on Reddit (Tan et al., 2016), or detecting convincing arguments (Habernal and Gurevych, 2016) are among recent attempts to tackle the quality of argumentation. However, these approaches are holistic and do not necessarily explain why a given argument is strong or convincing. Introduction People engage in argumentation in various contexts, both online and in the real life. Existing definitions of argumentation do not solely focus on giving reasons and laying out a logical framework of premises and conclusions, but also highlight its social purpose"
D16-1129,D13-1170,0,0.00177532,"- and bi-gram presence, ratio of adjective and adverb endings that may signalize neuroticism (Corney et al., 2002), contextuality measure (Heylighen and Dewaele, 2002), dependency tree depth, ratio of exclamation or quotation marks, ratio of modal verbs, counts of several named entity types, ratio of past vs. future tense verbs, POS n-grams, presence of dependency tree production rules, seven different readability measures (e.g., Ari (Senter and Smith, 1967), Coleman-Liau (Coleman and Liau, 1975), Flesch (Flesch, 1948), and others), five sentiment scores (from very negative to very positive) (Socher et al., 2013), spell-checking using standard Unix words, ratio of superlatives, and some surface features such as sentence lengths, longer words count, etc.6 It results into a sparse 60k-dimensional feature vector space. Results in Table 4 suggest that the SVM-RBF baseline system performs poorly and its results are on par with a majority class baseline (not reported in detail). Both deep learning models significantly outperform the baseline, yielding Macro-F1 score about 0.35. The attention-based model performs better than simple BLSTM in two classes (C5 and C6), but the overall Macro-F1 score is not signi"
D16-1129,D14-1006,1,0.799536,"3) We experiment with several computational models, both traditional and neu1215 ral network-based, and evaluate their performance quantitatively and qualitatively. The newly created data set UKPConvArg2 is available under CC-BY-SA license along with the experimental software for full reproducibility at GitHub.1 2 Related Work The growing field of computational argumentation has been traditionally devoted to structural tasks, such as argument component detection and classification (Habernal and Gurevych, 2017; Habernal and Gurevych, 2015), argument structure parsing (Peldszus and Stede, 2015; Stab and Gurevych, 2014), or argument schema classification (Lawrence and Reed, 2015), leaving the issues of argument evaluation or quality assessment as an open future work. There are only few attempts to tackle the qualitative aspects of arguments, especially in the Web discourse. Park and Cardie (2014) classified propositions in Web arguments into four classes with respect to their level of verifiability. Focusing on convincingness of Web arguments, Habernal and Gurevych (2016) annotated 16k pairs of arguments with a binary relation “is more convincing” and also elicited explanation for the annotators’ decisions."
D16-1129,N15-3001,0,0.00704234,"ation – acquiring appropriate information from experts by asking ”why?” (Reed and Rowe, 2004). We therefore used the reasons as a proxy for developing a scheme for labeling argument quality attributes. This was done in a purely bottom-up empirical manner, as opposed to using ‘standard’ evaluation criteria known from argumentation literature (Johnson and Blair, 2006; Schiappa and Nordin, 2013). In particular, we split all reasons into several reason units by simple preprocessing (splitting using Stanford CoreNLP (Manning et al., 2014), segmentation into Elementary Discourse Units by RST tools (Surdeanu et al., 2015)) and identified the referenced arguments (A1 or A2) by pattern matching and dependency parsing. For example, each reason from Figure 1 would be transformed into two reason units.3 Overall, we obtained about 70k reason units from the entire UKPConvArg1 corpus. 3.1 Annotation scheme In order to develop a code book for assigning a label to each reason unit, we ran several pilot expert annotation studies (each with 200-300 reason units). Having a set of ≈ 25 distinct labels, we ran two larger studies on Amazon Mechanical Turk (AMT), each with 500 reason units and 10 workers. The workers were spli"
D16-1129,W15-4631,0,0.0301105,"et al., 2014; Blair, 2011). Assessing the quality and strength of perceived arguments therefore plays an inherent role in argumentative discourse. Despite strong theoretical foundations and plethora of normative theories, such as Walton’s schemes and their critical questions (Walton, 1989), an ideal model of critical discussion in the pragma-dialectic view (Van Eemeren and Grootendorst, 1987), or research into fallacies (Boudry et al., 2015), assessing qualitative criteria of everyday argumentation represents a challenge for argumentation scholars and practitioners (Weltzer-Ward et al., 2009; Swanson et al., 2015; Rosenfeld and Kraus, 2015). This article tackles a new challenging task in computational argumentation. Given a pair of two arguments to a certain controversial topic, we aim to directly assess qualitative properties of the arguments in order to explain why one argument is more convincing than the other one. We approach this task in a fully empirical manner by annotating 26k explanations written in natural language. These explanations describe convincingness of arguments in the given argument pair, such as their strengths or flaws. We create a new crowd-sourced corpus containing 9,111 argume"
D17-1035,P16-1231,0,0.0736109,"Missing"
D17-1035,P13-1166,0,0.0779594,"Missing"
D17-1035,N16-1175,0,0.107149,"Missing"
D17-1035,N16-1030,0,0.781663,"hes might be rejected too early, as they fail to deliver an outperformance simply due to a less favorable sequence of random numbers. • Reproducing results is difficult. To study the impact of the random seed value on the performance we will focus on five linguistic sequence tagging tasks: POS-tagging, Chunking, Named Entity Recognition, Entity Recognition4 , and Event Detection. Further we will focus on Long-Short-Term-Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997b), as those demonstrated state-of-the-art performance for a wide variety of sequence tagging tasks (Ma and Hovy, 2016; Lample et al., 2016; Søgaard and Goldberg, 2016). Fixing the random seed value would solve the issue with the reproducibility, however, there is no justification for choosing one seed value over another seed value. Hence, instead of reporting and comparing a single performance, we show that comparing score distributions can lead to new insights into the functioning of algorithms. 2 Background Validating and reproducing results is an important activity in science to manifest the correctness of previous conclusions and to gain new insights into the presented approaches. Fokkens et al. (2013) show that reproducing"
D17-1035,P14-2050,0,0.236549,"variational dropout (Gal and Ghahramani, 2016). Naive dropout applies a new dropout mask at every time step of the LSTM-layers. Variational dropout applies the same dropout mask for all time steps in the same sentence. Further, it applies dropout to the recurrent units. We evaluate the dropout rates {0.05, 0.1, 0.25, 0.5}. Evaluated Parameters We evaluate the following design choices and hyperparameters: Pre-trained Word Embeddings. We evaluate the Google News embeddings (G. News)7 from Mikolov et al. (2013), the Bag of Words (Le. BoW) as well as the dependency based embeddings (Le. Dep.)8 by Levy and Goldberg (2014), three different GloVe embeddings9 from Pennington et al. (2014) trained either on Wikipedia 2014 + Gigaword 5 (GloVe1 with 100 dimensions and GloVe2 with 300 dimensions) or on Common Crawl (GloVe3), and the Komninos and Manandhar (2016) embeddings (Komn.)10 . We also evaluate the approach of Bojanowski et al. (2016) (FastText), which trains embeddings for n-grams with length 3 to 6. The embedding for a word is defined as the sum of the embeddings of the ngrams. Character Representation. We evaluate the approaches of Ma and Hovy (2016) using Convolutional Neural Networks (CNN) as well as the"
D17-1035,P13-1008,0,0.0247795,"Missing"
D17-1035,P16-1101,0,0.649108,"res, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTMnetworks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters. The full experimental results are published in (Reimers and Gurevych, 2017).1 The implementation of our network is publicly available.2 1 In recent years, deep neural networks were shown to achieve state-of-the-art performance for a wide range of NLP tasks, including many sequence tagging tasks (Ma and Hovy, 2016), dependency parsing (Andor et al., 2016), and machine translation (Wu et al., 2016). The training process for neural networks is highly non-deterministic as it usually depends on a random weight initialization, a random shuffling of the training data for each epoch, and repeatedly applying random dropout masks. The error function of a neural network is a highly non-convex function of the parameters with the potential for many distinct local minima (LeCun et al., 1998; Erhan et al., 2010). Depending on the seed value for the pseudo-random number generator, the network will converge to a differ"
D17-1035,J93-2004,0,0.0592487,"Missing"
D17-1035,D14-1162,0,0.0800835,"Missing"
D17-1035,P16-2038,0,0.0625581,"too early, as they fail to deliver an outperformance simply due to a less favorable sequence of random numbers. • Reproducing results is difficult. To study the impact of the random seed value on the performance we will focus on five linguistic sequence tagging tasks: POS-tagging, Chunking, Named Entity Recognition, Entity Recognition4 , and Event Detection. Further we will focus on Long-Short-Term-Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997b), as those demonstrated state-of-the-art performance for a wide variety of sequence tagging tasks (Ma and Hovy, 2016; Lample et al., 2016; Søgaard and Goldberg, 2016). Fixing the random seed value would solve the issue with the reproducibility, however, there is no justification for choosing one seed value over another seed value. Hence, instead of reporting and comparing a single performance, we show that comparing score distributions can lead to new insights into the functioning of algorithms. 2 Background Validating and reproducing results is an important activity in science to manifest the correctness of previous conclusions and to gain new insights into the presented approaches. Fokkens et al. (2013) show that reproducing results is not always straigh"
D17-1035,N03-1033,0,0.106828,"Missing"
D17-1188,P11-1055,0,0.0455396,"t the target relations. Recently, Zeng et al. (2015) and Zhao et al. (2015) have shown that one can successfully apply convo1784 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1784–1789 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics lutional neural networks to extract sentence-level features automatically. Most of the methods (Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) focus on predicting a single relation type based on the combined evidence from all of the occurrences of an entity pair. Hoffmann et al. (2011) and Surdeanu et al. (2012) assign multiple relation types to each entity pair, such that the predictions are tied to particular occurrences of the entity pair. We regard the relation extraction task similarly and predict relation types on the sentence level. We use a distant supervision approach (Mintz et al., 2009) to construct the dataset. Mintz et al. (2009) and Riedel et al. (2010) have applied it to create relation extraction datasets for a large-scale KB. In contrast to our dataset, their data contains a single relation instance per sentence. That makes it incompatible with our method."
D17-1188,P13-1008,0,0.00880168,"the relation between the target entities and disregard other relations that might be present in the same sentence. Our method uses context relations to predict the target relation. One can also use other types of structured information from the nearby context to improve relation extraction. Roth and Yih (2004) have combined named entity recognition and relation extraction in a structured prediction approach to improve both tasks. Later, Miwa and Bansal (2016) have implemented an end-to-end neural network to construct a context representation for joint entity and relation extraction. Finally, Li et al. (2013) have designed global features and constraints to extract multiple events and their arguments from the same sentence. We don’t implement global constraints in our approach, since unlike events and arguments, there are no restrictions as to what relations can appear together. Instead we encode all relations in the same context into fixed-size vectors and use an attention mechanism to combine them. # of relation triples # of relation inst. Train Validation Held-out 284,295 578,199 113,852 190,160 287,902 600,804 Table 1: Statistics of the generated dataset. Wikidata is a collaboratively construc"
D17-1188,P16-1200,0,0.202391,"t al. (2009) and Riedel et al. (2010) have used manually engineered features based on part-of-speech tags and dependency parses to represent the target relations. Recently, Zeng et al. (2015) and Zhao et al. (2015) have shown that one can successfully apply convo1784 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1784–1789 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics lutional neural networks to extract sentence-level features automatically. Most of the methods (Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) focus on predicting a single relation type based on the combined evidence from all of the occurrences of an entity pair. Hoffmann et al. (2011) and Surdeanu et al. (2012) assign multiple relation types to each entity pair, such that the predictions are tied to particular occurrences of the entity pair. We regard the relation extraction task similarly and predict relation types on the sentence level. We use a distant supervision approach (Mintz et al., 2009) to construct the dataset. Mintz et al. (2009) and Riedel et al. (2010) have applied it to create relation extraction datasets for a large"
D17-1188,P14-5010,0,0.00293709,"Missing"
D17-1188,P09-1113,0,0.117376,"make the final prediction. To facilitate the experiments we construct a dataset that contains multiple positive and negative relation instances per sentence. We employ a fast growing community managed knowledge base (KB) Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) to build the dataset. Our main contribution is the new neural network architecture for extracting relations between an entity pair that takes into account other relations in the sentence. 2 Related Work We employ a neural network to automatically encode the target relation and the sentential context into a fixed-size feature vector. Mintz et al. (2009) and Riedel et al. (2010) have used manually engineered features based on part-of-speech tags and dependency parses to represent the target relations. Recently, Zeng et al. (2015) and Zhao et al. (2015) have shown that one can successfully apply convo1784 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1784–1789 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics lutional neural networks to extract sentence-level features automatically. Most of the methods (Riedel et al., 2010; Zeng et al., 2015; Lin et al.,"
D17-1188,P16-1105,0,0.0306276,"t, their data contains a single relation instance per sentence. That makes it incompatible with our method. All of the aforementioned approaches consider just the relation between the target entities and disregard other relations that might be present in the same sentence. Our method uses context relations to predict the target relation. One can also use other types of structured information from the nearby context to improve relation extraction. Roth and Yih (2004) have combined named entity recognition and relation extraction in a structured prediction approach to improve both tasks. Later, Miwa and Bansal (2016) have implemented an end-to-end neural network to construct a context representation for joint entity and relation extraction. Finally, Li et al. (2013) have designed global features and constraints to extract multiple events and their arguments from the same sentence. We don’t implement global constraints in our approach, since unlike events and arguments, there are no restrictions as to what relations can appear together. Instead we encode all relations in the same context into fixed-size vectors and use an attention mechanism to combine them. # of relation triples # of relation inst. Train"
D17-1188,D14-1162,0,0.118404,"riant of RNN (Hochreiter and Schmidhuber, 1997) that was successfully applied to information extraction before (Miwa and Bansal, 2016). 4.2 Model architecture Relation encoder The relation encoder produces a fixed-size vector representation os of a relation between two entities in a sentence (see Figure 1). First, each token of the sentence x = {x1 , x2 . . . xn } is mapped to a k-dimensional embedding vector using a matrix W ∈ R|V |×k , where |V | is the size of the vocabulary. Throughout the experiments in this paper, we use 50-dimensional GloVe embeddings pre-trained on a 6 billion corpus (Pennington et al., 2014). Second, we mark each token in the sentence as either belonging to the first entity e1 , the second entity e2 or to neither of those. A marker embedding matrix P ∈ R3×d is randomly initialized (d is the dimension of the position embedding and there are three marker types). For each token, we concatenate the marker embedding with the word embedding: (Wn , Pn ). We apply a recurrent neural network (RNN) on the token embeddings. The length n naturally varies from sentence to sentence and an RNN provides a way to accommodate inputs of various Model variants LSTM baseline As the first model varian"
D17-1188,W14-2508,0,0.00516691,"is to determine a type of relation between two target entities that appear together in a text. In this paper, we consider the sentential relation extraction task: to each occurrence of the target entity pair he1 , e2 i in some sentence s one has to assign a relation type r from a given set R (Hoffmann et al., 2011). A triple he1 , r, e2 i is called a relation instance and we refer to the relation of the target entity pair as target relation. Relation extraction is a fundamental task that enables a wide range of semantic applications from question answering (Xu et al., 2016) to fact checking (Vlachos and Riedel, 2014). For relation extraction, it is crucial to be able to extract relevant features from the sentential context (Riedel et al., 2010; Zeng et al., 2015). Modern approaches focus just on the relation between the target entities and disregard other relations that might [e1 Star Wars VII] is an American [e3 space opera epic film] directed by [e2 J. J. Abrams]. We present a novel architecture that considers other relations in the sentence as a context for predicting the label of the target relation. We use the term context relations to refer to them throughout the paper. Our architecture uses an LSTM"
D17-1188,P16-1220,0,0.0361569,"he main goal of relation extraction is to determine a type of relation between two target entities that appear together in a text. In this paper, we consider the sentential relation extraction task: to each occurrence of the target entity pair he1 , e2 i in some sentence s one has to assign a relation type r from a given set R (Hoffmann et al., 2011). A triple he1 , r, e2 i is called a relation instance and we refer to the relation of the target entity pair as target relation. Relation extraction is a fundamental task that enables a wide range of semantic applications from question answering (Xu et al., 2016) to fact checking (Vlachos and Riedel, 2014). For relation extraction, it is crucial to be able to extract relevant features from the sentential context (Riedel et al., 2010; Zeng et al., 2015). Modern approaches focus just on the relation between the target entities and disregard other relations that might [e1 Star Wars VII] is an American [e3 space opera epic film] directed by [e2 J. J. Abrams]. We present a novel architecture that considers other relations in the sentence as a context for predicting the label of the target relation. We use the term context relations to refer to them through"
D17-1188,D15-1203,0,0.173056,"n task: to each occurrence of the target entity pair he1 , e2 i in some sentence s one has to assign a relation type r from a given set R (Hoffmann et al., 2011). A triple he1 , r, e2 i is called a relation instance and we refer to the relation of the target entity pair as target relation. Relation extraction is a fundamental task that enables a wide range of semantic applications from question answering (Xu et al., 2016) to fact checking (Vlachos and Riedel, 2014). For relation extraction, it is crucial to be able to extract relevant features from the sentential context (Riedel et al., 2010; Zeng et al., 2015). Modern approaches focus just on the relation between the target entities and disregard other relations that might [e1 Star Wars VII] is an American [e3 space opera epic film] directed by [e2 J. J. Abrams]. We present a novel architecture that considers other relations in the sentence as a context for predicting the label of the target relation. We use the term context relations to refer to them throughout the paper. Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence. The representation of the target relation and representations"
D17-1188,W04-2401,0,0.0459367,". Mintz et al. (2009) and Riedel et al. (2010) have applied it to create relation extraction datasets for a large-scale KB. In contrast to our dataset, their data contains a single relation instance per sentence. That makes it incompatible with our method. All of the aforementioned approaches consider just the relation between the target entities and disregard other relations that might be present in the same sentence. Our method uses context relations to predict the target relation. One can also use other types of structured information from the nearby context to improve relation extraction. Roth and Yih (2004) have combined named entity recognition and relation extraction in a structured prediction approach to improve both tasks. Later, Miwa and Bansal (2016) have implemented an end-to-end neural network to construct a context representation for joint entity and relation extraction. Finally, Li et al. (2013) have designed global features and constraints to extract multiple events and their arguments from the same sentence. We don’t implement global constraints in our approach, since unlike events and arguments, there are no restrictions as to what relations can appear together. Instead we encode al"
D17-1188,D12-1042,0,0.295702,"ently, Zeng et al. (2015) and Zhao et al. (2015) have shown that one can successfully apply convo1784 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1784–1789 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics lutional neural networks to extract sentence-level features automatically. Most of the methods (Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) focus on predicting a single relation type based on the combined evidence from all of the occurrences of an entity pair. Hoffmann et al. (2011) and Surdeanu et al. (2012) assign multiple relation types to each entity pair, such that the predictions are tied to particular occurrences of the entity pair. We regard the relation extraction task similarly and predict relation types on the sentence level. We use a distant supervision approach (Mintz et al., 2009) to construct the dataset. Mintz et al. (2009) and Riedel et al. (2010) have applied it to create relation extraction datasets for a large-scale KB. In contrast to our dataset, their data contains a single relation instance per sentence. That makes it incompatible with our method. All of the aforementioned a"
D17-1218,N16-1165,0,0.141831,"Missing"
D17-1218,W11-1701,0,0.0351308,"ce level. We include information about the part-of-speech and parse tree for each sentence. 4 For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated. 5 Using the liblinear library (Fan et al., 2008). Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group). 4.2 Deep Learning Approaches As alternatives to our feature-based systems, we consider three deep learning approaches. The first is the Convolutional Neural Net of Kim (Kim, 2014) which has shown to perform excellent"
D17-1218,W06-1615,0,0.166945,"Missing"
D17-1218,P07-1033,0,0.112158,"Missing"
D17-1218,P17-1002,1,0.781919,"13a). ‘Mining’ arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored. This situation motivated us to perform a detaile"
D17-1218,P11-1099,0,0.0686932,"fly describe six English datasets used in our empirical study; they all capture claims on the discourse level. Table 1 summarizes the dataset statistics relevant to claim identification. 3.1 Datasets The AraucariaDB corpus (Reed et al., 2008) includes various genres (VG) such as newspaper editorials, parliamentary records, or judicial summaries. The annotation scheme structures arguments as trees and distinguishes between claims and premises at the clause level. Although the reliability of the annotations is unknown, the corpus has been extensively used in argument mining (Moens et al., 2007; Feng and Hirst, 2011; Rooney et al., 2012). The corpus from Habernal and Gurevych (2017) includes user-generated web discourse (WD) such as blog posts, or user comments annotated with claims and premises as well as backings, rebuttals and refutations (αU 0.48) inspired by Toulmin’s model of argument (Toulmin, 2003). The persuasive essay (PE) corpus (Stab and Gurevych, 2017) includes 402 student essays. The scheme comprises major claims, claims and premises at the clause level (αU 0.77). The corpus has been extensively used in the argument mining community (Persing and Ng, 2015; Lippi and Torroni, 2015; Nguyen and"
D17-1218,N16-1138,0,0.0210111,"learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences. 2058 want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5 , which is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016); and b) several deep learning approaches, using state-of-the-art neural network architectures. The in-domain experiments were carried out in a 10-fold cross-validation setup with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting. To address class-imbalance in our datasets (see Table 1), we downsample the negative class (nonclaim) both in-domain and cross-domain, so that positive and nega"
D17-1218,D14-1012,0,0.0325979,"atures are lowercased unigrams. Syntax Features account for grammatical information at the sentence level. We include information about the part-of-speech and parse tree for each sentence. 4 For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated. 5 Using the liblinear library (Fan et al., 2008). Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group). 4.2 Deep Learning Approaches As alternatives to our feature-based systems, we consider three deep learning approaches"
D17-1218,D15-1255,1,0.867498,"cal information at the sentence level. We include information about the part-of-speech and parse tree for each sentence. 4 For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated. 5 Using the liblinear library (Fan et al., 2008). Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group). 4.2 Deep Learning Approaches As alternatives to our feature-based systems, we consider three deep learning approaches. The first is the Convolutional Neural Net of Kim (Kim, 2014) which has shown"
D17-1218,J17-1004,1,0.842441,"its, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored. This situation motivated us to perform a detailed analysis of the concept of claims (as a key component of an argument) in existing argument mining datasets from different d"
D17-1218,D15-1076,0,0.037877,"Missing"
D17-1218,D14-1181,0,0.00585286,"Missing"
D17-1218,N16-1175,0,0.0437333,"Missing"
D17-1218,C14-1141,0,0.0631362,", politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored. This situation motivated us to perform a detailed analysis of the concept of claims (as a key component of an argument) in existing argument mining datasets from different domains.1 We first review and qualitatively analyze six existing publicly available datasets for argument mining (§3), showing that the conceptualizations of claims in these datasets differ largely. In a next step, we analyze the"
D17-1218,P14-5010,0,0.00291693,"how they could be dealt with in practice. Put simply, the task we are trying to solve in the following is: given a sentence, classify whether or not it contains a claim. We opted to model the claim identification task on sentence level, as this is the only way to make all datasets compatible to each other. Different datasets model claim boundaries differently, e.g. MT includes discourse markers within the same sentence, whereas they are excluded in PE. All six datasets described in the previous section have been preprocessed by first segmenting documents into sentences using Stanford CoreNLP (Manning et al., 2014) and then annotating every sentence as claim, if one or more tokens within the sentence were labeled as claim (or major claim in PE). Analogously, each sentence is annotated as non-claim, if none of its tokens were labeled as claim (or major claim). Although our basic units of interest are sentences, we keep the content of the entire document to be able to retrieve information about the context of (non-)claims.3 We are not interested in optimizing the properties of a certain learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have ac"
D17-1218,N15-1046,0,0.0386276,"rt-of-speech and parse tree for each sentence. 4 For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated. 5 Using the liblinear library (Fan et al., 2008). Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group). 4.2 Deep Learning Approaches As alternatives to our feature-based systems, we consider three deep learning approaches. The first is the Convolutional Neural Net of Kim (Kim, 2014) which has shown to perform excellently on many diverse classification tasks such"
D17-1218,N15-1055,0,0.052016,"Missing"
D17-1218,D15-1050,0,0.0884378,"Missing"
D17-1218,W12-4301,0,0.0692407,"o support with reasons’ (Govier, 2010). Argument mining, a computational counterpart of manual argumentation analysis, is a recent growing sub-field of NLP (Peldszus and Stede, 2013a). ‘Mining’ arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The"
D17-1218,W13-2324,0,0.247374,"lthough argumentation scholars provide us with a plethora of often clashing theories and models (van Eemeren et al., 2014). Despite the lack of a precise definition in the contemporary argumentation theory, Toulmin’s influential work on argumentation in the 1950’s introduced a claim as an ‘assertion that deserves our attention’ (Toulmin, 2003, p. 11); recent works describe a claim as ‘a statement that is in dispute and that we are trying to support with reasons’ (Govier, 2010). Argument mining, a computational counterpart of manual argumentation analysis, is a recent growing sub-field of NLP (Peldszus and Stede, 2013a). ‘Mining’ arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eg"
D17-1218,D15-1110,0,0.0770528,"uch as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored. This situation motivated us to perform a detailed analysis of the concept of claims (as a key component of an argument) in existing argument mining datasets from different domains.1 We first review and qualitatively analyze six existing publicly available datasets for argument mining (§3), showing that the conceptualizations of claims in these datasets differ"
D17-1218,P15-1053,0,0.0364649,"argument mining (Moens et al., 2007; Feng and Hirst, 2011; Rooney et al., 2012). The corpus from Habernal and Gurevych (2017) includes user-generated web discourse (WD) such as blog posts, or user comments annotated with claims and premises as well as backings, rebuttals and refutations (αU 0.48) inspired by Toulmin’s model of argument (Toulmin, 2003). The persuasive essay (PE) corpus (Stab and Gurevych, 2017) includes 402 student essays. The scheme comprises major claims, claims and premises at the clause level (αU 0.77). The corpus has been extensively used in the argument mining community (Persing and Ng, 2015; Lippi and Torroni, 2015; Nguyen and Litman, 2016). Biran and Rambow (2011a) annotated claims and premises in online comments (OC) from blog threads of LiveJournal (κ 0.69). In a subsequent work, Biran and Rambow (2011b) applied their annotation scheme to documents from Wikipedia talk pages (WTP) and annotated 118 threads. For our experiments, we consider each user comment in both corpora as a document, which yields 2, 805 documents in the OC corpus and 1, 985 documents in the WTP corpus. Peldszus and Stede (2016) created a corpus of German microtexts (MT) of controlled linguistic and rhetori"
D17-1218,P14-2083,0,0.0203363,"e not interested in optimizing the properties of a certain learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences. 2058 want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5 , which is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016); and b) several deep learning approaches, using state-of-the-art neural network architectures. The in-domain experiments were carried out in a 10-fold cross-validation setup with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting. To address class-imbalance in our datasets (see Table 1), we downsample the negative class (n"
D17-1218,reed-etal-2008-language,0,0.165527,"Missing"
D17-1218,W15-4625,0,0.031781,"se tree for each sentence. 4 For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated. 5 Using the liblinear library (Fan et al., 2008). Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group). 4.2 Deep Learning Approaches As alternatives to our feature-based systems, we consider three deep learning approaches. The first is the Convolutional Neural Net of Kim (Kim, 2014) which has shown to perform excellently on many diverse classification tasks such as sentiment analysis and que"
D17-1218,I13-1023,0,0.0605698,"Missing"
D17-1218,P16-2038,0,0.0594238,"Missing"
D17-1218,D14-1006,1,0.847867,"P (Peldszus and Stede, 2013a). ‘Mining’ arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored. This situation motivated us"
D17-1218,W10-2605,0,0.0762813,"Missing"
D17-1218,W14-2508,0,0.0628573,"); recent works describe a claim as ‘a statement that is in dispute and that we are trying to support with reasons’ (Govier, 2010). Argument mining, a computational counterpart of manual argumentation analysis, is a recent growing sub-field of NLP (Peldszus and Stede, 2013a). ‘Mining’ arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles ("
D17-1218,E17-1105,0,0.0404748,"Missing"
D17-1218,N15-1069,0,0.0543343,"Missing"
D17-1218,N16-3008,0,0.126993,"operties of a certain learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences. 2058 want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5 , which is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016); and b) several deep learning approaches, using state-of-the-art neural network architectures. The in-domain experiments were carried out in a 10-fold cross-validation setup with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting. To address class-imbalance in our datasets (see Table 1), we downsample the negative class (nonclaim) both in-domain and cross-dom"
D17-1218,N16-1177,0,0.115384,"operties of a certain learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences. 2058 want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5 , which is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016); and b) several deep learning approaches, using state-of-the-art neural network architectures. The in-domain experiments were carried out in a 10-fold cross-validation setup with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting. To address class-imbalance in our datasets (see Table 1), we downsample the negative class (nonclaim) both in-domain and cross-dom"
D17-1218,N16-1178,0,0.106465,"operties of a certain learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences. 2058 want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5 , which is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016); and b) several deep learning approaches, using state-of-the-art neural network architectures. The in-domain experiments were carried out in a 10-fold cross-validation setup with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting. To address class-imbalance in our datasets (see Table 1), we downsample the negative class (nonclaim) both in-domain and cross-dom"
D17-1218,D09-1036,0,\N,Missing
D17-1320,W10-4201,0,0.0317338,"llow more fine-grained annotations. The obtained labels are translated into scores (5..1) and the average of all scores for a proposition is used as an estimate for its importance. This follows the idea that while single workers might find the task subjective, the consensus of multiple workers, represented in the average score, tends to be less subjective due to the “wisdom of the crowd”. We randomly group five propositions into a task. Comparison Tasks As an alternative, we use a second task design based on pairwise comparisons. Comparisons are known to be easier to make and more consistent (Belz and Kow, 2010), but also more expensive, as the number of pairs grows quadratically with the number of objects.3 To reduce the cost, we group five propositions into a task and ask workers to order them by importance per drag-and-drop. From the results, we derive pairwise comparisons and use TrueSkill (Herbrich et al., 2007), a powerful Bayesian rank induction model (Zhang et al., 2016), to obtain importance estimates for each proposition. 4.2 Pilot Study To verify the proposed approach, we conducted a pilot study on Amazon Mechanical Turk using data from TAC2008 (Dang and Owczarzak, 2008). We collected impo"
D17-1320,C16-1099,1,0.898459,"Missing"
D17-1320,W14-3348,0,0.0226394,"of concepts co-occurring in a sentence, select the tokens in between as a potential relation if they contain a verb. Evaluation Metrics In order to automatically compare generated concept maps with reference maps, we propose three metrics.9 As a concept map is fully defined by the set of its propositions, we can compute precision, recall and F1-scores between the two proposition set of generated and reference map. A proposition is represented as the concatenation of concept and relation labels. Strict Match compares them after stemming and only counts exact and complete matches. Using METEOR (Denkowski and Lavie, 2014), we offer a second metric that takes synonyms and paraphrases into account and also scores partial matches. And finally, we compute ROUGE-2 (Lin, 2004) between the concatenation of all propositions from the maps. These automatic measures might be complemented with a human evaluation. 4. If a pair of concepts has more than one relation, select the one with the shortest label. Results Table 4 shows the performance of the baseline. An analysis of the single pipeline steps In this section, we briefly describe a baseline and evaluation scripts that we release, with a detailed documentation, along"
D17-1320,D17-2004,1,0.311156,"tance (Villalon, 2012) or to structure information repositories (Briggs et al., 2004; Richardson and Fox, 2005). For summarization, concept maps make it possible to represent a summary concisely and clearly reveal relationships. Moreover, we see a second interesting use case that goes beyond the capabilities of textual summaries: When concepts and relations are linked to corresponding locations in the documents they have been extracted from, the graph can be used to navigate in a document collection, similar to a table of contents. An implementation of this idea has been recently described by Falke and Gurevych (2017). The corresponding task that we propose is concept-map-based MDS, the summarization of a document cluster in the form of a concept map. In order to develop and evaluate methods for the task, gold-standard corpora are necessary, but no suitable corpus is available. The manual creation of such a dataset is very time-consuming, as the annotation includes many subtasks. In particular, an annotator would need to manually identify all concepts in the documents, while only a few of them will eventually end up in the summary. 2951 Proceedings of the 2017 Conference on Empirical Methods in Natural Lan"
D17-1320,C16-1055,0,0.049355,"Missing"
D17-1320,N15-1114,0,0.078799,"tory. How useful would the following statements be for you? (P1) students with bad credit history - apply for - federal loans with the FAFSA 2 Extremely Important 2 Very Important 2 Moderately Important 2 Slightly Important 2 Not at all Important (P2) students - encounter - unforeseen financial emergencies 2 Extremely Important 2 Very Important 2 Moderately Important 2 Not at all Important 2 Slightly Important Figure 3: Likert-scale crowdsourcing task with topic description and two example propositions. that build graphs of propositions to create a summary (Fang et al., 2016; Li et al., 2016; Liu et al., 2015; Li, 2015) seem to be particularly related, however, there is one important difference: While they use graphs as an intermediate representation from which a textual summary is then generated, the goal of the proposed task is to create a graph that is directly interpretable and useful for a user. In contrast, these intermediate graphs, e.g. AMR, are hardly useful for a typical, non-linguist user. For traditional summarization, the most wellknown datasets emerged out of the DUC and TAC competitions.2 They provide clusters of news articles with gold-standard summaries. Extending these efforts, s"
D17-1320,P14-1119,0,0.0347026,"onnect them to build a concept map. However, common task definitions and comparable evaluations are missing. In addition, only a few of them, namely Villalon (2012) and Valerio and Leake (2006), define summarization as their goal and try to compress the input to a substantially smaller size. Our newly proposed task and the created large-cluster dataset fill these gaps as they emphasize the summarization aspect of the task. For the subtask of selecting summary-worthy concepts and relations, techniques developed for traditional summarization (Nenkova and McKeown, 2011) and keyphrase extraction (Hasan and Ng, 2014) are related and applicable. Approaches 2952 Imagine you want to learn something about students loans without credit history. How useful would the following statements be for you? (P1) students with bad credit history - apply for - federal loans with the FAFSA 2 Extremely Important 2 Very Important 2 Moderately Important 2 Slightly Important 2 Not at all Important (P2) students - encounter - unforeseen financial emergencies 2 Extremely Important 2 Very Important 2 Moderately Important 2 Not at all Important 2 Slightly Important Figure 3: Likert-scale crowdsourcing task with topic description a"
D17-1320,N16-1095,0,0.0311722,"d 1.14), indicating that the workers did the task properly. Agreement and Reliability For Likert-scale tasks, we follow Snow et al. (2008) and calculate agreement as the average Pearson correlation of a worker’s Likert-score with the average score of the remaining workers.5 This measure is less strict than exact label agreement and can account for close labels and high- or low-scoring workers. We observe a correlation of 0.81, indicating substantial agreement. For comparisons, the majority agreement is 0.73. To further examine the reliability of the collected data, we followed the approach of Kiritchenko and Mohammed (2016) and simply repeated the crowdsourcing for one of the three topics. Between the importance estimates calculated from the first and second run, we found a Pearson correlation of 0.82 (Spearman 0.78) for Likert-scale tasks and 0.69 (Spearman 0.66) for comparison tasks. This shows that the approach, despite the subjectiveness of the task, allows us to collect reliable annotations. Peer Evaluation In addition to the reliability studies, we extrinsically evaluated the annotations in the task of summary evaluation. For each of the 58 peer summaries in TAC2008, we calculated a score as the sum of the"
D17-1320,D15-1219,0,0.0254921,"ould the following statements be for you? (P1) students with bad credit history - apply for - federal loans with the FAFSA 2 Extremely Important 2 Very Important 2 Moderately Important 2 Slightly Important 2 Not at all Important (P2) students - encounter - unforeseen financial emergencies 2 Extremely Important 2 Very Important 2 Moderately Important 2 Not at all Important 2 Slightly Important Figure 3: Likert-scale crowdsourcing task with topic description and two example propositions. that build graphs of propositions to create a summary (Fang et al., 2016; Li et al., 2016; Liu et al., 2015; Li, 2015) seem to be particularly related, however, there is one important difference: While they use graphs as an intermediate representation from which a textual summary is then generated, the goal of the proposed task is to create a graph that is directly interpretable and useful for a user. In contrast, these intermediate graphs, e.g. AMR, are hardly useful for a typical, non-linguist user. For traditional summarization, the most wellknown datasets emerged out of the DUC and TAC competitions.2 They provide clusters of news articles with gold-standard summaries. Extending these efforts, several more"
D17-1320,C16-1023,0,0.0623465,"ithout credit history. How useful would the following statements be for you? (P1) students with bad credit history - apply for - federal loans with the FAFSA 2 Extremely Important 2 Very Important 2 Moderately Important 2 Slightly Important 2 Not at all Important (P2) students - encounter - unforeseen financial emergencies 2 Extremely Important 2 Very Important 2 Moderately Important 2 Not at all Important 2 Slightly Important Figure 3: Likert-scale crowdsourcing task with topic description and two example propositions. that build graphs of propositions to create a summary (Fang et al., 2016; Li et al., 2016; Liu et al., 2015; Li, 2015) seem to be particularly related, however, there is one important difference: While they use graphs as an intermediate representation from which a textual summary is then generated, the goal of the proposed task is to create a graph that is directly interpretable and useful for a user. In contrast, these intermediate graphs, e.g. AMR, are hardly useful for a typical, non-linguist user. For traditional summarization, the most wellknown datasets emerged out of the DUC and TAC competitions.2 They provide clusters of news articles with gold-standard summaries. Extendin"
D17-1320,nakano-etal-2010-construction,0,0.159494,"tant difference: While they use graphs as an intermediate representation from which a textual summary is then generated, the goal of the proposed task is to create a graph that is directly interpretable and useful for a user. In contrast, these intermediate graphs, e.g. AMR, are hardly useful for a typical, non-linguist user. For traditional summarization, the most wellknown datasets emerged out of the DUC and TAC competitions.2 They provide clusters of news articles with gold-standard summaries. Extending these efforts, several more specialized corpora have been created: With regard to size, Nakano et al. (2010) present a corpus of summaries for large-scale collections of web pages. Recently, corpora with more heterogeneous documents have been suggested, e.g. (Zopf et al., 2016) and (Benikova et al., 2016). The corpus we present combines these aspects, as it has large clusters of heterogeneous documents, and provides a necessary benchmark to evaluate the proposed task. For concept map generation, one corpus with human-created summary concept maps for student essays has been created (Villalon et al., 2010). In contrast to our corpus, it only deals with single documents, requires a two orders of magnit"
D17-1320,P11-5003,0,0.116095,"t and relation labels from syntactic structures and connect them to build a concept map. However, common task definitions and comparable evaluations are missing. In addition, only a few of them, namely Villalon (2012) and Valerio and Leake (2006), define summarization as their goal and try to compress the input to a substantially smaller size. Our newly proposed task and the created large-cluster dataset fill these gaps as they emphasize the summarization aspect of the task. For the subtask of selecting summary-worthy concepts and relations, techniques developed for traditional summarization (Nenkova and McKeown, 2011) and keyphrase extraction (Hasan and Ng, 2014) are related and applicable. Approaches 2952 Imagine you want to learn something about students loans without credit history. How useful would the following statements be for you? (P1) students with bad credit history - apply for - federal loans with the FAFSA 2 Extremely Important 2 Very Important 2 Moderately Important 2 Slightly Important 2 Not at all Important (P2) students - encounter - unforeseen financial emergencies 2 Extremely Important 2 Very Important 2 Moderately Important 2 Not at all Important 2 Slightly Important Figure 3: Likert-sca"
D17-1320,P13-1132,0,0.0697625,"Missing"
D17-1320,sabou-etal-2014-corpus,0,0.0222414,"ts to crowdsource reference summaries. Workers are asked to read 10 documents and then select 10 summary sentences from them for a reward of $0.05. They discovered several challenges, including poor work quality and the subjectiveness of the annotation task, indicating that crowdsourcing is not useful for this purpose. To overcome these issues, we introduce a new task design, low-context importance annotation, to determine summary-worthy parts of documents. Compared to Lloret et al.’s approach, it is more in line with crowdsourcing best practices, as the tasks are simple, intuitive and small (Sabou et al., 2014) and workers receive reasonable payment (Fort et al., 2011). Most importantly, it is also much more efficient and scalable, as it does not require workers to read all documents in a cluster. 4.1 Task Design We break down the task of importance annotation to the level of single propositions. The goal of our crowdsourcing scheme is to obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary. In contrast to other work, we do not show the documents to the w"
D17-1320,D08-1027,0,0.02666,"Missing"
D17-1320,D16-1252,0,0.14703,"Missing"
D17-1320,C16-1145,0,0.0784345,"that is directly interpretable and useful for a user. In contrast, these intermediate graphs, e.g. AMR, are hardly useful for a typical, non-linguist user. For traditional summarization, the most wellknown datasets emerged out of the DUC and TAC competitions.2 They provide clusters of news articles with gold-standard summaries. Extending these efforts, several more specialized corpora have been created: With regard to size, Nakano et al. (2010) present a corpus of summaries for large-scale collections of web pages. Recently, corpora with more heterogeneous documents have been suggested, e.g. (Zopf et al., 2016) and (Benikova et al., 2016). The corpus we present combines these aspects, as it has large clusters of heterogeneous documents, and provides a necessary benchmark to evaluate the proposed task. For concept map generation, one corpus with human-created summary concept maps for student essays has been created (Villalon et al., 2010). In contrast to our corpus, it only deals with single documents, requires a two orders of magnitude smaller amount of compression of the input and is not publicly available . Other types of information representation that also model concepts and their relationships"
D17-1320,W04-1013,0,\N,Missing
D17-2002,D16-1129,1,0.865221,"me that deals with fallacies in everyday argumentation. Argotario is a multilingual, open-source, platform-independent application with strong educational aspects, accessible at www.argotario.net. 1 Introduction Argumentation in natural language has been gaining much interest in the NLP community in recent years. While understanding the structure of an argument is the predominant task of argument mining/computational argumentation (Mochales and Moens, 2011; Stab and Gurevych, 2014; Habernal and Gurevych, 2017), a parallel strand of research tries to assess qualitative properties of arguments (Habernal and Gurevych, 2016b; Stab and Gurevych, 2017). Yet the gap between theories and everyday argumentation, in understanding what ‘argument quality’ actually is, remains an open research question (Wachsmuth et al., 2017; Habernal and Gurevych, 2016a). 1 Attacking the opponent instead of her argument to irrelevant issues 2 Distracting 7 Proceedings of the 2017 EMNLP System Demonstrations, pages 7–12 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics design challenges centered around the key question: how to make data creation and annotation efforts fun and entertaining in the"
D17-2002,P16-1150,1,0.857458,"me that deals with fallacies in everyday argumentation. Argotario is a multilingual, open-source, platform-independent application with strong educational aspects, accessible at www.argotario.net. 1 Introduction Argumentation in natural language has been gaining much interest in the NLP community in recent years. While understanding the structure of an argument is the predominant task of argument mining/computational argumentation (Mochales and Moens, 2011; Stab and Gurevych, 2014; Habernal and Gurevych, 2017), a parallel strand of research tries to assess qualitative properties of arguments (Habernal and Gurevych, 2016b; Stab and Gurevych, 2017). Yet the gap between theories and everyday argumentation, in understanding what ‘argument quality’ actually is, remains an open research question (Wachsmuth et al., 2017; Habernal and Gurevych, 2016a). 1 Attacking the opponent instead of her argument to irrelevant issues 2 Distracting 7 Proceedings of the 2017 EMNLP System Demonstrations, pages 7–12 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics design challenges centered around the key question: how to make data creation and annotation efforts fun and entertaining in the"
D17-2002,D14-1162,0,0.0805316,"Missing"
D17-2002,N13-1132,0,0.127666,"(see Figure 1b).6 In the serious-game terminology of von Ahn and Dabbish (2008, p. 61), recognizing the correct fallacy type combines the inversion-problem game (the guesser produces the input that was originally given to the describer‘) and a modification of the output-agreement game (the guesser has to produce the same output as the crowd; details will be discussed later in §4). we indirectly aim for high-quality labels. Predicting gold labels can be further parametrized by a threshold in MACE, which then provides only gold label estimates for instances whose entropy is below the threshold (Hovy et al., 2013, p. 1125). However, a deep analysis of the data quality is on our current research agenda. Feedback and Incentives Argotario provides two types of feedback: soft and hard one. For labeling arguments with yet unknown gold label, users get only one point without knowing whether their answer was right (soft feedback). For arguments with already estimated gold labels, hard feedback (see Figure 1d) is given: if the user makes an error, she receives no reward. Apparently, hard feedback is better from the educational point of view as one knows immediately whether her answer was right or wrong; howev"
D17-2002,Q14-1035,0,0.0593167,"there is neither any NLP research dealing with fallacies, nor any resources that would allow for empirical investigation of that matter. The lack of fallacy-annotated linguistic resources and thus the need for creating and labeling a new dataset from scratch motivated us to investigate serious games (also games with a purpose)—a scenario in which a task is gamified and users (players) enjoy playing a game without thinking much of the burden of annotations (von Ahn and Dabbish, 2008; Mayer et al., 2014). Serious games have been successful in NLP tasks that can be easily represented by images (Jurgens and Navigli, 2014; Kazemzadeh et al., 2014) or that can be simplified to assessing a single word or a pair of propositions (Nevˇeˇrilov´a, 2014; Poesio et al., 2013). More complex tasks such as argument understanding, reasoning, or composing pose several An important skill in critical thinking and argumentation is the ability to spot and recognize fallacies. Fallacious arguments, omnipresent in argumentative discourse, can be deceptive, manipulative, or simply leading to ‘wrong moves’ in a discussion. Despite their importance, argumentation scholars and NLP researchers with focus on argumentation quality have"
D17-2002,D14-1006,1,0.835175,"d annotation, for which the serious games methodology offers an appealing, yet unexplored, alternative. We present Argotario, a serious game that deals with fallacies in everyday argumentation. Argotario is a multilingual, open-source, platform-independent application with strong educational aspects, accessible at www.argotario.net. 1 Introduction Argumentation in natural language has been gaining much interest in the NLP community in recent years. While understanding the structure of an argument is the predominant task of argument mining/computational argumentation (Mochales and Moens, 2011; Stab and Gurevych, 2014; Habernal and Gurevych, 2017), a parallel strand of research tries to assess qualitative properties of arguments (Habernal and Gurevych, 2016b; Stab and Gurevych, 2017). Yet the gap between theories and everyday argumentation, in understanding what ‘argument quality’ actually is, remains an open research question (Wachsmuth et al., 2017; Habernal and Gurevych, 2016a). 1 Attacking the opponent instead of her argument to irrelevant issues 2 Distracting 7 Proceedings of the 2017 EMNLP System Demonstrations, pages 7–12 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computationa"
D17-2002,D14-1086,0,0.0133454,"esearch dealing with fallacies, nor any resources that would allow for empirical investigation of that matter. The lack of fallacy-annotated linguistic resources and thus the need for creating and labeling a new dataset from scratch motivated us to investigate serious games (also games with a purpose)—a scenario in which a task is gamified and users (players) enjoy playing a game without thinking much of the burden of annotations (von Ahn and Dabbish, 2008; Mayer et al., 2014). Serious games have been successful in NLP tasks that can be easily represented by images (Jurgens and Navigli, 2014; Kazemzadeh et al., 2014) or that can be simplified to assessing a single word or a pair of propositions (Nevˇeˇrilov´a, 2014; Poesio et al., 2013). More complex tasks such as argument understanding, reasoning, or composing pose several An important skill in critical thinking and argumentation is the ability to spot and recognize fallacies. Fallacious arguments, omnipresent in argumentative discourse, can be deceptive, manipulative, or simply leading to ‘wrong moves’ in a discussion. Despite their importance, argumentation scholars and NLP researchers with focus on argumentation quality have not yet investigated falla"
D17-2002,E17-1092,1,0.829148,"n everyday argumentation. Argotario is a multilingual, open-source, platform-independent application with strong educational aspects, accessible at www.argotario.net. 1 Introduction Argumentation in natural language has been gaining much interest in the NLP community in recent years. While understanding the structure of an argument is the predominant task of argument mining/computational argumentation (Mochales and Moens, 2011; Stab and Gurevych, 2014; Habernal and Gurevych, 2017), a parallel strand of research tries to assess qualitative properties of arguments (Habernal and Gurevych, 2016b; Stab and Gurevych, 2017). Yet the gap between theories and everyday argumentation, in understanding what ‘argument quality’ actually is, remains an open research question (Wachsmuth et al., 2017; Habernal and Gurevych, 2016a). 1 Attacking the opponent instead of her argument to irrelevant issues 2 Distracting 7 Proceedings of the 2017 EMNLP System Demonstrations, pages 7–12 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics design challenges centered around the key question: how to make data creation and annotation efforts fun and entertaining in the first place. To tackle thi"
D17-2002,N16-1070,0,0.0312601,"ad hominem, ad populum, appeal to guilt, slippery slope, hasty generalization, and few others. When scaling up annotations and resource acquisitions, serious games provide an alternative to paid crowdsourcing. Recent successful applications include knowledge base extension (Vannella et al., 2014), answering quizes related to medical topics (Ipeirotis and Gabrilovich, 2014), word definition acquisition (Parasca et al., 2016), or word sense labeling (Venhuizen et al., 2013); where the latter one resembles a standard annotation task with bonus rewards rather than a traditional entertaining game. Niculae and Danescu-Niculescu-Mizil (2016) built a game for guessing places given Google Street View images in order to collect data for investigating constructive discussions. An important aspect of serious games for NLP is their benefit to the users other than getting the annotations done quickly: learning a language in Duolingo4 has more added value than killing zombies (despite its obvious fun factor) in Infection (Vannella et al., 2014). Background and Related Work Fallacies have been an active topic in argumentation theory research in the past several decades. While Aristotle’s legacy was still noticeable in the twentieth centur"
D17-2002,P14-1122,0,0.0466707,"Automatic gold label and quality estimation based solely on the crowd • Multilingual, platform independent, opensource, modular, with native look-and-feel on smartphones 2 fallacies in newswire editorials in major U.S. newspapers before invading Iraq in 2003. These two works rely on a list of several fallacy types, such as ad hominem, ad populum, appeal to guilt, slippery slope, hasty generalization, and few others. When scaling up annotations and resource acquisitions, serious games provide an alternative to paid crowdsourcing. Recent successful applications include knowledge base extension (Vannella et al., 2014), answering quizes related to medical topics (Ipeirotis and Gabrilovich, 2014), word definition acquisition (Parasca et al., 2016), or word sense labeling (Venhuizen et al., 2013); where the latter one resembles a standard annotation task with bonus rewards rather than a traditional entertaining game. Niculae and Danescu-Niculescu-Mizil (2016) built a game for guessing places given Google Street View images in order to collect data for investigating constructive discussions. An important aspect of serious games for NLP is their benefit to the users other than getting the annotations done quick"
D17-2002,W13-0215,0,0.0589051,"ies in newswire editorials in major U.S. newspapers before invading Iraq in 2003. These two works rely on a list of several fallacy types, such as ad hominem, ad populum, appeal to guilt, slippery slope, hasty generalization, and few others. When scaling up annotations and resource acquisitions, serious games provide an alternative to paid crowdsourcing. Recent successful applications include knowledge base extension (Vannella et al., 2014), answering quizes related to medical topics (Ipeirotis and Gabrilovich, 2014), word definition acquisition (Parasca et al., 2016), or word sense labeling (Venhuizen et al., 2013); where the latter one resembles a standard annotation task with bonus rewards rather than a traditional entertaining game. Niculae and Danescu-Niculescu-Mizil (2016) built a game for guessing places given Google Street View images in order to collect data for investigating constructive discussions. An important aspect of serious games for NLP is their benefit to the users other than getting the annotations done quickly: learning a language in Duolingo4 has more added value than killing zombies (despite its obvious fun factor) in Infection (Vannella et al., 2014). Background and Related Work F"
D17-2002,P17-2039,1,0.85205,"roduction Argumentation in natural language has been gaining much interest in the NLP community in recent years. While understanding the structure of an argument is the predominant task of argument mining/computational argumentation (Mochales and Moens, 2011; Stab and Gurevych, 2014; Habernal and Gurevych, 2017), a parallel strand of research tries to assess qualitative properties of arguments (Habernal and Gurevych, 2016b; Stab and Gurevych, 2017). Yet the gap between theories and everyday argumentation, in understanding what ‘argument quality’ actually is, remains an open research question (Wachsmuth et al., 2017; Habernal and Gurevych, 2016a). 1 Attacking the opponent instead of her argument to irrelevant issues 2 Distracting 7 Proceedings of the 2017 EMNLP System Demonstrations, pages 7–12 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics design challenges centered around the key question: how to make data creation and annotation efforts fun and entertaining in the first place. To tackle this open research challenge, we created Argotario—an online serious game for acquiring a dataset with fallacious argumentation. The main research contributions and features"
D17-2002,W16-2522,0,0.0144757,"th native look-and-feel on smartphones 2 fallacies in newswire editorials in major U.S. newspapers before invading Iraq in 2003. These two works rely on a list of several fallacy types, such as ad hominem, ad populum, appeal to guilt, slippery slope, hasty generalization, and few others. When scaling up annotations and resource acquisitions, serious games provide an alternative to paid crowdsourcing. Recent successful applications include knowledge base extension (Vannella et al., 2014), answering quizes related to medical topics (Ipeirotis and Gabrilovich, 2014), word definition acquisition (Parasca et al., 2016), or word sense labeling (Venhuizen et al., 2013); where the latter one resembles a standard annotation task with bonus rewards rather than a traditional entertaining game. Niculae and Danescu-Niculescu-Mizil (2016) built a game for guessing places given Google Street View images in order to collect data for investigating constructive discussions. An important aspect of serious games for NLP is their benefit to the users other than getting the annotations done quickly: learning a language in Duolingo4 has more added value than killing zombies (despite its obvious fun factor) in Infection (Vann"
D18-1171,W15-1402,0,0.0132978,"high novelty score (0.65), while also being as abstract as now. One reason for this non-correlation between concreteness and novelty might be that the automatic induction of concreteness ratings introduces too much noise. However, an experiment where we only use tokens occurring in the manually composed list (Brysbaert et al., 2014) shows similarly low correlation. This could be influenced by artifacts in the concrete1418 5 https://github.com/spotify/annoy Figure 3: POM. Relation between average novelty score of verb lemmas and POM (correlation of ρ = 0.52). ness list: as laid out by Beigman Klebanov et al. (2015), it exhibits some problems. For example, it shows high variance in the annotated concreteness scores for various non-concrete adjectives. But this does not explain the extent of the non-correlation. We thus believe that our results indeed indicate no relation between concreteness and metaphor novelty. And indeed, if a difference of concreteness between components of an expression hints at a metaphor, as is proposed by the conceptual metaphor theory, then it is plausible that it does not hint at novelty of the metaphoric expression at the same time. Consequently, we investigate a feature with"
D18-1171,W14-2302,0,0.0494062,"te nearest neighbors nn(t) from Google News Embeddings (Mikolov et al., 2013) using Annoy.5 The concreteness value for t is then computed by averaging its neighbors’ concreteness values from the concreteness list. Subsequently, we calculate the correlation between the average novelty and the concreteness of the lemmas. Both Pearson correlation (r = 0.04) and Spearman’s rank correlation (ρ = 0.03) are close to zero and indicate no correlation (Figure 2). Thus, while concreteness has been shown to work well as a feature to distinguish between literal and non-literal language in general (Beigman Klebanov et al., 2014; Tsvetkov et al., 2014), it does not seem useful for discerning between novel and conventionalized metaphoric usage in particular. For example, the rather abstract now (1.48) and the very concrete people (4.82) are assigned similarly low novelty scores. On the other hand, justice has a quite high novelty score (0.65), while also being as abstract as now. One reason for this non-correlation between concreteness and novelty might be that the automatic induction of concreteness ratings introduces too much noise. However, an experiment where we only use tokens occurring in the manually composed l"
D18-1171,L16-1724,0,0.151759,"ther resources like WordNet— novel metaphors on the other hand pose a more difficult challenge. But the lack of resources incorporating this distinction leads to few researchers 1 https://www.macmillandictionary.com/dictionary/ american/penumbra 1412 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1412–1424 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics investigating novel metaphors, or the related measure of metaphoricity. They use small datasets that have been manually annotated by experts (Del Tredici and Bel, 2016), or focus crowdsourcing studies on a small number of instances (Dunn, 2014). It is only recently that any work has introduced larger-scale novel metaphor annotations (Parde and Nielsen, 2018). In contrast to our approach, they collect annotations on a relation level (see also Section 2). Annotating metaphors is not an easy task, due to the inherent ambiguity and subjectivity. Therefore, we investigate approaches for annotating novelty in metaphors, before closing the resource gap for token-based annotations by creating a layer of novelty scores. Our contributions are the following: (1) We aug"
D18-1171,P14-2121,0,0.341401,"allenge. But the lack of resources incorporating this distinction leads to few researchers 1 https://www.macmillandictionary.com/dictionary/ american/penumbra 1412 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1412–1424 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics investigating novel metaphors, or the related measure of metaphoricity. They use small datasets that have been manually annotated by experts (Del Tredici and Bel, 2016), or focus crowdsourcing studies on a small number of instances (Dunn, 2014). It is only recently that any work has introduced larger-scale novel metaphor annotations (Parde and Nielsen, 2018). In contrast to our approach, they collect annotations on a relation level (see also Section 2). Annotating metaphors is not an easy task, due to the inherent ambiguity and subjectivity. Therefore, we investigate approaches for annotating novelty in metaphors, before closing the resource gap for token-based annotations by creating a layer of novelty scores. Our contributions are the following: (1) We augment an existing metaphor corpus by assessing metaphor novelty on a token le"
D18-1171,W16-1102,0,0.0539341,"Content tokens include adjectives, adverbs, verbs (without have, be, do), and nouns. For the purpose of this table, metaphors with a novelty score higher than T = 0.5 are considered novel (the possible range is [–1,1]). Metaphoricity (POM) of verbs. The POM describes the inherent potential of a verb to take on a metaphoric meaning, derived from its distributional behavior. They infer that low-POM verbs are only able to have low degrees of metaphoricity, thus can only evoke conventionalized metaphors. Therefore, they propose to exclude such low-POM verbs from novel metaphor detection systems. Haagsma and Bjerva (2016) use violations of selectional preferences (Wilks, 1978) to find novel metaphors. Selectional preferences describe which semantic classes a verb prefers as its direct object. Since they are often mined from large corpora and based on frequency, they argue that this feature is more suited for novel metaphor detection than for general detection of (also conventionalized) metaphors. They evaluate their approach on the VUAMC; however, they acknowledge that their usage of this corpus is not optimal because it contains many conventionalized metaphors. Parde and Nielsen (2018) create a corpus of nove"
D18-1171,W13-0907,0,0.0734819,"the contextual flexibility of a verb. Generally, very novel metaphors also display a high metaphoricity. Therefore, we expect that low-POM verbs (i.e., verbs that occur similarly often in many different contexts) exhibit a low novelty score on average and low variance, while high-POM verbs should show a higher average novelty score. The POM can be regarded as a variant of selectional preference strength, which measures how strongly a verb constrains its direct object in terms of semantic classes. As such, we forgo an analysis of selectional preference violations in favor of examining the POM. Hovy et al. (2013) generalize the notion of selectional preferences to other forms of grammatical relations. However, instead of generating scores, they use dependency trees in an SVM with tree kernels. The POM could be similarly generalized to all POS tags, e.g., by including head and dependent tokens as context. We create the POM for all annotated verbs using the same procedure as Del Tredici and Bel (2016). First, we extract the context (i.e., subject and object) for each occurrence of a verb from a large, parsed corpus (Wikipedia). To compute context vectors, the word embeddings (Levy and Goldberg, 2014) of"
D18-1171,W15-4650,0,0.0432384,"Missing"
D18-1171,N16-1095,0,0.253934,"y), two other tasks require a binary decision (textual entailment recognition, event ordering), and a final task provides three options for the annotators (word sense disambiguation). Sukhareva et al. (2016) utilize crowdsourcing to annotate semantic frames. They design their task as a decision tree, with annotators moving down the tree when annotating. Mohammad et al. (2016) use crowdsourcing for metaphor and emotion annotation in order to investigate their correlation. They employ an ordering approach to annotation, and only consider verbs that already contain a metaphoric sense in WordNet. Kiritchenko and Mohammad (2016) obtain annotations for sentiment associations via crowdsourcing. They use Best–Worst Scaling (Louviere and Woodworth, 1990), an annotation approach which creates scores from ranking annotations. 3 Corpus To obtain a corpus of novel metaphor annotations, we employ an existing metaphor corpus. This can potentially reduce ambiguity for annotators and allows us to focus on the creation of novelty scores. We use the VU Amsterdam Metaphor Corpus (Steen et al., 2010) as the base corpus for our novelty annotations due to its comparably large size and genre diversity. It is comprised of over 200,000 t"
D18-1171,P14-2050,0,0.01275,"the POM. Hovy et al. (2013) generalize the notion of selectional preferences to other forms of grammatical relations. However, instead of generating scores, they use dependency trees in an SVM with tree kernels. The POM could be similarly generalized to all POS tags, e.g., by including head and dependent tokens as context. We create the POM for all annotated verbs using the same procedure as Del Tredici and Bel (2016). First, we extract the context (i.e., subject and object) for each occurrence of a verb from a large, parsed corpus (Wikipedia). To compute context vectors, the word embeddings (Levy and Goldberg, 2014) of subject and object are averaged (if only one of the two is available, the embedding for this token serves as the context). For each verb, the context vectors are then clustered using Birch clustering (Zhang et al., 1996). Finally, the standard deviation between the sizes of the context clusters denotes the POM of the verb. As with our previous experiments, we compute Spearman’s rank correlation between the mean novelty scores of the verb lemmas and the corresponding POMs. We arrive at Spearman’s ρ = 0.52, which indicates moderate correlation (Figure 3). Verbs like pique (POM: 0.218) and sl"
D18-1171,S16-2003,0,0.0939063,"n used for a variety of annotation tasks in NLP, often using different study designs. Snow et al. (2008) obtain good annotation results for five tasks with different setups: two tasks ask for numerical values (affect recognition, word similarity), two other tasks require a binary decision (textual entailment recognition, event ordering), and a final task provides three options for the annotators (word sense disambiguation). Sukhareva et al. (2016) utilize crowdsourcing to annotate semantic frames. They design their task as a decision tree, with annotators moving down the tree when annotating. Mohammad et al. (2016) use crowdsourcing for metaphor and emotion annotation in order to investigate their correlation. They employ an ordering approach to annotation, and only consider verbs that already contain a metaphoric sense in WordNet. Kiritchenko and Mohammad (2016) obtain annotations for sentiment associations via crowdsourcing. They use Best–Worst Scaling (Louviere and Woodworth, 1990), an annotation approach which creates scores from ranking annotations. 3 Corpus To obtain a corpus of novel metaphor annotations, we employ an existing metaphor corpus. This can potentially reduce ambiguity for annotators"
D18-1171,mohler-etal-2014-semi,0,0.0137604,"mple, in “He shot down my arguments,” the more concrete domain of A RMED C ONFLICT (shot) is mapped to the rather abstract domain D ISCUSSION (argument). To analyze the relation between novelty and concreteness, we first extend the concreteness list Figure 2: Concreteness. Relation between novelty score of metaphoric tokens and their concreteness, showing no discernible correlation (ρ = 0.03). A similar picture emerges if we only consider the manually annotated tokens included in the original concreteness list by Brysbaert et al. (2014). by Brysbaert et al. (2014) using a technique similar to Mohler et al. (2014). For a given token t, we extract 20 approximate nearest neighbors nn(t) from Google News Embeddings (Mikolov et al., 2013) using Annoy.5 The concreteness value for t is then computed by averaging its neighbors’ concreteness values from the concreteness list. Subsequently, we calculate the correlation between the average novelty and the concreteness of the lemmas. Both Pearson correlation (r = 0.04) and Spearman’s rank correlation (ρ = 0.03) are close to zero and indicate no correlation (Figure 2). Thus, while concreteness has been shown to work well as a feature to distinguish between literal"
D18-1171,L18-1243,0,0.333985,"w.macmillandictionary.com/dictionary/ american/penumbra 1412 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1412–1424 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics investigating novel metaphors, or the related measure of metaphoricity. They use small datasets that have been manually annotated by experts (Del Tredici and Bel, 2016), or focus crowdsourcing studies on a small number of instances (Dunn, 2014). It is only recently that any work has introduced larger-scale novel metaphor annotations (Parde and Nielsen, 2018). In contrast to our approach, they collect annotations on a relation level (see also Section 2). Annotating metaphors is not an easy task, due to the inherent ambiguity and subjectivity. Therefore, we investigate approaches for annotating novelty in metaphors, before closing the resource gap for token-based annotations by creating a layer of novelty scores. Our contributions are the following: (1) We augment an existing metaphor corpus by assessing metaphor novelty on a token level using crowdsourcing, enabling larger research on novel metaphors, (2) we analyze our corpus for correlation with"
D18-1171,D17-1162,0,0.0398943,"., 2014) do not differentiate between conventionalized and novel metaphors. Some even allow for auxiliary verbs and prepositions to be annotated as metaphors when they are not used in their original sense (e.g., the non-spatially used on in “She wrote a study on metaphors”). While such cases can be filtered out rather easily from any given corpus—e.g., by using POS tag and lemma filters—many conventionalized metaphors persist. Existing work avoids this problem partially by only annotating certain grammatical constructions, such as adjective–noun or verb–object relations (Shutova et al., 2016; Rei et al., 2017). However, these too usually do not distinguish between conventionalized and novel metaphors. Following Shutova (2015), we deem the distinction between conventionalized and novel metaphors important, because the meaning of conventionalized metaphors can usually be found in dictionaries or other resources like WordNet— novel metaphors on the other hand pose a more difficult challenge. But the lack of resources incorporating this distinction leads to few researchers 1 https://www.macmillandictionary.com/dictionary/ american/penumbra 1412 Proceedings of the 2018 Conference on Empirical Methods in"
D18-1171,N16-1020,0,0.0338966,", 2010; Tsvetkov et al., 2014) do not differentiate between conventionalized and novel metaphors. Some even allow for auxiliary verbs and prepositions to be annotated as metaphors when they are not used in their original sense (e.g., the non-spatially used on in “She wrote a study on metaphors”). While such cases can be filtered out rather easily from any given corpus—e.g., by using POS tag and lemma filters—many conventionalized metaphors persist. Existing work avoids this problem partially by only annotating certain grammatical constructions, such as adjective–noun or verb–object relations (Shutova et al., 2016; Rei et al., 2017). However, these too usually do not distinguish between conventionalized and novel metaphors. Following Shutova (2015), we deem the distinction between conventionalized and novel metaphors important, because the meaning of conventionalized metaphors can usually be found in dictionaries or other resources like WordNet— novel metaphors on the other hand pose a more difficult challenge. But the lack of resources incorporating this distinction leads to few researchers 1 https://www.macmillandictionary.com/dictionary/ american/penumbra 1412 Proceedings of the 2018 Conference on E"
D18-1171,shutova-teufel-2010-metaphor,0,0.0425578,"a more basic meaning that can be understood in comparison with the one it expresses in its current context. More basic is described as being: • More concrete; what they evoke is easier to imagine, see, hear, feel, smell, and taste. • Related to bodily action. • More precise (as opposed to vague). • Historically older. They note that this basic meaning does not necessarily have to be the most frequent one. An extended version, MIPVU, was used to annotate parts of the British National Corpus (BNC Consortium, 2007, BNC), resulting in the VU Amsterdam Metaphor Corpus (Steen et al., 2010, VUAMC). Shutova and Teufel (2010) adapt the MIP as a prerequisite to annotating source and target domains of metaphor—the metaphoric mapping—in parts of the BNC. Others use rather relaxed guidelines. Tsvetkov et al. (2014) rely on intuitive definitions by their annotators, not specifying metaphor more closely. They ask their annotators to mark words that “are used non-literally in the following sentences.” Jang et al. (2015) provide a Wikipedia definition of metaphor to users in a crowdsourcing study. Subsequently, the users are tasked with annotating forum posts by deciding “whether the highlighted word is used metaphoricall"
D18-1171,D08-1027,0,0.199613,"Missing"
D18-1171,L16-1338,1,0.855759,"ing corpus using crowdsourcing; i.e., by splitting up the task in many small chunks which different, non-expert annotators are instructed to complete. Crowdsourcing has been used for a variety of annotation tasks in NLP, often using different study designs. Snow et al. (2008) obtain good annotation results for five tasks with different setups: two tasks ask for numerical values (affect recognition, word similarity), two other tasks require a binary decision (textual entailment recognition, event ordering), and a final task provides three options for the annotators (word sense disambiguation). Sukhareva et al. (2016) utilize crowdsourcing to annotate semantic frames. They design their task as a decision tree, with annotators moving down the tree when annotating. Mohammad et al. (2016) use crowdsourcing for metaphor and emotion annotation in order to investigate their correlation. They employ an ordering approach to annotation, and only consider verbs that already contain a metaphoric sense in WordNet. Kiritchenko and Mohammad (2016) obtain annotations for sentiment associations via crowdsourcing. They use Best–Worst Scaling (Louviere and Woodworth, 1990), an annotation approach which creates scores from r"
D18-1171,P14-1024,0,0.469325,"with the more abstract budget, the metaphoric use as meaning limited can be readily understood. In contrast, the use of penumbra in (2) is more creative and novel. Its literal meaning is “an area covered by the outer part of a shadow.”1 Its metaphoric meaning is seldom encountered: Shadows follow objects that cast them, and especially penumbras can be perceived as having fuzzy outlines; attributes which are picked up by the metaphorical sense of a rather unspecified group of people following someone in differing vicinity. Common linguistic metaphor definitions used in NLP (Steen et al., 2010; Tsvetkov et al., 2014) do not differentiate between conventionalized and novel metaphors. Some even allow for auxiliary verbs and prepositions to be annotated as metaphors when they are not used in their original sense (e.g., the non-spatially used on in “She wrote a study on metaphors”). While such cases can be filtered out rather easily from any given corpus—e.g., by using POS tag and lemma filters—many conventionalized metaphors persist. Existing work avoids this problem partially by only annotating certain grammatical constructions, such as adjective–noun or verb–object relations (Shutova et al., 2016; Rei et a"
D18-1402,W17-5115,0,0.107181,"ll outperforms common attention-based approaches in two- and three-label cross-topic experiments. (4) We further improve the performance of the modified LSTM cell by leveraging additional data for topic relevance in a multi-task learning setup. (5) In the more challenging setup of cross-topic experiments, we show that our models yield considerably better performance than common BiLSTM models when little data of the target topic is available. 2 Related work Most existing approaches treat argument mining at the discourse level, focusing on tasks such as segmenting argumentative discourse units (Ajjour et al., 2017; Goudas et al., 2014), classifying the function of argumentative discourse units (for example, as claims or premises) (Mochales-Palau and Moens, 2009; Stab and Gurevych, 2014), and recognizing argumentative discourse relations (Eger et al., 2017; Stab and Gurevych, 2017; Nguyen and Litman, 2016). These discourse-level approaches address the identification of argumentative structures within a single document but do not consider relevance to externally defined topics. To date, there has been little research on the identification of topic-relevant arguments for argument search. Wachsmuth et al."
D18-1402,N16-1165,0,0.0257464,"ny argument relevant to a given topic. The fact that we are concerned with retrieval of arguments also sets our work apart from the discourse-agnostic stance detection task of Mohammad et al. (2016), which is concerned with the identification of sentences expressing support or opposition to a given topic, irrespective of whether those sentences contain supporting evidence (as opposed to mere statements of opinion). Cross-domain AM experiments have so far been conducted only for discourse-level tasks such as claim identification (Daxenberger et al., 2017), argumentative segment identification (Al-Khatib et al., 2016), and argumentative unit segmentation (Ajjour et al., 2017). However, the discourse-level argumentation models these studies employ seem to be highly dependent on the text types for which they were designed; they do not work well when applied to other text types (Daxenberger et al., 2017). The crucial difference between our own work and prior cross-domain experiments is that we investigate AM from heterogeneous texts across different topics instead of studying specific discourse-level AM tasks across restricted text types of existing corpora. 3 Corpus creation There exists a great diversity in"
D18-1402,J96-2004,0,0.501795,"Missing"
D18-1402,P17-1110,0,0.0297679,"l setting, we have to modify the transfer model slightly for the DIP2016 corpus, since it provides only two labels for each training sample. In this case, we simply add a layer with two neurons on top of the layer with three neurons for training with the DIP2016 corpus and remove it afterwards for training with our corpus. 4We only use 300K of the corpus’s 600K samples to ease hyperparameter tuning for our computation-heavy models. Multi-task learning (mtl). For mtl, we use a shared–private model (Liu et al., 2017), which showed promising results for text classification and word segmentation (Chen et al., 2017). (We also experimented with their adversarial approach to learn topic-invariant features, but abandoned this due to low scores.) The mtl base model consists of a private recurrent neural network (RNN) for both the auxiliary dataset and our dataset, plus a shared RNN that both datasets use (Fig. 3). The last hidden states of the RNNs are concatenated and fed through a dense layer and a softmax activation function. The model is trained in an alternating fashion—i.e., after each epoch the loss for the other dataset is minimized until each dataset has run for the set number of epochs, where the l"
D18-1402,D17-1218,1,0.947385,"2). Automating the argument search process could ease much of the manual effort involved in these tasks, particularly if it can be made to robustly handle arguments from different text types and topics. But despite its obvious usefulness, this sort of argument search has attracted little attention in the research community. This may be due in part to the limitations of the underlying models and training resources, particularly as they relate to heterogeneous sources. That is, most current approaches to AM are designed for use with particular text types, faring poorly when applied to new data (Daxenberger et al., 2017). Indeed, as Habernal et al. (2014) observe, while there is a great diversity of perspectives on how arguments can be best characterized and modelled, there is no “one-size-fits-all” argumentation theory that applies to the variety of text sources found on the Web. To approach these challenges, we propose the novel task of topic-based sentential argument mining. Our contributions are as follows: (1) We propose a new argument annotation scheme applicable to the information-seeking perspective of argument search. We show it to be general enough for use on heterogeneous data sources, and simple e"
D18-1402,P17-1002,1,0.832723,"In the more challenging setup of cross-topic experiments, we show that our models yield considerably better performance than common BiLSTM models when little data of the target topic is available. 2 Related work Most existing approaches treat argument mining at the discourse level, focusing on tasks such as segmenting argumentative discourse units (Ajjour et al., 2017; Goudas et al., 2014), classifying the function of argumentative discourse units (for example, as claims or premises) (Mochales-Palau and Moens, 2009; Stab and Gurevych, 2014), and recognizing argumentative discourse relations (Eger et al., 2017; Stab and Gurevych, 2017; Nguyen and Litman, 2016). These discourse-level approaches address the identification of argumentative structures within a single document but do not consider relevance to externally defined topics. To date, there has been little research on the identification of topic-relevant arguments for argument search. Wachsmuth et al. (2017) present a generic argument search framework. However, it relies on already-structured arguments from debate portals and is not yet able to retrieve arguments from arbitrary texts. Levy et al. (2014) investigate the identification of topic-"
D18-1402,P82-1020,0,0.837696,"Missing"
D18-1402,N13-1132,0,0.145887,"Missing"
D18-1402,P17-2032,0,0.113521,"to retrieve arguments from arbitrary texts. Levy et al. (2014) investigate the identification of topic-relevant claims, an approach that was later extended with evidence extraction to mine supporting statements for claims (Rinott et al., 2015). However, both approaches are designed to mine arguments from Wikipedia articles; it is unclear whether their annotation scheme is applicable to other text types. It is also uncertain that it can be easily and accurately applied by untrained annotators, since it requires unitizing (i.e., finding the boundaries of argument components at the token level). Hua and Wang (2017) identify sentences in cited documents that have been used by an editor to formulate an argument. By contrast, we do not limit our approach to the identification of sentences related to a given argument, but rather focus on the retrieval of any argument relevant to a given topic. The fact that we are concerned with retrieval of arguments also sets our work apart from the discourse-agnostic stance detection task of Mohammad et al. (2016), which is concerned with the identification of sentences expressing support or opposition to a given topic, irrespective of whether those sentences contain sup"
D18-1402,C14-1141,0,0.104371,"nizing argumentative discourse relations (Eger et al., 2017; Stab and Gurevych, 2017; Nguyen and Litman, 2016). These discourse-level approaches address the identification of argumentative structures within a single document but do not consider relevance to externally defined topics. To date, there has been little research on the identification of topic-relevant arguments for argument search. Wachsmuth et al. (2017) present a generic argument search framework. However, it relies on already-structured arguments from debate portals and is not yet able to retrieve arguments from arbitrary texts. Levy et al. (2014) investigate the identification of topic-relevant claims, an approach that was later extended with evidence extraction to mine supporting statements for claims (Rinott et al., 2015). However, both approaches are designed to mine arguments from Wikipedia articles; it is unclear whether their annotation scheme is applicable to other text types. It is also uncertain that it can be easily and accurately applied by untrained annotators, since it requires unitizing (i.e., finding the boundaries of argument components at the token level). Hua and Wang (2017) identify sentences in cited documents that"
D18-1402,P17-1001,0,0.0268062,"ond time, we keep the trained model’s weights and train it with our own corpus. For the threelabel setting, we have to modify the transfer model slightly for the DIP2016 corpus, since it provides only two labels for each training sample. In this case, we simply add a layer with two neurons on top of the layer with three neurons for training with the DIP2016 corpus and remove it afterwards for training with our corpus. 4We only use 300K of the corpus’s 600K samples to ease hyperparameter tuning for our computation-heavy models. Multi-task learning (mtl). For mtl, we use a shared–private model (Liu et al., 2017), which showed promising results for text classification and word segmentation (Chen et al., 2017). (We also experimented with their adversarial approach to learn topic-invariant features, but abandoned this due to low scores.) The mtl base model consists of a private recurrent neural network (RNN) for both the auxiliary dataset and our dataset, plus a shared RNN that both datasets use (Fig. 3). The last hidden states of the RNNs are concatenated and fed through a dense layer and a softmax activation function. The model is trained in an alternating fashion—i.e., after each epoch the loss for t"
D18-1402,P14-5010,0,0.00272474,"ecting eight topics (see Table 2) from online lists of controversial topics.2 For each topic, we made a Google query for the topic name, removed results not archived by the Wayback Machine,3 and truncated the list to the top 50 results. This resulted in a set of persistent, topic-relevant, largely polemical Web documents representing a range of genres and text types, including news reports, editorials, blogs, debate forums, and encyclopedia articles. We preprocessed each document with Apache Tika (Mattmann and Zitting, 2011) to remove boilerplate text. We then used the Stanford CoreNLP tools (Manning et al., 2014) to perform tokenization, sentence segmentation, and part-ofspeech tagging on the remaining text, and removed all sentences without verbs or with less than three tokens. This left us with a raw dataset of 27,520 sentences (about 2,700 to 4,400 per topic). Annotators classified the sentences using a browser-based interface that presents a set of in2https://www.questia.com/library/ controversial-topics, https://www.procon.org/ 3https://web.archive.org/ structions, a topic, a list of sentences, and a multiplechoice form for specifying whether each sentence is a supporting argument, an opposing ar"
D18-1402,S16-1003,0,0.119898,"Missing"
D18-1402,P16-1107,0,0.0272193,"c experiments, we show that our models yield considerably better performance than common BiLSTM models when little data of the target topic is available. 2 Related work Most existing approaches treat argument mining at the discourse level, focusing on tasks such as segmenting argumentative discourse units (Ajjour et al., 2017; Goudas et al., 2014), classifying the function of argumentative discourse units (for example, as claims or premises) (Mochales-Palau and Moens, 2009; Stab and Gurevych, 2014), and recognizing argumentative discourse relations (Eger et al., 2017; Stab and Gurevych, 2017; Nguyen and Litman, 2016). These discourse-level approaches address the identification of argumentative structures within a single document but do not consider relevance to externally defined topics. To date, there has been little research on the identification of topic-relevant arguments for argument search. Wachsmuth et al. (2017) present a generic argument search framework. However, it relies on already-structured arguments from debate portals and is not yet able to retrieve arguments from arbitrary texts. Levy et al. (2014) investigate the identification of topic-relevant claims, an approach that was later extende"
D18-1402,D17-1035,1,0.743308,"Missing"
D18-1402,D15-1050,0,0.0990091,"mentative structures within a single document but do not consider relevance to externally defined topics. To date, there has been little research on the identification of topic-relevant arguments for argument search. Wachsmuth et al. (2017) present a generic argument search framework. However, it relies on already-structured arguments from debate portals and is not yet able to retrieve arguments from arbitrary texts. Levy et al. (2014) investigate the identification of topic-relevant claims, an approach that was later extended with evidence extraction to mine supporting statements for claims (Rinott et al., 2015). However, both approaches are designed to mine arguments from Wikipedia articles; it is unclear whether their annotation scheme is applicable to other text types. It is also uncertain that it can be easily and accurately applied by untrained annotators, since it requires unitizing (i.e., finding the boundaries of argument components at the token level). Hua and Wang (2017) identify sentences in cited documents that have been used by an editor to formulate an argument. By contrast, we do not limit our approach to the identification of sentences related to a given argument, but rather focus on"
D18-1402,W17-5106,0,0.0442389,"jour et al., 2017; Goudas et al., 2014), classifying the function of argumentative discourse units (for example, as claims or premises) (Mochales-Palau and Moens, 2009; Stab and Gurevych, 2014), and recognizing argumentative discourse relations (Eger et al., 2017; Stab and Gurevych, 2017; Nguyen and Litman, 2016). These discourse-level approaches address the identification of argumentative structures within a single document but do not consider relevance to externally defined topics. To date, there has been little research on the identification of topic-relevant arguments for argument search. Wachsmuth et al. (2017) present a generic argument search framework. However, it relies on already-structured arguments from debate portals and is not yet able to retrieve arguments from arbitrary texts. Levy et al. (2014) investigate the identification of topic-relevant claims, an approach that was later extended with evidence extraction to mine supporting statements for claims (Rinott et al., 2015). However, both approaches are designed to mine arguments from Wikipedia articles; it is unclear whether their annotation scheme is applicable to other text types. It is also uncertain that it can be easily and accuratel"
D18-1402,N16-1170,0,0.0127503,"ludes the topic vector directly in the LSTM cell. Outer-attention BiLSTM (outer-att). To let the model learn which parts of the sentence are relevant (or irrelevant) to the given topic, we use an attentionbased neural network (Bahdanau et al., 2014) that learns an importance weighting of the input words depending on the given topic. In particular, we adopt an outer-attention mechanism similar to the one proposed by Hermann et al. (2015), which has achieved state-of-the-art results in related tasks such as natural language inference and recognizing textual entailment (Rocktäschel et al., 2015; Wang and Jiang, 2016). We combine the attention mechanism with a common BiLSTM model and, at time step t, determine the importance weighting for each hidden state h (t) as W h h (t) + W p p ) m (t) = tanh(W (1) wTmm (t)) exp(w fattention (hh (t), p ) = Í wTmm (t)) t exp(w (2) where W h , W p , and w m are trainable parameters of the attention mechanism and p is the average of all word embeddings of topic words v1, . . . , vnτ . Using the importance weighting, we determine the final, weighted hidden output state s as αt ∝ fattention (hh (t), p ) n Õ s= h (t)αt . xt xt p ht-1 Integrating topic information (3) (4) t="
D18-1402,N18-5005,1,0.880555,"Missing"
D18-1402,D14-1006,1,0.860292,"aging additional data for topic relevance in a multi-task learning setup. (5) In the more challenging setup of cross-topic experiments, we show that our models yield considerably better performance than common BiLSTM models when little data of the target topic is available. 2 Related work Most existing approaches treat argument mining at the discourse level, focusing on tasks such as segmenting argumentative discourse units (Ajjour et al., 2017; Goudas et al., 2014), classifying the function of argumentative discourse units (for example, as claims or premises) (Mochales-Palau and Moens, 2009; Stab and Gurevych, 2014), and recognizing argumentative discourse relations (Eger et al., 2017; Stab and Gurevych, 2017; Nguyen and Litman, 2016). These discourse-level approaches address the identification of argumentative structures within a single document but do not consider relevance to externally defined topics. To date, there has been little research on the identification of topic-relevant arguments for argument search. Wachsmuth et al. (2017) present a generic argument search framework. However, it relies on already-structured arguments from debate portals and is not yet able to retrieve arguments from arbitr"
D18-1402,J17-3005,1,0.808746,"nging setup of cross-topic experiments, we show that our models yield considerably better performance than common BiLSTM models when little data of the target topic is available. 2 Related work Most existing approaches treat argument mining at the discourse level, focusing on tasks such as segmenting argumentative discourse units (Ajjour et al., 2017; Goudas et al., 2014), classifying the function of argumentative discourse units (for example, as claims or premises) (Mochales-Palau and Moens, 2009; Stab and Gurevych, 2014), and recognizing argumentative discourse relations (Eger et al., 2017; Stab and Gurevych, 2017; Nguyen and Litman, 2016). These discourse-level approaches address the identification of argumentative structures within a single document but do not consider relevance to externally defined topics. To date, there has been little research on the identification of topic-relevant arguments for argument search. Wachsmuth et al. (2017) present a generic argument search framework. However, it relies on already-structured arguments from debate portals and is not yet able to retrieve arguments from arbitrary texts. Levy et al. (2014) investigate the identification of topic-relevant claims, an appro"
D18-1445,P17-1138,0,0.0694383,"Missing"
D18-1445,W04-1013,0,0.0138023,"ty. Both simulation and real-user experiments suggest that our method significantly advances the state of the art. Our source code is freely available at https://github.com/UKPLab/ emnlp2018-april. 1 Introduction With the rapid growth of text-based information on the Internet, automatic document summarisation attracts increasing research attention from the Natural Language Processing (NLP) community (Nenkova and McKeown, 2012). Most existing document summarisation techniques require access to reference summaries to train their systems. However, obtaining reference summaries is very expensive: Lin (2004) reported that 3,000 hours of human effort were required for a simple evaluation of the summaries for the Document Understanding Conferences (DUC). Although previous work has proposed heuristics-based methods to summarise without reference summaries (Ryang and Abekawa, 2012; Rioux et al., 2014), the gap between their performance and the upper bound is still large: the ROUGE-2 upper bound of .212 on DUC’04 (P.V.S. and Meyer, 2017) is, for example, twice as high as Rioux et al.’s (2014) .114. The Structured Prediction from Partial Information (SPPI) framework has been proposed to learn to make s"
D18-1445,N18-1158,0,0.0307794,"el, and used the Temporal Difference (TD) algorithm (Sutton, 1984) to solve the MDP. In a follow-up work, Rioux et al. (2014) proposed a different reward function, which also did not require reference summaries; their experiments suggested that using their new reward function improved the summary quality. Henß et al. (2015) proposed a different RL formulation of EMDS and jointly used supervised learning and RL to perform the task. However, their method requires the access to reference summaries. More recent works applied encoderdecoder-based RL to document summarisation (Ranzato et al., 2015; Narayan et al., 2018; Paulus et al., 2017; Pasunuru and Bansal, 2018). These works outperformed standard encoder-decoder as P.V.S. and Meyer (2017) proposed a bigrambased interactive EMDS framework. They asked users to label important bigrams in candidate summaries and used integer linear programming (ILP) to extract sentences covering as many important bigrams as possible. Their method requires no access to reference summaries, but it requires considerable human effort during the interaction: in simulation experiments, their system needed to collect up to 350 bigram annotations from a (simulated) user. In additi"
D18-1445,N18-2102,0,0.025973,"lgorithm (Sutton, 1984) to solve the MDP. In a follow-up work, Rioux et al. (2014) proposed a different reward function, which also did not require reference summaries; their experiments suggested that using their new reward function improved the summary quality. Henß et al. (2015) proposed a different RL formulation of EMDS and jointly used supervised learning and RL to perform the task. However, their method requires the access to reference summaries. More recent works applied encoderdecoder-based RL to document summarisation (Ranzato et al., 2015; Narayan et al., 2018; Paulus et al., 2017; Pasunuru and Bansal, 2018). These works outperformed standard encoder-decoder as P.V.S. and Meyer (2017) proposed a bigrambased interactive EMDS framework. They asked users to label important bigrams in candidate summaries and used integer linear programming (ILP) to extract sentences covering as many important bigrams as possible. Their method requires no access to reference summaries, but it requires considerable human effort during the interaction: in simulation experiments, their system needed to collect up to 350 bigram annotations from a (simulated) user. In addition, they did not consider noise in users’ annotat"
D18-1445,P17-1124,1,0.61105,"Missing"
D18-1445,D14-1075,0,0.58441,"Missing"
D18-1445,N18-1152,0,0.26615,"sed to learn to make structured predictions without access to gold standard data (Sokolov et al., 2016b). SPPI is an interactive NLP paradigm: It interacts with a user for multiple rounds and learns from the user’s feedback. SPPI can learn from two forms of feedback: point-based feedback, i.e. a numeric score for the presented prediction, or preference-based feedback, i.e. a preference over a pair of predictions. Providing preference-based feedback yields a lower cognitive burden for humans than providing ratings or categorical labels (Thurstone, 1927; Kendall, 1948; Kingsley and Brown, 2010; Zopf, 2018). Preference-based SPPI has been applied to multiple NLP applications, including text classification, chunking and machine translation (Sokolov et al., 2016a; Kreutzer et al., 2017). However, SPPI has prohibitively high sample complexities in the aforementioned NLP tasks, as it needs at least hundreds of thousands rounds of interaction to make near-optimal predictions, even with simulated “perfect” users. Figure 1a illustrates the workflow of the preference-based SPPI. To reduce the sample complexity, in this work, we propose a novel preference-based interactive learning framework, called APRI"
D18-1445,D12-1024,0,0.327898,"on the Internet, automatic document summarisation attracts increasing research attention from the Natural Language Processing (NLP) community (Nenkova and McKeown, 2012). Most existing document summarisation techniques require access to reference summaries to train their systems. However, obtaining reference summaries is very expensive: Lin (2004) reported that 3,000 hours of human effort were required for a simple evaluation of the summaries for the Document Understanding Conferences (DUC). Although previous work has proposed heuristics-based methods to summarise without reference summaries (Ryang and Abekawa, 2012; Rioux et al., 2014), the gap between their performance and the upper bound is still large: the ROUGE-2 upper bound of .212 on DUC’04 (P.V.S. and Meyer, 2017) is, for example, twice as high as Rioux et al.’s (2014) .114. The Structured Prediction from Partial Information (SPPI) framework has been proposed to learn to make structured predictions without access to gold standard data (Sokolov et al., 2016b). SPPI is an interactive NLP paradigm: It interacts with a user for multiple rounds and learns from the user’s feedback. SPPI can learn from two forms of feedback: point-based feedback, i.e. a"
D18-1445,Q18-1026,1,0.873039,"date summaries and used integer linear programming (ILP) to extract sentences covering as many important bigrams as possible. Their method requires no access to reference summaries, but it requires considerable human effort during the interaction: in simulation experiments, their system needed to collect up to 350 bigram annotations from a (simulated) user. In addition, they did not consider noise in users’ annotations but simulated perfect oracles. Preference learning aims at obtaining the ranking (i.e. total ordering) of objects from pairwise preferences (F¨urnkranz and H¨ullermeier, 2010). Simpson and Gurevych (2018) proposed to use an improved Gaussian process preference learning (Chu and Ghahramani, 2005) for learning to rank arguments in terms of convincingness from crowdsourced annotations. However, such Bayesian methods can hardly scale and suffer from high computation time. Zopf (2018) recently proposed to learn a sentence ranker from preferences. The resulting ranker can be used to identify the important sentences and thus to evaluate the quality of the summaries. His study also suggests that providing sentence preferences takes less time than writing reference summaries. APRIL not only learns a ra"
D18-1445,P16-1152,0,0.110734,"a simple evaluation of the summaries for the Document Understanding Conferences (DUC). Although previous work has proposed heuristics-based methods to summarise without reference summaries (Ryang and Abekawa, 2012; Rioux et al., 2014), the gap between their performance and the upper bound is still large: the ROUGE-2 upper bound of .212 on DUC’04 (P.V.S. and Meyer, 2017) is, for example, twice as high as Rioux et al.’s (2014) .114. The Structured Prediction from Partial Information (SPPI) framework has been proposed to learn to make structured predictions without access to gold standard data (Sokolov et al., 2016b). SPPI is an interactive NLP paradigm: It interacts with a user for multiple rounds and learns from the user’s feedback. SPPI can learn from two forms of feedback: point-based feedback, i.e. a numeric score for the presented prediction, or preference-based feedback, i.e. a preference over a pair of predictions. Providing preference-based feedback yields a lower cognitive burden for humans than providing ratings or categorical labels (Thurstone, 1927; Kendall, 1948; Kingsley and Brown, 2010; Zopf, 2018). Preference-based SPPI has been applied to multiple NLP applications, including text class"
D18-1472,P17-2037,0,0.0552652,"Missing"
D18-1472,D14-1082,0,0.0440121,"twork learning, e.g., in Parascandolo et al. (2016), where it was shown to enable faster learning on certain tasks than more established functions. penalized tanh (Xu et al., 2016) has been defined in analogy to the LReLU functions, which can be thought of as “penalizing” the identity function in the negative region. The reported good performance of penalized tanh on CIFAR-100 (Krizhevsky, 2009) lets the authors speculate that the slope of activation functions near the origin may be crucial for learning. linear is the identity function, f (x) = x. cube is the function f (x) = x3 , proposed in Chen and Manning (2014) for an MLP used in dependency parsing. elu (Clevert et al., 2015) has been proposed as (yet another) variant of relu that assumes negative values, making the mean activations more zero-centered. selu is a scaled variant of elu used in Klambauer et al. (2017) in the context of socalled self-normalizing neural nets. Properties of activation functions Many properties of activation functions have been speculated to be crucial for successful learning. Some of these are listed in Table 2, together with brief de4416 Property Description 0 derivative f zero-centered saturating monotonicity range cent"
D18-1472,D17-1070,0,0.0200354,"(6): The AM dataset with original split in train, dev, and test (Eger et al., 2017), and with InferSent input embeddings. (7): the same mini-experiment with Sent2Vec-unigram embeddings. Model We experiment with a multi-layer perceptron (MLP) applied to sentence-level classification tasks. That is, input to the MLP is a sentence or short text, represented as a fixed-size vector embedding. The output of the MLP is a label which classifies the sentence or short text. We use two sentence representation techniques, namely, Sent2Vec (Pagliardini et al., 2018), of dimensionality 600, and InferSent (Conneau et al., 2017), of dimensionality 4096. Our MLP has the form: xi = f (xi−1 · Wi + bi ) y = softmax(xN WN +1 + bN +1 ) where x0 is the input representation, x1 , . . . , xN are hidden layer representations, and y is the output, a probability distribution over the classes in the classification task. Vectors b and matrices W are the learnable parameters of our network. The activation function is given by f and ranges over the choices described in §2. Data We use four sentence classification tasks, namely: movie review classification (MR), subjectivitiy classification (SUBJ), question type classification (TREC)"
D18-1472,P17-1002,1,0.888027,"Missing"
D18-1472,P17-1052,0,0.0227763,"is < 0, we choose it to be m). sin function wins, followed by penalized tanh, maxout and swish. The difference between the best mean function, sin, and the worst, cube, is more than 30pp. This means that using cube is much riskier and requires more careful hyperparameter search compared to sin and the other top performers. 3.2 CNN & Document Classification Model Our second paradigm is document classification using a CNN. This approach has been popularized in NLP by the ground-breaking work of Kim (2014). Even though shallow CNNs do not reach state-of-the-art results on large datasets anymore (Johnson and Zhang, 2017), simple approaches like (shallow) CNNs are still very competitive for smaller datasets (Joulin et al., 2016). Our model operates on token-level and first embeds a sequence of tokens x1 , . . . , xn , represented as 1-hot vectors, into learnable embeddings x1 , . . . , xn . The model then applies 1Dconvolution on top of these embeddings. That is, a filter w of size h takes h successive embeddings xi:i+h−1 , performs a scalar product and obtains a feature ci : ci = f (w · xi:i+h−1 + b). Here, f is the activation function and b is a bias term. We take the number nk of different filters as a hype"
D18-1472,D14-1181,0,0.00399313,"µ and std s; µ = m is the default value from keras for the specific optimizer (if drawn learning rate is < 0, we choose it to be m). sin function wins, followed by penalized tanh, maxout and swish. The difference between the best mean function, sin, and the worst, cube, is more than 30pp. This means that using cube is much riskier and requires more careful hyperparameter search compared to sin and the other top performers. 3.2 CNN & Document Classification Model Our second paradigm is document classification using a CNN. This approach has been popularized in NLP by the ground-breaking work of Kim (2014). Even though shallow CNNs do not reach state-of-the-art results on large datasets anymore (Johnson and Zhang, 2017), simple approaches like (shallow) CNNs are still very competitive for smaller datasets (Joulin et al., 2016). Our model operates on token-level and first embeds a sequence of tokens x1 , . . . , xn , represented as 1-hot vectors, into learnable embeddings x1 , . . . , xn . The model then applies 1Dconvolution on top of these embeddings. That is, a filter w of size h takes h successive embeddings xi:i+h−1 , performs a scalar product and obtains a feature ci : ci = f (w · xi:i+h−1"
D18-1472,P14-2050,0,0.0591638,"Missing"
D18-1472,N18-1049,0,0.0324684,"Missing"
D18-1472,D17-1035,1,0.859144,"softmax(hi V + c) Here, wi are (pre-trained) word embeddings of words wi . Vectors b, c and matrices U, V, W are parameters to be learned during training. The above describes an RNN with only one hidden layer, hi , at each time step, but we consider the generalized form with N ≥ 1 hidden layers; we also choose a bidirectional RNN in which the hidden outputs of a forward RNN and a backward RNN are combined. RNNs are particularly deep We report macro-F1 for mini-experiments (1-4) and accuracy for (5-6). For our RNN implementations, we use the accompanying code of (the state-of-the-art model of) Reimers and Gurevych (2017), which is implemented in keras. The network uses a CRF layer as an output layer. We use a batch size of 32, train for 50 epochs and use a patience of 5 for early stopping. Results Figure 3 shows best and mean results, averaged over all 6 mini-experiments, for each activation function. We exclude prelu and the maxout functions because the keras implementation 4420 does not natively support these activation functions for RNNs. We also exclude the cube function because it performed very badly. it = σ([ht−1 ; xt ] · Wi ), 90 ot = σ([ht−1 ; xt ] · Wo ) 92 80 ct = ft ct−1 + it τ ([ht−1 ; xt ] · Wc"
D18-1472,N18-2006,1,0.878384,"Missing"
D18-1472,J17-3005,1,0.867373,"Missing"
D18-2020,P14-5010,0,0.00247773,"rate request is sent to the backend service for each processing step of the QA pipeline. Thus, we are able to show the results to the user as they are being delivered by the back-end services. The front-end is responsible for aggregating and visualizing the information after each step. In case any service fails, a partial result from the previous steps would be available to the user. The back-end services include the preprocessing, the entity linking and the semantic parsing modules. The pre-processing module performs tokenization and part-of speech tagging using the Stanford CoreNLP toolkit (Manning et al., 2014). The entity linking module recognizes mentions of the KB entities in the input question. We provide a default model for entity linking on Wikidata that is freely available feature-based implementation of Sorokin and Gurevych (2018a). The semantic parsing module includes the construction of structured semantic representations and a learned model that selects the correct representation. The integrated model uses a convolutional neural network architecture to learn vector encodings for questions and semantic representations (Sorokin and Gurevych, 2017). We provide an integrated model for demonst"
D18-2020,P14-1133,0,0.205173,"der to retrieve the correct answer. It is common to break down the task into three main steps: entity linking, semantic parsing or relation disambiguation and answer retrieval. We show in Figure 1 how the outlined steps lead to an answer on an example question and how the 1 https://www.wikidata.org/ Figure 1: Typical steps undertaken by a QA system. Gray dashed arrows show how the output of the previous step is passed into the next one. Qxxx stands for a KB identifier output of each step is re-used in the next one. This approach has been exhibited by the most of the recent works on the KB QA (Berant and Liang, 2014; Reddy et al., 2016; Yih et al., 2015; Peng et al., 2017; Sorokin and Gurevych, 2018b). The multi-step pipeline poses particular challenges for error analysis, since many unique errors can arise at different processing stages. With our tool, we aim at supporting the evaluation of QA systems and helping to identify problems that do not necessarily form generalisable error patterns, but hinder the overall system performance nonetheless. Some frameworks have been introduced recently to streamline the evaluation of KB QA systems. The ParlAI framework focuses on building a uni114 Proceedings of th"
D18-2020,P15-1026,0,0.0252017,"language, a semantic parsing model constructs multiple representations that can match the question and assigns probabilities to them (Liang, 2016). It is common to learn a vector encoding for the question and the structured representations and then use a similarity function to compute the probabilities (Yih et al., 2015; Reddy et al., 2016). The most probable structured representation is then translated into a query and used to extract the answer from the KB. Some approaches circumvent building a structured representation and instead directly compose vector encodings of the potential answers (Dong et al., 2015). Since this is a less common architecture type for KB QA, we focused on semantic parsing approaches while developing our interface. The described pipeline lets us outline the main requirements for an interactive debugging tool: 1. It needs to represent all stages of the QA pipeline in a sequential manner to let the user identify where the error occurs and how it propagates. 2. It needs to account for the specific properties of semantic parsing approaches to KB QA, such as structured semantic representations. 3. It needs to include an analysis block that shows if the model has learned meaningf"
D18-2020,D17-2014,0,0.0265986,"m at supporting the evaluation of QA systems and helping to identify problems that do not necessarily form generalisable error patterns, but hinder the overall system performance nonetheless. Some frameworks have been introduced recently to streamline the evaluation of KB QA systems. The ParlAI framework focuses on building a uni114 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 114–119 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics fied interface for multiple QA data sets (Miller et al., 2017), while GerbilQA2 introduces evaluation of individual steps of a QA pipeline. However, none of them addresses an interactive debugging scenario, that can be used by the researchers to do instance-base error analysis. This is especially relevant in the context of such benchmarks as QALD, where each individual question is meant to test a particular aspect of the system and debugging individual instances is crucial for understanding of the system performance (Unger et al., 2016). Another set of tools have focused on building an infrastructure to support the development for KB QA. Ask Wikidata3 of"
D18-2020,D17-1252,0,0.0216359,"the task into three main steps: entity linking, semantic parsing or relation disambiguation and answer retrieval. We show in Figure 1 how the outlined steps lead to an answer on an example question and how the 1 https://www.wikidata.org/ Figure 1: Typical steps undertaken by a QA system. Gray dashed arrows show how the output of the previous step is passed into the next one. Qxxx stands for a KB identifier output of each step is re-used in the next one. This approach has been exhibited by the most of the recent works on the KB QA (Berant and Liang, 2014; Reddy et al., 2016; Yih et al., 2015; Peng et al., 2017; Sorokin and Gurevych, 2018b). The multi-step pipeline poses particular challenges for error analysis, since many unique errors can arise at different processing stages. With our tool, we aim at supporting the evaluation of QA systems and helping to identify problems that do not necessarily form generalisable error patterns, but hinder the overall system performance nonetheless. Some frameworks have been introduced recently to streamline the evaluation of KB QA systems. The ParlAI framework focuses on building a uni114 Proceedings of the 2018 Conference on Empirical Methods in Natural Languag"
D18-2020,S18-2007,1,0.912814,"e main steps: entity linking, semantic parsing or relation disambiguation and answer retrieval. We show in Figure 1 how the outlined steps lead to an answer on an example question and how the 1 https://www.wikidata.org/ Figure 1: Typical steps undertaken by a QA system. Gray dashed arrows show how the output of the previous step is passed into the next one. Qxxx stands for a KB identifier output of each step is re-used in the next one. This approach has been exhibited by the most of the recent works on the KB QA (Berant and Liang, 2014; Reddy et al., 2016; Yih et al., 2015; Peng et al., 2017; Sorokin and Gurevych, 2018b). The multi-step pipeline poses particular challenges for error analysis, since many unique errors can arise at different processing stages. With our tool, we aim at supporting the evaluation of QA systems and helping to identify problems that do not necessarily form generalisable error patterns, but hinder the overall system performance nonetheless. Some frameworks have been introduced recently to streamline the evaluation of KB QA systems. The ParlAI framework focuses on building a uni114 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonst"
D18-2020,C18-1280,1,0.90627,"e main steps: entity linking, semantic parsing or relation disambiguation and answer retrieval. We show in Figure 1 how the outlined steps lead to an answer on an example question and how the 1 https://www.wikidata.org/ Figure 1: Typical steps undertaken by a QA system. Gray dashed arrows show how the output of the previous step is passed into the next one. Qxxx stands for a KB identifier output of each step is re-used in the next one. This approach has been exhibited by the most of the recent works on the KB QA (Berant and Liang, 2014; Reddy et al., 2016; Yih et al., 2015; Peng et al., 2017; Sorokin and Gurevych, 2018b). The multi-step pipeline poses particular challenges for error analysis, since many unique errors can arise at different processing stages. With our tool, we aim at supporting the evaluation of QA systems and helping to identify problems that do not necessarily form generalisable error patterns, but hinder the overall system performance nonetheless. Some frameworks have been introduced recently to streamline the evaluation of KB QA systems. The ParlAI framework focuses on building a uni114 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonst"
D18-2020,P15-1128,0,0.486341,"mmon to break down the task into three main steps: entity linking, semantic parsing or relation disambiguation and answer retrieval. We show in Figure 1 how the outlined steps lead to an answer on an example question and how the 1 https://www.wikidata.org/ Figure 1: Typical steps undertaken by a QA system. Gray dashed arrows show how the output of the previous step is passed into the next one. Qxxx stands for a KB identifier output of each step is re-used in the next one. This approach has been exhibited by the most of the recent works on the KB QA (Berant and Liang, 2014; Reddy et al., 2016; Yih et al., 2015; Peng et al., 2017; Sorokin and Gurevych, 2018b). The multi-step pipeline poses particular challenges for error analysis, since many unique errors can arise at different processing stages. With our tool, we aim at supporting the evaluation of QA systems and helping to identify problems that do not necessarily form generalisable error patterns, but hinder the overall system performance nonetheless. Some frameworks have been introduced recently to streamline the evaluation of KB QA systems. The ParlAI framework focuses on building a uni114 Proceedings of the 2018 Conference on Empirical Methods"
D18-2022,W16-4011,1,0.879197,"Missing"
D18-2022,girardi-etal-2014-cromer,0,0.0300952,"COUNTRY attribute only if the TYPE 6 https://www.w3.org/TR/rdf-schema/ 131 https://www.w3.org/2004/02/skos/ property of the entity has the value location. Tagsets can then be used to control which values are acceptable for the entity type or country properties. However, WebAnno has no support for search. AlvisAE (Papazian et al., 2012) supports linguistic and semantic annotations and can connect them to a structured vocabulary. However, it does not offer the ability to search over annotations and consequently also has no ability to make use of the vocabulary structure in such queries. CROMER (Girardi et al., 2014) is a tool for entity and event coreference annotation. It allows to annotate and link entity mentions to entities defined in a knowledge base and in this way to create implicit cross-document coreference links. It also offers a simple string-based search to locate potential entity mentions. However, it does not allow to perform further searches involving the created annotations or the structure of the vocabulary. NeuroCurator (O’Reilly et al., 2017) is a collaborative framework for annotating experiment parameters in scientific papers using an ontology-driven approach. It is rather an interac"
D18-2022,C18-2002,1,0.878042,"Missing"
D18-2022,W12-3621,0,0.0277174,"Missing"
D19-1101,C16-1168,0,0.0387211,"Missing"
D19-1101,N13-1132,0,0.600241,"mple et al., 2016). This presents a challenge when facing new domains or tasks, where obtaining labels is often timeconsuming or costly. Labelled data can be obtained cheaply by crowdsourcing, in which large numbers of untrained workers annotate documents instead of more expensive experts. For sequence tagging, this results in multiple sequences of unreliable labels for each document. Probabilistic methods for aggregating crowdsourced data have been shown to be more accurate than simple heuristics such as majority voting (Raykar et al., 2010; Sheshadri and Lease, 2013; Rodrigues et al., 2013; Hovy et al., 2013). However, existing methods for aggregating sequence labels cannot model dependencies between the annotators’ labels (Rodrigues et al., 2014; Nguyen et al., 2017) and hence do not account for their effect on annotator noise and bias. In this paper, we remedy this by proposing a sequential annotator model and applying it to tasks that follow a beginning, inside, outside (BIO) scheme, in which the first token in a span of type ‘x’ is labelled ‘B-x’, subsequent tokens are labelled ‘I-x’, and tokens outside spans are labelled ‘O’. When learning from noisy or small datasets, commonly-used methods b"
D19-1101,N16-1030,0,0.0546401,"tators who complete few tasks. We evaluate our model on crowdsourced data for named entity recognition, information extraction and argument mining, showing that our sequential model outperforms the previous state of the art. We also find that our approach can reduce crowdsourcing costs through more effective active learning, as it better captures uncertainty in the sequence labels when there are few annotations. 1 Introduction Current methods for sequence tagging, a core task in NLP, use deep neural networks that require tens of thousands of labelled documents for training (Ma and Hovy, 2016; Lample et al., 2016). This presents a challenge when facing new domains or tasks, where obtaining labels is often timeconsuming or costly. Labelled data can be obtained cheaply by crowdsourcing, in which large numbers of untrained workers annotate documents instead of more expensive experts. For sequence tagging, this results in multiple sequences of unreliable labels for each document. Probabilistic methods for aggregating crowdsourced data have been shown to be more accurate than simple heuristics such as majority voting (Raykar et al., 2010; Sheshadri and Lease, 2013; Rodrigues et al., 2013; Hovy et al., 2013)"
D19-1101,P17-1028,0,0.461298,"tained cheaply by crowdsourcing, in which large numbers of untrained workers annotate documents instead of more expensive experts. For sequence tagging, this results in multiple sequences of unreliable labels for each document. Probabilistic methods for aggregating crowdsourced data have been shown to be more accurate than simple heuristics such as majority voting (Raykar et al., 2010; Sheshadri and Lease, 2013; Rodrigues et al., 2013; Hovy et al., 2013). However, existing methods for aggregating sequence labels cannot model dependencies between the annotators’ labels (Rodrigues et al., 2014; Nguyen et al., 2017) and hence do not account for their effect on annotator noise and bias. In this paper, we remedy this by proposing a sequential annotator model and applying it to tasks that follow a beginning, inside, outside (BIO) scheme, in which the first token in a span of type ‘x’ is labelled ‘B-x’, subsequent tokens are labelled ‘I-x’, and tokens outside spans are labelled ‘O’. When learning from noisy or small datasets, commonly-used methods based on maximum likelihood estimation may produce over-confident predictions (Xiong et al., 2011; Srivastava et al., 2014). In contrast, Bayesian inference accoun"
D19-1101,Q18-1040,0,0.241052,"eighted by a prior distribution that captures background knowledge. The resulting posterior probabilities improve downstream decision making as they include the probability of errors due to a lack of knowledge. For example, during active learning, posterior probabilities assist with selecting the most informative data points (Settles, 2010). In this paper, we develop Bayesian sequence combination (BSC), building on prior work that has demonstrated the advantages of Bayesian inference for aggregating unreliable classifications (Kim and Ghahramani, 2012; Simpson et al., 2013; Felt et al., 2016; Paun et al., 2018). BSC is the first fully-Bayesian method for aggregating sequence labels from multiple annotators. As a core component of BSC, we also introduce the sequential confusion matrix (seq), a probabilistic model 1093 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1093–1104, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics of annotator noise and bias, which goes beyond previous work by modelling sequential dependencies between annotators’ label"
D19-1101,D08-1112,0,0.0810335,"ue labels, t, by selecting documents for the crowd to label. Each document can be labelled multiple times by different workers. In contrast, in a passive learning setup, the number of annotations per document is usually constant across all documents. For example, in the PICO dataset, each document was labelled six times. The aim of active learning is to decrease the number of annotations required by avoiding relabelling documents whose true labels can be determined with high confidence from fewer labels. We simulate active learning using the least confidence strategy, shown to be effective by Settles and Craven (2008), as described in Algorithm 2. At each iteration, we estimate t from the current set of crowdsourced labels, c, using one of the methods from our previous experiments as the labelling model, then use this model to determine the least confident batch size documents to be labelled by the crowd. If the simulation has requested all the labels for a document that are available in our dataset, this document is simply ignored when choosing new batches and is not selected again. We hypothesise that BSC will learn more quickly than non-sequential and non-Bayesian methods in the active learning simulati"
D19-1101,P16-1101,0,0.0354998,"for modelling annotators who complete few tasks. We evaluate our model on crowdsourced data for named entity recognition, information extraction and argument mining, showing that our sequential model outperforms the previous state of the art. We also find that our approach can reduce crowdsourcing costs through more effective active learning, as it better captures uncertainty in the sequence labels when there are few annotations. 1 Introduction Current methods for sequence tagging, a core task in NLP, use deep neural networks that require tens of thousands of labelled documents for training (Ma and Hovy, 2016; Lample et al., 2016). This presents a challenge when facing new domains or tasks, where obtaining labels is often timeconsuming or costly. Labelled data can be obtained cheaply by crowdsourcing, in which large numbers of untrained workers annotate documents instead of more expensive experts. For sequence tagging, this results in multiple sequences of unreliable labels for each document. Probabilistic methods for aggregating crowdsourced data have been shown to be more accurate than simple heuristics such as majority voting (Raykar et al., 2010; Sheshadri and Lease, 2013; Rodrigues et al., 20"
D19-1101,W03-0419,0,0.161454,"Missing"
D19-1171,E17-2115,0,0.0261188,"2 Related Work Duplicate question detection is closely related to question-question similarity and question retrieval. Early approaches use translation models (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011) that were further enhanced with question category information (Cao et al., 2012) and topic models (Ji et al., 2012; Zhang et al., 2014). More recent works in the context of the SemEval cQA challenges (Nakov et al., 2017) improve upon this and use tree kernels (TK) (Da San Martino et al., 2016), TK with neural networks (Romeo et al., 2016), neural networks with multi-task learning (Bonadiman et al., 2017), and encoder-decoder architectures together with shallow lexical matching and mismatching (Zhang and Wu, 2018). Common neural models such as CNNs achieved superior performance compared to TK when they were trained on sufficiently large numbers of labeled question pairs (Uva et al., 2018). Similarly, neural representation learning methods have proved to be most effective in technical cQA domains. Santos et al. (2015), for example, learn representations of questions with CNNs and compare them with cosine similarity for scoring. Lei et al. (2016) propose RCNN, which extends CNN with a recurrent"
D19-1171,N16-1012,0,0.0335763,"ecture, e.g., a question can only be encoded independently (no inter-attention). Our proposed methods do not suffer from these drawbacks, i.e., they do not require labeled data and they do not impose architectural limitations. Duplicate question generation (DQG) generates new question titles from question bodies, which we then consider as duplicates to the original titles. Our overall approach is depicted in Figure 2. First, we train a question generation model QG to maximize P (title(qn )|body(qn )). This is similar to news headline generation or abstractive summarization (Rush et al., 2015; Chopra et al., 2016) because QG needs to identify the most relevant aspects in the body that best characterize the question. However, restoring the exact title is usually not possible because titles and bodies often contain complementary information (see, e.g., Figure 1). We therefore consider QG(body(qn )) as a duplicate of title(qn ) and obtain positive labeled instances x+ n = (title(qn ), QG(body(qn ))). Train / Dev / Test 12,584 / 200 / 200 9106 / 1000 / 1000 9106 / 1000 / 1000 - / 1000 / 1000 - / 1000 / 1000 |Q| 288k 288k 377k 89k 47k |A| 84k 84k 142k 29k 14k Table 2: The dataset statistics. Numbers for Tra"
D19-1171,K17-1024,0,0.0429115,"Missing"
D19-1171,N16-1153,0,0.1969,"Missing"
D19-1171,2021.ccl-1.108,0,0.171807,"Missing"
D19-1171,P18-1177,0,0.181934,"ed duplicates we thus need other methods that do not require any annotations while performing on-par with supervised in-domain training. In this work, we propose two novel methods for scenarios where we only have access to unlabeled questions (title-body), including (1) automatic duplicate question generation (DQG); and (2) weak supervision with the title-body pairs (WS-TB). Because a question body typically provides additional important information that is not included in the title (Wu et al., 2018), we hypothesize that titles and bodies have similar properties as duplicate ques2 Shah et al. (2018) argue that even larger StackExchange sites do not offer enough duplicates for supervised training. Further, there exist many platforms that do not contain any labeled duplicates (e.g., https://gutefrage.net). 1607 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1607–1617, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tions. For instance, they are only partially redundant but fundamentally describe the same question (see Figure 1 for a"
D19-1171,S17-2003,0,0.0437668,"Missing"
D19-1171,P17-1123,0,0.0178724,"domain and adversarially adapted to a target domain (Shah et al., 2018). However, such approaches typically fall short of the performances that are being achieved with in-domain supervised training. In contrast, we propose two novel methods, DQG and WS-TB, that do not require any annotations for model training and in some cases perform better than in-domain supervised training with duplicate questions. While WS-TB is related to the approaches mentioned before, DQG is is also related to question generation (QG). Most of the previous work in QG is in the context of reading comprehension (e.g., Du et al., 2017; Subramanian et al., 2018; Zhao et al., 2018; Du and Cardie, 2018) or QG for question answering (Duan et al., 2017). They substantially differ from our approach because they generate questions based on specific answer spans, while DQG generates a new title from a question’s body that can be used as a question duplicate. Training Methods Given a pair of questions, our goal is to determine whether they are duplicates or not. In practice, the model predictions are often used to rank a list of potentially similar questions in regard to a new user question, e.g., to retrieve the most likely duplic"
D19-1171,D17-1090,0,0.0180905,"short of the performances that are being achieved with in-domain supervised training. In contrast, we propose two novel methods, DQG and WS-TB, that do not require any annotations for model training and in some cases perform better than in-domain supervised training with duplicate questions. While WS-TB is related to the approaches mentioned before, DQG is is also related to question generation (QG). Most of the previous work in QG is in the context of reading comprehension (e.g., Du et al., 2017; Subramanian et al., 2018; Zhao et al., 2018; Du and Cardie, 2018) or QG for question answering (Duan et al., 2017). They substantially differ from our approach because they generate questions based on specific answer spans, while DQG generates a new title from a question’s body that can be used as a question duplicate. Training Methods Given a pair of questions, our goal is to determine whether they are duplicates or not. In practice, the model predictions are often used to rank a list of potentially similar questions in regard to a new user question, e.g., to retrieve the most likely duplicate for automatic question answering. To train models, we obtain a set of examples {(x1 , y1 ), . . . , (xN , yN )}"
D19-1171,C16-1163,0,0.0266629,"Missing"
D19-1171,C18-1042,0,0.0248281,"cosine similarity for scoring. Lei et al. (2016) propose RCNN, which extends CNN with a recurrent mechanism (adaptive gated 1608 4 https://superuser.com/q/1393090 Method Duplicates Answers Bodies Supervised WS-QA Domain Transfer 5 5⇤ 5 - (5) (5) (5) DQG WS-TB - - 5 5 3 Table 1: The different training methods and the data they use. Models typically also use text from the bodies during training and evaluation, which we indicate with (5). 5⇤ = domain transfer requires duplicates from a sufficiently similar source domain. decay). This approach was further extended with question-type information (Gupta et al., 2018). If in-domain training data is scarce—i.e., if the cQA platform does not offer enough labeled duplicates—alternative training strategies are required. If there exist some labeled question pairs (thousands), one can first train a less data-hungry non-neural model and use it for supervised training of neural models (Uva et al., 2018). Further, if there exist large numbers of labeled question-answer pairs, we can use them for weakly-supervised training (Wang et al., 2017; Qiu and Huang, 2015). More related to our work are methods that do not rely on any labeled data in the target domain. Existin"
D19-1171,W17-6935,1,0.696058,"Missing"
D19-1171,D15-1044,0,0.0517694,"the network architecture, e.g., a question can only be encoded independently (no inter-attention). Our proposed methods do not suffer from these drawbacks, i.e., they do not require labeled data and they do not impose architectural limitations. Duplicate question generation (DQG) generates new question titles from question bodies, which we then consider as duplicates to the original titles. Our overall approach is depicted in Figure 2. First, we train a question generation model QG to maximize P (title(qn )|body(qn )). This is similar to news headline generation or abstractive summarization (Rush et al., 2015; Chopra et al., 2016) because QG needs to identify the most relevant aspects in the body that best characterize the question. However, restoring the exact title is usually not possible because titles and bodies often contain complementary information (see, e.g., Figure 1). We therefore consider QG(body(qn )) as a duplicate of title(qn ) and obtain positive labeled instances x+ n = (title(qn ), QG(body(qn ))). Train / Dev / Test 12,584 / 200 / 200 9106 / 1000 / 1000 9106 / 1000 / 1000 - / 1000 / 1000 - / 1000 / 1000 |Q| 288k 288k 377k 89k 47k |A| 84k 84k 142k 29k 14k Table 2: The dataset stati"
D19-1171,P15-2114,0,0.154325,"et al., 2017) improve upon this and use tree kernels (TK) (Da San Martino et al., 2016), TK with neural networks (Romeo et al., 2016), neural networks with multi-task learning (Bonadiman et al., 2017), and encoder-decoder architectures together with shallow lexical matching and mismatching (Zhang and Wu, 2018). Common neural models such as CNNs achieved superior performance compared to TK when they were trained on sufficiently large numbers of labeled question pairs (Uva et al., 2018). Similarly, neural representation learning methods have proved to be most effective in technical cQA domains. Santos et al. (2015), for example, learn representations of questions with CNNs and compare them with cosine similarity for scoring. Lei et al. (2016) propose RCNN, which extends CNN with a recurrent mechanism (adaptive gated 1608 4 https://superuser.com/q/1393090 Method Duplicates Answers Bodies Supervised WS-QA Domain Transfer 5 5⇤ 5 - (5) (5) (5) DQG WS-TB - - 5 5 3 Table 1: The different training methods and the data they use. Models typically also use text from the bodies during training and evaluation, which we indicate with (5). 5⇤ = domain transfer requires duplicates from a sufficiently similar source do"
D19-1171,P18-2046,0,0.26817,"e information.1 1 For example, “Nautilus shortcut for new blank files?” and “How do you create a new document keyboard shortcut?” are titles of labeled duplicate questions from AskUbuntu.com. In practice, it is often difficult to obtain such data because of the immense manual effort that is required for annotation. A large number of cQA forums thus do not contain enough labeled data for supervised training of neural models.2 Therefore, recent works have used alternative training methods. This includes weak supervision with question-answer pairs (Qiu and Huang, 2015), semi-supervised training (Uva et al., 2018), and adversarial domain transfer (Shah et al., 2018). An important limitation of these methods is that they still rely on substantial amounts of labeled data— either thousands of duplicate questions (e.g., from a similar source domain in the case of domain transfer) or large numbers of question-answer pairs. Furthermore, unsupervised methods rely on encoderdecoder architectures that impose limitations on the model architectures and they often fall short of the performances that are achieved with supervised training (Lei et al., 2016), or they need to be combined with complex features to achie"
D19-1171,W18-1819,0,0.0587906,"Missing"
D19-1171,P18-1162,0,0.106896,"2018). To train effective duplicate question detection models for the large number of cQA forums without labeled duplicates we thus need other methods that do not require any annotations while performing on-par with supervised in-domain training. In this work, we propose two novel methods for scenarios where we only have access to unlabeled questions (title-body), including (1) automatic duplicate question generation (DQG); and (2) weak supervision with the title-body pairs (WS-TB). Because a question body typically provides additional important information that is not included in the title (Wu et al., 2018), we hypothesize that titles and bodies have similar properties as duplicate ques2 Shah et al. (2018) argue that even larger StackExchange sites do not offer enough duplicates for supervised training. Further, there exist many platforms that do not contain any labeled duplicates (e.g., https://gutefrage.net). 1607 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1607–1617, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tions. For instanc"
D19-1171,P17-1099,0,0.0951701,"Missing"
D19-1171,D18-1131,0,0.211324,"r new blank files?” and “How do you create a new document keyboard shortcut?” are titles of labeled duplicate questions from AskUbuntu.com. In practice, it is often difficult to obtain such data because of the immense manual effort that is required for annotation. A large number of cQA forums thus do not contain enough labeled data for supervised training of neural models.2 Therefore, recent works have used alternative training methods. This includes weak supervision with question-answer pairs (Qiu and Huang, 2015), semi-supervised training (Uva et al., 2018), and adversarial domain transfer (Shah et al., 2018). An important limitation of these methods is that they still rely on substantial amounts of labeled data— either thousands of duplicate questions (e.g., from a similar source domain in the case of domain transfer) or large numbers of question-answer pairs. Furthermore, unsupervised methods rely on encoderdecoder architectures that impose limitations on the model architectures and they often fall short of the performances that are achieved with supervised training (Lei et al., 2016), or they need to be combined with complex features to achieve stateof-the-art results (Zhang and Wu, 2018). To t"
D19-1171,W18-2609,0,0.0204737,"sarially adapted to a target domain (Shah et al., 2018). However, such approaches typically fall short of the performances that are being achieved with in-domain supervised training. In contrast, we propose two novel methods, DQG and WS-TB, that do not require any annotations for model training and in some cases perform better than in-domain supervised training with duplicate questions. While WS-TB is related to the approaches mentioned before, DQG is is also related to question generation (QG). Most of the previous work in QG is in the context of reading comprehension (e.g., Du et al., 2017; Subramanian et al., 2018; Zhao et al., 2018; Du and Cardie, 2018) or QG for question answering (Duan et al., 2017). They substantially differ from our approach because they generate questions based on specific answer spans, while DQG generates a new title from a question’s body that can be used as a question duplicate. Training Methods Given a pair of questions, our goal is to determine whether they are duplicates or not. In practice, the model predictions are often used to rank a list of potentially similar questions in regard to a new user question, e.g., to retrieve the most likely duplicate for automatic question"
D19-1171,P16-1044,0,0.0909398,"se, we also transfer from StackExchange Academia (only 23k titlebody pairs to train question generation) to the technical target domains. This could, e.g., be realistic for other languages with fewer cQA forums. Most notably, the performance of DQG decreases only mildly, which demonstrates its practical applicability in even more challenging scenarios. This is mostly due to the copy mechanism of MQAN, which is stable across domains (see §6). 5.2 Answer Selection In answer selection we predict whether a candidate answer is relevant in regard to a question (Tay et al., 2017; Nakov et al., 2017; Tan et al., 2016; R¨uckl´e and Gurevych, 2017), which is similar to duplicate question detection. To test whether our strategy to train models with title-body pairs is also suitable for answer selection, we use the data and code of R¨uckl´e et al. (2019a) and train two different types of models with WSTB on their five datasets that are based on StackExchange Apple, Aviation, Academia, Cooking, and Travel. We train (1) a siamese BiLSTM, which 1613 Model BiLSTM COALA Supervised WS-TB WS-TB (all) 0.4 35.3 44.7 37.5 45.2 42.5 44.5 0.3 0.2 0.1 Table 5: Answer selection performances (averaged over five datasets) wh"
D19-1171,D18-1424,0,0.0212086,"et domain (Shah et al., 2018). However, such approaches typically fall short of the performances that are being achieved with in-domain supervised training. In contrast, we propose two novel methods, DQG and WS-TB, that do not require any annotations for model training and in some cases perform better than in-domain supervised training with duplicate questions. While WS-TB is related to the approaches mentioned before, DQG is is also related to question generation (QG). Most of the previous work in QG is in the context of reading comprehension (e.g., Du et al., 2017; Subramanian et al., 2018; Zhao et al., 2018; Du and Cardie, 2018) or QG for question answering (Duan et al., 2017). They substantially differ from our approach because they generate questions based on specific answer spans, while DQG generates a new title from a question’s body that can be used as a question duplicate. Training Methods Given a pair of questions, our goal is to determine whether they are duplicates or not. In practice, the model predictions are often used to rank a list of potentially similar questions in regard to a new user question, e.g., to retrieve the most likely duplicate for automatic question answering. To trai"
D19-1171,P11-1066,0,0.0890166,"Missing"
D19-1307,N19-1264,0,0.0243353,"expensive to provide. Our method only needs human ratings on some generated summaries, also known as the bandit feedback (Kreutzer et al., 2017), to learn the reward function. Furthermore, when employing certain loss functions (see §4 and Eq. (2)), our method can even learn the reward function from preferences over generated summaries, an even cheaper feedback to elicit (Kreutzer et al., 2018; Gao et al., 2018). Heuristic-based rewards. Prior work proposed heuristic-based reward functions to train crossinput RL summarisers, in order to strengthen certain properties of the generated summaries. Arumae and Liu (2019) propose four reward functions to train an RL-based extractive summariser, including the question-answering competency rewards, which encourage the RL agent to generate summaries that can answer cloze-style questions. Such questions are automatically created by removing some words in the reference summaries. Experiments suggest that human subjects can answer the questions with high accuracy by reading their generated summaries; but the human judgement scores of their summaries are not higher than the summaries generated by the stateof-the-art supervised system. Kryscinski et al. (2018) propose"
D19-1307,D15-1075,0,0.0120894,"ition, the above rewards require reference summaries, unlike our reward that only takes a document and a generated summary as input. Rewards learned with extra data. Pasunuru and Bansal (2018) propose two novel rewards for training RL-based abstractive summarisers: RougeSal, which up-weights the salient phrases and words detected via a keyphrase classifier, and Entail reward, which gives high scores to 3111 logically-entailed summaries using an entailment classifier. RougeSal is trained with the SQuAD reading comprehension dataset (Rajpurkar et al., 2016), and Entail is trained with the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018) datasets. Although their system achieves new state-of-the-art results in terms of ROUGE, it remains unclear whether their system generates more human-appealing summaries as they do not perform human evaluation experiments. Additionally, both rewards require reference summaries. Louis and Nenkova (2013), Peyrard et al. (2017) and Peyrard and Gurevych (2018) build featurerich regression models to learn a summary evaluation metric directly from the human judgement scores (Pyramid and Responsiveness) provided in the TAC’08 and ’09 datasets1 . Some features th"
D19-1307,P18-1060,0,0.153201,"es to guide the optimisation direction, the poor performance of ROUGE at summary level severely misleads the RL agents. The reliability of ROUGE as RL reward is further challenged by the fact that most large-scale summarisation datasets only have one reference summary available for each input document (e.g. CNN/DailyMail (Hermann et al., 2015; See et al., 2017) and NewsRooms (Grusky et al., 2018)). In order to find better rewards that can guide RL-based summarisers to generate more humanappealing summaries, we learn a reward function directly from human ratings. We use the dataset compiled by Chaganty et al. (2018), which includes human ratings on 2,500 summaries for 500 news articles from CNN/DailyMail. Unlike ROUGE that requires one or multiple reference summaries to compute the scores, our reward function only takes the document and the generated summary as input. Hence, once trained, our 3110 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3110–3120, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics reward can be used to train RL-based summarisa"
D19-1307,P18-1063,0,0.398885,"ts show that, compared to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summaries with higher human ratings. The learned reward function and our source code are available at https://github.com/yg211/ summary-reward-no-reference. 1 Introduction Document summarisation aims at generating a summary for a long document or multiple documents on the same topic. Reinforcement Learning (RL) becomes an increasingly popular technique to build document summarisation systems in recent years (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018). Compared to the supervised learning paradigm, which “pushes” the summariser to reproduce the reference summaries * Since June 2019, Yang Gao is affiliated with Dept. of Computer Science, Royal Holloway, University of London. at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries. The ROUGE metrics (Lin, 2004b) are the most widely used rewards in training RL-based summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the re"
D19-1307,D17-1070,0,0.0509167,"Missing"
D19-1307,N19-1423,0,0.0346382,"Missing"
D19-1307,D18-1409,0,0.0811759,"Missing"
D19-1307,D18-1445,1,0.870321,"ctions, IRL algorithms learn a reward function that is consistent with the observed sequences. In the case of summarisation, human-written reference summaries are the (near-)optimal sequences, which are expensive to provide. Our method only needs human ratings on some generated summaries, also known as the bandit feedback (Kreutzer et al., 2017), to learn the reward function. Furthermore, when employing certain loss functions (see §4 and Eq. (2)), our method can even learn the reward function from preferences over generated summaries, an even cheaper feedback to elicit (Kreutzer et al., 2018; Gao et al., 2018). Heuristic-based rewards. Prior work proposed heuristic-based reward functions to train crossinput RL summarisers, in order to strengthen certain properties of the generated summaries. Arumae and Liu (2019) propose four reward functions to train an RL-based extractive summariser, including the question-answering competency rewards, which encourage the RL agent to generate summaries that can answer cloze-style questions. Such questions are automatically created by removing some words in the reference summaries. Experiments suggest that human subjects can answer the questions with high accuracy"
D19-1307,N18-1065,0,0.130978,"Missing"
D19-1307,D18-1208,0,0.0578087,"Missing"
D19-1307,D14-1181,0,0.00341498,"x and summary y as two embeddings, and feed the concatenated embedding into a single-layer MLP to minimise the loss functions Eq. (1) and (3). We consider three text encoders to vectorise x and yx . In supplementary material, we provide figures to further illustrate the architectures of these text encoders. 3113 CNN-RNN. We use convolutional neural networks (CNNs) to encode the sentences in the input text, and feed the sentence embeddings into an LSTM to generate the embedding of the whole input text. In the CNN part, convolutions with different filter widths are applied independently as in (Kim, 2014). The most relevant features are selected afterwards with max-over-time pooling. In line with Narayan et al. (2018b), we reverse the order of sentence embeddings before feeding them into the LSTM. This encoder network yields strong performance on summarisation and sentence classification tasks (Narayan et al., 2018a,b). PMeans-RNN. PMeans is a simple yet powerful sentence encoding method (R¨uckl´e et al., 2018). PMeans encodes a sentence by computing the power means of the embeddings of the words in the sentence. PMeans uses a parameter p to control the weights for each word embedding: with p"
D19-1307,P17-1138,0,0.0670637,"Missing"
D19-1307,P18-1165,0,0.0475632,")optimal sequences of actions, IRL algorithms learn a reward function that is consistent with the observed sequences. In the case of summarisation, human-written reference summaries are the (near-)optimal sequences, which are expensive to provide. Our method only needs human ratings on some generated summaries, also known as the bandit feedback (Kreutzer et al., 2017), to learn the reward function. Furthermore, when employing certain loss functions (see §4 and Eq. (2)), our method can even learn the reward function from preferences over generated summaries, an even cheaper feedback to elicit (Kreutzer et al., 2018; Gao et al., 2018). Heuristic-based rewards. Prior work proposed heuristic-based reward functions to train crossinput RL summarisers, in order to strengthen certain properties of the generated summaries. Arumae and Liu (2019) propose four reward functions to train an RL-based extractive summariser, including the question-answering competency rewards, which encourage the RL agent to generate summaries that can answer cloze-style questions. Such questions are automatically created by removing some words in the reference summaries. Experiments suggest that human subjects can answer the questions"
D19-1307,D18-1207,0,0.232854,"erate summaries with significantly higher human ratings than the state-of-the-art systems (§7). 2 Related Work RL-based summarisation. Most existing RLbased summarisers fall into two categories: crossinput systems and input-specific systems (Gao et al., 2019). Cross-input systems learn a summarisation policy at training time by letting the RL agent interact with a ROUGE-based reward function. At test time, the learned policy is used to generate a summary for each input document. Most RL-based summarisers fall into this category (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018; Kryscinski et al., 2018; Pasunuru and Bansal, 2018; Paulus et al., 2018). As an alternative, input-specific RL (Rioux et al., 2014; Ryang and Abekawa, 2012) does not require reference summaries: for each input document at test time, a summarisation policy is trained specifically for the input, by letting the RL summariser interact with a heuristic-based reward function, e.g. ROUGE between the generated summary and the input document (without using any reference summaries). However, the performance of inputspecific RL falls far behind the cross-input counterparts. In §7 we use our learned reward to train both cross-i"
D19-1307,W04-1013,0,0.155076,"documents on the same topic. Reinforcement Learning (RL) becomes an increasingly popular technique to build document summarisation systems in recent years (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018). Compared to the supervised learning paradigm, which “pushes” the summariser to reproduce the reference summaries * Since June 2019, Yang Gao is affiliated with Dept. of Computer Science, Royal Holloway, University of London. at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries. The ROUGE metrics (Lin, 2004b) are the most widely used rewards in training RL-based summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the reference summaries appear in the generated summary. ROUGE correlates well with human judgements at system level (Lin, 2004a; Louis and Nenkova, 2013), i.e. by aggregating system summaries’ ROUGE scores across multiple input documents, we can reliably rank summarisation systems by their quality. However, ROUGE performs poorly at summary level: given multiple summaries for the same input document, ROUGE can hardly distinguish the “"
D19-1307,J13-2002,0,0.562287,"reproduce the reference summaries * Since June 2019, Yang Gao is affiliated with Dept. of Computer Science, Royal Holloway, University of London. at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries. The ROUGE metrics (Lin, 2004b) are the most widely used rewards in training RL-based summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the reference summaries appear in the generated summary. ROUGE correlates well with human judgements at system level (Lin, 2004a; Louis and Nenkova, 2013), i.e. by aggregating system summaries’ ROUGE scores across multiple input documents, we can reliably rank summarisation systems by their quality. However, ROUGE performs poorly at summary level: given multiple summaries for the same input document, ROUGE can hardly distinguish the “good” summaries from the “mediocre” and “bad” ones (Novikova et al., 2017). Because existing RL-based summarisation systems rely on summary-level ROUGE scores to guide the optimisation direction, the poor performance of ROUGE at summary level severely misleads the RL agents. The reliability of ROUGE as RL reward is"
D19-1307,P18-1188,0,0.290027,"to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summaries with higher human ratings. The learned reward function and our source code are available at https://github.com/yg211/ summary-reward-no-reference. 1 Introduction Document summarisation aims at generating a summary for a long document or multiple documents on the same topic. Reinforcement Learning (RL) becomes an increasingly popular technique to build document summarisation systems in recent years (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018). Compared to the supervised learning paradigm, which “pushes” the summariser to reproduce the reference summaries * Since June 2019, Yang Gao is affiliated with Dept. of Computer Science, Royal Holloway, University of London. at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries. The ROUGE metrics (Lin, 2004b) are the most widely used rewards in training RL-based summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the reference summaries appe"
D19-1307,N18-1158,0,0.402438,"to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summaries with higher human ratings. The learned reward function and our source code are available at https://github.com/yg211/ summary-reward-no-reference. 1 Introduction Document summarisation aims at generating a summary for a long document or multiple documents on the same topic. Reinforcement Learning (RL) becomes an increasingly popular technique to build document summarisation systems in recent years (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018). Compared to the supervised learning paradigm, which “pushes” the summariser to reproduce the reference summaries * Since June 2019, Yang Gao is affiliated with Dept. of Computer Science, Royal Holloway, University of London. at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries. The ROUGE metrics (Lin, 2004b) are the most widely used rewards in training RL-based summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the reference summaries appe"
D19-1307,D17-1238,0,0.0571248,"ased summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the reference summaries appear in the generated summary. ROUGE correlates well with human judgements at system level (Lin, 2004a; Louis and Nenkova, 2013), i.e. by aggregating system summaries’ ROUGE scores across multiple input documents, we can reliably rank summarisation systems by their quality. However, ROUGE performs poorly at summary level: given multiple summaries for the same input document, ROUGE can hardly distinguish the “good” summaries from the “mediocre” and “bad” ones (Novikova et al., 2017). Because existing RL-based summarisation systems rely on summary-level ROUGE scores to guide the optimisation direction, the poor performance of ROUGE at summary level severely misleads the RL agents. The reliability of ROUGE as RL reward is further challenged by the fact that most large-scale summarisation datasets only have one reference summary available for each input document (e.g. CNN/DailyMail (Hermann et al., 2015; See et al., 2017) and NewsRooms (Grusky et al., 2018)). In order to find better rewards that can guide RL-based summarisers to generate more humanappealing summaries, we le"
D19-1307,P02-1040,0,0.110128,"o correctly rank summaries of different quality levels, while a good reward function focuses more on distinguishing the best summaries from the mediocre and bad summaries. Also, an evaluation metric should be able to evaluate summaries of different types (e.g. extractive and abstractive) and from different genres, while a reward function can be specifically designed for a single task. We leave the learning of a generic summarisation evaluation metric for future work. so as to further motivate the need for a stronger reward for RL. Metrics we consider include ROUGE (full length F-score), BLEU (Papineni et al., 2002) and METEOR (Lavie and Denkowski, 2009). Furthermore, in line with Chaganty et al. (2018), we also use the cosine similarity between the embeddings of the generated summary and the reference summary as metrics: we use InferSent (Conneau et al., 2017) and BERT-Large-Cased (Devlin et al., 2019) to generate the embeddings. 3 From Table 1, we find that all metrics we consider have low correlation with the human judgement. More importantly, their G-Pre and G-Rec scores are all below .50, which means that more than half of the good summaries identified by the metrics are actually not good, and more"
D19-1307,N18-2102,0,0.138869,"ificantly higher human ratings than the state-of-the-art systems (§7). 2 Related Work RL-based summarisation. Most existing RLbased summarisers fall into two categories: crossinput systems and input-specific systems (Gao et al., 2019). Cross-input systems learn a summarisation policy at training time by letting the RL agent interact with a ROUGE-based reward function. At test time, the learned policy is used to generate a summary for each input document. Most RL-based summarisers fall into this category (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018; Kryscinski et al., 2018; Pasunuru and Bansal, 2018; Paulus et al., 2018). As an alternative, input-specific RL (Rioux et al., 2014; Ryang and Abekawa, 2012) does not require reference summaries: for each input document at test time, a summarisation policy is trained specifically for the input, by letting the RL summariser interact with a heuristic-based reward function, e.g. ROUGE between the generated summary and the input document (without using any reference summaries). However, the performance of inputspecific RL falls far behind the cross-input counterparts. In §7 we use our learned reward to train both cross-input and input-specific RL"
D19-1307,D14-1162,0,0.0828786,"ries (described in §3) to measure the performance of our reward R. In each fold, the data is split with ratio 64:16:20 for training, validation and test. The parameters of our model are detailed as follows, decided in a pilot study on one fold of the data split. The CNN-RNN encoder (see §5.1) uses filter widths 1-10 for the CNN part, and uses a unidirectional LSTM with a single layer whose dimension is 600 for the RNN part. For PMeans, we obtain sentence embeddings for each p ∈ {−∞, +∞, 1, 2} and concatenate them per sentence. Both these two encoders use the pre-trained GloVe word embeddings (Pennington et al., 2014). On top of these encoders, we use an MLP with one hidden ReLU layer and a linear activation at the output layer. For the MLP that uses CNNRNN and PMeans-RNN, the dimension of its hidden layer is 100, while for the MLP with BERT as encoder, the dimension of the hidden layer is 1024. As for SimRed, we set α (see Eq. (4)) to be 0.85. The trainable layer on top of BERT and PMeans – when used with SimRed – is a single 3114 layer perceptron whose output dimension is equal to the input dimension. Reward Quality. Table 2 shows the quality of different reward learning models. As a baseline, we also co"
D19-1307,W17-4510,1,0.497627,"Entail reward, which gives high scores to 3111 logically-entailed summaries using an entailment classifier. RougeSal is trained with the SQuAD reading comprehension dataset (Rajpurkar et al., 2016), and Entail is trained with the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018) datasets. Although their system achieves new state-of-the-art results in terms of ROUGE, it remains unclear whether their system generates more human-appealing summaries as they do not perform human evaluation experiments. Additionally, both rewards require reference summaries. Louis and Nenkova (2013), Peyrard et al. (2017) and Peyrard and Gurevych (2018) build featurerich regression models to learn a summary evaluation metric directly from the human judgement scores (Pyramid and Responsiveness) provided in the TAC’08 and ’09 datasets1 . Some features they use require reference summaries (e.g. ROUGE metrics); the others are heuristic-based and do not use reference summaries (e.g. the JensenShannon divergence between the word distributions in the summary and the documents). Their experiments suggest that with only non-referencesummary-based features, the correlation of their learned metric with human judgements i"
D19-1307,N18-2103,1,0.628066,"high scores to 3111 logically-entailed summaries using an entailment classifier. RougeSal is trained with the SQuAD reading comprehension dataset (Rajpurkar et al., 2016), and Entail is trained with the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018) datasets. Although their system achieves new state-of-the-art results in terms of ROUGE, it remains unclear whether their system generates more human-appealing summaries as they do not perform human evaluation experiments. Additionally, both rewards require reference summaries. Louis and Nenkova (2013), Peyrard et al. (2017) and Peyrard and Gurevych (2018) build featurerich regression models to learn a summary evaluation metric directly from the human judgement scores (Pyramid and Responsiveness) provided in the TAC’08 and ’09 datasets1 . Some features they use require reference summaries (e.g. ROUGE metrics); the others are heuristic-based and do not use reference summaries (e.g. the JensenShannon divergence between the word distributions in the summary and the documents). Their experiments suggest that with only non-referencesummary-based features, the correlation of their learned metric with human judgements is lower than ROUGE; with referen"
D19-1307,D18-1088,0,0.0741606,"Missing"
D19-1307,P18-1061,0,0.0557763,"Missing"
D19-1307,D14-1075,0,0.198249,"Missing"
D19-1307,D12-1024,0,0.363777,"n. Most existing RLbased summarisers fall into two categories: crossinput systems and input-specific systems (Gao et al., 2019). Cross-input systems learn a summarisation policy at training time by letting the RL agent interact with a ROUGE-based reward function. At test time, the learned policy is used to generate a summary for each input document. Most RL-based summarisers fall into this category (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018; Kryscinski et al., 2018; Pasunuru and Bansal, 2018; Paulus et al., 2018). As an alternative, input-specific RL (Rioux et al., 2014; Ryang and Abekawa, 2012) does not require reference summaries: for each input document at test time, a summarisation policy is trained specifically for the input, by letting the RL summariser interact with a heuristic-based reward function, e.g. ROUGE between the generated summary and the input document (without using any reference summaries). However, the performance of inputspecific RL falls far behind the cross-input counterparts. In §7 we use our learned reward to train both cross-input and input-specific RL systems. A similar idea has been explored by Gao et al. (2019), but unlike their work that learns the rewa"
D19-1307,P17-1099,0,0.751171,"ry level: given multiple summaries for the same input document, ROUGE can hardly distinguish the “good” summaries from the “mediocre” and “bad” ones (Novikova et al., 2017). Because existing RL-based summarisation systems rely on summary-level ROUGE scores to guide the optimisation direction, the poor performance of ROUGE at summary level severely misleads the RL agents. The reliability of ROUGE as RL reward is further challenged by the fact that most large-scale summarisation datasets only have one reference summary available for each input document (e.g. CNN/DailyMail (Hermann et al., 2015; See et al., 2017) and NewsRooms (Grusky et al., 2018)). In order to find better rewards that can guide RL-based summarisers to generate more humanappealing summaries, we learn a reward function directly from human ratings. We use the dataset compiled by Chaganty et al. (2018), which includes human ratings on 2,500 summaries for 500 news articles from CNN/DailyMail. Unlike ROUGE that requires one or multiple reference summaries to compute the scores, our reward function only takes the document and the generated summary as input. Hence, once trained, our 3110 Proceedings of the 2019 Conference on Empirical Metho"
D19-1307,W19-2303,1,0.889806,"Missing"
D19-1307,N18-1101,0,0.0129741,"erence summaries, unlike our reward that only takes a document and a generated summary as input. Rewards learned with extra data. Pasunuru and Bansal (2018) propose two novel rewards for training RL-based abstractive summarisers: RougeSal, which up-weights the salient phrases and words detected via a keyphrase classifier, and Entail reward, which gives high scores to 3111 logically-entailed summaries using an entailment classifier. RougeSal is trained with the SQuAD reading comprehension dataset (Rajpurkar et al., 2016), and Entail is trained with the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018) datasets. Although their system achieves new state-of-the-art results in terms of ROUGE, it remains unclear whether their system generates more human-appealing summaries as they do not perform human evaluation experiments. Additionally, both rewards require reference summaries. Louis and Nenkova (2013), Peyrard et al. (2017) and Peyrard and Gurevych (2018) build featurerich regression models to learn a summary evaluation metric directly from the human judgement scores (Pyramid and Responsiveness) provided in the TAC’08 and ’09 datasets1 . Some features they use require reference summaries (e."
D19-1314,W13-2322,0,0.134002,"structural information contained in the AMR graph. The model learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-ofthe-art results on two AMR datasets1 . 1 Introduction Abstract Meaning Representation (AMR; Banarescu et al. (2013)) is a linguistically-grounded semantic formalism that represents the meaning of a sentence as a rooted directed graph, where nodes are concepts and edges are semantic relations. As AMR abstracts away from surface word strings and syntactic structure producing a language neutral representation of meaning, its usage is beneficial in many semantic related NLP tasks, including text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). The purpose of AMR-to-text generation is to produce a text which verbalises the meaning encoded by an input AMR graph. This is a challengin"
D19-1314,P18-1026,0,0.46298,"s capturing the complex structural information stored in graph-based data is not trivial, as these are non-Euclidean structures, which 1 Code is available at https://github.com/UKPLab/emnlp2019-dualgraph implies that properties such as global parametrization, vector space structure, or shift-invariance do not hold (Bronstein et al., 2017). Recently, Graph Neural Networks (GNNs) have emerged as a powerful class of methods for learning effective graph latent representations (Xu et al., 2019) and graph-to-sequence models have been applied to the task of AMR-to-text generation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019). In this paper, we propose a novel graph-tosequence approach to AMR-to-text generation, which is inspired by pre-neural generation algorithms. These approaches explored alternative (top-down, bottom-up and mixed) traversals of the input graph and showed that a hybrid traversal combining both top-down (TD) and bottom-up (BU) information was best as this permits integrating both global constraints top-down from the input and local constraints bottom-up from the semantic heads (Shieber et al., 1990; Narayan and Gardent, 2012). Similarly, we present an"
D19-1314,W11-2832,0,0.0303653,"on and use graph encoders to represent nodes. Cao and Clark (2019) factor the generation process leveraging syntactic information to improve the performance. However, they linearize both AMR and constituency graphs, which implies that important parts of the graphs cannot well be represented (e.g., coreference). Several graph-to-sequence models have been proposed. Marcheggiani and Perez Beltrachini (2018) show that explicitly encoding the structure of the graph is beneficial with respect to sequential encoding. They evaluate their model on two tasks, WebNLG (Gardent et al., 2017) and SR11Deep (Belz et al., 2011), but do not apply it to AMR benchmarks. Song et al. (2018) and Beck et al. (2018) apply recurrent neural networks to directly encode AMR graphs. Song et al. (2018) use a graph LSTM as the graph encoder, whereas Beck et al. (2018) develop a model based on GRUs. We go a step further in that direction by developing parallel encodings of graphs which are able to highlight different graph properties. In a related task, Koncel-Kedziorski et al. (2019) propose an attention-based graph model that generates sentences from knowledge graphs. Schlichtkrull et al. (2018) use Graph Convolutional Networks ("
D19-1314,N19-1223,0,0.440619,"November 3–7, 2019. 2019 Association for Computational Linguistics nates the need for additional positional information (Beck et al., 2018) which is required when the same graph is used to encode both TD and BU information, thereby making the edges undirected. Our main contributions are the following: • We present a novel architecture for AMR-to-text generation which explicitly encodes two separate TD and BU views of the input graph. • We show that our approach outperforms recent AMR-to-text generation models on two datasets, including a model that leverages additional syntactic information (Cao and Clark, 2019). • We compare the performance of three graph encoders, which have not been studied so far for AMR-to-text generation. 2 Related Work Early works on AMR-to-text generation employ statistical methods (Flanigan et al., 2016b; Pourdamghani et al., 2016; Castro Ferreira et al., 2017) and apply linearization of the graph by means of a depth-first traversal. Recent neural approaches have exhibited success by linearising the input graph and using a sequence-to-sequence architecture. Konstas et al. (2017) achieve promising results on this task. However, they strongly rely on named entities anonymisati"
D19-1314,W17-3501,0,0.063384,"utions are the following: • We present a novel architecture for AMR-to-text generation which explicitly encodes two separate TD and BU views of the input graph. • We show that our approach outperforms recent AMR-to-text generation models on two datasets, including a model that leverages additional syntactic information (Cao and Clark, 2019). • We compare the performance of three graph encoders, which have not been studied so far for AMR-to-text generation. 2 Related Work Early works on AMR-to-text generation employ statistical methods (Flanigan et al., 2016b; Pourdamghani et al., 2016; Castro Ferreira et al., 2017) and apply linearization of the graph by means of a depth-first traversal. Recent neural approaches have exhibited success by linearising the input graph and using a sequence-to-sequence architecture. Konstas et al. (2017) achieve promising results on this task. However, they strongly rely on named entities anonymisation. Anonymisation requires an ad hoc procedure for each new corpus. The matching procedure needs to match a rare input item correctly (e.g., “United States of America”) with the corresponding part in the output text (e.g., “USA”) which may be challenging and may result in incorre"
D19-1314,D14-1179,0,0.0233632,"Missing"
D19-1314,N19-1366,0,0.344844,"plex structural information stored in graph-based data is not trivial, as these are non-Euclidean structures, which 1 Code is available at https://github.com/UKPLab/emnlp2019-dualgraph implies that properties such as global parametrization, vector space structure, or shift-invariance do not hold (Bronstein et al., 2017). Recently, Graph Neural Networks (GNNs) have emerged as a powerful class of methods for learning effective graph latent representations (Xu et al., 2019) and graph-to-sequence models have been applied to the task of AMR-to-text generation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019). In this paper, we propose a novel graph-tosequence approach to AMR-to-text generation, which is inspired by pre-neural generation algorithms. These approaches explored alternative (top-down, bottom-up and mixed) traversals of the input graph and showed that a hybrid traversal combining both top-down (TD) and bottom-up (BU) information was best as this permits integrating both global constraints top-down from the input and local constraints bottom-up from the semantic heads (Shieber et al., 1990; Narayan and Gardent, 2012). Similarly, we present an approach where the input"
D19-1314,W14-3348,0,0.109133,"Missing"
D19-1314,N19-1423,0,0.0380882,"Missing"
D19-1314,P19-1213,1,0.867711,"Missing"
D19-1314,S16-1186,0,0.114335,"ion, thereby making the edges undirected. Our main contributions are the following: • We present a novel architecture for AMR-to-text generation which explicitly encodes two separate TD and BU views of the input graph. • We show that our approach outperforms recent AMR-to-text generation models on two datasets, including a model that leverages additional syntactic information (Cao and Clark, 2019). • We compare the performance of three graph encoders, which have not been studied so far for AMR-to-text generation. 2 Related Work Early works on AMR-to-text generation employ statistical methods (Flanigan et al., 2016b; Pourdamghani et al., 2016; Castro Ferreira et al., 2017) and apply linearization of the graph by means of a depth-first traversal. Recent neural approaches have exhibited success by linearising the input graph and using a sequence-to-sequence architecture. Konstas et al. (2017) achieve promising results on this task. However, they strongly rely on named entities anonymisation. Anonymisation requires an ad hoc procedure for each new corpus. The matching procedure needs to match a rare input item correctly (e.g., “United States of America”) with the corresponding part in the output text (e.g."
D19-1314,N16-1087,0,0.611973,"ion, thereby making the edges undirected. Our main contributions are the following: • We present a novel architecture for AMR-to-text generation which explicitly encodes two separate TD and BU views of the input graph. • We show that our approach outperforms recent AMR-to-text generation models on two datasets, including a model that leverages additional syntactic information (Cao and Clark, 2019). • We compare the performance of three graph encoders, which have not been studied so far for AMR-to-text generation. 2 Related Work Early works on AMR-to-text generation employ statistical methods (Flanigan et al., 2016b; Pourdamghani et al., 2016; Castro Ferreira et al., 2017) and apply linearization of the graph by means of a depth-first traversal. Recent neural approaches have exhibited success by linearising the input graph and using a sequence-to-sequence architecture. Konstas et al. (2017) achieve promising results on this task. However, they strongly rely on named entities anonymisation. Anonymisation requires an ad hoc procedure for each new corpus. The matching procedure needs to match a rare input item correctly (e.g., “United States of America”) with the corresponding part in the output text (e.g."
D19-1314,W17-3518,1,0.827968,"uild a dual TD/BU graph representation and use graph encoders to represent nodes. Cao and Clark (2019) factor the generation process leveraging syntactic information to improve the performance. However, they linearize both AMR and constituency graphs, which implies that important parts of the graphs cannot well be represented (e.g., coreference). Several graph-to-sequence models have been proposed. Marcheggiani and Perez Beltrachini (2018) show that explicitly encoding the structure of the graph is beneficial with respect to sequential encoding. They evaluate their model on two tasks, WebNLG (Gardent et al., 2017) and SR11Deep (Belz et al., 2011), but do not apply it to AMR benchmarks. Song et al. (2018) and Beck et al. (2018) apply recurrent neural networks to directly encode AMR graphs. Song et al. (2018) use a graph LSTM as the graph encoder, whereas Beck et al. (2018) develop a model based on GRUs. We go a step further in that direction by developing parallel encodings of graphs which are able to highlight different graph properties. In a related task, Koncel-Kedziorski et al. (2019) propose an attention-based graph model that generates sentences from knowledge graphs. Schlichtkrull et al. (2018) u"
D19-1314,Q19-1019,0,0.452769,"on stored in graph-based data is not trivial, as these are non-Euclidean structures, which 1 Code is available at https://github.com/UKPLab/emnlp2019-dualgraph implies that properties such as global parametrization, vector space structure, or shift-invariance do not hold (Bronstein et al., 2017). Recently, Graph Neural Networks (GNNs) have emerged as a powerful class of methods for learning effective graph latent representations (Xu et al., 2019) and graph-to-sequence models have been applied to the task of AMR-to-text generation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019). In this paper, we propose a novel graph-tosequence approach to AMR-to-text generation, which is inspired by pre-neural generation algorithms. These approaches explored alternative (top-down, bottom-up and mixed) traversals of the input graph and showed that a hybrid traversal combining both top-down (TD) and bottom-up (BU) information was best as this permits integrating both global constraints top-down from the input and local constraints bottom-up from the semantic heads (Shieber et al., 1990; Narayan and Gardent, 2012). Similarly, we present an approach where the input graph is represente"
D19-1314,P07-2045,0,0.00610917,"ively. In contrast to their work, we do not rely on (i) leveraging supplementary syntactic information and (ii) we do not require an anonymization preprocessing step. G2S-GIN and G2S-GAT have comparable performance on both datasets. Interestingly, G2S-GGNN has better performance among our models. This suggests that graph encoders based on gating mechanisms are very effective in text generation models. We hypothesize that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph. 3 For BLEU, we use the multi-BLEU script from the MOSES decoder suite (Koehn et al., 2007). For METEOR, we use the original meteor-1.5.jar script (https://github.com/cmu-mtlab/meteor). 3188 Model biLSTM GEt + biLSTM GEb + biLSTM GEt + GEb + biLSTM BLEU METEOR 22.50 26.33 26.12 27.37 30.42 32.62 32.49 33.30 Size 57.6M 59.6M 59.6M 61.7M Model Table 4: Results of the ablation study on the LDC2017T10 development set. Additional Training Data Following previous works (Konstas et al., 2017; Song et al., 2018; Guo et al., 2019), we also evaluate our models employing additional data from English Gigaword corpus (Napoles et al., 2012). We sample 200K Gigaword sentences and use JAMR4 (Flanig"
D19-1314,N19-1238,0,0.0992376,"Missing"
D19-1314,P17-1014,0,0.199886,"eration models on two datasets, including a model that leverages additional syntactic information (Cao and Clark, 2019). • We compare the performance of three graph encoders, which have not been studied so far for AMR-to-text generation. 2 Related Work Early works on AMR-to-text generation employ statistical methods (Flanigan et al., 2016b; Pourdamghani et al., 2016; Castro Ferreira et al., 2017) and apply linearization of the graph by means of a depth-first traversal. Recent neural approaches have exhibited success by linearising the input graph and using a sequence-to-sequence architecture. Konstas et al. (2017) achieve promising results on this task. However, they strongly rely on named entities anonymisation. Anonymisation requires an ad hoc procedure for each new corpus. The matching procedure needs to match a rare input item correctly (e.g., “United States of America”) with the corresponding part in the output text (e.g., “USA”) which may be challenging and may result in incorrect or incomplete delexicalisations. In contrast, our approach omits anonymisation. Instead, we use a copy mechanism (See et al., 2017), a generic technique which is easy to integrate in the encoder-decoder framework and ca"
D19-1314,C18-1101,0,0.0600318,"graph representation leads to improvements in AMR-to-text generation, achieving state-ofthe-art results on two AMR datasets1 . 1 Introduction Abstract Meaning Representation (AMR; Banarescu et al. (2013)) is a linguistically-grounded semantic formalism that represents the meaning of a sentence as a rooted directed graph, where nodes are concepts and edges are semantic relations. As AMR abstracts away from surface word strings and syntactic structure producing a language neutral representation of meaning, its usage is beneficial in many semantic related NLP tasks, including text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). The purpose of AMR-to-text generation is to produce a text which verbalises the meaning encoded by an input AMR graph. This is a challenging task as capturing the complex structural information stored in graph-based data is not trivial, as these are non-Euclidean structures, which 1 Code is available at https://github.com/UKPLab/emnlp2019-dualgraph implies that properties such as global parametrization, vector space structure, or shift-invariance do not hold (Bronstein et al., 2017). Recently, Graph Neural Networks (GNNs) have emerged as a powerful"
D19-1314,W18-6501,0,0.122099,"Missing"
D19-1314,W18-3601,0,0.0661719,"Missing"
D19-1314,W12-3018,0,0.0181848,"Missing"
D19-1314,C12-1124,1,0.816294,"text generation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019). In this paper, we propose a novel graph-tosequence approach to AMR-to-text generation, which is inspired by pre-neural generation algorithms. These approaches explored alternative (top-down, bottom-up and mixed) traversals of the input graph and showed that a hybrid traversal combining both top-down (TD) and bottom-up (BU) information was best as this permits integrating both global constraints top-down from the input and local constraints bottom-up from the semantic heads (Shieber et al., 1990; Narayan and Gardent, 2012). Similarly, we present an approach where the input graph is represented by two separate structures, each representing a different view of the graph. The nodes of these two structures are encoded using separate graph encoders so that each concept and relation in the input graph is assigned both a TD and a BU representation. Our approach markedly differs from existing graph-to-sequence models for MR-to-Text generation (Marcheggiani and Perez Beltrachini, 2018; Beck et al., 2018; Damonte and Cohen, 2019) in that these approaches aggregate all the immediate neighborhood information of a node in a"
D19-1314,P02-1040,0,0.106388,"Missing"
D19-1314,D14-1162,0,0.0820871,"Missing"
D19-1314,W16-6603,0,0.366354,"edges undirected. Our main contributions are the following: • We present a novel architecture for AMR-to-text generation which explicitly encodes two separate TD and BU views of the input graph. • We show that our approach outperforms recent AMR-to-text generation models on two datasets, including a model that leverages additional syntactic information (Cao and Clark, 2019). • We compare the performance of three graph encoders, which have not been studied so far for AMR-to-text generation. 2 Related Work Early works on AMR-to-text generation employ statistical methods (Flanigan et al., 2016b; Pourdamghani et al., 2016; Castro Ferreira et al., 2017) and apply linearization of the graph by means of a depth-first traversal. Recent neural approaches have exhibited success by linearising the input graph and using a sequence-to-sequence architecture. Konstas et al. (2017) achieve promising results on this task. However, they strongly rely on named entities anonymisation. Anonymisation requires an ad hoc procedure for each new corpus. The matching procedure needs to match a rare input item correctly (e.g., “United States of America”) with the corresponding part in the output text (e.g., “USA”) which may be challe"
D19-1314,P17-1099,0,0.244136,"ccess by linearising the input graph and using a sequence-to-sequence architecture. Konstas et al. (2017) achieve promising results on this task. However, they strongly rely on named entities anonymisation. Anonymisation requires an ad hoc procedure for each new corpus. The matching procedure needs to match a rare input item correctly (e.g., “United States of America”) with the corresponding part in the output text (e.g., “USA”) which may be challenging and may result in incorrect or incomplete delexicalisations. In contrast, our approach omits anonymisation. Instead, we use a copy mechanism (See et al., 2017), a generic technique which is easy to integrate in the encoder-decoder framework and can be used independently of the particular domain and application. Our approach further differs from Konstas et al. (2017) in that we build a dual TD/BU graph representation and use graph encoders to represent nodes. Cao and Clark (2019) factor the generation process leveraging syntactic information to improve the performance. However, they linearize both AMR and constituency graphs, which implies that important parts of the graphs cannot well be represented (e.g., coreference). Several graph-to-sequence mod"
D19-1314,Q19-1002,0,0.0799941,"n AMR-to-text generation, achieving state-ofthe-art results on two AMR datasets1 . 1 Introduction Abstract Meaning Representation (AMR; Banarescu et al. (2013)) is a linguistically-grounded semantic formalism that represents the meaning of a sentence as a rooted directed graph, where nodes are concepts and edges are semantic relations. As AMR abstracts away from surface word strings and syntactic structure producing a language neutral representation of meaning, its usage is beneficial in many semantic related NLP tasks, including text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). The purpose of AMR-to-text generation is to produce a text which verbalises the meaning encoded by an input AMR graph. This is a challenging task as capturing the complex structural information stored in graph-based data is not trivial, as these are non-Euclidean structures, which 1 Code is available at https://github.com/UKPLab/emnlp2019-dualgraph implies that properties such as global parametrization, vector space structure, or shift-invariance do not hold (Bronstein et al., 2017). Recently, Graph Neural Networks (GNNs) have emerged as a powerful class of methods for learning effective gra"
D19-1314,P18-1150,0,0.437341,"challenging task as capturing the complex structural information stored in graph-based data is not trivial, as these are non-Euclidean structures, which 1 Code is available at https://github.com/UKPLab/emnlp2019-dualgraph implies that properties such as global parametrization, vector space structure, or shift-invariance do not hold (Bronstein et al., 2017). Recently, Graph Neural Networks (GNNs) have emerged as a powerful class of methods for learning effective graph latent representations (Xu et al., 2019) and graph-to-sequence models have been applied to the task of AMR-to-text generation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019). In this paper, we propose a novel graph-tosequence approach to AMR-to-text generation, which is inspired by pre-neural generation algorithms. These approaches explored alternative (top-down, bottom-up and mixed) traversals of the input graph and showed that a hybrid traversal combining both top-down (TD) and bottom-up (BU) information was best as this permits integrating both global constraints top-down from the input and local constraints bottom-up from the semantic heads (Shieber et al., 1990; Narayan and Gardent, 2012). Simila"
D19-1314,N18-1101,0,0.0386878,"Missing"
D19-1410,D15-1075,0,0.590122,"r by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository3 . Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings. Sentence embeddings are a well studied area with dozens of proposed methods. Skip-Thought (Kiros et al., 2015) trains an encoder-decoder architecture to predict the surrounding sentences. InferSent (Conneau et al., 2017) uses labeled data of the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to train a siamese BiLSTM network with max-pooling over the output. Conneau et al. showed, that InferSent consistently outperforms unsupervised methods like SkipThought. Universal Sentence Encoder (Cer et al., 2018) trains a transformer network and augments unsupervised learning with training on SNLI. Hill et al. (2016) showed, that the task on which sentence embeddings are trained significantly impacts their quality. Previous work (Conneau et al., 2017; Cer et al., 2018) found that the SNLI datasets are suitable for training sentence emb"
D19-1410,D18-2029,0,0.282997,"Missing"
D19-1410,D17-1070,0,0.360786,"November 3–7, 2019. 2019 Association for Computational Linguistics most similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings (~5 seconds with SBERT) and computing cosinesimilarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017). We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively. SBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a"
D19-1410,S13-1004,0,0.201959,"Missing"
D19-1410,S12-1051,0,0.134643,"nce Encoder is SICK-R. Universal Sentence Encoder was trained on various datasets, including news, question-answer pages and discussion forums, which appears to be more suitable to the data of SICK-R. In contrast, SBERT was pre-trained only on Wikipedia (via BERT) and on NLI data. While RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings. Unsupervised STS We evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in (Reimers et al., 2016) that Pearson correlation is badly suited for 4.2 Supervised STS The STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories captions, news, and forums. It is divided into train (5,749), dev (1,500) and test (1,379). BERT set a new sta"
D19-1410,L18-1269,0,0.284735,"similarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds (Johnson et al., 2017). We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively. SBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018). The paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates S"
D19-1410,C04-1051,0,0.509616,"Missing"
D19-1410,P18-2009,0,0.159569,"ence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively. SBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018). The paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods. 2 Related Work We first introduce BERT, then, we discuss stateof-the-art sentence embedding methods. BERT (Devlin et al., 2018)"
D19-1410,N16-1162,0,0.0388908,"thods. Skip-Thought (Kiros et al., 2015) trains an encoder-decoder architecture to predict the surrounding sentences. InferSent (Conneau et al., 2017) uses labeled data of the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to train a siamese BiLSTM network with max-pooling over the output. Conneau et al. showed, that InferSent consistently outperforms unsupervised methods like SkipThought. Universal Sentence Encoder (Cer et al., 2018) trains a transformer network and augments unsupervised learning with training on SNLI. Hill et al. (2016) showed, that the task on which sentence embeddings are trained significantly impacts their quality. Previous work (Conneau et al., 2017; Cer et al., 2018) found that the SNLI datasets are suitable for training sentence embeddings. Yang et al. (2018) presented a method to train on conversations from Reddit using siamese DAN and siamese transformer networks, which yielded good results on the STS benchmark dataset. Humeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between m context vectors and pre3 https://"
D19-1410,P05-1015,0,0.0536597,"is computed for the test-fold. The purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by Devlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks. We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks: • MR: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005). • CR: Sentiment prediction of customer product reviews (Hu and Liu, 2004). • SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004). • MPQA: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005). • SST: Stanford Sentiment Treebank with binary labels (Socher et al., 2013). • TREC: Fine grained question-type classification from TREC (Li and Roth, 2002). • MRPC: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004). The results can be found in Table 5. SBERT is able to achieve the best perfor"
D19-1410,D14-1162,0,0.0965899,"ever, answering a single query would require over 50 hours. A common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixedsize sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014). To alleviate this issue, we developed SBERT. The siamese network architecture enables that fixed-sized vectors for input sentences can be derived. Using a similarity measure like cosinesimilarity or Manhatten / Euclidean distance, semantically similar sentences can be found. These similarity measures can be performed extremely efficient on modern hardware, allowing SBERT to be used for semantic similarity search as well as for clustering. The complexity for finding the 3982 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint"
D19-1410,C16-1009,1,0.948047,"pedia (via BERT) and on NLI data. While RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings. Unsupervised STS We evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in (Reimers et al., 2016) that Pearson correlation is badly suited for 4.2 Supervised STS The STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories captions, news, and forums. It is divided into train (5,749), dev (1,500) and test (1,379). BERT set a new state-of-the-art performance on this dataset by passing both sentences to the network and using a simple regres3985 sion method for the output. Model Spearman Not trained for STS Avg. GloVe embeddings 58.02 Avg. BERT embeddings 46.35 InferSent - GloVe 6"
D19-1410,C02-1150,0,0.0872723,"mbeddings to other sentence embeddings methods on the following seven SentEval transfer tasks: • MR: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005). • CR: Sentiment prediction of customer product reviews (Hu and Liu, 2004). • SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004). • MPQA: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005). • SST: Stanford Sentiment Treebank with binary labels (Socher et al., 2013). • TREC: Fine grained question-type classification from TREC (Li and Roth, 2002). • MRPC: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004). The results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task. 3987 Model Avg. GloVe embeddings Avg. fast-text embeddings Avg. BERT embeddings BERT CLS-vector InferSent - GloVe Universal Sentence"
D19-1410,2021.ccl-1.108,0,0.457097,"Missing"
D19-1410,P19-1054,1,0.841862,"eath penalty. The data was annotated on a scale from 0 (“different topic”) to 5 (“completely equivalent”). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually descriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset (Reimers et al., 2019). We evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate SBERT using 10-fold cross-validation. A drawback of this evaluation setup is that it is not clear how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat this for all three topics and average the results. SBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We a"
D19-1410,marelli-etal-2014-sick,0,0.0489763,"es and discussion forums, which appears to be more suitable to the data of SICK-R. In contrast, SBERT was pre-trained only on Wikipedia (via BERT) and on NLI data. While RoBERTa was able to improve the performance for several supervised tasks, we only observe minor difference between SBERT and SRoBERTa for generating sentence embeddings. Unsupervised STS We evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), the STS benchmark (Cer et al., 2017), and the SICK-Relatedness dataset (Marelli et al., 2014). These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in (Reimers et al., 2016) that Pearson correlation is badly suited for 4.2 Supervised STS The STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised STS systems. The data includes 8,628 sentence pairs from the three categories captions, news, and forums. It is divided into train (5,749), dev (1,500) and test (1,379). BERT set a new state-of-the-art performance on this dataset by passing both sentences to the network and using a simple regres3985 sion me"
D19-1410,N19-1063,0,0.0124375,"rformance of BERT can further improved by small adaptations to the pre-training process. We also tested XLNet (Yang et al., 2019), but it led in general to worse results than BERT. A large disadvantage of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to derive sentence embeddings from BERT. To bypass this limitations, researchers passed single sentences through BERT and then derive a fixed sized vector by either averaging the outputs (similar to average word embeddings) or by using the output of the special CLS token (for example: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository3 . Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings. Sentence embeddings are a well studied area with dozens of proposed methods. Skip-Thought (Kiros et al., 2015) trains an encoder-decoder architecture to predict the surrounding sentences. InferSent (Conneau et al., 2017) uses labeled data of the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to"
D19-1410,W16-3636,0,0.262427,"outperform other state-of-the-art sentence embedding methods like InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018). On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively. SBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018). The paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods. 2 Related Wo"
D19-1410,P04-1035,0,0.0991052,"evlin et al. (2018) for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks. We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks: • MR: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005). • CR: Sentiment prediction of customer product reviews (Hu and Liu, 2004). • SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004). • MPQA: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005). • SST: Stanford Sentiment Treebank with binary labels (Socher et al., 2013). • TREC: Fine grained question-type classification from TREC (Li and Roth, 2002). • MRPC: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004). The results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer lear"
D19-1410,D13-1170,0,0.0242156,"ality of our sentence embeddings for various tasks. We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks: • MR: Sentiment prediction for movie reviews snippets on a five start scale (Pang and Lee, 2005). • CR: Sentiment prediction of customer product reviews (Hu and Liu, 2004). • SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries (Pang and Lee, 2004). • MPQA: Phrase level opinion polarity classification from newswire (Wiebe et al., 2005). • SST: Stanford Sentiment Treebank with binary labels (Socher et al., 2013). • TREC: Fine grained question-type classification from TREC (Li and Roth, 2002). • MRPC: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al., 2004). The results can be found in Table 5. SBERT is able to achieve the best performance in 5 out of 7 tasks. The average performance increases by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder. Even though transfer learning is not the purpose of SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task. 3987 Model Avg. GloVe embeddings Avg. fast-text embe"
D19-1410,N18-1101,0,0.0981169,"xample: May et al. (2019); Zhang et al. (2019); Qiao et al. (2019)). These two options are also provided by the popular bert-as-a-service-repository3 . Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings. Sentence embeddings are a well studied area with dozens of proposed methods. Skip-Thought (Kiros et al., 2015) trains an encoder-decoder architecture to predict the surrounding sentences. InferSent (Conneau et al., 2017) uses labeled data of the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to train a siamese BiLSTM network with max-pooling over the output. Conneau et al. showed, that InferSent consistently outperforms unsupervised methods like SkipThought. Universal Sentence Encoder (Cer et al., 2018) trains a transformer network and augments unsupervised learning with training on SNLI. Hill et al. (2016) showed, that the task on which sentence embeddings are trained significantly impacts their quality. Previous work (Conneau et al., 2017; Cer et al., 2018) found that the SNLI datasets are suitable for training sentence embeddings. Yang et al. (2018) presented a method to train"
D19-1410,W18-3022,0,0.01505,"MultiGenre NLI dataset (Williams et al., 2018) to train a siamese BiLSTM network with max-pooling over the output. Conneau et al. showed, that InferSent consistently outperforms unsupervised methods like SkipThought. Universal Sentence Encoder (Cer et al., 2018) trains a transformer network and augments unsupervised learning with training on SNLI. Hill et al. (2016) showed, that the task on which sentence embeddings are trained significantly impacts their quality. Previous work (Conneau et al., 2017; Cer et al., 2018) found that the SNLI datasets are suitable for training sentence embeddings. Yang et al. (2018) presented a method to train on conversations from Reddit using siamese DAN and siamese transformer networks, which yielded good results on the STS benchmark dataset. Humeau et al. (2019) addresses the run-time overhead of the cross-encoder from BERT and present a method (poly-encoders) to compute a score between m context vectors and pre3 https://github.com/hanxiao/ bert-as-service/ 3983 Softmax classifier -1 … 1 (u, v, |u-v|) cosine-sim(u, v) u v u v pooling pooling pooling pooling BERT BERT BERT BERT Sentence A Sentence B Sentence A Sentence B Figure 1: SBERT architecture with classificatio"
D19-3013,W18-0502,0,0.020998,"tal, LMU M¨unchen, Germany 3 Chair of Education and Educational Psychology, LMU M¨unchen, Germany http://famulus-project.de Abstract diagnostic skills are vital for many professions (e.g., medical doctors searching for a therapy, teachers identifying potential mental disorders at an early stage, engineers diagnosing a machine failure, etc.). Training diagnostic skills is hard and typically relies on time-consuming and hard-to-control live role-plays. Online case simulations involving socalled virtual patients crystallized as an effective alternative to role-playing games (Berman et al., 2016; Jin et al., 2018). In case simulations, students collect information on a virtual patient across multiple screens, e.g., from patient–doctor dialogs, lab results, and medical imaging. To date, the students formulate their final diagnosis by means of multiple-choice questions, which are easy to assess, but prevent important analyses of the effectiveness and the efficiency of the diagnostic reasoning process. This is why we propose to complement multiple-choice questions with prompts asking for explanations of the students’ thought process. The open-form textual explanations enable good insight into the diagnost"
D19-3013,C18-2002,1,0.81403,"Missing"
D19-3013,N16-1136,0,0.0233819,"Missing"
D19-3013,P16-4014,0,0.0222806,"g in medicine and teacher education and outline how our system can be extended to further use cases. 1 Introduction Motivation. Supporting students in learning has been the life purpose of many teachers throughout history. With the growing number of people who choose an academic path, it becomes increasingly important to leverage automatic methods to guide students and give them individualized feedback. However, existing systems for technologyenhanced learning, mostly address skills on recalling, explaining, and applying knowledge, e.g., in automatically generated language learning exercises (Madnani et al., 2016) and math word problems (Koncel-Kedziorski et al., 2016). More complex cognitive tasks such as diagnostic reasoning require analytic and decision-making skills, for which there are yet only few solutions, even though Contributions. To tackle this task, we propose our FAMULUS system to generate individual feedback on the students’ diagnostic skills. FAMULUS integrates (a) state-of-the-art neural sequence labeling models to generate individualized feedback, incorporated in our novel NeuralWeb service, and (b) a corpus construction tool enabling interactive model training with (c) an existing too"
D19-3013,D17-1035,1,0.744427,"ly improve the classifiers. We leverage this functionality to create an efficient interactive annotation process for our diagnostic classes and thus to create training data for our NLP models. Figure 2 shows an example of the labeling process with suggestions by our pre-trained model. Model. The wrapper class includes a loading function which leverages the downstream model architecture and copies the respective weights into memory. The supported neural architectures are written in Keras2 , and PyTorch3 and are therefore easy to adapt. NeuralWeb currently provides a recent BiLSTM architecture (Reimers and Gurevych, 2017) implemented in Keras and Flair (Akbik et al., 2018) implemented in PyTorch, which holds the current state of the art on many sequence-labeling tasks. A prediction function of the wrapper preprocesses a text (sentence splitting and tokenization using NLTK) and leverages the pre-trained model to predict and return the diagnostic classes. NeuralWeb additionally enables automatic retraining of the model within the framework which is useful when new data has been generated and annotated, improving the model automatically. This functionality is currently implemented for the Keras-based model. 3.3 3"
D19-3013,P19-1265,1,0.891322,"Missing"
D19-3013,C18-1139,0,0.0137381,"o create an efficient interactive annotation process for our diagnostic classes and thus to create training data for our NLP models. Figure 2 shows an example of the labeling process with suggestions by our pre-trained model. Model. The wrapper class includes a loading function which leverages the downstream model architecture and copies the respective weights into memory. The supported neural architectures are written in Keras2 , and PyTorch3 and are therefore easy to adapt. NeuralWeb currently provides a recent BiLSTM architecture (Reimers and Gurevych, 2017) implemented in Keras and Flair (Akbik et al., 2018) implemented in PyTorch, which holds the current state of the art on many sequence-labeling tasks. A prediction function of the wrapper preprocesses a text (sentence splitting and tokenization using NLTK) and leverages the pre-trained model to predict and return the diagnostic classes. NeuralWeb additionally enables automatic retraining of the model within the framework which is useful when new data has been generated and annotated, improving the model automatically. This functionality is currently implemented for the Keras-based model. 3.3 3 CASUS CASUS4 is an interactive system designed for"
D19-3013,Q17-1010,0,0.00727702,"not. The feedback for a student who diagnoses Hepatitis A thus needs to be different with respect to the case she/he currently works on. The Feedback DB is 2 INCEpTION 4 FAMULUS Process The FAMULUS system consists of an interactive learning cycle connecting the three components keras.io pytorch.org 4 75 www.instruct.eu EG EE HG DC Med BiLSTM UB 71.60 85.61 80.20 90.25 69.28 86.37 65.32 85.58 TEd BiLSTM UB 78.53 93.29 78.87 90.71 57.16 81.77 61.77 82.11 Table 1: Individual macro-F1 scores following Schulz et al. (2019a) for each of the epistemic activities. The BiLSTM uses FastText embeddings (Bojanowski et al., 2017). This architecture is equal to Flair when only using FastText embeddings. UB reports the human upper bound (inter-annotator agreement) indicating room for improvement. Figure 3: FAMULUS process for annotating data, training models, and generating individualized feedback. During the cold-start phase, only the blue edges are used, until enough labeled data exists to train a model. noted in yellow) which are additionally presented to the instructor (see Figure 2). ® These predictions should increase the labeling speed, as in many cases, the instructor simply has to accept the suggestions the mod"
D19-3022,W16-2504,0,0.104783,"d has witnessed the creation of various word embedding models such as monolingual (Mikolov et al., 2013), contextualized (Peters et al., 2018), multi-sense (Pilehvar et al., 2017) and dependency-based (Levy and Goldberg, 2014); as well as adaptation and design of neural network architectures for a wide range of NLP tasks. Despite their impressive performance, interpreting, analyzing and evaluating such black-box models have been shown to be challenging, which even led to a set of workshop series (Linzen et al., 2018). Early works for evaluating word representations (Faruqui and Dyer, 2014a,b; Nayak et al., 2016) have mostly focused on English and used 127 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 127–132 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics (Faruqui and Dyer, 2014a) (Nayak et al., 2016) (K¨ohn, 2015) Ours #Lang 4 1 7 28 #Task-Type 10-WST 7-DT 7-PT 16-PT Web × × × Offline × × × × Static × × × × Models Layers Epochs × × × Table 1: Features of previous evaluation applications compared to Ours (L INSPECTOR W EB). #Lang: Number of supported languages, #Task-Type: Number and type of the tasks, where WST: Word similari"
D19-3022,Q19-1004,0,0.0139416,"ity tests do not necessarily correlate well with downstream tasks and evaluating embeddings on downstream tasks can be too computationally demanding for low-resource scenarios. To address some of these challenges, Shi et al. (2016); Adi et al. (2017); Veldhoen et al. (2016); Conneau et al. (2018) have introduced probing tasks, a.k.a. diagnostic classifiers, that take as input a representation generated by a fully trained neural model and output predictions for a linguistic feature of interest. Due to its simplicity and low computational cost, it has been employed by many studies summarized by Belinkov and Glass (2019), mostly focusing on English. Unlike most studies, K¨ohn (2015) introduced a set of multilingual probing tasks, however its scope has been limited to syntactic tests and 7 languages. More importantly it is not accessible as a web application and the source code does not have support to probe pretrained downstream NLP models out of the box. Given the above information, most of the lowerresource non-English academic NLP communities still suffer from (1) the amount of required human and computational resources to search for the right model configuration, and (2) the lack of diagnostics tools to a"
D19-3022,P17-1152,0,0.0191908,"ustom AllenNLP dataset reader and classifier should be added. It can be extended to new AllenNLP models by adding the matching predictor to the supported list or writing a custom predictor if the model requires dual input values (e.g. ESIM). Finally, other frameworks (e.g. ONNX format) can be supported by adding a method to extract embeddings from the model. Features: Models, Layers and Epochs We support the following classifier architectures implemented by AllenNLP: BiaffineDependencyParser (Dozat and Manning, 2016), CrfTagger (Sutton et al., 2007), SimpleTagger (Gardner et al., 2018), ESIM (Chen et al., 2017). BiaffineDependencyParser and CrfTagger are highlighted as the default choice for dependency parsing and named entity recognition by (Gardner et al., 2018), while ESIM was picked as one of two available natural language inference models, and SimpleTagger support was added as the entry level AllenNLP classifier to solve tasks like partsof-speech tagging. The users can choose the layers they want to probe. This allows the users to analyze what linguistic information is captured by different layers of the model (e.g., POS information in lower layers, semantic information in higher levels). It is"
D19-3022,P18-1198,0,0.0576088,"Missing"
D19-3022,D14-1162,0,0.092604,"16-PT Web × × × Offline × × × × Static × × × × Models Layers Epochs × × × Table 1: Features of previous evaluation applications compared to Ours (L INSPECTOR W EB). #Lang: Number of supported languages, #Task-Type: Number and type of the tasks, where WST: Word similarity tasks, DT: Downstream Tasks, PT: Probing Tasks. Static: Static word embeddings and Models: Pretrained downstream models. computational requirements by simply uploading a file. The users can either upload their pretrained static embeddings files (e.g. word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2016), GloVe (Pennington et al., 2014)); 1 or their pretrained archived AllenNLP models. In this version, we only give support to AllenNLP, due to its high usage rate by low-resource community and being up-to-date, i.e., containing state-of-the-art models for many NLP tasks and being continuously maintained at the Allen Institute for Artificial Intelligence (Gardner et al., 2018). 2018) models at different epochs and (2) measure the performance of static word embeddings for language-specific linguistic properties. To the best of our knowledge, this is the first web application that (a) performs online probing; (b) enables users to"
D19-3022,P81-1022,0,0.546105,"Missing"
D19-3022,N18-1202,0,0.0606037,"are commonly used for NLP downstream tasks such as named entity recognition, natural language inference and dependency parsing. The results are visualized in a polar chart and also provided as a table. L INSPECTOR W EB is available as an offline tool or at https://linspector. ukp.informatik.tu-darmstadt.de. 1 Introduction Natural language processing (NLP) has seen great progress after the introduction of continuous, dense, low dimensional vectors to represent text. The field has witnessed the creation of various word embedding models such as monolingual (Mikolov et al., 2013), contextualized (Peters et al., 2018), multi-sense (Pilehvar et al., 2017) and dependency-based (Levy and Goldberg, 2014); as well as adaptation and design of neural network architectures for a wide range of NLP tasks. Despite their impressive performance, interpreting, analyzing and evaluating such black-box models have been shown to be challenging, which even led to a set of workshop series (Linzen et al., 2018). Early works for evaluating word representations (Faruqui and Dyer, 2014a,b; Nayak et al., 2016) have mostly focused on English and used 127 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), page"
D19-3022,P14-5004,0,0.169908,"to represent text. The field has witnessed the creation of various word embedding models such as monolingual (Mikolov et al., 2013), contextualized (Peters et al., 2018), multi-sense (Pilehvar et al., 2017) and dependency-based (Levy and Goldberg, 2014); as well as adaptation and design of neural network architectures for a wide range of NLP tasks. Despite their impressive performance, interpreting, analyzing and evaluating such black-box models have been shown to be challenging, which even led to a set of workshop series (Linzen et al., 2018). Early works for evaluating word representations (Faruqui and Dyer, 2014a,b; Nayak et al., 2016) have mostly focused on English and used 127 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 127–132 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics (Faruqui and Dyer, 2014a) (Nayak et al., 2016) (K¨ohn, 2015) Ours #Lang 4 1 7 28 #Task-Type 10-WST 7-DT 7-PT 16-PT Web × × × Offline × × × × Static × × × × Models Layers Epochs × × × Table 1: Features of previous evaluation applications compared to Ours (L INSPECTOR W EB). #Lang: Number of supported languages, #Task-Type: Number and type of the tasks,"
D19-3022,P17-1170,0,0.0286674,"m tasks such as named entity recognition, natural language inference and dependency parsing. The results are visualized in a polar chart and also provided as a table. L INSPECTOR W EB is available as an offline tool or at https://linspector. ukp.informatik.tu-darmstadt.de. 1 Introduction Natural language processing (NLP) has seen great progress after the introduction of continuous, dense, low dimensional vectors to represent text. The field has witnessed the creation of various word embedding models such as monolingual (Mikolov et al., 2013), contextualized (Peters et al., 2018), multi-sense (Pilehvar et al., 2017) and dependency-based (Levy and Goldberg, 2014); as well as adaptation and design of neural network architectures for a wide range of NLP tasks. Despite their impressive performance, interpreting, analyzing and evaluating such black-box models have been shown to be challenging, which even led to a set of workshop series (Linzen et al., 2018). Early works for evaluating word representations (Faruqui and Dyer, 2014a,b; Nayak et al., 2016) have mostly focused on English and used 127 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 127–132 c Hong Kong, China, Novembe"
D19-3022,E14-1049,0,0.280614,"to represent text. The field has witnessed the creation of various word embedding models such as monolingual (Mikolov et al., 2013), contextualized (Peters et al., 2018), multi-sense (Pilehvar et al., 2017) and dependency-based (Levy and Goldberg, 2014); as well as adaptation and design of neural network architectures for a wide range of NLP tasks. Despite their impressive performance, interpreting, analyzing and evaluating such black-box models have been shown to be challenging, which even led to a set of workshop series (Linzen et al., 2018). Early works for evaluating word representations (Faruqui and Dyer, 2014a,b; Nayak et al., 2016) have mostly focused on English and used 127 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 127–132 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics (Faruqui and Dyer, 2014a) (Nayak et al., 2016) (K¨ohn, 2015) Ours #Lang 4 1 7 28 #Task-Type 10-WST 7-DT 7-PT 16-PT Web × × × Offline × × × × Static × × × × Models Layers Epochs × × × Table 1: Features of previous evaluation applications compared to Ours (L INSPECTOR W EB). #Lang: Number of supported languages, #Task-Type: Number and type of the tasks,"
D19-3022,N19-1329,0,0.0180739,"rent layers of the model (e.g., POS information in lower layers, semantic information in higher levels). It is possible to select any AllenNLP encoder layer for classifiers with token, sentence, or document based input and models with dual input (e.g. ESIM: premise, hypothesis) that allow probing of selected layers depending on their internal architecture as described in Sec. 4.2. Additionally a user can specify up to 3 epochs for probing to inspect what their model learns and forgets during training. This is considered a crucial feature as it provides insights on learning dynamics of models (Saphra and Lopez, 2019). For instance, a user diagnosing a pretrained NLI task, can probe for the tasks that have been shown to correlate well (Mood, Person, Polarity, and Tense) (S¸ahin et al., 2019) for additional epochs, and analyze how their 4 System Description L INSPECTOR W EB is based on the Python Django framework 4 which manages everything related to performance, security, scalability, and database handling. 4.1 Frontend First, the user selects the language of the model and a number of probing tests they want to perform. The probing test selection menu will vary with the selected language. Next the user has"
D19-3022,D15-1246,0,0.0463653,"Missing"
D19-3022,P14-2050,0,0.0437046,"ural language inference and dependency parsing. The results are visualized in a polar chart and also provided as a table. L INSPECTOR W EB is available as an offline tool or at https://linspector. ukp.informatik.tu-darmstadt.de. 1 Introduction Natural language processing (NLP) has seen great progress after the introduction of continuous, dense, low dimensional vectors to represent text. The field has witnessed the creation of various word embedding models such as monolingual (Mikolov et al., 2013), contextualized (Peters et al., 2018), multi-sense (Pilehvar et al., 2017) and dependency-based (Levy and Goldberg, 2014); as well as adaptation and design of neural network architectures for a wide range of NLP tasks. Despite their impressive performance, interpreting, analyzing and evaluating such black-box models have been shown to be challenging, which even led to a set of workshop series (Linzen et al., 2018). Early works for evaluating word representations (Faruqui and Dyer, 2014a,b; Nayak et al., 2016) have mostly focused on English and used 127 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 127–132 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computation"
D19-3022,Q17-1010,0,\N,Missing
D19-3022,D16-1159,0,\N,Missing
D19-3022,W18-2501,0,\N,Missing
D19-3022,W18-5400,0,\N,Missing
E12-1056,W04-3220,0,0.035078,"al lexicons providing accurate lexical-syntactic information, such as subcategorization frames (SCFs) are vital for many NLP applications involving parsing and word sense disambiguation. In parsing, SCFs have been successfully used to improve the output of statistical parsers (Klenner (2007), Deoskar (2008), Sigogne et al. (2011)) which is particularly significant in high-precision domain-independent parsing. In word sense disambiguation, SCFs have been identified as important features for verb sense disambiguation (Brown et al., 2011), which is due to the correlation of verb senses and SCFs (Andrew et al., 2004). SCFs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1 takes two arguments that can be realized, for instance, as noun phrase and that-clause as in He says that the window is open. Although a number of freely available, largescale and accurate SCF lexicons exist, e.g. COMLEX (Grishman et al., 1994), VerbNet (Kipper et al., 2008) for English, availability and limitations in size and coverage remain an inherent issue. This applies even more to languages other than English. One particular approach to address this issue is the combination and integration"
E12-1056,broeder-etal-2010-data,0,0.0606807,"Missing"
E12-1056,W11-0110,0,0.0283258,"VerbNet and IMSlex subset are publicly available.1 1 Introduction Computational lexicons providing accurate lexical-syntactic information, such as subcategorization frames (SCFs) are vital for many NLP applications involving parsing and word sense disambiguation. In parsing, SCFs have been successfully used to improve the output of statistical parsers (Klenner (2007), Deoskar (2008), Sigogne et al. (2011)) which is particularly significant in high-precision domain-independent parsing. In word sense disambiguation, SCFs have been identified as important features for verb sense disambiguation (Brown et al., 2011), which is due to the correlation of verb senses and SCFs (Andrew et al., 2004). SCFs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1 takes two arguments that can be realized, for instance, as noun phrase and that-clause as in He says that the window is open. Although a number of freely available, largescale and accurate SCF lexicons exist, e.g. COMLEX (Grishman et al., 1994), VerbNet (Kipper et al., 2008) for English, availability and limitations in size and coverage remain an inherent issue. This applies even more to languages other than English. On"
E12-1056,burchardt-etal-2006-salsa,0,0.0751702,"Missing"
E12-1056,C08-1025,0,0.0265305,"valuate our LMF-model, we performed a crosslingual comparison of SCF coverage and overlap for the standardized versions of the English and German lexicons. The SubcatLMF DTD, the conversion tools and the standardized versions of VerbNet and IMSlex subset are publicly available.1 1 Introduction Computational lexicons providing accurate lexical-syntactic information, such as subcategorization frames (SCFs) are vital for many NLP applications involving parsing and word sense disambiguation. In parsing, SCFs have been successfully used to improve the output of statistical parsers (Klenner (2007), Deoskar (2008), Sigogne et al. (2011)) which is particularly significant in high-precision domain-independent parsing. In word sense disambiguation, SCFs have been identified as important features for verb sense disambiguation (Brown et al., 2011), which is due to the correlation of verb senses and SCFs (Andrew et al., 2004). SCFs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1 takes two arguments that can be realized, for instance, as noun phrase and that-clause as in He says that the window is open. Although a number of freely available, largescale and accurate S"
E12-1056,eckle-kohler-etal-2012-uby,1,0.79857,"yet, a situation that is complicated by the fact that SCFs are highly languagespecific. The goal of this paper is to address these gaps for the two languages English and German by presenting a uniform LMF representation of SCFs for English and German which is utilized for the standardization of large-scale English and German SCF lexicons. The contributions of this paper are threefold: (1) We present the LMF model Subcat-LMF, an LMF-compliant lexicon representation format featuring a uniform and very fine-grained representation of SCFs for English and German. Subcat-LMF is a subset of Uby-LMF (Eckle-Kohler et al., 2012), the LMF model of the large integrated lexical resource Uby (Gurevych et al., 2012). (2) We convert lexicons with large-scale SCF information to Subcat-LMF: the English VerbNet and two German lexicons, i.e., GermaNet (Kunze and Lemnitzer, 2002) and a subset of IMSlex3 (Eckle-Kohler, 1999). (3) We perform a comparison of these three lexicons regarding SCF coverage and SCF overlap, based on the standardized representation. The remainder of this paper is structured as follows: Section 2 gives a detailed description of Subcat-LMF and section 3 demonstrates its usefulness for representing and cros"
E12-1056,francopoulo-etal-2006-lexical,0,0.0344749,"common, interoperable representation format. Monolingual SCF integration based on a common representation format has already been addressed by King and Crouch (2005) and just recently by Necsulescu et al. (2011) and Padr´o et al. (2011). However, neither King and Crouch (2005) nor Necsulescu et al. (2011) or Padr´o et al. (2011) make use of existing standards in order to create a uniform SCF representation for lexicon merging. The definition of an interoperable representation format according to an existing standard, such as the ISO standard Lexical Markup Framework (LMF, ISO 24613:2008, see Francopoulo et al. (2006)), is the 2 http://www.ukp.tu-darmstadt.de/data/uby http://verbs.colorado.edu/semlink/ 550 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 550–560, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics prerequisite for re-using this format in different contexts, thus contributing to the standardization and interoperability of language resources. While LMF models exist that cover the representation of SCFs (see Quochi et al. (2008), Buitelaar et al. (2009)), their suitability for representing SCFs"
E12-1056,J02-3001,0,0.0228767,"other lexical-syntactic information types in English and German. As our cross-lingual comparison of lexicons has revealed many complementary SCFs in VN, GN and ILS, mono- and cross-lingual alignments of these lexicons at sense level would lead to a major increase in SCF coverage. Moreover, the cross-lingually uniform representation of SCFs can be exploited for an additional alignment of the lexicons at the level of SCF arguments. Such a fine-grained alignment of SCFs can be used, for instance, to project VN semantic roles to GN, thus yielding a German resource for semantic role labeling (see Gildea and Jurafsky (2002), Swier and Stevenson (2005)). Subcat-LMF could be used for standardizing further English and German lexicons. The automatic conversion of lexicons to Subcat-LMF requires the manual definition of a mapping, at least for syntactic arguments. Furthermore, the automatic merging approach by Padr´o et al. (2011) could be tested for English: given our standardized version of VN, other English SCF lexicons could be merged fully automatically with the Subcat-LMF version of VN. 5 Conclusion Subcat-LMF contributes to fostering the standardization of language resources and their interoperability at the l"
E12-1056,C94-1042,0,0.544124,"s particularly significant in high-precision domain-independent parsing. In word sense disambiguation, SCFs have been identified as important features for verb sense disambiguation (Brown et al., 2011), which is due to the correlation of verb senses and SCFs (Andrew et al., 2004). SCFs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1 takes two arguments that can be realized, for instance, as noun phrase and that-clause as in He says that the window is open. Although a number of freely available, largescale and accurate SCF lexicons exist, e.g. COMLEX (Grishman et al., 1994), VerbNet (Kipper et al., 2008) for English, availability and limitations in size and coverage remain an inherent issue. This applies even more to languages other than English. One particular approach to address this issue is the combination and integration of existing manually built SCF lexicons. Lexicon integration has widely been adopted for increasing the coverage of lexicons regarding lexical-semantic information types, such as semantic roles, selectional restrictions, and word senses (e.g., Shi and Mihalcea (2005), the Semlink project2 , Navigli and Ponzetto (2010), Niemann and Gurevych"
E12-1056,E12-1059,1,0.827763,"The goal of this paper is to address these gaps for the two languages English and German by presenting a uniform LMF representation of SCFs for English and German which is utilized for the standardization of large-scale English and German SCF lexicons. The contributions of this paper are threefold: (1) We present the LMF model Subcat-LMF, an LMF-compliant lexicon representation format featuring a uniform and very fine-grained representation of SCFs for English and German. Subcat-LMF is a subset of Uby-LMF (Eckle-Kohler et al., 2012), the LMF model of the large integrated lexical resource Uby (Gurevych et al., 2012). (2) We convert lexicons with large-scale SCF information to Subcat-LMF: the English VerbNet and two German lexicons, i.e., GermaNet (Kunze and Lemnitzer, 2002) and a subset of IMSlex3 (Eckle-Kohler, 1999). (3) We perform a comparison of these three lexicons regarding SCF coverage and SCF overlap, based on the standardized representation. The remainder of this paper is structured as follows: Section 2 gives a detailed description of Subcat-LMF and section 3 demonstrates its usefulness for representing and cross-lingually comparing large-scale English and German lexicons. Section 4 provides a"
E12-1056,C10-1052,0,0.0190846,"ation format, i.e. given two lexicons, they map one lexicon to the format of the other. Moreover, their approach requires a significant overlap of SCFs and verbs in any two lexicons to be merged. The authors state that it is presently unclear, how much overlap is required to obtain sufficiently precise merging results. Standardizing SCFs: Much previous work on standardizing NLP lexicons in LMF has focused on WordNet-like resources. Soria et al. (2009) describe WordNet-LMF, an LMF model for representing wordnets which has been used in the KYOTO project.21 Later, WordNet-LMF has been adapted by Henrich and Hinrichs (2010) to GermaNet and by Toral et al. (2010) to the Italian WordNet. WordNet-LMF does not provide the possibility to represent subcategorization at all. The adaption of WordNet-LMF to GN (Henrich and Hinrichs, 2010) allows SCFs to be respresented as string values. However, this extension is not sufficient, because it provides no means to model the syntax-semantics interface, which specifies correspondences between syntactic and semantic arguments of verbs and other predicates. Quochi et al. (2008) report on an LMF model that covers the syntax-semantics mapping just mentioned; it has been used for s"
E12-1056,P07-2051,0,0.0240825,"aNet verbs. To evaluate our LMF-model, we performed a crosslingual comparison of SCF coverage and overlap for the standardized versions of the English and German lexicons. The SubcatLMF DTD, the conversion tools and the standardized versions of VerbNet and IMSlex subset are publicly available.1 1 Introduction Computational lexicons providing accurate lexical-syntactic information, such as subcategorization frames (SCFs) are vital for many NLP applications involving parsing and word sense disambiguation. In parsing, SCFs have been successfully used to improve the output of statistical parsers (Klenner (2007), Deoskar (2008), Sigogne et al. (2011)) which is particularly significant in high-precision domain-independent parsing. In word sense disambiguation, SCFs have been identified as important features for verb sense disambiguation (Brown et al., 2011), which is due to the correlation of verb senses and SCFs (Andrew et al., 2004). SCFs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1 takes two arguments that can be realized, for instance, as noun phrase and that-clause as in He says that the window is open. Although a number of freely available, largescal"
E12-1056,kunze-lemnitzer-2002-germanet,0,0.29702,"erman which is utilized for the standardization of large-scale English and German SCF lexicons. The contributions of this paper are threefold: (1) We present the LMF model Subcat-LMF, an LMF-compliant lexicon representation format featuring a uniform and very fine-grained representation of SCFs for English and German. Subcat-LMF is a subset of Uby-LMF (Eckle-Kohler et al., 2012), the LMF model of the large integrated lexical resource Uby (Gurevych et al., 2012). (2) We convert lexicons with large-scale SCF information to Subcat-LMF: the English VerbNet and two German lexicons, i.e., GermaNet (Kunze and Lemnitzer, 2002) and a subset of IMSlex3 (Eckle-Kohler, 1999). (3) We perform a comparison of these three lexicons regarding SCF coverage and SCF overlap, based on the standardized representation. The remainder of this paper is structured as follows: Section 2 gives a detailed description of Subcat-LMF and section 3 demonstrates its usefulness for representing and cross-lingually comparing large-scale English and German lexicons. Section 4 provides a discussion including related work and section 5 concludes. fies a core package and a number of extensions for modeling different types of lexicons, including sub"
E12-1056,I11-1099,1,0.817571,"t (Kipper et al., 2008) for English, availability and limitations in size and coverage remain an inherent issue. This applies even more to languages other than English. One particular approach to address this issue is the combination and integration of existing manually built SCF lexicons. Lexicon integration has widely been adopted for increasing the coverage of lexicons regarding lexical-semantic information types, such as semantic roles, selectional restrictions, and word senses (e.g., Shi and Mihalcea (2005), the Semlink project2 , Navigli and Ponzetto (2010), Niemann and Gurevych (2011), Meyer and Gurevych (2011)). Currently, SCFs are represented idiosyncratically in existing SCF lexicons. However, integration of SCFs requires a common, interoperable representation format. Monolingual SCF integration based on a common representation format has already been addressed by King and Crouch (2005) and just recently by Necsulescu et al. (2011) and Padr´o et al. (2011). However, neither King and Crouch (2005) nor Necsulescu et al. (2011) or Padr´o et al. (2011) make use of existing standards in order to create a uniform SCF representation for lexicon merging. The definition of an interoperable representation"
E12-1056,P10-1023,0,0.0353159,"exicons exist, e.g. COMLEX (Grishman et al., 1994), VerbNet (Kipper et al., 2008) for English, availability and limitations in size and coverage remain an inherent issue. This applies even more to languages other than English. One particular approach to address this issue is the combination and integration of existing manually built SCF lexicons. Lexicon integration has widely been adopted for increasing the coverage of lexicons regarding lexical-semantic information types, such as semantic roles, selectional restrictions, and word senses (e.g., Shi and Mihalcea (2005), the Semlink project2 , Navigli and Ponzetto (2010), Niemann and Gurevych (2011), Meyer and Gurevych (2011)). Currently, SCFs are represented idiosyncratically in existing SCF lexicons. However, integration of SCFs requires a common, interoperable representation format. Monolingual SCF integration based on a common representation format has already been addressed by King and Crouch (2005) and just recently by Necsulescu et al. (2011) and Padr´o et al. (2011). However, neither King and Crouch (2005) nor Necsulescu et al. (2011) or Padr´o et al. (2011) make use of existing standards in order to create a uniform SCF representation for lexicon mer"
E12-1056,W11-0122,1,0.824011,"rishman et al., 1994), VerbNet (Kipper et al., 2008) for English, availability and limitations in size and coverage remain an inherent issue. This applies even more to languages other than English. One particular approach to address this issue is the combination and integration of existing manually built SCF lexicons. Lexicon integration has widely been adopted for increasing the coverage of lexicons regarding lexical-semantic information types, such as semantic roles, selectional restrictions, and word senses (e.g., Shi and Mihalcea (2005), the Semlink project2 , Navigli and Ponzetto (2010), Niemann and Gurevych (2011), Meyer and Gurevych (2011)). Currently, SCFs are represented idiosyncratically in existing SCF lexicons. However, integration of SCFs requires a common, interoperable representation format. Monolingual SCF integration based on a common representation format has already been addressed by King and Crouch (2005) and just recently by Necsulescu et al. (2011) and Padr´o et al. (2011). However, neither King and Crouch (2005) nor Necsulescu et al. (2011) or Padr´o et al. (2011) make use of existing standards in order to create a uniform SCF representation for lexicon merging. The definition of an in"
E12-1056,R11-1041,0,0.0593244,"Missing"
E12-1056,quochi-etal-2008-lexicon,0,0.118805,"O standard Lexical Markup Framework (LMF, ISO 24613:2008, see Francopoulo et al. (2006)), is the 2 http://www.ukp.tu-darmstadt.de/data/uby http://verbs.colorado.edu/semlink/ 550 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 550–560, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics prerequisite for re-using this format in different contexts, thus contributing to the standardization and interoperability of language resources. While LMF models exist that cover the representation of SCFs (see Quochi et al. (2008), Buitelaar et al. (2009)), their suitability for representing SCFs at a large scale remains unclear: neither of these LMF-models has been used for standardizing lexicons with a large number of SCFs, such as VerbNet. Furthermore, the question of their applicability to different languages has not been investigated yet, a situation that is complicated by the fact that SCFs are highly languagespecific. The goal of this paper is to address these gaps for the two languages English and German by presenting a uniform LMF representation of SCFs for English and German which is utilized for the standard"
E12-1056,R11-1050,0,0.0221373,"model, we performed a crosslingual comparison of SCF coverage and overlap for the standardized versions of the English and German lexicons. The SubcatLMF DTD, the conversion tools and the standardized versions of VerbNet and IMSlex subset are publicly available.1 1 Introduction Computational lexicons providing accurate lexical-syntactic information, such as subcategorization frames (SCFs) are vital for many NLP applications involving parsing and word sense disambiguation. In parsing, SCFs have been successfully used to improve the output of statistical parsers (Klenner (2007), Deoskar (2008), Sigogne et al. (2011)) which is particularly significant in high-precision domain-independent parsing. In word sense disambiguation, SCFs have been identified as important features for verb sense disambiguation (Brown et al., 2011), which is due to the correlation of verb senses and SCFs (Andrew et al., 2004). SCFs specify syntactic arguments of verbs and other predicate-like lexemes, e.g. the verb say 1 takes two arguments that can be realized, for instance, as noun phrase and that-clause as in He says that the window is open. Although a number of freely available, largescale and accurate SCF lexicons exist, e.g."
E12-1056,H05-1111,0,0.0257114,"ormation types in English and German. As our cross-lingual comparison of lexicons has revealed many complementary SCFs in VN, GN and ILS, mono- and cross-lingual alignments of these lexicons at sense level would lead to a major increase in SCF coverage. Moreover, the cross-lingually uniform representation of SCFs can be exploited for an additional alignment of the lexicons at the level of SCF arguments. Such a fine-grained alignment of SCFs can be used, for instance, to project VN semantic roles to GN, thus yielding a German resource for semantic role labeling (see Gildea and Jurafsky (2002), Swier and Stevenson (2005)). Subcat-LMF could be used for standardizing further English and German lexicons. The automatic conversion of lexicons to Subcat-LMF requires the manual definition of a mapping, at least for syntactic arguments. Furthermore, the automatic merging approach by Padr´o et al. (2011) could be tested for English: given our standardized version of VN, other English SCF lexicons could be merged fully automatically with the Subcat-LMF version of VN. 5 Conclusion Subcat-LMF contributes to fostering the standardization of language resources and their interoperability at the lexical-syntactic level acros"
E12-1059,W09-3021,0,0.0329839,". We have designed U BYLMF8 as a model of the union of various heterogeneous resources, namely WN, GN, FN, and VN on the one hand and CCRs on the other hand. Two design principles guided our development of U BY-LMF: first, to preserve the information available in the original resources and to uniformly represent it in U BY-LMF. Second, to be able to extend U BY in the future by further languages, resources, and types of linguistic information, in particular, alignments between different LSRs. Wordnets, FN and VN are largely complementary regarding the information types they provide, see, e.g. Baker and Fellbaum (2009). Accordingly, they use different organizational units to represent this information. Wordnets, such as WN and GN, primarily contain information on lexical-semantic relations, such as synonymy, and use synsets (groups of lexemes that are synonymous) as organizational units. FN focuses on groups of lexemes that evoke the same prototypical situation (so-called semantic frames, Fillmore (1982)) involving semantic roles (so-called frame elements). VN, a large-scale verb lexicon, is organized in Levin-style verb classes (Levin, 1993) (groups of verbs that share the same syntactic alternations and s"
E12-1059,P98-1013,0,0.39973,"Missing"
E12-1059,broeder-etal-2010-data,0,0.0444109,"Missing"
E12-1059,francopoulo-etal-2006-lexical,0,0.0932985,"an open set of LSRs in multiple languages and the information mined automatically from corpora. The previous work also lacked the aspects of lexicon format standardization and API access. We believe that easy access to information in LSRs is crucial in terms of their acceptance and broad applicability in NLP. In this paper, we propose a solution to this. We define a standardized format for modeling LSRs. This is a prerequisite for resource interoperability and the smooth integration of resources. We employ the ISO standard Lexical Markup Framework (LMF: ISO 24613:2008), a metamodel for LSRs (Francopoulo et al., 2006), and Data Categories (DCs) selected from ISOCat.1 One of the main challenges of our work is to develop a model that is standard-compliant, yet able to express the information contained in diverse LSRs, and that in the long term supports the integration of the various resources. The main contributions of this paper can be 1 http://www.isocat.org/ 580 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 580–590, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics summarized as follows: (1) We present"
E12-1059,C10-1052,0,0.191469,"ake it easy for the NLP community to utilize U BY in a variety of tasks in the future. 2 Related Work The work presented in this paper concerns standardization of LSRs, large-scale integration thereof at the representational level, and the unified access to lexical-semantic information in the integrated resources. Standardization of resources. Previous work includes models for representing lexical information relative to ontologies (Buitelaar et al., 2009; McCrae et al., 2011), and standardized single wordnets (English, German and Italian wordnets) in the ISO standard LMF (Soria et al., 2009; Henrich and Hinrichs, 2010; Toral et al., 2010). 2 http://www.wiktionary.org/ http://www.wikipedia.org/ 4 http://www.omegawiki.org/ 5 http://www.ukp.tu-darmstadt.de/data/uby 3 McCrae et al. (2011) propose LEMON, a conceptual model for lexicalizing ontologies as an extension of the LexInfo model (Buitelaar et al., 2009). L EMON provides an LMF-implementation in the Web Ontology Language (OWL), which is similar to U BY-LMF, as it also uses DCs from ISOCat, but diverges further from the standard (e.g. by removing structural elements such as the predicative representation class). While we focus on modeling lexical-semantic"
E12-1059,P11-2063,0,0.0223923,"f the work is to improve word sense disambiguation. This work is similar to ours, as it 581 aims at a large-scale multilingual resource and includes several resources. It is however restricted to a single type of resource (wordnets) and features a single type of lexical information (semantic relations) specified upon synsets. Similarly, de Melo and Weikum (2009) create a multilingual wordnet by integrating wordnets, bilingual dictionaries and information from parallel corpora. None of these resources integrate lexicalsemantic information, such as syntactic subcategorization or semantic roles. McFate and Forbus (2011) present NULEX, a syntactic lexicon automatically compiled from WN, WKT-en and VN. As their goal is to create an open-license resource to enhance syntactic parsing, they enrich verbs and nouns in WN with inflection information from WKT-en and syntactic frames from VN. Thus, they only use a small part of the lexical information present in WKT-en. Padr´o et al. (2011) present their work on lexicon merging within the Panacea Project. One goal of Panacea is to create a lexical resource development platform that supports large-scale lexical acquisition and can be used to combine existing lexicons w"
E12-1059,I11-1099,1,0.448233,"ces (ECRs), such as WordNet and FrameNet, whose coverage is limited, nor by collaboratively constructed resources (CCRs), such as Wikipedia and Wiktionary, which encode lexical-semantic knowledge in a less systematic form than ECRs, because they are lacking expert supervision. Previously, there have been several independent efforts of combining existing LSRs to enhance their coverage w.r.t. their breadth and depth, i.e. (i) the number of lexical items, and (ii) the types of lexical-semantic information contained (Shi and Mihalcea, 2005; Johansson and Nugues, 2007; Navigli and Ponzetto, 2010b; Meyer and Gurevych, 2011). As these efforts often targeted particular applications, they focused on aligning selected, specialized information types. To our knowledge, no single work focused on modeling a wide range of ECRs and CCRs in multiple languages and a large variety of information types in a standardized format. Frequently, the presented model is not easily scalable to accommodate an open set of LSRs in multiple languages and the information mined automatically from corpora. The previous work also lacked the aspects of lexicon format standardization and API access. We believe that easy access to information in"
E12-1059,P10-1023,0,0.348604,"gle expert-constructed resources (ECRs), such as WordNet and FrameNet, whose coverage is limited, nor by collaboratively constructed resources (CCRs), such as Wikipedia and Wiktionary, which encode lexical-semantic knowledge in a less systematic form than ECRs, because they are lacking expert supervision. Previously, there have been several independent efforts of combining existing LSRs to enhance their coverage w.r.t. their breadth and depth, i.e. (i) the number of lexical items, and (ii) the types of lexical-semantic information contained (Shi and Mihalcea, 2005; Johansson and Nugues, 2007; Navigli and Ponzetto, 2010b; Meyer and Gurevych, 2011). As these efforts often targeted particular applications, they focused on aligning selected, specialized information types. To our knowledge, no single work focused on modeling a wide range of ECRs and CCRs in multiple languages and a large variety of information types in a standardized format. Frequently, the presented model is not easily scalable to accommodate an open set of LSRs in multiple languages and the information mined automatically from corpora. The previous work also lacked the aspects of lexicon format standardization and API access. We believe that e"
E12-1059,P10-1154,0,0.0365466,"Missing"
E12-1059,P06-1014,0,0.0495444,"ense alignments already present in OW and WP. Some OW entries provide links to the corresponding WP page. Also, the German and English language editions of WP and OW are connected by inter-language links between articles (Senses in U BY). We can expect that these links have high quality, as they were entered manually by users and are subject to community control. Therefore, we straightforwardly imported them into U BY. Alignment Framework. Automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations (Navigli, 2006). To support this task for a large number of resources across languages, we have designed a flexible alignment framework based on the state-of-the-art method of Niemann and Gurevych (2011). The framework is generic in order to allow alignments between different kinds of entities as found in different resources, e.g. WN synsets, FN frames or WP articles. The only requirement is that the individual entities are distinguishable by a unique identifier in each resource. The alignment consists of the following steps: First, we extract the alignment candidates for a given resource pair, e.g. WN sense"
E12-1059,W11-0122,1,0.418666,"omprises 10 Lexicon instances, one each for OW-de and OW-en, and one lexicon each for the remaining eight LSRs. 4.2 Adding Sense Alignments Besides the uniform and standardized representation of the single LSRs, one major asset of U BY is the semantic interoperability of resources at the sense level. In the following, we (i) describe how we converted already existing sense alignments of resources into LMF, and (ii) present a framework to infer alignments automatically for any pair of resources. Existing Alignments. Previous work on sense alignment yielded several alignments, such as WN–WP-en (Niemann and Gurevych, 2011), WN–WKT-en (Meyer and Gurevych, 2011) and VN–FN (Palmer, 2009). We converted these alignments into U BY-LMF by creating a SenseAxis instance for each pair of aligned senses. This involved mapping the sense IDs from the proprietary alignment files to the corresponding sense IDs in U BY. In addition, we integrated the sense alignments already present in OW and WP. Some OW entries provide links to the corresponding WP page. Also, the German and English language editions of WP and OW are connected by inter-language links between articles (Senses in U BY). We can expect that these links have high"
E12-1059,R11-1041,0,0.0647688,"Missing"
E12-1059,C98-1013,0,\N,Missing
E12-1059,kunze-lemnitzer-2002-germanet,0,\N,Missing
E12-1079,J08-4004,0,0.0433896,"Missing"
E12-1079,W11-0707,0,0.712874,"), chats (Carpenter and Fujioka, 2011) and emails (Cohen et al., 2004), Wikipedia Talk pages have recently moved into the center of attention of the research community. Vi´egas et al. (2007) manually annotate 25 Wikipedia article discussion pages with a set of 11 labels in order to analyze how Talk pages are used for planning the work on articles and resolving disputes among the editors. Schneider et al. (2011) extend this schema and manually annotate 100 Talk pages with 15 labels. They confirm the findings of Vi´egas et al. that coordination requests occur most frequently in the discussions. Bender et al. (2011) describe a corpus of 47 Talk pages which have been annotated for authority claims and alignment moves. With this corpus, the authors analyze how the participants in Wikipedia discussions establish their credibility and how they express agreement and disagreement towards other participants or topics. From a different perspective, Stvilia et al. (2008) analyze 60 discussion pages in regard to how information quality (IQ) in Wikipedia articles is assessed on the Talk pages and which types of IQ problems are identified by the community. They describe a Wikipedia IQ assessment model and map it to"
E12-1079,J96-2004,0,0.220396,"scussed in Section 4.2. Corpus Format We publish our SEWD corpus in two formats9 , the original MMAX format, and as XMI files for further processing with the Apache Unstructured Information Management Architecture10 . For the latter format, we also provide the type system which defines all necessary corpus specific types needed for using the data in an NLP pipeline. 4.1 Inter-Annotator Agreement To evaluate the reliability of our dataset, we perform a detailed inter-rater agreement study. For measuring the agreement of the individual labels, we report the observed agreement, Kappa statistics (Carletta, 1996), and F1 -scores. The latter are computed by treating one annotator as the gold standard and the other one as predictions (Hripcsak and Rothschild, 2005). The scores can be seen in Table 2. The average observed agreement across all labels is P¯O = .94. The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). Furthermore, we obtain an overall pooled Kappa (De Vries et al., 2008) of κpool = .67, 8 http://www.mmax2.net http://w"
E12-1079,W04-3240,0,0.758971,"Missing"
E12-1079,P11-4017,1,0.734472,"Missing"
E12-1079,W10-1012,0,0.333617,"ce level by the Discourse Resource Initiative. It uses a four-dimensional tagset that allows arbitrary label combinations for each utterance. Jurafsky et al. (1997) augmented the DAMSL schema to fit the peculiarities of the Switchboard corpus. The resulting SWDB-DAMSL schema contained more than 220 distinct labels which have been clustered to 42 coarse grained labels. Both schemata have often been adapted for special purpose annotation tasks. With the rise of the social web, the amount of research analyzing user generated discourse substantially increased. In addition to analyzing web forums (Kim et al., 2010a), chats (Carpenter and Fujioka, 2011) and emails (Cohen et al., 2004), Wikipedia Talk pages have recently moved into the center of attention of the research community. Vi´egas et al. (2007) manually annotate 25 Wikipedia article discussion pages with a set of 11 labels in order to analyze how Talk pages are used for planning the work on articles and resolving disputes among the editors. Schneider et al. (2011) extend this schema and manually annotate 100 Talk pages with 15 labels. They confirm the findings of Vi´egas et al. that coordination requests occur most frequently in the discussions."
E12-1079,W10-2923,0,0.499028,"ce level by the Discourse Resource Initiative. It uses a four-dimensional tagset that allows arbitrary label combinations for each utterance. Jurafsky et al. (1997) augmented the DAMSL schema to fit the peculiarities of the Switchboard corpus. The resulting SWDB-DAMSL schema contained more than 220 distinct labels which have been clustered to 42 coarse grained labels. Both schemata have often been adapted for special purpose annotation tasks. With the rise of the social web, the amount of research analyzing user generated discourse substantially increased. In addition to analyzing web forums (Kim et al., 2010a), chats (Carpenter and Fujioka, 2011) and emails (Cohen et al., 2004), Wikipedia Talk pages have recently moved into the center of attention of the research community. Vi´egas et al. (2007) manually annotate 25 Wikipedia article discussion pages with a set of 11 labels in order to analyze how Talk pages are used for planning the work on articles and resolving disputes among the editors. Schneider et al. (2011) extend this schema and manually annotate 100 Talk pages with 15 labels. They confirm the findings of Vi´egas et al. that coordination requests occur most frequently in the discussions."
E12-1079,passonneau-2006-measuring,0,0.0123517,"ave been labeled with the given label by at least one annotator. PO denotes the observed agreement. which is defined as κpool = P¯O − P¯E 1 − P¯E (1) with L 1X POl P¯O = L l=1 L , 1X P¯E = PEl L (2) l=1 where L denotes the number of labels, PEl the expected agreement and POl the observed agreement of the lth label. κpool is regarded to be more accurate than an averaged Kappa. For assessing the overall inter-rater reliability of the label set assignments per turn, we chose Krippendorff’s Alpha (Krippendorff, 1980) using MASI, a measure of agreement on setvalued items, as the distance function (Passonneau, 2006). MASI accounts for partial agreement if the label sets of both annotators overlap in at least one label. We achieved an Alpha score of α = .75. According to Krippendorff, datasets with this score are considered reliable and allow tentative conclusions to be drawn. The CO label showed the lowest agreement of only κ = .18. The label was supposed to cover any criticism that is not covered by a dedicated label. However, the annotators reported that they chose this label when they were unsure whether a particular criticism label would fit a certain turn or not. Labels in the interpersonal category"
E12-1079,zesch-etal-2008-extracting,1,0.729287,"Missing"
E14-1008,E09-1005,0,0.381414,"Missing"
E14-1008,P98-1013,0,0.301047,"le LLR for automatically creating sense annotated data and (ii) we perform meaningful intrinsic and application-based evaluations of our method on large sense annotated datasets. LLRs are the result of integrating several lexical-semantic resources by linking them at the word sense level. Examples of large LLRs are the multilingual BabelNet (Navigli and Ponzetto, 2012), an integration of wordnets and Wikipedia3 , or UBY, (Gurevych et al., 2012), the resource we employ in our work here. UBY is an integration of multiple resources, such as wordnets, Wikipedia, Wiktionary (WKT)4 , FrameNet (FN; (Baker et al., 1998)) and VerbNet (VN; (Kipper et al., 2008)) for English and German. A distinguishing feature of LLRs is the enriched sense representation for word senses that are interlinked since different resources provide different, often complementary information. Annotating corpora with such enriched sense representations turns them into versatile training data for statistical systems. Our first contribution (i) also addresses a considerable gap in recent research regarding automated sense labelling of verbs. Most previous work is done on nouns. However, verbs pose a bigger challenge due to their high poly"
E14-1008,I11-1076,0,0.0177588,"a. 74 various types of corpora, including web corpora. They employ a Lesk-based algorithm to automatically annotate the target word instances in the extracted example sentences with WN senses and use them in one of their experiments as training data for a WSD classifier. However, the performance of the system decreased significantly achieving the lowest accuracy among all system configurations. The authors provide only the overall accuracy score, so we do not know how disambiguation of verbs was affected. small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Summary. We consider the ability to establish a link between the rich knowledge available in LLRs and c"
E14-1008,W10-4001,0,0.0396516,"Missing"
E14-1008,E12-1059,1,0.859186,"tically identify instances of word senses in text corpora. We significantly extend previous work on this task by making two important contributions: (i) we employ a large-scale LLR for automatically creating sense annotated data and (ii) we perform meaningful intrinsic and application-based evaluations of our method on large sense annotated datasets. LLRs are the result of integrating several lexical-semantic resources by linking them at the word sense level. Examples of large LLRs are the multilingual BabelNet (Navigli and Ponzetto, 2012), an integration of wordnets and Wikipedia3 , or UBY, (Gurevych et al., 2012), the resource we employ in our work here. UBY is an integration of multiple resources, such as wordnets, Wikipedia, Wiktionary (WKT)4 , FrameNet (FN; (Baker et al., 1998)) and VerbNet (VN; (Kipper et al., 2008)) for English and German. A distinguishing feature of LLRs is the enriched sense representation for word senses that are interlinked since different resources provide different, often complementary information. Annotating corpora with such enriched sense representations turns them into versatile training data for statistical systems. Our first contribution (i) also addresses a considera"
E14-1008,C08-1021,0,0.0151083,"to other languages where (linked) lexical resources are available. Acquisition of sense-annotated data. Most previous work on using lexical resources for automatically acquiring sense-annotated data either was mostly restricted to noun senses or, unlike us, did not present a meaningful evaluation. Leacock et al. (1998) describe the automated creation of training data for supervised WSD on the basis of WN as a lexical resource combined with corpus statistics, but they evaluate their approach just on one noun, verb, and adjective, and thus it is unclear whether their results can be generalized. Cuadros and Rigau (2008) used the approach of Leacock et al. (1998) to automatically build a large KnowNet from the Web, but they evaluated this resource only for WSD of nouns. However, the system based on KnowNet yields results below the SemCor-MFS baseline. Mihalcea and Moldovan (1999) use WordNet glosses to extract sense examples from the Web via a search engine and use this approach in a subsequent paper (Mihalcea, 2002) to generate a sense tagged corpus. For five randomly selected nouns, they performed a comparative evaluation of a WSD classifier trained on an automatically tagged corpus on the one hand, and on"
E14-1008,P03-1054,0,0.00496335,"on dependency parsing. While this might seem to be in contrast to our labelling algorithm which is based on shallow linguistic preprocessing, it is fully justified by the purpose of our extrinsic evaluation: The main purpose of the extrinsic evaluation is not to outperform state-of-the-art VSD systems, but to show that, when operating with reasonable features, a classifier trained on the data automatically labelled with our method performs equally well as when this classifier is trained on manually annotated data. 4.2.1 Features The training and test data are parsed with the Stanford parser (Klein and Manning, 2003) which provides Stanford Dependencies output (De Marneffe et al., 2006) as well as phrase structure trees. We employ the Stanford Named Entity Recogniser to identify named entities. We then extract lexical, syntactic, and semantic features from the parse results for classification. Lexical features include the lemmas and POS tags of the two words before and after the target verb. To extract syntactic features we select all dependency relations from the parser output in 72 SemCor instance SP derived from SemCor score WN sense ID (gold sense in brackets) Some of the New York Philharmonic musicia"
E14-1008,de-marneffe-etal-2006-generating,0,0.0945384,"Missing"
E14-1008,W03-0428,0,0.0351378,"verbs, e.g. POS tags for personal pronouns which are potential verb arguments. In our experiments, we tried different sets of function words and POS tags. For instance, we found that some function words (e.g., reflexive pronouns) and some POS tags (e.g., those for past participles and comparative adjectives) introduced too much noise in the data and therefore we did not select them for the final vocabulary.8 In order to create SPs from sense examples, we apply POS tagging and lemmatisation using the TreeTagger (Schmid, 1994) and named entity tagging using the Stanford Named Entity Recogniser (Klein et al., 2003). The named entity tags attached by the Named Entity Recogniser are mapped to WN semantic fields. For the generation of ASPs from sense examples, we used a window size of w = 7, while the generation of LSPs has been performed with w = 5 in order to put a focus on the closely neighbouring lexemes in multiword verb lemmas. The window size was set empirically using the English Lexical Sample task of the Senseval-2 dataset as a development set. The same set was also used for the development of the linguistically motivated vocabulary for ASPs.9 From the abstract predicate-argument structure informa"
E14-1008,N10-1088,0,0.195765,"ubsequent paper (Mihalcea, 2002) to generate a sense tagged corpus. For five randomly selected nouns, they performed a comparative evaluation of a WSD classifier trained on an automatically tagged corpus on the one hand, and on the manually annotated data from the Senseval-2 English lexical sample task on the other hand. The results obtained for these five nouns seem to be similar but the dataset used is too small to draw meaningful conclusions and moreover, it does not cover verbs. Mostow and Duan (2011) presented a system that extracts example contexts for nouns and apply these contexts in (Duan and Yates, 2010) for WSD by using them to label text and train a statistical classifier. An evaluation of this classifier yielded results similar to those obtained by a supervised WSD system. 6 Conclusion In this paper, we presented a novel method for creating sense labelled corpora automatically. We exploit LLRs and perform large-scale intrinsic and application-based evaluations. The results of those evaluations show that the quality of the sense labelled corpora created with our method matches that of manually annotated corpora. In future research, we plan to use PropBank (Palmer et al., 2005) in order to e"
E14-1008,E12-1056,1,0.894445,"Missing"
E14-1008,R09-1037,0,0.514654,"Missing"
E14-1008,J98-1006,0,0.458249,"to be the main advantage of our automated labelling method. However, to automatically label a suffcient amount of data for supervised learning, very large corpora are required. Our method can be extended to other POS (using sense examples and possibly other types of lexical information), as well as to other languages where (linked) lexical resources are available. Acquisition of sense-annotated data. Most previous work on using lexical resources for automatically acquiring sense-annotated data either was mostly restricted to noun senses or, unlike us, did not present a meaningful evaluation. Leacock et al. (1998) describe the automated creation of training data for supervised WSD on the basis of WN as a lexical resource combined with corpus statistics, but they evaluate their approach just on one noun, verb, and adjective, and thus it is unclear whether their results can be generalized. Cuadros and Rigau (2008) used the approach of Leacock et al. (1998) to automatically build a large KnowNet from the Web, but they evaluated this resource only for WSD of nouns. However, the system based on KnowNet yields results below the SemCor-MFS baseline. Mihalcea and Moldovan (1999) use WordNet glosses to extract"
E14-1008,eckle-kohler-etal-2012-uby,1,0.898521,"Section 3 describes the data used in the experiments. Section 4 presents the results of the evaluations. Section 5 analyses in detail the differences between our method and previous work. Section 6 concludes the paper. 2 2.2 For the creation of SPs, we employ the large-scale LLR UBY which combines 10 lexical resources for English and German to make use of the enriched verb sense representations provided by the sense links between various resources available in UBY. Although our method can work with any LLR, we choose UBY because the various resources are represented in a standardised format (Eckle-Kohler et al., 2012) and sense links between them can uniformly and conveniently be accessed via the freely available UBY-API.5 Since we evaluate our method on data annotated with WN senses, we create SPs for enriched WN senses (see example given in Table 1). We enrich WN senses by aggregating lexical information that can be accessed through links given in UBY to corresponding verb senses in other resources. In this setting, enrichment means that we make use of sense examples from WN, from FN via the WN–FN linking, and from WKT via the WN–WKT linking. In addition, we use abstract predicate-argument structure info"
E14-1008,J05-1004,0,0.250348,"contexts in (Duan and Yates, 2010) for WSD by using them to label text and train a statistical classifier. An evaluation of this classifier yielded results similar to those obtained by a supervised WSD system. 6 Conclusion In this paper, we presented a novel method for creating sense labelled corpora automatically. We exploit LLRs and perform large-scale intrinsic and application-based evaluations. The results of those evaluations show that the quality of the sense labelled corpora created with our method matches that of manually annotated corpora. In future research, we plan to use PropBank (Palmer et al., 2005) in order to extract sense examples for VN as well. This might improve the performance of lexical resource combinations which include VN. We will also apply our method to languages (e.g., German) for which lexical resources are available but no or little sense annotated corpora exist. Acknowledgments This work has been supported by the Volkswagen Foundation as part of the Lichtenberg- Professorship Program under grant No. I/82806 and by the German Research Foundation under grant No. GU 798/9-1. We would like to thank the anonymous reviewers for their valuable feedback. K¨ubler and Zhekova (200"
E14-1008,J93-2004,0,0.0449901,"cation Table 1: Examples of SPs derived from an enriched WN sense in UBY. PP, JJ, and VV are POS tags from the Penn Treebank tagset, standing for personal pronoun, adjective and full verb. verbal multiword expressions in a corpus, whereas ASPs are necessary to identify productively used verb senses that are constrained in their use only by their syntactic behaviour and particular semantic properties, such as selectional preferences on their arguments. The fixed vocabulary used for the creation of ASPs consists of (i) the target verb lemma, (ii) selected POS tags from the Penn Treebank Tagset (Marcus et al., 1993), (iii) a list of particular function words that play an important role in finegrained subcategorisation frames of verbs (EckleKohler and Gurevych, 2012) and (iv) semantic categories of nouns given by WN semantic fields. We selected POS tags that play an important role in syntactic realisations of verbs, e.g. POS tags for personal pronouns which are potential verb arguments. In our experiments, we tried different sets of function words and POS tags. For instance, we found that some function words (e.g., reflexive pronouns) and some POS tags (e.g., those for past participles and comparative adj"
E14-1008,P10-1154,0,0.111771,"Missing"
E14-1008,mihalcea-2002-bootstrapping,0,0.0407329,"lexical resource combined with corpus statistics, but they evaluate their approach just on one noun, verb, and adjective, and thus it is unclear whether their results can be generalized. Cuadros and Rigau (2008) used the approach of Leacock et al. (1998) to automatically build a large KnowNet from the Web, but they evaluated this resource only for WSD of nouns. However, the system based on KnowNet yields results below the SemCor-MFS baseline. Mihalcea and Moldovan (1999) use WordNet glosses to extract sense examples from the Web via a search engine and use this approach in a subsequent paper (Mihalcea, 2002) to generate a sense tagged corpus. For five randomly selected nouns, they performed a comparative evaluation of a WSD classifier trained on an automatically tagged corpus on the one hand, and on the manually annotated data from the Senseval-2 English lexical sample task on the other hand. The results obtained for these five nouns seem to be similar but the dataset used is too small to draw meaningful conclusions and moreover, it does not cover verbs. Mostow and Duan (2011) presented a system that extracts example contexts for nouns and apply these contexts in (Duan and Yates, 2010) for WSD by"
E14-1008,D12-1042,0,0.0133242,"ignificantly achieving the lowest accuracy among all system configurations. The authors provide only the overall accuracy score, so we do not know how disambiguation of verbs was affected. small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Summary. We consider the ability to establish a link between the rich knowledge available in LLRs and corpora of any kind to be the main advantage of our automated labelling method. However, to automatically label a suffcient amount of data for supervised learning, very large corpora are required. Our method can be extended to other POS (using sense examples and possibly other types of lexical information), as well as t"
E14-1008,W04-2405,0,0.00968327,"se-annotated data. 74 various types of corpora, including web corpora. They employ a Lesk-based algorithm to automatically annotate the target word instances in the extracted example sentences with WN senses and use them in one of their experiments as training data for a WSD classifier. However, the performance of the system decreased significantly achieving the lowest accuracy among all system configurations. The authors provide only the overall accuracy score, so we do not know how disambiguation of verbs was affected. small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Summary. We consider the ability to establish a link between the rich knowled"
E14-1008,C12-1109,1,0.922331,"Missing"
E14-1008,P95-1026,0,0.668595,"quisition of sense-annotated data. 74 various types of corpora, including web corpora. They employ a Lesk-based algorithm to automatically annotate the target word instances in the extracted example sentences with WN senses and use them in one of their experiments as training data for a WSD classifier. However, the performance of the system decreased significantly achieving the lowest accuracy among all system configurations. The authors provide only the overall accuracy score, so we do not know how disambiguation of verbs was affected. small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Summary. We consider the ability to establish a link between"
E14-1008,zesch-etal-2008-extracting,1,0.571015,"Missing"
E14-1008,P09-1113,0,0.0543924,"e system decreased significantly achieving the lowest accuracy among all system configurations. The authors provide only the overall accuracy score, so we do not know how disambiguation of verbs was affected. small number of seed instances to do bootstrap learning (Yarowsky, 1995; Mihalcea, 2004; Fujita and Fujino, 2011), in distant supervision training data are created in a single run from scratch by aligning corpus instances with entries in a knowledge base. Distant supervision methods that have used LLRs as knowledge bases have been previously applied in relation extraction, e.g. Freebase (Mintz et al., 2009; Surdeanu et al., 2012) and BabelNet (Krause et al., 2012; Moro et al., 2013). However, as far as we are aware, we are the first to apply distant supervision to the task of verb sense disambiguation. Summary. We consider the ability to establish a link between the rich knowledge available in LLRs and corpora of any kind to be the main advantage of our automated labelling method. However, to automatically label a suffcient amount of data for supervised learning, very large corpora are required. Our method can be extended to other POS (using sense examples and possibly other types of lexical in"
E14-1008,W11-1413,0,0.0129397,"999) use WordNet glosses to extract sense examples from the Web via a search engine and use this approach in a subsequent paper (Mihalcea, 2002) to generate a sense tagged corpus. For five randomly selected nouns, they performed a comparative evaluation of a WSD classifier trained on an automatically tagged corpus on the one hand, and on the manually annotated data from the Senseval-2 English lexical sample task on the other hand. The results obtained for these five nouns seem to be similar but the dataset used is too small to draw meaningful conclusions and moreover, it does not cover verbs. Mostow and Duan (2011) presented a system that extracts example contexts for nouns and apply these contexts in (Duan and Yates, 2010) for WSD by using them to label text and train a statistical classifier. An evaluation of this classifier yielded results similar to those obtained by a supervised WSD system. 6 Conclusion In this paper, we presented a novel method for creating sense labelled corpora automatically. We exploit LLRs and perform large-scale intrinsic and application-based evaluations. The results of those evaluations show that the quality of the sense labelled corpora created with our method matches that"
E14-1008,C98-1013,0,\N,Missing
E17-1045,W09-2420,0,0.0633576,"Missing"
E17-1045,S07-1018,0,0.305745,"Missing"
E17-1045,D14-1159,0,0.0249343,"Missing"
E17-1045,W06-1615,0,0.129013,"general domain adaptation methods for NLP. This section briefly introduces some of the relevant approaches in these areas, and then summarizes the state-of-the-art in FrameNet frame identification. Domain adaptation in NLP Low out-ofdomain performance is a problem common to many supervised machine learning tasks. The goal of domain adaptation is to improve model performance on the test data originating from a different distribution than the training data (Søgaard, 2013). For NLP, domain adaptation has been studied for various tasks such as POS-tagging and syntactic parsing (Daum´e III, 2007; Blitzer et al., 2006). For the complex task of SRL, it is strongly associated with PropBank, because the corresponding CoNLL shared tasks promote out-of-domain evaluation (Surdeanu et al., 2008; Hajiˇc et al., 2009). In the shared tasks, in-domain newspaper text from the WSJ Corpus is contrasted to out-of-domain data from fiction texts in the Brown Corpus. Most of the participants in the shared tasks do not consider domain adaptation and report systematically lower scores for the out-of-domain data (Hajiˇc et al., 2009). Representation learning has been successfully used to improve on the CoNLL shared task results"
E17-1045,W05-0620,0,0.143665,"Missing"
E17-1045,P10-1025,0,0.0179276,"istically well-formed. The FrameNet test setup thus cannot provide information on SRL performance on less edited out-ofdomain data, e.g. user-generated web data. There are few studies related to the out-ofdomain generalization of FrameNet SRL. Johansson and Nugues (2008) evaluate the impact of different parsers on FrameNet SRL using the Nuclear Threats Initiative (NTI) data as an out-of-domain test set. They observe low domain generalization abilities of their supervised system, but find that using dependency parsers instead of constituency parsers is beneficial in the out-of-domain scenario. Croce et al. (2010) use a similar in-domain/out-ofdomain split to evaluate their approach to opendomain FrameNet SRL. They integrate a distributional model into their SRL system to generalize lexicalized features to previously unseen arguments and thus create an SRL system with a smaller performance gap between in-domain and out-ofdomain test data (only 4.5 percentage points F1 ). 472 Note that they only evaluate the role labeling step. It is not transparent how their results would transfer to the current state-of-the-art SRL systems that already integrate methods to improve generalization, for instance using di"
E17-1045,P11-1144,0,0.53655,"l., 2015). Yang et al. (2015) report the smallest performance difference (5.5 points in F1 ) between in-domain and out-of-domain test data, leading to the best results to date on the CoNLL 2009 out-of-domain test. Their system learns common representations for in-domain and out-of-domain data based on deep belief networks. Domain dependence of FrameNet SRL The FrameNet 1.5 fulltext corpus, used as a standard dataset for training and evaluating FrameNet SRL systems, contains texts from several domains (Ruppenhofer et al., 2010). However, the standard data split used to evaluate modern systems (Das and Smith, 2011) ensures the presence of all domains in the training as well as test data and cannot be used to assess the systems’ ability to generalize. Moreover, all the texts in the FrameNet fulltext corpus, based on newspaper and literary texts, are post-edited and linguistically well-formed. The FrameNet test setup thus cannot provide information on SRL performance on less edited out-ofdomain data, e.g. user-generated web data. There are few studies related to the out-ofdomain generalization of FrameNet SRL. Johansson and Nugues (2008) evaluate the impact of different parsers on FrameNet SRL using the N"
E17-1045,J14-1002,0,0.597471,"data for research purposes.1 1 [The mill]Grinding cause grindsGrinding malt]P atient [to grist]Result . Introduction Domain dependence is a major problem for supervised NLP tasks such as FrameNet semantic role labeling (SRL): systems generally exhibit a strong performance drop when applied to test data from a different distribution than the training data. This prohibits their large-scale use in language technology applications. The same problems are expected for FrameNet SRL, but due to a lack of datasets, state-of-theart FrameNet SRL is only evaluated on a single in-domain test set, see e.g. Das et al. (2014) and FitzGerald et al. (2015). In this work, we present the first comprehensive study of the domain dependence of FrameNet SRL 1 www.ukp.tu-darmstadt.de/ood-fn-srl [the FrameNet SRL consists of two steps, frame identification (frameId), assigning a frame to the current predicate, and role labeling (roleId), identifying the participants and assigning them role labels licensed by the frame. The frameId step reduces the hundreds of role labels in FrameNet to a manageable set of up to 30 roles. Thus, FrameNet SRL differs from PropBank SRL (Carreras and M`arquez, 2005), that only uses a small set o"
E17-1045,P07-1033,0,0.221407,"Missing"
E17-1045,erk-pado-2006-shalmaneser,0,0.0953555,"Missing"
E17-1045,D15-1112,0,0.142302,"Missing"
E17-1045,P14-1136,0,0.51994,"ther out-of-domain test sets, to perform a detailed analysis of the domain dependence of FrameNet SRL using Semafor (Das et al., 2014; Kshirsagar et al., 2015) to identify which of the stages of FrameNet SRL, frameId or roleId, is particularly sensitive to domain shifts. Our results confirm that the major bottleneck in FrameNet SRL is the frame identification step. Motivated by that, we develop a simple, yet efficient frame identification method based on distributed word representations that promise better domain generalization. Our system’s performance matches the state-of-the-art in-domain (Hermann et al., 2014), despite using a simpler model, and improves on the out-of-domain performance of Semafor. The contributions of the present work are twofold: 1) we perform the first comprehensive study of the domain generalization capabilities of opensource FrameNet SRL, and 2) we propose a new frame identification method based on distributed word representations that enhances out-of-domain performance of frame identification. To enable our study, we created YAGS, a new, substantially-sized benchmark dataset for the out-of-domain testing of FrameNet SRL; we publish the annotations for the YAGS benchmark set a"
E17-1045,P10-1099,0,0.0266746,"For the complex task of SRL, it is strongly associated with PropBank, because the corresponding CoNLL shared tasks promote out-of-domain evaluation (Surdeanu et al., 2008; Hajiˇc et al., 2009). In the shared tasks, in-domain newspaper text from the WSJ Corpus is contrasted to out-of-domain data from fiction texts in the Brown Corpus. Most of the participants in the shared tasks do not consider domain adaptation and report systematically lower scores for the out-of-domain data (Hajiˇc et al., 2009). Representation learning has been successfully used to improve on the CoNLL shared task results (Huang and Yates, 2010; FitzGerald et al., 2015; Yang et al., 2015). Yang et al. (2015) report the smallest performance difference (5.5 points in F1 ) between in-domain and out-of-domain test data, leading to the best results to date on the CoNLL 2009 out-of-domain test. Their system learns common representations for in-domain and out-of-domain data based on deep belief networks. Domain dependence of FrameNet SRL The FrameNet 1.5 fulltext corpus, used as a standard dataset for training and evaluating FrameNet SRL systems, contains texts from several domains (Ruppenhofer et al., 2010). However, the standard data spl"
E17-1045,P16-1085,0,0.031148,"Missing"
E17-1045,D15-1245,0,0.0992766,"Missing"
E17-1045,C08-1050,0,0.0675409,"., 2010). However, the standard data split used to evaluate modern systems (Das and Smith, 2011) ensures the presence of all domains in the training as well as test data and cannot be used to assess the systems’ ability to generalize. Moreover, all the texts in the FrameNet fulltext corpus, based on newspaper and literary texts, are post-edited and linguistically well-formed. The FrameNet test setup thus cannot provide information on SRL performance on less edited out-ofdomain data, e.g. user-generated web data. There are few studies related to the out-ofdomain generalization of FrameNet SRL. Johansson and Nugues (2008) evaluate the impact of different parsers on FrameNet SRL using the Nuclear Threats Initiative (NTI) data as an out-of-domain test set. They observe low domain generalization abilities of their supervised system, but find that using dependency parsers instead of constituency parsers is beneficial in the out-of-domain scenario. Croce et al. (2010) use a similar in-domain/out-ofdomain split to evaluate their approach to opendomain FrameNet SRL. They integrate a distributional model into their SRL system to generalize lexicalized features to previously unseen arguments and thus create an SRL syst"
E17-1045,P15-2036,0,0.290519,"s problem as follows: we introduce a new benchmark dataset YAGS 471 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 471–482, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics (Yahoo! Answers Gold Standard), which is based on user-generated questions and answers and exemplifies an out-of-domain application use case. We use YAGS, along with other out-of-domain test sets, to perform a detailed analysis of the domain dependence of FrameNet SRL using Semafor (Das et al., 2014; Kshirsagar et al., 2015) to identify which of the stages of FrameNet SRL, frameId or roleId, is particularly sensitive to domain shifts. Our results confirm that the major bottleneck in FrameNet SRL is the frame identification step. Motivated by that, we develop a simple, yet efficient frame identification method based on distributed word representations that promise better domain generalization. Our system’s performance matches the state-of-the-art in-domain (Hermann et al., 2014), despite using a simpler model, and improves on the out-of-domain performance of Semafor. The contributions of the present work are twofo"
E17-1045,P14-2050,0,0.0143134,"ning data. The LexiconBaseline calculates overall frame counts first (i.e. how often a frame appears in the training data in general), and, given the predicate, selects the overall most frequent frame among the ones available for this predicate. We expect this baseline to better handle the cases when limited data is available for a given predicate sense. Experiments In our experiments, we generate the lexicon L in the same way as in Hermann-14, by scanning the “frames” folder of the FrameNet 1.5 distribution. For the external vector space model vsm we use dependency-based word embeddings from Levy and Goldberg (2014). 3 In our implementation, we use the LightFM package (Kula, 2015) with the WARP option for hybrid matrix factorization. 4 A justification for this can also be found in Hermann et al. (2014): the difference in Hermann-14 accuracy when switching from the Semafor lexicon to the full lexicon is comparable to the difference between Semafor and Hermann-14 when evaluated on the same lexicon. 477 In-domain performance We report the performance of our system in the in-domain setting to compare to the state-of-the-art results from Hermann-14.5 We train our system on das-train and test it on das-test us"
E17-1045,K16-1006,0,0.0134207,"ults are currently lower. Manually or automatically increasing both predicate and predicate-frame association coverage of the FrameNet lexicon could help, and we suggest investigating this line of research in future work. While our method achieves state-of-the-art results on out-of-domain data, overall results are still significantly lower than the human performance observed for YAGS and TW, which shows that there is large room for improvement. Some further benefits could be gained from combining the WSABIE and NN-based classification, using advanced context representations, e.g. context2vec (Melamud et al., 2016) and incorporating syntactic information into the model. The out-of-domain performance could be further improved by adapting word representations to a new domain. A direct comparison to the Hermann-14 system in the out-of-domain setup would shed some more light on the properties of the task affecting the out-of-domain performance. On the one hand, we expect Hermann-14 to perform worse due to its heavy reliance on syntactic information, which might decline in quality when moved to a new domain; on the other hand, the WSABIE-based classification might smoothen this effect. We make our dataset pu"
E17-1045,C10-2107,0,0.284367,"Missing"
E17-1045,passonneau-etal-2012-masc,0,0.0638585,"Missing"
E17-1045,D14-1162,0,0.0840069,"Missing"
E17-1045,Q15-1032,0,0.218757,"Missing"
E17-1045,W08-2121,0,0.0565499,"Missing"
E17-1045,J11-2003,0,0.0386101,"Missing"
E17-1045,N15-1035,0,0.0539029,"Missing"
E17-1045,Q15-1020,0,0.0613311,"ciated with PropBank, because the corresponding CoNLL shared tasks promote out-of-domain evaluation (Surdeanu et al., 2008; Hajiˇc et al., 2009). In the shared tasks, in-domain newspaper text from the WSJ Corpus is contrasted to out-of-domain data from fiction texts in the Brown Corpus. Most of the participants in the shared tasks do not consider domain adaptation and report systematically lower scores for the out-of-domain data (Hajiˇc et al., 2009). Representation learning has been successfully used to improve on the CoNLL shared task results (Huang and Yates, 2010; FitzGerald et al., 2015; Yang et al., 2015). Yang et al. (2015) report the smallest performance difference (5.5 points in F1 ) between in-domain and out-of-domain test data, leading to the best results to date on the CoNLL 2009 out-of-domain test. Their system learns common representations for in-domain and out-of-domain data based on deep belief networks. Domain dependence of FrameNet SRL The FrameNet 1.5 fulltext corpus, used as a standard dataset for training and evaluating FrameNet SRL systems, contains texts from several domains (Ruppenhofer et al., 2010). However, the standard data split used to evaluate modern systems (Das and S"
E17-1045,P14-5016,1,0.899066,"Missing"
E17-1082,H92-1046,0,0.725172,"te, they have the advantage of being more flexible and more adaptable to new languages and domains. For knowledge-based methods, this has been especially true since the advent of large, multilingual, collaboratively constructed resources such as Wikipedia and Wiktionary (Zesch et al., 2008). In this paper, we present two novel approaches to lexical substitution which are knowledge-based, generally language-independent, and use a combination of traditional wordnets and Wiktionary. The first approach uses simulated annealing (Kirkpatrick et al., 1983), which was first proposed for use in WSD by Cowie et al. (1992) but has attracted relatively little attention since then. The second approach uses D-Bees (Abualhaija and Zimmermann, 2016), a relatively new, biologically inspired disambiguation algorithm that models swarm intelligence. Both algorithms are metaheuristic (Talbi, 2009) in that they treat WSD as an optimization problem and modify heuristic (approximate) solu870 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 870–880, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics tions"
E17-1082,S07-1029,0,0.291522,"t task uses a sample of 201 target words (nouns, verbs, adjectives and adverbs); for each word, ten context sentences are selected from the English Internet Corpus (Sharoff, 2006). Five human annotators provided up to three substitutes for each target. The dataset is split into a training set (300 sentences) and a test set (1710 sentences). McCarthy and Navigli (2009) provide results for the aforementioned “top-ranked synonyms” algorithm as a lower bound on performance. State-ofthe-art performance across the nine evaluation metrics is represented by the top-performing systems at SemEval-2007 (Giuliano et al., 2007; Hassan et al., 2007; Yuret, 2007; Zhao et al., 2007) and by several later systems (Biemann and Riedl, 2013; Melamud et al., 2015).4 Of these systems, only Yuret (2007) is supervised. 5.2 Results and Analysis Table 2 shows the results for the state-of-the-art and na¨ıve baselines, along with results of our two basic systems and, as before, an enhanced version 4 We are aware of several further lexical substitution sys´ S´eaghdha and Korhonen tems (Moon and Erk (2013), O (2014), Roller and Erk (2016), Sinha and Mihalcea (2011), Szarvas et al. (2013b), and Thater et al. (2010) as reimplemented b"
E17-1082,W97-0802,0,0.511297,"antic similarity, we use a variant of the adapted Lesk algorithm (Banerjee and Pedersen, 2002). For each sense, we build a textual representation by concatenating its gloss with those of its hyper- and hyponyms. We then calculate the lexical overlap between the two texts. 3.3 Substitute Generation Once the target word is disambiguated with respect to a particular sense inventory, we generate an unordered list of substitutes (to be subsequently ordered by the ranking module). The sense inventory we use for disambiguation is WordNet 3.1 (Fellbaum, 1998) for our English tasks, and GermaNet 10.0 (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) for the German one. These are expert-built resources in which words representing the same concept are grouped together into synsets; synsets are in turn linked into a network by semantic relations such as hypernymy and meronymy. In preliminary experiments on generating substitutes, we varied two independent parameters: which lexical-semantic resources to use as the source of substitutes, and which semantic relations to follow from the disambiguated synset. With respect to the first parameter, we tried drawing substitutes from the disambiguation inventory (WordNet"
E17-1082,S07-1091,0,0.419963,"201 target words (nouns, verbs, adjectives and adverbs); for each word, ten context sentences are selected from the English Internet Corpus (Sharoff, 2006). Five human annotators provided up to three substitutes for each target. The dataset is split into a training set (300 sentences) and a test set (1710 sentences). McCarthy and Navigli (2009) provide results for the aforementioned “top-ranked synonyms” algorithm as a lower bound on performance. State-ofthe-art performance across the nine evaluation metrics is represented by the top-performing systems at SemEval-2007 (Giuliano et al., 2007; Hassan et al., 2007; Yuret, 2007; Zhao et al., 2007) and by several later systems (Biemann and Riedl, 2013; Melamud et al., 2015).4 Of these systems, only Yuret (2007) is supervised. 5.2 Results and Analysis Table 2 shows the results for the state-of-the-art and na¨ıve baselines, along with results of our two basic systems and, as before, an enhanced version 4 We are aware of several further lexical substitution sys´ S´eaghdha and Korhonen tems (Moon and Erk (2013), O (2014), Roller and Erk (2016), Sinha and Mihalcea (2011), Szarvas et al. (2013b), and Thater et al. (2010) as reimplemented by Kremer et al. (2014"
E17-1082,henrich-hinrichs-2010-gernedit,0,0.0300244,"Missing"
E17-1082,henrich-hinrichs-2012-comparative,0,0.0296697,"y. In preliminary experiments on generating substitutes, we varied two independent parameters: which lexical-semantic resources to use as the source of substitutes, and which semantic relations to follow from the disambiguated synset. With respect to the first parameter, we tried drawing substitutes from the disambiguation inventory (WordNet or GermaNet) alone, and also drawing additional substitutes from Wiktionary. Our use of Wiktionary as a complementary resource is motivated by Meyer and Gurevych (2012), who found its coverage to be complementary to those of expert-built resources, and by Henrich and Hinrichs (2012), who found that using information from both GermaNet and Wiktionary improved WSD performance. We used a relatively simple, Lesklike method for mapping senses from WordNet/ GermaNet to Wiktionary. For the second parameter, we tried one setup in which we took all synonyms found in the disambiguated synset and in its hypernyms, and one in which we additionally pulled in synonyms from the hyponyms and all other related synsets (except antonyms). The first setup was informed by the annotation guidelines of the lexical substitution datasets, which indicate that it is permissible to suggest substitu"
E17-1082,S12-1066,0,0.12637,"stitutes that do not also appear in the goldstandard list. This puts us at somewhat of a disadvantage, since our substitute lists often contain only a subset of the gold-standard substitutes. It also makes use of the κ metric problematic, since κ expects the system and gold-standard lists to contain the same set of substitutes. We therefore report only TRnk and R@n scores. Specia et al. (2012) report scores for two lowerbound baselines: one puts the substitute lists in random order, and the other orders them by inverse frequency of occurrence in Web 1T.5 The state of the art is represented by Jauhar and Specia (2012), 876 5 A third baseline leaves the lists in their original order (i.e., by inverse number of annotators who chose them). We ignore it here as it relies entirely on manual labelling. Table 3: System performance on the SemEval-2012 lexical simplification dataset. System TRnk R@1 R@2 R@3 D-Bees (enhanced) (original ordering) D-Bees (enhanced) (unigram ordering) D-Bees (enhanced) (n-gram ordering) Jauhar and Specia (2012) 37.5 50.9 47.1 60.2 71.6 72.8 71.3 57.5 75.5 75.2 74.5 68.9 76.4 76.3 75.7 76.9 unigram ordering baseline random ordering baseline 58.5 34.0 55.9 32.1 68.1 61.2 76.0 82.5 a supe"
E17-1082,E14-1057,0,0.045519,"Missing"
E17-1082,W15-1501,0,0.0184557,"from the English Internet Corpus (Sharoff, 2006). Five human annotators provided up to three substitutes for each target. The dataset is split into a training set (300 sentences) and a test set (1710 sentences). McCarthy and Navigli (2009) provide results for the aforementioned “top-ranked synonyms” algorithm as a lower bound on performance. State-ofthe-art performance across the nine evaluation metrics is represented by the top-performing systems at SemEval-2007 (Giuliano et al., 2007; Hassan et al., 2007; Yuret, 2007; Zhao et al., 2007) and by several later systems (Biemann and Riedl, 2013; Melamud et al., 2015).4 Of these systems, only Yuret (2007) is supervised. 5.2 Results and Analysis Table 2 shows the results for the state-of-the-art and na¨ıve baselines, along with results of our two basic systems and, as before, an enhanced version 4 We are aware of several further lexical substitution sys´ S´eaghdha and Korhonen tems (Moon and Erk (2013), O (2014), Roller and Erk (2016), Sinha and Mihalcea (2011), Szarvas et al. (2013b), and Thater et al. (2010) as reimplemented by Kremer et al. (2014)), though they do not report results on the full SemEval-2007 test set, or else do not report any of the same"
E17-1082,L16-1134,1,0.912116,"r mapping senses from WordNet/ GermaNet to Wiktionary. For the second parameter, we tried one setup in which we took all synonyms found in the disambiguated synset and in its hypernyms, and one in which we additionally pulled in synonyms from the hyponyms and all other related synsets (except antonyms). The first setup was informed by the annotation guidelines of the lexical substitution datasets, which indicate that it is permissible to suggest substitute terms that are more generic but not more specific. The second setup was informed by the analyses of Kremer et al. (2014) and Miller et al. (2016), which found, contrarily, that other semantic relations, including hyponyms, were a fruitful source of substitutes. We obtained the best overall results when using both WordNet/GermaNet and Wiktionary, and when following semantic relations of all types (other than antonymy), to build the substitute list. We therefore used this setup for all our lexical substitution and simplification experiments. 3.4 Ranking The final step of lexical substitution is to rank the substitutes. Our method, like those employed in previous lexical substitution tasks, assumes that a substitute’s suitability depends"
E17-1082,J14-3005,0,0.0390253,"Missing"
E17-1082,N16-1131,0,0.0172394,"cross the nine evaluation metrics is represented by the top-performing systems at SemEval-2007 (Giuliano et al., 2007; Hassan et al., 2007; Yuret, 2007; Zhao et al., 2007) and by several later systems (Biemann and Riedl, 2013; Melamud et al., 2015).4 Of these systems, only Yuret (2007) is supervised. 5.2 Results and Analysis Table 2 shows the results for the state-of-the-art and na¨ıve baselines, along with results of our two basic systems and, as before, an enhanced version 4 We are aware of several further lexical substitution sys´ S´eaghdha and Korhonen tems (Moon and Erk (2013), O (2014), Roller and Erk (2016), Sinha and Mihalcea (2011), Szarvas et al. (2013b), and Thater et al. (2010) as reimplemented by Kremer et al. (2014)), though they do not report results on the full SemEval-2007 test set, or else do not report any of the same metrics we do, or else are concerned only with ranking but not generating substitutes. 875 of the D-Bees system. Our systems’ performance is generally much lower here than on the Germanlanguage data, with D-Bees failing to exceed the state of the art. As with our German experiments, we tried modifying the D-Bees–based system to work around the language-specific problems"
E17-1082,S12-1046,0,0.10392,"eaders to the cited papers. Lexical simplification is a variant of lexical substitution in which the correct ranking is determined not just by the substitutes’ contextual fitness but also by their simplicity. (For example, rare words are generally considered to be more complex, as readers are less likely to be familiar with their meanings.) As with other types of text simplification, lexical simplification can be used to make complex texts understandable by a wider range of readers, such as children or second language learners. To date there has been one shared task in lexical simplification (Specia et al., 2012). Its main evaluation metric is based on Cohen’s (1960) κ. Two post-hoc evaluation metrics are also used. The first, top-ranked (TRnk), evaluates the simplest set of substitutes that is ranked first by the system, compared with the top-ranked set of substitutes in the gold standard. This represents the intersection between the first substitute set found by the system with the first set in the gold standard. The intersection should include at least one substitute. The second metric, recall at n (R@n) is the ratio of candidates from the top n sets of substitutes to those in the gold standard, wh"
E17-1082,N13-1133,1,0.9014,"ynonyms of all possible senses of the target, as well as synonyms of closely related senses such as hypernyms, and then ranking these words by their frequency (either within the list itself or in a large corpus). We consider these two na¨ıve baselines as reasonable lower bounds. The more challenging baseline performance comes from the best-performing participating systems at GermEval 2015, which represent the state of the art in German-language lexical substitution. One of these systems (Hintz and Biemann, 2015) is a supervised, bottom-up approach inspired by previous English-language work by Szarvas et al. (2013a). It first retrieves a list of substitutes from various lexicons, then applies a maxent classifier to determine whether each substitute fits the context. The second system (Jackov, 2015) is based on techniques from machine translation. It first disambiguates the input text by mapping German words to concepts represented by WordNet synsets. It then produces and scores various parsing hypotheses, and selects the synonyms and hypernyms of the target in the best-scoring hypothesis. 4.2 Results and Analysis Table 1 shows the results of the baselines described above, along with those of our basic"
E17-1082,D13-1198,0,0.0342484,"Missing"
E17-1082,P10-1097,0,0.0829908,"Missing"
E17-1082,S07-1044,0,0.171205,"uns, verbs, adjectives and adverbs); for each word, ten context sentences are selected from the English Internet Corpus (Sharoff, 2006). Five human annotators provided up to three substitutes for each target. The dataset is split into a training set (300 sentences) and a test set (1710 sentences). McCarthy and Navigli (2009) provide results for the aforementioned “top-ranked synonyms” algorithm as a lower bound on performance. State-ofthe-art performance across the nine evaluation metrics is represented by the top-performing systems at SemEval-2007 (Giuliano et al., 2007; Hassan et al., 2007; Yuret, 2007; Zhao et al., 2007) and by several later systems (Biemann and Riedl, 2013; Melamud et al., 2015).4 Of these systems, only Yuret (2007) is supervised. 5.2 Results and Analysis Table 2 shows the results for the state-of-the-art and na¨ıve baselines, along with results of our two basic systems and, as before, an enhanced version 4 We are aware of several further lexical substitution sys´ S´eaghdha and Korhonen tems (Moon and Erk (2013), O (2014), Roller and Erk (2016), Sinha and Mihalcea (2011), Szarvas et al. (2013b), and Thater et al. (2010) as reimplemented by Kremer et al. (2014)), though th"
E17-1082,zesch-etal-2008-extracting,1,0.770307,"Missing"
E17-1082,S07-1036,0,0.459767,"djectives and adverbs); for each word, ten context sentences are selected from the English Internet Corpus (Sharoff, 2006). Five human annotators provided up to three substitutes for each target. The dataset is split into a training set (300 sentences) and a test set (1710 sentences). McCarthy and Navigli (2009) provide results for the aforementioned “top-ranked synonyms” algorithm as a lower bound on performance. State-ofthe-art performance across the nine evaluation metrics is represented by the top-performing systems at SemEval-2007 (Giuliano et al., 2007; Hassan et al., 2007; Yuret, 2007; Zhao et al., 2007) and by several later systems (Biemann and Riedl, 2013; Melamud et al., 2015).4 Of these systems, only Yuret (2007) is supervised. 5.2 Results and Analysis Table 2 shows the results for the state-of-the-art and na¨ıve baselines, along with results of our two basic systems and, as before, an enhanced version 4 We are aware of several further lexical substitution sys´ S´eaghdha and Korhonen tems (Moon and Erk (2013), O (2014), Roller and Erk (2016), Sinha and Mihalcea (2011), Szarvas et al. (2013b), and Thater et al. (2010) as reimplemented by Kremer et al. (2014)), though they do not report res"
E17-1092,P14-5011,1,0.825574,", we use the following two baselines: First, we employ a majority baseline that classifies each argument as sufficient. Second, we use a support vector machine with polynomial kernel implemented in the Weka framework (Hall et al., 2009). We employ the 4,000 most frequent lowercased words as binary features and refer to this model as SVM-bow. 5.2 Manually Created Features (SVM) Our first system is based on manually created features. As a learner, we use the same support vector machine as for SVM-bow. For feature extraction and experimentation, we use the DKPro TC text classification framework (Daxenberger et al., 2014). We tried various features which have been used previously for assessing the quality or the persuasiveness of arguments (cf. Section 2). For instance, we experimented with argument structures (Stab and Gurevych, 2014), transitional phrases (Persing and Ng, 2015), semantic roles (Das et al., 2014) and discourse relations (Lin et al., 2014). However, we found that only the following features are effective for recognizing insufficiently supported arguments: Lexical: To capture lexical properties, we employ the 4,000 most frequent lowercased words as binary features analogous to SVM-bow. Length:"
E17-1092,W14-2109,0,0.030268,"4) recognize context-dependent claims and Rinott et al. (2015) retrieve several types of evidence from Wikipedia. Approaches for identifying the structure of arguments recognize argumentative relations between argument components using context-free grammars (MochalesPalau and Moens, 2009), pair classification (Stab and Gurevych, 2014), or maximum spanning trees (Peldszus and Stede, 2015). However, none of these approaches consider the quality of arguments. Similarly, most existing corpora in computational argumentation are only annotated with argument components (Habernal and Gurevych, 2016a; Aharoni et al., 2014; Mochales-Palau and Moens, 2009) or argument structures (Reed et al., 2008; Stab and Gurevych, 2014; Peldszus and Stede, 2015) and do not include annotations of argumentative quality issues. Other resources in the field contain arguments annotated with different properties such as emotions and sarcasm (Walker et al., 2012), the type of reasoning (Reed et al., The premise of this argument represents a particular example (second sentence) that supports a general claim in the first sentence. The argument is a generalization from one sample to the general case. However, a single sample is not eno"
E17-1092,D15-1267,1,0.90809,"Missing"
E17-1092,J08-4004,0,0.172372,"Missing"
E17-1092,P05-1045,0,0.0109646,"anation of how to handle modal verbs, the number of premises and undetermined qualifiers could further improve the agreement between the annotators in future annotation studies. 984 fold as a development set. For significance testing, we employ Wilcoxon signed-rank test on macro F1 scores with a significance level of α = .005. We employ several models from the DKPro Framework (Eckart de Castilho and Gurevych, 2014) for preprocessing. We use the language tool segmenter2 for tokenization and sentence splitting. We employ the Stanford parser (Klein and Manning, 2003) and named entity recognizer (Finkel et al., 2005) for constituency parsing and recognizing organizations, persons and locations. Note that only the model described in Section 5.2 requires all preprocessing steps. All other models use only the tokenization of the language tool segmenter. Syntax: For capturing syntactic properties, we extract binary production rules from the constituent parse trees of each sentence of the argument as described by Stab and Gurevych (2014). Named Entities (ner): We assume that arguments with insufficient support refer to particular entities in order to justify more general claims (cf. example 1 in Section 1). Th"
E17-1092,P12-2041,0,0.0300937,"eductive inference rules (Damer, 2009; van Eemeren et al., 1996). Informal logic aims at developing theoretical frameworks for analyzing arguments in ordinary natural language (Groarke, 2015). These include, for example, fallacy theories which focus on determining particular argumentative mistakes that can be observed with a marked degree of frequency. Current theories list various forms of fallacious arguments. For instance, the framework proposed by Damer (2009) describes 61 different fallacy Currently there are only few approaches that focus on the automatic assessment of argument quality. Cabrio and Villata (2012) employed textual entailment for identifying undisputed arguments in online discussions. They built a graph that represents attack and support relations between arguments and applied the abstract argumentation framework (Dung, 1995) for identifying accepted arguments. Although their approach is capable of finding undisputed arguments among a given set of arguments, it does not answer why a specific argument is of inferior quality than another argument. Thus, their approach is of limited use for guiding students since it does not pinpoint particular weaknesses of arguments. Park and Cardie (201"
E17-1092,N16-1166,0,0.0456398,"Missing"
E17-1092,P16-1150,1,0.926492,"limited use for argumentative writing support systems since they do not recognize the weak points of arguments. Despite the comprehensive theoretical framework on argument quality in logic and argumentation theory (van Eemeren et al., 1996; Damer, 2009), there are only few computational approaches that focus on the assessment of arguments in natural language texts. These existing approaches either identify undisputed arguments in online communities (Cabrio and Villata, 2012), assess the persuasiveness of arguments (Wei et al., 2016), compare and rank arguments regarding their convincingness (Habernal and Gurevych, 2016b), or summarize the argumentation strength In this paper, we propose a new task for assessing the quality of natural language arguments. The premises of a well-reasoned argument should provide enough evidence for accepting or rejecting its claim. Although this criterion, known as sufficiency, is widely adopted in argumentation theory, there are no empirical studies on its applicability to real arguments. In this work, we show that human annotators substantially agree on the sufficiency criterion and introduce a novel annotated corpus. Furthermore, we experiment with feature-rich SVMs and conv"
E17-1092,D14-1181,0,0.00253071,"at arguments with insufficient support refer to particular entities in order to justify more general claims (cf. example 1 in Section 1). Thus, we add the number of named entities appearing in the argument and the average occurrence of named entities per sentence to our feature set. We consider organizations, persons and locations separately. Thus the named entity features comprise six features in total, i.e. three binary and three numeric features. 5.1 Our second model is a convolutional neural network with max-over time pooling (Collobert et al., 2011). We use the implementation provided by Kim (2014). The selection of this model is motivated by the excellent performance that the model achieves in many different classification tasks like sentiment classification of question classification. We found in our experiments that instead of using several convolutional layers with different window sizes, a single convolutional layer with a window size of 2 and 250 feature maps performs best. For representing each word of an argument, we use word embeddings trained on the google news data set by Mikolov et al. (2013). In order to adapt these vectors to the identification of insufficient arguments, w"
E17-1092,W14-2105,0,0.0196667,"o and Villata (2012) employed textual entailment for identifying undisputed arguments in online discussions. They built a graph that represents attack and support relations between arguments and applied the abstract argumentation framework (Dung, 1995) for identifying accepted arguments. Although their approach is capable of finding undisputed arguments among a given set of arguments, it does not answer why a specific argument is of inferior quality than another argument. Thus, their approach is of limited use for guiding students since it does not pinpoint particular weaknesses of arguments. Park and Cardie (2014) proposed an approach for classifying propositions as verifiable (experiential and non-experiential) or unverifiable. Their best approach based on a support vector machine achieves a macro F1 score of .690. Although the verifiability of propositions enables to determine appropriate types of support, it does not answer if an argument is sufficiently supported or not. Persing and Ng (2015) introduced an approach for recognizing the argumentation strength of an essay. They found that pos n-grams, prompt adherence features, and predicted argument components perform best. However, their model deter"
E17-1092,P03-1054,0,0.057901,"Thus, extending the annotation guideline with an explanation of how to handle modal verbs, the number of premises and undetermined qualifiers could further improve the agreement between the annotators in future annotation studies. 984 fold as a development set. For significance testing, we employ Wilcoxon signed-rank test on macro F1 scores with a significance level of α = .005. We employ several models from the DKPro Framework (Eckart de Castilho and Gurevych, 2014) for preprocessing. We use the language tool segmenter2 for tokenization and sentence splitting. We employ the Stanford parser (Klein and Manning, 2003) and named entity recognizer (Finkel et al., 2005) for constituency parsing and recognizing organizations, persons and locations. Note that only the model described in Section 5.2 requires all preprocessing steps. All other models use only the tokenization of the language tool segmenter. Syntax: For capturing syntactic properties, we extract binary production rules from the constituent parse trees of each sentence of the argument as described by Stab and Gurevych (2014). Named Entities (ner): We assume that arguments with insufficient support refer to particular entities in order to justify mo"
E17-1092,D15-1110,0,0.0289078,"lassification of argument components into claims and premises (Mochales-Palau and Moens, 2009), supporting and opposing claims (Kwon et al., 2007), or backings, rebuttals and refutations (Habernal and Gurevych, 2016a). Levy et al. (2014) recognize context-dependent claims and Rinott et al. (2015) retrieve several types of evidence from Wikipedia. Approaches for identifying the structure of arguments recognize argumentative relations between argument components using context-free grammars (MochalesPalau and Moens, 2009), pair classification (Stab and Gurevych, 2014), or maximum spanning trees (Peldszus and Stede, 2015). However, none of these approaches consider the quality of arguments. Similarly, most existing corpora in computational argumentation are only annotated with argument components (Habernal and Gurevych, 2016a; Aharoni et al., 2014; Mochales-Palau and Moens, 2009) or argument structures (Reed et al., 2008; Stab and Gurevych, 2014; Peldszus and Stede, 2015) and do not include annotations of argumentative quality issues. Other resources in the field contain arguments annotated with different properties such as emotions and sarcasm (Walker et al., 2012), the type of reasoning (Reed et al., The pre"
E17-1092,P15-1053,0,0.37074,"man annotators agree on the sufficiency criterion. We present the results of an annotation study with three annotators and show that our annotation guideline successfully guides annotators to substantial agreement. Second, we show that insufficiently supported arguments can be identified with high accuracy using convolutional neural networks (CNN). The experimental results show that a CNN significantly outperforms several challenging baselines and manually created features. Third, we introduce a novel corpus for studying the quality of arguments. of an entire essay in a single holistic score (Persing and Ng, 2015). Our approach is based on the theoretical framework proposed by Johnson and Blair (2006). In particular, we focus on the sufficiency criterion that an argument fulfills if its premises provide enough evidence for accepting or rejecting the claim. The following example argument illustrates a violation of the sufficiency criterion: Example 1: “It is an undeniable fact that tourism harms the natural habitats of the destination countries. As Australia’s Great Barrier Reef has shown, the visitors cause immense destruction by breaking corals as souvenirs, throwing boat anchors or dropping fuel and"
E17-1092,reed-etal-2008-language,0,0.0606334,"l types of evidence from Wikipedia. Approaches for identifying the structure of arguments recognize argumentative relations between argument components using context-free grammars (MochalesPalau and Moens, 2009), pair classification (Stab and Gurevych, 2014), or maximum spanning trees (Peldszus and Stede, 2015). However, none of these approaches consider the quality of arguments. Similarly, most existing corpora in computational argumentation are only annotated with argument components (Habernal and Gurevych, 2016a; Aharoni et al., 2014; Mochales-Palau and Moens, 2009) or argument structures (Reed et al., 2008; Stab and Gurevych, 2014; Peldszus and Stede, 2015) and do not include annotations of argumentative quality issues. Other resources in the field contain arguments annotated with different properties such as emotions and sarcasm (Walker et al., 2012), the type of reasoning (Reed et al., The premise of this argument represents a particular example (second sentence) that supports a general claim in the first sentence. The argument is a generalization from one sample to the general case. However, a single sample is not enough to support the general case. Therefore, the argument does not comply wi"
E17-1092,C14-1141,0,0.0193465,"Related Work Previous works in computational argumentation focused primarily on approaches for argument mining. These include, for example, methods for the identification of arguments in legal texts (Moens et al., 2007), news articles (EckleKohler et al., 2015; Sardianos et al., 2015), or user-generated web discourse (Habernal and Gurevych, 2016a). Other approaches address the classification of argument components into claims and premises (Mochales-Palau and Moens, 2009), supporting and opposing claims (Kwon et al., 2007), or backings, rebuttals and refutations (Habernal and Gurevych, 2016a). Levy et al. (2014) recognize context-dependent claims and Rinott et al. (2015) retrieve several types of evidence from Wikipedia. Approaches for identifying the structure of arguments recognize argumentative relations between argument components using context-free grammars (MochalesPalau and Moens, 2009), pair classification (Stab and Gurevych, 2014), or maximum spanning trees (Peldszus and Stede, 2015). However, none of these approaches consider the quality of arguments. Similarly, most existing corpora in computational argumentation are only annotated with argument components (Habernal and Gurevych, 2016a; Ah"
E17-1092,D15-1050,0,0.0930603,"focused primarily on approaches for argument mining. These include, for example, methods for the identification of arguments in legal texts (Moens et al., 2007), news articles (EckleKohler et al., 2015; Sardianos et al., 2015), or user-generated web discourse (Habernal and Gurevych, 2016a). Other approaches address the classification of argument components into claims and premises (Mochales-Palau and Moens, 2009), supporting and opposing claims (Kwon et al., 2007), or backings, rebuttals and refutations (Habernal and Gurevych, 2016a). Levy et al. (2014) recognize context-dependent claims and Rinott et al. (2015) retrieve several types of evidence from Wikipedia. Approaches for identifying the structure of arguments recognize argumentative relations between argument components using context-free grammars (MochalesPalau and Moens, 2009), pair classification (Stab and Gurevych, 2014), or maximum spanning trees (Peldszus and Stede, 2015). However, none of these approaches consider the quality of arguments. Similarly, most existing corpora in computational argumentation are only annotated with argument components (Habernal and Gurevych, 2016a; Aharoni et al., 2014; Mochales-Palau and Moens, 2009) or argum"
E17-1092,C14-2023,1,0.903902,"Missing"
E17-1092,P09-1026,0,0.0316799,"ses provide enough evidence for accepting its claim (example 2) or as insufficient if its premises do not provide enough evidence (example 1). Therefore, our first research question is whether human annotators can reliably apply the sufficiency criterion to real arguments and if it is possible to create annotated data of high quality. The second research question addresses the automatic recognition of insufficiently supported arguments. We investigate if, and how accurately, insufficiently supported arguments can be identified by computational techniques. 981 3 2008) or the stance on a topic (Somasundaran and Wiebe, 2009). However, there is no corpus of arguments annotated with the sufficiency criterion. Argument Quality: Theoretical Background An argument consists of several argument components. It includes a claim and one or more premises. The claim (also called conclusion) is a controversial statement and the central component of an argument. The premises constitute the reasons for believing the claim to be true or false (Damer, 2009, p. 14). Assessing the quality of arguments is a complex task since arguments in natural language are hardly ever in a standardized form (Damer, 2009; Govier, 2010). Moreover,"
E17-1092,W14-2110,0,0.115608,"Missing"
E17-1092,D14-1006,1,0.93697,"essays. Most of the existing approaches in computational argumentation consider argumentation as discourse structures and focus on the identification of arguments in natural language texts. For instance, existing approaches classify text units as argumentative or non-argumentative (Moens et al., 2007), recognize argument components such as claims or premises at the sentence-level (Mochales-Palau and Moens, 2009; Kwon et al., 2007; Eckle-Kohler et al., 2015) or clause-level (Levy et al., 2014; Sardianos et al., 2015), or identify argument structures by classifying pairs of argument components (Stab and Gurevych, 2014). However, these approaches are of limited use for argumentative writing support systems since they do not recognize the weak points of arguments. Despite the comprehensive theoretical framework on argument quality in logic and argumentation theory (van Eemeren et al., 1996; Damer, 2009), there are only few computational approaches that focus on the assessment of arguments in natural language texts. These existing approaches either identify undisputed arguments in online communities (Cabrio and Villata, 2012), assess the persuasiveness of arguments (Wei et al., 2016), compare and rank argument"
E17-1092,E12-2021,0,0.138807,"Missing"
E17-1092,walker-etal-2012-corpus,0,0.0262135,"vych, 2014), or maximum spanning trees (Peldszus and Stede, 2015). However, none of these approaches consider the quality of arguments. Similarly, most existing corpora in computational argumentation are only annotated with argument components (Habernal and Gurevych, 2016a; Aharoni et al., 2014; Mochales-Palau and Moens, 2009) or argument structures (Reed et al., 2008; Stab and Gurevych, 2014; Peldszus and Stede, 2015) and do not include annotations of argumentative quality issues. Other resources in the field contain arguments annotated with different properties such as emotions and sarcasm (Walker et al., 2012), the type of reasoning (Reed et al., The premise of this argument represents a particular example (second sentence) that supports a general claim in the first sentence. The argument is a generalization from one sample to the general case. However, a single sample is not enough to support the general case. Therefore, the argument does not comply with the sufficiency criterion. Example 2: “Cloning will be beneficial for people who are in need of organ transplants. Cloned organs will match perfectly to the blood group and tissue of patients since they can be raised from cloned stem cells of the"
E17-1092,P16-2032,0,0.0411144,"answer if an argument is sufficiently supported or not. Persing and Ng (2015) introduced an approach for recognizing the argumentation strength of an essay. They found that pos n-grams, prompt adherence features, and predicted argument components perform best. However, their model determines a single holistic score that summarizes the argumentation quality of the entire essay. Consequently, it does not provide formative feedback that guides students to improve their arguments. Recently, researchers proposed approaches for automatically assessing the persuasiveness of arguments. For instance, Wei et al. (2016) proposed an approach for ranking user comments taken from online fora and found that argumentation related features are effective for this task. CanoBasave and He (2016) ranked speakers in political debates by using semantic frames which indicate persuasive argumentation features, and Habernal and Gurevych (2016b) compared the convincingness of argument pairs using feature-rich SVMs and bidirectional LSTMs. However, the persuasiveness score of an argument is only of limited use for argumentative writing support, since it summarizes various quality criteria and does not explain why an argument"
E17-3018,W14-5201,1,0.878567,"Missing"
eckle-kohler-etal-2012-uby,C10-1052,0,\N,Missing
eckle-kohler-etal-2012-uby,W11-0117,0,\N,Missing
eckle-kohler-etal-2012-uby,W09-3418,0,\N,Missing
eckle-kohler-etal-2012-uby,savas-etal-2010-lmf,0,\N,Missing
eckle-kohler-etal-2012-uby,P10-1023,0,\N,Missing
eckle-kohler-etal-2012-uby,R11-1041,0,\N,Missing
eckle-kohler-etal-2012-uby,W03-1905,0,\N,Missing
eckle-kohler-etal-2012-uby,P98-1013,0,\N,Missing
eckle-kohler-etal-2012-uby,C98-1013,0,\N,Missing
eckle-kohler-etal-2012-uby,E12-1059,1,\N,Missing
eckle-kohler-etal-2012-uby,W09-3021,0,\N,Missing
eckle-kohler-etal-2012-uby,I11-1099,1,\N,Missing
eckle-kohler-etal-2012-uby,kunze-lemnitzer-2002-germanet,0,\N,Missing
eckle-kohler-etal-2012-uby,francopoulo-etal-2006-lexical,0,\N,Missing
eckle-kohler-etal-2012-uby,maks-etal-2008-standardising,0,\N,Missing
eckle-kohler-etal-2012-uby,zesch-etal-2008-extracting,1,\N,Missing
eckle-kohler-etal-2012-uby,quochi-etal-2008-lexicon,0,\N,Missing
eckle-kohler-etal-2012-uby,W11-0122,1,\N,Missing
I05-1067,N04-3012,0,0.0445668,"ications, e.g. word sense disambiguation or information retrieval, do not need to determine the exact type of a semantic relation, but rather to judge if two words are closely semantically related or not. For example, for an application in the domain of career consultancy it might be important to conclude that the words “baker” and “bagel” are closely related, while the exact type of a semantic relation does not need to be assigned. Metrics of semantic relatedness are increasingly embedded into natural language processing applications for English due to the availability of free software, e.g. [3] and pre-computed information content values from English corpora. The evaluation of all approaches to compute semantic relatedness has so far been done for the task of semantic similarity. The underlying data was based on the English language [4,5]. We propose the following classification of the metrics of semantic relatedness: R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 767–778, 2005. c Springer-Verlag Berlin Heidelberg 2005  768 I. Gurevych – intrinsic or extrinsic. Intrinsic metrics employ no external evidence, i.e. no knowledge sources except for the conceptual network itself [1,6"
I05-1067,O97-1002,0,0.241952,"relatedness has so far been done for the task of semantic similarity. The underlying data was based on the English language [4,5]. We propose the following classification of the metrics of semantic relatedness: R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 767–778, 2005. c Springer-Verlag Berlin Heidelberg 2005  768 I. Gurevych – intrinsic or extrinsic. Intrinsic metrics employ no external evidence, i.e. no knowledge sources except for the conceptual network itself [1,6,7]. Extrinsic metrics require additional knowledge, e.g. information content values of concepts computed from corpora [8,9,10]. – the type of knowledge source employed, e.g. a dictionary or a conceptual network. Metrics can either employ a machine readable dictionary, i.e. textual definitions of words therein as an underlying knowledge base [1,11], or operate on the structure of a conceptual network, whereby textual definitions themselves are not available [9,7]. Researchers working on the processing of languages such as English, for which many resources exist, have a large choice of options for choosing a metric or a knowledge source. This is, however, not the case for many other languages. Extrinsic metrics relying"
I05-1067,kunze-lemnitzer-2002-germanet,0,0.00870338,"y to include or exclude certain types of information from a gloss. This way, glosses can be easily tailored to a specific task at hand. In our application, this amounts to experimentally determining the types of information crucial for computing semantic relatedness. The knowledge base employed in our experiments is GermaNet [14], the German counterpart of WordNet [15]. Direct re-implementation of semantic relatedness metrics developed for WordNet on the basis of GermaNet is not a trivial task. While sharing many design principles with WordNet, GermaNet displays a number of divergent features [16]. Some of them, such as the lack of conceptual glosses, make it impossible to apply dictionary based metrics in a straightforward manner. Therefore, pseudo glosses are generated directly from the conceptual network. 770 I. Gurevych We experimented with different parameters that control which concepts are included in a pseudo gloss: – size determines the length of a path for the hypernyms to be included in a pseudo gloss. The values of size range over the interval [1, depthmax ], where depthmax is the maximum path length in a conceptual network. The depth is equivalent to the height in this con"
I05-1067,J03-3001,0,0.019348,"Missing"
I05-7005,O97-1002,0,\N,Missing
I11-1099,W98-0710,0,0.643543,"her industrial or institutional building or facility”. Another Wiktionary sense “an organism that is not an animal [. . .]”, however, clearly denotes a different meaning and should thus not be aligned to the WordNet synset. 3 Related Work In the last twenty years, there have been many works on aligning lexical resources at the level of word senses. Almost all alignment approaches for the English language include WordNet, which is the de facto standard resource in the field. Early works address the alignment of WordNet with: Roget’s thesaurus and the Longman Dictionary of Contemporary English (Kwong, 1998) [ K 98], the HECTOR corpus (Litkowski, 1999) [ L 99], the Unified Medical Language System (Burgun and Bodenreider, 2001) [ BB 01], CYC (Reed and Lenat, 2002) [ RL 02], VerbNet and FrameNet (Shi and Mihalcea, 2005) [ SM 05], as well as the Oxford Dictionary of English (Navigli, 2006) [ N 06]. The great potential of the collaborative resource Wikipedia in many NLP applications, such as semantic relatedness (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008), word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010), or named entity recognition (Bunescu and Pas¸ca, 2006), mot"
I11-1099,E09-1005,0,0.0401701,"olds for alignments of Wiktionary and WordNet. For example, the Wiktionary word sense “the people who decide on the verdict; the judiciary” for the word ‘bench’ can be aligned to the two WordNet synsets “persons who administer justice” and “the magistrate or judge or judges [. . .]”. Accordingly, the Wiktionary word sense “the bottom part of a sand casting mold” for the noun ‘drag’ is not covered by any WordNet synset and should thus not be aligned. Therefore, we follow the alignment approach by Niemann and Gurevych (2011), which includes a state-of-the-art word sense disambiguation method by Agirre and Soroa (2009) that is known to outperform word overlap based measures. The method consists of the two steps (i) candidate extraction and (ii) candidate alignment that we briefly review in the following. In the candidate extraction step, the algorithm iterates over all word senses in one lexical resource and extracts suitable candidates within the other resource that might form a valid alignment. In our case, we iterate over all synsets in WordNet and extract all word senses from Wiktionary that are encoded for one of the synset’s synonymous words. For example, we extract all 9 Wiktionary word senses from t"
I11-1099,W99-0505,0,0.243921,"or facility”. Another Wiktionary sense “an organism that is not an animal [. . .]”, however, clearly denotes a different meaning and should thus not be aligned to the WordNet synset. 3 Related Work In the last twenty years, there have been many works on aligning lexical resources at the level of word senses. Almost all alignment approaches for the English language include WordNet, which is the de facto standard resource in the field. Early works address the alignment of WordNet with: Roget’s thesaurus and the Longman Dictionary of Contemporary English (Kwong, 1998) [ K 98], the HECTOR corpus (Litkowski, 1999) [ L 99], the Unified Medical Language System (Burgun and Bodenreider, 2001) [ BB 01], CYC (Reed and Lenat, 2002) [ RL 02], VerbNet and FrameNet (Shi and Mihalcea, 2005) [ SM 05], as well as the Oxford Dictionary of English (Navigli, 2006) [ N 06]. The great potential of the collaborative resource Wikipedia in many NLP applications, such as semantic relatedness (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008), word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010), or named entity recognition (Bunescu and Pas¸ca, 2006), motivates aligning WordNet and Wikipedia to bene"
I11-1099,J08-4004,0,0.0270881,"asked 10 annotators to rate each sense pair as describing the same meaning (class 1) or describing a different meaning (class 0). The annotators are students in computer science, math, or linguistics, whereby two of them had previous experience with annotation studies. We described the annotation task in an annotation guidebook6 and trained the annotators with some example cases. Figure 2: Pseudo code of the alignment algorithm Inter-rater agreement. To ensure the reliability of our annotated dataset, we calculate the interrater agreement between the annotators using the measures described by Artstein and Poesio (2008). The average observed agreement is AO = .93 and the multi-rater chance-corrected agreement is κ = .70. Table 2 shows the pairwise κ for each pair of raters. The annotators C and F have the lowest inter-rater agreement between each other (.58) and with all other raters (.62 and .65). These two raters are thus on the opposite sides of the scale. Further analysis reveals that C is biased towards class 0 (different meaning) and F is biased towards class 1 (same meaning). We removed the annotations of these two raters, which yields an inter-rater agreement of κ = .74. A dataset with such an agreem"
I11-1099,W04-2214,0,0.0230772,"mpiled lexicons are often missing domainspecific word senses, which is an important aspect for domain-aware NLP tools. In their manual Wiktionary–WordNet alignment, Meyer and Gurevych (2010) come to the conclusion that WordNet has a focus on humanities and social sciences, while Wiktionary has a higher coverage of natural sciences and sports. Their findings are, however, limited to a very small set of word senses and thus might not hold for the entire resources. Therefore, we analyze the encoded domains for the whole aligned resource. To identify the domain of a sense, we use WordNet Domains (Bentivogli et al., 2004) to classify the WordNet synsets into 157 domains (e.g., ‘biology’). For Wiktionary, we use the domain markers encoded in the glosses. An example is the sense “(snooker) A play in which the cue ball knocks one (usually red) ball onto another [. . .]” of the word ‘plant’, labeled with the ‘snooker’ domain. We count 714 Characteristics of the Wiktionary–WordNet Alignment Aligning lexical resources is only one side of the coin. Another one is the question, how the aligned resource can be applied in practice and which NLP tasks can benefit from it. Our alignment of Wiktionary and WordNet yields a"
I11-1099,E06-1002,0,0.0121616,"Missing"
I11-1099,N07-1025,0,0.0258116,"ment of WordNet with: Roget’s thesaurus and the Longman Dictionary of Contemporary English (Kwong, 1998) [ K 98], the HECTOR corpus (Litkowski, 1999) [ L 99], the Unified Medical Language System (Burgun and Bodenreider, 2001) [ BB 01], CYC (Reed and Lenat, 2002) [ RL 02], VerbNet and FrameNet (Shi and Mihalcea, 2005) [ SM 05], as well as the Oxford Dictionary of English (Navigli, 2006) [ N 06]. The great potential of the collaborative resource Wikipedia in many NLP applications, such as semantic relatedness (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008), word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010), or named entity recognition (Bunescu and Pas¸ca, 2006), motivates aligning WordNet and Wikipedia to benefit from the advantages of both these resources. One line of research is thereby the alignment of WordNet synsets and Wikipedia categories, which has been done based on the shared taxonomic structure (Toral et al., 2008) [ T 08], textual entailment and semantic relatedness methods (Toral et al., 2009) [ T 09], as well as graph algorithms (Ponzetto and Navigli, 2009) [ PN 09]. Since the vast majority of knowledge is encoded in the Wikipedia article pages, also t"
I11-1099,de-melo-weikum-2010-providing,0,0.136304,"Missing"
I11-1099,2007.mtsummit-papers.24,0,0.027947,"rgies, which lead to better performance than using the resources individually. For instance, Shi and Mihalcea (2005) improve semantic parsing using the knowledge of an aligned resource of FrameNet, WordNet, and VerbNet. Recently, Ponzetto and Navigli (2010) observed improvements for coarse-grained and domain-specific word sense disambiguation using an alignment between WordNet and Wikipedia for adding new relations to WordNet. In another line of research, the community based online dictionary Wiktionary has been successfully applied in several NLP tasks, such as cross-lingual image retrieval (Etzioni et al., 2007), named entity recognition (Richman and Schone, 2008), or synonymy mining (Navarro et al., 2009; Sajous et al., 2010). Zesch et al. (2008b) compare different semantic relatedness measures using either WordNet, Wikipedia, or Wiktionary and find Introduction Though WordNet has been extensively used in knowledge-rich natural language processing (NLP) systems, there is no best lexical resource for all purposes. Jarmasz and Szpakowicz (2003), for example, found better results for solving word choice problems when using Roget’s thesaurus instead of WordNet. There is indeed a large number of differen"
I11-1099,P10-1023,0,0.0404577,"other (.58) and with all other raters (.62 and .65). These two raters are thus on the opposite sides of the scale. Further analysis reveals that C is biased towards class 0 (different meaning) and F is biased towards class 1 (same meaning). We removed the annotations of these two raters, which yields an inter-rater agreement of κ = .74. A dataset with such an agreement is considered reliable and allows to draw tentative conclusions (Krippendorff, 1980), although its agreement is lower than for WordNet–Wikipedia alignment datasets. More precisely, Niemann and Gurevych (2011) report κ = .87 and Navigli and Ponzetto (2010) measure κ = .9. Since even the two skilled annotators I and J only obtained an agreement of .80, we conclude that the alignment task of WordNet and Wiktionary is harder than the alignment of WordNet and Wikipedia. This does not come as a surprise, because Wikipedia contains encyclopedic knowledge that is largely complementary to the linguistic knowledge in WordNet and thus does not require to make fine-grained sense distinctions. WordNet and Wiktionary, however, both encode lexicographic knowledge about common words of the English language and thus require the distinction of very subtle diffe"
I11-1099,P06-1014,0,0.314875,"any works on aligning lexical resources at the level of word senses. Almost all alignment approaches for the English language include WordNet, which is the de facto standard resource in the field. Early works address the alignment of WordNet with: Roget’s thesaurus and the Longman Dictionary of Contemporary English (Kwong, 1998) [ K 98], the HECTOR corpus (Litkowski, 1999) [ L 99], the Unified Medical Language System (Burgun and Bodenreider, 2001) [ BB 01], CYC (Reed and Lenat, 2002) [ RL 02], VerbNet and FrameNet (Shi and Mihalcea, 2005) [ SM 05], as well as the Oxford Dictionary of English (Navigli, 2006) [ N 06]. The great potential of the collaborative resource Wikipedia in many NLP applications, such as semantic relatedness (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008), word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010), or named entity recognition (Bunescu and Pas¸ca, 2006), motivates aligning WordNet and Wikipedia to benefit from the advantages of both these resources. One line of research is thereby the alignment of WordNet synsets and Wikipedia categories, which has been done based on the shared taxonomic structure (Toral et al., 2008) [ T 08], textual e"
I11-1099,W11-0122,1,0.609708,"Resource LDOCE & Roget HECTOR UMLS CYC VerbNet & FrameNet Oxford Dictionary Wikipedia categories Wikipedia categories Wikipedia categories Simple Wikipedia art. Wikipedia articles Wikipedia articles Wikipedia articles Wikipedia articles Wikipedia articles Wiktionary senses Wiktionary senses Full − − − − √ √ √ √ √ √ − √ √ √ √ − √ Table 1: Previous work on aligning WordNet 2011) [ NG 11]. Each approach has been evaluated on a separate, manually annotated dataset: De Melo and Weikum (2010) report a precision of P = .85, Navigli and Ponzetto (2010) observe F1 = .79, and the alignment described by Niemann and Gurevych (2011) evaluates to F1 = .78. It should be noted that these numbers are not comparable to each other, since they are based on different datasets and annotation schemes. Recently, also Wiktionary has been found to be a very promising resource for NLP tasks. So far, Wiktionary knowledge has been used for image search (Etzioni et al., 2007), calculating semantic relatedness (Zesch et al., 2008b), information retrieval (M¨uller and Gurevych, 2009), and synonymy detection (Navarro et al., 2009). An alignment has been done manually for a small number of word senses shared by Wiktionary and WordNet (Meyer"
I11-1099,toral-etal-2008-named,0,0.0210219,"Missing"
I11-1099,R09-1080,0,0.07806,"Missing"
I11-1099,zesch-etal-2008-extracting,1,0.0371053,"Missing"
I11-1099,P10-1154,0,0.0167921,"Missing"
I11-1099,P08-1001,0,0.00422968,"g the resources individually. For instance, Shi and Mihalcea (2005) improve semantic parsing using the knowledge of an aligned resource of FrameNet, WordNet, and VerbNet. Recently, Ponzetto and Navigli (2010) observed improvements for coarse-grained and domain-specific word sense disambiguation using an alignment between WordNet and Wikipedia for adding new relations to WordNet. In another line of research, the community based online dictionary Wiktionary has been successfully applied in several NLP tasks, such as cross-lingual image retrieval (Etzioni et al., 2007), named entity recognition (Richman and Schone, 2008), or synonymy mining (Navarro et al., 2009; Sajous et al., 2010). Zesch et al. (2008b) compare different semantic relatedness measures using either WordNet, Wikipedia, or Wiktionary and find Introduction Though WordNet has been extensively used in knowledge-rich natural language processing (NLP) systems, there is no best lexical resource for all purposes. Jarmasz and Szpakowicz (2003), for example, found better results for solving word choice problems when using Roget’s thesaurus instead of WordNet. There is indeed a large number of different lexical resources: The ACL Special Interest Group o"
I13-1112,P12-2059,0,0.188451,"uction. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration, characters from one alphabet are mapped onto corresponding letters in another alphabet. Cognates follow more complex production patterns. Nakov and Tiedemann (2012) aim at improving MT quality using cognates detected by character-based alignment. They focus on the language pair MacedonianBulgarian and use English as a bridge language. As they use cognate identification only as an intermediary step and do not provide evaluation results, we cannot directly compare with their work. To the best of our knowledge, we are the first to use statistical character-based MT for the goal of directly producing cognates. Character-Based Machine Translation Our approach relies on statistical phrase-based machine translation (MT). As we are not interested in the translat"
I13-1112,I08-8003,0,0.0278785,"r MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration, characters from one alphabet are mapped onto corresponding letters in another alphabet. Cognates follow more complex production patterns. Nakov and Tiedemann (2012) aim at improving MT quality using cognates detected by character-based alignment. They focus on the language pair MacedonianBulgarian and use English as a bridge language. As they use cognate identification only as an intermediary step and do not provide evaluation results, we cannot directly compare with their work. To the best of our knowledge, we are the first to use statistical character-based MT for the"
I13-1112,J03-1002,0,0.00657185,"ge in the 5 best productions and the MRR for each parameter. N-gram Size We start with the n-gram size parameter that determines the tokenization of the input, the respective format for unigrams, bigrams, and trigrams for the word banc looks as follows: # b a n c $ / #b ba an nc c$ / #ba ban anc nc$ Higher order n-grams in general increase the vocabulary and thus lead to better alignment. However, they also require a larger amount of training data, otherwise the number of unseen instances is Moses Parameters Finally, we tune the Moses parameter weights by applying minimum error rate training (Och and Ney, 2003) using the development set, but it makes almost no difference in this setting. Tuning optimizes the model with respect to the BLEU score. For our data, the BLEU score is quite high for all produced cognate candidates, but it is not indicative of the usefulness of the transformation. A word containing one wrong character is not necessarily better than a word con3 The cognates have been retrieved from several web resources and merged with the set used by Montalvo et al. (2012). All test cognate list can be found at: http://www.ukp.tu-darmstadt.de/data 886 0.7 Training Size 0.6 Cognates 0.5 Trans"
I13-1112,P02-1040,0,0.0867573,"Missing"
I13-1112,E12-1059,1,0.738926,"rve in Figure 3 shows the results. As expected, both coverage and MRR improve with increasing size of the training data, but we do not see much improvement after about 1,000 training instances. Thus, COP is able to learn stable patterns from relatively few training instances. However, even a list of 1,000 cognates is a hard constraint for some language pairs. Thus, we test if we can also produce satisfactory results with lower quality sets of training pairs that might be easier to obtain than a list of cognates. We use word pairs extracted from the freely available multilingual resources UBY (Gurevych et al., 2012) and Universal WordNet (UWN) (de Melo and Weikum, 2009). UBY combines several lexical-semantic resources, we use translations which were extracted from Wiktionary. UWN is based on WordNet and Wikipedia and provides automatically extracted translations for over 200 languages that are a bit noisier compared to UBY translations. Additionally, we queried the Microsoft Bing translation API using all words from 4.4 Comparison to Previous Work Previous work (Kondrak and Dorr, 2004; Inkpen et al., 2005; Sep´ulveda Torres and Aluisio, 2011; 4 http://www.bing.com/translator We only use every 5th word in"
I13-1112,I11-1109,0,0.0192185,"tly produce a cognate in the target language from an input word in another language. Consequently, in the remainder of the paper, we refer to our method as COP (COgnate Production). Exploiting the orthographic similarity of cognates to improve the alignment of words has already been analyzed as a useful preparation for MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration, characters from one alphabet are mapped onto corresponding letters in another alphabet. Cognates follow more complex production patterns. Nakov and Tiedemann (2012) aim at improving MT quality using cognates detected by character-based alignment. They focus"
I13-1112,W09-3528,0,0.0207182,"useful preparation for MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration, characters from one alphabet are mapped onto corresponding letters in another alphabet. Cognates follow more complex production patterns. Nakov and Tiedemann (2012) aim at improving MT quality using cognates detected by character-based alignment. They focus on the language pair MacedonianBulgarian and use English as a bridge language. As they use cognate identification only as an intermediary step and do not provide evaluation results, we cannot directly compare with their work. To the best of our knowledge, we are the first to use statistical ch"
I13-1112,W02-0902,0,0.12847,"ne translation rithm selects the best combination of sequences. The transformation is thus not performed on isolated characters, it also considers the surrounding sequences and can account for context-dependent phenomena. The goal of the approach is to directly produce a cognate in the target language from an input word in another language. Consequently, in the remainder of the paper, we refer to our method as COP (COgnate Production). Exploiting the orthographic similarity of cognates to improve the alignment of words has already been analyzed as a useful preparation for MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration,"
I13-1112,C04-1117,0,0.0333755,"et language. If we already have candidate pairs, string similarity measures can be used to distinguish cognates and unrelated pairs (Montalvo et al., 2012; Sep´ulveda Torres and Aluisio, 2011; Inkpen et al., 2005; Kondrak and Dorr, 2004). However, these measures do not take the regular production processes into account that can be found for most cognates, e.g. the English suffix ~tion becomes ~ci´on in Spanish like in nation-naci´on or addition-adici´on. Thus, an alternative approach is to manually extract or learn production rules that reflect the regularities (Gomes and Pereira Lopes, 2011; Schulz et al., 2004). All these methods are based on string alignment and thus cannot be directly applied to language pairs with different alphabets. A possible workaround would be to first transliterate foreign alphabets into Latin, but unambiguous transliteration is only possible for some languages. Methods that rely on the phonetic similarity of words (Kondrak, 2000) require a phonetic transcription that is not always available. Thus, we propose a novel production approach using statistical characterbased machine translation in order to directly produce cognates. We argue that this has the following advantages"
I13-1112,C04-1137,0,0.277757,"t of the sentence with the help of associated words like Konferenz-conference or FebruarFebruary. Such pairs of associated words are called cognates. 883 International Joint Conference on Natural Language Processing, pages 883–891, Nagoya, Japan, 14-18 October 2013. In order to construct such cognate lists, we need to decide whether a word in a source language has a cognate in a target language. If we already have candidate pairs, string similarity measures can be used to distinguish cognates and unrelated pairs (Montalvo et al., 2012; Sep´ulveda Torres and Aluisio, 2011; Inkpen et al., 2005; Kondrak and Dorr, 2004). However, these measures do not take the regular production processes into account that can be found for most cognates, e.g. the English suffix ~tion becomes ~ci´on in Spanish like in nation-naci´on or addition-adici´on. Thus, an alternative approach is to manually extract or learn production rules that reflect the regularities (Gomes and Pereira Lopes, 2011; Schulz et al., 2004). All these methods are based on string alignment and thus cannot be directly applied to language pairs with different alphabets. A possible workaround would be to first transliterate foreign alphabets into Latin, but"
I13-1112,W11-4508,0,0.0175818,"Everybody who knows English might grasp the gist of the sentence with the help of associated words like Konferenz-conference or FebruarFebruary. Such pairs of associated words are called cognates. 883 International Joint Conference on Natural Language Processing, pages 883–891, Nagoya, Japan, 14-18 October 2013. In order to construct such cognate lists, we need to decide whether a word in a source language has a cognate in a target language. If we already have candidate pairs, string similarity measures can be used to distinguish cognates and unrelated pairs (Montalvo et al., 2012; Sep´ulveda Torres and Aluisio, 2011; Inkpen et al., 2005; Kondrak and Dorr, 2004). However, these measures do not take the regular production processes into account that can be found for most cognates, e.g. the English suffix ~tion becomes ~ci´on in Spanish like in nation-naci´on or addition-adici´on. Thus, an alternative approach is to manually extract or learn production rules that reflect the regularities (Gomes and Pereira Lopes, 2011; Schulz et al., 2004). All these methods are based on string alignment and thus cannot be directly applied to language pairs with different alphabets. A possible workaround would be to first t"
I13-1112,A00-2038,0,0.520697,"e.g. the English suffix ~tion becomes ~ci´on in Spanish like in nation-naci´on or addition-adici´on. Thus, an alternative approach is to manually extract or learn production rules that reflect the regularities (Gomes and Pereira Lopes, 2011; Schulz et al., 2004). All these methods are based on string alignment and thus cannot be directly applied to language pairs with different alphabets. A possible workaround would be to first transliterate foreign alphabets into Latin, but unambiguous transliteration is only possible for some languages. Methods that rely on the phonetic similarity of words (Kondrak, 2000) require a phonetic transcription that is not always available. Thus, we propose a novel production approach using statistical characterbased machine translation in order to directly produce cognates. We argue that this has the following advantages: (i) it captures complex patterns in the same way machine translation captures complex rephrasing of sentences, (ii) it performs better than similarity measures from previous work on cognates, and (iii) it also works for language pairs with different alphabets. 2 Figure 1: Character-based machine translation rithm selects the best combination of seq"
I13-1112,J99-1003,0,0.07861,"candidate pairs, we get many pairs with tied ranks, which is problematic for computing coverage and MRR. Thus, we randomize pairs within one rank and report averaged results over 10 randomization runs.8 We compare COP to three frequently used string similarity measures (LCSR, DICE, and XDICE), which performed well in (Inkpen et al., 2005; Montalvo et al., 2012), and to SpSim which is based on learning production rules. The longest common subsequence ratio (LCSR) calculates the ratio of the length of the longest (not necessarily contiguous) common subsequence and the length of the longer word (Melamed, 1999). DICE (Adamson and Boreham, 1974) measures the shared character bigrams, while the variant XDICE (Brew and McKelvie, 1996) uses extended bigrams, i.e. trigrams without the middle letter. SpSim (Gomes and Pereira Lopes, 2011) is based on string alignment of identical characters for the extraction and generalization of the most frequent cognate patterns. Word pairs that follow these extracted cognate patterns are considered equally similar as pairs with identical spelling. Table 3 shows the results. The differences between the individual similarity measures are very small, string similarity per"
I13-1112,W11-2159,0,0.0160037,"ge. Consequently, in the remainder of the paper, we refer to our method as COP (COgnate Production). Exploiting the orthographic similarity of cognates to improve the alignment of words has already been analyzed as a useful preparation for MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration, characters from one alphabet are mapped onto corresponding letters in another alphabet. Cognates follow more complex production patterns. Nakov and Tiedemann (2012) aim at improving MT quality using cognates detected by character-based alignment. They focus on the language pair MacedonianBulgarian and use English as a bridge la"
I13-1112,2009.eamt-1.3,0,0.0319562,"acter-based machine translation rithm selects the best combination of sequences. The transformation is thus not performed on isolated characters, it also considers the surrounding sequences and can account for context-dependent phenomena. The goal of the approach is to directly produce a cognate in the target language from an input word in another language. Consequently, in the remainder of the paper, we refer to our method as COP (COgnate Production). Exploiting the orthographic similarity of cognates to improve the alignment of words has already been analyzed as a useful preparation for MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 200"
I13-1112,P07-2045,0,\N,Missing
I13-1169,D10-1116,0,0.0157447,"t performance across domains. For a general and detailed overview of linguistic steganography, including methods other than paraphrasing, see for example Bennett (2004) and Topkara et al. (2005). 3.1 Paraphrasing for Linguistic Steganography As regards synonym substitution, the first studies made no use (Topkara et al., 2006) or just limited use (Bolshakov, 2005) of the context through collocation lists. While this approach offers a relatively high capacity, the transformations result in frequent semantic, or even grammatical, errors in the text, which is undesirable (Bennett, 2004). Recently Chang and Clark (2010a,b) proposed the use of contextual paraphrases for linguistic steganography. This offers a higher perceived quality and is therefore more suited to text watermarking where quality is a crucial aspect. Chang and Clark (2010b) used the English Lexical Substitution data (McCarthy and Navigli, 2007) from SemEval 2007 for evaluation, WordNet as the source of potential synonyms and n-gram frequencies for candidate ranking to experiment with paraphrasing for linguistic steganography. They introduced a graph coloring approach to embed information in a text through the substitution of words with their"
I13-1169,W07-1011,0,0.0724346,"Missing"
I13-1169,N10-1084,0,0.0283169,"t performance across domains. For a general and detailed overview of linguistic steganography, including methods other than paraphrasing, see for example Bennett (2004) and Topkara et al. (2005). 3.1 Paraphrasing for Linguistic Steganography As regards synonym substitution, the first studies made no use (Topkara et al., 2006) or just limited use (Bolshakov, 2005) of the context through collocation lists. While this approach offers a relatively high capacity, the transformations result in frequent semantic, or even grammatical, errors in the text, which is undesirable (Bennett, 2004). Recently Chang and Clark (2010a,b) proposed the use of contextual paraphrases for linguistic steganography. This offers a higher perceived quality and is therefore more suited to text watermarking where quality is a crucial aspect. Chang and Clark (2010b) used the English Lexical Substitution data (McCarthy and Navigli, 2007) from SemEval 2007 for evaluation, WordNet as the source of potential synonyms and n-gram frequencies for candidate ranking to experiment with paraphrasing for linguistic steganography. They introduced a graph coloring approach to embed information in a text through the substitution of words with their"
I13-1169,W10-3001,1,0.860306,"Missing"
I13-1169,W08-0607,0,0.043464,"Missing"
I13-1169,W04-3103,0,0.0876659,"Missing"
I13-1169,S07-1009,0,0.064177,"Missing"
I13-1169,P07-1125,0,0.0262156,"Missing"
I13-1169,W09-1304,0,0.0150069,"cements was not evaluated. 3.2 Uncertainty Cue Detection Another major field of work related to this study is the detection of uncertainty cues, which we propose to use for paraphrasing in Section 4. The first approaches to uncertainty detection were based on hand-crafted lexicons (Light et al., 2004; Saur´ı, 2008). In particular, ConText (Chapman et al., 2007) used lexicons and regular expressions not only to detect cues, but also to recognize contexts where a cue word does not imply uncertainty. Supervised uncertainty cue detectors have also been developed using either token classification (Morante and Daelemans, 2009) or sequence labeling approaches (Tang et al., 2010). A good overview and comparison of different statistical approaches is given in Farkas et al. (2010). Szarvas et al. (2012) addressed uncertainty cue detection in a multi-domain setting, using surfacelevel, part-of-speech and chunk-level features and sequence labeling (CRF) models. They found that cue words can be accurately detected in texts with various topics and stylistic properties. We make use of the multidomain corpora presented in their study and evaluate a cross-domain cue detection model for text watermarking. 4 Uncertainty Cue Det"
I13-1169,W10-3002,0,0.0308563,"Missing"
I13-1169,S07-1044,0,0.0665659,"Missing"
I13-1169,J12-2004,1,\N,Missing
I17-1081,D17-2004,1,0.793543,"ion using Concept Coreference Resolution and Global Importance Optimization Tobias Falke, Christian M. Meyer, Iryna Gurevych Research Training Group AIPHES and UKP Lab Department of Computer Science, Technische Universit¨at Darmstadt https://www.aiphes.tu-darmstadt.de Abstract document collection, structures it across document boundaries and can be used as a table-of-contents to navigate in the collection. Several studies report successful applications of concept maps in this direction (Carvalho et al., 2001; Briggs et al., 2004; Richardson and Fox, 2005; Villalon, 2012; Valerio et al., 2012; Falke and Gurevych, 2017b). The task we consider in this work is defined as follows: Given a set of documents on a certain topic, extract a concept map that represents the most important content on that topic, satisfies a specified size limit and is connected. Although work dealing with the automatic extraction of concept maps from text exists (§2), current methods have several limitations. First, most approaches do not attempt to detect coreferences between extracted concepts. For instance, if both ADHD symptoms and symptoms of ADHD are found, they treat them as separate concepts. In a concept map, such duplicate co"
I17-1081,W17-6909,1,0.799481,"ion using Concept Coreference Resolution and Global Importance Optimization Tobias Falke, Christian M. Meyer, Iryna Gurevych Research Training Group AIPHES and UKP Lab Department of Computer Science, Technische Universit¨at Darmstadt https://www.aiphes.tu-darmstadt.de Abstract document collection, structures it across document boundaries and can be used as a table-of-contents to navigate in the collection. Several studies report successful applications of concept maps in this direction (Carvalho et al., 2001; Briggs et al., 2004; Richardson and Fox, 2005; Villalon, 2012; Valerio et al., 2012; Falke and Gurevych, 2017b). The task we consider in this work is defined as follows: Given a set of documents on a certain topic, extract a concept map that represents the most important content on that topic, satisfies a specified size limit and is connected. Although work dealing with the automatic extraction of concept maps from text exists (§2), current methods have several limitations. First, most approaches do not attempt to detect coreferences between extracted concepts. For instance, if both ADHD symptoms and symptoms of ADHD are found, they treat them as separate concepts. In a concept map, such duplicate co"
I17-1081,N06-1046,0,0.141449,"n S that partitions M into a set of sets C = {C1 , . . . Cn } where each Ci is a set of mentions representing a concept. Mention Partitioning The task of grouping mentions to concepts can be seen as finding a partition of M based on the pairwise classifications. However, this is non-trivial, as single predictions might conflict: Both (a, b) and (b, c) could be classified as coreferent, but not (a, c). Formally, the relation of all coreferent pairs S ⊆ M 2 has to be an equivalence relation, i.e. reflexive, symmetric and transitive, to represent a consistent partitioning. For a similar problem, Barzilay and Lapata (2006) propose to use ILP to find a valid partitioning that maximally agrees with the pairwise classifications. Let xp ∈ {0, 1} indicate the coreference of mentions p = (m1 , m2 ) and be c(p) = P (y = 1|m1 , m2 ). Then they optimize the assignments xp to maximize X c(p) xp + (1 − c(p)) (1 − xp ) (2) (1) 3.2.3 p∈M 2 Graph Construction Using the partitioning, we can now connect the extracted propositions to a graph G = (C, R) in 4 Using the sum of vectors for all tokens; 300-dimensional word2vec Google News embeddings (Mikolov et al., 2013). 804 which the nodes are concepts C and an edge with label r"
I17-1081,C16-1023,0,0.254419,"l a connected graph of the target size remains. However, it is not guaranteed that the optimal subset is found. Integer Linear Programming (ILP) has been successfully used to solve the knapsack problem that arises in sentence-level extractive summarization (McDonald, 2007). In our task, the knapsack problem is not present, as both the scoring and size restriction are defined on the level of concepts, but the connectedness requirement poses a similar constraint that restricts the subset selection. ILP formulations for such a problem have been proposed for graph-based abstractive summarization (Li et al., 2016; Liu et al., 2015). In our work, we transfer these ideas to concept maps and evaluate their efficacy. This is important, as the methods were originally proposed for different kinds of graphs (event networks and AMR graphs) and introduced to generate abstractive textual summaries, while we use our concept map graphs directly as the final summaries. Related Work Previous approaches to construct concept maps from text, working with either single documents (Zubrinic et al., 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkam"
I17-1081,W04-1013,0,0.0178311,"raining topics. The final models are trained on the full training set with the best parameter. We did this separately for all ablations of our model that produce different training data, obtaining best parameters of C = 10 for coref=lem on E DUC and all models on W IKI as well as C = 30 for coref=doc and our model on E DUC (see Table 2). 1 X max{meteor(p, pr ) |pr ∈ PR } |PS | p∈PS Re = 1 X max{meteor(p, ps ) |ps ∈ PS } |PR | p∈PR The F1-score is the equally weighted harmonic mean of precision and recall. Scores per map are macro-averaged over all topics. As a second metric, we compute ROUGE (Lin, 2004), the standard metric for textual sumarization. We concatenate all propositions of a map into a single string, sS and sR , and separate propositions with a dot to ensure that no bigrams span across propositions and the metric is therefore order-independent. We run ROUGE 1.5.56 with sS as the peer summary and sR as a single model summary to compute ROUGE-2. 4.3 5 Results and Analysis 5.1 Evaluation Results We compare our model against several previously suggested methods. As unsupervised methods, we include concept selection based on frequency (Valerio and Leake, 2006), denoted as Valerio 06, s"
I17-1081,N15-1114,0,0.152562,"ph of the target size remains. However, it is not guaranteed that the optimal subset is found. Integer Linear Programming (ILP) has been successfully used to solve the knapsack problem that arises in sentence-level extractive summarization (McDonald, 2007). In our task, the knapsack problem is not present, as both the scoring and size restriction are defined on the level of concepts, but the connectedness requirement poses a similar constraint that restricts the subset selection. ILP formulations for such a problem have been proposed for graph-based abstractive summarization (Li et al., 2016; Liu et al., 2015). In our work, we transfer these ideas to concept maps and evaluate their efficacy. This is important, as the methods were originally proposed for different kinds of graphs (event networks and AMR graphs) and introduced to generate abstractive textual summaries, while we use our concept map graphs directly as the final summaries. Related Work Previous approaches to construct concept maps from text, working with either single documents (Zubrinic et al., 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajarama"
I17-1081,W14-3348,0,0.0133597,"the corresponding topic description and the number of concepts in the reference concept map as the size limit. To compare a systemgenerated concept map with a reference concept 806 map we represent both as sets of propositions P , i.e. a set in which each element is the concatenation of a relation label with its two concept labels. We then calculate the overlap between the set PS for the system map and the set PR for the reference map. As the number of relations and thus propositions of the generated map can differ, we report precision, recall and F1-scores. Our first metric based on METEOR (Denkowski and Lavie, 2014) has the advantage that it takes synonyms and paraphrases into account and does not solely rely on lexical matches. For each pair of propositions ps ∈ PS and pr ∈ PR we calculate the match score meteor(ps , pr ) ∈ [0, 1]. Then, precision and recall per map are computed as: Pr = et al., 2009). For E DUC, we trained it on 17,500 pairs of mentions, and for W IKI, on 4,500 pairs of mentions, which were in both cases derived from the reference concept maps of the training part of the respective dataset. The SVMrank model for importance scoring is trained with Dlib9 . We use the set of all extracted"
I17-1081,P14-5010,0,0.00267412,"dge, this is all existing work for this task to which we can compare the proposed model. Table 2 shows METEOR and ROUGE-2 scores for all methods on both datasets. Our model outperforms all three unsupervised approaches significantly on both datasets, demonstrating the superiority of the supervised scoring model. With regard to Falke 17, which is supervised to a similar extent, the results are twofold: While our model improves in ROUGE-2, it has a lower METEOR score. We looked into these results in deImplementation and Training All source documents are preprocessed with Stanford CoreNLP 3.7.0 (Manning et al., 2014) to obtain tokenization, sentence splitting, part-ofspeech tags, named entities, dependency parses and coreference chains. For Open Information Extraction, we use OpenIE-47 , a system developed at the University of Washington that is currently state-of-the-art according to a recent comparison (Stanovsky and Dagan, 2016). ILPs are solved with the IBM CPLEX optimizer.8 The concept coreference model is implemented using the logistic regression model of Weka (Hall 6 Parameter: -n 2 -x -m -c 95 -r 1000 -f A -p 0.5 -t 0 -d -a https://github.com/knowitall/openie 8 https://ibm.com/software/commerce/ o"
I17-1081,D17-1320,1,0.824609,"ion using Concept Coreference Resolution and Global Importance Optimization Tobias Falke, Christian M. Meyer, Iryna Gurevych Research Training Group AIPHES and UKP Lab Department of Computer Science, Technische Universit¨at Darmstadt https://www.aiphes.tu-darmstadt.de Abstract document collection, structures it across document boundaries and can be used as a table-of-contents to navigate in the collection. Several studies report successful applications of concept maps in this direction (Carvalho et al., 2001; Briggs et al., 2004; Richardson and Fox, 2005; Villalon, 2012; Valerio et al., 2012; Falke and Gurevych, 2017b). The task we consider in this work is defined as follows: Given a set of documents on a certain topic, extract a concept map that represents the most important content on that topic, satisfies a specified size limit and is connected. Although work dealing with the automatic extraction of concept maps from text exists (§2), current methods have several limitations. First, most approaches do not attempt to detect coreferences between extracted concepts. For instance, if both ADHD symptoms and symptoms of ADHD are found, they treat them as separate concepts. In a concept map, such duplicate co"
I17-1081,D16-1252,0,0.0147726,"th regard to Falke 17, which is supervised to a similar extent, the results are twofold: While our model improves in ROUGE-2, it has a lower METEOR score. We looked into these results in deImplementation and Training All source documents are preprocessed with Stanford CoreNLP 3.7.0 (Manning et al., 2014) to obtain tokenization, sentence splitting, part-ofspeech tags, named entities, dependency parses and coreference chains. For Open Information Extraction, we use OpenIE-47 , a system developed at the University of Washington that is currently state-of-the-art according to a recent comparison (Stanovsky and Dagan, 2016). ILPs are solved with the IBM CPLEX optimizer.8 The concept coreference model is implemented using the logistic regression model of Weka (Hall 6 Parameter: -n 2 -x -m -c 95 -r 1000 -f A -p 0.5 -t 0 -d -a https://github.com/knowitall/openie 8 https://ibm.com/software/commerce/ optimization/cplex-optimizer/ 7 9 807 http://dlib.net/ml.html Approach PageRank Valerio 06 Zubrinic 15 Falke 17 Our model - coref=lem - coref=doc - w/o ILP s*, ILP s*, w/o ILP Pr 11.78 11.89 12.48 15.12 15.14 13.93 14.14 15.29 23.32 18.28 E DUC METEOR ROUGE-2 Re F1 Pr Re F1 † 16.21 † 13.61 7.14 11.66 8.66 ‡ 16.12 † 13.65"
I17-1081,D16-1191,0,0.0666778,"Missing"
I17-1081,W11-1414,0,0.0207487,"ment clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002), all follow a similar pipeline: concept extraction, relation extraction, scoring and concept map construction. During concept extraction, most approaches apply hand-written patterns to extract labels for concepts from syntactic representations, focusing on noun phrases-like structures. Similar approaches are used to extract relation labels for pairs of concepts. Alternatively, semantic representations have been suggested as a more easily accessible representation compared to syntax (Falke and Gurevych, 2017c; Olney et al., 2011). Given these extractions, few attempts beyond string matching have been made to identify unique concepts. Valerio and Leake (2006) suggest to consider only certain part-of-speech during string matching, while the earlier approach of Rajaraman and Tan (2002) uses a clustering algorithm based on a vector space model. Our work proposes a more comprehensive approach, leveraging state-of-the-art semantic similarity measures and set partitioning to also detect coreferent concept labels that are paraphrases. The selection of a summary-worthy subset of 2 3 3 Model Given a document set D, topic t and"
I17-1081,E17-2112,0,0.0169733,"eral methods suggested in previous work. 2 all extracted concepts and relations was largely ignored in previous work, as many studies did not have a focus on summarization. However, when dealing with larger document clusters, this step becomes inevitable. Zubrinic et al. (2015) suggest a tf-idf metric on the level of concept labels, Villalon (2012) uses Latent Semantic Analysis and Valerio and Leake (2006) suggest simple concept frequencies. Our model goes a step further and combines these with other features in a supervised model, which works well for textual summarization (Cao et al., 2016; Yang et al., 2017). For building a summary concept map that is connected, does not exceed the target size and contains as many important concepts as possible, we are only aware of a heuristic approach suggested by Zubrinic et al. (2015). It iteratively removes low-scoring concepts from all extractions until a connected graph of the target size remains. However, it is not guaranteed that the optimal subset is found. Integer Linear Programming (ILP) has been successfully used to solve the knapsack problem that arises in sentence-level extractive summarization (McDonald, 2007). In our task, the knapsack problem is"
I17-1081,C16-1145,0,0.0176899,"onary and an additional list of concreteness values (Brysbaert et al., 2014). 4 4.1 Experimental Setup Data We evaluate our approach using two benchmark datasets and compare the generated concept maps against reference maps. As the first dataset, we use a recently published corpus by Falke and Gurevych (2017a) that provides summary concept maps for document clusters on educational topics. They were manually created using crowdsourcing and expert annotators. As the second dataset, we use a corpus in which the introductions of featured Wikipedia articles are used as summaries for web documents (Zopf et al., 2016). This property allows us to make use of the links to other Wikipedia pages in the summaries as annotations of concepts. In combination with Open Information Extraction, we can therefore automatically derive concepts and relations from the Wikipedia summaries to obtain a second corpus of summary concept maps. We refer to these datasets as E DUC and W IKI. Table 1 shows their characteristics. Note that in both datasets the summaries are much smaller than the document sets, posing a challenging summarization task. In addition, the document clusters of E DUC are very large, constituting a challen"
I17-1081,P13-4028,0,0.0242163,"r a pair of mentions, σ is the sigmoid function and θ are the learned parameters. As features we use different similarity measures that indicate if both terms have the same meaning. Lexical features are normalized Levenshtein distance and the overlap (Jaccard coefficient) between stemmed content words. To capture similarity on a semantic level, we use cosine similarity between concept embeddings4 and two measures using word-level similarity based on Latent Semantic Analysis (Deerwester et al., 1990) and WordNet (Resnik, 1995) together with a word alignment method, both implemented in Semilar (Rus et al., 2013). The selection of these features is driven by practical reasons: Since the number of pairs is in O(|M |2 ), the feature set has to be small and restricted to fast-to-compute metrics to make the approach computationally feasible. 3.2.2 under the transitivity constraints xpi ≥ xpj + xpk − 1 for all pi , pj , pk ∈ M 2 where i 6= j 6= k. Unfortunately, this ILP needs O(|M |2 ) variables and O(|M |3 ) constraints, which makes it difficult to solve for our problem (where |M |is up to 20k and we thus have up to 400 million variables and 8 trillion constraints). As an alternative approach, we use an"
J12-2004,W07-1011,0,0.381457,"Missing"
J12-2004,W10-3017,0,0.0948903,"d “pseudo-triggers”. A pseudo-trigger is a superstring of a cue and it is basically used for recognizing contexts where a cue does not imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁcation ¨ ur ¨ an"
J12-2004,W09-1318,0,0.0333556,"Missing"
J12-2004,P07-2009,0,0.0431689,"Missing"
J12-2004,P07-1033,0,0.0872072,"Missing"
J12-2004,W10-3001,1,0.879192,"Missing"
J12-2004,P09-2044,0,0.188168,"Missing"
J12-2004,W10-3004,0,0.129684,"Missing"
J12-2004,W08-0607,0,0.0742456,"Missing"
J12-2004,W09-1418,0,0.0271284,"Missing"
J12-2004,W09-1401,0,0.038846,"Missing"
J12-2004,W10-3011,0,0.0641227,"re a cue does not imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁcation ¨ ur ¨ and Radev (2009) and Velldal (2010) match cues from a lexicon then approaches. Ozg apply a binary classiﬁer based on feat"
J12-2004,W04-3103,0,0.0604055,"Missing"
J12-2004,W09-1410,0,0.0488634,"Missing"
J12-2004,P07-1125,0,0.422088,"g corpora and uncertainty recognition tools and our chief goal here is to provide a computational linguistics-oriented classiﬁcation. With this in mind, our subclasses are intended to be well-deﬁned and easily identiﬁable by automatic tools. Moreover, this classiﬁcation allows different applications to choose the subset of phenomena to be recognized in accordance with their main task (i.e., we tried to avoid an overly coarse or ﬁne-grained categorization). 2.1 Classiﬁcation of Uncertainty Types Several corpora annotated for uncertainty have been published in different domains such as biology (Medlock and Briscoe 2007; Kim, Ohta, and Tsujii 2008; Settles, Craven, and Friedland 2008; Shatkay et al. 2008; Vincze et al. 2008; Nawaz, Thompson, and Ananiadou 2010), medicine (Uzuner, Zhang, and Sibanda 2009), news media (Rubin, Liddy, and Kando 2005; Wilson 2008; Saur´ı and Pustejovsky 2009; Rubin 2010), and encyclopedia (Farkas et al. 2010). As can be seen from publicly available annotation guidelines, there are many overlaps but differences as well in the understanding of uncertainty, which is sometimes connected to domain- and genre-speciﬁc features of the texts. Here we introduce a domain- and genre-independ"
J12-2004,W09-1304,0,0.0179162,"expressions to deﬁne cues and “pseudo-triggers”. A pseudo-trigger is a superstring of a cue and it is basically used for recognizing contexts where a cue does not imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁca"
J12-2004,W10-3006,0,0.060851,"Missing"
J12-2004,W10-3112,0,0.388287,"Missing"
J12-2004,D09-1145,0,0.0310857,"Missing"
J12-2004,W10-3008,0,0.0728038,"t imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁcation ¨ ur ¨ and Radev (2009) and Velldal (2010) match cues from a lexicon then approaches. Ozg apply a binary classiﬁer based on features describing the co"
J12-2004,W10-3018,0,0.132999,"Missing"
J12-2004,P08-1033,1,0.873695,"ing applications), and a recent pilot task sought to exploit negation and hedge cue detectors in machine reading (Morante and Daelemans 2011). As the focus of our paper is cue recognition, however, we omit their detailed description here. 4.1 In-Domain Cue Detection In-domain uncertainty detectors have been developed since the mid 1990s. Most of these systems use hand-crafted lexicons for cue recognition and they treat each occurrence of the lexicon items as a cue—that is, they do not address the problem of disambiguating cues (Friedman et al. 1994; Light, Qiu, and Srinivasan 2004; Farkas and Szarvas 2008; Saur´ı 2008; Conway, Doan, and Collier 2009; Van Landeghem et al. 2009). ConText (Chapman, Chu, and Dowling 2007) uses regular expressions to deﬁne cues and “pseudo-triggers”. A pseudo-trigger is a superstring of a cue and it is basically used for recognizing contexts where a cue does not imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have al"
J12-2004,W10-3012,0,0.0255504,"Missing"
J12-2004,W10-3002,0,0.214012,".e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁcation ¨ ur ¨ and Radev (2009) and Velldal (2010) match cues from a lexicon then approaches. Ozg apply a binary classiﬁer based on features describing the context of the cue c"
J12-2004,W10-3022,0,0.044217,"Missing"
J12-2004,W10-2605,0,0.0210482,"Missing"
J12-2004,W09-1419,0,0.0223659,"Missing"
J12-2004,W10-3007,0,0.0436532,"Missing"
J12-2004,W08-0606,1,0.908141,"Missing"
J12-2004,W10-3013,0,0.0602854,"arded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁcation ¨ ur ¨ and Radev (2009) and Velldal (2010) match cues from a lexicon then approaches. Ozg apply a binary classiﬁer based on features describing the context of the cue candidate. Each of th"
J12-2004,W10-3009,0,\N,Missing
J17-1004,J08-4004,0,0.0248364,"feel more comfortable cooperating, even in a competitive environment), whereas men are motivated to compete if necessary to achieve independence. 4.4 Annotation Study 2: Annotating Micro-structure of Arguments The goal of this study was to annotate documents on a detailed level with respect to an argumentation model. First, we will present the annotation scheme. Second, we will 13 Following the terminology proposed by Landis and Koch (1977, page 165), although they claim that the divisions are clearly arbitrary. For a detailed discussion on interpretation of agreement values, see for example Artstein and Poesio (2008). 14 We also experimented with different task definition before in the preliminary studies. However, identifying argumentative documents was misleading, as the annotators expected a reasonable argument. For instance, consider the following example: Doc#1247 (artcomment, prayer-in-schools): Keep church and state separate. Period. This is not an argumentative text in the traditional sense of giving reason, however, the persuasion is obvious. We are interested in all kinds of persuasive documents, not only in those that contain some clearly defined argument structures, as they can still contain u"
J17-1004,bal-saint-dizier-2010-towards,0,0.020306,"Missing"
J17-1004,I13-1041,0,0.0192302,"0.7) was not promising. 165 Computational Linguistics Volume 43, Number 1 Table 13 Additional metrics to evaluate the performance of argument component identification applied to the results of 10-fold cross-validation scenario (Table 9). *Measured only on a subset of the data (refer to Section 4.4.6). Macro-F1 Krippendorff’s αU Boundary similarity 0.60 0.16 0.25 0.48* 0.11 0.30 0.70 0.18 0.32 Human Baseline Best system were trained on newswire corpora, so one has to expect performance drop when applied to user-generated content. This is, however, a well-known issue in NLP (Foster et al. 2011; Baldwin et al. 2013; Eisenstein 2013). To obtain an impression of the actual performance of the system on the data, we also provide the complete output of our best performing system in one PDF document together with the gold annotations in the logos dimension side by side in the accompanying software package. We believe this will help the community to see the strengths of our model as well as possible limitations of our current approaches. 6. Conclusions Let us begin with summarizing answers to the research questions stated in the Introduction. First, as we showed in Section 4.4.2, existing argumentation theorie"
J17-1004,W14-2107,0,0.115443,"Missing"
J17-1004,P11-1151,0,0.0169046,"Missing"
J17-1004,P12-2041,0,0.012464,"on-argumentative segments). In the first step of their two-phase approach, Goudas et al. (2014) sampled the data set to be balanced and identified argumentative sentences with F1 = 0.77, using the maximum entropy classifier. For identifying premises, they used BIO encoding of tokens and achieved an F1 score of 0.42 using conditional random fields (CRFs). Saint-Dizier (2012) developed a Prolog engine using a lexicon of 1,300 words and a set of 78 hand-crafted rules with the focus on a particular argument structure, “reasons supporting conclusions,” in French. Taking the dialogical perspective, Cabrio and Villata (2012) built upon an argumentation framework proposed by Dung (1995), which models arguments within a graph structure and provides a reasoning mechanism for resolving accepted arguments. For identifying support and attack, they relied on existing research on textual entailment (Dagan et al. 2009), namely, using the off-the-shelf EDITS system. The test data were taken from a debate portal Debatepedia and covered 19 topics. Evaluation was performed in terms of measuring the acceptance of the “main argument” using the automatically recognized entailments, yielding an F1 score of about 0.75. In contrast"
J17-1004,E12-1085,0,0.0363952,"Missing"
J17-1004,W12-3810,0,0.0149746,"Missing"
J17-1004,W14-5201,1,0.557216,"Missing"
J17-1004,N13-1037,0,0.0114721,"g. 165 Computational Linguistics Volume 43, Number 1 Table 13 Additional metrics to evaluate the performance of argument component identification applied to the results of 10-fold cross-validation scenario (Table 9). *Measured only on a subset of the data (refer to Section 4.4.6). Macro-F1 Krippendorff’s αU Boundary similarity 0.60 0.16 0.25 0.48* 0.11 0.30 0.70 0.18 0.32 Human Baseline Best system were trained on newswire corpora, so one has to expect performance drop when applied to user-generated content. This is, however, a well-known issue in NLP (Foster et al. 2011; Baldwin et al. 2013; Eisenstein 2013). To obtain an impression of the actual performance of the system on the data, we also provide the complete output of our best performing system in one PDF document together with the gold annotations in the logos dimension side by side in the accompanying software package. We believe this will help the community to see the strengths of our model as well as possible limitations of our current approaches. 6. Conclusions Let us begin with summarizing answers to the research questions stated in the Introduction. First, as we showed in Section 4.4.2, existing argumentation theories do offer models"
J17-1004,P11-1099,0,0.552729,"to learn, not merely how to pass tests. Reasons against homeschooling • Keeping your kids away from knowledge that you don’t like is a moral crime. • Religious zealotry is no excuse for raising a kid devoid of a proper education. • Consciously depriving a child of an adequate education solely because ”father knows best,” or thinks he does, is tantamount to child abuse. Figure 2 Motivation example 2: Extracting evidence for a certain standpoint with respect to a given controversial topic. All statements are taken from the corpus introduced in this article. annotated resources is often unknown (Feng and Hirst 2011; Mochales and Moens 2011; Walton 2012; Villalba and Saint-Dizier 2012; Florou et al. 2013). Annotating and automatically analyzing arguments in unconstrained usergenerated Web discourse represent challenging tasks. So far, the research in argumentation mining “has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e., propositions with supporting reasons and evidence present in the text” (Park and ˇ Cardie 2014, page 29). Boltuˇzi´c and Snajder (2014, page 50) point out that “unlike in debates"
J17-1004,W13-2707,0,0.298027,"way from knowledge that you don’t like is a moral crime. • Religious zealotry is no excuse for raising a kid devoid of a proper education. • Consciously depriving a child of an adequate education solely because ”father knows best,” or thinks he does, is tantamount to child abuse. Figure 2 Motivation example 2: Extracting evidence for a certain standpoint with respect to a given controversial topic. All statements are taken from the corpus introduced in this article. annotated resources is often unknown (Feng and Hirst 2011; Mochales and Moens 2011; Walton 2012; Villalba and Saint-Dizier 2012; Florou et al. 2013). Annotating and automatically analyzing arguments in unconstrained usergenerated Web discourse represent challenging tasks. So far, the research in argumentation mining “has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e., propositions with supporting reasons and evidence present in the text” (Park and ˇ Cardie 2014, page 29). Boltuˇzi´c and Snajder (2014, page 50) point out that “unlike in debates or other more formal argumentation sources, the arguments provided by the users, if any, ar"
J17-1004,I11-1100,0,0.0307832,"Missing"
J17-1004,P13-1167,0,0.0123212,"low at first glance. One obvious reason is the actual performance of the system, which gives plenty of room for improvement in the future.40 But the main cause of low F1 numbers is the evaluation measure—using 11 classes on the token level is very strict, as it penalizes a mismatch in argument component boundaries the same way as a wrongly predicted argument component type. Therefore we also report two other evaluation metrics that help to put our results into context. r r Krippendorff’s αU —This was also used for evaluating inter-annotator agreement (see Section 4.4.6). Boundary similarity (Fournier 2013)—Using this metric, the problem is treated solely as a segmentation task without recognizing the argument component types. As shown in Table 13 (the Macro-F1 scores are repeated from Table 9), the bestperforming system achieves a 0.30 score using Krippendorf’s αU , which is in the middle between the baseline and human performance (0.48) but is considered poor from the inter-annotator agreement point of view (Artstein and Poesio 2008). The boundary similarity metrics are not directly suitable for evaluating argument component classification, but reveal a subtask of finding the component boundar"
J17-1004,W11-2030,0,0.0271962,"spaper editorials in French. A coarse-grained view on dialogs in social media was examined by Bracewell, Tomlinson, and Wang (2013), who proposed a set of 15 social acts (such as agreement, disagreement, or supportive behavior) to infer the social goals of dialog participants and presented a semi-supervised model for their classification. Their social act types were inspired by research in psychology and organizational behavior and were motivated by work in dialog understanding. They annotated a corpus in three languages using in-house annotators and achieved κ in the range from 0.13 to 0.53. Georgila et al. (2011) focused on cross-cultural aspects of persuasion or argumentation dialogs. They developed a novel annotation scheme stemming from different literature sources on negotiation and argumentation as well as from their original analysis of the phenomena. The annotation scheme is claimed to cover three dimensions of an utterance, namely, speech act, topic, and response or reference to a previous utterance. They annotated 21 dialogs and reached Krippendorff’s α between 0.38 and 0.57. Summary of Related Work Section. Given the broad landscape of various approaches to argument analysis and persuasion s"
J17-1004,W14-2106,0,0.0604068,"he claim and its premises, backings, etc.). Therefore we allow multiple arguments to be annotated in one document. At the same time, we restrained the annotators from creating complex argument hierarchies. Terminology. Toulmin’s grounds have an equivalent role to a premise in the classical view of an argument (Reed and Rowe 2006; van Eemeren et al. 2014) in terms that they offer the reasons why one should accept the standpoint expressed by the claim. As this terminology has been used in several related works in the argumentation mining field (Mochales and Moens 2011; Peldszus and Stede 2013b; Ghosh et al. 2014; Stab and Gurevych 2014a), we will keep this convention and denote the grounds as premises. The Role of Backing. One of the main critiques of the original Toulmin model was the vague distinction between grounds, warrant, and backing (Freeman 1991; Newman and Marshall 1991; Hitchcock 2003). The role of backing is to give additional support to the warrant, but there is no warrant in our model anymore. However, what we observed during the analysis was the presence of some additional evidence. Such evidence does not play the role of the grounds (premises) as it is not meant as a reason supporting"
J17-1004,D13-1191,0,0.0256546,"Missing"
J17-1004,D14-1012,0,0.0141328,"ttributions. Motivation: It has been claimed that discourse relations play a role in argumentation mining (Cabrio, Tonelli, and Villata 2013). – r – FS4: Embedding features 300 features from word embedding vectors using word embeddings trained on part of the Google News data set (Mikolov et al. 2013). In particular, we sum up embedding vectors (dimensionality 300) of each word, resulting in a single vector for the entire sentence. This vector is then directly used as a feature vector.35 Motivation: Embeddings helped to achieve state-of-the-art results in various NLP tasks (Socher et al. 2013; Guo et al. 2014). Except for the baseline lexical features, all feature types are extracted not only for the current sentence si , but also for C preceding and subsequent sentences, namely, si−C , si−C+1 , . . . si+C−1 , si+C , where C was empirically set to 4.36 Each feature is then represented with a prefix to determine its relative position to the current sequence unit.37 5.1.4 Results. Let us first discuss the upper bounds of the system. Performance of the three human annotators is shown in the first column of Table 9 (results are obtained 34 Explained in detail in annotation guidelines at http://clear.co"
J17-1004,D15-1255,1,0.836568,"to date. We choose several target domains from educational controversies, such as homeschooling, single-sex education, and mainstreaming.5 A novel aspect of the corpus is its coverage of different registers of user-generated Web content, such as comments on articles, discussion forum posts, blog posts, as well as professional newswire articles. Because the data come from a variety of sources and no assumptions about their actual content with respect to argumentation can be drawn, we conduct two extensive 4 We used the data set and core methods from this article in our subsequent publication (Habernal and Gurevych 2015). The main difference is that this article focuses mainly on corpus annotation, analysis, and argumentation on Web data in general, whereas in Habernal and Gurevych (2015) we explored whether methods for recognizing argument components benefit from using semi-supervised features obtained from noisy debate portals. 5 Controversial educational topics attract a wide range of participants, such as parents, journalists, education experts, policy makers, and students, which contributes to the linguistic breadth of the discourse. 128 Habernal and Gurevych Argumentation Mining in User-Generated Web Di"
J17-1004,C12-2045,0,0.0377042,"Missing"
J17-1004,I13-1191,0,0.00800312,"Missing"
J17-1004,J13-4004,0,0.0115272,"ositive) obtained from the Stanford sentiment analyzer (Socher et al. 2013). Motivation: Claims usually express opinions and carry sentiment. FS3: Semantic, coreference, and discourse features Binary features from Clear NLP Semantic Role Labeler (Choi 2012). Namely, we extract agent, predicate + agent, predicate + agent + patient + (optional) negation, argument type + argument value, and discourse marker, which are based on PropBank semantic role labels.34 Motivation: Exploit the semantics of capturing the semantics of the sentences. – Binary features from Stanford Coreference Chain Resolver (Lee et al. 2013), for example, presence of the sentence in a chain, transition type (i.e., nominal–pronominal), distance to previous/next sentences in the chain, or number of inter-sentence coreference links. Motivation: Presence of coreference chains indicates links outside the sentence and thus may be informative, for example, for classifying whether the sentence is a part of a larger argument component. – Results of a PTDB-style discourse parser (Lin, Ng, and Kan 2014), namely, the type of discourse relation (explicit, implicit), presence of discourse connectives, and attributions. Motivation: It has been"
J17-1004,llewellyn-etal-2014-using,0,0.0306487,"l flaws in argumentation of others. Satisfying these information needs cannot be directly tackled by current methods (e.g., opinion mining, question answering,2 or summarization3 ), and requires novel approaches within the argumentation mining field. Although user-generated Web content has already been considered in argumentation mining, many limitations and research gaps can be identified in the existing research. First, the scope of the current approaches is restricted to a particular domain or register—for example, hotel reviews (Wachsmuth et al. 2014), Tweets related to local riot events (Llewellyn et al. 2014), student essays (Stab and Gurevych 2014a), airline passenger rights and consumer protection (Park and Cardie 2014), or renewable energy sources (Goudas et al. 2014). Second, not all related works are tightly connected to argumentation theories, resulting in a gap between the substantial research in argumentation itself and its adaptation in natural language processing (NLP) applications. Third, as an emerging research area, argumentation mining still suffers from a lack of labeled corpora, which is crucial for designing, training, and evaluating the algorithms. Although some works have dealt"
J17-1004,N12-1003,0,0.0102461,"got into University. I do not feel disadvantaged at all. On the other hand some mentioned that those who are educated in the public schools are less educated. rest of that students schooling life fees gone. Whereas in a public school, its just the student gone. I have always gone to public schools and when I finished I got into University. I do not feel disadvantaged at all. Figure 1 Motivation example 1: Extracting argument gist by means of analyzing the argument structure and summarizing the argument components. The bold phrases are generated automatically and invoke the component function; Madnani et al. (2012) refers to these organizational elements as shells. Doc#4733, forum post, public-private schools. Reasons for homeschooling • Schools provide a totally unstimulating environment. • Lesson plans (and the national curriculum) are the death of real education. • Evidence including our own suggests strongly that this kind of education prepares children to enter further and higher education, or the workforce - and offers them the freedom to learn in the ways that suit them best. • We teach our children how to learn, not merely how to pass tests. Reasons against homeschooling • Keeping your kids away"
J17-1004,P14-5010,0,0.00202451,"rd Core NLP sentence 17 All of them came from the same source Web site, which does not support any HTML formatting of quotations. 18 We also considered sentences or clauses. The sentence level seems to be reasonable in most of the cases, however, it is too coarse-grained if a sentence contains multiple clauses that belong to different argumentation components. Segmentation to clauses is not trivial and has been considered as a separate task since CoNLL 2001 (Tjong, Sang, and D´ejean 2001). Best systems based on Join-CRF reach 0.81 F1 145 Computational Linguistics Volume 43, Number 1 splitter (Manning et al. 2014) embedded in the DKPro Core framework (Eckart de Castilho and Gurevych 2014). Annotators were asked to stick to the sentence level by default and label entire pre-segmented sentences. They should switch to annotations on the token level only if (a) a particular sentence contained more than one argument component, or (b) if the automatic sentence segmentation was wrong. Given the “noise” in user-generated Web data (wrong or missing punctuation, casing, etc.), this was often the case. Annotators were also asked to rephrase (summarize) each annotated argument component into a simple statement whe"
J17-1004,W14-2104,0,0.018188,"Missing"
J17-1004,W14-2105,0,0.521294,"(e.g., opinion mining, question answering,2 or summarization3 ), and requires novel approaches within the argumentation mining field. Although user-generated Web content has already been considered in argumentation mining, many limitations and research gaps can be identified in the existing research. First, the scope of the current approaches is restricted to a particular domain or register—for example, hotel reviews (Wachsmuth et al. 2014), Tweets related to local riot events (Llewellyn et al. 2014), student essays (Stab and Gurevych 2014a), airline passenger rights and consumer protection (Park and Cardie 2014), or renewable energy sources (Goudas et al. 2014). Second, not all related works are tightly connected to argumentation theories, resulting in a gap between the substantial research in argumentation itself and its adaptation in natural language processing (NLP) applications. Third, as an emerging research area, argumentation mining still suffers from a lack of labeled corpora, which is crucial for designing, training, and evaluating the algorithms. Although some works have dealt with creating new data sets, the reliability (in terms of inter-annotator agreement) of the 1 Despite few recent mu"
J17-1004,P11-1035,0,0.0536541,"Missing"
J17-1004,W14-2112,0,0.0372082,"ment (760 out of 16,000). They distinguished claims and premises, but the claims were always implicit. However, the annotation agreement was not reported; neither was the number of annotators or the guidelines. A study on annotation of arguments was conducted by Peldszus and Stede (2013b), who evaluate agreement among 26 “naive” annotators (annotators with very little training). They manually constructed 23 German short texts, each of them containing exactly one central claim, two premises, and one objection (rebuttal or undercut), and analyzed annotator agreement on this artificial data set. Peldszus (2014) later achieved higher inter-rater agreement with expert annotators on an extended version of the same data. Kluge (2014b) built a corpus of argumentative German Web documents, containing 79 documents from seven educational topics, which were annotated by three annotators according to the claim-premise argumentation model. The corpus comprises 70,000 tokens and the inter-annotator agreement was 0.40 (Krippendorff’s αU ). Houy et al. (2013) targeted argumentation mining of German legal cases. Table 1 gives an overview of annotation studies with their respective argumentation model, domain, size"
J17-1004,W13-2324,0,0.0874207,"Walton, Reed, and Macagno (2008) (top 5 schemes) proprietary 309 + 118 21 dialogs Cohen’s κ (0.69) Krippendorff’s α (0.37-0.56) 641 documents w/ 641 arguments (AracuariaDB) 67 documents w/ 257 arguments (EHRC) 256 arguments not reported blog posts, Wikipedia discussions editorials and blog post about ObamaCare 4000 sentences camera reviews N/A (proposal/position paper) N/A (proposal/position paper) 50 documents Cohen’s κ (0.50-0.57) Cohen’s κ (0.68) on 10 documents N/A political argumentation unspecified social media hotel reviews, hi-fi products, political campaign Potsdam Commentary Corpus Peldszus and Stede (2013a) Freeman (1991) + RST Florou et al. (2013) none public policy making Peldszus and Stede (2013b) based on Freeman (1991) Sergeant (2013) N/A not reported, artificial documents created for the study Car Review Corpus (CRC) Wachsmuth et al. (2014) Procter, Vis, and Voss (2013) Stab and Gurevych (2014a) Aharoni et al. (2014) Park and Cardie (2014) none hotel reviews proprietary (Claim, Counter-claim) Claim-Premise based on Freeman (1991) proprietary (claims, evidence) proprietary (argument propositions) Riot Twitter Corpus Goudas et al. (2014) Faulkner (2014) proprietary (premises) none (“suppor"
J17-1004,D15-1110,0,0.330859,"Missing"
J17-1004,prasad-etal-2008-penn,0,0.356579,"about 0.75. In contrast to our work, which deals with micro-level argumentation, Dung’s model is an abstract framework intended to model dialogical argumentation. Finding a bridge between existing discourse research and argumentation has been targeted by several researchers. Peldszus and Stede (2013a) surveyed literature on argumentation and proposed utilization of Rhetorical Structure Theory (RST) (Mann and Thompson 1987). They claimed that RST is by its design well-suited for studying argumentative texts, but an empirical evidence has not yet been provided. Penn Discourse Tree Bank (PDTB) (Prasad et al. 2008) relations have been under examination by argumentation mining researchers too. Cabrio, Tonelli, and Villata (2013) examined a connection between five of Walton’s schemes and discourse markers in PDTB: however, an empirical evaluation is missing. 135 Computational Linguistics Volume 43, Number 1 3.2 Stance Detection Research related to argumentation mining also involves stance detection. In this case, the whole document (discussion post, article) is assumed to represent the writer’s standpoint on the discussed topic. Because the topic is stated as a controversial question, the author is either"
J17-1004,N13-1123,0,0.00867556,"f stances in debate posts. Recent research has involved joint modeling, taking into account information about the users, the dialog sequences, and others. Hasan and Ng (2012) proposed a machine learning approach to debate stance classification by leveraging contextual information and author’s stances towards the topic. Qiu, Yang, and Jiang (2013) introduced a computational debate side model to cluster posts or users by sides for general threaded discussions using a generative graphical model utilizing words from various subjectivity lexicons as well as all adjectives and adverbs in the posts. Qiu and Jiang (2013) proposed a graphical model for viewpoint discovery in discussion threads. Burfoot, Bird, and Baldwin (2011) exploited the informal citation structure in U.S. Congressional floor-debate transcripts and used a collective classification, which outperforms methods that consider documents in isolation. Some works also utilize argumentation-motivated features. Park, Lee, and Song (2011) dealt with contentious issues in Korean newswire discourse. Although they annotate the documents with “argument frames,” the formalism remains unexplained and does not refer to any existing research in argumentation"
J17-1004,D13-1170,0,0.00198387,"Missing"
J17-1004,P09-1026,0,0.0244726,"under examination by argumentation mining researchers too. Cabrio, Tonelli, and Villata (2013) examined a connection between five of Walton’s schemes and discourse markers in PDTB: however, an empirical evaluation is missing. 135 Computational Linguistics Volume 43, Number 1 3.2 Stance Detection Research related to argumentation mining also involves stance detection. In this case, the whole document (discussion post, article) is assumed to represent the writer’s standpoint on the discussed topic. Because the topic is stated as a controversial question, the author is either for or against it. Somasundaran and Wiebe (2009) built a computational model for recognizing stances in dual-topic debates about named entities in the electronic products domain by combining preferences learned from the Web data and discourse markers from PDTB (Prasad et al. 2008). Hasan and Ng (2013) determined stance in online ideological debates on four topics using data from createdebate.com, utilizing supervised machine learning and features ranging from n-grams to semantic frames. Predicting stance of posts in Debatepedia as well as external articles using a probabilistic graphical model was presented in Gottipati et al. (2013). This"
J17-1004,C14-1142,1,0.121324,"sfying these information needs cannot be directly tackled by current methods (e.g., opinion mining, question answering,2 or summarization3 ), and requires novel approaches within the argumentation mining field. Although user-generated Web content has already been considered in argumentation mining, many limitations and research gaps can be identified in the existing research. First, the scope of the current approaches is restricted to a particular domain or register—for example, hotel reviews (Wachsmuth et al. 2014), Tweets related to local riot events (Llewellyn et al. 2014), student essays (Stab and Gurevych 2014a), airline passenger rights and consumer protection (Park and Cardie 2014), or renewable energy sources (Goudas et al. 2014). Second, not all related works are tightly connected to argumentation theories, resulting in a gap between the substantial research in argumentation itself and its adaptation in natural language processing (NLP) applications. Third, as an emerging research area, argumentation mining still suffers from a lack of labeled corpora, which is crucial for designing, training, and evaluating the algorithms. Although some works have dealt with creating new data sets, the reliabi"
J17-1004,D14-1006,1,0.792717,"Missing"
J17-1004,E99-1015,0,0.203362,"Missing"
J17-1004,J02-4002,0,0.12269,"reating new data sets, the reliability (in terms of inter-annotator agreement) of the 1 Despite few recent multi-modal approaches to process argumentation and persuasion (e.g., Brilman and Scherer 2015), the main mode of argumentation mining is natural language text. 2 These research fields are still related and complementary to argumentation mining. For example, personal decision-making queries (such as Should I homeschool my children?) might be tackled by research exploiting social question-answering sites. 3 The role of argumentation moves in summarizing scientific articles was examined by Teufel and Moens (2002). 126 Habernal and Gurevych Argumentation Mining in User-Generated Web Discourse Original text The public schooling system is not as bad as some may think. Some mentioned that those who are educated in the public schools are less educated, well I actually think it would be in the reverse. Student who study in the private sector actually pay a fair amount of fees to do so and I believe that the students actually get let off for a lot more than anyone would in a public school. And its all because of the money. In a private school, a student being expelled or suspended is not just one student out"
J17-1004,D09-1155,0,0.014661,"Missing"
J17-1004,W01-0708,0,0.0641376,"Missing"
J17-1004,W14-1305,0,0.0119859,"t task definition before in the preliminary studies. However, identifying argumentative documents was misleading, as the annotators expected a reasonable argument. For instance, consider the following example: Doc#1247 (artcomment, prayer-in-schools): Keep church and state separate. Period. This is not an argumentative text in the traditional sense of giving reason, however, the persuasion is obvious. We are interested in all kinds of persuasive documents, not only in those that contain some clearly defined argument structures, as they can still contain useful information for decision-making. Trabelsi and Zaiane (2014) defined a contentious document as a document that contains expressions of one or more divergent viewpoints in response to the contention question but they did not tackle the classification of these documents. Our task also resembles aspect-based sentiment analysis (ABSA), where the aspect in our case would be the controversial topic. However, in contrast to the research in ABSA, the aspects in our case are purely abstract entities and current approaches to model ABSA do not clearly fit our task. 140 Habernal and Gurevych Argumentation Mining in User-Generated Web Discourse describe the annota"
J17-1004,W14-4918,0,0.0417863,"Missing"
J17-1004,N12-1072,0,0.00650825,"roposed a graphical model for viewpoint discovery in discussion threads. Burfoot, Bird, and Baldwin (2011) exploited the informal citation structure in U.S. Congressional floor-debate transcripts and used a collective classification, which outperforms methods that consider documents in isolation. Some works also utilize argumentation-motivated features. Park, Lee, and Song (2011) dealt with contentious issues in Korean newswire discourse. Although they annotate the documents with “argument frames,” the formalism remains unexplained and does not refer to any existing research in argumentation. Walker et al. (2012) incorporated features with some limited aspects of the argument structure, such as cue words signaling rhetorical relations between posts, POS generalized dependencies, and a representation of the parent post (context) to improve stance classification over 14 topics from convinceme.net. 3.3 Online Persuasion Another stream of research has been devoted to persuasion in online media, which we consider as a more general research topic than argumentation. Schlosser (2011) investigated persuasiveness of online reviews and concluded that presenting two sides is not always more helpful and can even"
J17-1004,D13-1097,0,0.0109689,"der, but interpretation of such an averaged value has no evidence either in Krippendorff (2004) or other papers based upon it. Thus we use the other methodology and treat the whole corpus as a single long continuum (which yields in the example of claim = αU 0.541).19 Table 4 shows the inter-annotator agreement as measured on documents from the last annotation phase (see Section 4.4.4). The overall αU for all register types, topics, and argument components is 0.48 in the logos dimension (annotated with the score (Nguyen, Nguyen, and Shimazu 2009) for embedded clauses and 0.92 for non-embedded (Zhang et al. 2013). To the best of our knowledge, there is no available out-of-box solution for clause segmentation, thus we took sentences as another level of segmentation. Nevertheless, pre-segmenting the text to clauses and their relation to argument components deserves future investigation. 19 Another pitfall of the αU measure when documents are concatenated to create a single continuum is that its value depends on the order of the documents (the annotated spans, respectively). We did the following experiment: Using 10 random annotated documents, we created all 362,880 possible concatenations and measured t"
J17-1004,W10-3001,0,\N,Missing
J17-1004,W14-2109,0,\N,Missing
J17-3005,D15-1109,0,0.0187341,"isting approaches on discourse analysis mainly differ in the discourse theory utilized. RST (Mann and Thompson 1987), for instance, models discourse structures as trees by iteratively linking adjacent discourse units (Feng and Hirst 2014; Hernault et al. 2010) whereas approaches based on PDTB (Prasad et al. 2008) identify more shallow structures by linking two adjacent sentences or clauses (Lin, Ng, and Kan 2014). RST and PDTB are limited to discourse relations between adjacent discourse units, but SDRT (Asher and Lascarides 2003) also allows long distance relations (Afantenos and Asher 2014; Afantenos et al. 2015). However, similar to argumentation structure parsing, the main challenge of discourse analysis is to identify implicit discourse relations (Braud and Denis 2014, page 1694). Marcu and Echihabi (2002) proposed one of the first approaches for identifying implicit discourse relations. In order to collect large amounts of training data, they exploited several discourse markers like “because” or “but”. After removing the discourse markers, they found that word pair features are useful for identifying implicit discourse relations. Pitler, Louis, and Nenkova (2009) proposed an approach for identifyi"
J17-3005,W14-2109,0,0.0345479,"Missing"
J17-3005,J08-4004,0,0.0632227,"Missing"
J17-3005,W12-2007,0,0.0331239,"Missing"
J17-3005,Q13-1034,0,0.0244284,"Missing"
J17-3005,W14-2107,0,0.017069,"Missing"
J17-3005,C14-1160,0,0.0340552,"Missing"
J17-3005,W01-1605,0,0.0735083,"Missing"
J17-3005,W15-0504,0,0.0386124,"(van Eemeren, Grootendorst, and Snoeck Henkemans 1996, page 5). It is a routine that is omnipresent in our daily verbal communication and thinking. Wellreasoned arguments are not only important for decision making and learning but also play a crucial role in drawing widely accepted conclusions. Computational argumentation is a recent research field in computational linguistics that focuses on the analysis of arguments in natural language texts. Novel methods have broad application potential in various areas such as legal decision support (MochalesPalau and Moens 2009), information retrieval (Carstens and Toni 2015), policy making (Sardianos et al. 2015), and debating technologies (Levy et al. 2014; Rinott et al. 2015). Recently, computational argumentation has been receiving increased attention in computer-assisted writing (Song et al. 2014; Stab et al. 2014) because it allows the creation of writing support systems that provide feedback about written arguments. ∗ Technische Universit¨at Darmstadt, Ubiquitous Knowledge Processing (UKP) Lab, Hochschulstrasse 10, D-64289 Darmstadt, Germany. E-mail: stab@ukp.informatik.tu-darmstadt.de. ∗∗ Technische Universit¨at Darmstadt, Ubiquitous Knowledge Processing ("
J17-3005,W14-5201,1,0.315342,"Missing"
J17-3005,E12-1085,0,0.0240265,"Missing"
J17-3005,J87-1002,0,0.831301,"Missing"
J17-3005,W02-1001,0,0.0356937,"s of all three annotator pairs on our test data. 5.2 Identifying Argument Components We consider the identification of argument components as a sequence labeling task at the token level. We encode the argument components using an IOB-tagset (Ramshaw and Marcus 1995) and consider an entire essay as a single sequence. Accordingly, we label the first token of each argument component as “Arg-B”, the tokens covered by an argument component as “Arg-I”, and non-argumentative tokens as “O”. As a learner, we use a CRF (Lafferty, McCallum, and Pereira 2001) with the averaged perceptron training method (Collins 2002). Because a CRF considers contextual information, the model is particularly suited for sequence labeling tasks (Goudas et al. 2014, page 292). For each token, we extract the following features (Table 7): Structural features capture the position of the token. We expect these features to be effective for filtering non-argumentative text units, since the introductions and conclusions of essays include few argumentatively relevant content. The punctuation features indicate if the token is a punctuation and if the token is adjacent to a punctuation. Syntactic features consist of the token’s POS as"
J17-3005,J03-4003,0,0.0293111,"lowing and preceding token in the parse tree The two constituent types of the LCA of the current token and its preceding and following token LexSyn Lexico-syntactic Combination of lexical and syntactic features as described by Soricut and Marcu (2003) Prob Probability Conditional probability of the current token being the beginning of a component given its preceding tokens Lexico-syntactic features have been shown to be effective for segmenting elementary discourse units (Hernault et al. 2010). We adopt the features introduced by Soricut and Marcu (2003). We use lexical head projection rules (Collins 2003) implemented in the Stanford tool suite to lexicalize the constituent parse tree. For each token t, we extract its uppermost node n in the parse tree with the lexical head t and define a lexicosyntactic feature as the combination of t and the constituent type of n. We also consider the child node of n in the path to t and its right sibling, and combine their lexical heads and constituent types as described by Soricut and Marcu (2003). The probability feature is the conditional probability of the current token ti being the beginning of an argument component (“Arg-B”) given its preceding tokens."
J17-3005,P14-5011,1,0.217009,"DKPro Framework (Eckart de Castilho and Gurevych 2014). We identify tokens and sentence boundaries using the LanguageTool segmenter6 and identify paragraphs by checking for line breaks. We lemmatize each token using the Mate Tools lemmatizer (Bohnet et al. 2013) and apply the Stanford part-of-speech (POS) tagger (Toutanova et al. 2003), constituent and dependency parsers (Klein and Manning 2003), and sentiment analyzer (Socher et al. 2013). We use a discourse parser from Lin, Ng, and Kan (2014) for recognizing PDTB-style discourse relations. We use the DKPro TC text classification framework (Daxenberger et al. 2014) for feature extraction and experimentation. In the following sections, we describe each model in detail. For finding the bestperforming models, we conduct model selection on our training data using 5-fold crossvalidation. Then, we conduct model assessment on our test data. We determine the evaluation scores of each cross-validation experiment by accumulating the confusion matrices of each fold into one confusion matrix, which has been shown to be the least biased method for evaluating cross-validation experiments (Forman and Scholz 2010). We use macro-averaging as described by Sokolova and La"
J17-3005,P11-1099,0,0.00959024,"ourse analysis. In Section 3, we derive our annotation scheme from argumentation theory. Section 4 presents the results of an annotation study and the corpus creation. In Section 5, we introduce the argumentation structure parser. We show that our model significantly outperforms challenging heuristic baselines on two different types of discourse. We discuss our results in Section 6, and provide our conclusions in Section 7. 2. Related Work Existing work in computational argumentation addresses a variety of different tasks. These include, for example, approaches for identifying reasoning type (Feng and Hirst 2011), argumentation style (Oraby et al. 2015), the stance of the author (Hasan and Ng 2014; Somasundaran and Wiebe 2009), the acceptability of arguments (Cabrio and Villata 2012), and appropriate support types (Park and Cardie 2014). Most relevant to our work, however, are approaches on argument mining that focus on the identification of argumentation structures in natural language texts. We categorize related approaches into the following three subtasks: r r r Component identification focuses on the separation of argumentative from non-argumentative text units and the identification of argument c"
J17-3005,P14-1048,0,0.020929,"provides an overview of existing corpora annotated with argumentation structures at the discourse-level. 2.5 Discourse Analysis The identification of argumentation structures is closely related to discourse analysis. Similar to the identification of argumentation structures, discourse analysis aims at identifying elementary discourse units and discourse relations between them. Existing approaches on discourse analysis mainly differ in the discourse theory utilized. RST (Mann and Thompson 1987), for instance, models discourse structures as trees by iteratively linking adjacent discourse units (Feng and Hirst 2014; Hernault et al. 2010) whereas approaches based on PDTB (Prasad et al. 2008) identify more shallow structures by linking two adjacent sentences or clauses (Lin, Ng, and Kan 2014). RST and PDTB are limited to discourse relations between adjacent discourse units, but SDRT (Asher and Lascarides 2003) also allows long distance relations (Afantenos and Asher 2014; Afantenos et al. 2015). However, similar to argumentation structure parsing, the main challenge of discourse analysis is to identify implicit discourse relations (Braud and Denis 2014, page 1694). Marcu and Echihabi (2002) proposed one o"
J17-3005,W13-2707,0,0.767627,"es a claim and one or more premises (Govier 2010). The claim is a controversial statement and the central component of an argument, and premises are reasons for justifying (or refuting) the claim. Moreover, arguments have directed argumentative relations, describing the relationships one component has with another. Each such relation indicates that the source component is either a justification for or a refutation of the target component. The identification of argumentation structures involves several subtasks like separating argumentative from non-argumentative text units (Moens et al. 2007; Florou et al. 2013), classifying argument components into claims and premises (MochalesPalau and Moens 2011; Rooney, Wang, and Browne 2012; Stab and Gurevych 2014b), and identifying argumentative relations (Mochales-Palau and Moens 2009; Peldszus 2014; Stab and Gurevych 2014b). However, an approach that covers all subtasks is still missing. Furthermore, most approaches operate locally and do not optimize the global argumentation structure. Recently, Peldszus and Stede (2015) proposed an approach based on Minimum Spanning Trees, which jointly models argumentation structures. However, it links all argument compone"
J17-3005,W14-2106,0,0.0258095,"core of 0.726 using structural, lexical, syntactic, indicator, and contextual features. Recently, Nguyen and Litman (2015) found that argument and domain words from unlabeled data increase F1 score to 0.76 in the same experimental setup, and Lippi and Torroni (2015) achieved an F1 score of 0.714 for identifying sentences containing a claim in student essays using partial tree kernels. 2.3 Structure Identification Approaches on structure identification can be divided into macro-level approaches and micro-level approaches. Macro-level approaches such as presented by Cabrio and ˇ Villata (2012), Ghosh et al. (2014), or Boltuˇzi´c and Snajder (2014) address relations between complete arguments and ignore the microstructure of arguments. More relevant to our work, however, are micro-level approaches, which focus on relations between argument components. Mochales-Palau and Moens (2009) introduced one of the first approaches for identifying the microstructure of arguments. Their approach is based on a manually created Context-Free Grammar and recognizes argument structures as trees. However, it is tailored to legal argumentation and does not recognize implicit argumentative relations (i.e., relations that a"
J17-3005,J17-1004,1,0.419043,"Missing"
J17-3005,D14-1083,0,0.023643,"Missing"
J17-3005,W15-0501,1,0.610487,"Missing"
J17-3005,P03-1054,0,0.0405116,"model differentiates between support and attack relations. 634 Stab and Gurevych Parsing Argumentation Structures Figure 3 Architecture of the argumentation structure parser. For preprocessing, we use several models from the DKPro Framework (Eckart de Castilho and Gurevych 2014). We identify tokens and sentence boundaries using the LanguageTool segmenter6 and identify paragraphs by checking for line breaks. We lemmatize each token using the Mate Tools lemmatizer (Bohnet et al. 2013) and apply the Stanford part-of-speech (POS) tagger (Toutanova et al. 2003), constituent and dependency parsers (Klein and Manning 2003), and sentiment analyzer (Socher et al. 2013). We use a discourse parser from Lin, Ng, and Kan (2014) for recognizing PDTB-style discourse relations. We use the DKPro TC text classification framework (Daxenberger et al. 2014) for feature extraction and experimentation. In the following sections, we describe each model in detail. For finding the bestperforming models, we conduct model selection on our training data using 5-fold crossvalidation. Then, we conduct model assessment on our test data. We determine the evaluation scores of each cross-validation experiment by accumulating the confusion"
J17-3005,C14-1141,0,0.188783,"Missing"
J17-3005,D09-1036,0,0.0132953,"Missing"
J17-3005,W10-4310,0,0.0227701,"l discourse markers like “because” or “but”. After removing the discourse markers, they found that word pair features are useful for identifying implicit discourse relations. Pitler, Louis, and Nenkova (2009) proposed an approach for identifying four implicit types of discourse relations in the PDTB and achieved F1 scores between 0.22 and 0.76. They found that using features tailored to each individual relation leads to the best results. Lin, Kan, and Ng (2009) showed that production rules collected from 624 Stab and Gurevych Parsing Argumentation Structures parse trees yield good results and Louis et al. (2010) found that features based on named entities do not perform as well as lexical features. Approaches to discourse analysis usually aim at identifying various different types of discourse relations. However, only a subset of these relations is relevant for argumentation structure parsing. For example, Peldszus and Stede (2013) proposed support, attack, and counter-attack relations for modeling argumentation structures, whereas our work focuses on support and attack relations. This difference is also illustrated by the work of Biran and Rambow (2011). They selected a subset of 12 relations from t"
J17-3005,P02-1047,0,0.0217048,"ent discourse units (Feng and Hirst 2014; Hernault et al. 2010) whereas approaches based on PDTB (Prasad et al. 2008) identify more shallow structures by linking two adjacent sentences or clauses (Lin, Ng, and Kan 2014). RST and PDTB are limited to discourse relations between adjacent discourse units, but SDRT (Asher and Lascarides 2003) also allows long distance relations (Afantenos and Asher 2014; Afantenos et al. 2015). However, similar to argumentation structure parsing, the main challenge of discourse analysis is to identify implicit discourse relations (Braud and Denis 2014, page 1694). Marcu and Echihabi (2002) proposed one of the first approaches for identifying implicit discourse relations. In order to collect large amounts of training data, they exploited several discourse markers like “because” or “but”. After removing the discourse markers, they found that word pair features are useful for identifying implicit discourse relations. Pitler, Louis, and Nenkova (2009) proposed an approach for identifying four implicit types of discourse relations in the PDTB and achieved F1 scores between 0.22 and 0.76. They found that using features tailored to each individual relation leads to the best results. L"
J17-3005,C14-2023,1,0.555045,"Missing"
J17-3005,W15-0503,0,0.0199262,"ined an accuracy of 0.65. Mochales-Palau and Moens (2011) classified sentences in legal decisions as claim or premise. They achieved an F1 score of 0.741 for claims and 0.681 for premises using a Support Vector Machine (SVM) with domain-dependent key phrases, text statistics, verbs, and the tense of the sentence. In our previous work, we used a multiclass SVM for labeling text units of student essays as major claim, claim, premise, or nonargumentative (Stab and Gurevych 2014b). We obtained an F1 score of 0.726 using structural, lexical, syntactic, indicator, and contextual features. Recently, Nguyen and Litman (2015) found that argument and domain words from unlabeled data increase F1 score to 0.76 in the same experimental setup, and Lippi and Torroni (2015) achieved an F1 score of 0.714 for identifying sentences containing a claim in student essays using partial tree kernels. 2.3 Structure Identification Approaches on structure identification can be divided into macro-level approaches and micro-level approaches. Macro-level approaches such as presented by Cabrio and ˇ Villata (2012), Ghosh et al. (2014), or Boltuˇzi´c and Snajder (2014) address relations between complete arguments and ignore the microstr"
J17-3005,W15-0515,0,0.0100196,"annotation scheme from argumentation theory. Section 4 presents the results of an annotation study and the corpus creation. In Section 5, we introduce the argumentation structure parser. We show that our model significantly outperforms challenging heuristic baselines on two different types of discourse. We discuss our results in Section 6, and provide our conclusions in Section 7. 2. Related Work Existing work in computational argumentation addresses a variety of different tasks. These include, for example, approaches for identifying reasoning type (Feng and Hirst 2011), argumentation style (Oraby et al. 2015), the stance of the author (Hasan and Ng 2014; Somasundaran and Wiebe 2009), the acceptability of arguments (Cabrio and Villata 2012), and appropriate support types (Park and Cardie 2014). Most relevant to our work, however, are approaches on argument mining that focus on the identification of argumentation structures in natural language texts. We categorize related approaches into the following three subtasks: r r r Component identification focuses on the separation of argumentative from non-argumentative text units and the identification of argument component boundaries. Component classifica"
J17-3005,W14-2105,0,0.0283955,"er. We show that our model significantly outperforms challenging heuristic baselines on two different types of discourse. We discuss our results in Section 6, and provide our conclusions in Section 7. 2. Related Work Existing work in computational argumentation addresses a variety of different tasks. These include, for example, approaches for identifying reasoning type (Feng and Hirst 2011), argumentation style (Oraby et al. 2015), the stance of the author (Hasan and Ng 2014; Somasundaran and Wiebe 2009), the acceptability of arguments (Cabrio and Villata 2012), and appropriate support types (Park and Cardie 2014). Most relevant to our work, however, are approaches on argument mining that focus on the identification of argumentation structures in natural language texts. We categorize related approaches into the following three subtasks: r r r Component identification focuses on the separation of argumentative from non-argumentative text units and the identification of argument component boundaries. Component classification addresses the function of argument components. It aims at classifying argument components into different types such as claims and premises. Structure identification focuses on linkin"
J17-3005,W14-2112,0,0.0336393,"tative relations, describing the relationships one component has with another. Each such relation indicates that the source component is either a justification for or a refutation of the target component. The identification of argumentation structures involves several subtasks like separating argumentative from non-argumentative text units (Moens et al. 2007; Florou et al. 2013), classifying argument components into claims and premises (MochalesPalau and Moens 2011; Rooney, Wang, and Browne 2012; Stab and Gurevych 2014b), and identifying argumentative relations (Mochales-Palau and Moens 2009; Peldszus 2014; Stab and Gurevych 2014b). However, an approach that covers all subtasks is still missing. Furthermore, most approaches operate locally and do not optimize the global argumentation structure. Recently, Peldszus and Stede (2015) proposed an approach based on Minimum Spanning Trees, which jointly models argumentation structures. However, it links all argument components in a single tree structure. Consequently, it is not capable of splitting a text containing more than one argument. In addition to the lack of end-to-end approaches for parsing argumentation structures, there are relatively few c"
J17-3005,D15-1110,0,0.0408674,"Missing"
J17-3005,P15-1053,0,0.0443914,"Missing"
J17-3005,P09-1077,0,0.0196299,"Missing"
J17-3005,prasad-etal-2008-penn,0,0.03264,"ßstraße 29, D-60486 Frankfurt am Main, Germany. Submission received: 26 October 2015; revised version received: 27 February 2017; accepted for publication: 10 April 2017. doi:10.1162/COLI a 00295 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 3 Argumentation structures are closely related to discourse structures such as those defined by Rhetorical Structure Theory (RST) (Mann and Thompson 1987), the Penn Discourse Treebank (PDTB) (Prasad et al. 2008), or Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides 2003). The internal structure of an argument consists of several argument components. It includes a claim and one or more premises (Govier 2010). The claim is a controversial statement and the central component of an argument, and premises are reasons for justifying (or refuting) the claim. Moreover, arguments have directed argumentative relations, describing the relationships one component has with another. Each such relation indicates that the source component is either a justification for or a refutation of the targ"
J17-3005,W95-0107,0,0.382593,"The heuristic baseline of the stance recognition classifies each argument component in the second to last paragraph as attack. The motivation for this baseline stems from essay writing guidelines, which recommend including opposing arguments in the second to last paragraph. We determine the human upper bound for each task by averaging the evaluation scores of all three annotator pairs on our test data. 5.2 Identifying Argument Components We consider the identification of argument components as a sequence labeling task at the token level. We encode the argument components using an IOB-tagset (Ramshaw and Marcus 1995) and consider an entire essay as a single sequence. Accordingly, we label the first token of each argument component as “Arg-B”, the tokens covered by an argument component as “Arg-I”, and non-argumentative tokens as “O”. As a learner, we use a CRF (Lafferty, McCallum, and Pereira 2001) with the averaged perceptron training method (Collins 2002). Because a CRF considers contextual information, the model is particularly suited for sequence labeling tasks (Goudas et al. 2014, page 292). For each token, we extract the following features (Table 7): Structural features capture the position of the t"
J17-3005,reed-etal-2008-language,0,0.0861317,"Missing"
J17-3005,D15-1050,0,0.0904793,"aily verbal communication and thinking. Wellreasoned arguments are not only important for decision making and learning but also play a crucial role in drawing widely accepted conclusions. Computational argumentation is a recent research field in computational linguistics that focuses on the analysis of arguments in natural language texts. Novel methods have broad application potential in various areas such as legal decision support (MochalesPalau and Moens 2009), information retrieval (Carstens and Toni 2015), policy making (Sardianos et al. 2015), and debating technologies (Levy et al. 2014; Rinott et al. 2015). Recently, computational argumentation has been receiving increased attention in computer-assisted writing (Song et al. 2014; Stab et al. 2014) because it allows the creation of writing support systems that provide feedback about written arguments. ∗ Technische Universit¨at Darmstadt, Ubiquitous Knowledge Processing (UKP) Lab, Hochschulstrasse 10, D-64289 Darmstadt, Germany. E-mail: stab@ukp.informatik.tu-darmstadt.de. ∗∗ Technische Universit¨at Darmstadt, Ubiquitous Knowledge Processing (UKP) Lab, Hochschulstrasse 10, D-64289 Darmstadt, Germany and Ubiquitous Knowledge Processing Lab (UKP-DI"
J17-3005,D13-1170,0,0.0137456,"elations. 634 Stab and Gurevych Parsing Argumentation Structures Figure 3 Architecture of the argumentation structure parser. For preprocessing, we use several models from the DKPro Framework (Eckart de Castilho and Gurevych 2014). We identify tokens and sentence boundaries using the LanguageTool segmenter6 and identify paragraphs by checking for line breaks. We lemmatize each token using the Mate Tools lemmatizer (Bohnet et al. 2013) and apply the Stanford part-of-speech (POS) tagger (Toutanova et al. 2003), constituent and dependency parsers (Klein and Manning 2003), and sentiment analyzer (Socher et al. 2013). We use a discourse parser from Lin, Ng, and Kan (2014) for recognizing PDTB-style discourse relations. We use the DKPro TC text classification framework (Daxenberger et al. 2014) for feature extraction and experimentation. In the following sections, we describe each model in detail. For finding the bestperforming models, we conduct model selection on our training data using 5-fold crossvalidation. Then, we conduct model assessment on our test data. We determine the evaluation scores of each cross-validation experiment by accumulating the confusion matrices of each fold into one confusion mat"
J17-3005,N13-1068,0,0.0117303,"st data. We determine the evaluation scores of each cross-validation experiment by accumulating the confusion matrices of each fold into one confusion matrix, which has been shown to be the least biased method for evaluating cross-validation experiments (Forman and Scholz 2010). We use macro-averaging as described by Sokolova and Lapalme (2009) and report macro precision (P), macro recall (R), and macro F1 scores (F1). We use a two-sided Wilcoxon signed-rank test with p = 0.01 for significance testing. Because most evaluation measures for comparing system outputs are not normally distributed (Søgaard 2013), this non-parametric test is preferable to parametric tests, which make stronger assumptions about the underlying distribution of the random variables. We apply this test to all reported evaluation scores obtained for each of the 80 essays in our test set. The remainder of this section is structured as follows: In the following section, we introduce the baselines and the upper bound for each task. In Section 5.2, we present the identification model that detects argument components and their boundaries. In Section 5.3, we propose a new joint model for identifying argumentation structures. In S"
J17-3005,P09-1026,0,0.0101293,"the results of an annotation study and the corpus creation. In Section 5, we introduce the argumentation structure parser. We show that our model significantly outperforms challenging heuristic baselines on two different types of discourse. We discuss our results in Section 6, and provide our conclusions in Section 7. 2. Related Work Existing work in computational argumentation addresses a variety of different tasks. These include, for example, approaches for identifying reasoning type (Feng and Hirst 2011), argumentation style (Oraby et al. 2015), the stance of the author (Hasan and Ng 2014; Somasundaran and Wiebe 2009), the acceptability of arguments (Cabrio and Villata 2012), and appropriate support types (Park and Cardie 2014). Most relevant to our work, however, are approaches on argument mining that focus on the identification of argumentation structures in natural language texts. We categorize related approaches into the following three subtasks: r r r Component identification focuses on the separation of argumentative from non-argumentative text units and the identification of argument component boundaries. Component classification addresses the function of argument components. It aims at classifying"
J17-3005,W14-2110,0,0.0628417,"play a crucial role in drawing widely accepted conclusions. Computational argumentation is a recent research field in computational linguistics that focuses on the analysis of arguments in natural language texts. Novel methods have broad application potential in various areas such as legal decision support (MochalesPalau and Moens 2009), information retrieval (Carstens and Toni 2015), policy making (Sardianos et al. 2015), and debating technologies (Levy et al. 2014; Rinott et al. 2015). Recently, computational argumentation has been receiving increased attention in computer-assisted writing (Song et al. 2014; Stab et al. 2014) because it allows the creation of writing support systems that provide feedback about written arguments. ∗ Technische Universit¨at Darmstadt, Ubiquitous Knowledge Processing (UKP) Lab, Hochschulstrasse 10, D-64289 Darmstadt, Germany. E-mail: stab@ukp.informatik.tu-darmstadt.de. ∗∗ Technische Universit¨at Darmstadt, Ubiquitous Knowledge Processing (UKP) Lab, Hochschulstrasse 10, D-64289 Darmstadt, Germany and Ubiquitous Knowledge Processing Lab (UKP-DIPF), German Institute for Educational Research, Schloßstraße 29, D-60486 Frankfurt am Main, Germany. Submission received: 26"
J17-3005,N03-1030,0,0.099866,"Missing"
J17-3005,C14-1142,1,0.751415,"Missing"
J17-3005,D14-1006,1,0.90135,"Missing"
J17-3005,E12-2021,0,0.0825473,"Missing"
J17-3005,N03-1033,0,0.0382759,"al ones) in each paragraph. Finally, the stance recognition model differentiates between support and attack relations. 634 Stab and Gurevych Parsing Argumentation Structures Figure 3 Architecture of the argumentation structure parser. For preprocessing, we use several models from the DKPro Framework (Eckart de Castilho and Gurevych 2014). We identify tokens and sentence boundaries using the LanguageTool segmenter6 and identify paragraphs by checking for line breaks. We lemmatize each token using the Mate Tools lemmatizer (Bohnet et al. 2013) and apply the Stanford part-of-speech (POS) tagger (Toutanova et al. 2003), constituent and dependency parsers (Klein and Manning 2003), and sentiment analyzer (Socher et al. 2013). We use a discourse parser from Lin, Ng, and Kan (2014) for recognizing PDTB-style discourse relations. We use the DKPro TC text classification framework (Daxenberger et al. 2014) for feature extraction and experimentation. In the following sections, we describe each model in detail. For finding the bestperforming models, we conduct model selection on our training data using 5-fold crossvalidation. Then, we conduct model assessment on our test data. We determine the evaluation scores of e"
J17-3005,P02-1053,0,0.0512272,"Missing"
J17-3005,walker-etal-2012-corpus,0,0.0137113,"Missing"
J17-3005,J90-1003,0,\N,Missing
J17-3005,W15-0508,0,\N,Missing
K18-1006,P98-1013,0,0.484749,"ank SRL is based on the PropBank corpus (Palmer et al., 2005) which utilizes a set of predicate-specific core roles (A0-5) and a set 55 tem (Roth and Woodsend, 2014) designed with PropBank generalization level in mind. of general, predicate-independent adjunct roles (AM-TMP, AM-LOC etc.). Core roles are defined on verb sense level. An effort is made to ensure consistency in assigning A0 (Agent-like) and A1 (Patient-like). The rest of the core arguments (A2-5) are verb sense-specific; no finer-grained distinctions between roles are made. PropBank annotation is closely tied to syntax. FrameNet (Baker et al., 1998) takes a different stance and focuses on accurate and detailed representation of event semantics. Verbs (as well as lexemes from other categories) are grouped into frames so that members of the same frame share a set of fine-grained frame-specific semantic roles (e.g. Impactee, Force, Buyer, Goods). Both PropBank and FrameNet SRL operate on the verb sense/verb class generalization level. VerbNet (Schuler, 2006) groups verbs into Levininspired verb classes and defines sets of general, lexicon-level thematic roles and constructions for each class. It is the only SRL formalism that operates with"
K18-1006,W13-5503,0,0.0368382,"Missing"
K18-1006,burchardt-etal-2006-salsa,0,0.0328381,"on and the corresponding SemLink reference are constituents-based. However, UD is a dependency formalism, and we employ a number of heuristics to align original PropBank annotations with the CoNLL-2009 datasets (Hajiˇc et al., 2009) to recover the head node positions. We employ additional transformations, filtering out the predications in which not all PropBank core roles got aligned to the VerbNet thematic roles. For German, we use the recently introduced SR3de dataset (M´ujdricza-Maydt et al., 2016; Hartmann et al., 2017) which explicitly provides VerbNet annotations on top of SALSA corpus (Burchardt et al., 2006). There exist no gold UD annotations for the SALSA corpus, and we use the SALSA’s default TIGER syntactic formalism (Dipper et al., 2001) in our experiments. Following previous work, we employ certain restrictions on our data. Since thematic roles in both VerbNet and SR3de are only defined for verbal predicates, we restrict the scope of our study to verbs. We only consider direct dependents of the verbs in active voice, and since having access to the full argument set is important to study conGlobal ranker The pairwise ranking approach takes context into account. However, some role pairs only"
K18-1006,N10-1138,0,0.0121472,"not generalize to the out-of-vocabulary (OOV) predicates. A step towards a more general representation is verb class grouping (Levin, 1993): verbs senses can be grouped into verb classes with syntactic behavior shared among the members of the class. For example, syntactically break behaves like crash, shred and split, while hit behaves like bash and whack in the corresponding verb senses. This significantly reduces the lexicon redundancy and allows treatment of the OOV verbs if the verb class can be determined. A similar level of granularity is used by the major SRL frameworks: FrameNet SRL (Das et al., 2010) and, to some extent, PropBank SRL (Roth and Woodsend, 2014). Semantic arguments share similarities across verb classes, giving rise to the notion of gen2.2 Major SRL Frameworks The choice of linguistic theory in SRL is mostly dictated by the availability of training data. PropBank SRL is based on the PropBank corpus (Palmer et al., 2005) which utilizes a set of predicate-specific core roles (A0-5) and a set 55 tem (Roth and Woodsend, 2014) designed with PropBank generalization level in mind. of general, predicate-independent adjunct roles (AM-TMP, AM-LOC etc.). Core roles are defined on verb"
K18-1006,L16-1484,1,0.897043,"Missing"
K18-1006,L16-1262,0,0.0615464,"Missing"
K18-1006,W17-0814,1,0.85886,"Missing"
K18-1006,P17-1044,0,0.0229624,"n et al., 2010). The goal of SRL is to label the semantic arguments of a predicate (e.g. a verb) with roles from a pre-defined role inventory. Conceptually, role assignment in SRL can be split in two steps: local labeling estimates the likelihood of a certain semantic argument bearing a certain role; global optimization takes context-dependent role interactions into account and enforces certain theoretically motivated constraints (e.g. “each role must appear only once per predication”). State of the art in SRL is held by the systems based on deep neural networks (Marcheggiani and Titov, 2017; He et al., 2017). While achieving remarkable quality on benchmark datasets, modern systems show a considerable ≈10-point performance drop when applied out-of-domain. This • We suggest a method for global thematic hierarchy induction from corpus data; 1 We use ≺ for rank precedence, and / for ties 54 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 54–64 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics • We propose several thematic and syntactic ranking models and evaluate them on English and German data; • We sh"
K18-1006,J05-1004,0,0.727338,"not present, e.g. “[John]Ag broke the window with a [hammer]In ” → “A [hammer]In broke the window”. THs have received considerable attention in linguistic literature, but were so far impractical for use in NLP and SRL due to incompatibility and limited scope of the existing hierarchies. As a first step towards including THs into the NLP tool inventory we suggest an empirical framework for inducing THs from role-annotated corpora. Since VerbNet (Schuler, 2006) is the only SRL framework that operates with thematic roles, we choose it as our basis and perform experiments on the PropBank corpus (Palmer et al., 2005) enriched with VerbNet role labels via SemLink (Bonial et al., 2013). The contributions of this paper are as follows: Thematic role hierarchy is a linguistic tool used to describe interactions between semantic roles and their syntactic realizations. Despite decades of dedicated research and numerous thematic hierarchy suggestions in the literature, this concept has not been used in NLP so far due to incompatibility and limited scope of existing hierarchies. We introduce an empirical framework for thematic hierarchy induction and evaluate several role ranking strategies on English and German co"
K18-1006,peterson-etal-2014-focusing,0,0.0509849,"Missing"
K18-1006,C10-1081,0,0.0160762,"isting hierarchies. We introduce an empirical framework for thematic hierarchy induction and evaluate several role ranking strategies on English and German corpus data. We hypothesize that inducing a thematic hierarchy is feasible, that a hierarchy can be induced from small amounts of data and that resulting hierarchies apply cross-lingually. We evaluate these assumptions empirically. 1 Introduction Semantic roles are one of the core concepts in NLP, and automatic semantic role labeling (SRL) is a major task with applications in question answering (Shen and Lapata, 2007), machine translation (Liu and Gildea, 2010) and information extraction (Christensen et al., 2010). The goal of SRL is to label the semantic arguments of a predicate (e.g. a verb) with roles from a pre-defined role inventory. Conceptually, role assignment in SRL can be split in two steps: local labeling estimates the likelihood of a certain semantic argument bearing a certain role; global optimization takes context-dependent role interactions into account and enforces certain theoretically motivated constraints (e.g. “each role must appear only once per predication”). State of the art in SRL is held by the systems based on deep neural n"
K18-1006,D14-1045,0,0.0186632,"ates. A step towards a more general representation is verb class grouping (Levin, 1993): verbs senses can be grouped into verb classes with syntactic behavior shared among the members of the class. For example, syntactically break behaves like crash, shred and split, while hit behaves like bash and whack in the corresponding verb senses. This significantly reduces the lexicon redundancy and allows treatment of the OOV verbs if the verb class can be determined. A similar level of granularity is used by the major SRL frameworks: FrameNet SRL (Das et al., 2010) and, to some extent, PropBank SRL (Roth and Woodsend, 2014). Semantic arguments share similarities across verb classes, giving rise to the notion of gen2.2 Major SRL Frameworks The choice of linguistic theory in SRL is mostly dictated by the availability of training data. PropBank SRL is based on the PropBank corpus (Palmer et al., 2005) which utilizes a set of predicate-specific core roles (A0-5) and a set 55 tem (Roth and Woodsend, 2014) designed with PropBank generalization level in mind. of general, predicate-independent adjunct roles (AM-TMP, AM-LOC etc.). Core roles are defined on verb sense level. An effort is made to ensure consistency in assi"
K18-1006,D17-1159,0,0.0134671,"rmation extraction (Christensen et al., 2010). The goal of SRL is to label the semantic arguments of a predicate (e.g. a verb) with roles from a pre-defined role inventory. Conceptually, role assignment in SRL can be split in two steps: local labeling estimates the likelihood of a certain semantic argument bearing a certain role; global optimization takes context-dependent role interactions into account and enforces certain theoretically motivated constraints (e.g. “each role must appear only once per predication”). State of the art in SRL is held by the systems based on deep neural networks (Marcheggiani and Titov, 2017; He et al., 2017). While achieving remarkable quality on benchmark datasets, modern systems show a considerable ≈10-point performance drop when applied out-of-domain. This • We suggest a method for global thematic hierarchy induction from corpus data; 1 We use ≺ for rank precedence, and / for ties 54 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 54–64 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics • We propose several thematic and syntactic ranking models and evaluate them on English and Ge"
K18-1006,L16-1376,0,0.0276735,"Missing"
K18-1006,P09-1033,0,0.0477,"Missing"
K18-1006,D07-1002,0,0.0459092,"ue to incompatibility and limited scope of existing hierarchies. We introduce an empirical framework for thematic hierarchy induction and evaluate several role ranking strategies on English and German corpus data. We hypothesize that inducing a thematic hierarchy is feasible, that a hierarchy can be induced from small amounts of data and that resulting hierarchies apply cross-lingually. We evaluate these assumptions empirically. 1 Introduction Semantic roles are one of the core concepts in NLP, and automatic semantic role labeling (SRL) is a major task with applications in question answering (Shen and Lapata, 2007), machine translation (Liu and Gildea, 2010) and information extraction (Christensen et al., 2010). The goal of SRL is to label the semantic arguments of a predicate (e.g. a verb) with roles from a pre-defined role inventory. Conceptually, role assignment in SRL can be split in two steps: local labeling estimates the likelihood of a certain semantic argument bearing a certain role; global optimization takes context-dependent role interactions into account and enforces certain theoretically motivated constraints (e.g. “each role must appear only once per predication”). State of the art in SRL i"
K18-1006,P09-2064,0,0.032273,"ile an optimal TH that would successfully describe semantic roles’ behavior across languages might not exist (and would imply the existence of a universal role inventory and grammar), our evidence suggests that this concept is at least partially applicable. To the best of our knowledge, there exists no prior work explicitly aiming at discovering thematic hierarchies in corpora. However, the hierarchy-related effects are reported in some studies. For example, White et al. (2017) observe on a reduced role set that VerbNet roles disprefer the violations of thematic/syntactic hierarchy alignment. Sun et al. (2009) experiment on thematic rank prediction for PropBank A0 and A1, but extend their analysis neither to VerbNet thematic roles, nor to the PropBank A2-5. Thematic roles in SRL So far only few studies have considered VerbNetlevel granularity in SRL and we are not aware of SRL systems specifically designed to exploit the thematic role generalizations. Zapirain et al. (2008) compare PropBank and VerbNet performance using a simple SRL system and conclude that PropBank labels generally perform better; however, they do not use any additional modeling possibilities offered by VerbNet’s general, predicat"
K18-1006,E17-2015,0,0.0576152,"Missing"
K18-1006,P08-1063,0,0.0365616,"es in corpora. However, the hierarchy-related effects are reported in some studies. For example, White et al. (2017) observe on a reduced role set that VerbNet roles disprefer the violations of thematic/syntactic hierarchy alignment. Sun et al. (2009) experiment on thematic rank prediction for PropBank A0 and A1, but extend their analysis neither to VerbNet thematic roles, nor to the PropBank A2-5. Thematic roles in SRL So far only few studies have considered VerbNetlevel granularity in SRL and we are not aware of SRL systems specifically designed to exploit the thematic role generalizations. Zapirain et al. (2008) compare PropBank and VerbNet performance using a simple SRL system and conclude that PropBank labels generally perform better; however, they do not use any additional modeling possibilities offered by VerbNet’s general, predicate-independent role set. Loper et al. (2007) show that replacing verb-specific PropBank roles A2-5 with the corresponding VerbNet roles improves the SRL performance. Merlo and van der Plas (2009) report a statistical analysis of PropBank and VerbNet annotations and conclude that while PropBank role inventory better correlates with syntax and is therefore easier to learn"
K19-1046,J08-4004,0,0.202375,"nter-annotator agreement Stance annotation. Every ETS was annotated by at least six crowd workers. We evaluate the interannotator agreement between groups of workers as proposed by Habernal et al. (2017), i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE (Hovy et al., 2013). The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen’s Kappa of κ = 0.7 (Cohen, 1968), indicating a substantial agreement between the crowd workers (Artstein and Poesio, 2008). The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e. κ = 0.7. The agreement between the experts’ annotations and the computed gold annotations from the crowd workers is also substantial, κ = 0.683. FGE Annotation. Similar to the stance annotation, we used the approach of Habernal et al. (2017) to compute the agreement. The interann"
K19-1046,C16-1099,1,0.908833,"Missing"
K19-1046,D15-1075,0,0.054704,"4.1). 5.1 Stance detection In the stance detection task, models need to determine whether an ETS supports or refutes a claim, or expresses no stance with respect to the claim. 5.1.1 Models and Results We report the performance of the following models: AtheneMLP is a feature-based multi-layer perceptron (Hanselowski et al., 2018a), which has reached the second rank in the Fake News Challenge. DecompAttent (Parikh et al., 2016) is a neural network with a relatively small number of parameters that uses decomposable attention, reaching good results on the Stanford Natural Language Inference task (Bowman et al., 2015). USE+Attent is a model which uses the Universal Sentence Encoder (USE) (Cer et al., 2018) to extract representations for the sentences of the ETSs and the claim. For the classification of the stance, an attention mechanism and a MLP is used. The results in Table 6 show that AtheneMLP scores highest. Similar to the outcome of the Fake News Challenge, feature-based models outperform neural networks based on word embeddings (Hanselowski et al., 2018a). As the comparison to the human agreement bound suggests, there is still substantial room for improvement. model recall precision F1m agreement bo"
K19-1046,C18-1158,1,0.843696,"Missing"
K19-1046,W18-5516,1,0.926794,"ndependent tasks, we use recently suggested models that achieved high performance in similar problem settings. In addition, we provide the human agreement bound, which is determined by comparing expert annotations for 200 ETSs to the gold standard derived from crowd worker annotations (Section 4.1). 5.1 Stance detection In the stance detection task, models need to determine whether an ETS supports or refutes a claim, or expresses no stance with respect to the claim. 5.1.1 Models and Results We report the performance of the following models: AtheneMLP is a feature-based multi-layer perceptron (Hanselowski et al., 2018a), which has reached the second rank in the Fake News Challenge. DecompAttent (Parikh et al., 2016) is a neural network with a relatively small number of parameters that uses decomposable attention, reaching good results on the Stanford Natural Language Inference task (Bowman et al., 2015). USE+Attent is a model which uses the Universal Sentence Encoder (USE) (Cer et al., 2018) to extract representations for the sentences of the ETSs and the claim. For the classification of the stance, an attention mechanism and a MLP is used. The results in Table 6 show that AtheneMLP scores highest. Similar"
K19-1046,D18-2029,0,0.013915,"ETS supports or refutes a claim, or expresses no stance with respect to the claim. 5.1.1 Models and Results We report the performance of the following models: AtheneMLP is a feature-based multi-layer perceptron (Hanselowski et al., 2018a), which has reached the second rank in the Fake News Challenge. DecompAttent (Parikh et al., 2016) is a neural network with a relatively small number of parameters that uses decomposable attention, reaching good results on the Stanford Natural Language Inference task (Bowman et al., 2015). USE+Attent is a model which uses the Universal Sentence Encoder (USE) (Cer et al., 2018) to extract representations for the sentences of the ETSs and the claim. For the classification of the stance, an attention mechanism and a MLP is used. The results in Table 6 show that AtheneMLP scores highest. Similar to the outcome of the Fake News Challenge, feature-based models outperform neural networks based on word embeddings (Hanselowski et al., 2018a). As the comparison to the human agreement bound suggests, there is still substantial room for improvement. model recall precision F1m agreement bound random baseline majority vote 0.770 0.333 0.150 0.837 0.333 0.333 0.802 0.333 0.206 At"
K19-1046,N13-1132,0,0.0314322,"with the Associated Press, however, Trump said he thinks Le Pen is stronger than Macron on what’s been going on in France. Table 2: Examples of FGE annotation in supporting (top) and refuting (bottom) ETSs, sentences selected as FGE in italic. 4 4.1 Corpus analysis Inter-annotator agreement Stance annotation. Every ETS was annotated by at least six crowd workers. We evaluate the interannotator agreement between groups of workers as proposed by Habernal et al. (2017), i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE (Hovy et al., 2013). The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen’s Kappa of κ = 0.7 (Cohen, 1968), indicating a substantial agreement between the crowd workers (Artstein and Poesio, 2008). The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e. κ = 0"
K19-1046,N16-1138,0,0.516636,"velopment of a full-fledged fact-checking system requires that the underlying corpus satisfies certain characteristics. First, training data needs to contain a large number of instances with highquality annotations for the different fact-checking sub-tasks. Second, the training data should not be limited to a particular domain, since potentially wrong information sources can range from official statements to blog and Twitter posts. We analyzed existing corpora regarding their adherence to the above criteria and identified several drawbacks. The corpora introduced by Vlachos and Riedel (2014); Ferreira and Vlachos (2016); Derczynski et al. (2017) are valuable for the analysis of the fact-checking problem and pro493 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 493–503 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics vide annotations for stance detection. However, they contain only several hundreds of validated claims and it is therefore unlikely that deep learning models can generalize to unobserved claims if trained on these datasets. A corpus with significantly more validated claims was introduced by Popat et al. (2017). Neverthele"
K19-1046,D16-1244,0,0.242017,"Missing"
K19-1046,L18-1503,1,0.848066,"er modify the example in order to make the problem more obvious: (a) The channel announced today that it is planing a shutdown. (b) Fox News made an announcement today. As the example illustrates, there is a gradual transition between sentences that can be considered as essential for the validation of the claim and those which just provide minor negligible details or unrelated information. Nevertheless, even though the inter-annotator agreement for the annotation of FGE is lower than for the annotation of ETS stance, compared to other annotation problems (Zechner, 2002; Benikova et al., 2016; Tauchmann et al., 2018) that are similar to the annotation of FGE, our framework leads to a better agreement. 4.2 Corpus statistics Table 3 displays the main statistics of the corpus. In the table, FGE sets denotes groups of FGE extracted from the same ETS. Many of the ETSs have been annotated as no stance (see Table 5) and, following our annotation study setup, are not used for FGE extraction. Therefore, the number of FGE sets is much lower than that of ETSs. We have found that, on average, an ETS consists of 6.5 sentences. For those ETS that have support/refute stance, on average, 2.3 sentences are selected as FGE"
K19-1046,N18-1074,0,0.203312,"s arguably the single most important development in the media over the past decades. While it has led to unprecedented growth in information coverage and distribution speed, it comes at a cost. False information can be shared through this channel reaching a much wider audience than traditional means of disinformation (Howell et al., 2013). While human fact-checking still remains the primary method to counter this issue, the amount and the speed at which new information is spread makes manual validation challenging and costly. This motivates the development of automated factchecking pipelines (Thorne et al., 2018a; Popat et al., 2017; Hanselowski and Gurevych, 2017) consisting of several consecutive tasks. The following four tasks are commonly included in the pipeline. Given a controversial claim, document retrieval is applied to identify documents that contain important information for the validation of the claim. Evidence extraction aims at retrieving text snippets or sentences from the identified documents that are related to the claim. This evidence can be further processed via stance detection to infer whether it supports or refutes the claim. Finally, claim validation assesses the validity of th"
K19-1046,W18-5501,0,0.047112,"Missing"
K19-1046,W14-2508,0,0.703456,"ortantly, the successful development of a full-fledged fact-checking system requires that the underlying corpus satisfies certain characteristics. First, training data needs to contain a large number of instances with highquality annotations for the different fact-checking sub-tasks. Second, the training data should not be limited to a particular domain, since potentially wrong information sources can range from official statements to blog and Twitter posts. We analyzed existing corpora regarding their adherence to the above criteria and identified several drawbacks. The corpora introduced by Vlachos and Riedel (2014); Ferreira and Vlachos (2016); Derczynski et al. (2017) are valuable for the analysis of the fact-checking problem and pro493 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 493–503 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics vide annotations for stance detection. However, they contain only several hundreds of validated claims and it is therefore unlikely that deep learning models can generalize to unobserved claims if trained on these datasets. A corpus with significantly more validated claims was introduced by Po"
K19-1046,P17-2067,0,0.0888421,"been validated by journalists. The corpus provides 2,595 news articles that are related to the claims. Each article is summarized into a headline and is annotated with the article’s stance regarding the claim. The corpus is well suited for training stance detection systems in the news domain and it was therefore chosen in the Fake News Challenge (Pomerleau and Rao, 2017) for training and evaluation of competing systems. However, the number of claims in the corpus is relatively small, thus it is unlikely that sophisticated claim validation systems can be trained using this corpus. PolitiFact17 Wang (2017) extracted 12,800 validated claims made by public figures in various contexts from Politifact. For each statement, the corpus provides a verdict and meta information, such as the name and party affiliation of the speaker or subject of the debate. Nevertheless, the corpus does not include evidence and thus the models can only be trained on the basis of the claim, the verdict, and meta information. RumourEval17 Derczynski et al. (2017) organized the RumourEval shared task, for which they provided a corpus of 297 rumourous threads from Twitter, comprising 4,519 tweets. The shared task 9 http://ww"
L16-1134,P13-1133,0,0.0340467,"ed annotation 3 Personal communication with V. Henrich, 7 September 2015. support software. Several tools have been developed for applying WordNet (Fellbaum, 1998) senses to English text. SATANiC (Passonneau et al., 2009), for example, was used to build the MASC corpus; Punnotator (Miller and Turkovi´c, 2016) was created specifically to support WordNet sense annotation of English puns. The only multilingual sense annotation tool we are aware of, IMI (Bond et al., 2015), was used to annotate the NTU-Multilingual Corpus (Tan and Bond, 2011) with senses from the Open Multilingual Wordnet (OMWN) (Bond and Foster, 2013). Although WordNet, OMWN, and GermaNet share a similar structure, the aforementioned tools do not support GermaNet (nor any other German sense inventory). To our knowledge, only two sense annotation tools work with GermaNet. The first of these is KiC, which was used to produce the MuchMore data set. It does not appear to have been publically released, though a brief description appears in Raileanu et al. (2002). KiC displays sentences for a given target word in KWIC (key word in context) format alongside a list of candidate senses from GermaNet. Annotators select the appropriate senses for eac"
L16-1134,P15-4002,0,0.0198119,"s known to be a particularly arduous and expensive task (Mihalcea and Chklovski, 2003). The process can be facilitated through the use of dedicated annotation 3 Personal communication with V. Henrich, 7 September 2015. support software. Several tools have been developed for applying WordNet (Fellbaum, 1998) senses to English text. SATANiC (Passonneau et al., 2009), for example, was used to build the MASC corpus; Punnotator (Miller and Turkovi´c, 2016) was created specifically to support WordNet sense annotation of English puns. The only multilingual sense annotation tool we are aware of, IMI (Bond et al., 2015), was used to annotate the NTU-Multilingual Corpus (Tan and Bond, 2011) with senses from the Open Multilingual Wordnet (OMWN) (Bond and Foster, 2013). Although WordNet, OMWN, and GermaNet share a similar structure, the aforementioned tools do not support GermaNet (nor any other German sense inventory). To our knowledge, only two sense annotation tools work with GermaNet. The first of these is KiC, which was used to produce the MuchMore data set. It does not appear to have been publically released, though a brief description appears in Raileanu et al. (2002). KiC displays sentences for a given"
L16-1134,cholakov-etal-2014-lexical,1,0.924206,"hs, 2013; Henrich and Hinrichs, 2014) include manually applied GermaNet 8.0 annotations for 17 910 occurrences of 109 lemmas (30 nouns and 79 verbs). Lemmas were selected to ensure a good balance of word frequencies, number of distinct senses, and (for verbs) valence frames. Interannotator agreement was generally good (mean Dice coefficient of 0.964 for nouns and 0.937 for verbs). While the data is available for non-profit academic use, it is not released under a free content licence. With respect to German-language lexical substitution data sets, the only one of which we are aware is that of Cholakov et al. (2014). As the present work greatly builds upon its existing content, we reserve our description of it for §3. 2.2 Annotation Tools Manual linguistic annotation, and sense annotation in particular, is known to be a particularly arduous and expensive task (Mihalcea and Chklovski, 2003). The process can be facilitated through the use of dedicated annotation 3 Personal communication with V. Henrich, 7 September 2015. support software. Several tools have been developed for applying WordNet (Fellbaum, 1998) senses to English text. SATANiC (Passonneau et al., 2009), for example, was used to build the MASC"
L16-1134,W14-5201,1,0.877699,"Missing"
L16-1134,E12-1059,1,0.828844,"version 9.0 of the resource, which contains 93 246 synsets covering 121 810 lexical units. 4 Ubyline As discussed in §2.2, most existing sense annotation tools do not support using GermaNet as the sense inventory, and those that do are unpublished. We therefore developed Ubyline, a web-based sense annotation tool. Our tool is a Javabased web application; it uses CQP (Evert and Hardie, 2011) for querying, and MySQL for managing the sense inventory and storing the user annotations. Data Preparation. Ubyline is able to import a sense inventory from any of the lexical resources supported by UBY (Gurevych et al., 2012), including GermaNet. Since the GermaNet licence does not allow redistribution, we build a UBY lexicon from GermaNet using ubycreate.4 Ubyline employs DKPro Core (Eckart de Castilho and Gurevych, 4 https://github.com/dkpro/dkpro-uby/tree/ master/de.tudarmstadt.ukp.uby.ubycreate-gpl Figure 1: Hovering over a sense in Ubyline Figure 2: Drag-and-drop for sense linking in Ubyline 2014) to read the corpus data, mark lemmas to be annotated, segment the texts, and create the CQP indexes. User Interface. Upon log-in, Ubyline presents an overview of lemmas and occurrences requiring annotation. Clicking"
L16-1134,W97-0802,0,0.201154,"0 by one professional annotator and five additional annotators recruited via crowdsourcing. The data set has already seen use in an organized lexical substitution evaluation exercise (Miller et al., 2015). Our review of the original lexical substitution data set revealed that two of the lemmas had duplicate context sentences. We removed these, lowering the total number of contexts in the data set to 2038. We also corrected a number of inconsistencies in the segmentation of words. In line with the sense-annotated corpora discussed previously, we chose GermaNet as our sense inventory. GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) is a lexical-semantic network that relates German-language nouns, verbs, and adjectives. Like its English analogue, WordNet, GermaNet represents semantic concepts as synsets which are interlinked through labelled semantic relations. We used version 9.0 of the resource, which contains 93 246 synsets covering 121 810 lexical units. 4 Ubyline As discussed in §2.2, most existing sense annotation tools do not support using GermaNet as the sense inventory, and those that do are unpublished. We therefore developed Ubyline, a web-based sense annotation tool. Our tool is a"
L16-1134,henrich-hinrichs-2010-gernedit,0,0.602374,"archies. The second GermaNet-capable annotation tool is an unnamed browser-based interface used to extend the T¨uBa-D/Z treebank. As with KiC, it has not been published, though it is described by Henrich (2015). This tool displays target word occurrences in their sentential context, though separately rather than in KWIC format. Below each context is a list of candidate senses, identified only by their GermaNet numeric IDs and a brief description. If this information is not sufficient to discriminate between the senses, annotators must use a separate GermaNet exploration tool such as GernEdiT (Henrich and Hinrichs, 2010). Users select senses by clicking on them; for problematic cases, there is a text field to type natural-language comments. 3 Resources GLASS applies sense annotations to the lexical substitution data set previously described by Cholakov et al. (2014) and later released in full under the Creative Commons 829 Attribution-ShareAlike 3.0 Unported licence (Miller et al., 2015). Our decision to use this data set was motivated by its free licensing, and by the fact that having both sense and lexical substitution annotations will allow for intrinsic and extrinsic evaluations of WSD systems to be carri"
L16-1134,E12-1039,0,0.034733,"Missing"
L16-1134,E14-1057,0,0.491199,"previously described by Cholakov et al. (2014) and later released in full under the Creative Commons 829 Attribution-ShareAlike 3.0 Unported licence (Miller et al., 2015). Our decision to use this data set was motivated by its free licensing, and by the fact that having both sense and lexical substitution annotations will allow for intrinsic and extrinsic evaluations of WSD systems to be carried out on the same data. Moreover, the double annotations provide a rich resource for investigating the relationship between word senses and lexical substitutions. (The only previous study on this topic, Kremer et al. (2014), uses automatically induced rather than manually applied word sense annotations.) The Cholakov et al. (2014) data is provided as XML and delimited text files, and consists of 2040 context sentences from the German edition of Wikipedia, each containing one annotated target word. There are 153 unique target words, equally distributed across parts of speech (nouns, verbs, and adjectives) and three frequency bands as measured by word frequency counts in the German deWaC corpus (Baroni et al., 2009). The data set’s creators did not control for polysemy or synonymy as they did not wish to introduce"
L16-1134,W02-0816,0,0.0796942,"remain rare, particularly for languages other than English. A more recent and increasingly popular evaluation method, which has the advantage of not requiring all human and machine annotators to use the same sense inventory, is lexical substitution. Here the lexical annotations which are applied and compared are not sense labels, but rather lists of plausible synonyms. It has been argued that, since the identification and ranking of these substitutions depends on a proper understanding of the word’s meaning in context, accuracy in this “in vivo” task is an indirect measure of WSD performance (McCarthy, 2002). The contributions of the present work are twofold. First, to ease the considerable technical and ergonomic burdens of creating sense-annotated data, we provide Ubyline,1 an Apache-licensed, web-based sense annotation tool whose user interface is optimized for lexical sample data. (Lexical sample data sets feature documents or short texts in which annotations are applied to all instances of a fixed set of lemmas.) Ubyline supports a wide range of sense inventories in several languages, including WordNet and GermaNet. It is, to our knowledge, the only published GermaNet-compatible sense annota"
L16-1134,W03-2408,0,0.0642474,"es. Interannotator agreement was generally good (mean Dice coefficient of 0.964 for nouns and 0.937 for verbs). While the data is available for non-profit academic use, it is not released under a free content licence. With respect to German-language lexical substitution data sets, the only one of which we are aware is that of Cholakov et al. (2014). As the present work greatly builds upon its existing content, we reserve our description of it for §3. 2.2 Annotation Tools Manual linguistic annotation, and sense annotation in particular, is known to be a particularly arduous and expensive task (Mihalcea and Chklovski, 2003). The process can be facilitated through the use of dedicated annotation 3 Personal communication with V. Henrich, 7 September 2015. support software. Several tools have been developed for applying WordNet (Fellbaum, 1998) senses to English text. SATANiC (Passonneau et al., 2009), for example, was used to build the MASC corpus; Punnotator (Miller and Turkovi´c, 2016) was created specifically to support WordNet sense annotation of English puns. The only multilingual sense annotation tool we are aware of, IMI (Bond et al., 2015), was used to annotate the NTU-Multilingual Corpus (Tan and Bond, 20"
L16-1134,W09-2402,0,0.03273,"Missing"
L16-1134,raileanu-etal-2002-evaluation,0,0.3078,"ol we are aware of, IMI (Bond et al., 2015), was used to annotate the NTU-Multilingual Corpus (Tan and Bond, 2011) with senses from the Open Multilingual Wordnet (OMWN) (Bond and Foster, 2013). Although WordNet, OMWN, and GermaNet share a similar structure, the aforementioned tools do not support GermaNet (nor any other German sense inventory). To our knowledge, only two sense annotation tools work with GermaNet. The first of these is KiC, which was used to produce the MuchMore data set. It does not appear to have been publically released, though a brief description appears in Raileanu et al. (2002). KiC displays sentences for a given target word in KWIC (key word in context) format alongside a list of candidate senses from GermaNet. Annotators select the appropriate senses for each occurrence of the target word; they also have the option of marking an occurrence as “unspecified” if GermaNet does not contain its sense. To help distinguish between problematic senses, KiC can show their corresponding hypernym–hyponym hierarchies. The second GermaNet-capable annotation tool is an unnamed browser-based interface used to extend the T¨uBa-D/Z treebank. As with KiC, it has not been published, t"
L16-1134,Y11-1038,0,0.029507,"Chklovski, 2003). The process can be facilitated through the use of dedicated annotation 3 Personal communication with V. Henrich, 7 September 2015. support software. Several tools have been developed for applying WordNet (Fellbaum, 1998) senses to English text. SATANiC (Passonneau et al., 2009), for example, was used to build the MASC corpus; Punnotator (Miller and Turkovi´c, 2016) was created specifically to support WordNet sense annotation of English puns. The only multilingual sense annotation tool we are aware of, IMI (Bond et al., 2015), was used to annotate the NTU-Multilingual Corpus (Tan and Bond, 2011) with senses from the Open Multilingual Wordnet (OMWN) (Bond and Foster, 2013). Although WordNet, OMWN, and GermaNet share a similar structure, the aforementioned tools do not support GermaNet (nor any other German sense inventory). To our knowledge, only two sense annotation tools work with GermaNet. The first of these is KiC, which was used to produce the MuchMore data set. It does not appear to have been publically released, though a brief description appears in Raileanu et al. (2002). KiC displays sentences for a given target word in KWIC (key word in context) format alongside a list of ca"
L16-1146,baroni-etal-2008-cleaneval,0,0.168558,"iteria make use of a set of textual features extracted from the HTML page such as the link density, text density, and others (Pomik´alek, 2011). After removing the boilerplates, our algorithm can be parametrized to output plain text (by default) or to produce a minimal HTML, where the retaining text parts are printed along with their original HTML tags (such as &lt;p>, &lt;h1>, etc.). The minimal HTML option allows the user to render the plain text with simple markups and keep some minimal HTML semantics of the output. Evaluation of this component is performed using the benchmark CleanEval dataset (Baroni et al., 2008) as well as the cleaneval.py script created by Evert (2008) in order to be able to compare our Java implementation to the original JustText Python implementation by Pomik´alek (2011). We ran both JusText and our Java re-implementation on the CleanEval Test set which consists of 681 web pages. As shown in Table 3, the obtained results are comparable to (Pomik´alek, 2011). The results, after running the CleanEval script, are given in terms of macro-averaged precision, recall and F-score. 1. Cluster possible near-duplicate candidates using the SimHash algorithm. Our Java re-implementation P R F1"
L16-1146,P15-1153,0,0.0226983,"rvised methods in a wide variety of language processing tasks. In recent years, tremendous progress has been made with sentence-level tasks (such as dependency parsing) and genre-specific benchmarks (such as work on the Penn Discourse Treebank). There is also an increasing demand for solutions scaling to heterogeneous document collections on the web. Current trends lean toward multilingual solutions, e.g., universal POS tags (Petrov et al., 2012), which requires heterogeneous corpora in multiple languages. Furthermore, recent documentlevel research tasks, such as multi-document summarization (Bing et al., 2015) or argumentation analysis (Habernal and Gurevych, 2015), heavily depend on document-level training and evaluation corpora. One of the big obstacles for the current research is the lack of large-scale freely-licensed heterogeneous corpora in multiple languages, which can be re-distributed in the form of entire documents. Existing corpora are limited along several dimensions. First, they often exhibit monolingual nature, e.g., ClueWeb1 , Annotated English Gigaword2 (Napoles et al., 2012), and several *WaC corpora (Ljubeˇsi´c and Klubiˇcka, 2014; Faaß and Eckart, 2013). Second, they are usually"
L16-1146,buck-etal-2014-n,0,0.0814647,"Missing"
L16-1146,evert-2008-lightweight,0,0.0826504,"Missing"
L16-1146,goldhahn-etal-2012-building,0,0.0794398,"Missing"
L16-1146,D15-1255,1,0.828935,"rocessing tasks. In recent years, tremendous progress has been made with sentence-level tasks (such as dependency parsing) and genre-specific benchmarks (such as work on the Penn Discourse Treebank). There is also an increasing demand for solutions scaling to heterogeneous document collections on the web. Current trends lean toward multilingual solutions, e.g., universal POS tags (Petrov et al., 2012), which requires heterogeneous corpora in multiple languages. Furthermore, recent documentlevel research tasks, such as multi-document summarization (Bing et al., 2015) or argumentation analysis (Habernal and Gurevych, 2015), heavily depend on document-level training and evaluation corpora. One of the big obstacles for the current research is the lack of large-scale freely-licensed heterogeneous corpora in multiple languages, which can be re-distributed in the form of entire documents. Existing corpora are limited along several dimensions. First, they often exhibit monolingual nature, e.g., ClueWeb1 , Annotated English Gigaword2 (Napoles et al., 2012), and several *WaC corpora (Ljubeˇsi´c and Klubiˇcka, 2014; Faaß and Eckart, 2013). Second, they are usually available as either n-grams (Brants and Franz, 2006) or"
L16-1146,W14-0405,0,0.0390847,"Missing"
L16-1146,W12-3018,0,0.0818072,"Missing"
L16-1146,petrov-etal-2012-universal,0,0.0158171,"Processing (NLP). The importance of both annotated and raw large-scale corpora is rapidly increasing due to recent success of neural networks and similar semi- or unsupervised methods in a wide variety of language processing tasks. In recent years, tremendous progress has been made with sentence-level tasks (such as dependency parsing) and genre-specific benchmarks (such as work on the Penn Discourse Treebank). There is also an increasing demand for solutions scaling to heterogeneous document collections on the web. Current trends lean toward multilingual solutions, e.g., universal POS tags (Petrov et al., 2012), which requires heterogeneous corpora in multiple languages. Furthermore, recent documentlevel research tasks, such as multi-document summarization (Bing et al., 2015) or argumentation analysis (Habernal and Gurevych, 2015), heavily depend on document-level training and evaluation corpora. One of the big obstacles for the current research is the lack of large-scale freely-licensed heterogeneous corpora in multiple languages, which can be re-distributed in the form of entire documents. Existing corpora are limited along several dimensions. First, they often exhibit monolingual nature, e.g., Cl"
L16-1146,schafer-bildhauer-2012-building,0,0.144974,"Missing"
L16-1146,P13-1135,0,0.0429215,"removal and de-duplication. The linguistic annotation includes tokenization, part-of-speech tagging and lemmatization. The tools and the tagged corpora are available on-line for academic purposes. Ljubeˇsi´c and Klubiˇcka (2014) based their work on existing tools from Suchomel and Pomik´alek (2012) (crawler and boilerplate removal) with focus on Bosnian, Croatian, and Serbian. The corpus contains ≈ 1B tokens annotated with the lemma, morphology and syntax layers and is available upon request. Relevant research that exploits CommonCrawl includes mining parallel texts for machine translation by Smith et al. (2013) or extracting n-grams and building language models by Buck et al. (2014). While these works tackle the issue of extracting data from CommonCrawl, they are very taskspecific and do not deal with creating general Web corpus (such as boilerplate removal, de-duplication on the document level, license detection, etc.). 3. Corpora This section introduces the corpora employed to test our processing pipeline and evaluate the performance of individual components. We experimented with two different corpora to assess the performance of the proposed pipeline (Section 4) on new data as well as report some"
L16-1146,spoustova-spousta-2012-high,0,0.122642,"Missing"
L16-1338,W08-2230,0,0.0578788,"Missing"
L16-1338,W04-3205,0,0.295247,"domain of everyday educational topics. This domain contains rather generic language, i.e., has low coverage of named entities. Consider the following example where the lexical-semantic happens-before relation between the verb senses experience an emotion and report an emotion triggers a happens-before relation between the two sentences (1) and (2). (1) Students with learning and behavioral disabilities are more likely to experience school stress. for the automatic acquisition of lexical-semantic relations between verbs from large-scale corpora have been developed (e.g., (Lin and Pantel, 2001; Chklovski and Pantel, 2004; Hashimoto et al., 2009)), the extracted relations are not specified on the sense level, but rather on the word level. For instance, Hashimoto et al. (2009) used templates with variables to represent verb relations, which, however, lack semantic information about the verb context, and thus also about the verb sense. We would like to emphasize that it is essential for resources of lexical-semantic verb relations to specify the relations on the word sense level, because verbs are highly polysemous. This can be achieved, for example, by using a context-sensitive representation that captures sema"
L16-1338,W14-5201,1,0.878195,"Missing"
L16-1338,E14-4044,0,0.0553025,"Missing"
L16-1338,P13-2130,0,0.0163438,"and thus also their word sense; word sense information is crucial especially for verbs due to their high polysemy. In recent years, crowdsourcing proved to be a fast and efficient way to address various linguistic annotation tasks that have traditionally been solved by trained linguists. The OpenCorpora project has gathered a large amount of morphological and morphosyntactic annotations for Russian (Ustalov, 2014). Chamberlain et al. (2008) produced a corpus annotated with co-reference resolutions. (Feizabadi and Pad´o, 2014) used crowdsourcing to create a dataset for semantic role labeling. Fossati et al. (2013) describes a crowdsourcing technique to produce FrameNet annotation. Snow et al. (2008) investigated the applicability of crowdsourcing to five semantic NLP tasks: word similarity, event annotation, word sense disambiguation, sentiment analysis and textual entailment; they concluded that crowdsourcing was well-suitable for these tasks. In particular, crowdsourcing of paraphrase and textual entailment annotations came into focus of multiple recent studies. Negri et al. (2011) built a cross-lingual textual entailment corpus by outsourcing the annotation tasks to Amazon Mechanical Turk and introd"
L16-1338,S13-1035,0,0.0324823,"ildren b. older school children, punish, children a. children, bully, children b. children, punish, children Sampling of proposition pairs for annotation can therefore be split into two subtasks: first, recovering propositions with semantic overlap, and second, generating naturally sounding pairs of propositions from them. An example of the first subtask is given in the work by Levy et al. (2014) who annotated entailment relations between pairs of propositions. They recovered propositions with identical subject and object head words in a domainspecific subset1 of the Google syntactic n-grams (Goldberg and Orwant, 2013), a huge dataset which is based on a corpus of 3.5 million English books. We also recover proposition pairs with this kind of overlap in our set of extracted propositions, but in addition, we consider three further kinds of semantic overlap. We specify these four kinds of semantic overlap in the following pseudo-code 1. Algorithm 1 Types of semantic overlap 1: Given prop1 , prop2 . two propositions 2: propi ← (vi , h(Si ), h(Oi ), h(P Oi ) . verb + head word for subject, object and prepositional object 3: if h(S1 ) = h(S2 ) and h(O1 ) = h(O2 ) 4: then overlapT ype ← SubjObj 5: if h(P O1 ) = h("
L16-1338,E12-1059,1,0.837161,", with and without the full argument phrase included as context, in order to illustrate the difference in interpretability: (4) Figure 2: Examples of different semantic relations from Section 3. 4.1.1. Extraction of Propositions For the proposition extraction, we first preprocessed the document collection using components from DKPro Core (Eckart de Castilho and Gurevych, 2014), in particular, the Stanford Core NLP POS-tagger, lemmatizer, Named Entity Recognizer, and dependency parser, as well as a semantic field annotator for noun, verb and adjective tokens which is based on the UBY resource (Gurevych et al., 2012). We build upon the results of dependency parsing and extract propositions from sentence fragments with a lexical verb (we do not consider auxiliary and modal verbs), where the parser has annotated at least one of the dependency types nsubj (or alternatively nsubjpass, agent), dopbj, and prep ?. We use a proposition template with four fixed slots for the verb, for the subject argument, the object argument and the prepositional argument. This way, we extract propositions that consist of one to three nominal arguments; the absence of a subject, object or prepositional object is indicated in the"
L16-1338,D09-1122,0,0.0812065,"Missing"
L16-1338,W14-1610,0,0.132332,"information would help the system to reveal an inference between the two sentences (1) and (2) which would otherwise remain hidden, because it is not expressed on the text surface via linguistic means, such as verb tense or discourse connectives. Apart from the temporal happens-before relation between verb senses, there are other relations which are important for information consolidation, e.g., the cause relation. Although WordNet (Fellbaum, 1998) contains semantic relations between verb senses, it has a very limited coverage of semantic verb relations, in particular for domainspecific text (Levy et al., 2014). While various approaches In this work, we therefore aim at constructing a large and representative dataset of context-sensitive verb relations by means of crowdsourcing. We go beyond previous work on extracting verb relations from corpora and propose a novel semantic verb relation scheme designed for the challenging task of automatic information consolidation in the particular domain of everyday educational topics. In particular, we make the following contributions: 2131 • We developed a linguistically motivated hierarchical annotation scheme of fine-grained semantic verb relations between p"
L16-1338,P14-5008,0,0.0711794,"Missing"
L16-1338,D11-1062,0,0.0257243,"-reference resolutions. (Feizabadi and Pad´o, 2014) used crowdsourcing to create a dataset for semantic role labeling. Fossati et al. (2013) describes a crowdsourcing technique to produce FrameNet annotation. Snow et al. (2008) investigated the applicability of crowdsourcing to five semantic NLP tasks: word similarity, event annotation, word sense disambiguation, sentiment analysis and textual entailment; they concluded that crowdsourcing was well-suitable for these tasks. In particular, crowdsourcing of paraphrase and textual entailment annotations came into focus of multiple recent studies. Negri et al. (2011) built a cross-lingual textual entailment corpus by outsourcing the annotation tasks to Amazon Mechanical Turk and introduced the methodology of annotation task decomposition into simple subtasks. Similarly, Zeller and Pad´o (2013) created a large dataset for German textual entailment by dividing the annotation task into three separate phases: summarization, paraphrasing and validation. Splitting a complex annotation task into a sequence of easily explainable microtasks was also successfully applied by Zeichner et al. (2012) for evaluating lexical inference rules. In our research we also decom"
L16-1338,P15-1146,0,0.0351487,"Missing"
L16-1338,D08-1027,0,0.288841,"Missing"
L16-1338,P12-2031,0,0.0196938,"xtual entailment annotations came into focus of multiple recent studies. Negri et al. (2011) built a cross-lingual textual entailment corpus by outsourcing the annotation tasks to Amazon Mechanical Turk and introduced the methodology of annotation task decomposition into simple subtasks. Similarly, Zeller and Pad´o (2013) created a large dataset for German textual entailment by dividing the annotation task into three separate phases: summarization, paraphrasing and validation. Splitting a complex annotation task into a sequence of easily explainable microtasks was also successfully applied by Zeichner et al. (2012) for evaluating lexical inference rules. In our research we also decompose the task of annotating lexical semantic relations between propositions into a series of atomic judgments. Unlike aforementioned approaches, we do not restrain ourselves to one specific semantic relation, such as the quite general (textual) entailment relation, but introduce a crowdsourcing methodology for annotating a range of more fined-grained semantic relations between propositions. 3. Annotation Scheme We developed an annotation scheme for semantic relations between propositions. Propositions describe events, states"
L16-1338,W13-0125,0,0.0487301,"Missing"
L16-1484,P12-1090,0,0.020379,"SemAF-SR based on LIRICS (ISO, 2014) proposed a semantic role inventory in close consideration of VerbNet roles. Bonial et al. (2011) and Hwang (2014) present revised VerbNet role hierarchies. They serve as a basis for proposed revisions to the VerbNet role inventory, as described in Sec. 3.2. and 3.6. VerbNet is based on a set of semantic verb classes. Assigning a verb to its class is similar to a sense disambiguation act, with senses defined by the syntactic alternation behaviour of verbs (Levin, 1993). Prior work has investigated the multilingual applicability of VerbNet semantic classes (Falk et al., 2012). While Levin-style verb classes are well established, the classes are relatively coarse-grained and not all of them are semantically homogeneous. German sense- and role-annotated corpora. There are only few sense- and role-annotated corpora for German. Corpora with word sense annotations according to GermaNet have been created by Broscheit et al. (2010), Henrich et al. (2012), and Henrich and Hinrichs (2014). However, none of these provide semantic role annotation. Schulte im Walde (2006) clustered German verbs to semantic verb classes inspired by Levin (1993), trying to match semantic verb ’"
L16-1484,W97-0802,0,0.691546,"ation scheme and establish detailed annotation guidelines. Finally, combining word sense and semantic role annotation raises several questions that we will address in our study: To what extent are word sense and role annotation dependent on each other? What kinds of preannotation for predicates and arguments are helpful to support role annotation (e.g., presenting dependency heads or full argument spans as role targets)? To answer these questions, we design contrastive annotation setups and evaluate their impact on annotation processes and results. In contrast to VerbNet, we use GermaNet 9.0 (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) – the German counterpart of Princeton WordNet (Fellbaum, 1998) – as a fine-grained sense inventory for predicate labeling, and investigate its suitability for combined sense and VerbNet-style role labeling. Our contributions We provide a novel adaptation of a VerbNet-style semantic role set for manual annotation on a German corpus. We identify best annotation practice for joint word sense and VerbNet-style role labeling, including analysis of IAA, and investigate systematic dependencies between predicate and role annotation. Using the combined annotation scheme, w"
L16-1484,henrich-hinrichs-2010-gernedit,0,0.027528,"sh detailed annotation guidelines. Finally, combining word sense and semantic role annotation raises several questions that we will address in our study: To what extent are word sense and role annotation dependent on each other? What kinds of preannotation for predicates and arguments are helpful to support role annotation (e.g., presenting dependency heads or full argument spans as role targets)? To answer these questions, we design contrastive annotation setups and evaluate their impact on annotation processes and results. In contrast to VerbNet, we use GermaNet 9.0 (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) – the German counterpart of Princeton WordNet (Fellbaum, 1998) – as a fine-grained sense inventory for predicate labeling, and investigate its suitability for combined sense and VerbNet-style role labeling. Our contributions We provide a novel adaptation of a VerbNet-style semantic role set for manual annotation on a German corpus. We identify best annotation practice for joint word sense and VerbNet-style role labeling, including analysis of IAA, and investigate systematic dependencies between predicate and role annotation. Using the combined annotation scheme, we conduct a large-scale annot"
L16-1484,E12-1039,0,0.0245846,"its class is similar to a sense disambiguation act, with senses defined by the syntactic alternation behaviour of verbs (Levin, 1993). Prior work has investigated the multilingual applicability of VerbNet semantic classes (Falk et al., 2012). While Levin-style verb classes are well established, the classes are relatively coarse-grained and not all of them are semantically homogeneous. German sense- and role-annotated corpora. There are only few sense- and role-annotated corpora for German. Corpora with word sense annotations according to GermaNet have been created by Broscheit et al. (2010), Henrich et al. (2012), and Henrich and Hinrichs (2014). However, none of these provide semantic role annotation. Schulte im Walde (2006) clustered German verbs to semantic verb classes inspired by Levin (1993), trying to match semantic verb ’fields’ as defined by Schumacher (1986). These classes have not been employed for manual corpus labeling or automatic sense tagging. For semantic role annotation, SALSA (Burchardt et al., 2009) is the only available corpus. It is annotated with FrameNet 1.3 frames and roles, and extended with protoframes for predicates unknown to FrameNet 1.3. For the CoNLL 2009 SRL task (Haji"
L16-1484,P09-1033,0,0.756801,"Missing"
L16-1484,C14-2023,1,0.897458,"Missing"
L16-1484,J05-1004,0,0.562795,"A corpus. We publish the annotated corpus and detailed guidelines for the new role annotation scheme. Keywords: word sense annotation, semantic role annotation, GermaNet, VerbNet, SALSA corpus, guidelines, German 1. Introduction Semantic annotation of predicate-argument structure is an important task in NLP. During decades, different frameworks for representing semantic predicate-argument structure have been established, notably FrameNet, VerbNet and PropBank, with accompanying sense and role inventories and annotated resources, primarily for English (Baker et al., 1998; Kipper-Schuler, 2005; Palmer et al., 2005). The resources are interoperable via SemLink and the Unified Verb Index (Loper et al., 2007). Combining GermaNet sense and VerbNet role annotation Within these established frameworks, the characterization of predicates and roles differ in important ways: FrameNet defines semantic roles for verbs, nouns and adjectives evoking a frame. Due to its large frame inventory, and since roles are specific to a frame, there is a vast amount of roles to distinguish, and roles do not generalize across frames. PropBank defines six major roles (A0-A5); except for A0 and A1, their interpretation is not consi"
L16-1484,petukhova-bunt-2008-lirics,0,0.162875,"ed semantic criteria, with roles gouped into five semantic groups Actor, Undergoer, Place, Time, and Circumstance. 3.3. Guidelines for Semantic Annotation We created annotation guidelines for both considered semantic annotation tasks that fulfill general desiderata: a) clear definition of annotation targets, b) instructions about the procedure, and c) clear definition of the annotation labels, including guides for deciding difficult and irregular cases, using guiding questions and examples. We created new role annotation guidelines for German that are assumed to generalize to other languages (Petukhova and Bunt, 2008). The original VerbNet guidelines2 do not provide detailed instructions. The UVI3 web interface with VerbNet roles gives insight into how roles are applied for specific verbs or verb classes, but does not serve as a general guideline. SemAF-SR provides more specific definitions which we refer to in some of the role descriptions. Beyond technical instructions for usage of the annotation tool, the annotation targets, ’predicate’ and ’argument’, have to been defined. Even though we provide preannotated targets, the annotators must be able to verify – and if needed, correct – the proposed targets."
L16-1484,Q15-1034,0,0.0777467,"Missing"
L16-1484,J06-2001,0,0.057914,"Missing"
L16-1484,S12-1001,1,0.837012,"Patient-like A1 roles generalize over syntactic alternations. Their definition is based on Dowty’s proto-roles (Dowty, 1991), capturing salient Agent- and Patient-like properties. Reisinger et al. (2015) attempt a characterization of ProtoRoles as distributions over properties, collected by crowdsourcing. In our study, we aim at a concise role inventory that covers the full range of arguments to a predicate. VerbNet is located between FrameNet and PropBank on a continous scale between a fine-grained interpretable role inventory on one side and a compact, coarse-grained inventory on the other. Silberer and Frank (2012) identified a stronger generalization capacity of VerbNet roles compared to FrameNet roles in the task of non-local role binding. Merlo and van der Plas (2009) found that PropBank roles, being closer to syntax, are easier to assign than VerbNet roles, while the latter provide better semantic generalization. VerbNet defines a set of up to 35 roles, which are defined independently from specific verb senses. Next to VerbNet, the ISO-standard SemAF-SR based on LIRICS (ISO, 2014) proposed a semantic role inventory in close consideration of VerbNet roles. Bonial et al. (2011) and Hwang (2014) presen"
L16-1484,P14-5016,1,0.891987,"Missing"
L16-1484,P98-1013,0,\N,Missing
L16-1484,C98-1013,0,\N,Missing
L18-1202,agic-ljubesic-2014-setimes,0,0.0228783,"Missing"
L18-1202,benikova-etal-2014-nosta,0,0.0503738,"Missing"
L18-1202,P05-1045,0,0.0370873,"Missing"
L18-1202,P14-5010,0,0.00479445,"sion in the present analysis, but it should be noted that 7 It is notably difficult to locate corpora under specific CC licence versions. E.g. at the time of writing, META-SHARE (http://www.meta-share.org) lists over 200 corpora under CC BY, 1268 different licence versions could lead to different assessments especially in relation to the SGDR right. Examples for corpora under this licence version are the recent GermEVAL 2014 dataset for NER (Benikova et al., 2014) or the Coptic Treebank (Schroeder and Zeldes, 2016).8 Training. In our scenarios, we train a NER model with the Stanford NER tool (Manning et al., 2014). To determine the relation of the trained model to the original data, we examine which information goes into the model. We describe the process in the present and following scenarios at increasing levels of detail, as required by the respective legal analyses. The training process requires the creation of a usually temporary copy (i.e. a reproduction) of the original data and usually its transformation into the training data format. The training data format used by the Stanford NER tool is very simple: a two-column format in which the first column contains a token (word or punctuation mark) a"
L18-1202,truyens-van-eecke-2014-legal,0,0.0697228,"Missing"
L18-1526,W13-3520,0,0.0253363,"Missing"
L18-1526,N16-2003,0,0.0273997,"b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious arguments. The majority of research on computational argumentation is English-centric (with several exceptions, such as (Peldszus and Stede, 2015; Liebeck et al., 2016; Chow, 2016)). Not only the language itself but the discussed topics and controversies are culture-specific. For example, ‘homeschooling’ or ‘death penalty’ are almost non-existent in Germany, while being highly controversial subjects of discussion in the United States. As Argotario had been developed within the English context, simply translating the topics and existing arguments and fallacies into another language does not meet the expectations of a serious game user. We thus asked the following research questions. First, what are the best means to tackle language adaptation in serious games that depend"
L18-1526,D17-1218,1,0.845084,"ame platform, such as topic selection, initial data creation, or effective campaigns. Moreover, we analyze users’ behavior and in-game created data in order to assess the dissemination strategies and qualitative aspects of the resulting corpus. We also report on classification experiments based on neural networks and feature-based models. Keywords: argumentation, fallacies, serious games 1. Introduction Computational argumentation and argument mining has been traditionally dealing with understanding argument’s structure (Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Eger et al., 2017; Daxenberger et al., 2017). Recently, attention has been paid to pragmatic aspects of arguments, such as convincingness (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious arguments. The majority of research on computat"
L18-1526,P17-1002,1,0.84652,"uccessful serious game platform, such as topic selection, initial data creation, or effective campaigns. Moreover, we analyze users’ behavior and in-game created data in order to assess the dissemination strategies and qualitative aspects of the resulting corpus. We also report on classification experiments based on neural networks and feature-based models. Keywords: argumentation, fallacies, serious games 1. Introduction Computational argumentation and argument mining has been traditionally dealing with understanding argument’s structure (Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Eger et al., 2017; Daxenberger et al., 2017). Recently, attention has been paid to pragmatic aspects of arguments, such as convincingness (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious arguments. The major"
L18-1526,D16-1129,1,0.852095,"or and in-game created data in order to assess the dissemination strategies and qualitative aspects of the resulting corpus. We also report on classification experiments based on neural networks and feature-based models. Keywords: argumentation, fallacies, serious games 1. Introduction Computational argumentation and argument mining has been traditionally dealing with understanding argument’s structure (Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Eger et al., 2017; Daxenberger et al., 2017). Recently, attention has been paid to pragmatic aspects of arguments, such as convincingness (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious arguments. The majority of research on computational argumentation is English-centric (with several exceptions, such as (Peldszus and Stede, 2015; Liebeck et al., 2016;"
L18-1526,P16-1150,1,0.871697,"or and in-game created data in order to assess the dissemination strategies and qualitative aspects of the resulting corpus. We also report on classification experiments based on neural networks and feature-based models. Keywords: argumentation, fallacies, serious games 1. Introduction Computational argumentation and argument mining has been traditionally dealing with understanding argument’s structure (Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Eger et al., 2017; Daxenberger et al., 2017). Recently, attention has been paid to pragmatic aspects of arguments, such as convincingness (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious arguments. The majority of research on computational argumentation is English-centric (with several exceptions, such as (Peldszus and Stede, 2015; Liebeck et al., 2016;"
L18-1526,J17-1004,1,0.790531,"xamine all steps that are necessary to end up with a successful serious game platform, such as topic selection, initial data creation, or effective campaigns. Moreover, we analyze users’ behavior and in-game created data in order to assess the dissemination strategies and qualitative aspects of the resulting corpus. We also report on classification experiments based on neural networks and feature-based models. Keywords: argumentation, fallacies, serious games 1. Introduction Computational argumentation and argument mining has been traditionally dealing with understanding argument’s structure (Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Eger et al., 2017; Daxenberger et al., 2017). Recently, attention has been paid to pragmatic aspects of arguments, such as convincingness (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time colle"
L18-1526,D17-2002,1,0.752136,"ument mining has been traditionally dealing with understanding argument’s structure (Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Eger et al., 2017; Daxenberger et al., 2017). Recently, attention has been paid to pragmatic aspects of arguments, such as convincingness (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious arguments. The majority of research on computational argumentation is English-centric (with several exceptions, such as (Peldszus and Stede, 2015; Liebeck et al., 2016; Chow, 2016)). Not only the language itself but the discussed topics and controversies are culture-specific. For example, ‘homeschooling’ or ‘death penalty’ are almost non-existent in Germany, while being highly controversial subjects of discussion in the United States. As Argotario had been developed within the Engl"
L18-1526,W16-2817,0,0.0234215,"nal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious arguments. The majority of research on computational argumentation is English-centric (with several exceptions, such as (Peldszus and Stede, 2015; Liebeck et al., 2016; Chow, 2016)). Not only the language itself but the discussed topics and controversies are culture-specific. For example, ‘homeschooling’ or ‘death penalty’ are almost non-existent in Germany, while being highly controversial subjects of discussion in the United States. As Argotario had been developed within the English context, simply translating the topics and existing arguments and fallacies into another language does not meet the expectations of a serious game user. We thus asked the following research questions. First, what are the best means to tackle language adaptation in serious game"
L18-1526,D15-1110,0,0.0276699,"h as convincingness (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious arguments. The majority of research on computational argumentation is English-centric (with several exceptions, such as (Peldszus and Stede, 2015; Liebeck et al., 2016; Chow, 2016)). Not only the language itself but the discussed topics and controversies are culture-specific. For example, ‘homeschooling’ or ‘death penalty’ are almost non-existent in Germany, while being highly controversial subjects of discussion in the United States. As Argotario had been developed within the English context, simply translating the topics and existing arguments and fallacies into another language does not meet the expectations of a serious game user. We thus asked the following research questions. First, what are the best means to tackle language adap"
L18-1526,W16-2813,1,0.853988,"4) or a manual analysis of fallacies in newswire editorials in major U.S. newspapers before invading Iraq in 2003 by (Sahlane, 2012). These examples demonstrate the enormous persuasive effect of fallacious argumentation; other examples of its rhetorical power can be found in (Macagno, 2013). The computational perspective on fallacies in natural language arguments has been bound to the process of obtaining reliable data from the crowd and serious-game players (Pollak, 2016; Habernal et al., 2017). There are also several related works devoted to argumentation quality, such as confirmation bias (Stab and Gurevych, 2016), or qualitative assessment of arguments from the Web (Wachsmuth et al., 2017b). 3. Overview of Argotario Argotario represents an instance of so-called serious games (Mayer et al., 2014) that deals with fallacies in everyday argumentation. Argotario is an open-source, platformindependent application with strong educational aspects.1 It 3329 1 www.argotario.net was primarily developed in English but has been extended to support multiple languages (Habernal et al., 2017). In short, players of Argotario learn to recognize several types of fallacies as well as to write them, both in a single-playe"
L18-1526,J17-3005,1,0.831623,"essary to end up with a successful serious game platform, such as topic selection, initial data creation, or effective campaigns. Moreover, we analyze users’ behavior and in-game created data in order to assess the dissemination strategies and qualitative aspects of the resulting corpus. We also report on classification experiments based on neural networks and feature-based models. Keywords: argumentation, fallacies, serious games 1. Introduction Computational argumentation and argument mining has been traditionally dealing with understanding argument’s structure (Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Eger et al., 2017; Daxenberger et al., 2017). Recently, attention has been paid to pragmatic aspects of arguments, such as convincingness (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious a"
L18-1526,P17-2039,1,0.897613,"qualitative aspects of the resulting corpus. We also report on classification experiments based on neural networks and feature-based models. Keywords: argumentation, fallacies, serious games 1. Introduction Computational argumentation and argument mining has been traditionally dealing with understanding argument’s structure (Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Eger et al., 2017; Daxenberger et al., 2017). Recently, attention has been paid to pragmatic aspects of arguments, such as convincingness (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious arguments. The majority of research on computational argumentation is English-centric (with several exceptions, such as (Peldszus and Stede, 2015; Liebeck et al., 2016; Chow, 2016)). Not only the language itself but the discussed topics and con"
L18-1526,E17-1017,0,0.0367444,"qualitative aspects of the resulting corpus. We also report on classification experiments based on neural networks and feature-based models. Keywords: argumentation, fallacies, serious games 1. Introduction Computational argumentation and argument mining has been traditionally dealing with understanding argument’s structure (Habernal and Gurevych, 2017; Stab and Gurevych, 2017; Eger et al., 2017; Daxenberger et al., 2017). Recently, attention has been paid to pragmatic aspects of arguments, such as convincingness (Habernal and Gurevych, 2016b; Habernal and Gurevych, 2016a) or overall quality (Wachsmuth et al., 2017a). A fairly unexplored area of computational argumentation is fallacies: arguments that seem to be valid but are not so (Hamblin, 1970). To tackle the nonexistence of corpora for dealing with fallacies computationally, Habernal et al. (2017) published Argotario—a serious game intended to educate players and at the same time collect annotated fallacious arguments. The majority of research on computational argumentation is English-centric (with several exceptions, such as (Peldszus and Stede, 2015; Liebeck et al., 2016; Chow, 2016)). Not only the language itself but the discussed topics and con"
N03-1012,P98-1013,0,0.0369531,"and Process. The class Physical Object describes any kind of objects we come in contact with - living as well as nonliving - having a location in space and time in contrast to abstract objects. These objects refer to different domains, such as Sight and Route in the tourism domain, Av Medium and Actor in the TV and cinema domain, etc., and can be associated with certain relations in the processes via slot constraint definitions. The modeling of Process as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the F RAME N ET data (Baker et al., 1998). Currently, there are four groups of processes (see Figure 1): General Process, a set of the most general processes such as duplication, imitation or repetition processes; Mental Process, a set of processes such as cognitive, emotional or perceptual processes; Physical Process, a set of processes such as motion, transaction or controlling processes; Social Process, a set of processes such as communication or instruction processes. Let us consider the definition of the Information Search Process in the ontology. It is modeled as a projects. For more detail, see www.w3c.org. subclass of the Cog"
N03-1012,J96-2004,0,0.00793854,"ucted by means of a hidden operator test. We had 29 subjects prompted to say certain inputs in 8 dialogues. 1.479 turns were recorded. Each user-turn in the dialogue corresponded to a single intention, e.g. a route request or a sight information request. The audio files were then sent to the speech recognizer and the input to the semantic coherence scoring module, i.e. n-best lists of SRHs were recorded in log-files. The final corpus consisted of 2.284 SRHs. All hypotheses were then randomly mixed to avoid contextual influences and given to separate annotators. The resulting Kappa statistics (Carletta, 1996) over the annotated data yields , which seems to indicate that human annotators can reliably distinguish between coherent samples (as in Example (1a)) and incoherent ones (as in Example (1b)). The aim of the work presented here, then, was to provide a knowledge-based score, that can be employed by any NLU system to select the best hypothesis from a given n-best list. O NTO S CORE, the resulting system will be described below, followed by its evaluation against the human gold standard.   3 The Knowledge Base In this section, we provide a description of the preexisting knowledge source em"
N03-1012,W02-0207,1,0.627777,"the system’s knowledge of the domains at hand. This increases the number of times where a suboptimal recognition hypothesis is passed through the system. This means that, while there was a better representation of the actual utterance in the n-best list, the NLU system is processing an inferior one, thereby causing overall dialogue metrics, in the sense of Walker et al. (2000), to decrease. We propose an alternative way to rank SRHs on the basis of their semantic coherence with respect to a given ontology representing the domains of the system. 2.2 Annotation Experiments In a previous study (Gurevych et al., 2002), we tested if human annotators could reliably classify SRHs in terms 2 As the numbers evident from large vocabulary speech recognition performance (Cox et al., 2000), the occurrence of less well formed and incoherent SRHs increases the more conversational a system becomes. of their semantic coherence. The task of the annotators was to determine whether a given hypothesis representsa n internally coherent utterance or not. In order to test the reliability of such annotations, we collected a corpus of SRHs. The data collection was conducted by means of a hidden operator test. We had 29 subjects"
N03-1012,P99-1040,0,0.0172376,"Missing"
N03-1012,C98-1013,0,\N,Missing
N07-2052,W04-2607,0,0.100419,"rman datasets. We show that the performance of measures strongly depends on the underlying knowledge 2 Datasets Several German datasets for evaluation of SS or SR have been created so far (see Table 1). Gurevych (2005) conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965), but argued that the dataset (Gur65) is too small (it contains only 65 noun pairs), and does not model SR. Thus, she created a German dataset containing 350 word pairs (Gur350) containing nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, the dataset is biased towards strong classical relations, as word pairs were manually selected. Thus, Zesch and Gurevych (2006) semi-automatically created word pairs from domain-specific corpora. The resulting ZG222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations. Hence, it is particularly suited for analyzing the capability of a measure to estimate SR. 205 Proceedings of NAACL HLT 2007, Companion Volume, pages 205–208, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics DATASET Gur65 Gur350 ZG222 Y EAR 2005 2006"
N07-2052,W06-1104,1,0.685605,"for evaluation of SS or SR have been created so far (see Table 1). Gurevych (2005) conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965), but argued that the dataset (Gur65) is too small (it contains only 65 noun pairs), and does not model SR. Thus, she created a German dataset containing 350 word pairs (Gur350) containing nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, the dataset is biased towards strong classical relations, as word pairs were manually selected. Thus, Zesch and Gurevych (2006) semi-automatically created word pairs from domain-specific corpora. The resulting ZG222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations. Hence, it is particularly suited for analyzing the capability of a measure to estimate SR. 205 Proceedings of NAACL HLT 2007, Companion Volume, pages 205–208, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics DATASET Gur65 Gur350 ZG222 Y EAR 2005 2006 2006 L ANGUAGE German German German # PAIRS 65 350 222 POS N N, V, A N, V, A T YPE SS SR SR S CORES discrete {0,1,2,3,4} discrete {0,1,2,3"
N07-2052,W07-0201,1,0.678532,"based measures, we find that the knowledge source has a major influence on performance. When evaluated on Gur65, that contains pairs connected by SS, GermaNet based measures perform near the upper bound and outperform Wikipedia based measures by a wide margin. On Gur350 containing a mix of SS and SR pairs, most measures perform comparably. Finally, on ZG222, that contains pairs connected by SR, the best Wikipedia based measure outperforms all GermaNet based measures. The impressive performance of P L on the SR datasets cannot be explained with the structural properties of the category graph (Zesch and Gurevych, 2007). Semantically related terms, that would not be closely related in a taxonomic wordnet structure, are very likely to be categorized under the same Wikipedia category, resulting in short path lengths leading to high SR. These findings are contrary to that of (Strube and Ponzetto, 2006), where LC outperformed path length. They limited the search depth using a manually defined threshold, and did not compute SR between all candidate article pairs. Our results show that judgments on the performance of a measure must always be made with respect to the task at hand: computing SS or SR. Depending on t"
N07-2052,I05-1067,1,\N,Missing
N07-2052,J06-1003,0,\N,Missing
N13-1133,S12-1051,0,0.00453916,"omplete in listing synonyms. Second, manually annotated substitutions show that not all synonyms of a word are appropriate in a given context, and many good substitutions have other lexical relation than synonymy to the original word. Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). In this work, we present a supervised lexical substitution system that, unlike the usual lexical sample supervised approaches, can produce substitutions for targets that are not contained in the training material. We reach this by using non-lexical features from heterogeneous evidence, including lexical-semantic resources and distributional similarity, n-gram and shallow syntactic features based on large, unannotated background corpora. In light of the existence of lexical resources such as WordNet (Fellbaum, 1998) or machine readable dictionarie"
N13-1133,D10-1116,0,0.0525266,"irst, the available lexical resources are seldom complete in listing synonyms. Second, manually annotated substitutions show that not all synonyms of a word are appropriate in a given context, and many good substitutions have other lexical relation than synonymy to the original word. Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). In this work, we present a supervised lexical substitution system that, unlike the usual lexical sample supervised approaches, can produce substitutions for targets that are not contained in the training material. We reach this by using non-lexical features from heterogeneous evidence, including lexical-semantic resources and distributional similarity, n-gram and shallow syntactic features based on large, unannotated background corpora. In light of the existence of lexical resources such as WordNet"
N13-1133,P06-1057,0,0.0937847,"chnische Universit¨at Darmstadt (4) Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information http://www.nuance.com , http://www.ukp.tu-darmstadt.de Abstract Lexical substitution, a special form of contextual paraphrasing where only a single word is replaced, is closely related to word sense disambiguation (WSD): polysemous words have possible substitutions reflecting several senses, and the correct sense has to be picked to avoid spurious system behavior. However, no explicit word sense inventory is required for lexical substitution (Dagan et al., 2006). We propose a supervised lexical substitution system that does not use separate classifiers per word and is therefore applicable to any word in the vocabulary. Instead of learning word-specific substitution patterns, a global model for lexical substitution is trained on delexicalized (i.e., non lexical) features, which allows to exploit the power of supervised methods while being able to generalize beyond target words in the training set. This way, our approach remains technically straightforward, provides better performance and similar coverage in comparison to unsupervised approaches. Using"
N13-1133,de-marneffe-etal-2006-generating,0,0.00669888,"Missing"
N13-1133,D10-1113,0,0.03443,"o (2010) and Reisinger and Mooney (2010b) to model word meaning with respect to its context: instead of representing the word and the context as separate vectors and combining them, a set of word occurrences in similar contexts is picked first, and then only these exemplars are used to represent the word in context. While this approach provides good results with relatively simple and transparent models, each occurrence of a word has a unique representation (that can only be computed at testing time), and it is computationally expensive to scale these models to a large number of examples. 1133 Dinu and Lapata (2010) used a bag of words latent variable model to characterize the meaning of a word as a distribution over a set of latent variables (that is, probabilistic senses). Contextualized representation of word meaning is then attained by conditioning the model on the context words in which the target word occurs. A similar approach has been evaluated for word similarity (Reisinger and Mooney, 2010a) and word sense disambiguation (Li et al., 2010). Although our main goal here is to develop a fullfledged lexical substitution system, we mainly focus on the construction of better ranking models based on su"
N13-1133,J93-1003,0,0.283666,"Features In order to create a Distributional Thesaurus (DT) similar to Lin (1998), we parsed a source corpus 1136 of 120M sentence English newspaper texts from the LCC5 (Richter et al., 2006) with the Stanford parser (de Marneffe et al., 2006) and used dependencies to extract features for words: each dependency triple (w1, r, w2) denoting a dependency of type r between words w1 and w2 results in a feature (r, w2) characterizing w1, and a feature (w1, r) characterizing w26 . After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (LL), (Dunning, 1993)), rank features per word according to their significance, and prune the data, keeping only the 1000 most salient features (Fw ) per word7 . The similarity of two words is then given by the number of their common features. Our distributional thesaurus provides a list of the 1000 most salient features and a ranked list of up to 200 similar words (simw , based on the number of shared features) for all words above a certain frequency in the source corpus. We compute the following features to characterize a target word / substitution pair: • To what P extent the context c characterizes si : P c∈Fs"
N13-1133,D08-1094,0,0.0878026,"Missing"
N13-1133,P10-2017,0,0.0447386,"Missing"
N13-1133,S07-1029,0,0.78648,"unnecessary. 8 The various values for k trade off the salience of this feature for coverage: only very few substitutions have overlap in the top 1-5 similar words set, but if this happens, it is a very strong indicator of contextual fitness, whereas overlap within the top 100-200 similar words is present for much more target/substitution pairs, but it is a weaker indicator of fitness. 6 4.3.3 Local n-gram Features (from Web 1T) Syntagmatic coherence, measured as the n-gram frequency of the context with the candidate substitution serves as the basis of ranking in the best Semeval 2007 system (Giuliano et al., 2007), which is also our baseline method here. We use the same ngrams as features in our supervised model: • 1-5-gram frequencies in a sliding window around t: f req(cl si cr )/f req(cl tcr ), normalized w.r.t t • 1-5-gram frequencies in a sliding window P around t: f req(cl si cr )/ f req(cl Scr ), normalized w.r.t. S • for each of x in {’and’, ’or’, ’,’}, 3-5gram frequencies in a sliding window around t: f req(cl txsi cr )/f req(cl tcr ) (how frequently the target and candidate are part of a list or conjunctive phrase) 4.3.4 Shallow Syntactic Features We also use part of speech information (from"
N13-1133,S07-1091,0,0.474545,"Missing"
N13-1133,P10-1116,0,0.00868198,"Missing"
N13-1133,P98-2127,0,0.0443239,"s IDs of the words’ hypernyms as features, which can capture more general semantics (the word to replace is ’animate’, ’abstract’, etc.). The following features were extracted from WordNet: • number of senses of t and si in WordNet • the sense numbers of t and si which are synonymous (in case they are direct synonyms, c.f. WN sense numbers encode sense frequencies) • binary features for synset IDs of the hypernyms of the synset containing t and si (this feature type did not significantly improve results) 4.3.2 Corpus-based Features In order to create a Distributional Thesaurus (DT) similar to Lin (1998), we parsed a source corpus 1136 of 120M sentence English newspaper texts from the LCC5 (Richter et al., 2006) with the Stanford parser (de Marneffe et al., 2006) and used dependencies to extract features for words: each dependency triple (w1, r, w2) denoting a dependency of type r between words w1 and w2 results in a feature (r, w2) characterizing w1, and a feature (w1, r) characterizing w26 . After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (LL), (Dunning, 1993)), rank features per word according to their significance, and prune"
N13-1133,S07-1050,0,0.386383,"rget words, instances with at least one non-multiword possible substitution, average size of candidate sets, and number of instances with no good candidate and frequency of different labels. The labels denote how many annotators proposed a particular word as substitution in the given context and can be interpreted as a measure of goodness: the higher the value, the better the candidate fits in the context. Similarly, the label 0 denotes the total number of negative examples in our datasets, i.e. bad substitutions – words that belong to the can3 This candidate set was found best for WordNet by Martinez et al. (2007). 1134 source # words #inst avg. set # empty #0 #1 #2 #3 #4 #5+ LexSub WN Gold St. 201 201 2002 2002 21 17 508 17 39465 27300 1302 4698 582 1251 308 571 212 319 129 179 TWSI WN Gold St. 908 1007 22543 24643 7.5 22 11165 620 151538 443993 10678 77417 4171 17585 2069 5629 74 325 121 411 Table 1: Details of the datasets: WN=WordNet didate set for a particular target word, but are not listed as good substitutions in the given context in the dataset. 4 Methodology 4.1 Experimental Setup and Evaluation We follow previous works in lexical substitution and evaluate our models using the Generalized Ave"
N13-1133,S07-1009,0,0.767833,"e target word and then to rank this set of possible substitutions according to their contextual fitness. The task to generate a high quality set of possible substitutions is challenging in itself, for two reasons. First, the available lexical resources are seldom complete in listing synonyms. Second, manually annotated substitutions show that not all synonyms of a word are appropriate in a given context, and many good substitutions have other lexical relation than synonymy to the original word. Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). In this work, we present a supervised lexical substitution system that, unlike the usual lexical sample supervised approaches, can produce substitutions for targets that are not contained in the training material. We reach this by using non-lexical features from heterogeneous evidence, includi"
N13-1133,P05-3014,0,0.0229166,"voting scheme, do not need training data per 1132 target. The combination of different signals, however, has to be done manually. Unsupervised systems that rely on distributional similarity (Thater et al., 2011) or topic models (Li et al., 2010) are single signals in this sense, and their development is guided by the performance and observations on standard datasets. Such signals, however, can also be kept simple avoiding any task-specific optimization and can be integrated in a single model for all words using a limited amount of training data and delexicalized features, as in Senselearner (Mihalcea and Csomai, 2005) for weakly supervised all-words disambiguation. This way, task specific development can be replaced by a machine learning component and the resulting model applies also to unseen words, similar to the knowledge-based approaches. 2.1 Full Lexical Substitution Systems Related works that address the lexical substitution problem according to the settings established by the English Lexical Substitution Task (McCarthy and Navigli, 2007) at Semeval 2007 (LexSub) typically employ a simple ranking strategy based on local n-gram frequencies and focus on finding an optimal source of possible substitutio"
N13-1133,D10-1114,0,0.00406824,"re used to characterize words according to their inverse selectional preference statistics for typical dependency relations. The representation of a word in its context is computed via combining the basic representation of a word with the inverse selectional preference vectors of its related words from the context. Ranking is done by comparing vectors of possible substitutions with the substitution target. Thater et al. (2010) took a similar approach but used second order co-occurrence vectors and report improved performance. An exemplar-based approach is presented by Erk and Pad´o (2010) and Reisinger and Mooney (2010b) to model word meaning with respect to its context: instead of representing the word and the context as separate vectors and combining them, a set of word occurrences in similar contexts is picked first, and then only these exemplars are used to represent the word in context. While this approach provides good results with relatively simple and transparent models, each occurrence of a word has a unique representation (that can only be computed at testing time), and it is computationally expensive to scale these models to a large number of examples. 1133 Dinu and Lapata (2010) used a bag of wo"
N13-1133,N10-1013,0,0.104571,"and a different learning setup: we train a model for contextualization, rather than to combine substitutions from several different resources. A recent work by Sinha and Mihalcea (2011) used an approach based on graph centrality to rank the candidates and achieved comparable performance to n-gram-frequency-based ranking. To summarize, the use of n-gram frequencies for ranking and WordNet as the (most appropriate single) source of synonyms is competitive to more complex solutions and provides a simple and strong lexical substitution system. This motivated the follow-up work by Chang and Clark (2010) to use WordNet and n-grams in a linguistic steganography application and this motivates us to use this method as our baseline. 2.2 Ranking Word Meaning in Context Another prominent line of related work focused solely on the accurate ranking of a pre-given set of possible synonyms, according to their plausibility as a substitution in a given context. Typically, lexical substitution data is used for evaluation purposes, taking the candidate substitutions directly from the test data. This choice is motivated by the assumption that better semantic models should rank nearsynonyms more accurately a"
N13-1133,R09-1073,0,0.285512,"Missing"
N13-1133,P10-1097,0,0.0860226,"Missing"
N13-1133,I11-1127,0,0.0486716,"Missing"
N13-1133,C98-2122,0,\N,Missing
N18-1036,jain-etal-2014-corpus,0,\N,Missing
N18-1036,P16-1150,1,\N,Missing
N18-1036,L18-1526,1,\N,Missing
N18-1134,P98-1013,0,0.51691,"g instances of the first in the second space. We aim to obtain visual correspondences for the textual embeddings in order to incorporate regularities from images into our system. The mapping is a nonlinear transformation using a simple neural network. The objective is to minimize the cosine distance between each mapped representation of a word and the corresponding visual representation. Finally, a multimodal representation for any word can be obtained by applying this mapping to the word embedding. 1484 Data and Preparation of Splits English FrameId: Berkeley FrameNet. The Berkeley FrameNet (Baker et al., 1998; Ruppenhofer et al., 2016) is an ongoing project for building a large lexical resource for English with expert annotations based on frame semantics (Fillmore, 1976). It consists of two parts, a manually created lexicon that maps predicates to the frames they can evoke, and fully annotated texts (fulltext). The mapping can be used to facilitate the frame identification for a predicate in a sentence, e.g., a sentence in the fulltext corpus. Table 1 contains the lexicon statistics, Table 2 (top left) the dataset statistics. In this work, we use FrameNet 1.5 to ensure comparability with the previ"
N18-1134,burchardt-etal-2006-salsa,0,0.138305,"Missing"
N18-1134,D14-1179,0,0.0103806,"Missing"
N18-1134,P11-1144,0,0.0246634,"or English with expert annotations based on frame semantics (Fillmore, 1976). It consists of two parts, a manually created lexicon that maps predicates to the frames they can evoke, and fully annotated texts (fulltext). The mapping can be used to facilitate the frame identification for a predicate in a sentence, e.g., a sentence in the fulltext corpus. Table 1 contains the lexicon statistics, Table 2 (top left) the dataset statistics. In this work, we use FrameNet 1.5 to ensure comparability with the previous state of the art, with the common evaluation split for FrameId systems introduced by Das and Smith (2011) (with the development split of Hermann et al., 2014). Due to having a single annotation as consent of experts, it is hard to estimate a performance bound of a single human for the fulltext annotation. German FrameId: SALSA. The SALSA project (Burchardt et al., 2006; Rehbein et al., 2012) is a completed annotation project, which serves as the German counterpart to FrameNet. Its annotations are based on FrameNet up to version 1.2. SALSA adds proto-frames to properly annotate senses that are not covered by the English FrameNet. For a more detailed description of differences between FrameNet and"
N18-1134,N16-1022,0,0.0194476,"articular the exploration our findings on visual contributions to FrameId in the context of further event prediction tasks forms an interesting next step. More precisely, future work should consider using implicit knowledge not only from images of the participants of the situation, but also from the entire scene in order to directly capture relations between the participants. This could provide access to a more holistic understanding of the scene. The following visual tasks with accompanying datasets could serve as a starting point: (a) visual Verb Sense Disambiguation with the VerSe dataset (Gella et al., 2016) and (b) visual SRL with several datasets, e.g., imSitu (Yatskar et al., 2016) (linked to FrameNet), V-COCO (Gupta and Malik, 2015) (verbs linked to COCO), VVN (Ronchi and Perona, 2015) (visual VerbNet) or even SRL grounded in video clips for the cooking-domain (Yang et al., 2016) and visual Situation Recognition (Mallya and Lazebnik, 2017). Such datasets could be used for extracting visual embeddings for verbs or even complex situations in order to improve the visual component in the embeddings 9 Conclusion Acknowledgments This work has been supported by the DFGfunded research training group"
N18-1134,E17-1045,1,0.433736,"ion tasks. These aim at linking events and their participants to script knowledge and at predicting events in narrative chains. Ahrendt and Demberg (2016) argue that knowing about the participants helps to identify the event, which suggests the need for implicit context knowledge also for FrameId. This specifically applies to images, which can reflect properties of the participants of a situation in a inherently different way, see Fig. 1. We analyze whether multimodal representations grounded in images can encode common sense knowledge to improve FrameId. To that end, we extend SimpleFrameId (Hartmann et al., 2017), a recent FrameId model based on distributed word embeddings, to the multimodal case and evaluate for English and German. Note that there is a general lack of evaluation of FrameId systems for languages other than English. This is problematic as they yield different challenges; German, for example, due to long distance dependencies. Also, word embeddings trained on different languages have different strengths in ambiguous words. We elaborate on insights from using different datasets by language. Contributions. (1) We propose a pipeline and architecture of a FrameId system, extending stateof-t"
N18-1134,P14-1136,0,0.476867,"he main challenge and source of prediction errors of FrameId systems are ambiguous predicates, which can evoke several frames, e.g., the verb sit evokes the frame Change posture in a context like ‘a person is sitting back on a bench’, while it evokes Being located when ‘a company is sitting in a city’. Understanding the predicate context, and thereby the context of the situation (here, ‘Who / what is sitting where?’), is crucial to identifying the correct frame for ambiguous cases. State-of-the-art FrameId systems model the situational context using pretrained distributed word embeddings (see Hermann et al., 2014). Hence, it is assumed that the context of the situation is explicitly expressed in words. However, language understanding involves implicit knowledge, which is not mentioned but still seems obvious to humans, e.g., ‘people can sit back on a bench, but companies cannot’, ‘companies are in cities’. Such implicit common sense knowledge is obvious enough to be rarely expressed in sentences, but is more likely to be present in images. Figure 1 takes the ambiguous predicate sit to illustrate An essential step in FrameNet Semantic Role Labeling is the Frame Identification (FrameId) task, which aims"
N18-1134,D15-1245,0,0.0180055,"to consider the data counts of the Data Baseline only for those frames available for a predicate. We propose to combine them into a Data-Lexicon Baseline, which uses the lexicon for unambiguous predicates and for ambiguous ones it uses the data majority. This way, we trust the lexicon for unambiguous predicates but not for ambiguous ones, there we rather consider the data majority. Comparing a system to these baselines helps to see whether it just memorizes the data majority or the lexicon, or actually captures more. All majority baselines strongly outperform the weak translation baseline of Johannsen et al. (2015) when training the system on English data and evaluating it on German data. 4 Preparation of Input Embeddings Textual embeddings for words. We use the 300-dimensional GloVe embeddings (Pennington et al., 2014) for English, and the 100-dimensional embeddings of Reimers et al. (2014) for German. GloVe and Reimers have been trained on the Wikipedia of their targeted language and on additional newswire text to cover more domains, resulting in similarly low out-of-vocabulary scores. Visual embeddings for synsets. We obtain visual embeddings for WordNet synsets (Fellbaum, 1998; , Ed.): we apply the"
N18-1134,D14-1005,0,0.0309023,"Id, our system is based on pretrained embeddings to build the input representation out of the predicate context and the predicate itself. However, different to SimpleFrameId, our representation of the predicate context is multimodal: beyond textual embeddings we also use I MAG INED and visual embeddings. More precisely, we concatenate all unimodal representations of the predicate context, which in turn are the unimodal mean embeddings of all words in the sentence. We use concatenation for fusing the different embeddings as it is the simplest yet successful fusion approach (Bruni et al., 2014; Kiela and Bottou, 2014). The input representation is processed by a two-layer Multilayer Perceptron (MLP, Rosenblatt, 1958), where we adapt the number of hidden nodes to the increased input size and apply dropout to all hidden layers to prevent overfitting (Srivastava et al., 2014). Each node in the output layer corresponds to one frame-label class. We use rectified linear units (Nair and Hinton, 2010) as activation function for the hidden layers, and a softFigure 2: Sketch of the pipeline. (1) Data: sentence with predicate. (2) Mapping: words to embeddings. (3) Representation: concatenation of modality-specific mea"
N18-1134,P10-1023,0,0.0452436,"Missing"
N18-1134,D17-1035,1,0.845817,"duce noise in sentences SALSA FrameN 5 frames reduced sentences syns-Vis syns-AutoExt train dev test 2819 707 2420 15 406 4593 4546 1310 320 913 2714 701 2318 train dev test 16 852 3561 3605 26 081 5533 5660 4707 1063 1032 16 736 3540 3570 Table 2: Dataset statistics for FrameNet 1.5 fulltext with Das split and for SALSA 2.0 with our split: number of sentences and frames (as used in our experiments). Right half (only used in further investigations): number of sentences when reduced to only those having synsets in the visual and in the linguistic AutoExtend embeddings. 1485 the evaluation (cf. Reimers and Gurevych, 2017) and report the mean for each metric. Use of lexicon. We evaluate our system in two settings: with and without lexicon, as suggested by Hartmann et al. (2017). In the with-lexicon setting, the lexicon is used to reduce the choice of frames for a predicate to only those listed in the lexicon. If the predicate is not in the lexicon, it corresponds to the without-lexicon setting, where the choice has to be done amongst all frames. Evaluation metrics. FrameId systems are usually compared in terms of accuracy, which we adopt for comparability. As a multiclass classification problem, FrameId has to"
N18-1134,P15-1173,0,0.054878,"Missing"
N18-1134,N16-1020,0,0.0273601,"than English. 2.2 Multimodal representation learning There is a growing interest in Natural Language Processing for enriching traditional approaches with knowledge from the visual domain, as images capture qualitatively different information compared to text. Regarding FrameId, to the best of our knowledge, multimodal approaches have not yet been investigated. For other tasks, multimodal approaches based on pretrained embeddings are reported to be superior to unimodal approaches. Textual embeddings have been enriched with information from the visual domain, e.g., for Metaphor Identification (Shutova et al., 2016), Question Answering (Wu et al., 2017), and Word Pair Similarity (Collell et al., 2017). The latter presents a simple, but effective way of extending textual embeddings with so-called multimodal I MAGINED embeddings by a learned mapping from language to vision. We apply the I MAG INED method to our problem. In this work, we aim to uncover whether representations that are grounded in images can help to improve the accuracy of FrameId. Our application case of FrameId is more complex than a comparison on the word-pair level as it considers a whole sentence in order to identify the predicate’s fra"
N18-1134,C10-2107,0,0.0607934,"Missing"
N18-1134,D14-1162,0,0.082155,"d for ambiguous ones it uses the data majority. This way, we trust the lexicon for unambiguous predicates but not for ambiguous ones, there we rather consider the data majority. Comparing a system to these baselines helps to see whether it just memorizes the data majority or the lexicon, or actually captures more. All majority baselines strongly outperform the weak translation baseline of Johannsen et al. (2015) when training the system on English data and evaluating it on German data. 4 Preparation of Input Embeddings Textual embeddings for words. We use the 300-dimensional GloVe embeddings (Pennington et al., 2014) for English, and the 100-dimensional embeddings of Reimers et al. (2014) for German. GloVe and Reimers have been trained on the Wikipedia of their targeted language and on additional newswire text to cover more domains, resulting in similarly low out-of-vocabulary scores. Visual embeddings for synsets. We obtain visual embeddings for WordNet synsets (Fellbaum, 1998; , Ed.): we apply the pretrained VGG-m128 Convolutional Neural Network model (Chatfield et al., 2014) to images for synsets from ImageNet (Deng et al., 2009), we extract the 128dimensional activation of the last layer (before the s"
N18-1134,N16-1019,0,0.0246854,"but also from the entire scene in order to directly capture relations between the participants. This could provide access to a more holistic understanding of the scene. The following visual tasks with accompanying datasets could serve as a starting point: (a) visual Verb Sense Disambiguation with the VerSe dataset (Gella et al., 2016) and (b) visual SRL with several datasets, e.g., imSitu (Yatskar et al., 2016) (linked to FrameNet), V-COCO (Gupta and Malik, 2015) (verbs linked to COCO), VVN (Ronchi and Perona, 2015) (visual VerbNet) or even SRL grounded in video clips for the cooking-domain (Yang et al., 2016) and visual Situation Recognition (Mallya and Lazebnik, 2017). Such datasets could be used for extracting visual embeddings for verbs or even complex situations in order to improve the visual component in the embeddings 9 Conclusion Acknowledgments This work has been supported by the DFGfunded research training group “Adaptive Preparation of Information form Heterogeneous Sources” (AIPHES, GRK 1994/1). We also acknowledge the useful comments of the anonymous reviewers. References Simon Ahrendt and Vera Demberg. 2016. Improving event prediction by representing script participants. In Proceeding"
N18-2006,N16-1165,0,0.150041,"Missing"
N18-2006,E17-1005,0,0.321473,"stics gree training a system to solve several conceptually different AM tasks jointly improves performance over learning in isolation, (2) compare performance gains across different dataset sizes, and (3) do so across various domains. Our findings show that MTL is helpful for AM—particularly in data sparsity settings—when treating other AM tasks as auxiliary.1 2 out of five semantic (i.e., higher level) tasks that they study. We are among the first to perform a structured investigation of MTL for higher-level pragmatic tasks, which are thought to be much more challenging than syntactic tasks (Alonso and Plank, 2017), and in particular, explore it for AM in cross-domain settings. 3 Related Work Experiments Data We experiment with six datasets for argument component identification, i.e. the token-level segmentation and typing of components. These datasets are all of different sizes, have different average text lengths, and different argument component types and label distributions, as summarized in Table 1. We only choose datasets containing both argumentative components and nonargumentative text. Claims are available in five of six datasets, and all datasets have premises (resp. “justification”), although"
N18-2006,P17-2054,0,0.0330385,"knowledge” is by means of multi-task learning (MTL), a paradigm that dates back to the 1990s (Caruana, 1993, 1996), but has only recently gained large attention (Collobert et al., 2011; Søgaard and Goldberg, 2016; Hashimoto et al., 2017). The idea behind MTL is to learn several tasks jointly, similarly to human learning, so that tasks serve as mutual sources of “inductive bias” for one another. MTL has been reported particularly beneficial when tasks exhibit “natural hierarchies” (Søgaard and Goldberg, 2016) or when the amount of training data for the main task is sparse (Benton et al., 2017; Augenstein and Søgaard, 2017), where the auxiliary tasks may act as regularizers to prevent overfitting (Ruder et al., 2017). The latter is precisely the scenario most relevant to us. In this paper, we (1) investigate to which deWe investigate whether and where multitask learning (MTL) can improve performance on NLP problems related to argumentation mining (AM), in particular argument component identification. Our results show that MTL performs particularly well (and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumpti"
N18-2006,E17-2026,0,0.057282,"Missing"
N18-2006,C16-1013,0,0.0371812,"corpora. We use six such corpora, described in more detail in Section 3. On the modeling side, Stab and Gurevych (2017) and Persing and Ng (2016) rely on pipeline approaches for AM, combining parts of the pipeline using integer linear programming (ILP). Eger et al. (2017) propose neural end-to-end models for AM. While Daxenberger et al. (2017) show that there is little consensus on the conceptualization of a claim across AM corpora, Al-Khatib et al. (2016) use distant supervision to overcome domain gaps for identifying (non-)argumentative text. MTL has been applied in many different settings. Bollmann and Søgaard (2016) and Peng and Dredze (2017) use data from different domains as different tasks and thereby improve historical spelling normalization and Chinese word segmentation and NER, respectively. Plank et al. (2016) apply an MTL setup to POS tagging across 22 different languages, where the auxiliary task is to predict token frequency. Eger et al. (2017) explore sub-tasks (such as component identification) of a complex AM tagging problem (including relations between components) as auxiliaries and find that this improves performances. However, they stay within one single domain and dataset, and thus their"
N18-2006,P17-2037,0,0.0541036,"segmentation and NER, respectively. Plank et al. (2016) apply an MTL setup to POS tagging across 22 different languages, where the auxiliary task is to predict token frequency. Eger et al. (2017) explore sub-tasks (such as component identification) of a complex AM tagging problem (including relations between components) as auxiliaries and find that this improves performances. However, they stay within one single domain and dataset, and thus their approach does not address the question how new AM datasets with sparse data can profit from existing AM resources. Conceptually closest to our work, Braud et al. (2017) leverage data from different languages as well as different domains in order to improve discourse parsing. While MTL was shown effective for syntactic tasks under certain conditions (Søgaard and Goldberg, 2016), Alonso and Plank (2017) find that MTL does not improve performances in four Approach Due to the difference in annotations used in the different datasets, we consider each dataset as a separate AM task. We treat all of them as sequence tagging problems, where predicting BIO tags (argument segmentation) and argument component types (component classification) is framed as a joint task. T"
N18-2006,N16-1175,0,0.0695824,"Missing"
N18-2006,C16-1179,0,0.022509,"ience Technische Universit¨at Darmstadt www.ukp.tu-darmstadt.de Abstract reliably in texts (Habernal and Gurevych, 2017). To tackle AM in a new domain or develop new AM tasks, it may thus not be possible to create large datasets as required by most state-of-the-art machine learning approaches. On the other hand, the different conceptualizations of argumentation resulted in AM corpora with different argument component types, with very little conceptual overlap between some of these corpora (Daxenberger et al., 2017). This distinguishes AM from more established NLP tasks like discourse parsing (Braud et al., 2016) and makes it particularly challenging. Therefore, a natural question is how to handle new AM datasets in a new domain and with sparse data. Here, we investigate how existing AM datasets from different domains and with different conceptualizations of arguments can be leveraged to tackle these challenges. More precisely, we study whether conceptually diverse AM datasets from different domains can help deal with new AM datasets when data is limited. A promising direction to incorporate existing datasets as “auxiliary knowledge” is by means of multi-task learning (MTL), a paradigm that dates back"
N18-2006,D17-1142,0,0.0553516,"Missing"
N18-2006,D17-1218,1,0.908083,"er, Tobias Kahse, Iryna Gurevych Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science Technische Universit¨at Darmstadt www.ukp.tu-darmstadt.de Abstract reliably in texts (Habernal and Gurevych, 2017). To tackle AM in a new domain or develop new AM tasks, it may thus not be possible to create large datasets as required by most state-of-the-art machine learning approaches. On the other hand, the different conceptualizations of argumentation resulted in AM corpora with different argument component types, with very little conceptual overlap between some of these corpora (Daxenberger et al., 2017). This distinguishes AM from more established NLP tasks like discourse parsing (Braud et al., 2016) and makes it particularly challenging. Therefore, a natural question is how to handle new AM datasets in a new domain and with sparse data. Here, we investigate how existing AM datasets from different domains and with different conceptualizations of arguments can be leveraged to tackle these challenges. More precisely, we study whether conceptually diverse AM datasets from different domains can help deal with new AM datasets when data is limited. A promising direction to incorporate existing dat"
N18-2006,P17-1002,1,0.926349,"nt and that MTL is difficult for semantic or higher-level tasks. 1 Introduction Computational argumentation mining (AM) deals with the automatic identification of argumentative structures within natural language. This can be beneficial in many applications such as summarizing arguments in texts to improve comprehensibility for end-users, or information retrieval and extraction (Persing and Ng, 2016). A common task is to segment a text into argumentative and nonargumentative components and identify the type of argumentative components. As an illustration, consider the (simplified) example from Eger et al. (2017): “Since [it killed many marine lives]Premise [tourism has threatened nature]Claim .” Here, the non-argumentative token “Since” is followed by two argumentative components: a premise that supports a claim. Argumentation is highly subjective and conceptualized in different ways (Peldszus and Stede, 2013; Al-Khatib et al., 2017). On the one hand, this implies that creating reliable ground-truth datasets for AM is costly, as it requires trained annotators. However, even trained annotators have problems identifying and classifying arguments 35 Proceedings of NAACL-HLT 2018, pages 35–41 c New Orlea"
N18-2006,D14-1162,0,0.0870575,"cannot be attributed to more available data. Figure 1 shows the general trends of our results. For each dataset, the figure plots the difference between normalized MTL and normalized STL macro-F1 scores (MTLnorm (k)−STLnorm (k)) for k = 1K, 6K, 12K, 21K training data points for the main task. For each specific dataset, the normalized macro-F1 score is defined as σnorm (k) = σ(k) STL(1K) , where σ(k) is the original macro-F1 score and STL(1K) denotes the STL score for 1K trainHyperparameter optimization For each sparsity scenario and dataset we train 50 STL/MTL systems using GloVe embeddings (Pennington et al., 2014) and 50 using the embeddings by Komninos and Manandhar (2016). For each run we randomly choose a layout with either one hidden layer of h ∈ {50, 100, 150} units or two layers of 100 units as well as variational dropout rates between 0.2 and 0.5 for the input layer and for the hidden units. 2 Or more, since whole documents are added to the training set until the sum of tokens is at least 21K. Similarly for smaller training and dev sets. 3 37 As implemented in scikit-learn (Pedregosa et al., 2011). Dataset 21K 12K 6K 1K 0.5 var – STL var – MTL var – BL 43.34 47.39 30.45 42.85 45.63 27.35 38.89 4"
N18-2006,N16-1164,0,0.209498,"and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumptions that conceptualizations across AM datasets are divergent and that MTL is difficult for semantic or higher-level tasks. 1 Introduction Computational argumentation mining (AM) deals with the automatic identification of argumentative structures within natural language. This can be beneficial in many applications such as summarizing arguments in texts to improve comprehensibility for end-users, or information retrieval and extraction (Persing and Ng, 2016). A common task is to segment a text into argumentative and nonargumentative components and identify the type of argumentative components. As an illustration, consider the (simplified) example from Eger et al. (2017): “Since [it killed many marine lives]Premise [tourism has threatened nature]Claim .” Here, the non-argumentative token “Since” is followed by two argumentative components: a premise that supports a claim. Argumentation is highly subjective and conceptualized in different ways (Peldszus and Stede, 2013; Al-Khatib et al., 2017). On the one hand, this implies that creating reliable g"
N18-2006,J17-1004,1,0.835779,"Missing"
N18-2006,P16-2067,0,0.0290742,"using integer linear programming (ILP). Eger et al. (2017) propose neural end-to-end models for AM. While Daxenberger et al. (2017) show that there is little consensus on the conceptualization of a claim across AM corpora, Al-Khatib et al. (2016) use distant supervision to overcome domain gaps for identifying (non-)argumentative text. MTL has been applied in many different settings. Bollmann and Søgaard (2016) and Peng and Dredze (2017) use data from different domains as different tasks and thereby improve historical spelling normalization and Chinese word segmentation and NER, respectively. Plank et al. (2016) apply an MTL setup to POS tagging across 22 different languages, where the auxiliary task is to predict token frequency. Eger et al. (2017) explore sub-tasks (such as component identification) of a complex AM tagging problem (including relations between components) as auxiliaries and find that this improves performances. However, they stay within one single domain and dataset, and thus their approach does not address the question how new AM datasets with sparse data can profit from existing AM resources. Conceptually closest to our work, Braud et al. (2017) leverage data from different langua"
N18-2006,S18-1121,1,0.881689,"Missing"
N18-2006,D17-1206,0,0.0267578,"data. Here, we investigate how existing AM datasets from different domains and with different conceptualizations of arguments can be leveraged to tackle these challenges. More precisely, we study whether conceptually diverse AM datasets from different domains can help deal with new AM datasets when data is limited. A promising direction to incorporate existing datasets as “auxiliary knowledge” is by means of multi-task learning (MTL), a paradigm that dates back to the 1990s (Caruana, 1993, 1996), but has only recently gained large attention (Collobert et al., 2011; Søgaard and Goldberg, 2016; Hashimoto et al., 2017). The idea behind MTL is to learn several tasks jointly, similarly to human learning, so that tasks serve as mutual sources of “inductive bias” for one another. MTL has been reported particularly beneficial when tasks exhibit “natural hierarchies” (Søgaard and Goldberg, 2016) or when the amount of training data for the main task is sparse (Benton et al., 2017; Augenstein and Søgaard, 2017), where the auxiliary tasks may act as regularizers to prevent overfitting (Ruder et al., 2017). The latter is precisely the scenario most relevant to us. In this paper, we (1) investigate to which deWe inves"
N18-2006,reed-etal-2008-language,0,0.163321,"Missing"
N18-2006,D17-1035,1,0.828917,"he different datasets, we consider each dataset as a separate AM task. We treat all of them as sequence tagging problems, where predicting BIO tags (argument segmentation) and argument component types (component classification) is framed as a joint task. This is achieved through token-level BIO tagging with the label set {O} ∪ {B, I} × T , where T is a dataset specific set of argument component types, e.g. T = {claim, premise, . . .}. Thus, the overall number of tags in each dataset is twice the number of non-“O” component types plus one (2 · |T |+ 1). We use the state-of-the-art framework by Reimers and Gurevych (2017) for both single-task learning (STL) and MTL. It employs a bidirectional LSTM (BILSTM) model with a CRF layer over individual LSTM outputs to account for label dependencies. We use nadam as optimizer. For MTL, the recurrent layers of the deep BILSTM are shared by all tasks, with a separate CRF layer for each task. All tasks terminate at the same level. The main task determines the number of mini-batches used for training, i.e. in every iteration the main task is trained on all its mini-batches and all other 1 The code and data used for our experiments are available from https://github.com/UKPL"
N18-2006,P16-2038,0,0.269499,"new domain and with sparse data. Here, we investigate how existing AM datasets from different domains and with different conceptualizations of arguments can be leveraged to tackle these challenges. More precisely, we study whether conceptually diverse AM datasets from different domains can help deal with new AM datasets when data is limited. A promising direction to incorporate existing datasets as “auxiliary knowledge” is by means of multi-task learning (MTL), a paradigm that dates back to the 1990s (Caruana, 1993, 1996), but has only recently gained large attention (Collobert et al., 2011; Søgaard and Goldberg, 2016; Hashimoto et al., 2017). The idea behind MTL is to learn several tasks jointly, similarly to human learning, so that tasks serve as mutual sources of “inductive bias” for one another. MTL has been reported particularly beneficial when tasks exhibit “natural hierarchies” (Søgaard and Goldberg, 2016) or when the amount of training data for the main task is sparse (Benton et al., 2017; Augenstein and Søgaard, 2017), where the auxiliary tasks may act as regularizers to prevent overfitting (Ruder et al., 2017). The latter is precisely the scenario most relevant to us. In this paper, we (1) invest"
N18-2006,J17-3005,1,0.650405,"tive text. Claims are available in five of six datasets, and all datasets have premises (resp. “justification”), although it is unclear how large the conceptual overlap is across datasets. Further component types are idiosyncractic. hotel has the largest number of types, namely, six. Most datasets also come with further information, e.g. relations between argument components, which are not considered here. AM is a relatively new field in NLP. Hence, a lot of related work revolves around creating new corpora. We use six such corpora, described in more detail in Section 3. On the modeling side, Stab and Gurevych (2017) and Persing and Ng (2016) rely on pipeline approaches for AM, combining parts of the pipeline using integer linear programming (ILP). Eger et al. (2017) propose neural end-to-end models for AM. While Daxenberger et al. (2017) show that there is little consensus on the conceptualization of a claim across AM corpora, Al-Khatib et al. (2016) use distant supervision to overcome domain gaps for identifying (non-)argumentative text. MTL has been applied in many different settings. Bollmann and Søgaard (2016) and Peng and Dredze (2017) use data from different domains as different tasks and thereby i"
N18-2006,D17-1141,0,\N,Missing
N18-2103,C16-1099,1,0.882699,"Missing"
N18-2103,P15-2136,0,0.113961,"se, this objective function would encode all the relevant quality aspects of a summary, such that by maximizing all these quality aspects we would obtain the best possible summary. However, we find several issues with the objective function in previous work on optimizationbased summarization. First, the choice of the objective function is based on ad-hoc assumptions about which quality aspects of a summary are relevant (Kupiec et al., 1995). This bias can be mitigated via supervised techniques guided by data. In practice, these approaches use signals at the sentence (Conroy and O’leary, 2001; Cao et al., 2015) or n-gram (Hong and Nenkova, 2014; Li et al., 2013) level and then define a combination function to estimate the quality of the whole summary (Carbonell and Goldstein, 1998; Ren et al., 2016). 654 Proceedings of NAACL-HLT 2018, pages 654–660 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics posing any mathematical restrictions on θ is feasible (Peyrard and Eckle-Kohler, 2016). In summary, our contributions are: (1) We propose to learn a summary-level scoring function θ and use human judgments as supervision. (2) We demonstrate a simple regularization s"
N18-2103,P13-1099,0,0.109209,"vant quality aspects of a summary, such that by maximizing all these quality aspects we would obtain the best possible summary. However, we find several issues with the objective function in previous work on optimizationbased summarization. First, the choice of the objective function is based on ad-hoc assumptions about which quality aspects of a summary are relevant (Kupiec et al., 1995). This bias can be mitigated via supervised techniques guided by data. In practice, these approaches use signals at the sentence (Conroy and O’leary, 2001; Cao et al., 2015) or n-gram (Hong and Nenkova, 2014; Li et al., 2013) level and then define a combination function to estimate the quality of the whole summary (Carbonell and Goldstein, 1998; Ren et al., 2016). 654 Proceedings of NAACL-HLT 2018, pages 654–660 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics posing any mathematical restrictions on θ is feasible (Peyrard and Eckle-Kohler, 2016). In summary, our contributions are: (1) We propose to learn a summary-level scoring function θ and use human judgments as supervision. (2) We demonstrate a simple regularization strategy based on automatic data generation to improv"
N18-2103,W04-1013,0,0.0596593,"up AIPHES and UKP Lab Computer Science Department, Technische Universit¨at Darmstadt www.aiphes.tu-darmstadt.de, www.ukp.tu-darmstadt.de Abstract This combination θ determines the trade-off between conflicting quality aspects (importance vs redundancy) encoded in the objective function by making simplistic assumptions to ensure convenient mathematical properties of θ like linearity or submodularity (Lin and Bilmes, 2011). This restriction comes from computational considerations without conceptual justifications. More importantly, the supervision signal comes from automatic metrics like ROUGE (Lin, 2004) which are convenient but noisy approximations for human judgment. In this work, we propose to learn the objective function θ at the summary-level from a pool of manually annotated system summaries to ensure the extraction of summaries considered good by humans. This explicitly targets the extraction of high-quality summaries as measured by humans and limits undesired gaming of the target evaluation metric. However, the number of data points is relatively low and the learned θ might not be well-behaved (high θ scores for bad summaries) pushing the optimizer to explore regions of the feature sp"
N18-2103,N06-1059,0,0.0530168,"tracted by the genetic optimizer. Table 1: Performance of learned θ’s compared to Summaries Evaluation Now, we evaluate the summaries extracted by the genetic optimizer with θ as fitness function (noted (θ, Gen)). We still train θ with leave-one-out cross-validation. To evaluate summaries, we report the ROUGE variant identified by Owczarzak et al. (2012) as strongly correlating with human evaluation methods: ROUGE-2 (R2) recall with stemming and stopwords not removed. We also report JS2, the Jensen-Shannon divergence between bigrams in the reference summaries and the candidate system summary (Lin et al., 2006). The last metric is S3 (Peyrard et al., 2017), a combination of several existing metrics trained explicitly to maximize its correlation with human judgments. Finally, our approach aims at improving summarization systems based on human judgments, therefore we also set up a manual evaluation for the two English datasets. Two annotators were given the summaries of every system for 10 randomly selected topic of both TAC-2008 and TAC2009. They annotated (with a Cohen’s kappa of 0.73) summaries on a LIKERT scale following the responsiveness guidelines. The results are reported in Table 2. We perfor"
N18-2103,P11-1052,0,0.229821,"Missing"
N18-2103,J13-2002,0,0.621097,"atures Learning a scoring function at the summary-level gives us access to both n-gram/sentence-level features and summary-level features. Sentence-level features can be transferred to the summary-level, while new features capturing the interactions between sentences in the summary become available. As sentence-level features, we used the standard: TF*IDF, n-gram frequency and overlap with the title. As new summary-level features, we used: number of sentences, summary-level redundancy and summary-level n-gram distributions: JensenShannon (JS) divergence with n-gram distribution in the source (Louis and Nenkova, 2013). 2 We train these models separately because the different annotations do not lie on the same scale 3 We didn’t automatically tune the different values of α but observed that [1, 0.5, 0.5] works well in practice. 1 https://github.com/UKPLab/ coling2016-genetic-swarm-MDS 655 N-gram Coverage. Each n-gram gi in the documents has a frequency tf (gi ), the summary S is scored by: X Covn (S) = tf (gi ) Where Un is the set of n-grams (without repetitions) composing S and Sn is the multiset of ngrams (with repetitions). Divergences. This is another feature that can only be computed at the summary-leve"
N18-2103,W09-1802,0,0.189592,"Missing"
N18-2103,N09-1041,0,0.465548,"hese models separately because the different annotations do not lie on the same scale 3 We didn’t automatically tune the different values of α but observed that [1, 0.5, 0.5] works well in practice. 1 https://github.com/UKPLab/ coling2016-genetic-swarm-MDS 655 N-gram Coverage. Each n-gram gi in the documents has a frequency tf (gi ), the summary S is scored by: X Covn (S) = tf (gi ) Where Un is the set of n-grams (without repetitions) composing S and Sn is the multiset of ngrams (with repetitions). Divergences. This is another feature that can only be computed at the summary-level inspired by Haghighi and Vanderwende (2009) and Peyrard and Eckle-Kohler (2016). We compute the KL divergence and JS divergence between n-gram probability distributions of the summaries and of the documents. The probability distributions are built from the two kinds of frequency distributions and for unigrams, bigrams and trigrams. g∈Sn Here Sn is the multiset of n-grams (with repetitions) composing S. Also, the frequency can be computed either by counting the number of occurrence of the n-gram or by counting the number of documents in which the n-gram appears. For both frequency computations, we extract features for unigrams, bigrams"
N18-2103,C14-1156,0,0.0634296,"Missing"
N18-2103,hong-etal-2014-repository,0,0.0396553,"f S cannot be computed at the sentence-level. This is an example of features available at the summary-level but not available at the sentence-level. We define it as the number of unique n-gram types (|Un |) in the summary divided by the total number of n-gram tokens (the length of S) Redn (S) = |Un | |Sn | Experiments Baselines (1) ICSI (Gillick and Favre, 2009) is a global linear optimization approach that extracts a summary by solving a maximum coverage problem considering the most frequent bigrams in the source documents. ICSI has been among the best systems in a standard ROUGE evaluation (Hong et al., 2014). (2) LexRank (Erkan 4 http://tac.nist.gov/2009/ Summarization/, http://tac.nist.gov/2008/ 656 ρ NDCG Best-Baseline-R θR2 .594 .663 .505 .536 Best-Baseline-Pyr θpyr .492 .554 .715 .780 Best-Baseline-Resp θresp .367 .391 .710 .741 ter than the best baselines. They have a high correlation with human judgments and are capable of identifying good summaries. However, we need to test whether the combination of the three θ’s is well behaved under optimization. For this, we perform an evaluation of the summaries extracted by the genetic optimizer. Table 1: Performance of learned θ’s compared to Summar"
N18-2103,W12-2601,0,0.0257541,"st baselines. They have a high correlation with human judgments and are capable of identifying good summaries. However, we need to test whether the combination of the three θ’s is well behaved under optimization. For this, we perform an evaluation of the summaries extracted by the genetic optimizer. Table 1: Performance of learned θ’s compared to Summaries Evaluation Now, we evaluate the summaries extracted by the genetic optimizer with θ as fitness function (noted (θ, Gen)). We still train θ with leave-one-out cross-validation. To evaluate summaries, we report the ROUGE variant identified by Owczarzak et al. (2012) as strongly correlating with human evaluation methods: ROUGE-2 (R2) recall with stemming and stopwords not removed. We also report JS2, the Jensen-Shannon divergence between bigrams in the reference summaries and the candidate system summary (Lin et al., 2006). The last metric is S3 (Peyrard et al., 2017), a combination of several existing metrics trained explicitly to maximize its correlation with human judgments. Finally, our approach aims at improving summarization systems based on human judgments, therefore we also set up a manual evaluation for the two English datasets. Two annotators we"
N18-2103,E14-1075,0,0.0781359,"ould encode all the relevant quality aspects of a summary, such that by maximizing all these quality aspects we would obtain the best possible summary. However, we find several issues with the objective function in previous work on optimizationbased summarization. First, the choice of the objective function is based on ad-hoc assumptions about which quality aspects of a summary are relevant (Kupiec et al., 1995). This bias can be mitigated via supervised techniques guided by data. In practice, these approaches use signals at the sentence (Conroy and O’leary, 2001; Cao et al., 2015) or n-gram (Hong and Nenkova, 2014; Li et al., 2013) level and then define a combination function to estimate the quality of the whole summary (Carbonell and Goldstein, 1998; Ren et al., 2016). 654 Proceedings of NAACL-HLT 2018, pages 654–660 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics posing any mathematical restrictions on θ is feasible (Peyrard and Eckle-Kohler, 2016). In summary, our contributions are: (1) We propose to learn a summary-level scoring function θ and use human judgments as supervision. (2) We demonstrate a simple regularization strategy based on automatic data ge"
N18-2103,C16-1024,1,0.538154,"ary are relevant (Kupiec et al., 1995). This bias can be mitigated via supervised techniques guided by data. In practice, these approaches use signals at the sentence (Conroy and O’leary, 2001; Cao et al., 2015) or n-gram (Hong and Nenkova, 2014; Li et al., 2013) level and then define a combination function to estimate the quality of the whole summary (Carbonell and Goldstein, 1998; Ren et al., 2016). 654 Proceedings of NAACL-HLT 2018, pages 654–660 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics posing any mathematical restrictions on θ is feasible (Peyrard and Eckle-Kohler, 2016). In summary, our contributions are: (1) We propose to learn a summary-level scoring function θ and use human judgments as supervision. (2) We demonstrate a simple regularization strategy based on automatic data generation to improve the behavior of θ under optimization. (3) We perform both automatic and manual evaluation of the extracted summaries, which indicate competitive performances. 2 2.1 reproduction and mutation rate and set the population size to 50. With x as fitness function, the resulting population is a set of summaries ranging from random to (close to) maximal value. After both"
N18-2103,P17-2005,1,0.740358,"ler (KL) divergence between the word distributions in the summary and the documents. (3) Peyrard and Eckle-Kohler (2016) optimize JS divergence with a genetic algorithm. (4) Finally, SFOUR is a supervised structured prediction approach that trains an end-to-end on a convex relaxation of ROUGE (Sipos et al., 2012). Objective function learning In this section, we measure how well our models can predict human judgments. We train each θ in a leave-one-out cross-validation setup for each dataset and compare their performance to the summary scoring function of baselines like it was done previously (Peyrard and Eckle-Kohler, 2017). Each individual feature is also included in the baselines. Correlations are measured with two complementary metrics: Spearman’s ρ and Normalized Discounted Cumulative Gain (NDCG). Spearman’s ρ is a rank correlation metric, which compares the ordering of systems induced by θ and the ordering of systems induced by human judgments. NDCG is a metric that compares ranked lists and puts more emphasis on the top elements with logarithmic decay weighting. Intuitively, it captures how well θ can recognize the best summaries. The optimization scenario benefits from high NDCG scores because only summar"
N18-2103,C16-1004,0,0.0519223,"find several issues with the objective function in previous work on optimizationbased summarization. First, the choice of the objective function is based on ad-hoc assumptions about which quality aspects of a summary are relevant (Kupiec et al., 1995). This bias can be mitigated via supervised techniques guided by data. In practice, these approaches use signals at the sentence (Conroy and O’leary, 2001; Cao et al., 2015) or n-gram (Hong and Nenkova, 2014; Li et al., 2013) level and then define a combination function to estimate the quality of the whole summary (Carbonell and Goldstein, 1998; Ren et al., 2016). 654 Proceedings of NAACL-HLT 2018, pages 654–660 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics posing any mathematical restrictions on θ is feasible (Peyrard and Eckle-Kohler, 2016). In summary, our contributions are: (1) We propose to learn a summary-level scoring function θ and use human judgments as supervision. (2) We demonstrate a simple regularization strategy based on automatic data generation to improve the behavior of θ under optimization. (3) We perform both automatic and manual evaluation of the extracted summaries, which indicate compe"
N18-2103,E12-1023,0,0.280122,"metrics. In particular, humans tend to prefer the summaries extracted by the best baselines for each type annotation types. and Radev, 2004) is a graph-based approach computing sentence centrality based on the PageRank algorithm. (3) KL-Greedy (Haghighi and Vanderwende, 2009) minimizes the Kullback-Leibler (KL) divergence between the word distributions in the summary and the documents. (3) Peyrard and Eckle-Kohler (2016) optimize JS divergence with a genetic algorithm. (4) Finally, SFOUR is a supervised structured prediction approach that trains an end-to-end on a convex relaxation of ROUGE (Sipos et al., 2012). Objective function learning In this section, we measure how well our models can predict human judgments. We train each θ in a leave-one-out cross-validation setup for each dataset and compare their performance to the summary scoring function of baselines like it was done previously (Peyrard and Eckle-Kohler, 2017). Each individual feature is also included in the baselines. Correlations are measured with two complementary metrics: Spearman’s ρ and Normalized Discounted Cumulative Gain (NDCG). Spearman’s ρ is a rank correlation metric, which compares the ordering of systems induced by θ and th"
N18-5005,W17-5115,0,0.023178,"hey provide no specialized support for them. Despite its obvious applications, argument search has attracted relatively little attention in the argument mining community. In this paper, we present ArgumenText, which we believe is the first system for topic-relevant argument search in heterogeneous texts. It takes a large collection of arbitrary Web texts, automatically identifies arguments relevant to a given topic, classifies them as “pro” or “con”, and 2 Related Work Most existing approaches consider argument mining at the discourse level and address tasks like argument unit identification (Ajjour et al., 2017), component classification (Mochales-Palau and Moens, 2009), or argument structure identification (Eger et al., 2017). These approaches focus on recognizing arguments within a single text but do not consider relevance to user-defined topics. Until now, there has been little work on identifying topic-relevant arguments. Wachsmuth et al. (2017) present a generic framework for argument search that relies on pre-structured arguments from debate portals. Levy et al. (2014) present a system designed specifically for detecting topic-relevant claims from Wikipedia, which was later extended to mine sup"
N18-5005,W14-5201,1,0.894337,"Missing"
N18-5005,D15-1050,0,0.102582,"hales-Palau and Moens, 2009), or argument structure identification (Eger et al., 2017). These approaches focus on recognizing arguments within a single text but do not consider relevance to user-defined topics. Until now, there has been little work on identifying topic-relevant arguments. Wachsmuth et al. (2017) present a generic framework for argument search that relies on pre-structured arguments from debate portals. Levy et al. (2014) present a system designed specifically for detecting topic-relevant claims from Wikipedia, which was later extended to mine supporting statements for claims (Rinott et al., 2015). The MARGOT system (Lippi and Torroni, 2015) is trained on Wikipedia data and extracts claims and evidence from user-provided texts. However, all these systems focus on specific text types and are not yet able to extract arguments 21 Proceedings of NAACL-HLT 2018: Demonstrations, pages 21–25 c New Orleans, Louisiana, June 2 - 4, 2018. 2018 Association for Computational Linguistics Online processing Offline processing Segmented documents Sentence segmentation Indexing Document Retrieval Topic-relevant documents Argument Recognition Pro and con arguments Web-Interface topic topic Documents Apac"
N18-5005,P17-1002,1,0.848666,"little attention in the argument mining community. In this paper, we present ArgumenText, which we believe is the first system for topic-relevant argument search in heterogeneous texts. It takes a large collection of arbitrary Web texts, automatically identifies arguments relevant to a given topic, classifies them as “pro” or “con”, and 2 Related Work Most existing approaches consider argument mining at the discourse level and address tasks like argument unit identification (Ajjour et al., 2017), component classification (Mochales-Palau and Moens, 2009), or argument structure identification (Eger et al., 2017). These approaches focus on recognizing arguments within a single text but do not consider relevance to user-defined topics. Until now, there has been little work on identifying topic-relevant arguments. Wachsmuth et al. (2017) present a generic framework for argument search that relies on pre-structured arguments from debate portals. Levy et al. (2014) present a system designed specifically for detecting topic-relevant claims from Wikipedia, which was later extended to mine supporting statements for claims (Rinott et al., 2015). The MARGOT system (Lippi and Torroni, 2015) is trained on Wikipe"
N18-5005,L16-1146,1,0.832586,"na, June 2 - 4, 2018. 2018 Association for Computational Linguistics Online processing Offline processing Segmented documents Sentence segmentation Indexing Document Retrieval Topic-relevant documents Argument Recognition Pro and con arguments Web-Interface topic topic Documents Apache UIMA Tensorflow / Keras Elasticsearch HTML / Javascript User Figure 1: System architecture. from a large collection of arbitrary texts. The approach most similar to ours, introduced by Hua and Wang (2017), extracts claim-relevant arguments from different text types, but is limited to sentential “pro” arguments. Habernal et al. (2016) for de-duplication, boilerplate removal using jusText (Pomikálek, 2011), and language detection.2 This left us with 400 million heterogeneous plain-text documents in English, with an overall size of 683 GiB. 3 3.2 System Description Each document is segmented into sentences with an Apache UIMA pipeline using components from DKPro Core (Eckart de Castilho and Gurevych, 2014). To facilitate processing of other languages in future work, we chose Apache OpenNLP which currently supports six languages. The modular nature of our setup allows us to easily integrate other sentence segmentation methods"
N18-5005,N13-1132,0,0.120427,"Missing"
N18-5005,P17-2032,0,0.0493008,"s and are not yet able to extract arguments 21 Proceedings of NAACL-HLT 2018: Demonstrations, pages 21–25 c New Orleans, Louisiana, June 2 - 4, 2018. 2018 Association for Computational Linguistics Online processing Offline processing Segmented documents Sentence segmentation Indexing Document Retrieval Topic-relevant documents Argument Recognition Pro and con arguments Web-Interface topic topic Documents Apache UIMA Tensorflow / Keras Elasticsearch HTML / Javascript User Figure 1: System architecture. from a large collection of arbitrary texts. The approach most similar to ours, introduced by Hua and Wang (2017), extracts claim-relevant arguments from different text types, but is limited to sentential “pro” arguments. Habernal et al. (2016) for de-duplication, boilerplate removal using jusText (Pomikálek, 2011), and language detection.2 This left us with 400 million heterogeneous plain-text documents in English, with an overall size of 683 GiB. 3 3.2 System Description Each document is segmented into sentences with an Apache UIMA pipeline using components from DKPro Core (Eckart de Castilho and Gurevych, 2014). To facilitate processing of other languages in future work, we chose Apache OpenNLP which"
N18-5005,C14-1141,0,0.185329,"Most existing approaches consider argument mining at the discourse level and address tasks like argument unit identification (Ajjour et al., 2017), component classification (Mochales-Palau and Moens, 2009), or argument structure identification (Eger et al., 2017). These approaches focus on recognizing arguments within a single text but do not consider relevance to user-defined topics. Until now, there has been little work on identifying topic-relevant arguments. Wachsmuth et al. (2017) present a generic framework for argument search that relies on pre-structured arguments from debate portals. Levy et al. (2014) present a system designed specifically for detecting topic-relevant claims from Wikipedia, which was later extended to mine supporting statements for claims (Rinott et al., 2015). The MARGOT system (Lippi and Torroni, 2015) is trained on Wikipedia data and extracts claims and evidence from user-provided texts. However, all these systems focus on specific text types and are not yet able to extract arguments 21 Proceedings of NAACL-HLT 2018: Demonstrations, pages 21–25 c New Orleans, Louisiana, June 2 - 4, 2018. 2018 Association for Computational Linguistics Online processing Offline processing"
N18-5005,W17-5106,0,0.0891137,"trary Web texts, automatically identifies arguments relevant to a given topic, classifies them as “pro” or “con”, and 2 Related Work Most existing approaches consider argument mining at the discourse level and address tasks like argument unit identification (Ajjour et al., 2017), component classification (Mochales-Palau and Moens, 2009), or argument structure identification (Eger et al., 2017). These approaches focus on recognizing arguments within a single text but do not consider relevance to user-defined topics. Until now, there has been little work on identifying topic-relevant arguments. Wachsmuth et al. (2017) present a generic framework for argument search that relies on pre-structured arguments from debate portals. Levy et al. (2014) present a system designed specifically for detecting topic-relevant claims from Wikipedia, which was later extended to mine supporting statements for claims (Rinott et al., 2015). The MARGOT system (Lippi and Torroni, 2015) is trained on Wikipedia data and extracts claims and evidence from user-provided texts. However, all these systems focus on specific text types and are not yet able to extract arguments 21 Proceedings of NAACL-HLT 2018: Demonstrations, pages 21–25"
N19-1074,N06-1046,0,0.0553845,", 2013), in a log-linear classifier to predict coreferences of mentions. Since such pairwise predictions can be inconsistent, e.g. the model might classify (m1 , m2 ) and (m2 , m3 ) as coreferent, but not (m1 , m3 ), Falke et al. (2017) further induce a transitive relation from the predictions to obtain a valid partitioning of M . They note that simply ignoring conflicting negative classifications by building the transitive closure over all positive ones typically yields undesired partitionings in which too many mentions are being lumped together. Following previous work on related NLP tasks (Barzilay and Lapata, 2006; Denis and Baldridge, 2007), they instead formulate an integer linear program (ILP) to find the transitive relation that maximally agrees with all pairwise predictions. However, as the resulting ILPs cannot be efficiently solved on the data they work with, they propose a local search algorithm that incrementally improves a greedy solution rather than finding the optimal partitioning, This technique requires making classifications for all pairs of mentions in O(n2 ) time and running the local search, which has a worst-case complexity of O(n4 ). As we will show in Section 6, that can quickly be"
N19-1074,W06-3812,0,0.37077,"mmarized is large, applying that technique can quickly become impractical. But exactly for those large document sets, a summary would be most helpful. As the first contribution of this paper, we propose two faster grouping techniques. First, we apply locality sensitive hashing (LSH) (Charikar, 2002) to word embeddings in order to find similar mentions without making all pairwise comparisons. That directly leads to a simple O(n) grouping method. Second, we also propose a novel grouping technique that combines the hashing approach with a fast partitioning algorithm called Chinese Whispers (CW) (Biemann, 2006). It has O(n log n) time complexity and the advantage of being more transparently controllable. Since the reduced complexity of the two proposed techniques is gained through approximations, the resulting grouping could of course be of lower quality. As the second contribution of this paper, we therefore carry out end-to-end experiments in the context of CM-MDS to analyze this trade-off. We compare both techniques against the state-of-the-art approach in automatic and manual evaluations. For both, we observe orders of magConcept map–based multi-document summarization has recently been proposed"
N19-1074,D17-1070,0,0.0213003,"Missing"
N19-1074,N07-1030,0,0.0232159,"assifier to predict coreferences of mentions. Since such pairwise predictions can be inconsistent, e.g. the model might classify (m1 , m2 ) and (m2 , m3 ) as coreferent, but not (m1 , m3 ), Falke et al. (2017) further induce a transitive relation from the predictions to obtain a valid partitioning of M . They note that simply ignoring conflicting negative classifications by building the transitive closure over all positive ones typically yields undesired partitionings in which too many mentions are being lumped together. Following previous work on related NLP tasks (Barzilay and Lapata, 2006; Denis and Baldridge, 2007), they instead formulate an integer linear program (ILP) to find the transitive relation that maximally agrees with all pairwise predictions. However, as the resulting ILPs cannot be efficiently solved on the data they work with, they propose a local search algorithm that incrementally improves a greedy solution rather than finding the optimal partitioning, This technique requires making classifications for all pairs of mentions in O(n2 ) time and running the local search, which has a worst-case complexity of O(n4 ). As we will show in Section 6, that can quickly become prohibitively expensive"
N19-1074,D17-1320,1,0.879929,", making them much more scalable. We report experimental results that confirm the improved runtime behavior while also showing that the quality of the summary concept maps remains comparable.1 1 Introduction Concept maps are labeled graphs with nodes representing concepts and edges showing relationships between them (Novak and Gowin, 1984). Following earlier work on the automatic extraction of concept maps from text (Rajaraman and Tan, 2002; Valerio and Leake, 2006; Villalon, 2012; Zubrinic et al., 2015), concept maps have recently been promoted as an alternative representation for summaries (Falke and Gurevych, 2017; Handler and O’Connor, 2018). In the corresponding task, concept map–based multi-document summarization (CM-MDS), a set of documents has to be automatically summarized as a concept map that does not exceed a pre-defined size limit. 1 Code used for experiments available at https:// github.com/UKPLab/naacl2019-cmaps-lshcw 695 Proceedings of NAACL-HLT 2019, pages 695–700 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics larities. Charikar (2002) introduced such a family for cosine similarity between vectors. nitude faster runtimes with only small red"
N19-1074,I17-1081,1,0.877931,"ash, with the i-th dimension defined as ( 1 : u · ri ≥ 0 h(u)[i] = (1) 0 : u · ri &lt; 0, Problem and Reference Approach Given a set of concept mentions M identified in the input documents, the goal of concept mention grouping is to derive a partitioning C of M such that for every set of mentions in C, the set contains all mentions and only mentions of one unique concept. Let n denote the number of mentions |M |. Previous work on concept map mining used stemming (Villalon, 2012), substring matches (Valerio and Leake, 2006) or WordNet (Aguiar et al., 2016) to detect coreferences between mentions. Falke et al. (2017) combined several of those features, including semantic similarities based on WordNet (Miller et al., 1990), latent semantic analysis (Deerwester et al., 1990) and word2vec embeddings (Mikolov et al., 2013), in a log-linear classifier to predict coreferences of mentions. Since such pairwise predictions can be inconsistent, e.g. the model might classify (m1 , m2 ) and (m2 , m3 ) as coreferent, but not (m1 , m3 ), Falke et al. (2017) further induce a transitive relation from the predictions to obtain a valid partitioning of M . They note that simply ignoring conflicting negative classifications"
N19-1074,D12-1005,0,0.0341537,"Missing"
N19-1074,N18-1159,0,0.137493,"Missing"
N19-1074,R09-1053,0,0.0373624,"Missing"
N19-1074,N18-1202,0,0.0316626,"Missing"
N19-1074,P05-1077,0,0.400445,"ion 6, that can quickly become prohibitively expensive. 3 Approximating Cosine Similarity where u · ri is the dot product with the i-th random vector. The Hamming distance ham between two hashes h(u) and h(v), i.e. the number of differing bits, can then be used to approximate the cosine similarity of u and v (Charikar, 2002):   u·v ham(h(u), h(v)) ≈ cos π (2) |u||v| d The longer the hashes are, i.e. the larger d is, the more accurate is the estimation of the similarity. In the past, LSH has been successfully used to speed up a range of NLP tasks, including noun similarity list construction (Ravichandran et al., 2005), word sense induction (Mouton et al., 2009), gender classification (van Durme, 2012) and text classification (Bollegala et al., 2018). 3.2 Naive Partitioning Given the mapping h from vectors to their bit hashes, we can partition a set of vectors by hash identity. Every unique hash becomes a group consisting of all vectors mapped to that hash. Since the hashes reflect similarity, the most similar vectors will be grouped together. The parameter d controls the degree of grouping: the smaller it is, the less unique hashes and thus fewer groups exist. In order to apply this technique to concept me"
N19-1165,N18-1202,0,0.137732,"Missing"
N19-1165,D17-1035,1,0.826645,"P I-NP . . . toxic, obscene, insult 5K/1K/1K 212K/44K/47K 212K/44K/47K 149K/10K/64K Table 4: NLP tasks considered in this work, along with (perturbed) examples and data split statistics. s∗ (p) 2009). We frame G2P as a sequence tagging task. To do so, we first hard-align input and output sequences using a 1-0,1-1,1-2 alignment scheme (Schnober et al., 2016) in which an input character is matched with zero, one, or two output characters. Once this preprocessing is done, input and output sequences have equal lengths and we can apply a standard BiLSTM on character-level to the aligned sequences (Reimers and Gurevych, 2017). 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 POS Chunk G2P TC 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 p POS & Chunking: We consider two word-level tasks. POS tagging associates each token with its corresponding word class (e.g., noun, adjective, verb). Chunking groups words into syntactic chunks such as noun and verb phrases (NP and VP), assigning a unique tag to each word, which encodes the position and type of the syntactic constituent, e.g., begin-noun-phrase (B-NP). We use the training, dev and test splits provided by the CoNLL-2000 shared task (Sang and Buchholz, 2000) and use the same BiLSTM"
N19-1165,W00-0726,0,\N,Missing
N19-1165,P14-5010,0,\N,Missing
N19-1165,D11-1006,0,\N,Missing
N19-1165,D13-1032,0,\N,Missing
N19-1165,N16-1175,0,\N,Missing
N19-1165,P18-1079,0,\N,Missing
N19-1165,P18-2006,0,\N,Missing
N19-1165,C18-1071,1,\N,Missing
N19-1165,P16-1101,0,\N,Missing
N19-1165,P18-1241,0,\N,Missing
N19-1165,P18-1096,0,\N,Missing
N19-1165,D18-1472,1,\N,Missing
N19-1177,W14-2109,0,0.0196752,".9. 5It is not possible to treat the missing annotations as “total disagreement” because per Krippendorff (1995), αU has no concept of this; there is no lowest disagreement score. 1792 Figure 1: Annotation interface for the second step (claim annotation) are lower than the agreement among extensively trained annotators reported by Stab and Gurevych (2014) (αU = 0.7726, 0.6033, 0.7594).6 However, they are broadly comparable to interannotator agreement scores reported in similar (and in some cases, even simpler) discourse-level argument annotation studies with expert-trained annotators, such as Aharoni et al. (2014) (κ = 0.4), Musi et al. (2018) (κ = 0.296), and Li et al. (2017) (αU = 0.2452). To measure how the number of crowd annotations impacts reliability, we performed an ablation study where we iteratively removed one crowd annotation at random from each review and repeated the MACE distillation and αU calculation. The study was repeated 100 times and the resulting αU scores averaged. The results are shown in Fig. 2, which plots the average αU scores for major claims, claims, and premises when using one to ten crowd annotations per review. The plots are shown as error bars, where the top of the bar"
N19-1177,P17-1002,1,0.840815,"ourse-level argumentation. This is because discourse-level annotation schemes are necessarily complex with respect to discrimination and delimitation (i.e., the variety of markable elements in the text and how to define their boundaries), expressiveness (i.e., In this paper, we present a method that facilitates the application of one such discourse-level argument annotation scheme (Stab and Gurevych, 2014). This scheme has been widely cited and used in argumentation studies (e.g., Lippi and Torroni, 2015; Persing and Ng, 2015; Nguyen and Litman, 2015; Persing and Ng, 2016; Ghosh et al., 2016; Eger et al., 2017; Nguyen and Litman, 2018), and while it is fairly coarse-grained, it is expensive to apply to new texts. Our method breaks down the annotation process into incremental, intuitive steps, each focusing on a small portion of the overall annotation scheme. We apply this method in a crowdsourcing setup with annotators who receive no training other than a brief set of annotation guidelines, as well as in a more traditional setup with extensively trained local annotators. We find that agreement between the two groups increases sublinearly with the number of crowd annotators, achieving up to αU = 0.5"
N19-1177,C12-1055,0,0.0284855,"tion Annotations from the Crowd Tristan Miller*† and Maria Sukhareva‡ and Iryna Gurevych*† * Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technische Universität Darmstadt https://www.ukp.tu-darmstadt.de/ † Research Training Group AIPHES Department of Computer Science, Technische Universität Darmstadt https://www.aiphes.tu-darmstadt.de/ ‡ NLP Lab BMW Group, Munich Maria.Sukhareva@bmwgroup.com Abstract the need to tag relationships between annotated elements), and context weighting (i.e., the amount of context around markable units that needs to be considered) (Fort et al., 2012). Successfully applying such schemes typically requires expensive and laborious work by expert-trained annotators. The study of argumentation and the development of argument mining tools depends on the availability of annotated data, which is challenging to obtain in sufficient quantity and quality. We present a method that breaks down a popular but relatively complex discourse-level argument annotation scheme into a simpler, iterative procedure that can be applied even by untrained annotators. We apply this method in a crowdsourcing setup and report on the reliability of the annotations obtai"
N19-1177,W10-1807,0,0.0321799,"e documents in order to obtain a common understanding of the task. This level of effort is in line with what has been reported for other discourse-level argumentation schemes. For example, annotation studies using the Freemanesque schemes of Peldszus and Stede (2013), Li et al. (2017), Haddadan et al. (2018), and Musi et al. (2018) all required one or more lengthy training sessions guided by argumentation experts and up to six pages of written instructions. Using existing methods to alleviate the knowledge acquisition bottleneck, such as incidental supervision (Roth, 2017), or pre-annotation (Fort and Sagot, 2010), could speed the work of annotators— possibly at the risk of introducing a training bias— but would not obviate the need for expert training. (In any case, pre-annotation has never, to our knowledge, been successfully applied to hard discourse-level tasks such as annotating argumentation structures.) The complexity of the annotation scheme also seemingly rules out the use of crowdsourcing (Howe, 2006) and gamification (von Ahn, 2006), which are geared towards microtasks that are quick and easy for humans. Though one previous study has decomposed a discourse-level scheme for use with crowdsour"
N19-1177,L18-1258,0,0.084354,"persuasive texts with this scheme, associating each argument component they identify with a contiguous span of text from the document. They report that the annotation process involved “several training sessions” with their annotators, including collaborative annotation of eight example documents in order to obtain a common understanding of the task. This level of effort is in line with what has been reported for other discourse-level argumentation schemes. For example, annotation studies using the Freemanesque schemes of Peldszus and Stede (2013), Li et al. (2017), Haddadan et al. (2018), and Musi et al. (2018) all required one or more lengthy training sessions guided by argumentation experts and up to six pages of written instructions. Using existing methods to alleviate the knowledge acquisition bottleneck, such as incidental supervision (Roth, 2017), or pre-annotation (Fort and Sagot, 2010), could speed the work of annotators— possibly at the risk of introducing a training bias— but would not obviate the need for expert training. (In any case, pre-annotation has never, to our knowledge, been successfully applied to hard discourse-level tasks such as annotating argumentation structures.) The compl"
N19-1177,P16-2089,0,0.0200399,"articularly for discourse-level argumentation. This is because discourse-level annotation schemes are necessarily complex with respect to discrimination and delimitation (i.e., the variety of markable elements in the text and how to define their boundaries), expressiveness (i.e., In this paper, we present a method that facilitates the application of one such discourse-level argument annotation scheme (Stab and Gurevych, 2014). This scheme has been widely cited and used in argumentation studies (e.g., Lippi and Torroni, 2015; Persing and Ng, 2015; Nguyen and Litman, 2015; Persing and Ng, 2016; Ghosh et al., 2016; Eger et al., 2017; Nguyen and Litman, 2018), and while it is fairly coarse-grained, it is expensive to apply to new texts. Our method breaks down the annotation process into incremental, intuitive steps, each focusing on a small portion of the overall annotation scheme. We apply this method in a crowdsourcing setup with annotators who receive no training other than a brief set of annotation guidelines, as well as in a more traditional setup with extensively trained local annotators. We find that agreement between the two groups increases sublinearly with the number of crowd annotators, achie"
N19-1177,W15-0503,0,0.026041,"to obtain in sufficient quantity and quality, particularly for discourse-level argumentation. This is because discourse-level annotation schemes are necessarily complex with respect to discrimination and delimitation (i.e., the variety of markable elements in the text and how to define their boundaries), expressiveness (i.e., In this paper, we present a method that facilitates the application of one such discourse-level argument annotation scheme (Stab and Gurevych, 2014). This scheme has been widely cited and used in argumentation studies (e.g., Lippi and Torroni, 2015; Persing and Ng, 2015; Nguyen and Litman, 2015; Persing and Ng, 2016; Ghosh et al., 2016; Eger et al., 2017; Nguyen and Litman, 2018), and while it is fairly coarse-grained, it is expensive to apply to new texts. Our method breaks down the annotation process into incremental, intuitive steps, each focusing on a small portion of the overall annotation scheme. We apply this method in a crowdsourcing setup with annotators who receive no training other than a brief set of annotation guidelines, as well as in a more traditional setup with extensively trained local annotators. We find that agreement between the two groups increases sublinearly"
N19-1177,N13-1132,0,0.0230726,"em to those produced by expert-trained annotators. For the experiment, we randomly selected 40 Amazon product reviews from the McAuley et al. (2015) data set—four from each of ten product categories. Each review was annotated for major claims by ten crowd workers; all 40 reviews were also annotated for major claims by a fixed group of three locally recruited annotators trained by argumentation experts.3 We then converted the 3We engaged US-based workers from Amazon Mechanical annotated reviews to BIO tokens (Ramshaw and Marcus, 1995) and applied the annotation aggregation/denoising tool MACE (Hovy et al., 2013) to select at most two gold-standard major-claim annotations per review, one from the crowd (crowd) and one from the trained annotators (train).4 We then compared the crowd and train gold standards, one review at a time, using Krippendorff’s (1995) αU , a unitizing measure that considers the tokenlevel boundaries of the text spans marked by each annotator. We repeated this process to obtain and evaluate crowd and train claim annotations on the train major claims, and then again for crowd and train premise annotations on the train claims. Note that in the gold standards for some reviews, there"
N19-1177,W13-2324,0,0.0315166,"(support and attack). Stab and Gurevych (2014) annotate a collection of persuasive texts with this scheme, associating each argument component they identify with a contiguous span of text from the document. They report that the annotation process involved “several training sessions” with their annotators, including collaborative annotation of eight example documents in order to obtain a common understanding of the task. This level of effort is in line with what has been reported for other discourse-level argumentation schemes. For example, annotation studies using the Freemanesque schemes of Peldszus and Stede (2013), Li et al. (2017), Haddadan et al. (2018), and Musi et al. (2018) all required one or more lengthy training sessions guided by argumentation experts and up to six pages of written instructions. Using existing methods to alleviate the knowledge acquisition bottleneck, such as incidental supervision (Roth, 2017), or pre-annotation (Fort and Sagot, 2010), could speed the work of annotators— possibly at the risk of introducing a training bias— but would not obviate the need for expert training. (In any case, pre-annotation has never, to our knowledge, been successfully applied to hard discourse-l"
N19-1177,C14-1027,0,0.0257244,"d speed the work of annotators— possibly at the risk of introducing a training bias— but would not obviate the need for expert training. (In any case, pre-annotation has never, to our knowledge, been successfully applied to hard discourse-level tasks such as annotating argumentation structures.) The complexity of the annotation scheme also seemingly rules out the use of crowdsourcing (Howe, 2006) and gamification (von Ahn, 2006), which are geared towards microtasks that are quick and easy for humans. Though one previous study has decomposed a discourse-level scheme for use with crowdsourcing (Kawahara et al., 2014), the constraints it imposes (fixed-size annotations, maximum document length of three sentences) are too restrictive for argumentation annotation. By contrast, the crowdsourcing approach of Sukhareva et al. (2016), while not concerned with discourse-spanning annotations, employs a few mechanisms that are relevant for our own task. Their approach, intended for the labelling of semantic verb relations, breaks down the annotation work into a series of hierarchical, atomic microtasks. Only those parts of the annotation instructions relevant to the current microtask are shown to the annotator. Fur"
N19-1177,W16-2812,0,0.0308156,"nnotators. We have used our method to quickly and cheaply produce a large, argument-annotated data set of product reviews, which we freely release, along with the source code to our annotation interface and processing tools. Unlike with flat, context-free argument data such as that of Stab et al. (2018), training on our annotations would conceivably permit the identification not just of isolated argument components but of more complex argument structures. Our resources may also be of use for qualitative research on the linguistic features and rhetorical mechanisms of argumentative text (e.g., Peldszus and Stede, 2016). For future work, we are investigating alternatives to MACE, which was designed for categorical annotations rather than the sequence labelling of our task. In particular, we are looking into the Bayesian method of Simpson and Gurevych (2018), which takes advantage of the sequential dependencies between BIO tags, and works more robustly with noisy, subjective data such as ours. Acknowledgments The authors thank Johannes Daxenberger and Christian Stab for many insightful discussions. This work has been supported by the German Federal Ministry of Education and Research (BMBF) under the promotion"
N19-1177,P15-1053,0,0.0237969,"ta can be challenging to obtain in sufficient quantity and quality, particularly for discourse-level argumentation. This is because discourse-level annotation schemes are necessarily complex with respect to discrimination and delimitation (i.e., the variety of markable elements in the text and how to define their boundaries), expressiveness (i.e., In this paper, we present a method that facilitates the application of one such discourse-level argument annotation scheme (Stab and Gurevych, 2014). This scheme has been widely cited and used in argumentation studies (e.g., Lippi and Torroni, 2015; Persing and Ng, 2015; Nguyen and Litman, 2015; Persing and Ng, 2016; Ghosh et al., 2016; Eger et al., 2017; Nguyen and Litman, 2018), and while it is fairly coarse-grained, it is expensive to apply to new texts. Our method breaks down the annotation process into incremental, intuitive steps, each focusing on a small portion of the overall annotation scheme. We apply this method in a crowdsourcing setup with annotators who receive no training other than a brief set of annotation guidelines, as well as in a more traditional setup with extensively trained local annotators. We find that agreement between the two grou"
N19-1177,N16-1164,0,0.0254724,"uantity and quality, particularly for discourse-level argumentation. This is because discourse-level annotation schemes are necessarily complex with respect to discrimination and delimitation (i.e., the variety of markable elements in the text and how to define their boundaries), expressiveness (i.e., In this paper, we present a method that facilitates the application of one such discourse-level argument annotation scheme (Stab and Gurevych, 2014). This scheme has been widely cited and used in argumentation studies (e.g., Lippi and Torroni, 2015; Persing and Ng, 2015; Nguyen and Litman, 2015; Persing and Ng, 2016; Ghosh et al., 2016; Eger et al., 2017; Nguyen and Litman, 2018), and while it is fairly coarse-grained, it is expensive to apply to new texts. Our method breaks down the annotation process into incremental, intuitive steps, each focusing on a small portion of the overall annotation scheme. We apply this method in a crowdsourcing setup with annotators who receive no training other than a brief set of annotation guidelines, as well as in a more traditional setup with extensively trained local annotators. We find that agreement between the two groups increases sublinearly with the number of cro"
N19-1177,W95-0107,0,0.489384,"y of our crowdsourced annotations, we instead conducted an experiment that compared them to those produced by expert-trained annotators. For the experiment, we randomly selected 40 Amazon product reviews from the McAuley et al. (2015) data set—four from each of ten product categories. Each review was annotated for major claims by ten crowd workers; all 40 reviews were also annotated for major claims by a fixed group of three locally recruited annotators trained by argumentation experts.3 We then converted the 3We engaged US-based workers from Amazon Mechanical annotated reviews to BIO tokens (Ramshaw and Marcus, 1995) and applied the annotation aggregation/denoising tool MACE (Hovy et al., 2013) to select at most two gold-standard major-claim annotations per review, one from the crowd (crowd) and one from the trained annotators (train).4 We then compared the crowd and train gold standards, one review at a time, using Krippendorff’s (1995) αU , a unitizing measure that considers the tokenlevel boundaries of the text spans marked by each annotator. We repeated this process to obtain and evaluate crowd and train claim annotations on the train major claims, and then again for crowd and train premise annotation"
N19-1177,C14-1142,1,0.76114,"rgument mining, can require significant amounts of argument-annotated data to achieve reasonable performance. However, this data can be challenging to obtain in sufficient quantity and quality, particularly for discourse-level argumentation. This is because discourse-level annotation schemes are necessarily complex with respect to discrimination and delimitation (i.e., the variety of markable elements in the text and how to define their boundaries), expressiveness (i.e., In this paper, we present a method that facilitates the application of one such discourse-level argument annotation scheme (Stab and Gurevych, 2014). This scheme has been widely cited and used in argumentation studies (e.g., Lippi and Torroni, 2015; Persing and Ng, 2015; Nguyen and Litman, 2015; Persing and Ng, 2016; Ghosh et al., 2016; Eger et al., 2017; Nguyen and Litman, 2018), and while it is fairly coarse-grained, it is expensive to apply to new texts. Our method breaks down the annotation process into incremental, intuitive steps, each focusing on a small portion of the overall annotation scheme. We apply this method in a crowdsourcing setup with annotators who receive no training other than a brief set of annotation guidelines, as"
N19-1177,D18-1402,1,0.852192,"on annotations of Stab and Gurevych (2014), which may be adaptable to other annotation schemes based on Freeman’s (2011) notion of argumentation. Our analysis shows that crowdsourced annotations obtained with our method yield substantial agreement with those obtained, with much greater effort, by expert-trained annotators. We have used our method to quickly and cheaply produce a large, argument-annotated data set of product reviews, which we freely release, along with the source code to our annotation interface and processing tools. Unlike with flat, context-free argument data such as that of Stab et al. (2018), training on our annotations would conceivably permit the identification not just of isolated argument components but of more complex argument structures. Our resources may also be of use for qualitative research on the linguistic features and rhetorical mechanisms of argumentative text (e.g., Peldszus and Stede, 2016). For future work, we are investigating alternatives to MACE, which was designed for categorical annotations rather than the sequence labelling of our task. In particular, we are looking into the Bayesian method of Simpson and Gurevych (2018), which takes advantage of the sequen"
N19-1177,L16-1338,1,0.829903,"applied to hard discourse-level tasks such as annotating argumentation structures.) The complexity of the annotation scheme also seemingly rules out the use of crowdsourcing (Howe, 2006) and gamification (von Ahn, 2006), which are geared towards microtasks that are quick and easy for humans. Though one previous study has decomposed a discourse-level scheme for use with crowdsourcing (Kawahara et al., 2014), the constraints it imposes (fixed-size annotations, maximum document length of three sentences) are too restrictive for argumentation annotation. By contrast, the crowdsourcing approach of Sukhareva et al. (2016), while not concerned with discourse-spanning annotations, employs a few mechanisms that are relevant for our own task. Their approach, intended for the labelling of semantic verb relations, breaks down the annotation work into a series of hierarchical, atomic microtasks. Only those parts of the annotation instructions relevant to the current microtask are shown to the annotator. Furthermore, annotators are encouraged to think of connecting words (“specifically”, “generally speaking”, “in other words”, etc.) that justify their relation annotations. As described in the following section, we ada"
P05-3002,I05-1067,1,0.702473,"basis of the conceptual hierarchy. Two metrics of semantic relatedness are, then, based on the application of the Lesk algorithm to definitions, generated automatically according to two system configurations. The generated definitions can be tailored to the task at hand according to a set of parameters defining which related concepts 7 Figure 1: The concept of user-system interaction. have to be included in the final definition. Experiments carried out to determine the most effective parameters for generating the definitions and employing those to compute semantic relatedness is described in Gurevych (2005). Gurevych & Niederlich (2005) present a description of the evaluation procedure for five implemented semantic relatedness metrics against a human Gold Standard and the evaluation results. 4 Graphical User Interface We developed a graphical user interface to interactively experiment with the software for computing semantic relatedness. The system runs on a standard Linux or Windows machine. Upon initialization, we configured the system to load an information content map computed from the German taz corpus. 5 The information content values encoded therein are employed by the information content"
P05-3002,C04-1110,1,0.840942,"in natural language processing applications. A graphical user interface allows to interactively experiment with the system. 1 Motivation The knowledge encoded in WordNet (Fellbaum, 1998) has proved valuable in many natural language processing (NLP) applications. One particular way to integrate semantic knowledge into applications is to compute semantic similarity of WordNet concepts. This can be used e.g. to perform word sense disambiguation (Patwardhan et al., 2003), to find predominant word senses in untagged text (McCarthy et al., 2004), to automatically generate spoken dialogue summaries (Gurevych & Strube, 2004), and to perform spelling correction (Hirst & Budanitsky, 2005). Extensive research concerning the integration of semantic knowledge into NLP for the English language has been arguably fostered by the emergence of WordNet::Similarity package (Pedersen et al., 2004).1 In its turn, the development of the WordNet based semantic similarity software has been facilitated by the availability of tools to easily retrieve 1 data from WordNet, e.g. WordNet::QueryData, 2 jwnl.3 Research integrating semantic knowledge into NLP for languages other than English is scarce. On the one hand, there are fewer com"
P05-3002,O97-1002,0,0.0489003,"ies to what degree the meanings of two words are related to each other. E.g. the meanings of Glas (Engl. glass) and Becher (Engl. cup) will be typically classified as being closely related to each other, while the relation between Glas and Juwel (Engl. gem) is more distant. RelatednessComparator is a class which takes two words as input and returns a numeric value indicating semantic relatedness for the two words. Semantic relatedness metrics have been implemented as descendants of this class. Three of the metrics for computing semantic relatedness are information content based (Resnik, 1995; Jiang & Conrath, 1997; Lin, 1998) and are also implemented in WordNet::Similarity package. However, some aspects in the normalization of their results and the task definition according to which the evaluation is conducted have been changed (Gurevych & Niederlich, 2005). The metrics are implemented as classes derived from InformationBasedComparator, which is in its turn derived from the class PathBasedComparator. They make use of both the GermaNet hierarchy and statistical corpus evidence, i.e. information content. 4 As mentioned before, GermaNet abandoned the clusterapproach taken in WordNet to group adjectives. I"
P05-3002,kunze-lemnitzer-2002-germanet,0,0.496217,"imilarity software has been facilitated by the availability of tools to easily retrieve 1 data from WordNet, e.g. WordNet::QueryData, 2 jwnl.3 Research integrating semantic knowledge into NLP for languages other than English is scarce. On the one hand, there are fewer computational knowledge resources like dictionaries, broad enough in coverage to be integrated in robust NLP applications. On the other hand, there is little off-the-shelf software that allows to develop applications utilizing semantic knowledge from scratch. While WordNet counterparts do exist for many languages, e.g. GermaNet (Kunze & Lemnitzer, 2002) and EuroWordNet (Vossen, 1999), they differ from WordNet in certain design aspects. E.g. GermaNet features nonlexicalized, so called artificial concepts that are nonexistent in WordNet. Also, the adjectives are structured hierarchically which is not the case in WordNet. These and other structural differences led to divergences in the data model. Therefore, WordNet based implementations are not applicable to GermaNet. Also, there is generally lack of experimental evidence concerning the portability of e.g. WordNet based semantic similarity metrics to other wordnets and their sensitivity to spe"
P05-3002,P04-1036,0,0.0207979,"metrics. The package can, again, serve as a software library to be deployed in natural language processing applications. A graphical user interface allows to interactively experiment with the system. 1 Motivation The knowledge encoded in WordNet (Fellbaum, 1998) has proved valuable in many natural language processing (NLP) applications. One particular way to integrate semantic knowledge into applications is to compute semantic similarity of WordNet concepts. This can be used e.g. to perform word sense disambiguation (Patwardhan et al., 2003), to find predominant word senses in untagged text (McCarthy et al., 2004), to automatically generate spoken dialogue summaries (Gurevych & Strube, 2004), and to perform spelling correction (Hirst & Budanitsky, 2005). Extensive research concerning the integration of semantic knowledge into NLP for the English language has been arguably fostered by the emergence of WordNet::Similarity package (Pedersen et al., 2004).1 In its turn, the development of the WordNet based semantic similarity software has been facilitated by the availability of tools to easily retrieve 1 data from WordNet, e.g. WordNet::QueryData, 2 jwnl.3 Research integrating semantic knowledge into NLP f"
P05-3002,N04-3012,0,0.0612283,"particular way to integrate semantic knowledge into applications is to compute semantic similarity of WordNet concepts. This can be used e.g. to perform word sense disambiguation (Patwardhan et al., 2003), to find predominant word senses in untagged text (McCarthy et al., 2004), to automatically generate spoken dialogue summaries (Gurevych & Strube, 2004), and to perform spelling correction (Hirst & Budanitsky, 2005). Extensive research concerning the integration of semantic knowledge into NLP for the English language has been arguably fostered by the emergence of WordNet::Similarity package (Pedersen et al., 2004).1 In its turn, the development of the WordNet based semantic similarity software has been facilitated by the availability of tools to easily retrieve 1 data from WordNet, e.g. WordNet::QueryData, 2 jwnl.3 Research integrating semantic knowledge into NLP for languages other than English is scarce. On the one hand, there are fewer computational knowledge resources like dictionaries, broad enough in coverage to be integrated in robust NLP applications. On the other hand, there is little off-the-shelf software that allows to develop applications utilizing semantic knowledge from scratch. While Wo"
P07-1130,E06-1002,0,0.0209052,".3 A large body of research exists on using wordnets in NLP applications and in particular in IR (Moldovan and Mihalcea, 2000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from this, we first compare two semantic relatedness (SR) measures based on the information either in the German wordnet (Lin, 1998) called LIN, or in Wikipedia (Gabrilovich and Markovitch, 2007) called Explicit Semantic Analysis, or ESA. We evaluate their performance intrinsically on the tasks of (T-1) computing semantic relatedness, and (T-2) solving Reader’s Digest Word Power (RDWP) questions a"
P07-1130,W98-0705,0,0.0279276,"ubstantiated below. 2 System Architecture Integrating lexical semantic knowledge in ECG requires the existence of knowledge bases encoding domain and lexical knowledge. In this paper, we investigate the utility of two knowledge bases: (i) a German wordnet, GermaNet (Kunze, 2004), and (ii) the German portion of Wikipedia.3 A large body of research exists on using wordnets in NLP applications and in particular in IR (Moldovan and Mihalcea, 2000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from this, we first compare two semantic relatedness (SR) measures based on"
P07-1130,I05-7005,1,0.824845,"Net displays some structural differences and content oriented modifications. Its designers relied mainly on linguistic evidence, such as corpus frequency, rather than psycholinguistic motivations. Also, GermaNet employs artificial, i.e. non-lexicalized concepts, and adjectives are structured hierarchically as opposed to WordNet. Currently, GermaNet includes about 40000 synsets with more than 60000 word senses modelling nouns, verbs and adjectives. We use the semantic relatedness measure by Lin (1998) (referred to as LIN), as it consistently is among the best performing wordnet based measures (Gurevych and Niederlich, 2005; Budanitsky and Hirst, 2006). Lin defined semantic similarity using a formula derived from information theory. This measure is sometimes called a universal semantic similarity measure as it is supposed to be application, domain, and resource independent. Lin is computed as: 2 × log p(LCS(c1 , c2 )) simc1 ,c2 = log p(c1 ) + log p(c2 ) where c1 and c2 are concepts (word senses) corresponding to w1 and w2 , log p(c) is the information content, and LCS(c1 , c2 ) is the lowest common subsumer of the two concepts. The probability p is computed as the relative frequency of words (representing that c"
P07-1130,I05-1067,1,0.259128,"f their corresponding concept vectors. If we want to measure the semantic relatedness of texts instead of terms, we can also use ESA concept vectors. A text is represented as the average concept vector of its terms’ concept vectors. Then, the relatedness of two texts is computed as the cosine of their average concept vectors. As ESA uses all textual information in Wikipedia, the measure shows excellent coverage. Therefore, we select it as the second measure for integration into our IR system. 3.2 Datasets Semantic relatedness datasets for German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards s"
P07-1130,W04-2607,0,0.0104968,"atasets for German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. DATASET Gur65 Gur350 ZG222 Y EAR 2005 2006 2006 L ANGUAGE German German German # PAIRS 65 350 222 POS N N, V, A N, V, A S CORES discrete {0,1,2,3,4} discrete {0,1,2,3,4} discrete {0,1,2,3,4} # S UBJECTS 24 8 21 C ORRELATION r I NTER I NTRA .810 .690 .490 .647 Table 1: Comparison of datasets used for evaluating semantic relatedness in German. ZG222 does not have this bias. Following the work by Jarmasz and Szpakowicz (2003) and Turney (2006), we created a second da"
P07-1130,J06-3003,0,0.0142156,"al relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. DATASET Gur65 Gur350 ZG222 Y EAR 2005 2006 2006 L ANGUAGE German German German # PAIRS 65 350 222 POS N N, V, A N, V, A S CORES discrete {0,1,2,3,4} discrete {0,1,2,3,4} discrete {0,1,2,3,4} # S UBJECTS 24 8 21 C ORRELATION r I NTER I NTRA .810 .690 .490 .647 Table 1: Comparison of datasets used for evaluating semantic relatedness in German. ZG222 does not have this bias. Following the work by Jarmasz and Szpakowicz (2003) and Turney (2006), we created a second dataset containing multiple choice questions. We collected 1072 multiple-choice word analogy questions from the German Reader’s Digest Word Power Game (RDWP) from January 2001 to December 2005 (Wallace and Wallace, 2005). We discarded 44 questions that had more than one correct answer, and 20 questions that used a phrase instead of a single term as query. The resulting 1008 questions form our evaluation dataset. An example question is given below: Muffin (muffin) a) Kleingeb¨ack (small cake) b) Spenglerwerkzeug (plumbing tool) c) Miesepeter (killjoy) d) Wildschaf (moufflo"
P07-1130,W06-1104,1,0.512394,"erms’ concept vectors. Then, the relatedness of two texts is computed as the cosine of their average concept vectors. As ESA uses all textual information in Wikipedia, the measure shows excellent coverage. Therefore, we select it as the second measure for integration into our IR system. 3.2 Datasets Semantic relatedness datasets for German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. DATASET Gur65 Gur350 ZG222 Y EAR 2005 2006 2006 L ANGUAGE German German German # PAIRS 65 350 222 POS N N, V, A N, V, A S CORES discrete {0,1,2,3,4"
P07-1130,N07-2052,1,0.675807,"Missing"
P07-1130,J06-1003,0,\N,Missing
P07-2032,N06-1027,0,0.0615823,"r usage frequency. Readers of a review are asked “Was this review helpful to you?” with the answer choices Yes/No. This is very well defined compared to forum posts, which are typically rated on a five star scale that does not advertise a specific semantics. Forums have been in the focus of another track of research. Kim et al. (2006b) found that the relation between a student’s posting behavior and the grade obtained by that student can be assessed automatically. The main features used are the number of posts, the average post length and the average number of replies to posts of the student. Feng et al. (2006) and Kim et al. (2006a) describe a system to find the most authoritative answer in a forum thread. The latter add speech act analysis as a feature for this classification. Another feature is the author’s trustworthiness, which could be computed based on the automatic quality classification scheme proposed in the present paper. Finding the most authoritative post could also be defined as a special case of the quality assessment. However, it is definitely different from the task studied in the present paper. We assess the perceived quality of a given post, based solely on its intrinsic features."
P07-2032,W06-1650,0,0.0213748,"ic assessment of other types of user generated content, such as essays and product reviews. On the other hand, student online discussions have been analyzed. Automatic text quality assessment has been studied in the area of automatic essay scoring (Valenti et al., 2003; Chodorow and Burstein, 2004; Attali and Burstein, 2006). While there exist guidelines for writing and assessing essays, this is not the case for forum posts, as different users cast their rating with possibly different quality criteria in mind. The same argument applies to the automatic assessment of product review usefulness (Kim et al., 2006c): 125 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125–128, c Prague, June 2007. 2007 Association for Computational Linguistics Stars ? ?? ??? ? ? ?? ????? Label on the website Poor Post Below Average Post Average Post Above Average Post Excellent Post Number 1251 44 69 183 421 Table 1: Categories and their usage frequency. Readers of a review are asked “Was this review helpful to you?” with the answer choices Yes/No. This is very well defined compared to forum posts, which are typically rated on a five star scale that does not advertise a specific semantics. Forums have been"
P07-2032,J93-2004,0,\N,Missing
P09-1082,J03-1002,0,0.00327401,"We use the Wiktionary dump from January 11, 2009. • English and Simple English Wikipedia. We use the Wikipedia dump from February 6, 2007 and the Simple Wikipedia dump from July 24, 2008. The Simple English Wikipedia is an English Wikipedia targeted at non-native speakers of English which uses simpler words than the English Wikipedia. Wikipedia and Simple Wikipedia articles do not directly correspond to glosses such as those found in dictionaries, we therefore considered the first paragraph in articles as a surrogate for glosses. 3.3 Translation Model Training We used the GIZA++ SMT Toolkit4 (Och and Ney, 2003) in order to obtain word-to-word translation probabilities from the parallel datasets described above. As is common practice in translation-based retrieval, we utilised the IBM translation model 1. The only pre-processing steps performed for all parallel datasets were tokenisation and stop word removal.5 3.4 Comparison of Word-to-Word Translations Table 1 gives some examples of word-to-word translations obtained for the different parallel corpora used (the column ALLPool will be described in the next section). As evidenced by this table, Given a list of 86,584 seed lexemes extracted from WordN"
P09-1082,P08-1017,0,0.0249326,"ector-space model for answer finding. 1 Introduction The lexical gap (or lexical chasm) often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question & Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) 728 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 728–736, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact tra"
P09-1082,P07-1059,0,0.298922,"on retrieval (M¨uller et al., 2007). Berger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training data for translation-based retrieval often consist in question-answer pairs, usually extracted from the evaluation corpus itself (Riezler et al., 2007; Xue et al., 2008; Lee et al., 2008). While collectionspecific translation models effectively encode statistical word associations for the target document collection, it also introduces a bias in the evaluation and makes it difficult to assess the quality of the translation model per se, independently from a specific task and document collection. In this paper, we propose new kinds of datasets for training domain-independent monolingual translation models. We use the definitions and glosses provided for the same term by different lexical semantic resources to automatically train the translati"
P09-1082,C08-1093,0,0.015246,"erms to build compact translation models. Other kinds of training data have also been proposed. Jeon et al. (2005) automatically clustered semantically similar questions based on their answers. Murdock and Croft (2005) created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus. Similar work has also been performed in the area of query expansion using training data consisting of FAQ pages (Riezler et al., 2007) or queries and clicked snippets from query logs (Riezler et al., 2008). All in all, translation models have been shown to significantly improve the retrieval results over traditional baselines for document retrieval (Berger and Lafferty, 1999), question retrieval in Question & Answer archives (Jeon et al., 2005; Lee et al., 2008; Xue et al., 2008) and for sentence retrieval (Murdock and Croft, 2005). Many of the approaches previously described have used parallel data extracted from the retrieval corpus itself. The translation models obtained are therefore domain and collection-specific, which introduces a bias in the evaluation and makes it difficult to assess t"
P09-1082,W03-1605,0,0.02758,"exical chasm) often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question & Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) 728 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 728–736, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact translation models. Other kinds of training data have also been proposed. Jeon"
P09-1082,D08-1043,0,0.745039,"ger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training data for translation-based retrieval often consist in question-answer pairs, usually extracted from the evaluation corpus itself (Riezler et al., 2007; Xue et al., 2008; Lee et al., 2008). While collectionspecific translation models effectively encode statistical word associations for the target document collection, it also introduces a bias in the evaluation and makes it difficult to assess the quality of the translation model per se, independently from a specific task and document collection. In this paper, we propose new kinds of datasets for training domain-independent monolingual translation models. We use the definitions and glosses provided for the same term by different lexical semantic resources to automatically train the translation models. This approach has been ver"
P09-1082,C02-1161,0,0.0305723,"often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question & Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) 728 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 728–736, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact translation models. Other kinds of training data have also been proposed. Jeon et al. (2005) automatically c"
P09-1082,H05-1086,0,\N,Missing
P10-1059,C08-2002,0,0.0562965,"010 Association for Computational Linguistics and providing anaphoric resolutions in discourse. We present an annotation scheme which fulfills the mentioned requirements, an inter-annotator agreement study, and discuss our observations. The rest of this paper is structured as follows: Section 2 presents the related work. In Sections 3, we describe the annotation scheme. Section 4 presents the data and the annotation study, while Section 5 summarizes the main conclusions. 2 opinions in discourse and discover implicit evaluations through link transitivity. Similar to Somasundaran et al. (2008), Asher et al. (2008) performs discourse level analysis of opinions. They propose a scheme which first identifies and assigns categories to the opinion segments as reporting, judgment, advice, or sentiment; and then links the opinion segments with each other via rhetorical relations including contrast, correction, support, result, or continuation. However, in contrast to our scheme and other schemes, instead of marking expression boundaries without any restriction they annotate an opinion segment only if it contains an opinion word from their lexicon, or if it has a rhetorical relation to another opinion segment."
P10-1059,cheng-xu-2008-fine,0,0.0477468,"ation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). 1 http://blog.nielsen.com/nielsenwire/ wp-content/uploads/2008/10/press_ release24.pdf 575 Proceedings of the 48th Annual Meeting of the Association for Com"
P10-1059,H05-1045,0,0.0355914,"tasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). 1 http://blog.nielsen.com/nielsenwire/ wp-content/uploads/2008/10/press_ release24.pdf 575 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575–584, c Uppsala, Sweden, 11"
P10-1059,esuli-sebastiani-2006-sentiwordnet,0,0.0151779,"stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (W"
P10-1059,wilson-2008-annotating,0,0.22563,"xpressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). 1 http://blog.nielsen.com/nielsenwire/ wp-content/uploads/2008/10/press_ release24.pdf 575 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575–584, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics and providing anaphoric resolutions in discourse. We present an annotation scheme which fulfills the mentioned requirements, an inter-annotator agreement study, and discuss our observations. The rest of this paper is structured as follows: Section 2 pr"
P10-1059,W06-0301,0,0.292015,"nt for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). 1 http://blog.nielsen.com/nielsenwire/ wp-content/uploads/2008/10/press_ release24.pdf 575 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575–584, c"
P10-1059,passonneau-2004-computing,0,0.0152932,"expression and target spans within the opinionated sentences which they agreed upon. Sentence level analysis indeed increases the reliability at the expression level. Compared to the high agreement on marking target spans, we obtain lower agreement values on marking polar target spans. We observe that it is easier to attribute explicit expressions of evaluations to topic relevant entities compared to attributing evaluations implied by experiences to specific topic relevant entities in the reviews. We calculated the agreement on identifying anaphoric references using the method introduced in (Passonneau, 2004) which utilizes Krippendorf’s α (Krippendorff, 2004) for computing reliability for coreference annotation. We considered the overlapping target and polar target spans together in this calculation, and obtained an α value of 0.29. Compared to Passonneau (α values from 0.46 to 0.74), we obtain a much lower agreement value. This may be due to the different definitions and organizations of the annotation tasks. Passonneau requires prior marking of all noun phrases (or instances which needs to be processed by the anExpression Level Agreement At the expression level, annotators focus only on the sen"
P10-1059,W03-1014,0,0.0283817,"estigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dial"
P10-1059,W08-0122,0,0.197997,"rni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). 1 http://blog.nielsen.com/nielsenwire/ wp-content/uploads/2008/10/press_ release24.pdf 575 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575–584, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics and providing anaphoric resolutions in discourse. We present an annotation scheme which fulfills the mentioned requirements, an inter-annotator agreement study, and discuss our observations. The rest of this paper is structured as follows: Section 2 presents the related work. In Sections 3, we describe"
P10-1059,C08-1103,0,0.0271209,"arking expression boundaries without any restriction they annotate an opinion segment only if it contains an opinion word from their lexicon, or if it has a rhetorical relation to another opinion segment. Previous Opinion Annotated Corpora 2.1 Newspaper Articles and Meeting Dialogs Most prominent work concerning the expression level annotation of opinions is the MultiPerspective Question Answering (MPQA) corpus2 (Wiebe et al., 2005). It was extended several times over the last years, either by adding new documents or annotating new types of opinion related information (Wilson and Wiebe, 2005; Stoyanov and Cardie, 2008; Wilson, 2008b). The MPQA annotation scheme builds upon the private state notion (Quirk et al., 1985) which describes mental states including opinions, emotions, speculations and beliefs among others. The annotation scheme strives to represent the private states in terms of their functional components (i.e. experiencer holding an attitude towards a target). It consists of frames (direct subjective, expressive subjective element, objective speech event, agent, attitude, and target frames) with slots representing various attributes and properties (e.g.intensity, nested source) of the private st"
P10-1059,W05-0308,0,0.351937,"), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). 1 http://blog.nielsen.com/nielsenwire/ wp-content/uploads/2008/10/press_ release24.pdf 575 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575–584, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics and providing anaphoric resolutions in discourse. We present an annotation scheme which fulfills the mentioned requirements, an inter-annotator agreement study, and discuss our observations. The rest of this paper is structured as follows"
P10-1059,H05-1044,0,0.131976,"components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). 1 http://blog.nielsen.com/nielsenwire/ wp-content/uploads/2008/10/pr"
P10-2049,H05-1043,0,0.300238,"ws is high enough for linguistically more advanced technologies such as parsing or AR to be successfully applied. approaches. Initial approaches combined statistical information and basic linguistic features such as part-of-speech tags. The goal was to identify the opinion targets, here in form of products and their attributes, without a pre-built knowledge base which models the domain. For the target candidate identification, simple part-of-speech patterns were employed. The relevance ranking and extraction was then performed with different statistical measures: Pointwise Mutual Information (Popescu and Etzioni, 2005), the Likelihood Ratio Test (Yi et al., 2003) and Association Mining (Hu and Liu, 2004). A more linguistically motivated approach was taken by Kim and Hovy (2006) through identifying opinion holders and targets with semantic role labeling. This approach was promising, since their goal was to extract opinions from professionally edited content i.e. newswire. Zhuang et al. (2006) present an algorithm for the extraction of opinion target - opinion word pairs. The opinion word and target candidates are identified in the annotated corpus and their extraction is then performed by applying possible p"
P10-2049,W97-1306,0,0.013865,"ng based algorithm for AR, they evaluate its performance in comparison to three non machine-learning based algorithms, since those are the only ones available. They observe that the best performing baseline algorithm (OpenNLP) is hardly documented. The algorithm with the next-to-highest results in (Charniak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit. This algorithm is based on statistical analysis of the antecedent candidates. Another promising algorithm for AR employs a rule based approach for antecedent identification. The CogNIAC algorithm (Baldwin, 1997) was designed for high-precision AR. This approach seems like an adequate strategy for our OM task, since in the dataset used in our experiments only a small fraction of the total number of pronouns are ac265 up over all folds. In Table 4, a true positive refers to an extracted pronoun which was annotated as an opinion target and is resolved to the correct antecedent. A false positive subsumes two error classes: A pronoun which was not annotated as an opinion target but extracted as such, or a pronoun which is resolved to an incorrect antecedent. As shown in Table 3, the recall of our reimplem"
P10-2049,C08-1103,0,0.0177578,"their goal was to extract opinions from professionally edited content i.e. newswire. Zhuang et al. (2006) present an algorithm for the extraction of opinion target - opinion word pairs. The opinion word and target candidates are identified in the annotated corpus and their extraction is then performed by applying possible paths connecting them in a dependency graph. These paths are combined with part-of-speech information and also learned from the annotated corpus. To the best of our knowledge, there is currently only one system which integrates coreference information in OM. The algorithm by Stoyanov and Cardie (2008) identifies coreferring targets in newspaper articles. A candidate selection or extraction step for the opinion targets is not required, since they rely on manually annotated targets and focus solely on the coreference resolution. However they do not resolve pronominal anaphora in order to achieve that. 2.2 3 Opinion Target Identification 3.1 Dataset Currently the only freely available dataset annotated with opinions including annotated anaphoric opinion targets is a corpus of movie reviews by Zhuang et al. (2006). Kessler and Nicolov (2009) describe a collection of product reviews in which an"
P10-2049,E09-1018,0,0.0183294,"sentences. [3.] The rules by which CogNIAC resolves anaphora were designed so that anaphora which have ambiguous antecedents are left unresolved. This strategy should lead to a high precision AR, but at the same time it can have a negative impact on the recall. In the OM context, it happens quite frequently that the authors comment on the entity they want to criticize in a series of arguments. In such argument chains, we try to solve cases of antecedent ambiguity by analyzing the opinions: If there are ambiguous antecedent candidates for a Algorithms for Anaphora Resolution As pointed out by Charniak and Elsner (2009) there are hardly any freely available systems for AR. Although Charniak and Elsner (2009) present a machine-learning based algorithm for AR, they evaluate its performance in comparison to three non machine-learning based algorithms, since those are the only ones available. They observe that the best performing baseline algorithm (OpenNLP) is hardly documented. The algorithm with the next-to-highest results in (Charniak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit. This algorithm is based on statistical analysis of the antecedent candidates. Anot"
P10-2049,P02-1053,0,0.00193494,"ogNIAC+[id] 117 95 105 180 CogNIAC+[id]+[1] 117 41 105 51 CogNIAC+[id]+[2] 117 95 153 410 CogNIAC+[id]+[3] 131 103 182 206 CogNIAC+[id]+[1]+[2]+[3] 124 64 194 132 1 personal, impersonal & demonstrative pronouns 2 true positives, false positives Algorithm tary regarding the extraction of impersonal and demonstrative pronouns. This configuration yields statistically significant improvements regarding fmeasure over the off-the-shelf CogNIAC configuration, while also having the overall highest recall. 5.1 Error Analysis When extracting opinions from movie reviews, we observe the same challenge as Turney (2002): The users often characterize events in the storyline or roles the characters play. These characterizations contain the same words which are also used to express opinions. Hence these combinations are frequently but falsely extracted as opinion target - opinion word pairs, negatively affecting the precision. The algorithm cannot distinguish them from opinions expressing the stance of the author. Overall, the recall of the baseline is rather low. This is due to the fact that the algorithm only learns a subset of the opinion words and opinion targets annotated in the training data. Currently, i"
P10-2049,P05-1045,0,0.00864833,"ges. In order to be able to identify rarely occurring opinion targets which are not in the candidate list, they expand it by crawling the cast and crew names of the movies from the IMDB. How this crawling and extraction is done is not explained. 4 4.1 Extensions of CogNIAC We identified a few typical sources of errors in a preliminary error analysis. We therefore suggest three extensions to the algorithm which are on the one hand possible in the OM setting and on the other hand represent special features of the target discourse type: [1.] We observed that the Stanford Named Entity Recognizer (Finkel et al., 2005) is superior to the Person detection of the (MUC6 trained) CogNIAC implementation. We therefore filter out Person antecedent candidates which the Stanford NER detects for the impersonal and demonstrative pronouns and Location & Organization candidates for the personal pronouns. This way the input to the AR is optimized. [2.] The second extension exploits the fact that reviews from the IMDB exhibit certain contextual properties. They are gathered and to be presented in the context of one particular entity (=movie). The context or topic under which it occurs is therefore typically clear to the r"
P10-2049,W06-0301,0,0.0316605,"ormation and basic linguistic features such as part-of-speech tags. The goal was to identify the opinion targets, here in form of products and their attributes, without a pre-built knowledge base which models the domain. For the target candidate identification, simple part-of-speech patterns were employed. The relevance ranking and extraction was then performed with different statistical measures: Pointwise Mutual Information (Popescu and Etzioni, 2005), the Likelihood Ratio Test (Yi et al., 2003) and Association Mining (Hu and Liu, 2004). A more linguistically motivated approach was taken by Kim and Hovy (2006) through identifying opinion holders and targets with semantic role labeling. This approach was promising, since their goal was to extract opinions from professionally edited content i.e. newswire. Zhuang et al. (2006) present an algorithm for the extraction of opinion target - opinion word pairs. The opinion word and target candidates are identified in the annotated corpus and their extraction is then performed by applying possible paths connecting them in a dependency graph. These paths are combined with part-of-speech information and also learned from the annotated corpus. To the best of ou"
P10-2049,P98-2143,0,0.045702,"zing the opinions: If there are ambiguous antecedent candidates for a Algorithms for Anaphora Resolution As pointed out by Charniak and Elsner (2009) there are hardly any freely available systems for AR. Although Charniak and Elsner (2009) present a machine-learning based algorithm for AR, they evaluate its performance in comparison to three non machine-learning based algorithms, since those are the only ones available. They observe that the best performing baseline algorithm (OpenNLP) is hardly documented. The algorithm with the next-to-highest results in (Charniak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit. This algorithm is based on statistical analysis of the antecedent candidates. Another promising algorithm for AR employs a rule based approach for antecedent identification. The CogNIAC algorithm (Baldwin, 1997) was designed for high-precision AR. This approach seems like an adequate strategy for our OM task, since in the dataset used in our experiments only a small fraction of the total number of pronouns are ac265 up over all folds. In Table 4, a true positive refers to an extracted pronoun which was annotated as an opinion target and is"
P10-2049,P05-1015,0,0.121145,"Missing"
P10-2049,poesio-kabadjov-2004-general,0,0.0182432,"ambiguous antecedent candidates for a Algorithms for Anaphora Resolution As pointed out by Charniak and Elsner (2009) there are hardly any freely available systems for AR. Although Charniak and Elsner (2009) present a machine-learning based algorithm for AR, they evaluate its performance in comparison to three non machine-learning based algorithms, since those are the only ones available. They observe that the best performing baseline algorithm (OpenNLP) is hardly documented. The algorithm with the next-to-highest results in (Charniak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit. This algorithm is based on statistical analysis of the antecedent candidates. Another promising algorithm for AR employs a rule based approach for antecedent identification. The CogNIAC algorithm (Baldwin, 1997) was designed for high-precision AR. This approach seems like an adequate strategy for our OM task, since in the dataset used in our experiments only a small fraction of the total number of pronouns are ac265 up over all folds. In Table 4, a true positive refers to an extracted pronoun which was annotated as an opinion target and is resolved to the correct antecedent. A false"
P10-2049,H05-2017,0,\N,Missing
P10-2049,C98-2138,0,\N,Missing
P11-4013,A00-2004,0,0.0429566,"in page. Wikulu therefore supports users by analyzing long pages through employing text segmentation algorithms which detect topically coherent segments of text. It then suggests segment boundaries which the user may or may not accept for inserting a subheading which makes pages easier to read and better to navigate. As shown in Figure 3, users are also encouraged to set a title for each segment.8 When accepting one or more of these suggested boundaries, Wikulu stores them persistently in the wiki. Wikulu currently integrates text segmentation methods such as TextTiling (Hearst, 1997) or C99 (Choi, 2000). Summarizing Pages Similarly to segmenting pages, Wikulu makes long wiki pages more accessible by generating an extractive summary. While generative summaries generate a summary in own words, extractive summaries analyze the original wiki text sentence-by-sentence, rank each sentence, and return a list of the most important ones (see Figure 4). Wikulu integrates extractive text summarization methods such as LexRank (Erkan and Radev, 2004). Highlighting Keyphrases Another approach to assist users in better grasping the idea of a wiki page at a glance is to highlight important keyphrases (see F"
P11-4013,P07-1130,1,0.898161,"Missing"
P11-4013,J97-1003,0,0.156092,"is present on a certain page. Wikulu therefore supports users by analyzing long pages through employing text segmentation algorithms which detect topically coherent segments of text. It then suggests segment boundaries which the user may or may not accept for inserting a subheading which makes pages easier to read and better to navigate. As shown in Figure 3, users are also encouraged to set a title for each segment.8 When accepting one or more of these suggested boundaries, Wikulu stores them persistently in the wiki. Wikulu currently integrates text segmentation methods such as TextTiling (Hearst, 1997) or C99 (Choi, 2000). Summarizing Pages Similarly to segmenting pages, Wikulu makes long wiki pages more accessible by generating an extractive summary. While generative summaries generate a summary in own words, extractive summaries analyze the original wiki text sentence-by-sentence, rank each sentence, and return a list of the most important ones (see Figure 4). Wikulu integrates extractive text summarization methods such as LexRank (Erkan and Radev, 2004). Highlighting Keyphrases Another approach to assist users in better grasping the idea of a wiki page at a glance is to highlight importa"
P11-4013,W04-3252,0,\N,Missing
P11-4017,J93-1001,0,0.114449,"Missing"
P11-4017,P08-2035,0,0.0698863,"toolkit that solves both issues by reconstructing a certain past state of Wikipedia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al., 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al., 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al., 2006). 1 http://download.wikimedia.org/ 97 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, c Portland, Oregon, USA, 21 June 2011. 2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size of the data. The revisions for all articles in the current English Wikipedia sum up to over 5"
P11-4017,N10-1056,0,0.0095054,"edia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al., 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al., 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al., 2006). 1 http://download.wikimedia.org/ 97 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, c Portland, Oregon, USA, 21 June 2011. 2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size of the data. The revisions for all articles in the current English Wikipedia sum up to over 5 terabytes of text. Consequently, most of the above mentioned previous work"
P11-4017,zesch-gurevych-2010-better,1,0.824501,"2009). The majority of Wikipedia-based NLP algorithms works on single snapshots of Wikipedia, which are published by the Wikimedia Foundation as XML dumps at irregular intervals.1 Such a snapshot only represents the state of Wikipedia at a certain fixed point in time, while Wikipedia actually is a dynamic resource that is constantly changed by its millions of editors. This rapid change is bound to have an influence on the performance of NLP algorithms using Wikipedia data. However, the exact consequences are largely unknown, as only very few papers have systematically analyzed this influence (Zesch and Gurevych, 2010). This is mainly due to older snapshots becoming unavailable, as there is no official backup server. As a consequence, older experimental results cannot be reproduced anymore. In this paper, we present a toolkit that solves both issues by reconstructing a certain past state of Wikipedia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The seq"
P11-4017,zesch-etal-2008-extracting,1,0.5734,"kit which provides access to Wikipedia with the help of a preprocessed database. It represents articles, categories and redirects as Java classes and provides access to the article content either as MediaWiki markup or as plain text. The toolkit mainly focuses on Wikipedia’s structure, the contained concepts, and semantic relations, but it makes little use of the textual content within the articles. Even though it was developed to work language independently, it focuses mainly on the English Wikipedia. Another open source API for accessing Wikipedia data from a preprocessed database is JWPL4 (Zesch et al., 2008). Like Wikipedia Miner, it also represents the content and structure of Wikipedia as Java objects. In addition to that, JWPL contains a MediaWiki markup parser to further analyze the article contents to make available fine-grained information like e.g. article sections, info-boxes, or first paragraphs. Furthermore, it was explicitly designed to work with all language versions of Wikipedia. We have chosen to extend JWPL with our revision toolkit, as it has better support for accessing article contents, natively supports multiple languages, and seems to have a larger and more active developer co"
P11-4017,W10-3504,0,\N,Missing
P13-1071,W10-3001,0,0.0269991,"Missing"
P13-1071,J12-2004,1,0.851612,"Missing"
P13-1071,P11-4017,1,0.831765,"used as positive training instances. However, we found upon manual inspection of the data that a substantial number of articles has been significantly edited between the time tτ , at which the template was first assigned, and the time te , at which the articles have been extracted. Using the latest version at time te can thus include articles in which the respective flaw has already been fixed without removing the cleanup template. Therefore, we use the revision of the article at time tτ to assure that the flaw is still present in the training instance. We use the Wikipedia Revision Toolkit (Ferschke et al., 2011), an enhancement of the Java Wikipedia Library, to gain access to the revision history of each article. For every article in the corpus of positive examples for flaw f that is marked 725 (a) Random negatives (b) Reliable negatives Figure 1: Sampling of negative instances for a given set of flawed articles (A f ). Random negatives (Arnd ) are sampled from articles without any cleanup templates (Au ). Reliable negatives (Arel ) are sampled from the set of articles (Atopic ) with the same topic distribution as A f result in a more realistic performance evaluation. In the following, we present our"
P13-1071,zesch-etal-2008-extracting,1,0.703005,"Missing"
P13-1071,E12-1079,1,0.887949,"an overly optimistic performance evaluation and classifiers that are biased towards particular article topics. Our approach factors out the topic bias from the training data by mining topically controlled training instances from the Wikipedia revision history. The results show that flaw detection is a much harder problem in a real-life scenario. nomenon that can occur when a template is not removed after fixing a problem in an article. In our experiments, we focus on neutrality and style flaws, since they are of particular high importance within the Wikipedia community (Stvilia et al., 2008; Ferschke et al., 2012a) and are recognized beyond Wikipedia in applications such as uncertainty recognition (Szarvas et al., 2012) and hedge detection (Farkas et al., 2010). 2 Related Work Topic bias is a known problem in text classification. Mikros and Argiri (2007) investigate the topic influence in authorship attribution. They found that even simple stylometric features, such as sentence and token length, readability measures or word length distributions show considerable correlations with topic. They argue that many features that were largely considered to be topic neutral are in fact topic-dependent variables"
P13-1134,E09-1005,0,0.0313098,"with all senses from Wiktionary which have the same lemma and thus are likely to describe the same sense. This step yields a set of candidate sense pairs Call . In the classification step, a similarity score between the textual information associated with the senses in a candidate pair (e.g., their gloss) is computed and a threshold-based classifier decides for each pair on valid alignments. Niemann and Gurevych (2011) combine two different types of similarity (i) cosine similarity on bag-of-words vectors (COS) and (ii) a personalized PageRank-based similarity measure (PPR). The PPR measure (Agirre and Soroa, 2009) maps the glosses of the two senses to a semantic vector space spanned up by WordNet synsets and then compares them using the chi-square measure. The semantic vectors ppr are computed using the personalized PageRank algorithm on the WordNet graph. They determine the important nodes in the graph as the nodes that a random walker following the edges visits most frequently: ppr = cM ppr + (1 − c)vppr , (1) where M is a transition probability matrix between the n WordNet synsets, c is a damping factor, and vppr is a vector of size n representing the probability of jumping to the node i associated"
P13-1134,J08-4004,0,0.0237135,"Missing"
P13-1134,P98-1013,0,0.057623,"eNet and the English Wiktionary. It results in a multilingual FrameNet FNWKxx, which links FrameNet senses to lemmas in 280 languages. (2) We create a sense-disambiguated English-German FrameNet lexicon FNWKde based on FNWKxx and translation disambiguation on the German Wiktionary.1 (3) We analyze the two resources and outline further steps for creating a multilingual FrameNet. This is a major step towards the vision of this paper: a simple, but powerful approach to partially construct a FrameNet in any language using Wiktionary as an interlingual representation. 2 Resource Overview FrameNet (Baker et al., 1998) is an expert-built lexical-semantic resource incorporating the theory of frame-semantics (Fillmore, 1976). It groups word senses in frames that represent particular situations. Thus, the verb complete and the noun completion belong to the Activity finish frame. The participants of these situations, typically realized as syntactic arguments, are the semantic roles of the frame, for instance the Agent performing an activity, or the Activity itself. FrameNet release 1.5 contains 1,015 frames, and 11,942 word senses. Corpus texts annotated with frames and their roles have been used to train autom"
P13-1134,W12-1902,0,0.130169,"mbiguation algorithm to establish the alignment and report F1 =0.75 on a gold standard based on Tonelli and Pighin (2009). Tonelli and Giuliano (2009) align FrameNet senses to Wikipedia entries with the goal to extract word senses and example sentences in Italian. Based on Wikipedia, this alignment is restricted to nouns. Subsequent work on Wikipedia and FrameNet follows a different path and tries to enhance the modeling of selectional preferences for FrameNet predicates (Tonelli et al., 2012). Finally, there have been suggestions to combine the corpus-based and the resource-based approaches: Borin et al. (2012) do this for Finnish and Swedish. They prove the feasibility of their approach by creating a preliminary Finnish FrameNet with 2,694 senses. Mouton et al. (2010) directly exploit the translations in the English and French Wiktionary editions to extend the French FrameNet. They match the FrameNet senses to Wiktionary lexical entries, thus encountering the problem of polysemy in the target language. To solve this, they define a set of filters that control how target lemmas are distributed over frames, increasing precision at the expense of recall (P=0.74, R=0.3, F1 =0.42). While their approach i"
P13-1134,burchardt-etal-2006-salsa,0,0.145586,"Missing"
P13-1134,W08-2208,0,0.152071,"Missing"
P13-1134,ferrandez-etal-2010-aligning,0,0.0174737,"espect to all the frames they evoke. To alleviate this problem, they use a disambiguation approach based on the most frequent frame; Basili et al. (2009) use distributional methods for frame disambiguation. Our approach is based on sense alignments and therefore explicitly aims to avoid such errors. The second line of work is resource-based: FrameNet is aligned to multilingual resources in order to extract senses in the target language. Using monolingual resources, this approach has also been employed to extend FrameNet coverage for English (Shi and Mihalcea, 2005; Johansson and Nugues, 2007; Ferrandez et al., 2010). De Cao et al. (2008) map FrameNet frames to WordNet synsets based on the embedding of FrameNet lemmas in WordNet. They use MultiWordNet, an English-Italian wordnet, to induce an Italian FrameNet lexicon with 15,000 entries. To create MapNet, Tonelli and Pianta (2009) align FrameNet senses with WordNet synsets by exploiting the textual similarity of their glosses. The similarity measure is based on stem overlap of the candidates’ glosses expanded by WordNet domains, the WordNet synset, and the set of senses for a FrameNet frame. In Tonelli and Pighin (2009), they use these features to train a"
P13-1134,E12-1059,1,0.868976,"ted equally over the m vector components (i.e., synsets) associated with a word in the sense gloss, other components receive a 0 value. For each similarity measure, Niemann and Gurevych (2011) determine a threshold (tppr and tcos ) independently on a manually annotated gold standard. The final alignment decision is the conjunction of two decision functions: a(ss , st ) = PPR(ss , st ) > tppr & COS(ss , st ) > tcos . (2) We differ from Niemann and Gurevych (2011) in that we use a joint training setup which determines tppr and tcos to optimize classification performance directly (as proposed in Gurevych et al. (2012)): (tppr , tcos ) = argmax(tppr ,tcos ) F1 (a), (3) where F1 is the maximized evaluation score and a is the decision function in equation (2). 5.2 Candidate Extraction To compile the candidate set, we paired senses from both resources with identical lemma-POS combinations. FrameNet senses are defined by a lemma, a gloss, and a frame. Wiktionary senses are defined by a lemma and a gloss. For the FrameNet sense Activity finish of the verb complete, we find two candidate senses in Wiktionary (to finish and to make whole). There are on average 3.7 candidates per FrameNet sense. The full candidate"
P13-1134,P06-2057,0,0.0191279,"ynonyms, e.g., beenden. As a side-benefit of our method, we also extend the English FrameNet by the linguistic information in Wiktionary. 1364 4 4.1 Related Work Creating FrameNets in New Languages There are two main lines of research in bootstrapping a FrameNet for languages other than English. The first, corpus-based approach is to automatically extract word senses in the target language based on parallel corpora and frame annotations in the source language. In this vein, Pad´o and Lapata (2005) propose a cross-lingual FrameNet extension to German and French; Johansson and Nugues (2005) and Johansson and Nugues (2006) do this for Spanish and Swedish, and Basili et al. (2009) for Italian. Pad´o and Lapata (2005) observe that their approach suffers from polysemy errors, because lemmas in the source language need to be disambiguated with respect to all the frames they evoke. To alleviate this problem, they use a disambiguation approach based on the most frequent frame; Basili et al. (2009) use distributional methods for frame disambiguation. Our approach is based on sense alignments and therefore explicitly aims to avoid such errors. The second line of work is resource-based: FrameNet is aligned to multilingu"
P13-1134,R09-1039,0,0.0160964,", Tonelli and Pianta (2009) align FrameNet senses with WordNet synsets by exploiting the textual similarity of their glosses. The similarity measure is based on stem overlap of the candidates’ glosses expanded by WordNet domains, the WordNet synset, and the set of senses for a FrameNet frame. In Tonelli and Pighin (2009), they use these features to train an SVMclassifier to identify valid alignments and report an F1 -score of 0.66 on a manually annotated gold standard. They report 4,265 new English senses and 6,429 new Italian senses, which were derived via MultiWordNet. ExtendedWordFramenet (Laparra and Rigau, 2009; Laparra and Rigau, 2010) is also based on the alignment of FrameNet senses to WordNet synsets. The goal is the multilingual coverage extension of FrameNet, which is achieved by linking WordNet to wordnets in other languages (Spanish, Italian, Basque, and Catalan) in the Multilingual Central Repository. For each language, they add more then 10,000 senses to FrameNet. They rely on a knowledge-based word sense disambiguation algorithm to establish the alignment and report F1 =0.75 on a gold standard based on Tonelli and Pighin (2009). Tonelli and Giuliano (2009) align FrameNet senses to Wikiped"
P13-1134,laparra-rigau-2010-extended,0,0.267737,"9) align FrameNet senses with WordNet synsets by exploiting the textual similarity of their glosses. The similarity measure is based on stem overlap of the candidates’ glosses expanded by WordNet domains, the WordNet synset, and the set of senses for a FrameNet frame. In Tonelli and Pighin (2009), they use these features to train an SVMclassifier to identify valid alignments and report an F1 -score of 0.66 on a manually annotated gold standard. They report 4,265 new English senses and 6,429 new Italian senses, which were derived via MultiWordNet. ExtendedWordFramenet (Laparra and Rigau, 2009; Laparra and Rigau, 2010) is also based on the alignment of FrameNet senses to WordNet synsets. The goal is the multilingual coverage extension of FrameNet, which is achieved by linking WordNet to wordnets in other languages (Spanish, Italian, Basque, and Catalan) in the Multilingual Central Repository. For each language, they add more then 10,000 senses to FrameNet. They rely on a knowledge-based word sense disambiguation algorithm to establish the alignment and report F1 =0.75 on a gold standard based on Tonelli and Pighin (2009). Tonelli and Giuliano (2009) align FrameNet senses to Wikipedia entries with the goal t"
P13-1134,eckle-kohler-etal-2012-uby,1,0.870752,"translation disambiguation in the target language. We validate our approach on the language pair English-German and discuss the options and requirements for creating FrameNets in further languages. As part of this work, we created the first sense alignment between FrameNet and the English Wiktionary. The resulting resource FNWKxx connects FrameNet senses to over 280 languages. The bilingual English-German FrameNet lexicon FNWKde competes with manually created resources, as shown by a comparison to the SALSA corpus. We make both resources publicly available in the standardized format UBY-LMF (Eckle-Kohler et al., 2012), which supports automatic processing of the resources via the UBY Java API, see http://www.ukp.tu-darmstadt.de/ fnwkde/. We also extended FrameNet by several thousand new English senses from Wiktionary which are provided as part of FNWKde. In our future work, we will evaluate the benefits of the extracted information to SRL. Acknowledgments This work has been supported by the Volkswagen Foundation as part of the LichtenbergProfessorship Program under grant No. I/82806 and by the German Research Foundation under grant No. GU 798/3-1 and grant No. GU 798/9-1. We thank Christian Meyer and Judith"
P13-1134,I11-1099,1,0.948639,"ense of recall (P=0.74, R=0.3, F1 =0.42). While their approach is in theory applicable to other languages, our approach goes beyond this by laying the ground for simultaneous FrameNet extension in multiple languages via FNWKxx. 4.2 Wiktionary Sense Alignments Collaboratively created resources have become popular for sense alignments for NLP, starting with the alignment between WordNet and Wikipedia (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2009). Wiktionary has been subject to few alignment efforts: de Melo and Weikum (2009) integrate information from Wiktionary into Universal WordNet. Meyer and Gurevych (2011) map WordNet synsets to Wiktionary senses and show their complementary domain coverage. 1365 5 5.1 FrameNet – Wiktionary Alignment Alignment Technique We follow the state-of-the-art sense alignment technique introduced by Niemann and Gurevych (2011). They align senses in WordNet to Wikipedia entries in a supervised setting using semantic similarity measures. One reason to use their method was that it allows zero alignments or one-to-many alignments. This is crucial for obtaining a high-quality alignment of heterogeneous resources, such as the presented one, because their sense granularity and"
P13-1134,C12-1108,1,0.925592,"language-specific types of information. Senses also provide translations to other languages. These are connected to lexical entries in the respective language editions via hyperlinks. This allows us to use Wiktionary as an interlingual connection between multiple languages. 1 The xx in FNWKxx stands for all the languages in the resource. After translation disambiguation in a specific language, xx is replaced by the corresponding language code. 2 as of May 2013, see http://en.wiktionary. org/wiki/Wiktionary:Statistics. Figure 1: Method overview. The quality of Wiktionary has been confirmed by Meyer and Gurevych (2012b) who also give an overview on the usage of Wiktionary in NLP applications such as speech synthesis. 3 Method Overview Our method consists of two steps visualized in Fig. 1. In the first step, we create a novel sense alignment between FrameNet and the English Wiktionary following Niemann and Gurevych (2011). Thus, the FrameNet sense of to complete with frame Activity finish is assigned to the sense of to complete in Wiktionary meaning to finish. This step establishes Wiktionary as an interlingual index between FrameNet senses and lemmas in many languages, and builds the foundation for the bil"
P13-1134,N03-2022,0,0.0321414,"uation into the target language. We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexicalsemantic resources. The created resource is publicly available at http://www. ukp.tu-darmstadt.de/fnwkde/. 1 Introduction FrameNet is a valuable resource for natural language processing (NLP): semantic role labeling (SRL) systems based on FrameNet provide semantic analysis for NLP applications, such as question answering (Narayanan and Harabagiu, 2004; Shi and Mihalcea, 2005) and information extraction (Mohit and Narayanan, 2003). However, their wide deployment has been prohibited by the poor coverage and limited availability of a similar resource in many languages. Expert-built lexical-semantic resources are expensive to create. Previous cross-lingual transfer of FrameNet used corpus-based approaches, or resource alignment with multilingual expert-built resources, such as EuroWordNet. The latter indirectly also suffers from the high cost and constrained coverage of expert-built resources. Recently, collaboratively created resources have been investigated for the multilingual extension of resources in NLP, beginning w"
P13-1134,mouton-etal-2010-framenet,0,0.0409207,"Missing"
P13-1134,C04-1100,0,0.0114662,"n a sense alignment of FrameNet and Wiktionary, and subsequent translation disambiguation into the target language. We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexicalsemantic resources. The created resource is publicly available at http://www. ukp.tu-darmstadt.de/fnwkde/. 1 Introduction FrameNet is a valuable resource for natural language processing (NLP): semantic role labeling (SRL) systems based on FrameNet provide semantic analysis for NLP applications, such as question answering (Narayanan and Harabagiu, 2004; Shi and Mihalcea, 2005) and information extraction (Mohit and Narayanan, 2003). However, their wide deployment has been prohibited by the poor coverage and limited availability of a similar resource in many languages. Expert-built lexical-semantic resources are expensive to create. Previous cross-lingual transfer of FrameNet used corpus-based approaches, or resource alignment with multilingual expert-built resources, such as EuroWordNet. The latter indirectly also suffers from the high cost and constrained coverage of expert-built resources. Recently, collaboratively created resources have b"
P13-1134,P10-1023,0,0.0394193,"r wide deployment has been prohibited by the poor coverage and limited availability of a similar resource in many languages. Expert-built lexical-semantic resources are expensive to create. Previous cross-lingual transfer of FrameNet used corpus-based approaches, or resource alignment with multilingual expert-built resources, such as EuroWordNet. The latter indirectly also suffers from the high cost and constrained coverage of expert-built resources. Recently, collaboratively created resources have been investigated for the multilingual extension of resources in NLP, beginning with Wikipedia (Navigli and Ponzetto, 2010). They rely on the socalled “Wisdom of the Crowds”, contributions by a large number of volunteers, which results in a continuously updated high-quality resource available in hundreds of languages. Due to the encyclopedic nature of Wikipedia, previous work focused on encyclopedic information for Wikipedia entries, i.e., almost exclusively on nouns. This is not enough for resources like FrameNet. Such resources need lexical-semantic information on various POS. For FrameNet, information on the predicates associated with a semantic frame – mostly verbs, nouns, and adjectives – is crucial, for inst"
P13-1134,W11-0122,1,0.933368,"all the languages in the resource. After translation disambiguation in a specific language, xx is replaced by the corresponding language code. 2 as of May 2013, see http://en.wiktionary. org/wiki/Wiktionary:Statistics. Figure 1: Method overview. The quality of Wiktionary has been confirmed by Meyer and Gurevych (2012b) who also give an overview on the usage of Wiktionary in NLP applications such as speech synthesis. 3 Method Overview Our method consists of two steps visualized in Fig. 1. In the first step, we create a novel sense alignment between FrameNet and the English Wiktionary following Niemann and Gurevych (2011). Thus, the FrameNet sense of to complete with frame Activity finish is assigned to the sense of to complete in Wiktionary meaning to finish. This step establishes Wiktionary as an interlingual index between FrameNet senses and lemmas in many languages, and builds the foundation for the bilingual FrameNet extension. It results in a basic multilingual FrameNet lexicon FNWKxx with translations to lemmas in 283 languages. An example: by aligning the FrameNet sense of the verb complete with gloss to finish with the corresponding English Wiktionary sense, we collect 39 translations to 22 languages,"
P13-1134,D09-1029,0,0.179297,"MultiWordNet. ExtendedWordFramenet (Laparra and Rigau, 2009; Laparra and Rigau, 2010) is also based on the alignment of FrameNet senses to WordNet synsets. The goal is the multilingual coverage extension of FrameNet, which is achieved by linking WordNet to wordnets in other languages (Spanish, Italian, Basque, and Catalan) in the Multilingual Central Repository. For each language, they add more then 10,000 senses to FrameNet. They rely on a knowledge-based word sense disambiguation algorithm to establish the alignment and report F1 =0.75 on a gold standard based on Tonelli and Pighin (2009). Tonelli and Giuliano (2009) align FrameNet senses to Wikipedia entries with the goal to extract word senses and example sentences in Italian. Based on Wikipedia, this alignment is restricted to nouns. Subsequent work on Wikipedia and FrameNet follows a different path and tries to enhance the modeling of selectional preferences for FrameNet predicates (Tonelli et al., 2012). Finally, there have been suggestions to combine the corpus-based and the resource-based approaches: Borin et al. (2012) do this for Finnish and Swedish. They prove the feasibility of their approach by creating a preliminary Finnish FrameNet with 2,69"
P13-1134,W09-3740,0,0.0156021,"icitly aims to avoid such errors. The second line of work is resource-based: FrameNet is aligned to multilingual resources in order to extract senses in the target language. Using monolingual resources, this approach has also been employed to extend FrameNet coverage for English (Shi and Mihalcea, 2005; Johansson and Nugues, 2007; Ferrandez et al., 2010). De Cao et al. (2008) map FrameNet frames to WordNet synsets based on the embedding of FrameNet lemmas in WordNet. They use MultiWordNet, an English-Italian wordnet, to induce an Italian FrameNet lexicon with 15,000 entries. To create MapNet, Tonelli and Pianta (2009) align FrameNet senses with WordNet synsets by exploiting the textual similarity of their glosses. The similarity measure is based on stem overlap of the candidates’ glosses expanded by WordNet domains, the WordNet synset, and the set of senses for a FrameNet frame. In Tonelli and Pighin (2009), they use these features to train an SVMclassifier to identify valid alignments and report an F1 -score of 0.66 on a manually annotated gold standard. They report 4,265 new English senses and 6,429 new Italian senses, which were derived via MultiWordNet. ExtendedWordFramenet (Laparra and Rigau, 2009; La"
P13-1134,W09-1127,0,0.0191981,"ea, 2005; Johansson and Nugues, 2007; Ferrandez et al., 2010). De Cao et al. (2008) map FrameNet frames to WordNet synsets based on the embedding of FrameNet lemmas in WordNet. They use MultiWordNet, an English-Italian wordnet, to induce an Italian FrameNet lexicon with 15,000 entries. To create MapNet, Tonelli and Pianta (2009) align FrameNet senses with WordNet synsets by exploiting the textual similarity of their glosses. The similarity measure is based on stem overlap of the candidates’ glosses expanded by WordNet domains, the WordNet synset, and the set of senses for a FrameNet frame. In Tonelli and Pighin (2009), they use these features to train an SVMclassifier to identify valid alignments and report an F1 -score of 0.66 on a manually annotated gold standard. They report 4,265 new English senses and 6,429 new Italian senses, which were derived via MultiWordNet. ExtendedWordFramenet (Laparra and Rigau, 2009; Laparra and Rigau, 2010) is also based on the alignment of FrameNet senses to WordNet synsets. The goal is the multilingual coverage extension of FrameNet, which is achieved by linking WordNet to wordnets in other languages (Spanish, Italian, Basque, and Catalan) in the Multilingual Central Repos"
P13-1134,C98-1013,0,\N,Missing
P13-2080,N12-1021,0,0.0900373,"Missing"
P13-2080,S13-2045,1,0.930106,"t is called partial textual entailment, because we are only interested in recognizing whether a single element of the hypothesis is entailed. To differentiate the two tasks, we will refer to the original textual entailment task as complete textual entailment. Partial textual entailment was first introduced by Nielsen et al. (2009), who presented a machine learning approach and showed significant improvement over baseline methods. Recently, a public benchmark has become available through the Joint Student Response Analysis and 8th Recognizing Textual Entailment (RTE) Challenge in SemEval 2013 (Dzikovska et al., 2013), on which we focus in this paper. Our goal in this paper is to investigate the idea of partial textual entailment, and assess whether Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is “almost entailed” by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to thi"
P13-2080,R11-1063,1,0.82929,"rate movement in the body. H: The main job of muscles is to move bones. Lexical Inference This feature checks whether both facet words, or semantically related words, appear in T . We use WordNet (Fellbaum, 1998) with the Resnik similarity measure (Resnik, 1995) and count a facet term wi as matched if the similarity score exceeds a certain threshold (0.9, empirically determined on the training set). Both w1 and w2 must match for this module’s entailment decision to be positive. Syntactic Inference This module builds upon the open source1 Bar-Ilan University Textual Entailment Engine (BIUTEE) (Stern and Dagan, 2011). BIUTEE operates on dependency trees by applying a sequence of knowledge-based transformations that converts T into H. It determines entailment according to the “cost” of generating the hypothesis from the text. The cost model can be automatically tuned with a relatively small training set. BIUTEE has shown state-of-the-art performance on previous recognizing textual entailment challenges (Stern and Dagan, 2012). Since BIUTEE processes dependency trees, both T and the facet must be parsed. We therefore extract a path in H’s dependency tree that represents the facet. This is done by first pars"
P13-2080,P11-2098,1,0.802976,"mainly on lexical inference and syntax. We examined three representative modules that reflect these levels: Exact Match, Lexical Inference, and Syntactic Inference. Exact Match We represent T as a bag-of-words containing all tokens and lemmas appearing in the text. We then check whether both facet lemmas w1 , w2 appear in the text’s bag-of-words. Exact matching was used as a baseline in previous recognizing textual entailment challenges (Bentivogli et al., 2011), and similar methods of lemmamatching were used as a component in recognizing textual entailment systems (Clark and Harrison, 2010; Shnarch et al., 2011). Task Definition In order to tackle partial entailment, we need to find a way to decompose a hypothesis. Nielsen et al. (2009) defined a model of facets, where each such facet is a pair of words in the hypothesis and the direct semantic relation connecting those two words. We assume the simplified model that was used in RTE-8, where the relation between the words is not explicitly stated. Instead, it remains unstated, but its interpreted meaning should correspond to the manner in which the words are related in the hypothesis. For example, in the sentence “the main job of muscles is to move bo"
P13-2080,S12-1051,0,0.0646707,"Missing"
P13-2080,P12-3013,1,0.842323,"2 must match for this module’s entailment decision to be positive. Syntactic Inference This module builds upon the open source1 Bar-Ilan University Textual Entailment Engine (BIUTEE) (Stern and Dagan, 2011). BIUTEE operates on dependency trees by applying a sequence of knowledge-based transformations that converts T into H. It determines entailment according to the “cost” of generating the hypothesis from the text. The cost model can be automatically tuned with a relatively small training set. BIUTEE has shown state-of-the-art performance on previous recognizing textual entailment challenges (Stern and Dagan, 2012). Since BIUTEE processes dependency trees, both T and the facet must be parsed. We therefore extract a path in H’s dependency tree that represents the facet. This is done by first parsing H, and then locating the two nodes whose words compose the facet. We then find their lowest common ancestor (LCA), and extract the path P from w1 to The facet (muscles, move) refers to the agent role in H, and is expressed by T . However, the facet (move, bones), which refers to a theme or direct object relation in H, is unaddressed by T . 3 Entailment Modules Recognizing Faceted Entailment Our goal is to inv"
P13-4001,J96-2004,0,0.449292,"de of Figure 3) which can be used to navigate to the sentences for curation. • Annotation layers: Annotators usually work on one or two annotations layers, such as part-of-speech and dependency or named entity annotation. Overloading the annotation page by displaying all annotation layers makes the annotation and visualization process slower. WebAnno provides an option to configure visible/editable annotation layers. 3.1.5 Monitoring WebAnno provides a monitoring component, to track the progress of a project. The project manager can check the progress and compute agreement with Kappa and Tau (Carletta, 1996) measures. The progress is visualized using a matrix of annotators and documents displaying which documents the annotators have marked as complete and which documents the curator adjudicated. Figure 4 shows the project progress, progress of individual annotator and completion statistics. • Immediate persistence: Every annotation is sent to the back end immediately and persisted there. An explicit interaction by the user to save changes is not required. 3.1.3 Workflow WebAnno implements a simple workflow to track the state of a project. Every annotator works on a 3 Figure 3: Curation user inter"
P13-4001,heid-etal-2010-corpus,0,0.152534,"annotation project is conducted by a project manager (cf. Figure 1) in a project definition form. It supports creating a project, loading un-annotated or preannotated documents in different formats2 , adding annotator and curator users, defining tagsets, and configuring the annotation layers. Only a project manager can administer a project. Figure 2 illustrates the project definition page with the tagset editor highlighted. System Architecture of WebAnno 1 Available for download at (this paper is based on v0.3.0): webanno.googlecode.com/ 2 Formats: plain text, CoNLL (Nivre et al., 2007), TCF (Heid et al., 2010), UIMA XMI (Ferrucci and Lally, 2004) The overall architecture of WebAnno is depicted in Figure 1. The modularity of the architecture, 2 Figure 2: The tagset editor on the project definition page 3.1.2 Annotation Annotation is carried out with an adapted version of the brat editor, which communicates with the server via Ajax (Wang et al., 2008) using the JSON (Lin et al., 2012) format. Annotators only see projects they are assigned to. The annotation page presents the annotator different options to set up the annotation environment, for customization: separate version of the document, which is"
P13-4001,N03-4009,0,0.0575767,"is carried out with locally downloaded software. An interface to crowdsourcing platforms is missing. The GATE Teamware system is heavily targeted towards template-based information extraction. It sets a focus on the integration of automatic annotation components rather than on the interface for manual annotation. Besides, the overall application is rather complex for average users, requires considerable training and does not offer an alternative simplified interface as it would be required for crowdsourcing. General-purpose annotation tools like MMAX2 (M¨uller and Strube, 2006) or WordFreak (Morton and LaCivita, 2003) are not web-based and do not provide annotation project management. They are also not sufficiently flexible regarding different annotation layers. The same holds for specialized tools for single annotation layers, which we cannot list here for the sake of brevity. With the brat rapid annotation tool (Stenetorp et al., 2012), for the first time a web-based opensource annotation tool was introduced, which supports collaborative annotation for multiple annotation layers simultaneously on a single copy of the document, and is based on a client-server architecture. However, the current version of"
P13-4001,E12-2021,0,0.344972,"Missing"
P13-4001,D07-1096,0,\N,Missing
P13-4007,E09-1005,0,0.392799,"r et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych 3 DKPro WSD"
P13-4007,P05-3014,0,0.0338443,"wn formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use gener"
P13-4007,H94-1046,0,0.257302,"he Senseval (and later SemEval) series of competitions, the first of which was held in 1998. Each competition defined a number of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soro"
P13-4007,P06-4018,0,0.0694924,"et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych 3 DKPro WSD Our system, DKPro WSD, is implemented as a framework of UIMA components (type systems, collection readers, annotators, CAS consumers, resources) which the user combines into a data processing pipeline. We can best illustrate this with an example: Figure 1 shows a pipeline for running two disambiguation algorithms on the Estonian all-words task from Senseval-2. UIMA components are the solid, rounded boxes in the lower half of the diagram, and the data and algorithms they encapsulate are the light grey shapes in the upper half. The first component of the pipeline"
P13-4007,C12-1109,1,0.833346,"we would pass an English instead of Estonian language model to TreeTagger, and we would substitute the sense inventory resource exposing the Estonian EuroWordNet with one for WordNet 1.7.1. Crucially, none of the WSD algorithms need to be changed. Algorithms. As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require. Algorithms and baselines already implemented include the analytically calculated random sense baseline; the most frequent sense baseline; the original, simplified, extended, and lexically expanded Lesk variants (Miller et al., 2012); various The most important features of our system are 39 graph connectivity approaches from Navigli and Lapata (2010); Personalized PageRank (Agirre and Soroa, 2009); the supervised TWSI system (Biemann, 2013); and IMS (Zhong and Ng, 2010). Our open API permits users to program support for further knowledge-based and supervised algorithms. Linguistic annotators. Many WSD algorithms require linguistic annotations from segmenters, lemmatizers, POS taggers, parsers, etc. Off-theshelf UIMA components for producing such annotations, such as those provided by DKPro Core (Gurevych et al., 2007), ca"
P13-4007,E12-1059,1,0.820008,"entity linking tasks (McNamee and Dang, 2009), and the aforementioned MASC, SemCor, and WebCAGe corpora. Our prepackaged corpus analysis modules can compute statistics on monosemous terms, average polysemy, terms absent from the sense inventory, etc. Sense inventories. Sense inventories are abstracted into a system of types and interfaces according to the sort of lexical-semantic information they provide. There is currently support for WordNet (Fellbaum, 1998), WordNet++ (Ponzetto and Navigli, 2010), EuroWordNet (Vossen, 1998), the Turk Bootstrap Word Sense Inventory (Biemann, 2013), and UBY (Gurevych et al., 2012), which provides access to WordNet, Wikipedia, Wiktionary, GermaNet, VerbNet, FrameNet, OmegaWiki, and various alignments between them. The system can automatically convert between various versions of WordNet using the UPC mappings (Daud´e et al., 2003). A pipeline of this sort can be written with just a few lines of code: one or two to declare each component and if necessary bind it to the appropriate resources, and a final one to string the components together into a pipeline. Moreover, once such a pipeline is written it is simple to substitute functionally equivalent components. For example"
P13-4007,P05-3019,0,0.0243504,"d corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) o"
P13-4007,E12-1039,0,0.0296995,"first of which was held in 1998. Each competition defined a number of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a par"
P13-4007,pazienza-etal-2008-jmwnl,0,0.0256454,"ian language model TreeTagger results and statistics JMWNL simplified Lesk degree centrality sense inventory corpus reader answer key annotator WSD annotator linguistic annotator WSD annotator evaluator Figure 1: A sample DKPro WSD pipeline for the Estonian all-words data set from Senseval-2. as follows: Then come the two disambiguation algorithms, also modelled as UIMA annotators wrapping nonUIMA-aware algorithms. Each WSD annotator iterates over the instances in the CAS and annotates them with sense IDs from EuroWordNet. (EuroWordNet itself is accessed via a UIMA resource which wraps JMWNL (Pazienza et al., 2008) and which is bound to the two WSD annotators.) Finally, control passes to a CAS consumer which compares the WSD algorithms’ sense annotations against the gold-standard annotations produced by the answer key annotator, and outputs these sense annotations along with various evaluation metrics (precision, recall, etc.). Corpora and data sets. DKPro WSD currently has collection readers for all Senseval and SemEval all-words and lexical sample tasks, the AIDA CoNLL-YAGO data set (Hoffart et al., 2011), the TAC KBP entity linking tasks (McNamee and Dang, 2009), and the aforementioned MASC, SemCor,"
P13-4007,D11-1072,0,0.165694,"Missing"
P13-4007,P10-1154,0,0.22145,"Missing"
P13-4007,P10-2013,0,0.0210432,"al) series of competitions, the first of which was held in 1998. Each competition defined a number of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong"
P13-4007,C12-3031,0,0.0158374,"Linguistics, pages 37–42, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics et al., 2007). Such toolkits provide individual components potentially useful for WSD, such as WordNet-based measures of sense similarity and readers for the odd corpus format. However, these toolkits are not specifically geared towards development and evaluation of WSD systems; there is no unified type system or architecture which allows WSD-specific components to be combined or substituted orthogonally. The only general-purpose dedicated WSD system we are aware of is I Can Sense It (Joshi et al., 2012), a Web-based interface for running and evaluating various WSD algorithms. It includes I/O support for several corpus formats and implementations of a number of baseline and state-of-theart disambiguation algorithms. However, as with previous single-algorithm systems, it is not possible to select the sense inventory, and the user is responsible for pre-annotating the input text with POS tags. The usability and extensibility of the system are greatly restricted by the fact that it is a proprietary, closed-source application fully hosted by the developers. our system and further explain its capa"
P13-4007,N10-2006,0,0.01964,"s, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. F"
P13-4007,P10-4014,0,0.369651,"2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych 3 DKPro WSD Our system, DKPro WSD, is impl"
P13-4021,O97-1002,0,0.836865,"rs of words (Hatzivassiloglou et al., 1999). Pairwise Word Similarity These measures are based on pairwise word similarity computations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5 , and Wikipedia, while the framework can easily be extended to other data sources that conform to a common interface (Garoufi et al., 2008). Pairwise similarity measures in DKPro Similarity include Jiang and Conrath (1997) or Resnik (1995). The aggregation for the complete texts can for example be done using the strategy by Mihalcea et al. (2006). Stylistic Similarity DKPro Similarity includes, for example, a measure which compares function word frequencies (Dinu and Popescu, 2009) between two texts. The framework also includes a set of measures which capture statistical properties of texts such as the type-token ratio (TTR) and the sequential TTR (McCarthy and Jarvis, 2010). Phonetic Similarity DKPro Similarity also allows to compute text similarity based on pairwise phonetic comparisons of words. It therefore"
P13-4021,P10-4006,0,0.0407944,"distance metrics. In DKPro Similarity, some string-based measures (see Section 3.1) are based on implementations from this library. SecondString Toolkit The freely available library by Cohen et al. (2003)12 is similar to SimMetrics, and also implemented in Java. It also contains several well-known text similarity measures on string sequences, and includes many of the measures which are also part of the SimMetrics Library. Some string-based measures in DKPro Similarity are based on the SecondString Toolkit. S-Space Package Even though no designated text similarity library, the S-Space Package (Jurgens and Stevens, 2010)8 contains some text similarity measures such as Latent Semantic Analysis (LSA) and Explicit Semantic Analysis (see Section 3.2). However, it is primarily focused on word space models which operate on word distributions in text. Besides such algorithms, it offers a variety of interfaces, data structures, evaluation datasets and metrics, and global operation utilities e.g. for dimension reduction using Singular Value Decomposition or randomized projections, which are particularly useful with such distributional word space models. DKPro Similarity integrates LSA based on the S-Space Package. 7 C"
P13-4021,S12-1059,1,0.847823,"Missing"
P13-4021,C12-1011,1,0.76946,"Missing"
P13-4021,P02-1020,0,0.0475005,"he-box without further configuration.6 DKPro Similarity contains two major types of experimental setups: (i) those for an intrinsic evaluation allow to evaluate the system performance in an isolated setting by comparing the system results with a human gold standard, and (ii) those for an extrinsic evaluation allow to evaluate the system with respect to a particular task at hand, where text similarity is a means for solving a concrete problem, e.g. recognizing textual entailment. readers come pre-packaged include, among others, the SemEval-2012 STS data (Agirre et al., 2012), the METER corpus (Clough et al., 2002), or the RTE 1–5 data (Dagan et al., 2006). As far as license terms allow redistribution, the datasets themselves are integrated into the framework. Similarity Scorer The Similarity Scorer allows to integrate any text similarity measure (which is decoupled from UIMA by default) into a UIMAbased pipeline. It builds upon the standardized text similarity interfaces and thus allows to easily exchange the text similarity measure as well as to specify the data types the measure should operate on, e.g. tokens or lemmas. Intrinsic Evaluation DKPro Similarity contains the setup (B¨ar et al., 2012a) whi"
P13-4021,N04-3012,0,0.128581,"mework. For that, DKPro Similarity also comes with an example module for getting started, which guides first-time users through both the stand-alone and the UIMA-coupled modes. Semantic Vectors The Semantic Vectors package is a package for distributional semantics (Widdows and Cohen, 2010)9 that contains measures such as LSA and allows for comparing documents within a given vector space. The main focus lies on word space models with a number of dimension reduction techniques, and applications on word spaces such as automatic thesaurus generation. WordNet::Similarity The open source package by Pedersen et al. (2004)10 is a popular Perl library for the similarity computation on WordNet. It comprises six word similarity measures that operate on WordNet, e.g. Jiang and Conrath (1997) or Resnik (1995). Unfortunately, no strategies have been added to the package yet which aggregate the word similarity scores for complete texts in a similar manner as described in Section 3.2. 8 code.google.com/p/airhead-research code.google.com/p/semanticvectors 10 sourceforge.net/projects/wn-similarity 9 125 Acknowledgements This work has been supported by the Volkswagen Foundation as part of the Lichtenberg Professorship Pro"
P13-4021,W99-0625,0,0.0170643,"ural, stylistic, and phonetic similarity. Semantic Similarity Measures DKPro Similarity also contains several measures which go beyond simple character sequences and compute text similarity on a semantic level. Structural Similarity Structural similarity between texts can be computed, for example, by comparing sets of stopword n-grams (Stamatatos, 2011). The idea here is that similar texts may preserve syntactic similarity while exchanging only content words. Other measures in DKPro Similarity allow to compare texts by part-of-speech ngrams, and order and distance features for pairs of words (Hatzivassiloglou et al., 1999). Pairwise Word Similarity These measures are based on pairwise word similarity computations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5 , and Wikipedia, while the framework can easily be extended to other data sources that conform to a common interface (Garoufi et al., 2008). Pairwise similarity measures in DKPro Similarity include Jiang and Conrath (1997) or Resnik (1995)."
P13-4021,S12-1051,0,\N,Missing
P14-2031,J08-4004,0,0.110471,"Missing"
P14-2031,E12-1036,0,0.0217427,"curacy. To deal with the class imbalance problem, we applied cost-sensitive classification. In correspondence with the distribution of class sizes in the training data, the cost for false negatives was set to 4, and for false positives to 1. A reduction of the feature set as judged by a χ2 ranker improved the results for both Random Forest as well as the SVM, so we limited our feature set to the 100 best features. In a 10-fold cross-validation experiment, we tested a Random Forest classifier (Breiman, 2001) and an SVM (Platt, 1998) with polynomial kernel. Previous work (Ferschke et al., 2012; Bronner and Monz, 2012) has shown that these algorithms work well for edit and turn classification. As baseline, we defined a majority class classifier, which labels all edit-turn-pairs as non-corresponding. Similarity between turn and edit text We propose a number of features which are purely based on the textual similarity between the text of the turn, and the edited text and context. We used the cosine similarity, longest common subsequence, and word n-gram similarity measures. Cosine similarity was applied on binary weighted term vectors (L2 norm). The word n-gram measure (Lyon et al., 2004) calculates a Jaccard"
P14-2031,C12-1044,1,0.853727,"nt to understand the nature of correspondence between Wikipedia article edits and discussion page turns. Second, we want to know the distinctive properties of corresponding edit-turn-pairs and how to use these to automatically detect corresponding pairs. 2 Edit-Turn-Pairs In this section, we will define the basic units of our task, namely edits and turns. Furthermore, we will explain the kind of correspondence between edits and turns we are interested in. Edits To capture a fine-grained picture of changes to Wikipedia article pages, we rely on the notion of edits defined in our previous work (Daxenberger and Gurevych, 2012). Edits are coherent modifications based on a pair of adjacent revisions from Wikipedia article pages. To calculate edits, a line-based diff comparison between the old revision and the new revision is made, followed by several post-processing steps. Each pair of adjacent revisions found in the edit history of an article consists of one or more edits, which describe either inserted, deleted, changed or relocated text. Edits are associated with metadata from the revision they belong to, this includes the comment (if present), the user name and the time stamp. 3 Corpus With the help of Amazon Mec"
P14-2031,D13-1055,1,0.897651,"Missing"
P14-2031,P14-5011,1,0.760131,"ponding and non-corresponding pairs, and required turkers to pass a qualification test. The average pairwise percentage agreement over all pairs is 0.66. This was calculated as PC c 1 PN c=1 vi , where N = 750 is the overall i=1 N C 2 number of annotated edit-turn-pairs, C = R 2−R is the number of pairwise comparisons, R = 5 is the number of raters per edit-turn-pair, and vic = 1 if a pair of raters c labeled edit-turn-pair i equally, and 0 otherwise. The moderate pairwise agreement reflects the complexity of this task for non-experts. 4 Machine Learning with Edit-Turn-Pairs We used DKPro TC (Daxenberger et al., 2014) to carry out the machine learning experiments on edit-turn-pairs. For each edit, we stored both the edited paragraph and its context from the old revision as well as the edited paragraph and context from the new revision. We used Apache Gold Standard To rule out ambiguous cases, we created the Gold Standard corpus with the help of majority voting. We counted an edit-turn-pair as corresponding, if it was annotated as “corresponding” by least three out of five annotators, and likewise for non-corresponding pairs. Furthermore, we deleted 21 pairs for which the turn seg3 http://www.ukp.tu-darmsta"
P14-2031,zesch-etal-2008-extracting,1,0.852191,"Missing"
P14-2031,E12-1079,1,0.920171,"ps to get a better picture of the collaborative writing process. To enable such interaction, we extract segments from discussion pages, called turns, and connect them to corresponding edits in the respective article. Consider the following snippet from the discussion page of the article “Boron” 187 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 187–192, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics A definition of correspondence is not straightforward in the context of edit-turn-pairs. Ferschke et al. (2012) suggest four types of explicit performatives in their annotation scheme for dialog acts of Wikipedia turns. Due to their performative nature, we assume that these dialog acts make the turn they belong to a good candidate for a corresponding edit-turn-pair. We therefore define an edit-turn-pair as corresponding, if: i) The turn is an explicit suggestion, recommendation or request and the edit performs this suggestion, recommendation or request, ii) the turn is an explicit reference or pointer and the edit adds or modifies this reference or pointer, iii) the turn is a commitment to an action in"
P14-5006,S10-1004,0,0.0762358,"ases further. Experiments where noun chunks are selected as keyphrases perform best for this example. Named entities are too restrictive, but applicable for identifying relevant entities in a text. This is useful for tasks that are targeted towards entities, e.g. for finding experts (D¨orner et al., 2007) in a collection of domaindependent texts. The selection of a linguistic type is not limited as preprocessing components might introduce further types. 2.3 Ranking 2.5 Evaluation DKPro Keyphrases ships with all the metrics that have been traditionally used for evaluating keyphrase extraction. Kim et al. (2010) use precision and recall for a different number of keyphrases (5, 10 and 15 keyphrases). These metrics are widely used for evaluation in information retrieval. Precision @5 is the ratio of true positives in the set of extracted keyphrases when 5 keyphrases are extracted. Recall @5 is the ratio of true positives in the set of gold keyphrases when 5 keyphrases are extracted. Moreover, DKPro Keyphrases evaluates with MAP and R-precision. MAP is the mean average precision of extracted keyphrases from the highest scored keyphrase to the total number of extracted keyphrases. For each position in th"
P14-5006,D09-1137,0,0.0187621,"API, which offers automatic keyphrase extraction from texts. They provide a supervised approach for keyphrase extraction. For each keyphrase, KEA computes frequency, position, and semantic relatedness as features. Thus, for using KEA, the user needs to provide annotated training data. KEA generates keyphrases from n-grams with length from 1 to 3 tokens. A controlled vocabulary can be used to filter keyphrases. The configuration for keyphrase selection and filtering is limited compared to DKPro Keyphrases, which offers capabilities for changing the entire preprocessing or adding filters. Maui (Medelyan et al., 2009) enhances KEA by allowing the computation of semantic relatedness of keyphrases. It uses Wikipedia as a thesaurus and computes the keyphraseness of each keyphrase, which is the number of times a candidate was used as keyphrase in the training data (Medelyan et al., 2009). Although Maui provides training data along with their software, this training data is highly domain-specific. A shortcoming of KEA and Maui is the lack of any evaluation capabilities or the possibility to run parameter sweeping experiments. DKPro Keyphrases provides evaluation tools for automatic testing of many parameter set"
P14-5006,R09-1086,1,0.833455,"n average precision of extracted keyphrases from the highest scored keyphrase to the total number of extracted keyphrases. For each position in the rank, the precision at that position will be computed. Summing up the precision at each recall point and then taking its average will return the average precision for the text being evaluated. The mean average precision will be the mean from the sum of each text’s average precision from the dataset. R-precision is the ratio of true positives in the set of extracted keyphrases, when the set is limited to the same size as the set of gold keyphrases (Zesch and Gurevych, 2009). Filtering Filtering can be used together with overgenerating selection approaches like taking all ngrams to decrease the number of keyphrases before ranking. One possible approach is based on POS patterns. For example, using the POS patterns, Adjective-Noun, Adjective, and Noun limits the set of possible keyphrases to “dog”, “old cat”, “cat”, and “garden” in the previous example. This step can also been performed as part of the selection step, however, keeping it separated enables researchers to apply filters to keyphrases of any linguistic type. DKPro Keyphrases provides the possibility to"
P14-5006,W00-0405,0,0.242278,"Missing"
P14-5006,W04-3252,0,\N,Missing
P14-5011,P13-1166,0,0.0421748,"Missing"
P14-5011,P13-4020,0,0.0156067,"ws TextClassificationException { int nrOfEmoticons = JCasUtil.select(annoDb, EMO.class).size(); int nrOfTokens = JCasUtil.select(annoDb, Token.class).size(); double ratio = (double) nrOfEmoticons / nrOfTokens; return new Feature(&quot;EmoticonRatio&quot;, ratio).asList(); } } Listing 2: A DKPro TC document mode feature extractor measuring the ratio of emoticons to tokens. customizable research environment for quick experiments and does not provide predefined text classification setups. Furthermore, it does not support parameter sweeping and has no explicit support for creating experiment reports. Argo (Rak et al., 2013) is a web-based workbench with support for manual annotation and automatic analysis of mainly bio-medical data. Like DKPro TC, Argo is based on UIMA, but focuses on sequence tagging, and it lacks DKPro TC’s parameter sweeping capabilities. NLTK (Bird et al., 2009) is a general-purpose NLP toolkit written in Python. It offers components for a wide range of preprocessing tasks and also supports feature extraction and machine learning for supervised text classification. Like DKPro TC, it can be used to quickly setup baseline experiments. As opposed to DKPro TC, NLTK lacks a modular structure with"
P14-5011,D12-1042,0,0.0252752,"· Text Categorization · Keyphrase Assignment · Text Readability Unit/Sequence Mode · Named Entity Recognition · Part-of-Speech Tagging · Dialogue Act Tagging · Word Difficulty Pair Mode · Paraphrase Identification · Textual Entailment · Relation Extraction · Text Similarity Table 1: Supervised learning scenarios and feature modes supported in DKPro TC, with example NLP applications. • The pair mode is intended for problems which require a pair of texts as input, e.g. a pair of sentences to be classified as paraphrase or non-paraphrase. It represents a special case of multi-instance learning (Surdeanu et al., 2012), in which a document contains exactly two instances. Flexibility Users of a system for supervised learning on textual data should be able to choose between different machine learning approaches depending on the task at hand. In supervised machine learning, we have to distinguish between approaches based on classification and approaches based on regression. In classification, given a document d ∈ D and a set of labels C = {c1 , c2 , ..., cn }, we want to label each document d with L ⊂ C, where L is the set of relevant or true labels. In single-label classification, each document d is labeled w"
P14-5011,P11-2008,0,\N,Missing
P14-5016,W09-0715,0,0.0241276,"Missing"
P14-5016,W07-1501,0,0.00902618,"(Bontcheva et al., 2013) automation component is most similar to our work. It is based either on plugins and externally trained classification models, or uses web services. Thus, it is highly task specific and requires extensive configuration. The automatic annotation suggestion component in our tool, in contrast, is easily configurable and adaptable to different annotation tasks and allows the use of annotations from the current annotation project. 2 Custom annotation layers Generic annotation data models are typically directed graph models (e.g. GATE, UIMA CAS (G¨otz and Suhre, 2004), GrAF (Ide and Suderman, 2007)). In addition, an annotation schema defines possible kinds of annotations, their properties and relations. While these models offer great expressiveness and flexibility, it is difficult to adequately transfer their power into a convenient annotation editor. For example, one schema may prescribe that the part-of-speech tag is a property on a Token annotation, another one may prescribe that the tag is a separate annotation, which is linked to the token. An annotator should not be exposed to these details in the UI and should be able to just edit a part-of-speech tag, ignorant of the internal re"
P14-5016,N03-4009,0,0.188268,"Missing"
P14-5016,petrov-etal-2012-universal,0,0.0283548,"Missing"
P14-5016,P13-2097,0,0.062934,"Missing"
P14-5016,benikova-etal-2014-nosta,1,\N,Missing
P14-5016,E12-2021,0,\N,Missing
P14-5016,P13-4001,1,\N,Missing
P15-1070,C14-1151,0,0.0466816,"Missing"
P15-1070,W00-0103,0,0.10102,"his assumption, lexical ambiguity arises due to there being a plurality of words with the same surface form but different meanings, and the task of the interpreter is to select correctly among them. An alternative view is that each word is a single lexical entry whose specific meaning is underspecified until it is activated by the context (Ludlow, 1996). In the case of systematically polysemous terms (i.e., words that have several related senses shared in a systematic way by a group of similar words), it may not be necessary to disambiguate them at all in order to interpret the communication (Buitelaar, 2000). While there has been some research in modelling intentional lexical-semantic underspecification (Jurgens, 2014), it is intended for closely related senses such as those of systematically polysemous terms, not those of coarser-grained homonyms which are the subject of this paper. 719 as paronomasia and syllepsis, or more generally as puns, in which homonymic (i.e., coarse-grained) lexical-semantic ambiguity is a deliberate effect of the communication act. That is, the writer intends for a certain word or other lexical item to be interpreted as simultaneously carrying two or more separate mean"
P15-1070,P92-1032,0,0.599221,"n and recall) (Palmer et al., 2006). The traditional approach to scoring individual targets is not usable as-is for pun disambiguation, because each pun carries two disjoint but equally valid sets of sense annotations. Instead, since our 5.2 Baselines System performance in WSD is normally interpreted with reference to one or more baselines. To our knowledge, ours is the very first study of automatic pun disambiguation on any scale, so at this point there are no previous systems against which to compare our results. However, traditional WSD systems are often compared with two na¨ıve baselines (Gale et al., 1992) which can be adapted for our purposes. 6 http://www.omegawiki.org/ 725 The second na¨ıve baseline for WSD, known as most frequent sense (MFS), is a supervised baseline, meaning that it depends on a manually senseannotated background corpus. As its name suggests, it involves always selecting from the candidates that sense which has the highest frequency in the corpus. As with our test algorithms, we adapt this technique to pun disambiguation by having it select the two most frequent senses (according to WordNet’s built-in sense frequency counts). In traditional WSD, MFS baselines are notorious"
P15-1070,W09-2004,0,0.487994,"possible definitions was maximized. The overlap scores for all word pairs were then averaged, and the punchline with the lowest average score selected as the most humorous. 2.3.2 Corpora There are a number of English-language corpora of intentional lexical ambiguity which have been used in past work, usually in linguistics or the social sciences. In their work on computer-generated humour, Lessard et al. (2002) use a corpus of 374 “Tom Swifty” puns taken from the Internet, plus a well-balanced corpus of 50 humorous and nonhumorous lexical ambiguities generated programmatically (Venour, 1999). Hong and Ong (2009) also study humour in natural language generation, using a smaller data set of 27 punning riddles derived from a mix of natural and artificial sources. In their study of wordplay in religious advertising, Bell et al. (2011) compile a corpus of 373 puns taken from church marquees and literature, and compare it against a general corpus of 1515 puns drawn from Internet websites and a specialized dictionary. Zwicky and Zwicky (1986) conduct a phonological analysis on a corpus of several thousand puns, some of which they collected themselves from advertisements and catalogues, and the remainder of"
P15-1070,S13-2049,0,0.0336499,"Missing"
P15-1070,jurgens-2014-analysis,0,0.0182775,"ferent meanings, and the task of the interpreter is to select correctly among them. An alternative view is that each word is a single lexical entry whose specific meaning is underspecified until it is activated by the context (Ludlow, 1996). In the case of systematically polysemous terms (i.e., words that have several related senses shared in a systematic way by a group of similar words), it may not be necessary to disambiguate them at all in order to interpret the communication (Buitelaar, 2000). While there has been some research in modelling intentional lexical-semantic underspecification (Jurgens, 2014), it is intended for closely related senses such as those of systematically polysemous terms, not those of coarser-grained homonyms which are the subject of this paper. 719 as paronomasia and syllepsis, or more generally as puns, in which homonymic (i.e., coarse-grained) lexical-semantic ambiguity is a deliberate effect of the communication act. That is, the writer intends for a certain word or other lexical item to be interpreted as simultaneously carrying two or more separate meanings, or alternatively for it to be unclear which meaning is the intended one. There are a variety of motivations"
P15-1070,P98-2127,0,0.0501128,"for the target word and comparing it against the sense information stored for that word. A seminal knowledge-based example is the Lesk algorithm (Lesk, 1986) which disambiguates a pair of target terms in context by comparing their respective dictionary definitions and selecting the two with the greatest number of words in common. Though simple, the Lesk algorithm performs surprisingly well, and has frequently served as the basis of more sophisticated approaches. In recent years, Lesk variants in which the contexts and definitions are supplemented with entries from a distributional thesaurus (Lin, 1998) have achieved state-of-the-art performance for knowledge-based systems on standard data sets (Miller et al., 2012; Basile et al., (5) Tom moped. (6) “I want a scooter,” Tom moped. In the first of these sentences, the word moped is unambiguously a verb with the lemma mope, and would be correctly recognized as such by any automatic lemmatizer and part-of-speech tagger. The moped of the second example is a pun, one of whose meanings is the same inflected form of the verb mope (“to sulk”) and the other of which is the noun moped (“motorized scooter”). For such cases an automated pun identifier wo"
P15-1070,C14-2023,1,0.88612,"Missing"
P15-1070,H05-1067,0,0.558684,"Missing"
P15-1070,C12-1109,1,0.786011,"edge-based example is the Lesk algorithm (Lesk, 1986) which disambiguates a pair of target terms in context by comparing their respective dictionary definitions and selecting the two with the greatest number of words in common. Though simple, the Lesk algorithm performs surprisingly well, and has frequently served as the basis of more sophisticated approaches. In recent years, Lesk variants in which the contexts and definitions are supplemented with entries from a distributional thesaurus (Lin, 1998) have achieved state-of-the-art performance for knowledge-based systems on standard data sets (Miller et al., 2012; Basile et al., (5) Tom moped. (6) “I want a scooter,” Tom moped. In the first of these sentences, the word moped is unambiguously a verb with the lemma mope, and would be correctly recognized as such by any automatic lemmatizer and part-of-speech tagger. The moped of the second example is a pun, one of whose meanings is the same inflected form of the verb mope (“to sulk”) and the other of which is the noun moped (“motorized scooter”). For such cases an automated pun identifier would therefore need to account for all possible lemmas for all possible parts of speech of the target word. The sit"
P15-1070,P13-4007,1,0.902503,"Missing"
P15-1070,passonneau-2006-measuring,0,0.0493062,"Missing"
P15-1070,P10-1154,0,0.031715,"Missing"
P15-1070,W04-0811,0,0.0227968,"Missing"
P15-1070,N03-1033,0,0.0299283,". The first is motivated by the informal observation that, though the two meanings of a pun may have different parts of speech, at least one of the parts 5 In our implementation, the sense definitions are formed by concatenating the synonyms, gloss, and example sentences provided by WordNet. of speech is grammatical in the context of the sentence, and so would probably be the one assigned by a stochastic or rule-based POS tagger. Our “POS” tie-breaker therefore preferentially selects the best sense, or pair of senses, whose POS matches the one applied to the target by the Stanford POS tagger (Toutanova et al., 2003). For our second tie-breaking strategy, we posit that since humour derives from the resolution of semantic incongruity (Raskin, 1985; Attardo, 1994), puns are more likely to exploit coarse-grained homonymy than than fine-grained systematic polysemy. Thus, following Matuschek et al. (2014), we induced a clustering of WordNet senses by aligning WordNet to the more coarse-grained OmegaWiki LSR.6 Our “cluster” fallback works the same as the “POS” one, with the addition that any remaining ties among senses with the second-highest overlap are resolved by preferentially selecting those which are not"
P15-1070,S07-1006,0,0.0149727,"Missing"
P15-1070,S01-1005,0,0.0218802,"Missing"
P15-1070,passonneau-etal-2006-inter,0,0.0324912,"Missing"
P15-1070,C98-2122,0,\N,Missing
P15-4003,W05-0619,0,0.034072,"search results for a given query by confidence of the classifier. Related work Yet another form of in-tool learning is active learning, as is implemented, e.g., in Dualist (Settles, 2011). In an active learning scenario the system aims to efficiently train an accurate classifier (i.e. with as little training data as possible) and thus repeatedly asks the user to annotate instances from which it can learn the most. Such an approach can work well for reducing the amount of training data needed to produce a model which achieves high accuracy, as has been — amongst others — shown by Hachey et al. (2005). However, they also learn in their experiments that those highly informative instances are often harder to annotate and increase required time and effort of annotators. Our approach is different from active Existing annotation tools include automation functionality for annotation tasks, ranging from rule-based tagging to more complex, machinelearning-based approaches. Such functionalities can be found in the annotation software WordFreak (Morton and LaCivita, 2003), where a plug-in architecture allows for a variety of different taggers and classifiers to be integrated, for example part-of-spe"
P15-4003,P03-1054,0,0.0136527,"Missing"
P15-4003,N03-4009,0,0.0456195,"Missing"
P15-4003,E06-1015,0,0.00927054,"t may be actual instances. This still leaves the annotator with the tedious task of clicking through the search results to mark the true instances. https://dkpro.github.io/dkpro-csniper 13 Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 13–18, c Beijing, China, July 26-31, 2015. 2015 ACL and AFNLP tences containing infrequent ambiguous grammatical constructions (cf. Section 3). Since parsing the query results is too time-intensive to be done during runtime, we parsed the corpora in advance and stored the parse trees in a database. To train the classifier, we employed SVM-light-tk (Moschitti, 2006; Joachims, 1999), a support vector machine implementation which uses a tree kernel to integrate all sub-trees of the parse tree as features. Consider the following typical scenario incorporating the ranking: A user constructs a query based on various features, such as POS tags or lemmata, which are used to search for matching sentences, e.g. To reduce the time and effort required, we present an extension of the annotation-by-query approach (Figure 1) that introduces a ranking of the query results (Section 2) by means of machine learning; we order the results by confidence of the used classifi"
P15-4003,D11-1136,0,0.03276,"uses a split-pane view, showing automatic suggestions in one pane and manually entered annotations in another. The user can accept a suggested annotation, which is transferred to the manual pane. Lacking the search capability, WebAnno lists automatic annotations in the running corpus text, which makes it unsuited for selective annotation in large corpora. The approach that we implemented on top of CSniper instead ranks the search results for a given query by confidence of the classifier. Related work Yet another form of in-tool learning is active learning, as is implemented, e.g., in Dualist (Settles, 2011). In an active learning scenario the system aims to efficiently train an accurate classifier (i.e. with as little training data as possible) and thus repeatedly asks the user to annotate instances from which it can learn the most. Such an approach can work well for reducing the amount of training data needed to produce a model which achieves high accuracy, as has been — amongst others — shown by Hachey et al. (2005). However, they also learn in their experiments that those highly informative instances are often harder to annotate and increase required time and effort of annotators. Our approac"
P15-4003,P14-5016,1,0.894414,"Missing"
P15-4003,P12-3015,1,\N,Missing
P15-4003,W14-5201,1,\N,Missing
P16-1150,D15-1075,0,0.0932182,"premises support a claim (Stegmann et al., 2011), or by the complexity of the analyzed argument scheme (Garcia-Mila et al., 2013). To the best of our knowledge, there have been only few attempts in computational argumentation that go deeper than analyzing argument structures (e.g., (Park and Cardie, 2014) mentioned above). Persing and Ng (2015) model argument strength in persuasive essays using a manually annotated corpus of 1,000 documents labeled with a 1–4 score value. Our newly created corpus of annotated pairs of arguments might resemble recent large-scale corpora for textual inference. Bowman et al. (2015) introduced a 570k sentence pairs written by crowd-workers, the largest corpus to date. Whereas their task is to classify whether the sentence pair represents entailment, contradiction, or is neutral (thus heading towards a deep semantic understanding), our goal is to assess the pragmatical properties of the given multiple sentence-long arguments (to which extent they fulfill the goal of persuasion). Moreover, each of our annotated argument pairs is accompanied with five textual reasons that explain the rationale behind the labeler’s decision. This is, to the best of our knowledge, a unique no"
P16-1150,D16-1053,0,0.00497753,"tal ordering, while convincingness of arguments is a yet unexplored task, thus no assumptions can be made apriori. There is also a substantial body of work on learning to rank, where also a pairwise approach is widely used (Cao et al., 2007). These methods have been traditionally used in IR, 1590 2 See (Shah et al., 2015) for a recent overview. where the retrieved documents are ranked according to their relevance and pairs of documents are automatically sampled. Employing LSTM for natural language inference tasks has recently gained popularity (Rockt¨aschel et al., 2016; Wang and Jiang, 2016; Cheng et al., 2016). These methods are usually tested on the SNLI data introduced above (Bowman et al., 2015). 3 Data annotation Since assessing convincingness of a single argument directly is a very subjective task with high probability of introducing annotator’s bias (because of personal preferences, beliefs, or background), we cast the problem as a relation annotation task. Given two arguments, one should be selected as more convincing, or they might be both equally convincing (see an example in Figure 1). 3.1 Sampling annotation candidates Sampling large sets of arguments for annotation from the Web poses se"
P16-1150,P15-1033,0,0.00620344,"ional Linguistics factor, and A2 is just harsh and attacks.1 We adapt pairwise comparison as our backbone approach. We propose a novel task of predicting convincingness of arguments in an argument pair, as well as ranking arguments related to a certain topic. Since no data for such a task are available, we create a new annotated corpus. We employ SVM model with rich linguistic features as well as bidirectional Long Short-Term Memory (BLSTM) neural networks because of their excellent performance across various end-to-end NLP tasks (Goodfellow et al., 2016; Piech et al., 2015; Wen et al., 2016; Dyer et al., 2015; Rockt¨aschel et al., 2016). Main contributions of this article are (1) large annotated dataset consisting of 16k argument pairs with 56k reasons in natural language (700k tokens), (2) thorough investigation of the annotated data with respect to properties of convincingness as a measure, (3) a SVM model and end-to-end BLSTM model. The annotated data, licensed under CC-BY-SA license, and the experimental code are publicly available at https://github.com/UKPLab/ acl2016-convincing-arguments. We hope it will foster future research in computational argumentation and beyond. 2 Related Work Recent"
P16-1150,D15-1255,1,0.928277,"h argument from an argument pair is more convincing and (2) ranking all arguments to the topic based on their convincingness. We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman’s correlation in a cross-topic evaluation. We release the newly created corpus UKPConvArg1 and the experimental software under open licenses. 1 Prompt: Should physical education be mandatory in schools? Stance: Yes! Introduction What makes a good argument? Despite the recent achievements in computational argumentation, such as identifying argument components (Habernal and Gurevych, 2015; Habernal and Gurevych, 2016), finding evidence for claims (Rinott et al., 2015), or predicting argument structure (Peldszus and Stede, 2015; Stab and Gurevych, 2014), this question remains too hard to be answered. Even Aristotle claimed that perceiving an argument as a “good” one depends on multiple factors (Aristotle and Kennedy (translator), 1991) Argument 1 Argument 2 physical education should be YES, mandatory cuhz 112,000 peochildren don’t underple have died in the year 2011 stand anything excect so far and it’s because of the physical lack of physical activity and especially rich child"
P16-1150,N13-1132,0,0.0378071,"Missing"
P16-1150,W14-2105,0,0.10779,"argumentation and beyond. 2 Related Work Recent years can be seen as a dawn of computational argumentation – an emerging sub-field of NLP in which natural language arguments and argumentation are modeled, searched, analyzed, generated, and evaluated. The main focus has been paid to analyzing argument structures, under the umbrella entitled argumentation mining. Web discourse as a data source has been exploited in several tasks in argumentation mining, such as classifying propositions in user comments into three classes (verifiable experiential, verifiable non-experiential, and unverifiable) (Park and Cardie, 2014), or mapping argument components to Toulmin’s model of argument in user-generated Web discourse (Habernal and Gurevych, 2015), to name a few. While these approaches are crucial for understanding the structure of an argument, they do not directly address any qualitative criteria of argumentation. Argumentation quality has been an active topic 1 These are actual reasons provided by annotators, as will be explained later in Section 3. among argumentation scholars. Walton (1989) discusses validity of arguments in informal logic, while Johnson and Blair (2006) elaborate on criteria for practical ar"
P16-1150,Q14-1025,0,0.0340558,"Missing"
P16-1150,D15-1110,0,0.0905999,"feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman’s correlation in a cross-topic evaluation. We release the newly created corpus UKPConvArg1 and the experimental software under open licenses. 1 Prompt: Should physical education be mandatory in schools? Stance: Yes! Introduction What makes a good argument? Despite the recent achievements in computational argumentation, such as identifying argument components (Habernal and Gurevych, 2015; Habernal and Gurevych, 2016), finding evidence for claims (Rinott et al., 2015), or predicting argument structure (Peldszus and Stede, 2015; Stab and Gurevych, 2014), this question remains too hard to be answered. Even Aristotle claimed that perceiving an argument as a “good” one depends on multiple factors (Aristotle and Kennedy (translator), 1991) Argument 1 Argument 2 physical education should be YES, mandatory cuhz 112,000 peochildren don’t underple have died in the year 2011 stand anything excect so far and it’s because of the physical lack of physical activity and especially rich children people are becoming obese!!!! of rich parents. because some education Figure 1: Example of an argument pair. If we take Argument 1 from F"
P16-1150,D14-1162,0,0.105773,"g., Ari (Senter and Smith, 1967), Coleman-Liau (Coleman and Liau, 1975), Flesch (Flesch, 1948), and others), five sentiment scores (from very negative to very positive) (Socher et al., 2013), spellchecking using standard Unix words, ratio of superlatives, and some surface features such as sentence lengths, longer words count, etc. The resulting feature vector dimension is about 64k. We also use bidirectional Long Short-Term 8 Using LISBVM (Chang and Lin, 2011). Memory (BLSTM) neural network for end-to-end processing.9 The input layer relies on pre-trained word embeddings, in particular GloVe (Pennington et al., 2014) trained on 840B tokens from Common Crawl;10 the embedding weights are further updated during training. The core of the model consists of two bi-directional LSTM networks with 64 output neurons each. Their output is then concatenated into a single drop-out layer and passed to the final sigmoid layer for binary predictions. We train the network with ADAM optimizer (Kingma and Ba, 2015) using binary crossentropy loss function and regularize by early stopping (5 training epochs) and high drop-out rate (0.5) in the dropout layer. For both models, each training/test instance simply concatenates A1"
P16-1150,P15-1053,0,0.273185,"rate on criteria for practical argument evaluation (namely Relevance, Acceptability, and Sufficiency). Yet, empirical research on argumentation quality does not seem to reflect these criteria and leans toward simplistic evaluation using argument structures, such as how many premises support a claim (Stegmann et al., 2011), or by the complexity of the analyzed argument scheme (Garcia-Mila et al., 2013). To the best of our knowledge, there have been only few attempts in computational argumentation that go deeper than analyzing argument structures (e.g., (Park and Cardie, 2014) mentioned above). Persing and Ng (2015) model argument strength in persuasive essays using a manually annotated corpus of 1,000 documents labeled with a 1–4 score value. Our newly created corpus of annotated pairs of arguments might resemble recent large-scale corpora for textual inference. Bowman et al. (2015) introduced a 570k sentence pairs written by crowd-workers, the largest corpus to date. Whereas their task is to classify whether the sentence pair represents entailment, contradiction, or is neutral (thus heading towards a deep semantic understanding), our goal is to assess the pragmatical properties of the given multiple se"
P16-1150,D15-1050,0,0.117107,"topic based on their convincingness. We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman’s correlation in a cross-topic evaluation. We release the newly created corpus UKPConvArg1 and the experimental software under open licenses. 1 Prompt: Should physical education be mandatory in schools? Stance: Yes! Introduction What makes a good argument? Despite the recent achievements in computational argumentation, such as identifying argument components (Habernal and Gurevych, 2015; Habernal and Gurevych, 2016), finding evidence for claims (Rinott et al., 2015), or predicting argument structure (Peldszus and Stede, 2015; Stab and Gurevych, 2014), this question remains too hard to be answered. Even Aristotle claimed that perceiving an argument as a “good” one depends on multiple factors (Aristotle and Kennedy (translator), 1991) Argument 1 Argument 2 physical education should be YES, mandatory cuhz 112,000 peochildren don’t underple have died in the year 2011 stand anything excect so far and it’s because of the physical lack of physical activity and especially rich children people are becoming obese!!!! of rich parents. because some education Figure"
P16-1150,D13-1170,0,0.00148751,"- and bi-gram presence, ratio of adjective and adverb endings that may signalize neuroticism (Corney et al., 2002), contextuality measure (Heylighen and Dewaele, 2002), dependency tree depth, ratio of exclamation or quotation marks, ratio of modal verbs, counts of several named entity types, ratio of past vs. future tense verbs, POS n-grams, presence of dependency tree production rules, seven different readability measures (e.g., Ari (Senter and Smith, 1967), Coleman-Liau (Coleman and Liau, 1975), Flesch (Flesch, 1948), and others), five sentiment scores (from very negative to very positive) (Socher et al., 2013), spellchecking using standard Unix words, ratio of superlatives, and some surface features such as sentence lengths, longer words count, etc. The resulting feature vector dimension is about 64k. We also use bidirectional Long Short-Term 8 Using LISBVM (Chang and Lin, 2011). Memory (BLSTM) neural network for end-to-end processing.9 The input layer relies on pre-trained word embeddings, in particular GloVe (Pennington et al., 2014) trained on 840B tokens from Common Crawl;10 the embedding weights are further updated during training. The core of the model consists of two bi-directional LSTM netw"
P16-1150,D14-1006,1,0.521221,"rectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman’s correlation in a cross-topic evaluation. We release the newly created corpus UKPConvArg1 and the experimental software under open licenses. 1 Prompt: Should physical education be mandatory in schools? Stance: Yes! Introduction What makes a good argument? Despite the recent achievements in computational argumentation, such as identifying argument components (Habernal and Gurevych, 2015; Habernal and Gurevych, 2016), finding evidence for claims (Rinott et al., 2015), or predicting argument structure (Peldszus and Stede, 2015; Stab and Gurevych, 2014), this question remains too hard to be answered. Even Aristotle claimed that perceiving an argument as a “good” one depends on multiple factors (Aristotle and Kennedy (translator), 1991) Argument 1 Argument 2 physical education should be YES, mandatory cuhz 112,000 peochildren don’t underple have died in the year 2011 stand anything excect so far and it’s because of the physical lack of physical activity and especially rich children people are becoming obese!!!! of rich parents. because some education Figure 1: Example of an argument pair. If we take Argument 1 from Figure 1, assigning a singl"
P16-1150,N16-1170,0,0.0119334,"known properties of total ordering, while convincingness of arguments is a yet unexplored task, thus no assumptions can be made apriori. There is also a substantial body of work on learning to rank, where also a pairwise approach is widely used (Cao et al., 2007). These methods have been traditionally used in IR, 1590 2 See (Shah et al., 2015) for a recent overview. where the retrieved documents are ranked according to their relevance and pairs of documents are automatically sampled. Employing LSTM for natural language inference tasks has recently gained popularity (Rockt¨aschel et al., 2016; Wang and Jiang, 2016; Cheng et al., 2016). These methods are usually tested on the SNLI data introduced above (Bowman et al., 2015). 3 Data annotation Since assessing convincingness of a single argument directly is a very subjective task with high probability of introducing annotator’s bias (because of personal preferences, beliefs, or background), we cast the problem as a relation annotation task. Given two arguments, one should be selected as more convincing, or they might be both equally convincing (see an example in Figure 1). 3.1 Sampling annotation candidates Sampling large sets of arguments for annotation"
P16-1150,N16-1015,0,0.00624519,"ation for Computational Linguistics factor, and A2 is just harsh and attacks.1 We adapt pairwise comparison as our backbone approach. We propose a novel task of predicting convincingness of arguments in an argument pair, as well as ranking arguments related to a certain topic. Since no data for such a task are available, we create a new annotated corpus. We employ SVM model with rich linguistic features as well as bidirectional Long Short-Term Memory (BLSTM) neural networks because of their excellent performance across various end-to-end NLP tasks (Goodfellow et al., 2016; Piech et al., 2015; Wen et al., 2016; Dyer et al., 2015; Rockt¨aschel et al., 2016). Main contributions of this article are (1) large annotated dataset consisting of 16k argument pairs with 56k reasons in natural language (700k tokens), (2) thorough investigation of the annotated data with respect to properties of convincingness as a measure, (3) a SVM model and end-to-end BLSTM model. The annotated data, licensed under CC-BY-SA license, and the experimental code are publicly available at https://github.com/UKPLab/ acl2016-convincing-arguments. We hope it will foster future research in computational argumentation and beyond. 2 R"
P16-1150,J17-1004,1,\N,Missing
P16-1191,N09-1003,0,0.172471,"Missing"
P16-1191,P11-2123,0,0.0488816,"and the contribution of WSD to downstream document classification tasks remains “mostly speculative”(Ciaramita and Altun, 2006), which can be attributed to the too subtle sense distinctions (Navigli, 2009). This is why supersenses, the coarse-grained word labels based on WordNet’s (Fellbaum, 1998) lexicographer files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL, PERSON or FEELING and 15 labels for verbs, such as COMMUNICATION, MOTION or COGNITION . Usage of supersense labels has been shown to improve dependency parsing (Agirre et al., 2011), named entity recognition (Marrero et al., 2009; R¨ud et al., 2011), non-factoid question answering (Surdeanu et al., 2011), question generation (Heilman, 2011), semantic role labeling (Laparra and Rigau, 2013), personality profiling (Flekova and Gurevych, 2015), semantic similarity (Severyn et al., 2013) and metaphor detection (Tsvetkov et al., 2013). An alternative path to semantic interpretation follows the distributional hypothesis (Harris, 1954). Recently, word vector representations learned with neural-network based language models have contributed to state-of-the-art results on various"
P16-1191,W11-0311,0,0.0167226,"nse disambiguation (WSD) task has therefore received a substantial amount of attention (see Navigli (2009) or Pal and Saha (2015) for an overview). Words in training and evaluation data are usually annotated with senses taken from a particular lexical semantic resource, most commonly WordNet (Miller, 1995). However, WordNet has been criticized to provide too fine-grained distinctions for end level applications. e.g. in machine translation or information retrieval (Izquierdo et al., 2009). Although some researchers report an improvement in sentiment prediction using WSD (Rentoumi et al., 2009; Akkaya et al., 2011; Sumanth and Inkpen, 2015), the publication bias toward positive results (Plank et al., 2014) impedes the comparison to experiments with the opposite conclusion, and the contribution of WSD to downstream document classification tasks remains “mostly speculative”(Ciaramita and Altun, 2006), which can be attributed to the too subtle sense distinctions (Navigli, 2009). This is why supersenses, the coarse-grained word labels based on WordNet’s (Fellbaum, 1998) lexicographer files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL"
P16-1191,D15-1084,0,0.0546556,"Missing"
P16-1191,D14-1110,0,0.0999887,"Missing"
P16-1191,W06-1670,0,0.276544,"y WordNet (Miller, 1995). However, WordNet has been criticized to provide too fine-grained distinctions for end level applications. e.g. in machine translation or information retrieval (Izquierdo et al., 2009). Although some researchers report an improvement in sentiment prediction using WSD (Rentoumi et al., 2009; Akkaya et al., 2011; Sumanth and Inkpen, 2015), the publication bias toward positive results (Plank et al., 2014) impedes the comparison to experiments with the opposite conclusion, and the contribution of WSD to downstream document classification tasks remains “mostly speculative”(Ciaramita and Altun, 2006), which can be attributed to the too subtle sense distinctions (Navigli, 2009). This is why supersenses, the coarse-grained word labels based on WordNet’s (Fellbaum, 1998) lexicographer files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL, PERSON or FEELING and 15 labels for verbs, such as COMMUNICATION, MOTION or COGNITION . Usage of supersense labels has been shown to improve dependency parsing (Agirre et al., 2011), named entity recognition (Marrero et al., 2009; R¨ud et al., 2011), non-factoid question answering (Surde"
P16-1191,W03-1022,0,0.535995,"Missing"
P16-1191,J15-2004,0,0.0726947,"Missing"
P16-1191,E14-1049,0,0.145466,"wledge base unification or semantic similarity, the contribution of such vectors to downstream document classification problems can be challenging, given the fine granularity of the WordNet senses (cf. the discussion in Navigli (2009)). As discussed above, supersenses have been shown to be better suited for carrying the relevant amount of semantic information. An alternative approach focuses on altering the objective of the learning mechanism to capture relational and similarity information from knowledge bases (Bordes et al., 2011; Bordes et al., 2012; Yu and Dredze, 2014; Bian et al., 2014; Faruqui and Dyer, 2014; Goikoetxea et al., 2015). While, in principle, supersenses could be seen as a relation between a word and its hypernym, to our knowledge they have not been explicitly employed in these works. Moreover, an important advantage of our explicit supersense embeddings compared to the retrained vectors is their direct interpretability. 2.2 Supersense Tagging Supersenses, also known as lexicographer files or semantic fields, were originally used to organize lexical-semantic resources (Fellbaum, 1990). The supersense tagging task was introduced by Ciaramita and Johnson (2003) for nouns and later expa"
P16-1191,W16-2506,0,0.0146861,"performance than those trained solely on the same text without supersenses on 4 out of 5 tasks (Table 5). In addition, the explicit supersense information could be further exploited, similarly to previous sense embedding works (Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014). Furthermore, note that while we report the performance of our embeddings on the word similarity tasks for completeness, there has been a substantial discussion on seeking alternative ways to quantify embedding quality with the focus on their purpose in downstream applications (Li and Jurafsky, 2015; Faruqui et al., 2016). Therefore, in the remainder of this paper we explore the usefulness of supersense embeddings in text classification tasks. 5 Table 4: Accuracy and standard error on analogy tasks. Tasks related to noun supersense distinctions show the tendency to improve, while syntax-related information is pushed to the background. In most cases, however, the difference is not significant. MEN 73.18 74.26 Building a Supersense Tagger The task of predicting supersenses has recently regained its popularity (Johannsen et al., 2014; Schneider and Smith, 2015), since supersenses provide disambiguating informatio"
P16-1191,D15-1208,1,0.843755,"labels based on WordNet’s (Fellbaum, 1998) lexicographer files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL, PERSON or FEELING and 15 labels for verbs, such as COMMUNICATION, MOTION or COGNITION . Usage of supersense labels has been shown to improve dependency parsing (Agirre et al., 2011), named entity recognition (Marrero et al., 2009; R¨ud et al., 2011), non-factoid question answering (Surdeanu et al., 2011), question generation (Heilman, 2011), semantic role labeling (Laparra and Rigau, 2013), personality profiling (Flekova and Gurevych, 2015), semantic similarity (Severyn et al., 2013) and metaphor detection (Tsvetkov et al., 2013). An alternative path to semantic interpretation follows the distributional hypothesis (Harris, 1954). Recently, word vector representations learned with neural-network based language models have contributed to state-of-the-art results on various linguistic tasks (Bordes et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015). In this work, we present a novel approach for incorporating the supersense information into the word embedding space and propose a new methodology for util"
P16-1191,P14-1024,0,0.0246129,"Missing"
P16-1191,P15-1010,0,0.243907,"ough, 1965) and MC-30 (Miller and Charles, 1991). 353-S 76.93 78.63 353-R 62.11 61.22 RG-65 79.13 79.75 MC-30 79.49 80.94 Table 5: Performance of our vectors (Spearman’s ρ) on five similarity datasets. Results indicate a trend of better performance of vectors trained jointly with supersenses. The word embeddings for words trained jointly with supersenses achieve higher performance than those trained solely on the same text without supersenses on 4 out of 5 tasks (Table 5). In addition, the explicit supersense information could be further exploited, similarly to previous sense embedding works (Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014). Furthermore, note that while we report the performance of our embeddings on the word similarity tasks for completeness, there has been a substantial discussion on seeking alternative ways to quantify embedding quality with the focus on their purpose in downstream applications (Li and Jurafsky, 2015; Faruqui et al., 2016). Therefore, in the remainder of this paper we explore the usefulness of supersense embeddings in text classification tasks. 5 Table 4: Accuracy and standard error on analogy tasks. Tasks related to noun supersense distinctions sh"
P16-1191,E09-1045,0,0.126473,"Missing"
P16-1191,S14-1001,0,0.14083,"ce then, the system, resp. its reimplementation by Heilman2 , was widely used in applied tasks (Agirre et al., 2011; Surdeanu et al., 2011; Laparra and Rigau, 2013). Supersense taggers have then been built also for Italian (Picca et al., 2008), Chinese (Qiu et al., 2011) and Arabic (Schneider et al., 2013). Tsvetkov et al. (2015) proposes the usage of SemCor supersense frequencies as a way to evaluate word embedding models, showing that a good alignment of embedding dimensions to supersenses correlates with performance of the vectors in word similarity and text classification tasks. Recently, Johannsen et al. (2014) introduced a task of multiword supersense tagging on Twitter. On their newly constructed dataset, they show poor domain adaptation performance of previous systems, achieving a maximum performance with a searchbased structured prediction model (Daum´e III et al., 2009) trained on both Twitter and SemCor data. In parallel, Schneider and Smith (2015) expanded a multiword expression (MWE) annotated corpus of online reviews with supersense information, following an alternative annotation scheme focused on MWE. Similarly to Johannsen et al. (2014), they find that SemCor may not be a sufficient reso"
P16-1191,D15-1278,0,0.0162258,"demonstrate an alternative, deep learning approach, in which we process the original text in parallel to the supersense information. The model can then flexibly learn the usefulness of provided input. We demonstrate that the model extended with supersense embeddings outperforms the same model using only word-based features on a range of classification tasks. 6.1 Experimental Setup Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) are state-of-the-art semantic composition models for a variety of text classification tasks (Kim, 2014; Li et al., 2015; Johnson and Zhang, 2014). Recently, their combinations have been proposed, achieving an unprecedented performance (Sainath et al., 2015). We extend the CNN-LSTM approach from the publicly available 2034 convolutional and LSTM layers in a similar fashion. Afterwards, we concatenate all LSTM outputs and feed them into a standard fully connected neural network layer, followed by the sigmoid for the binary output. The following subsections discuss our results on a range of classification tasks: subjectivity prediction, sentiment polarity classification and metaphor detection. Noun.Animal Verb.Co"
P16-1191,magnini-cavaglia-2000-integrating,0,0.0440666,"Missing"
P16-1191,N13-1090,0,0.521761,"l., 2009; R¨ud et al., 2011), non-factoid question answering (Surdeanu et al., 2011), question generation (Heilman, 2011), semantic role labeling (Laparra and Rigau, 2013), personality profiling (Flekova and Gurevych, 2015), semantic similarity (Severyn et al., 2013) and metaphor detection (Tsvetkov et al., 2013). An alternative path to semantic interpretation follows the distributional hypothesis (Harris, 1954). Recently, word vector representations learned with neural-network based language models have contributed to state-of-the-art results on various linguistic tasks (Bordes et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015). In this work, we present a novel approach for incorporating the supersense information into the word embedding space and propose a new methodology for utilizing these to label the text with supersenses and to exploit these joint word and supersense embeddings in a range of applied text classification tasks. Our contributions in this work include the following: • We are the first to provide a joint wordand supersense-embedding model, which we make publicly available1 for the research community. This provides an insight into the word and supersense"
P16-1191,D14-1181,0,0.00411228,"section, we demonstrate an alternative, deep learning approach, in which we process the original text in parallel to the supersense information. The model can then flexibly learn the usefulness of provided input. We demonstrate that the model extended with supersense embeddings outperforms the same model using only word-based features on a range of classification tasks. 6.1 Experimental Setup Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) are state-of-the-art semantic composition models for a variety of text classification tasks (Kim, 2014; Li et al., 2015; Johnson and Zhang, 2014). Recently, their combinations have been proposed, achieving an unprecedented performance (Sainath et al., 2015). We extend the CNN-LSTM approach from the publicly available 2034 convolutional and LSTM layers in a similar fashion. Afterwards, we concatenate all LSTM outputs and feed them into a standard fully connected neural network layer, followed by the sigmoid for the binary output. The following subsections discuss our results on a range of classification tasks: subjectivity prediction, sentiment polarity classification and metaphor detection. No"
P16-1191,P13-1116,0,0.0267982,"This is why supersenses, the coarse-grained word labels based on WordNet’s (Fellbaum, 1998) lexicographer files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL, PERSON or FEELING and 15 labels for verbs, such as COMMUNICATION, MOTION or COGNITION . Usage of supersense labels has been shown to improve dependency parsing (Agirre et al., 2011), named entity recognition (Marrero et al., 2009; R¨ud et al., 2011), non-factoid question answering (Surdeanu et al., 2011), question generation (Heilman, 2011), semantic role labeling (Laparra and Rigau, 2013), personality profiling (Flekova and Gurevych, 2015), semantic similarity (Severyn et al., 2013) and metaphor detection (Tsvetkov et al., 2013). An alternative path to semantic interpretation follows the distributional hypothesis (Harris, 1954). Recently, word vector representations learned with neural-network based language models have contributed to state-of-the-art results on various linguistic tasks (Bordes et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015). In this work, we present a novel approach for incorporating the supersense information into the word em"
P16-1191,Q15-1016,0,0.04957,"on answering (Surdeanu et al., 2011), question generation (Heilman, 2011), semantic role labeling (Laparra and Rigau, 2013), personality profiling (Flekova and Gurevych, 2015), semantic similarity (Severyn et al., 2013) and metaphor detection (Tsvetkov et al., 2013). An alternative path to semantic interpretation follows the distributional hypothesis (Harris, 1954). Recently, word vector representations learned with neural-network based language models have contributed to state-of-the-art results on various linguistic tasks (Bordes et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015). In this work, we present a novel approach for incorporating the supersense information into the word embedding space and propose a new methodology for utilizing these to label the text with supersenses and to exploit these joint word and supersense embeddings in a range of applied text classification tasks. Our contributions in this work include the following: • We are the first to provide a joint wordand supersense-embedding model, which we make publicly available1 for the research community. This provides an insight into the word and supersense positions in the vector space 1 https://githu"
P16-1191,D15-1200,0,0.0553654,"ersenses achieve higher performance than those trained solely on the same text without supersenses on 4 out of 5 tasks (Table 5). In addition, the explicit supersense information could be further exploited, similarly to previous sense embedding works (Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014). Furthermore, note that while we report the performance of our embeddings on the word similarity tasks for completeness, there has been a substantial discussion on seeking alternative ways to quantify embedding quality with the focus on their purpose in downstream applications (Li and Jurafsky, 2015; Faruqui et al., 2016). Therefore, in the remainder of this paper we explore the usefulness of supersense embeddings in text classification tasks. 5 Table 4: Accuracy and standard error on analogy tasks. Tasks related to noun supersense distinctions show the tendency to improve, while syntax-related information is pushed to the background. In most cases, however, the difference is not significant. MEN 73.18 74.26 Building a Supersense Tagger The task of predicting supersenses has recently regained its popularity (Johannsen et al., 2014; Schneider and Smith, 2015), since supersenses provide di"
P16-1191,H94-1046,0,0.0620783,"ork We have presented a novel joint embedding set of words and supersenses, which provides a new insight into the word and supersense positions in the vector space. We demonstrated the utility of these embeddings for predicting supersenses and manifested that the supersense enrichment can lead to a significant improvement in a range of downstream classification tasks, using our embeddings in a neural network model. The outcomes of this work are available to the research community.11 . In follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as SemCor (Miller et al., 1994) and STREUSLE (Schneider and Smith, 2015) to examine the impact of the corpus choice in detail and extend the training data beyond WordNet vocabulary. Moreover, the coarse semantic categorization contained in supersenses was shown to be preserved in translation (Schneider et al., 2013), making them a perfect candidate for a multilingual adaptation of the vector space, e.g. extending Faruqui and Dyer (2014). Acknowledgments Unlike previous research on supersenses, our work is not based on a manually produced gold stanThis work has been supported by the Volkswagen Foundation as part of the Licht"
P16-1191,P04-1035,0,0.00879648,"Missing"
P16-1191,P05-1015,0,0.0369808,"car nouncognition , while the nounlocation verbstative big enough for a nounartifact nounartifact to verbmotion through – if nounperson have n’t verbcommunication them all up . the nounperson verbstative no less nounstate to noungroup than the nouncommunication nounperson . a very slow , uneventful nounact around a pretty tattered old nounartifact . the nouncognition verbstative wholly unconvincing and the nouncommunication verbstative a truly annoying nounattribute . Table 8: Example of documents classified incorrectly with word embeddings and correctly with word and supersense embeddings on Pang and Lee (2005) movie review data. concepts such as GROUP, LOCATION , TIME and PERSON appear somewhat more frequently in positive reviews while certain verb supersenses such as PERCEPTION , SOCIAL and COMMUNICATION are more frequent in the negative ones. On the other hand, the supersense tagging introduces additional errors too - for example the director’s cut is persistently classified into FOOD. Table 8 shows an example of positive and negative reviews which were consistently (5x in repeated experiments with different random seeds) classified incorrectly with word embeddings and classified correctly with s"
P16-1191,D14-1162,0,0.108022,"2011), non-factoid question answering (Surdeanu et al., 2011), question generation (Heilman, 2011), semantic role labeling (Laparra and Rigau, 2013), personality profiling (Flekova and Gurevych, 2015), semantic similarity (Severyn et al., 2013) and metaphor detection (Tsvetkov et al., 2013). An alternative path to semantic interpretation follows the distributional hypothesis (Harris, 1954). Recently, word vector representations learned with neural-network based language models have contributed to state-of-the-art results on various linguistic tasks (Bordes et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015). In this work, we present a novel approach for incorporating the supersense information into the word embedding space and propose a new methodology for utilizing these to label the text with supersenses and to exploit these joint word and supersense embeddings in a range of applied text classification tasks. Our contributions in this work include the following: • We are the first to provide a joint wordand supersense-embedding model, which we make publicly available1 for the research community. This provides an insight into the word and supersense positions in the vector s"
P16-1191,picca-etal-2008-supersense,0,0.0396157,"Missing"
P16-1191,D14-1104,0,0.015428,"vigli (2009) or Pal and Saha (2015) for an overview). Words in training and evaluation data are usually annotated with senses taken from a particular lexical semantic resource, most commonly WordNet (Miller, 1995). However, WordNet has been criticized to provide too fine-grained distinctions for end level applications. e.g. in machine translation or information retrieval (Izquierdo et al., 2009). Although some researchers report an improvement in sentiment prediction using WSD (Rentoumi et al., 2009; Akkaya et al., 2011; Sumanth and Inkpen, 2015), the publication bias toward positive results (Plank et al., 2014) impedes the comparison to experiments with the opposite conclusion, and the contribution of WSD to downstream document classification tasks remains “mostly speculative”(Ciaramita and Altun, 2006), which can be attributed to the too subtle sense distinctions (Navigli, 2009). This is why supersenses, the coarse-grained word labels based on WordNet’s (Fellbaum, 1998) lexicographer files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL, PERSON or FEELING and 15 labels for verbs, such as COMMUNICATION, MOTION or COGNITION . Usag"
P16-1191,W95-0107,0,0.154264,"of predicting supersenses has recently regained its popularity (Johannsen et al., 2014; Schneider and Smith, 2015), since supersenses provide disambiguating information, useful for numerous downstream NLP tasks, without the need of tedious fine-grained WSD. Exploiting our joint embeddings, we build a deep neural network model to predict supersenses on the Twitter supersense corpus created by Johannsen et al. (2014), based on the Twitter NER task (Ritter et al., 2011), using the same training data as the authors. 45 The datasets follow the token-level annotation which combines the B-I-O flags (Ramshaw and Marcus, 1995) with the supersense class labels to represent the multiword expression segmentation and supersense labeling in a sentence. 5.1 Experimental Setup We implement a window-based approach with a multi-channel multi-layer perceptron model using 4 https://github.com/kutschkem/ SmithHeilmann_fork/tree/master/ MIRATagger/data 5 https://github.com/coastalcph/ supersense-data-twitter 2033 the Theano framework (Bastien et al., 2012). With a sliding window of size 5 for the sequence learning setup we extract for each word the following seven feature vectors: speech information, followed by the supersense"
P16-1191,R09-1067,0,0.0198917,"community. The word sense disambiguation (WSD) task has therefore received a substantial amount of attention (see Navigli (2009) or Pal and Saha (2015) for an overview). Words in training and evaluation data are usually annotated with senses taken from a particular lexical semantic resource, most commonly WordNet (Miller, 1995). However, WordNet has been criticized to provide too fine-grained distinctions for end level applications. e.g. in machine translation or information retrieval (Izquierdo et al., 2009). Although some researchers report an improvement in sentiment prediction using WSD (Rentoumi et al., 2009; Akkaya et al., 2011; Sumanth and Inkpen, 2015), the publication bias toward positive results (Plank et al., 2014) impedes the comparison to experiments with the opposite conclusion, and the contribution of WSD to downstream document classification tasks remains “mostly speculative”(Ciaramita and Altun, 2006), which can be attributed to the too subtle sense distinctions (Navigli, 2009). This is why supersenses, the coarse-grained word labels based on WordNet’s (Fellbaum, 1998) lexicographer files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for"
P16-1191,D11-1141,0,0.0227446,"Missing"
P16-1191,P15-1173,0,0.022734,"Missing"
P16-1191,P11-1097,0,0.256637,"Missing"
P16-1191,N15-1177,0,0.0737286,"urpose in downstream applications (Li and Jurafsky, 2015; Faruqui et al., 2016). Therefore, in the remainder of this paper we explore the usefulness of supersense embeddings in text classification tasks. 5 Table 4: Accuracy and standard error on analogy tasks. Tasks related to noun supersense distinctions show the tendency to improve, while syntax-related information is pushed to the background. In most cases, however, the difference is not significant. MEN 73.18 74.26 Building a Supersense Tagger The task of predicting supersenses has recently regained its popularity (Johannsen et al., 2014; Schneider and Smith, 2015), since supersenses provide disambiguating information, useful for numerous downstream NLP tasks, without the need of tedious fine-grained WSD. Exploiting our joint embeddings, we build a deep neural network model to predict supersenses on the Twitter supersense corpus created by Johannsen et al. (2014), based on the Twitter NER task (Ritter et al., 2011), using the same training data as the authors. 45 The datasets follow the token-level annotation which combines the B-I-O flags (Ramshaw and Marcus, 1995) with the supersense class labels to represent the multiword expression segmentation and"
P16-1191,P12-2050,0,0.0106965,"ppointed, established, elected, joined, assisted led, succeeded, encouraged, initiated, organized included, held, includes, featured, served, represented, referred, holds, continued, related glow, emitted, ignited, flare, emitting smoke, fumes, sunlight, lit, darkened Table 2: Top 10 most similar word embeddings for verb supersense vectors 2031 4.2 Figure 1: Verb supersense embeddings visualized in the vector space (t-SNE) Noun Supersenses Table 3 displays the most similar word embeddings for noun supersenses. In accordance with previous work on suppersense tagging (Ciaramita and Altun, 2006; Schneider et al., 2012; Johannsen et al., 2014), the assignments of more specific supersenses such as FOOD, PLANT, TIME or PERSON are in general more plausible than those for abstract concepts such as ACT, ARTIFACT or COG NITION . The same is visible in Figure 2, where these supersense embeddings are more central, with closer neighbors. In contrast to the observations by Schneider et al. (2012) and Johannsen et al. (2014), the COMMUNICATION supersense appears well defined, likely due to the character of Wikipedia. Furthermore, using a large corpus such as Wikipedia conveniently reduces the current need of lemmatiza"
P16-1191,P13-2125,0,0.098522,"rapher files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL, PERSON or FEELING and 15 labels for verbs, such as COMMUNICATION, MOTION or COGNITION . Usage of supersense labels has been shown to improve dependency parsing (Agirre et al., 2011), named entity recognition (Marrero et al., 2009; R¨ud et al., 2011), non-factoid question answering (Surdeanu et al., 2011), question generation (Heilman, 2011), semantic role labeling (Laparra and Rigau, 2013), personality profiling (Flekova and Gurevych, 2015), semantic similarity (Severyn et al., 2013) and metaphor detection (Tsvetkov et al., 2013). An alternative path to semantic interpretation follows the distributional hypothesis (Harris, 1954). Recently, word vector representations learned with neural-network based language models have contributed to state-of-the-art results on various linguistic tasks (Bordes et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015). In this work, we present a novel approach for incorporating the supersense information into the word embedding space and propose a new methodology for utilizing these to label the text with supersens"
P16-1191,D11-1014,0,0.1569,"Missing"
P16-1191,D12-1110,0,0.0821565,"Missing"
P16-1191,W15-2916,0,0.0136032,"SD) task has therefore received a substantial amount of attention (see Navigli (2009) or Pal and Saha (2015) for an overview). Words in training and evaluation data are usually annotated with senses taken from a particular lexical semantic resource, most commonly WordNet (Miller, 1995). However, WordNet has been criticized to provide too fine-grained distinctions for end level applications. e.g. in machine translation or information retrieval (Izquierdo et al., 2009). Although some researchers report an improvement in sentiment prediction using WSD (Rentoumi et al., 2009; Akkaya et al., 2011; Sumanth and Inkpen, 2015), the publication bias toward positive results (Plank et al., 2014) impedes the comparison to experiments with the opposite conclusion, and the contribution of WSD to downstream document classification tasks remains “mostly speculative”(Ciaramita and Altun, 2006), which can be attributed to the too subtle sense distinctions (Navigli, 2009). This is why supersenses, the coarse-grained word labels based on WordNet’s (Fellbaum, 1998) lexicographer files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL, PERSON or FEELING and 15"
P16-1191,J11-2003,0,0.0142651,"2006), which can be attributed to the too subtle sense distinctions (Navigli, 2009). This is why supersenses, the coarse-grained word labels based on WordNet’s (Fellbaum, 1998) lexicographer files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL, PERSON or FEELING and 15 labels for verbs, such as COMMUNICATION, MOTION or COGNITION . Usage of supersense labels has been shown to improve dependency parsing (Agirre et al., 2011), named entity recognition (Marrero et al., 2009; R¨ud et al., 2011), non-factoid question answering (Surdeanu et al., 2011), question generation (Heilman, 2011), semantic role labeling (Laparra and Rigau, 2013), personality profiling (Flekova and Gurevych, 2015), semantic similarity (Severyn et al., 2013) and metaphor detection (Tsvetkov et al., 2013). An alternative path to semantic interpretation follows the distributional hypothesis (Harris, 1954). Recently, word vector representations learned with neural-network based language models have contributed to state-of-the-art results on various linguistic tasks (Bordes et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015). In this work, we"
P16-1191,W13-0906,0,0.0133988,"or text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL, PERSON or FEELING and 15 labels for verbs, such as COMMUNICATION, MOTION or COGNITION . Usage of supersense labels has been shown to improve dependency parsing (Agirre et al., 2011), named entity recognition (Marrero et al., 2009; R¨ud et al., 2011), non-factoid question answering (Surdeanu et al., 2011), question generation (Heilman, 2011), semantic role labeling (Laparra and Rigau, 2013), personality profiling (Flekova and Gurevych, 2015), semantic similarity (Severyn et al., 2013) and metaphor detection (Tsvetkov et al., 2013). An alternative path to semantic interpretation follows the distributional hypothesis (Harris, 1954). Recently, word vector representations learned with neural-network based language models have contributed to state-of-the-art results on various linguistic tasks (Bordes et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015). In this work, we present a novel approach for incorporating the supersense information into the word embedding space and propose a new methodology for utilizing these to label the text with supersenses and to exploit these joint word and supersen"
P16-1191,tsvetkov-etal-2014-augmenting-english,0,0.0180747,"n be answered with word vector operations (Paris is to France as Athens are to...?). The questions are grouped into 13 categories. Table 4 presents our results. Word vectors trained in the SUPER setup achieve better results on groups related to entities, e.g. Family Relations and Citizen to State questions, where the PERSON and LOCATION supersenses can provide additional information to reduce noise. At the same time, performance on questions such as Opposites or Plurals drops, as this information is pushed to the background. Enriching our data with the recently proposed adjective supersenses (Tsvetkov et al., 2014) could be of interest for these categories. Group/Vectors: Capitals - common Capitals - world City in state Nationality to state Family relations Opposites Plurals Comparatives Superlatives Adjective to adverb Present to participle Present to past 3rd person verbs Total WORDS 91.1 87.6 65.2 94.5 93.0 56.7 89.4 90.6 79.4 20.2 64.2 60.0 84.3 75.0 SUPER 94.7±0.99 89.5±0.69 65.7±1.03 95.2±0.58 94.4±1.28 54.6±3.21 86.4±1.08 90.4±0.85 79.6±1.83 22.2±1.53 64.6±1.57 59.2±1.30 82.1±1.44 76.0±0.28 Without explicitly exploiting the sense infromation, we compare the performance of our texttrained (WORDS)"
P16-1191,D15-1243,0,0.0212652,"Missing"
P16-1191,D11-1063,0,0.0325973,"Missing"
P16-1191,P14-2089,0,0.0521181,"ese embeddings in tasks such as WSD, knowledge base unification or semantic similarity, the contribution of such vectors to downstream document classification problems can be challenging, given the fine granularity of the WordNet senses (cf. the discussion in Navigli (2009)). As discussed above, supersenses have been shown to be better suited for carrying the relevant amount of semantic information. An alternative approach focuses on altering the objective of the learning mechanism to capture relational and similarity information from knowledge bases (Bordes et al., 2011; Bordes et al., 2012; Yu and Dredze, 2014; Bian et al., 2014; Faruqui and Dyer, 2014; Goikoetxea et al., 2015). While, in principle, supersenses could be seen as a relation between a word and its hypernym, to our knowledge they have not been explicitly employed in these works. Moreover, an important advantage of our explicit supersense embeddings compared to the retrained vectors is their direct interpretability. 2.2 Supersense Tagging Supersenses, also known as lexicographer files or semantic fields, were originally used to organize lexical-semantic resources (Fellbaum, 1990). The supersense tagging task was introduced by Ciaramita"
P16-1191,N16-1178,0,0.0338695,"Missing"
P16-1191,N13-1076,0,\N,Missing
P16-1191,N15-1165,0,\N,Missing
P16-1191,atserias-etal-2008-semantically,0,\N,Missing
P16-1191,N15-1011,0,\N,Missing
P16-1207,W06-1623,0,0.345871,"tions (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) are based on the original TimeBank annotations, but tried to improve the coverage and added some further temporal links for mentions in the same sentence. The MEANtime corpus (van Erp et al., 2015) applied a sparse annotation and only temporal links between events and temporal expressions in the same and in succeeding sentences were annotated. The MEANtime corpus distinguishes between main event mentions and subordinated event mentions, and the focus for TLINKs was on main events. More dense annotations were applied by Bramsen et al. (2006), Kolomiyets et al. (2012), Do et al. (2012) and by Cassidy et al. (2014). While Bramsen et al., Kolomiyets et al., and Do et al. only annotated some temporal links, Cassidy et al. annotated all Event-Event, Event-Time, and TimeTime pairs in the same sentence as well as in directly succeeding sentences leading to the densest annotation for the TimeBank Corpus. 2196 A drawback of the previous annotation works is the limitation that only links between expressions in the same or in succeeding sentences are annotated. In case the important temporal expression, that defines when the event occurred,"
P16-1207,P14-2082,0,0.821405,") are based on the original TimeBank annotations, but tried to improve the coverage and added some further temporal links for mentions in the same sentence. The MEANtime corpus (van Erp et al., 2015) applied a sparse annotation and only temporal links between events and temporal expressions in the same and in succeeding sentences were annotated. The MEANtime corpus distinguishes between main event mentions and subordinated event mentions, and the focus for TLINKs was on main events. More dense annotations were applied by Bramsen et al. (2006), Kolomiyets et al. (2012), Do et al. (2012) and by Cassidy et al. (2014). While Bramsen et al., Kolomiyets et al., and Do et al. only annotated some temporal links, Cassidy et al. annotated all Event-Event, Event-Time, and TimeTime pairs in the same sentence as well as in directly succeeding sentences leading to the densest annotation for the TimeBank Corpus. 2196 A drawback of the previous annotation works is the limitation that only links between expressions in the same or in succeeding sentences are annotated. In case the important temporal expression, that defines when the event occurred, is more than one sentence away, the TLINK will not be annotated. Consequ"
P16-1207,D12-1062,0,0.209247,"10; UzZaman et al., 2013) are based on the original TimeBank annotations, but tried to improve the coverage and added some further temporal links for mentions in the same sentence. The MEANtime corpus (van Erp et al., 2015) applied a sparse annotation and only temporal links between events and temporal expressions in the same and in succeeding sentences were annotated. The MEANtime corpus distinguishes between main event mentions and subordinated event mentions, and the focus for TLINKs was on main events. More dense annotations were applied by Bramsen et al. (2006), Kolomiyets et al. (2012), Do et al. (2012) and by Cassidy et al. (2014). While Bramsen et al., Kolomiyets et al., and Do et al. only annotated some temporal links, Cassidy et al. annotated all Event-Event, Event-Time, and TimeTime pairs in the same sentence as well as in directly succeeding sentences leading to the densest annotation for the TimeBank Corpus. 2196 A drawback of the previous annotation works is the limitation that only links between expressions in the same or in succeeding sentences are annotated. In case the important temporal expression, that defines when the event occurred, is more than one sentence away, the TLINK w"
P16-1207,P12-1010,0,0.497287,"2007; Verhagen et al., 2010; UzZaman et al., 2013) are based on the original TimeBank annotations, but tried to improve the coverage and added some further temporal links for mentions in the same sentence. The MEANtime corpus (van Erp et al., 2015) applied a sparse annotation and only temporal links between events and temporal expressions in the same and in succeeding sentences were annotated. The MEANtime corpus distinguishes between main event mentions and subordinated event mentions, and the focus for TLINKs was on main events. More dense annotations were applied by Bramsen et al. (2006), Kolomiyets et al. (2012), Do et al. (2012) and by Cassidy et al. (2014). While Bramsen et al., Kolomiyets et al., and Do et al. only annotated some temporal links, Cassidy et al. annotated all Event-Event, Event-Time, and TimeTime pairs in the same sentence as well as in directly succeeding sentences leading to the densest annotation for the TimeBank Corpus. 2196 A drawback of the previous annotation works is the limitation that only links between expressions in the same or in succeeding sentences are annotated. In case the important temporal expression, that defines when the event occurred, is more than one sentence"
P16-1207,S07-1014,0,0.212286,"cted. The TimeBank Corpus (Pustejovsky et al., 2003) is a widely used corpus using the TimeML specifications (Saur´ı et al., 2004) for the annotations of event mentions and temporal expressions. In order to anchor events in time, the TimeBank Corpus uses the concept of temporal links (TLINKs) that were introduced by Setzer (2001). A TLINK states the temporal relation between two events or an event and a time expression. For example, an event could happen before, simultaneous, or after a certain expression of time. The TimeBank Corpus served as dataset for the shared tasks TempEval-1, 2 and 3 (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). In this paper we describe a new approach to anchor every event in time. Instead of using temporal links between events and temporal expressions, we consider the event time as an argument of the event mention. The annotators are asked to write down the date when an event happened in a normalized format for every event mention. The annotation effort is for this reason identical 2195 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2195–2204, c Berlin, Germany, August 7-12, 2016. 2016 Association for Com"
P16-1207,P13-4001,1,0.883787,"Missing"
P16-1207,S13-2001,0,0.334158,"2003) is a widely used corpus using the TimeML specifications (Saur´ı et al., 2004) for the annotations of event mentions and temporal expressions. In order to anchor events in time, the TimeBank Corpus uses the concept of temporal links (TLINKs) that were introduced by Setzer (2001). A TLINK states the temporal relation between two events or an event and a time expression. For example, an event could happen before, simultaneous, or after a certain expression of time. The TimeBank Corpus served as dataset for the shared tasks TempEval-1, 2 and 3 (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). In this paper we describe a new approach to anchor every event in time. Instead of using temporal links between events and temporal expressions, we consider the event time as an argument of the event mention. The annotators are asked to write down the date when an event happened in a normalized format for every event mention. The annotation effort is for this reason identical 2195 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2195–2204, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to the number of event"
P16-1207,Q14-1022,0,\N,Missing
P16-4017,W15-4638,0,0.0517007,"Missing"
P16-4017,nakano-etal-2010-construction,0,0.625331,"2 the assessment of written summaries, computerassisted summarization, or the manual construction of summarization corpora. The Pyramid annotation tool (Nenkova and Passonneau, 2004) and the tool2 used for the MultiLing shared tasks (Giannakopoulos et al., 2015) are limited to comparing and scoring summaries, but do not provide any writing functionality. Or˘asan et al.’s (2003) CAST tool assists users with summarizing a document based on the output of an automatic summarization algorithm. However, their tool is restricted to single-document summarization. The works by Ulrich et al. (2008) and Nakano et al. (2010) are most closely related to ours, since they discuss the creation of multi-document summarization corpora. Unfortunately, their proposed annotation tools are not available as open-source software and thus cannot be reused. In addition to that, they do not record user–system interactions, which we consider important for next-generation automatic summarization methods. Related work There is a vast number of general-purpose tools for annotating corpora, for example, WebAnno (Yimam et al., 2013), Anafora (Chen and Styler, 2013), CS NIPER (Eckart de Castilho et al., 2012), and the UAM CorpusTool ("
P16-4017,N04-1019,0,0.0584855,"viding access to process data and intermediate results. We designed an initial, multi-step workflow implemented in MDSWriter. However, our tool is flexible to deviate from this initial setup allowing a wide range of summary creation workflows, including singledocument summarization, and even other complex annotation tasks. We make MDSWriter available as open-source software, including our exemplary annotation guidelines and a video tutorial.1 2 the assessment of written summaries, computerassisted summarization, or the manual construction of summarization corpora. The Pyramid annotation tool (Nenkova and Passonneau, 2004) and the tool2 used for the MultiLing shared tasks (Giannakopoulos et al., 2015) are limited to comparing and scoring summaries, but do not provide any writing functionality. Or˘asan et al.’s (2003) CAST tool assists users with summarizing a document based on the output of an automatic summarization algorithm. However, their tool is restricted to single-document summarization. The works by Ulrich et al. (2008) and Nakano et al. (2010) are most closely related to ours, since they discuss the creation of multi-document summarization corpora. Unfortunately, their proposed annotation tools are not"
P16-4017,P08-4004,0,0.0814999,"Missing"
P16-4017,E03-1066,0,0.138184,"Missing"
P16-4017,P13-1119,0,0.0691022,"Missing"
P16-4017,N13-3004,0,0.0221319,"to single-document summarization. The works by Ulrich et al. (2008) and Nakano et al. (2010) are most closely related to ours, since they discuss the creation of multi-document summarization corpora. Unfortunately, their proposed annotation tools are not available as open-source software and thus cannot be reused. In addition to that, they do not record user–system interactions, which we consider important for next-generation automatic summarization methods. Related work There is a vast number of general-purpose tools for annotating corpora, for example, WebAnno (Yimam et al., 2013), Anafora (Chen and Styler, 2013), CS NIPER (Eckart de Castilho et al., 2012), and the UAM CorpusTool (O’Donnell, 2008). However, neither of these tools is suitable for tasks that require access to multiple documents at the same time, as they are focused on annotating linguistic phenomena within single documents or search results with limited contexts. Tools for cross-document annotation tasks are so far limited to event and entity co-reference, e.g., C ROMER (Girardi et al., 2014). These tools are, however, not directly applicable to the task of multi-document summarization. In fact, all tools discussed so far lack a definit"
P16-4017,P13-4001,1,0.8931,"Missing"
P16-4017,P12-3015,1,0.881261,"Missing"
P16-4017,girardi-etal-2014-cromer,0,\N,Missing
P17-1002,E17-1005,0,0.16357,"n Eger†‡ , Johannes Daxenberger† , Iryna Gurevych†‡ † Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technische Universitt Darmstadt ‡ Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information http://www.ukp.tu-darmstadt.de Abstract While different research has addressed different subsets of the AM problem (see below), the ultimate goal is to solve all of them, starting from unannotated plain text. Two recent approaches to this end-to-end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017). Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc. Besides their pipeline architecture the approaches also have in common that they heavily rely on hand-crafted features. Hand-crafted features pose a problem because AM is to some degree an “arbitrary” problem in that the notion of “argument” critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab"
P17-1002,D12-1133,0,0.044595,"Missing"
P17-1002,Q16-1023,0,0.0212672,"Missing"
P17-1002,W15-0501,1,0.0288887,"rprising that ‘discourse parsing’ (Muller et al., 2012) has been suggested for AM (Peldszus and Stede, 2015). What distinguishes our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of co"
P17-1002,D14-1082,0,0.00547069,"a structured prediction problem in which the goal is learning a scoring function over dependency trees such that correct trees are scored above all others. Traditional dependency parsers used handcrafted feature functions that look at “core” elements such as “word on top of the stack”, “POS of word on top of the stack”, and conjunctions of core features such as “word is X and POS is Y” (see McDonald et al. (2005)). Most neural parsers have not entirely abandoned feature engineering. Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations. Kiperwasser and Goldberg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer. In contrast, Dyer et al. (2015)’s neural parser associates each stack with a “stack LSTM” that encodes their contents. Actions are chosen based on the stack LSTM representations of the stacks, and no more feature engineering is necessary. Moreover, their parser has thus access to any part of the input, its history and stack contents. AM as Dependency Parsing: To frame a problem"
P17-1002,P15-1033,0,0.00538485,"as “word on top of the stack”, “POS of word on top of the stack”, and conjunctions of core features such as “word is X and POS is Y” (see McDonald et al. (2005)). Most neural parsers have not entirely abandoned feature engineering. Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations. Kiperwasser and Goldberg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer. In contrast, Dyer et al. (2015)’s neural parser associates each stack with a “stack LSTM” that encodes their contents. Actions are chosen based on the stack LSTM representations of the stacks, and no more feature engineering is necessary. Moreover, their parser has thus access to any part of the input, its history and stack contents. AM as Dependency Parsing: To frame a problem as a dependency parsing problem, each instance of the problem must be encoded as a directed tree, where tokens have heads, which in turn are labeled. For end-to-end AM, we propose the framing illustrated in Figure 3. We highlight two design decisions"
P17-1002,C14-1141,0,0.195964,"the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types. Second, we frame the problem as a sequence tagging problem. This is a natural choice especially for"
P17-1002,P16-1101,0,0.0110928,"Missing"
P17-1002,W13-2707,0,0.319706,"do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types. Second, we frame th"
P17-1002,H05-1066,0,0.107517,"Missing"
P17-1002,P16-1105,0,0.0654382,"e AM as a multi-task (tagging) problem (Caruana, 1997; Collobert and Weston, 2008). We experiment with subtasks of AM—e.g., component identification—as auxiliary tasks and investigate whether this improves performance on the AM problem. Adding such subtasks can be seen as analogous to de-coupling, e.g., component identification from the full AM problem. 3 Data We use the dataset of persuasive essays (PE) from Stab and Gurevych (2017), which contains student essays written in response to controversial topics such as “competition or cooperation—which is better?” Fourth, we evaluate the model of Miwa and Bansal (2016) that combines sequential (entity) and tree structure (relation) information and is in principle applicable to any problem where the aim is to extract entities and their relations. As such, this model makes fewer assumptions than our dependency parsing and tagging approaches. Essays Paragraphs Tokens Train Test 322 1786 118648 80 449 29538 Table 1: Corpus statistics The contributions of this paper are as follows. (1) We present the first neural end-to-end solutions to computational AM. (2) We show that several of them perform better than the state-of-theart joint ILP model. (3) We show that a"
P17-1002,N16-1164,0,0.54771,"or Computational Argumentation Mining Steffen Eger†‡ , Johannes Daxenberger† , Iryna Gurevych†‡ † Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technische Universitt Darmstadt ‡ Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information http://www.ukp.tu-darmstadt.de Abstract While different research has addressed different subsets of the AM problem (see below), the ultimate goal is to solve all of them, starting from unannotated plain text. Two recent approaches to this end-to-end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017). Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc. Besides their pipeline architecture the approaches also have in common that they heavily rely on hand-crafted features. Hand-crafted features pose a problem because AM is to some degree an “arbitrary” problem in that the notion of “argument” critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habe"
P17-1002,C12-1115,0,0.0262446,"ages 11–22 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1002 proposed for systems that operate on the coarser component level and that (4) a standard neural sequence tagging model that encodes distance information between components performs robustly in different environments. Finally, (5) we show that a multi-task learning setup where natural subtasks of the full AM problem are added as auxiliary tasks improves performance.1 or closely resemble them (see §3). Hence, it is not surprising that ‘discourse parsing’ (Muller et al., 2012) has been suggested for AM (Peldszus and Stede, 2015). What distinguishes our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educati"
P17-1002,reed-etal-2008-language,0,0.100994,"end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017). Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc. Besides their pipeline architecture the approaches also have in common that they heavily rely on hand-crafted features. Hand-crafted features pose a problem because AM is to some degree an “arbitrary” problem in that the notion of “argument” critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab and Gurevych, 2017). Accordingly, datasets typically differ with respect to their annotation of (often rather complex) argument structure. Thus, feature sets would have to be manually adapted to and designed for each new sample of data, a challenging task. The same critique applies to the designing of ILP constraints. Moreover, from a machine learning perspective, pipeline approaches are problematic because they solve subtasks independently and thus lead to error propagation rather than exploiting interrelationships between variables."
P17-1002,D13-1032,0,0.0236992,"Missing"
P17-1002,D15-1050,0,0.102406,"rgumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types. Second, we frame the problem as a sequence tagging problem. This is a natural choice especially for component identificati"
P17-1002,D15-1110,0,0.224931,"4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1002 proposed for systems that operate on the coarser component level and that (4) a standard neural sequence tagging model that encodes distance information between components performs robustly in different environments. Finally, (5) we show that a multi-task learning setup where natural subtasks of the full AM problem are added as auxiliary tasks improves performance.1 or closely resemble them (see §3). Hence, it is not surprising that ‘discourse parsing’ (Muller et al., 2012) has been suggested for AM (Peldszus and Stede, 2015). What distinguishes our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on wr"
P17-1002,P16-2038,0,0.0353023,"Missing"
P17-1002,C16-1148,0,0.0193574,"e token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-l"
P17-1002,J17-3005,1,0.537815,"Missing"
P17-1002,P13-1161,0,0.0642591,"Missing"
P17-1002,N16-1168,0,0.0294094,"es our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which an"
P17-1002,E17-1063,0,0.0126958,"Missing"
P17-1002,N03-1031,0,\N,Missing
P17-1002,J17-1004,1,\N,Missing
P17-1002,D15-1253,0,\N,Missing
P17-1002,D14-1162,0,\N,Missing
P17-1002,D15-1255,1,\N,Missing
P17-1002,P15-1053,0,\N,Missing
P17-1002,N15-1142,0,\N,Missing
P17-2039,W16-2808,1,0.847172,"cy, but arguments are judged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004). Wachsmuth et al. (2017a) point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. B"
P17-2039,P12-2041,0,0.0279224,"ying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors al"
P17-2039,D16-1129,1,0.101498,"better to give then to receive. It’s better to give other people you’re hand out in help then you holding your own hand.” Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other. 1 In the study of Habernal and Gurevych (2016b), annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from the"
P17-2039,P16-1150,1,0.106561,"better to give then to receive. It’s better to give other people you’re hand out in help then you holding your own hand.” Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other. 1 In the study of Habernal and Gurevych (2016b), annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from the"
P17-2039,N13-1132,0,0.0340534,"y dimensions with scores from 1 to 3 (or choose “cannot judge”). Each argument was rated 10 times at an offered price of $0.10 for each rating (102 annotators in total). Given the crowd ratings, we then performed two comparisons as detailed in the following. 4.2 Agreement of the Crowd with Experts First, we checked to what extent lay annotators and experts agree in terms of Krippendorff’s α. On one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of Wachsmuth et al. (2017a). On the other hand, we estimated a reliable rating from the crowd ratings using MACE (Hovy et al., 2013) and compared it to the experts. 253 (a) Crowd / Expert Quality Dimension Cog LA LR LS Eff Cre Emo Cla App Arr Rea GA GR GS OQ Cogency Local acceptability Local relevance Local sufficiency Effectiveness Credibility Emotional appeal Clarity Appropriateness Arrangement Reasonableness Global acceptability Global relevance Global sufficiency Overall quality (b) Crowd 1 / 2 / Expert (c) Crowd 1 / Expert (d) Crowd 2 / Expert Mean MACE Mean MACE Mean MACE Mean MACE .27 .49 .42 .18 .13 .41 .45 .42 .54 .53 .33 .54 .44 –.17 .43 .38 .35 .39 .31 .31 .27 .23 .28 .26 .30 .40 .40 .31 .19 .43 .24 .37 .33 .21"
P17-2039,P15-1053,0,0.080264,"dged specifically by their reasonableness for achieving agreement (van Eemeren and Grootendorst, 2004). Wachsmuth et al. (2017a) point out that dialectical builds on rhetorical, and rhetorical builds on logical quality. They derive a unifying taxonomy from the major theories, decomposing quality hierarchically into cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that r"
P17-2039,D15-1050,0,0.125974,"ng theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argument has acceptable premises that are relevant to its conclusion and sufficient to draw the conclusion (Johnson and Blair, 2006). Practitioners object that such quality dimensions are hard to assess for real-life arguments (Habernal and Gurevych, 2016b). Moreover, the normative nature of theory suggests absolute quality ratings, but in practice it seems much easier to state which argument is more convinc"
P17-2039,E17-1017,1,0.313544,"nnotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argu"
P17-2039,E17-1105,1,0.205942,"nnotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (Wachsmuth et al., 2017a) and one with 17 reasons for quality differences phrased spontaneously in practice (Habernal and Gurevych, 2016a). In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). Introduction The assessment of argumentation quality is critical for any application built upon argument mining, such as debating technologies (Rinott et al., 2015). However, research still disagrees on whether quality should be assessed from a theoretical or from a practical viewpoint (Allwood, 2016). Theory states, among other things, that a cogent argu"
P17-2039,P16-2032,0,0.0789411,"o cogency, effectiveness, reasonableness, and subdimensions. Table 1 lists all 15 dimensions 2.2 Practical Views of Quality Assessment There is an application area where absolute quality ratings of argumentative text are common practice: essay scoring (Beigman Klebanov et al., 2016). Persing and Ng (2015) annotated the argumentative strength of essays composing multiple arguments with notable agreement. For single arguments, however, all existing approaches that we are aware of assess quality in relative terms, e.g., Cabrio and Villata (2012) find accepted arguments based on attack relations, Wei et al. (2016) rank arguments by their persuasiveness, and Wachsmuth et al. (2017b) rank them by their relevance. Boudry et al. (2015) argue that normative concepts such as fallacies rarely apply to real-life arguments and that they are too sophisticated for operationalization. Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a foll"
P17-2056,L16-1699,0,0.106669,"Missing"
P17-2056,W13-2322,0,0.00583018,"e-Kohler (2016), these are the classes containing the majority of the verb types. 4 https://spacy.io Note that in our case this ranges between 0 (perfect performance) and 6 (worst performance). 6 UW used the IBM CPLEX Optimizer 5 355 library7 ). All hyperparameters were tuned on the development set. Dataset All-factual UW feat.† AMR Rule-based Supervised Semantic representation approach In addition to the rule-based and supervised approaches, we experimented with a semantic abstraction of the sentence. For that end, we extracted features inspired by the UW system on the popular AMR formalism (Banarescu et al., 2013) using a SoA parser (Pust et al., 2015). Our hope was that this would improve performance by focusing on the more semantically-significant portions of the predicate-argument structure. In particular, we extracted the following features from the predicted AMR structures: immediate parents, grandparents and siblings of the target node, lemma and POS tag of the target and preceding token in the sentence, and a Boolean feature based on the AMR polarity role (indicating semantic negation). MEANTIME MAE r MAE .80 .81 .66 .75 .59 0 .66 .66 .62 .71 .78 .51 .64 .72 .42 0 .71 .58 .63 .66 .31 .56 .44 .35"
P17-2056,W13-0501,0,0.0192302,"ves (dishonest), future tense (will, won’t), and more. Looking at the numerous and varied possibilities language offers to express all the different shades of modality, it is clear that factuality does not assume any fixed set of discrete values either. Instead, the underlying linguistic system forms a continuous spectrum ranging from factual to counterfactual (Saur´ı and Pustejovsky, 2009). While linguistic theory assigns a spectrum of factuality values, recent years have seen many practical efforts to capture the notion of factuality in a consistent annotation (Saur´ı and Pustejovsky, 2009; Nissim et al., 2013; Lee et al., 2015; OGorman et al., 2016; Minard et al., 2016; Ghia et al., 2016). Each of these make certain deci1 https://github.com/gabrielStanovsky/ unified-factuality 353 Corpus FactBank MEANTIME† UW #Tokens/Sentences 77231 / 3839 9743 / 631 106371 / 4234 Factuality Values Original Our mapping Factual (CT+/-) +3.0 / -3.0 Probable (PR+/-) +2.0 / -2.0 Possible (PS+/-) +1.0 / -1.0 Unknown (Uu/CTu ) 0.0 Fact / Counterfact +3.0 / -3.0 Possibility (uncertain) +1.5 / -1.5 Possibility (future) +0.5 / -0.5 [-3.0, 3.0] Type Annotators Perspective Discrete Experts Author’s and discourse-internal sou"
P17-2056,P16-1077,1,0.890518,"uality value. advcl advmod acomp nsubj nsubj ccomp nsubj dobj Don was dishonest when he said he paid taxes mod prop of mod subj comp subj dobj Don was dishonest when he said he paid taxes Figure 2: Dependency tree (top, obtained with spaCy) versus PropS representation (bottom, obtained via the online demo). Note that PropS posits dishonest as the head of said, while the dependency tree obstructs this relation. 5 Evaluation Extending TruthTeller’s lexicon We extended the TruthTeller lexicon of single-word predicates by integrating a large resource of modality markers. Following the approach of Eckle-Kohler (2016), we first induced the modality status of English adjectives and nouns from the subcategorization frames of their German counterparts listed in a large valency lexicon (using the “IMSLex German Lexicon” (Fitschen, 2004) and Google Translate for obtaining the translations2 ). We focused on four modality classes (the classes whfactual and wh/if-factual indicating factuality, and the two classes future-orientation and non-factual, indicating uncertainty)3 and semi-automatically mapped them to the signatures used in TruthTeller. We performed the same kind of mapping for the modality classes of Eng"
P17-2056,W16-5706,0,0.00694973,"n’t), and more. Looking at the numerous and varied possibilities language offers to express all the different shades of modality, it is clear that factuality does not assume any fixed set of discrete values either. Instead, the underlying linguistic system forms a continuous spectrum ranging from factual to counterfactual (Saur´ı and Pustejovsky, 2009). While linguistic theory assigns a spectrum of factuality values, recent years have seen many practical efforts to capture the notion of factuality in a consistent annotation (Saur´ı and Pustejovsky, 2009; Nissim et al., 2013; Lee et al., 2015; OGorman et al., 2016; Minard et al., 2016; Ghia et al., 2016). Each of these make certain deci1 https://github.com/gabrielStanovsky/ unified-factuality 353 Corpus FactBank MEANTIME† UW #Tokens/Sentences 77231 / 3839 9743 / 631 106371 / 4234 Factuality Values Original Our mapping Factual (CT+/-) +3.0 / -3.0 Probable (PR+/-) +2.0 / -2.0 Possible (PS+/-) +1.0 / -1.0 Unknown (Uu/CTu ) 0.0 Fact / Counterfact +3.0 / -3.0 Possibility (uncertain) +1.5 / -1.5 Possibility (future) +0.5 / -0.5 [-3.0, 3.0] Type Annotators Perspective Discrete Experts Author’s and discourse-internal sources Discrete Experts Author’s Continuou"
P17-2056,D15-1136,0,0.0405415,"ining the majority of the verb types. 4 https://spacy.io Note that in our case this ranges between 0 (perfect performance) and 6 (worst performance). 6 UW used the IBM CPLEX Optimizer 5 355 library7 ). All hyperparameters were tuned on the development set. Dataset All-factual UW feat.† AMR Rule-based Supervised Semantic representation approach In addition to the rule-based and supervised approaches, we experimented with a semantic abstraction of the sentence. For that end, we extracted features inspired by the UW system on the popular AMR formalism (Banarescu et al., 2013) using a SoA parser (Pust et al., 2015). Our hope was that this would improve performance by focusing on the more semantically-significant portions of the predicate-argument structure. In particular, we extracted the following features from the predicted AMR structures: immediate parents, grandparents and siblings of the target node, lemma and POS tag of the target and preceding token in the sentence, and a Boolean feature based on the AMR polarity role (indicating semantic negation). MEANTIME MAE r MAE .80 .81 .66 .75 .59 0 .66 .66 .62 .71 .78 .51 .64 .72 .42 0 .71 .58 .63 .66 .31 .56 .44 .35 .34 r 0 .33 .30 .23 .47 is due to its"
P17-2056,J12-2002,0,0.0450076,"Missing"
P17-2056,S12-1020,0,0.315353,"d into rule-based systems which examine deep linguistic features, and machine learning algorithms which generally extract more shallow features. The De Facto factuality profiler (Saur´ı and Pustejovsky, 2012) and TruthTeller algorithms (Lotan et al., 2013) take the rule-based approach and assign a discrete annotation of factuality (following the values assigned by FactBank) using a deterministic rule-based topdown approach on dependency trees, changing the factuality assessment when encountering factuality affecting predicates or modality and negation cues (following implicative signatures by Karttunen (2012)). In addition to a factuality assessment, TruthTeller assigns three values per predicate in the sentence: (1) implicative signature from a hand-coded lexicon indicating how this predicate changes the factuality of its embedded clause, in positive and negative contexts, (2) clause truth, marking the factuality assessment of the entire clause, and (3) negation and uncertainty, indicating whether this predicate is affected by negation or modality. Both of these algorithms rely on a hand-written lexicon of predicates, indicating how they modify the factuality status of their embedded predicates ("
P17-2056,D15-1189,0,0.276512,"se, while hypothetical or negated ones should be left out. Similarly, for argumentation analysis and question answering, factuality can play a major role in backing a specific claim or supporting evidence for an answer to a question at hand. Recent research efforts have approached the factuality task from two complementing directions: automatic prediction and large scale annotation. Previous attempts for automatic factuality prediction either took a rule-based, deep syntactic approach (Lotan et al., 2013; Saur´ı and Pustejovsky, 2012) or a machine learning approach over more shallow features (Lee et al., 2015). In terms of annotation, each effort was largely carried out independently of the others, picking up different factuality flavors and different annotation scales. In correlation, the proposed algorithms have targeted a single annotated resource which they aim to recover. Subsequently, this separation between annotated corpora has prevented a comparison across datasets. Further, the models are nonportable, inhibiting advancements in one dataset to carry over to any of the other annotations. Our contribution in this work is twofold. First, we suggest that the task can benefit from a unified rep"
P17-2056,N13-1091,1,0.814565,"e, in knowledge base population, only propositions marked as factual should be admitted into the knowledge base, while hypothetical or negated ones should be left out. Similarly, for argumentation analysis and question answering, factuality can play a major role in backing a specific claim or supporting evidence for an answer to a question at hand. Recent research efforts have approached the factuality task from two complementing directions: automatic prediction and large scale annotation. Previous attempts for automatic factuality prediction either took a rule-based, deep syntactic approach (Lotan et al., 2013; Saur´ı and Pustejovsky, 2012) or a machine learning approach over more shallow features (Lee et al., 2015). In terms of annotation, each effort was largely carried out independently of the others, picking up different factuality flavors and different annotation scales. In correlation, the proposed algorithms have targeted a single annotated resource which they aim to recover. Subsequently, this separation between annotated corpora has prevented a comparison across datasets. Further, the models are nonportable, inhibiting advancements in one dataset to carry over to any of the other annotatio"
P17-2056,W09-3714,0,0.0913577,"Missing"
P17-4004,N16-1181,0,0.033485,"nd the capabilities of the different approaches and can aid qualitative analyses. The source-code of our system is publicly available.1 1 Introduction Attention-based neural networks are increasingly popular because of their ability to focus on the most important segments of a given input. These models have proven to be extremely effective in many different tasks, for example neural machine translation (Luong et al., 2015; Tu et al., 2016), neural image caption generation (Xu et al., 2015), and multiple sub-tasks in question answering (Hermann et al., 2015; Tan et al., 2016; Yin et al., 2016; Andreas et al., 2016). Attention-based neural networks are especially successful in answer selection for non-factoid ques1 https://github.com/UKPLab/ acl2017-non-factoid-qa 19 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 19–24 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-4004 InsuranceQA Model A StackExchange Model B Candidate Ranking C HTTP REST an n st io Q ue s io st te n, da C di ue Q an di da Q ue te At s s te tio nt n, io A n n W sw ei e gh rs ts , Candidate"
P17-4004,I08-1055,0,0.0392899,"0 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-4004 InsuranceQA Model A StackExchange Model B Candidate Ranking C HTTP REST an n st io Q ue s io st te n, da C di ue Q an di da Q ue te At s s te tio nt n, io A n n W sw ei e gh rs ts , Candidate Retrieval which enables us to interactively visualize the attention vectors in the user interface. Our architecture is similar to the pipelined structures of earlier work in question answering that rely on a retrieval step followed by a more expensive supervised ranking approach (Surdeanu et al., 2011; Higashinaka and Isozaki, 2008). We primarily chose this architecture because it allows the user to directly relate the results of the system to the answer selection model. The use of more advanced components (e.g. query expansion or answer merging) would negate this possibility due to the added complexity. Because all components in our extensible service architecture are loosely coupled, it is possible to use multiple candidate ranking services with different attention mechanisms at the same time. The user interface exploits this ability and allows researchers to interactively compare two models side-by-side within the sam"
P17-4004,D15-1166,0,0.118538,"Missing"
P17-4004,Q16-1019,0,0.0764211,"Missing"
P17-4004,J11-2003,0,0.0891603,"ncouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-4004 InsuranceQA Model A StackExchange Model B Candidate Ranking C HTTP REST an n st io Q ue s io st te n, da C di ue Q an di da Q ue te At s s te tio nt n, io A n n W sw ei e gh rs ts , Candidate Retrieval which enables us to interactively visualize the attention vectors in the user interface. Our architecture is similar to the pipelined structures of earlier work in question answering that rely on a retrieval step followed by a more expensive supervised ranking approach (Surdeanu et al., 2011; Higashinaka and Isozaki, 2008). We primarily chose this architecture because it allows the user to directly relate the results of the system to the answer selection model. The use of more advanced components (e.g. query expansion or answer merging) would negate this possibility due to the added complexity. Because all components in our extensible service architecture are loosely coupled, it is possible to use multiple candidate ranking services with different attention mechanisms at the same time. The user interface exploits this ability and allows researchers to interactively compare two mo"
P17-4004,P16-1044,0,0.383182,"or questions and candidates, where representations of a question and an associated correct answer should lie closely together within the vector space (Feng et al., 2015). Accordingly, the ranking score can be determined with a simple similarity metric. Attention in this scenario works by calculating weights for each individual segment in the input (attention vector), where segments with a higher weight should have a stronger impact on the resulting representation. Several approaches have been recently proposed, achieving state-of-the-art results on different datasets (Dos Santos et al., 2016; Tan et al., 2016; Wang et al., 2016). The success of these approaches clearly shows the importance of sophisticated attention mechanisms for effective answer selection models. However, it has also been shown that attention mechanisms can introduce certain biases that negatively influence the results (Wang et al., 2016). As a consequence, the creation of better attention mechanisms can improve the overall answer selection performance. To achieve this goal, researchers are required to perform in-depth analyses and comparisons of different approaches to understand what the individual models learn and how they ca"
P17-4004,P16-1008,0,0.0438643,"Missing"
P17-4004,P16-1122,0,0.0313331,"andidates, where representations of a question and an associated correct answer should lie closely together within the vector space (Feng et al., 2015). Accordingly, the ranking score can be determined with a simple similarity metric. Attention in this scenario works by calculating weights for each individual segment in the input (attention vector), where segments with a higher weight should have a stronger impact on the resulting representation. Several approaches have been recently proposed, achieving state-of-the-art results on different datasets (Dos Santos et al., 2016; Tan et al., 2016; Wang et al., 2016). The success of these approaches clearly shows the importance of sophisticated attention mechanisms for effective answer selection models. However, it has also been shown that attention mechanisms can introduce certain biases that negatively influence the results (Wang et al., 2016). As a consequence, the creation of better attention mechanisms can improve the overall answer selection performance. To achieve this goal, researchers are required to perform in-depth analyses and comparisons of different approaches to understand what the individual models learn and how they can be improved. Due t"
P19-1054,W15-0514,0,0.0412787,"Missing"
P19-1054,D17-1070,0,0.0445482,"gs .6539 .5232 .7848 Supervised Methods: Cross-Topic Evaluation BERT-base .7401 .6695 .8107 BERT-large .7244 .6297 .8191 With Clustering Fmean Fsim Fdissim .7070 .6188 .7951 .4253 .3162 .5344 .5800 .6344 .6149 .5926 .6366 .6070 .4892 .5443 .4587 .4605 .5347 .4818 .6708 .7584 .7711 .7246 .7384 .7323 .7007 .7135 .6269 .6125 .7746 .8146 Table 2: F1 scores on the UKP ASPECT Corpus. (without stop-words) in our training corpus and compute the cosine similarity between the Tf-Idf vectors of a sentence. InferSent. We compute the cosine-similarity between the sentence embeddings returned by InferSent (Conneau et al., 2017). Average Word Embeddings. We compute the cosine-similarity between the average word embeddings for GloVe, ELMo and BERT. BERT. We fine-tune the BERT-uncased model to predict the similarity between two given arguments. We add a sigmoid layer to the special [CLS] token and trained it on some of the topics. We fine-tuned for three epochs, with a learning rate of 2e-5 and a batch-size of 32. Human Performance. We approximated the human upper bound on the UKP ASPECT corpus in the following way: we randomly split the seven pair-wise annotations in two groups, computed their corresponding MACE (Hovy"
P19-1054,P17-1002,1,0.860482,"en applied to different tasks such as identifying reasoning structures (Stab and Gurevych, 2014), assessing the quality of arguments (Wachsmuth et al., 2017), or linking arguments from different documents (Cabrio and Villata, 2012). Broadly speaking, existing methods either approach argument mining from the discourse-level perspective (aiming to analyze local argumentation structures), or from an information-seeking perspective (aiming to detect arguments relevant to a predefined topic). While discourse-level approaches mostly focus on the analysis of single documents or document collections (Eger et al., 2017), information-seeking approaches need to be capable of dealing with heterogeneous sources and topics (Shnarch et al., 2018) and also face the problem of redundancy, as A1 The ultimate goal is fast, affordable, open Internet access for everyone, everywhere. A2 If this does not happen, we will create an Internet where only users able to pay for privileged access enjoy the network’s full capabilities. Figure 1: Similar pro arguments for the topic “net neutrality”. Contextualized word embeddings, especially ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) could offer a viable solution to"
P19-1054,N13-1132,0,0.0332882,"017). Average Word Embeddings. We compute the cosine-similarity between the average word embeddings for GloVe, ELMo and BERT. BERT. We fine-tune the BERT-uncased model to predict the similarity between two given arguments. We add a sigmoid layer to the special [CLS] token and trained it on some of the topics. We fine-tuned for three epochs, with a learning rate of 2e-5 and a batch-size of 32. Human Performance. We approximated the human upper bound on the UKP ASPECT corpus in the following way: we randomly split the seven pair-wise annotations in two groups, computed their corresponding MACE (Hovy et al., 2013) scores and calculated Fsim , Fdissim and Fmean . We repeated this process ten times and averaged over all runs (without clustering setup). For the with clustering setup, we applied agglomerative hierarchical clustering on the MACE scores of one of both groups and computed the evaluation metrics using the other group as the gold label. For the AFS dataset, Misra et al. (2016) computed the correlation between the three human annotators. their experiments on the AFS corpus, Misra et al. (2016) only performed a within-topic evaluation by using 10-fold cross-validation. As we are primarily interes"
P19-1054,C14-1141,0,0.0583405,"ERT (Devlin et al., 2018) could offer a viable solution to this problem. In contrast to traditional word embeddings like word2vec (Mikolov et al., 2013) or 1 Code and models available: https://github.com/UKPLab/ acl2019-BERT-argument-classification-and-clustering 567 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 567–578 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Argument Classification, as viewed in this work, aims to identify topic-related, sentencelevel arguments from (heterogeneous) documents. Levy et al. (2014) identify context-dependent claims (CDCs) by splitting the problem into smaller sub-problems. Rinott et al. (2015) extend this work with a pipeline of feature-based models that find and rank supporting evidence from Wikipedia for the CDCs. However, neither of these approaches leverage the potential of word embeddings in capturing semantic relations between words. GloVe (Pennington et al., 2014), these methods compute the embeddings for a sentence on the fly by taking the context of a target word into account. This yields word representations that better match the specific sense of the word in"
P19-1054,W16-3636,0,0.139946,"arch, contextualized representations need to be able to adapt to new, unseen textual topics. We thus analyze ELMo and BERT in a cross-topic scenario for the tasks of argument classification and clustering on four different datasets. For argument classification, we use the UKP Sentential Argument Mining Corpus by R Stab et al. (2018b) and the IBM Debater : Evidence Sentences corpus by Shnarch et al. (2018). For argument clustering, we introduce a novel corpus on aspect-based argument clustering and evaluate the proposed methods on this corpus as well as on the Argument Facet Similarity Corpus (Misra et al., 2016). The contributions in this publications are: (1) We frame the problem of open-domain argument search as a combination of topic-dependent argument classification and clustering and discuss how contextualized word embeddings can help to improve these tasks across four different datasets. (2) We show that our suggested methods improve the state-of-the-art for argument classification when fine-tuning the models, thus significantly reducing the gap to human performance. (3) We introduce a novel corpus on aspect-based argument similarity and demonstrate how contextualized word embeddings help to im"
P19-1054,D14-1162,0,0.0896464,"uly 28 - August 2, 2019. 2019 Association for Computational Linguistics Argument Classification, as viewed in this work, aims to identify topic-related, sentencelevel arguments from (heterogeneous) documents. Levy et al. (2014) identify context-dependent claims (CDCs) by splitting the problem into smaller sub-problems. Rinott et al. (2015) extend this work with a pipeline of feature-based models that find and rank supporting evidence from Wikipedia for the CDCs. However, neither of these approaches leverage the potential of word embeddings in capturing semantic relations between words. GloVe (Pennington et al., 2014), these methods compute the embeddings for a sentence on the fly by taking the context of a target word into account. This yields word representations that better match the specific sense of the word in a sentence. In cross-topic scenarios, with which we are dealing in open-domain argument search, contextualized representations need to be able to adapt to new, unseen textual topics. We thus analyze ELMo and BERT in a cross-topic scenario for the tasks of argument classification and clustering on four different datasets. For argument classification, we use the UKP Sentential Argument Mining Cor"
P19-1054,P17-2039,1,0.858843,"prove the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater - Evidence Sentences dataset by 7.4 percentage points. For the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus.1 1 Introduction Argument mining methods have been applied to different tasks such as identifying reasoning structures (Stab and Gurevych, 2014), assessing the quality of arguments (Wachsmuth et al., 2017), or linking arguments from different documents (Cabrio and Villata, 2012). Broadly speaking, existing methods either approach argument mining from the discourse-level perspective (aiming to analyze local argumentation structures), or from an information-seeking perspective (aiming to detect arguments relevant to a predefined topic). While discourse-level approaches mostly focus on the analysis of single documents or document collections (Eger et al., 2017), information-seeking approaches need to be capable of dealing with heterogeneous sources and topics (Shnarch et al., 2018) and also face t"
P19-1054,N18-1202,0,0.0797636,"focus on the analysis of single documents or document collections (Eger et al., 2017), information-seeking approaches need to be capable of dealing with heterogeneous sources and topics (Shnarch et al., 2018) and also face the problem of redundancy, as A1 The ultimate goal is fast, affordable, open Internet access for everyone, everywhere. A2 If this does not happen, we will create an Internet where only users able to pay for privileged access enjoy the network’s full capabilities. Figure 1: Similar pro arguments for the topic “net neutrality”. Contextualized word embeddings, especially ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) could offer a viable solution to this problem. In contrast to traditional word embeddings like word2vec (Mikolov et al., 2013) or 1 Code and models available: https://github.com/UKPLab/ acl2019-BERT-argument-classification-and-clustering 567 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 567–578 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Argument Classification, as viewed in this work, aims to identify topic-related, sentencelevel arguments from (heterogeneous) docum"
P19-1054,P18-1023,0,0.0470376,"eventually propagate further down to the clustering of similar arguments. Hence, in this work, we aim to tackle this problem by leveraging superior contextualized language models to improve on precision and recall of argumentative sentences. Argument Clustering aims to identify similar arguments. Previous research in this area mainly used feature-based approaches in combination with traditional word embeddings like ˇ word2vec or GloVe. Boltuˇzi´c and Snajder (2015) applied hierarchical clustering on semantic similarities between users’ posts from a two-side online debate forum using word2vec. Wachsmuth et al. (2018) experimented with different word embeddings techniques for (counter)argument similarity. Misra et al. (2016) presented a new corpus on argument similarity on three topics. They trained a Support Vector Regression model using different hand-engineered features including custom trained word2vec. Trabelsi and Za¨ıane (2015) used an augmented LDA to automatically extract coherent words and phrases describing arguing expressions and apply constrained clusterRelated Work In the following, we concentrate on the fundamental tasks involved in open-domain argument search. First, we discuss work that ex"
P19-1054,D15-1050,0,0.0579524,"ngs like word2vec (Mikolov et al., 2013) or 1 Code and models available: https://github.com/UKPLab/ acl2019-BERT-argument-classification-and-clustering 567 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 567–578 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Argument Classification, as viewed in this work, aims to identify topic-related, sentencelevel arguments from (heterogeneous) documents. Levy et al. (2014) identify context-dependent claims (CDCs) by splitting the problem into smaller sub-problems. Rinott et al. (2015) extend this work with a pipeline of feature-based models that find and rank supporting evidence from Wikipedia for the CDCs. However, neither of these approaches leverage the potential of word embeddings in capturing semantic relations between words. GloVe (Pennington et al., 2014), these methods compute the embeddings for a sentence on the fly by taking the context of a target word into account. This yields word representations that better match the specific sense of the word in a sentence. In cross-topic scenarios, with which we are dealing in open-domain argument search, contextualized rep"
P19-1054,P18-2095,0,0.259083,"of arguments (Wachsmuth et al., 2017), or linking arguments from different documents (Cabrio and Villata, 2012). Broadly speaking, existing methods either approach argument mining from the discourse-level perspective (aiming to analyze local argumentation structures), or from an information-seeking perspective (aiming to detect arguments relevant to a predefined topic). While discourse-level approaches mostly focus on the analysis of single documents or document collections (Eger et al., 2017), information-seeking approaches need to be capable of dealing with heterogeneous sources and topics (Shnarch et al., 2018) and also face the problem of redundancy, as A1 The ultimate goal is fast, affordable, open Internet access for everyone, everywhere. A2 If this does not happen, we will create an Internet where only users able to pay for privileged access enjoy the network’s full capabilities. Figure 1: Similar pro arguments for the topic “net neutrality”. Contextualized word embeddings, especially ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) could offer a viable solution to this problem. In contrast to traditional word embeddings like word2vec (Mikolov et al., 2013) or 1 Code and models availabl"
P19-1054,N18-5005,1,0.693042,"ethods compute the embeddings for a sentence on the fly by taking the context of a target word into account. This yields word representations that better match the specific sense of the word in a sentence. In cross-topic scenarios, with which we are dealing in open-domain argument search, contextualized representations need to be able to adapt to new, unseen textual topics. We thus analyze ELMo and BERT in a cross-topic scenario for the tasks of argument classification and clustering on four different datasets. For argument classification, we use the UKP Sentential Argument Mining Corpus by R Stab et al. (2018b) and the IBM Debater : Evidence Sentences corpus by Shnarch et al. (2018). For argument clustering, we introduce a novel corpus on aspect-based argument clustering and evaluate the proposed methods on this corpus as well as on the Argument Facet Similarity Corpus (Misra et al., 2016). The contributions in this publications are: (1) We frame the problem of open-domain argument search as a combination of topic-dependent argument classification and clustering and discuss how contextualized word embeddings can help to improve these tasks across four different datasets. (2) We show that our sugge"
P19-1054,D14-1006,1,0.750684,"d across multiple datasets. For argument classification, we improve the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater - Evidence Sentences dataset by 7.4 percentage points. For the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus.1 1 Introduction Argument mining methods have been applied to different tasks such as identifying reasoning structures (Stab and Gurevych, 2014), assessing the quality of arguments (Wachsmuth et al., 2017), or linking arguments from different documents (Cabrio and Villata, 2012). Broadly speaking, existing methods either approach argument mining from the discourse-level perspective (aiming to analyze local argumentation structures), or from an information-seeking perspective (aiming to detect arguments relevant to a predefined topic). While discourse-level approaches mostly focus on the analysis of single documents or document collections (Eger et al., 2017), information-seeking approaches need to be capable of dealing with heterogene"
P19-1054,D18-1402,1,0.456798,"ethods compute the embeddings for a sentence on the fly by taking the context of a target word into account. This yields word representations that better match the specific sense of the word in a sentence. In cross-topic scenarios, with which we are dealing in open-domain argument search, contextualized representations need to be able to adapt to new, unseen textual topics. We thus analyze ELMo and BERT in a cross-topic scenario for the tasks of argument classification and clustering on four different datasets. For argument classification, we use the UKP Sentential Argument Mining Corpus by R Stab et al. (2018b) and the IBM Debater : Evidence Sentences corpus by Shnarch et al. (2018). For argument clustering, we introduce a novel corpus on aspect-based argument clustering and evaluate the proposed methods on this corpus as well as on the Argument Facet Similarity Corpus (Misra et al., 2016). The contributions in this publications are: (1) We frame the problem of open-domain argument search as a combination of topic-dependent argument classification and clustering and discuss how contextualized word embeddings can help to improve these tasks across four different datasets. (2) We show that our sugge"
P19-1213,D15-1075,0,0.160168,"Missing"
P19-1213,P17-1152,0,0.09963,"Missing"
P19-1213,P18-1063,0,0.0915052,"fter six years this summer. [...] england’s premier league clubs set to leave liverpool after six years this summer. [...] Figure 3: Examples of incorrect sentences produced by different summarization models on the CNN-DM test set. 4 Correctness of State-of-the-Art Models Using the crowd-based evaluation, we assessed the correctness of summaries for a randomly sampled subset of 100 summaries from the CNN-DM test set. We included three summarization models: PGC The pointer-generator model with coverage as introduced by See et al. (2017). FAS The hybrid extractive-abstractive system proposed by Chen and Bansal (2018) including their redundancy-based reranking. BUS The bottom-up summarization system recently proposed by Gehrmann et al. (2018). To the best of our knowledge, BUS is the state-ofthe-art abstractive model on the non-anonymized version of CNN-DM as of writing this, while FAS is only slightly behind. We use the original generated summaries provided by the authors and crowdsource correctness labels using 9 workers. Table 1 shows the evaluation results3 . In line with the findings for sentence summarization (Cao et al., 2018; Li et al., 2018), we observe that factual errors are also a frequent prob"
P19-1213,N16-1012,0,0.10932,"Missing"
P19-1213,D17-1070,0,0.0698726,"Missing"
P19-1213,N19-1423,0,0.121144,"Missing"
P19-1213,D18-1443,0,0.126101,"re 3: Examples of incorrect sentences produced by different summarization models on the CNN-DM test set. 4 Correctness of State-of-the-Art Models Using the crowd-based evaluation, we assessed the correctness of summaries for a randomly sampled subset of 100 summaries from the CNN-DM test set. We included three summarization models: PGC The pointer-generator model with coverage as introduced by See et al. (2017). FAS The hybrid extractive-abstractive system proposed by Chen and Bansal (2018) including their redundancy-based reranking. BUS The bottom-up summarization system recently proposed by Gehrmann et al. (2018). To the best of our knowledge, BUS is the state-ofthe-art abstractive model on the non-anonymized version of CNN-DM as of writing this, while FAS is only slightly behind. We use the original generated summaries provided by the authors and crowdsource correctness labels using 9 workers. Table 1 shows the evaluation results3 . In line with the findings for sentence summarization (Cao et al., 2018; Li et al., 2018), we observe that factual errors are also a frequent problem for document summarization. Interestingly, the fraction of incorrect summaries is substantially higher for FAS and BUS comp"
P19-1213,P18-1064,0,0.0684475,"otations to be used as additional test data in future work.1 2 Previous work already proposed the use of explicit proposition structures (Cao et al., 2018) and multi-task learning with NLI (Li et al., 2018; Pasunuru et al., 2017) to successfully improve the correctness of abstractive sentence summaries. In this work, we instead focus on the more challenging single-document summarization, where longer summaries allow for more errors. Very recently, Fan et al. (2018) showed that with ideas similar to Cao et al. (2018)’s work, the correctness of document summaries can also be improved. Moreover, Guo et al. (2018) and Pasunuru and Bansal (2018) proposed to use NLI-based loss functions or multi-task learning with NLI for document summarization. But unfortunately, their experiments do not evaluate whether the techniques improve summarization correctness. We are the first to use NLI in a reranking setup, which is beneficial for this study as it allows to us to clearly isolate the net impact of the NLI component. Evaluating Summary Correctness Similar to previous work by Cao et al. (2018) and Li et al. (2018), we argue that the correctness of a generated summary can only be reliably evaluated by manual ins"
P19-1213,N13-1132,0,0.0333089,"for every sentence, we first merge the labels collected from different annotators. A summary then receives the label incorrect if at least one of its sentences has been labeled as such, otherwise, it is labeled as correct. A challenge of crowdsourcing is that workers are untrained and some might produce low quality annotations (Sabou et al., 2014). For our task, an additional challenge is that some errors are rather subtle, while on the other hand the majority of summary sentences are correct, which requires workers to carry out the task very carefully to catch these rare cases. We use MACE (Hovy et al., 2013), a Bayesian model that incorporates the reliability of individual workers, to merge sentence-level labels. We also performed an experiment to determine the necessary number of workers to obtain reliable labels. Two annotators from our lab labeled 50 generated summaries (140 sentences) manually and then merged their labels to obtain a gold standard. For the same data, we collected 14 labels per sentence from crowdworkers. Figure 2 shows the agreement, measured as Cohen’s κ, between the MACE-merged labels of different subsets of the crowdsourced labels and the gold standard. We find that the ag"
P19-1213,C18-1121,0,0.209652,"e hybrid extractive-abstractive system proposed by Chen and Bansal (2018) including their redundancy-based reranking. BUS The bottom-up summarization system recently proposed by Gehrmann et al. (2018). To the best of our knowledge, BUS is the state-ofthe-art abstractive model on the non-anonymized version of CNN-DM as of writing this, while FAS is only slightly behind. We use the original generated summaries provided by the authors and crowdsource correctness labels using 9 workers. Table 1 shows the evaluation results3 . In line with the findings for sentence summarization (Cao et al., 2018; Li et al., 2018), we observe that factual errors are also a frequent problem for document summarization. Interestingly, the fraction of incorrect summaries is substantially higher for FAS and BUS compared to PGC. The length of the 3 The ROUGE scores have been recomputed by us on the used data and match the reported scores very closely. generated summaries appears to be unrelated to the number of errors. Instead, the higher abstractiveness of summaries produced by FAS and BUS, as analyzed in their respective papers, seems to also increase the chance of introducing errors. In addition, we also observe that amon"
P19-1213,P19-1334,0,0.0289414,"Missing"
P19-1213,K16-1028,0,0.253769,"Missing"
P19-1213,P19-1449,0,0.0253317,"Missing"
P19-1213,D16-1244,0,0.0953935,"Missing"
P19-1213,N18-2102,0,0.0798384,"additional test data in future work.1 2 Previous work already proposed the use of explicit proposition structures (Cao et al., 2018) and multi-task learning with NLI (Li et al., 2018; Pasunuru et al., 2017) to successfully improve the correctness of abstractive sentence summaries. In this work, we instead focus on the more challenging single-document summarization, where longer summaries allow for more errors. Very recently, Fan et al. (2018) showed that with ideas similar to Cao et al. (2018)’s work, the correctness of document summaries can also be improved. Moreover, Guo et al. (2018) and Pasunuru and Bansal (2018) proposed to use NLI-based loss functions or multi-task learning with NLI for document summarization. But unfortunately, their experiments do not evaluate whether the techniques improve summarization correctness. We are the first to use NLI in a reranking setup, which is beneficial for this study as it allows to us to clearly isolate the net impact of the NLI component. Evaluating Summary Correctness Similar to previous work by Cao et al. (2018) and Li et al. (2018), we argue that the correctness of a generated summary can only be reliably evaluated by manual inspection. But in contrast to pre"
P19-1213,W17-4504,0,0.0864398,"Missing"
P19-1213,N18-1202,0,0.0248347,"Missing"
P19-1213,D15-1044,0,0.210206,"Missing"
P19-1213,sabou-etal-2014-corpus,0,0.0132901,"extractive, our interface also highlights the sentence in the source document with the highest word overlap, helping the worker to find the relevant information faster. We pay workers $0.20 per task (labeling all sentences of one summary). Given the correctness labels for every sentence, we first merge the labels collected from different annotators. A summary then receives the label incorrect if at least one of its sentences has been labeled as such, otherwise, it is labeled as correct. A challenge of crowdsourcing is that workers are untrained and some might produce low quality annotations (Sabou et al., 2014). For our task, an additional challenge is that some errors are rather subtle, while on the other hand the majority of summary sentences are correct, which requires workers to carry out the task very carefully to catch these rare cases. We use MACE (Hovy et al., 2013), a Bayesian model that incorporates the reliability of individual workers, to merge sentence-level labels. We also performed an experiment to determine the necessary number of workers to obtain reliable labels. Two annotators from our lab labeled 50 generated summaries (140 sentences) manually and then merged their labels to obta"
P19-1213,P17-1099,0,0.295138,"nd’s first-choice right-back at the world cup looks set to leave liverpool after six years this summer. [...] england’s premier league clubs set to leave liverpool after six years this summer. [...] Figure 3: Examples of incorrect sentences produced by different summarization models on the CNN-DM test set. 4 Correctness of State-of-the-Art Models Using the crowd-based evaluation, we assessed the correctness of summaries for a randomly sampled subset of 100 summaries from the CNN-DM test set. We included three summarization models: PGC The pointer-generator model with coverage as introduced by See et al. (2017). FAS The hybrid extractive-abstractive system proposed by Chen and Bansal (2018) including their redundancy-based reranking. BUS The bottom-up summarization system recently proposed by Gehrmann et al. (2018). To the best of our knowledge, BUS is the state-ofthe-art abstractive model on the non-anonymized version of CNN-DM as of writing this, while FAS is only slightly behind. We use the original generated summaries provided by the authors and crowdsource correctness labels using 9 workers. Table 1 shows the evaluation results3 . In line with the findings for sentence summarization (Cao et al."
P19-1213,N18-1101,0,0.124691,"Missing"
P19-1265,W16-4009,0,0.0611492,"Missing"
P19-1265,P17-1002,1,0.844512,"e 3, annotation suggestions are shown in grey, distinguishing them clearly from differently coloured manual annotations. Suggestions can be easily accepted or rejected by single or double clicking. Additionally, manual annotations can be created as usual. Figure 3: Annotation suggestion (grey) and accepted suggestion (orange) in the INCEpTION platform. Suggestion Models To suggest annotations, we use a state-of-the-art BiLSTM network with a conditional random field output layer (Reimers and Gurevych, 2017), which has proven to be a suitable architecture for related tasks (Ajjour et al., 2017; Eger et al., 2017; Levy et al., 2018). We train this model using the gold standard of Schulz et al. (2018), consisting of annotations for all texts from phases S1 and S2. The learning task is framed as standard sequence labelling with a BIOencoding (Begin, Inside, Outside of a sequence) for the four epistemic activities hypothesis generation (HG), evidence generation (EG), evidence evaluation (EE), and drawing conclusions (DC). More precisely, each token is assigned one of the labels ({B, I} × {HG, EG, EE, DC}) ∪ {O}, where B-HG denotes the first token of a HG segment, I-HG denotes a continuation token of a HG"
P19-1265,W10-1807,0,0.0926886,"Missing"
P19-1265,D18-1346,0,0.015352,"ning text from the medicine domain, annotated with epistemic activity segments: evidence generation, evidence evaluation, drawing conclusions, hypothesis generation. adjustment. Andrade et al. (2017), Castro et al. (2018), and Rusu et al. (2016) investigate adapting neural networks to new data with additional classes or even new tasks, requiring to change the structure of the neural network. Our setting is less complex as the neural network is trained on all possible classes from the beginning. Recent work also investigates pre-training neural networks before training them on the actual data (Garg et al., 2018; Shimizu et al., 2018; Serban et al., 2016). The model is thus adapted only once instead of continuously as in our work. 3 Diagnostic Reasoning Task The annotation task proposed by Schulz et al. (2018) has interesting properties for studying the effects of annotation suggestions in hard expert tasks: (1) A small set of annotated data is available for two different domains. (2) Other than in wellunderstood low-level tasks, such as part-of-speech tagging or named entity recognition, the expert annotators require the discourse context to identify epistemic activities. This is a hard task yieldin"
P19-1265,L18-1550,0,0.015395,"a speed-up in annotation time. Note, however, that each of these personalised models is trained using only 250 texts, 150 annotated by the respective annotator in S1 and 100 in S2. Instead, the universal model is trained using 650 (MeD) or 550 (TeD) texts. We train ten models with different seeds for each setup (universal and three personalised for MeD and TeD), applying the same parameters for all of them: one hidden layer of 100 units, variational dropout rates for input and hidden layer of 0.25, and the nadam optimiser (Dozat, 2016). We furthermore use the German fastText word embeddings (Grave et al., 2018) to represent the input. We apply early stopping after five epochs without improvement. For the actual suggestions in our experiments, we choose the model with the best performance among the ten for each setup. 3 We utilise the non-overlapping gold annotations of Schulz et al. (2018), where a preference order over epistemic activities was applied to avoid overlapping segments. 2764 4.2 Suggestion Quality Epistemic activity identification is a particularly hard discourse-level sequence labelling task, both for expert annotators and machine learning models. Before beginning with our annotation e"
P19-1265,C14-2017,0,0.0709655,"Missing"
P19-1265,C18-2002,1,0.861931,"Missing"
P19-1265,C18-1176,0,0.0164122,"gestions are shown in grey, distinguishing them clearly from differently coloured manual annotations. Suggestions can be easily accepted or rejected by single or double clicking. Additionally, manual annotations can be created as usual. Figure 3: Annotation suggestion (grey) and accepted suggestion (orange) in the INCEpTION platform. Suggestion Models To suggest annotations, we use a state-of-the-art BiLSTM network with a conditional random field output layer (Reimers and Gurevych, 2017), which has proven to be a suitable architecture for related tasks (Ajjour et al., 2017; Eger et al., 2017; Levy et al., 2018). We train this model using the gold standard of Schulz et al. (2018), consisting of annotations for all texts from phases S1 and S2. The learning task is framed as standard sequence labelling with a BIOencoding (Begin, Inside, Outside of a sequence) for the four epistemic activities hypothesis generation (HG), evidence generation (EG), evidence evaluation (EE), and drawing conclusions (DC). More precisely, each token is assigned one of the labels ({B, I} × {HG, EG, EE, DC}) ∪ {O}, where B-HG denotes the first token of a HG segment, I-HG denotes a continuation token of a HG segment (similarly"
P19-1265,C14-2023,1,0.795171,"oss all annotation phases described in Figure 2. Evaluation and Findings In this section, we examine the effects of annotation suggestions in detail, considering interannotator agreement, intra-annotator consistency, annotation bias and speed, as well as usefulness of suggestions and the impact of universal versus personalised suggestion models. Effectiveness of Suggestions Since the annotation of epistemic activities involves determining spans as well as labels, we measure the interannotator agreement (IAA) in terms of Krippendorff’s αU (Krippendorff, 1995) as implemented in DKPro Agreement (Meyer et al., 2014). To First, we compare the overall IAA of both groups for the previous annotation phase S1 by Schulz et al. (2018) and all of our annotation phases O1–O4.2. We observe for TeD that the IAA of the S UGGESTION group is consistently higher than of the S TANDARD group, as soon as annotators receive suggestions (starting in O1). Since the IAAs of the two groups were similar in S1, when no suggestions were given, we deduce that suggestions cause less annotation discrepancies between annotators in TeD. Below, we will investigate if this also introduces an annotation bias. For MeD, results are less cl"
P19-1265,L18-1458,0,0.0375703,"Missing"
P19-1265,L18-1177,0,0.0276609,"Missing"
P19-1265,C18-1127,0,0.0178786,"sh a new dataset covering two domains and carefully analyse the suggested annotations. We find that suggestions have positive effects on annotation speed and performance, while not introducing noteworthy biases. Envisioning suggestion models that improve with newly annotated texts, we contrast methods for continuous model adjustment and suggest the most effective setup for suggestions in future expert tasks. 1 Introduction Current deep learning methods require large amounts of training data to achieve reasonable performance. Scalable solutions to acquire labelled data use crowdsourcing (e.g., Potthast et al., 2018), gamification (Ahn, 2006), or incidental supervision (Roth, 2017). For many complex tasks in expert domains, such as law or medicine, this is, however, not an option since crowdworkers and gamers lack the necessary expertise. Annotating data manually is therefore often the only way to train a model for tasks aiding experts with their work. But the more expertise an annotation task requires, the more time- and funding-intensive it typically is, which is why many projects suffer from small corpora and deficient models. In this paper, we propose and analyse an annotation setup aiming to increase"
P19-1265,D17-1035,1,0.798012,"with the interface. INCEpTION furthermore provides a rich API to integrate our suggestion models. As shown in Figure 3, annotation suggestions are shown in grey, distinguishing them clearly from differently coloured manual annotations. Suggestions can be easily accepted or rejected by single or double clicking. Additionally, manual annotations can be created as usual. Figure 3: Annotation suggestion (grey) and accepted suggestion (orange) in the INCEpTION platform. Suggestion Models To suggest annotations, we use a state-of-the-art BiLSTM network with a conditional random field output layer (Reimers and Gurevych, 2017), which has proven to be a suitable architecture for related tasks (Ajjour et al., 2017; Eger et al., 2017; Levy et al., 2018). We train this model using the gold standard of Schulz et al. (2018), consisting of annotations for all texts from phases S1 and S2. The learning task is framed as standard sequence labelling with a BIOencoding (Begin, Inside, Outside of a sequence) for the four epistemic activities hypothesis generation (HG), evidence generation (EG), evidence evaluation (EE), and drawing conclusions (DC). More precisely, each token is assigned one of the labels ({B, I} × {HG, EG, EE,"
P19-1265,W13-2321,0,0.0645694,"Missing"
P19-1265,P18-2121,0,0.0178031,"medicine domain, annotated with epistemic activity segments: evidence generation, evidence evaluation, drawing conclusions, hypothesis generation. adjustment. Andrade et al. (2017), Castro et al. (2018), and Rusu et al. (2016) investigate adapting neural networks to new data with additional classes or even new tasks, requiring to change the structure of the neural network. Our setting is less complex as the neural network is trained on all possible classes from the beginning. Recent work also investigates pre-training neural networks before training them on the actual data (Garg et al., 2018; Shimizu et al., 2018; Serban et al., 2016). The model is thus adapted only once instead of continuously as in our work. 3 Diagnostic Reasoning Task The annotation task proposed by Schulz et al. (2018) has interesting properties for studying the effects of annotation suggestions in hard expert tasks: (1) A small set of annotated data is available for two different domains. (2) Other than in wellunderstood low-level tasks, such as part-of-speech tagging or named entity recognition, the expert annotators require the discourse context to identify epistemic activities. This is a hard task yielding only inter-rater agr"
P19-1265,C16-1043,0,0.0495976,"Missing"
P19-1265,P14-5016,1,0.858965,"Missing"
P19-1265,W17-5115,0,\N,Missing
P19-1572,J08-4004,0,0.059963,"icient to produce a reliable gold standard, we randomly subsampled the annotations to produce subsamples with one to four annotators per pair. We then computed Spearman’s rank correlation coefficient, ρ, between the gold-standard ranking and BWS scores computed for each subsample. The results averaged over ten random repeats (see Table 3) show that the rankings are very similar even when fewer annotators label each pair. We also computed the mean interannotator agreement (Krippendorff’s α) across instances. The result, 0.80, indicates a satisfactory level of agreement among the crowd workers (Artstein and Poesio, 2008). Taken together, these results suggest that five annotators per pair is more than sufficient to reach a consensus ranking using BWS. # annotators 1 2 3 4 Spearman’s ρ 0.81 0.92 0.97 0.99 Table 3: Agreement measures for the humour dataset. # instances # unique pairs # unique pairs for each instance annotations/pair humour metaphor 4,030 28,210 14 5 15,181 65,323 (avg) 8.6 (avg) 1.55 Table 4: Statistics for the humour and metaphor novelty datasets. Metaphor Novelty Dataset. We use the metaphor novelty dataset of Do Dinh et al. (2018), which contains novelty scores for metaphors (i.e., metaphori"
P19-1572,D14-1190,0,0.0391706,"Missing"
P19-1572,W14-2302,0,0.0272567,"e average word embeddings with linguistic features: average token frequency (taken from a 2017 Wikipedia dump), a polysemy measure represented by the average number of synsets (taken from WordNet 3.0), and average bigram frequency (taken from Google Books Ngrams). Again for the metaphor task, we additionally append the metaphor token frequency if the frequency feature is selected. We repeat Task 2 with different subsets of these features to determine the most effective combination. The token frequency feature has previously been shown to distinguish between metaphoric and literal use (Beigman Klebanov et al., 2014), but also to be indicative of metaphor novelty (Do Dinh et al., 2018). By incorporating the polysemy feature we seek to increase performance especially for the funniness dataset, which includes many puns. The bigram feature reinforces the frequency feature by highlighting instances that include rare bigrams. For best–worst scaling, we use the implementation provided by Kiritchenko and Mohammad (2016). We use the GPPL implementation provided by Simpson and Gurevych (2018). To ensure a reasonable computation time, we follow the authors’ recommendations for hyperparameters and set the number of"
P19-1572,P18-2087,0,0.02642,"lenge (Miller et al., 2017). Several factors motivated our selection of this dataset: (1) Unlike the multimodal datasets of Shahaf et al. 5719 (2015) and Radev et al. (2016), the humour in Miller et al. (2017) is purely verbal. (2) Unlike the cartoon caption and Twitter datasets used in previous studies, the SemEval-2017 jokes were sourced largely from professional humorists and curated joke collections, providing a better a priori expectation of their quality and use of standard language. (3) The dataset has seen use even outside the original shared task (e.g., Mikhalkova and Karyakin, 2017; Cai et al., 2018; Poliak et al., 2018). (4) The jokes have been pre-classified according to their type (homographic puns, heterographic puns, and non-puns), so our extension of it could serve the needs of future qualitative research into humour. The original dataset consists of 4030 short texts averaging about 11 words in length. Of the texts, 3398 contain humour (mostly, but not exclusively, punning jokes) and 632 do not (proverbs and aphorisms). Our examination of the data revealed three duplicate instances in the humour class; to preserve the size of the dataset, we replaced these with three new punning jo"
P19-1572,W18-3502,0,0.053618,"Missing"
P19-1572,N18-2018,0,0.019987,"e automatic processing of verbal humour has applications in human–computer interaction, machine and machine-assisted translation, and the digital humanities (Miller et al., 2017). To give just one example, an intelligent conversational agent should ideally detect and respond appropriately to comments made in jest. The vast majority of past approaches to the automatic recognition of humour (e.g., Mihalcea and Strapparava, 2006; Purandare and Litman, 2006; Sjöbergh and Araki, 2007; Mihalcea et al., 2010; Zhang and Liu, 2014; Yang et al., 2015; Miller et al., 2017; Mikhalkova and Karyakin, 2017; Chen and Soo, 2018) have framed the problem as a binary classification task, which is sufficient for the detection step of our example. However, the ability to assess the degree of humour embodied in an utterance may be necessary for the agent to make a contextually appropriate, humanlike response – for example, a groan for a terrible joke, a chuckle for a middling one, or uproarious laughter for a clever one. Only a few studies have dealt with determining the (relative) funniness of texts. Shahaf et al. (2015) presented a supervised system for determining which of a given pair of cartoon captions is funnier, us"
P19-1572,P13-1004,0,0.0271642,"scaling annotations for metaphor novelty and pairwise annotations for funniness. To make predictions for unlabelled instances and cope better with sparse pairwise labels, Chu and Ghahramani (2005) proposed Gaussian process preference learning (GPPL), a Thurstone– Mosteller–based model that accounts for the features of the instances when inferring their scores. GPPL uses Bayesian inference, which has been shown to cope better with sparse and noisy data (Xiong et al., 2011; Titov and Klementiev, 2012; Beck et al., 2014; Lampos et al., 2014), including disagreements between multiple annotators (Cohn and Specia, 2013; Simpson et al., 2015; Felt et al., 2016; Kido and Okamoto, 2017). Through the random utility model, GPPL is able to handle disagreements between annotators as noise, since no label has a probability of one of being selected. Given a set of pairwise labels, and the features of labelled instances, GPPL can estimate the posterior distribution over the utilities of any instances given their features. Relationships between instances are modelled by a Gaussian process (GP), which computes the covariance between instance utilities as a function of their features (see Rasmussen and Williams, 2006)."
P19-1572,D18-1171,1,0.446538,"seen my collection of ancient Chinese artifacts?” asked Tom charmingly. Table 1: Examples from the SemEval-2017 Task 7 dataset (Miller et al., 2017). The upper example was among those rated funniest by our annotators, while the lower example was among those rated least funny (presumably due to its very tortured pun on “Ming”). girls often produce responses like ‘often go through a bad patch for a year’ ‘when you tried to read the book, there was nothing there, because the words started as a coat-hanger to hang pictures on.’ Table 2: Examples of statements from the Metaphor Novelty dataset (Do Dinh et al., 2018) containing highlighted metaphors. The upper example is highly conventionalised, while the lower is more novel and creative. ten, some annotators may choose middling values while others may prefer extremes. To improve the reliability of annotations, we ask annotators to compare pairs of texts and choose the funniest or most metaphorically novel of the two. Unlike categorical labels, pairwise labels allow a total sorting of the texts since they avoid items having the same value, and can reduce the time taken to label a dataset (Yang and Chen, 2011; Kingsley and Brown, 2010; Kendall, 1948). Pair"
P19-1572,P14-2121,0,0.129959,"s to a Cohen’s κ of 0.80. However, the two-class modelling of metaphor has certain limits. These become obvious when looking at examples from the aforementioned datasets (see Table 2, which includes an example from VUAMC). In particular, many metaphors annotated in the binary datasets differ widely in their metaphoricity – i.e., their degree of being a metaphor. Thus, while the annotations might be reliable, they might not be very meaningful. A graded approach to metaphor better accommodates its subjective and fuzzy nature, but previous work taking such a fine-grained approach is less common. Dunn (2014) conducted experiments regarding the notion of metaphoricity on a sentence basis. Using crowdsourcing, he obtained a small corpus of 60 sentences with metaphoricity scores between 0 (non-metaphoric) and 1 (highly metaphoric). This dataset was then used to determine various features from which a metaphoricity measure could be computed. Due to the lack of a large, graded evaluation corpus, the measure was tested on VUAMC along with a threshold relative to the number of contained metaphors. Haagsma and Bjerva (2016) employed clustering and neural network approaches using selectional preferences t"
P19-1572,C16-1168,0,0.0711434,"Missing"
P19-1572,W06-1673,0,0.0517059,"ych (2018) applied GPPL to ranking arguments by convincingness, which, like funniness and metaphor novelty, is an abstract linguistic property that is hard to quantify directly. They found that GPPL outperformed SVM and BiLSTM regression models that were trained directly on gold-standard scores. Regression approaches are also unsuitable for our scenario, since utilities for training the regression model would first need to be estimated from pairwise labels using, for example, BWS. This type of pipeline approach often suffers from error propagation, which integrated methods such as GPPL avoid (Finkel et al., 2006). We therefore propose the use of GPPL for our creative language tasks to provide a strong baseline that, unlike BWS, can exploit textual features as well as pairwise labels. 3 Data Humour dataset. Our humour dataset is an extension of the data provided for the SemEval-2017 pun recognition challenge (Miller et al., 2017). Several factors motivated our selection of this dataset: (1) Unlike the multimodal datasets of Shahaf et al. 5719 (2015) and Radev et al. (2016), the humour in Miller et al. (2017) is purely verbal. (2) Unlike the cartoon caption and Twitter datasets used in previous studies,"
P19-1572,W16-1102,0,0.0253534,"jective and fuzzy nature, but previous work taking such a fine-grained approach is less common. Dunn (2014) conducted experiments regarding the notion of metaphoricity on a sentence basis. Using crowdsourcing, he obtained a small corpus of 60 sentences with metaphoricity scores between 0 (non-metaphoric) and 1 (highly metaphoric). This dataset was then used to determine various features from which a metaphoricity measure could be computed. Due to the lack of a large, graded evaluation corpus, the measure was tested on VUAMC along with a threshold relative to the number of contained metaphors. Haagsma and Bjerva (2016) employed clustering and neural network approaches using selectional preferences to detect novel metaphors. While the violation of selectional preferences had been used in general metaphor detection before, Haagsma and Bjerva (2016) argue that they are specifically indicative of novel metaphors as opposed to conventionalised ones. However, the authors also struggled with the lack of graded annotations to test their approach. More recently, Parde and Nielsen (2018) and Do Dinh et al. (2018) created graded metaphoricity layers for VUAMC using crowdsourcing, with the former approach labelling gra"
P19-1572,N16-1095,0,0.0724034,"Missing"
P19-1572,P17-2074,0,0.0176225,"examples in Tables 1 and 2 illustrate the difficulty of classifying text as humorous or metaphorical: in both cases, the examples are at least somewhat humorous or somewhat metaphorical, which makes it harder to assign discrete labels such as “funny”/“not funny” or “metaphor”/“literal”. Alternatively, we could assign numerical scores to quantify the humorousness or novelty. However, this can present problems for establishing a gold standard, as human annotators can assign scores inconsistently over time or interpret scores differently to one another (Ovadia, 2004; Yannakakis and Hallam, 2011; Kiritchenko and Mohammad, 2017). For example, if assigning scores between zero and 5716 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5716–5728 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Money is the Root of All Evil. For more info, send $10. “Have you seen my collection of ancient Chinese artifacts?” asked Tom charmingly. Table 1: Examples from the SemEval-2017 Task 7 dataset (Miller et al., 2017). The upper example was among those rated funniest by our annotators, while the lower example was among those rated least funny (pre"
P19-1572,E14-1043,0,0.0170389,"as BWS, and apply it to the tasks of inferring scores from both best– worst scaling annotations for metaphor novelty and pairwise annotations for funniness. To make predictions for unlabelled instances and cope better with sparse pairwise labels, Chu and Ghahramani (2005) proposed Gaussian process preference learning (GPPL), a Thurstone– Mosteller–based model that accounts for the features of the instances when inferring their scores. GPPL uses Bayesian inference, which has been shown to cope better with sparse and noisy data (Xiong et al., 2011; Titov and Klementiev, 2012; Beck et al., 2014; Lampos et al., 2014), including disagreements between multiple annotators (Cohn and Specia, 2013; Simpson et al., 2015; Felt et al., 2016; Kido and Okamoto, 2017). Through the random utility model, GPPL is able to handle disagreements between annotators as noise, since no label has a probability of one of being selected. Given a set of pairwise labels, and the features of labelled instances, GPPL can estimate the posterior distribution over the utilities of any instances given their features. Relationships between instances are modelled by a Gaussian process (GP), which computes the covariance between instance ut"
P19-1572,S17-2005,1,0.622816,"ores inconsistently over time or interpret scores differently to one another (Ovadia, 2004; Yannakakis and Hallam, 2011; Kiritchenko and Mohammad, 2017). For example, if assigning scores between zero and 5716 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5716–5728 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Money is the Root of All Evil. For more info, send $10. “Have you seen my collection of ancient Chinese artifacts?” asked Tom charmingly. Table 1: Examples from the SemEval-2017 Task 7 dataset (Miller et al., 2017). The upper example was among those rated funniest by our annotators, while the lower example was among those rated least funny (presumably due to its very tortured pun on “Ming”). girls often produce responses like ‘often go through a bad patch for a year’ ‘when you tried to read the book, there was nothing there, because the words started as a coat-hanger to hang pictures on.’ Table 2: Examples of statements from the Metaphor Novelty dataset (Do Dinh et al., 2018) containing highlighted metaphors. The upper example is highly conventionalised, while the lower is more novel and creative. ten,"
P19-1572,L18-1243,0,0.0136117,"e, graded evaluation corpus, the measure was tested on VUAMC along with a threshold relative to the number of contained metaphors. Haagsma and Bjerva (2016) employed clustering and neural network approaches using selectional preferences to detect novel metaphors. While the violation of selectional preferences had been used in general metaphor detection before, Haagsma and Bjerva (2016) argue that they are specifically indicative of novel metaphors as opposed to conventionalised ones. However, the authors also struggled with the lack of graded annotations to test their approach. More recently, Parde and Nielsen (2018) and Do Dinh et al. (2018) created graded metaphoricity layers for VUAMC using crowdsourcing, with the former approach labelling grammatical constructions and the latter labelling tokens. However, manually labelling larger amounts of data is costly, even with crowdsourcing. Further, while VUAMC covers multiple domains, it is still limited in scope, size, and language. Thus, an approach is needed to generalise from few graded or ranked metaphor 5718 annotations to a larger corpus or different domains. 2.3 Learning from Pairwise Comparisons Pairwise comparisons can be used to infer rankings or r"
P19-1572,W18-5441,0,0.0501827,"Missing"
P19-1572,S17-2004,0,0.0514275,"Missing"
P19-1572,W06-1625,0,0.0320443,"ge further research on these tasks, and to serve the needs of qualitative humanities research into humour and metaphor. 2 2.1 Background and Related Work Humorousness The automatic processing of verbal humour has applications in human–computer interaction, machine and machine-assisted translation, and the digital humanities (Miller et al., 2017). To give just one example, an intelligent conversational agent should ideally detect and respond appropriately to comments made in jest. The vast majority of past approaches to the automatic recognition of humour (e.g., Mihalcea and Strapparava, 2006; Purandare and Litman, 2006; Sjöbergh and Araki, 2007; Mihalcea et al., 2010; Zhang and Liu, 2014; Yang et al., 2015; Miller et al., 2017; Mikhalkova and Karyakin, 2017; Chen and Soo, 2018) have framed the problem as a binary classification task, which is sufficient for the detection step of our example. However, the ability to assess the degree of humour embodied in an utterance may be necessary for the agent to make a contextually appropriate, humanlike response – for example, a groan for a terrible joke, a chuckle for a middling one, or uproarious laughter for a clever one. Only a few studies have dealt with determin"
P19-1572,L16-1076,0,0.0754559,"Missing"
P19-1572,P10-1071,0,0.025632,"that given sparse, crowdsourced annotation data, ranking using GPPL outperforms best–worst scaling. We release a new dataset for evaluating humour containing 28,210 pairwise comparisons of 4030 texts, and make our software freely available. 1 Introduction Creative language, such as humour and metaphor, is an essential part of everyday communication, yet remains a challenge for computational methods. Unlike much literal language, humour and figurative language require complex linguistic and background knowledge to understand, which are difficult to integrate with NLP methods (Hempelmann, 2008; Shutova, 2010). An important step in processing creative language is to recognise its presence in a piece of text. Humour and metaphors are two of the most frequently used types of creative language whose use most obscures the true meaning of a piece of text from its surface interpretation (Raskin, 1985, pp. 1– 5, 100–104; Black, 1955) and whose attributes, such as funniness and novelty, may be present or perceived to varying degrees (Bell, 2017; Dunn, 2010). For example, the level of appreciation (i.e., humorousness or equivalently funniness) of jokes can vary according to their content and structural feat"
P19-1572,J15-4002,0,0.032689,"present or perceived to varying degrees (Bell, 2017; Dunn, 2010). For example, the level of appreciation (i.e., humorousness or equivalently funniness) of jokes can vary according to their content and structural features, such as nonsense or disparagement (Carretero-Dios et al., 2010) or, in the case of puns, contextual coherence (Lippman and Dunn, 2000) and the cognitive effort required to recover the target word (Hempelmann, 2003, pp. 123–124). With metaphors, the literal meaning of frequently used metaphors can drop out of everyday usage, leaving the metaphorical sense as the expected one (Shutova, 2015). For such conventionalised metaphors, NLP methods may identify the metaphorical sense from training data or resources such as WordNet, whereas novel metaphors require the ability to recognise the analogy being made. While previous work (see §2) has considered mainly binary classification approaches to humour or metaphor recognition, this paper focuses on quantifying humorousness and metaphor novelty. These tasks are important for downstream applications such as conversational agents or machine translation, which must choose the correct tone in response to humour, or find appropriate metaphors"
P19-1572,Q18-1026,1,0.948407,"extensive sets of pairwise training labels. We apply these tasks to datasets for humorousness and metaphor novelty, which extend the datasets of Miller et al. (2017) and Do Dinh et al. (2018), respectively, and contain crowdsourced pairwise labels. As a baseline scoring method, we employ the scoring technique for best–worst scaling (BWS; Flynn and Marley, 2014), an established method that can also be applied to pairwise labels to estimate scores very efficiently. Our use of sparse, unreliable crowdsourced data motivates a second, Bayesian approach: Gaussian process preference learning (GPPL; Simpson and Gurevych, 2018), which exploits text features to boost performance when labels are sparse and make predictions for items not compared in the training set. Our main contributions are (1) four novel tasks for quantifying aspects of creative language, (2) an annotated dataset containing pairwise comparisons of humorousness between sentences, (3) a Bayesian approach for scoring short texts by humorousness and metaphor novelty given sparse pairwise annotations, and (4) an empirical investigation showing that word embeddings and linguistic features can be used to predict humorousness and metaphor novelty, and that"
P19-1572,E12-1003,0,0.0238297,"s point on, we refer to the counting procedure as BWS, and apply it to the tasks of inferring scores from both best– worst scaling annotations for metaphor novelty and pairwise annotations for funniness. To make predictions for unlabelled instances and cope better with sparse pairwise labels, Chu and Ghahramani (2005) proposed Gaussian process preference learning (GPPL), a Thurstone– Mosteller–based model that accounts for the features of the instances when inferring their scores. GPPL uses Bayesian inference, which has been shown to cope better with sparse and noisy data (Xiong et al., 2011; Titov and Klementiev, 2012; Beck et al., 2014; Lampos et al., 2014), including disagreements between multiple annotators (Cohn and Specia, 2013; Simpson et al., 2015; Felt et al., 2016; Kido and Okamoto, 2017). Through the random utility model, GPPL is able to handle disagreements between annotators as noise, since no label has a probability of one of being selected. Given a set of pairwise labels, and the features of labelled instances, GPPL can estimate the posterior distribution over the utilities of any instances given their features. Relationships between instances are modelled by a Gaussian process (GP), which co"
P19-1572,P14-1024,0,0.0355561,"s Potash et al.’s (2017) three. Mindful of psychological studies on subjective evaluations (Thurstone, 1927), Shahaf et al. (2015) reject the idea that such ordinal rating data can be treated as interval data, and argue that direct comparisons are preferable for humour judgements. 2.2 Metaphor Novelty Most previous work on metaphor detection has been conducted with a binary classification in mind (metaphor vs. literal). This dichotomy is reflected in more widely used datasets, such as the VU Amsterdam Metaphor Corpus (VUAMC; Steen et al., 2010) or the datasets in multiple languages created by Tsvetkov et al. (2014). Advantages include the wide variety of approaches that can be (and have been) employed for automatic detection and a rather straightforward annotation process. This usually also entails a high interannotator agreement, meaning that the annotations are reliable. In the case of VUAMC, this amounts to a Cohen’s κ of 0.80. However, the two-class modelling of metaphor has certain limits. These become obvious when looking at examples from the aforementioned datasets (see Table 2, which includes an example from VUAMC). In particular, many metaphors annotated in the binary datasets differ widely in"
P19-1572,D15-1284,0,0.0221112,"humour and metaphor. 2 2.1 Background and Related Work Humorousness The automatic processing of verbal humour has applications in human–computer interaction, machine and machine-assisted translation, and the digital humanities (Miller et al., 2017). To give just one example, an intelligent conversational agent should ideally detect and respond appropriately to comments made in jest. The vast majority of past approaches to the automatic recognition of humour (e.g., Mihalcea and Strapparava, 2006; Purandare and Litman, 2006; Sjöbergh and Araki, 2007; Mihalcea et al., 2010; Zhang and Liu, 2014; Yang et al., 2015; Miller et al., 2017; Mikhalkova and Karyakin, 2017; Chen and Soo, 2018) have framed the problem as a binary classification task, which is sufficient for the detection step of our example. However, the ability to assess the degree of humour embodied in an utterance may be necessary for the agent to make a contextually appropriate, humanlike response – for example, a groan for a terrible joke, a chuckle for a middling one, or uproarious laughter for a clever one. Only a few studies have dealt with determining the (relative) funniness of texts. Shahaf et al. (2015) presented a supervised system"
Q13-1013,E09-1005,0,0.203437,"lated Work The are two strands of closely related work: Similarity-based and graph-based approaches to word sense alignment. To our knowledge, there exists no previous work which fully represents both LSRs involved in an alignment as graphs. We give a summary of different approaches in Table 1. 3.1 Similarity-based Approaches Niemann and Gurevych (2011) and Meyer and Gurevych (2011) created WN-WP and WN-WKT alignments using a framework which first calculates the similarity of glosses (or glosses and articles in the case of WN-WP) using either cosine or personalized page rank (PPR) similarity (Agirre and Soroa, 2009) and then learns a threshold on the gold standard to classify each pair of senses as a (non-)valid alignment. This approach was later extended to cross-lingual alignment between the German OW and WN (Gurevych et al., 2012) using a machine translation component. However, its applicability depends on the availability and quality of the 153 glosses, which are not present in every case (e.g. for VN). Moreover, as it involves supervised machine learning, it requires the initial effort of manually annotating a sufficient amount of training data. Henrich et al. (2011) use a similar approach for align"
Q13-1013,de-melo-weikum-2010-providing,0,0.241892,"Missing"
Q13-1013,eckle-kohler-etal-2012-uby,1,0.864098,"2012) and semantic role labeling using a combination of PropBank, VN and FN in the SemLink project (Palmer, 2009). Some of these approaches to WSA either rely heavily on manual labor (e.g. Shi and Mihalcea (2005)) or on information which is only present in few resources such as the most frequent sense (MFS) (Suchanek et al., 2008). This makes it difficult to apply them to a larger set of resources. In earlier work, we presented the large-scale resource UBY (Gurevych et al., 2012). It contains nine resources in two languages which are mapped to a uniform representation using the LMF standard (Eckle-Kohler et al., 2012). They are thus structurally interoperable. UBY contains pairwise sense alignments between a subset of these resources, and this work also presented a framework for creating alignments based on the similarity of glosses (Meyer and Gurevych, 2011). However, it is not clear to what extent this approach can be applied to resources which lack this kind of information (see Section 3). In summary, aligning senses is a key requirement for semantic interoperability of LSRs to increase the 151 Transactions of the Association for Computational Linguistics, 1 (2013) 151–164. Action Editor: Patrick Pantel"
Q13-1013,E12-1059,1,0.455425,", and VerbNet (VN) (Shi and Mihalcea, 2005), word sense disambiguation using an alignment of WN and Wikipedia (WP) (Navigli and Ponzetto, 2012) and semantic role labeling using a combination of PropBank, VN and FN in the SemLink project (Palmer, 2009). Some of these approaches to WSA either rely heavily on manual labor (e.g. Shi and Mihalcea (2005)) or on information which is only present in few resources such as the most frequent sense (MFS) (Suchanek et al., 2008). This makes it difficult to apply them to a larger set of resources. In earlier work, we presented the large-scale resource UBY (Gurevych et al., 2012). It contains nine resources in two languages which are mapped to a uniform representation using the LMF standard (Eckle-Kohler et al., 2012). They are thus structurally interoperable. UBY contains pairwise sense alignments between a subset of these resources, and this work also presented a framework for creating alignments based on the similarity of glosses (Meyer and Gurevych, 2011). However, it is not clear to what extent this approach can be applied to resources which lack this kind of information (see Section 3). In summary, aligning senses is a key requirement for semantic interoperabili"
Q13-1013,W97-0802,0,0.128906,"able 1: Summary of various approaches to WSA. “NA” stands for “Not Available”. OmegaWiki (OW) is a freely editable online dictionary like WKT. However, there do not exist distinct language editions as OW is organized in language-independent concepts (“Defined Meanings”) to which lexicalizations in various languages are attached. These can be considered as multilingual synsets, and they are interconnected by unambiguous relations just like WN. As of February 2013, OW contains over 46,000 of these concepts and lexicalizations in over 400 languages. GermaNet (GN) is the German counterpart to WN (Hamp and Feldweg, 1997). It is also organized in synsets (around 70,000 in the latest version 7.0) which are connected via semantic relations. 3 Related Work The are two strands of closely related work: Similarity-based and graph-based approaches to word sense alignment. To our knowledge, there exists no previous work which fully represents both LSRs involved in an alignment as graphs. We give a summary of different approaches in Table 1. 3.1 Similarity-based Approaches Niemann and Gurevych (2011) and Meyer and Gurevych (2011) created WN-WP and WN-WKT alignments using a framework which first calculates the similarit"
Q13-1013,I11-1099,1,0.129267,"present in few resources such as the most frequent sense (MFS) (Suchanek et al., 2008). This makes it difficult to apply them to a larger set of resources. In earlier work, we presented the large-scale resource UBY (Gurevych et al., 2012). It contains nine resources in two languages which are mapped to a uniform representation using the LMF standard (Eckle-Kohler et al., 2012). They are thus structurally interoperable. UBY contains pairwise sense alignments between a subset of these resources, and this work also presented a framework for creating alignments based on the similarity of glosses (Meyer and Gurevych, 2011). However, it is not clear to what extent this approach can be applied to resources which lack this kind of information (see Section 3). In summary, aligning senses is a key requirement for semantic interoperability of LSRs to increase the 151 Transactions of the Association for Computational Linguistics, 1 (2013) 151–164. Action Editor: Patrick Pantel. c Submitted 12/2012; Revised 2/2013; Published 5/2013. 2013 Association for Computational Linguistics. coverage and effectiveness in NLP tasks. Still, existing efforts are mostly focused on specific types of resources (most often requiring glos"
Q13-1013,E09-1068,0,0.400442,"Wiktionary (WKT) is the dictionary pendant to WP. By February 2013 the English WKT contained over 3,200,000 article pages, while the German edition contained over 200,000 ones. For each word, multiple senses can be encoded. Similar to WN, they are represented by a gloss and usage examples. There also exist hyperlinks to synonyms, hypernyms, meronyms etc. The targets of these relations are not senses, however, but merely lexemes (i.e. the relations are not disambiguated). Meyer and Gurevych (2011) Niemann and Gurevych (2011) Henrich et al. (2011) de Melo and Weikum (2010) Laparra et al. (2010) Navigli (2009) Ponzetto and Navigli (2009) Navigli and Ponzetto (2012) LSRs WN-WKT WN-WP GN-WKT WN-WP FN-WN WN WN-WP WN-WP P /R/F1 /Acc. 0.67/0.65/0.66/0.91 0.78/0.78/0.78/0.95 0.84/0.85/0.84/0.94 0.86/NA/NA/NA 0.79/0.79/0.79/NA 0.64/0.64/0.64/NA NA/NA/NA/0.81 0.81/0.75/0.78/0.83 Approach Gloss similarity + Machine learning Gloss similarity + Machine learning Pseudo-gloss overlap Gloss/article overlap Dijkstra-SSI+ (WSD algorithm) Graph-based WSD of WN glosses Graph-based, only for WP categories Graph-based WSA using WN relations Table 1: Summary of various approaches to WSA. “NA” stands for “Not Available”"
Q13-1013,W11-0122,1,0.464397,"ment, or alignment for short, is formally defined as a list of pairs of senses from 152 two LSRs. A pair of aligned senses denote the same meaning. E.g., the two senses of letter “The conventional characters of the alphabet used to represent speech” and “A symbol in an alphabet, bookstave” (taken from WN and WKT, respectively) are clearly equivalent and should be aligned. 2.2 Evaluation Resources For the evaluation of Dijkstra-WSA, we align four pairs of LSRs used in previous work, namely WNOW (Gurevych et al., 2012), WN-WKT (Meyer and Gurevych, 2011), GN-WKT (Henrich et al., 2011) and WN-WP (Niemann and Gurevych, 2011). Our goal is to cover resources with different characteristics: Expert-built (WN, GN) and collaboratively constructed LSRs (WP, WKT, OW), resources in different languages (English and German) and also resources with few sense descriptions (GN) or semantic relations (WKT). We contrastively discuss the results of the Dijkstra-WSA algorithm on these different datasets and relate the results to the properties of the LSRs involved. Moreover, using existing datasets ensures comparability to previous work which discusses only one dataset at a time. WordNet (WN) (Fellbaum, 1998) is a lexical resource"
Q14-1040,W11-1407,0,0.0588452,"nly recognition. However, Jakschik et al. (2010) transform the C-test 518 into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test beca"
Q14-1040,I13-1112,1,0.841584,"e words might not be part of the students active vocabulary and are only guessed because they occur as cognates in the students L1. This is supported by the fact that many of the cognate answers resemble orthographic principles from other languages, e.g. for skeletons we find *skellets, *skelleton(s), *skelets, *skelletts, *skeletton(s), *skeltons, *skeletes, and *skelette(s).11 In order to account for this phenomenon, we estimate the cognateness of words by gathering data from four different lists. We retrieve cognates from UBY using string similarity and from a cognate production algorithm (Beinborn et al., 2013). In addition, we consult the COCA list of academic words12 and a list of words with latin roots.13 Inflection Many errors are caused by wrong morphological inflection as in this example: And in har times like these, ... [harder] The base form hard (72) is provided more often than the correct comparative harder (48), although it is too short. Other inflection errors are caused by singular/plural and adjective/adverb confusion. In order to account for this phenomenon, we test whether the solution is in lemma form or carries any inflection markers using a lemmatizer. We also check whether the wo"
Q14-1040,H05-1103,0,0.258217,"ltiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that de"
Q14-1040,P14-5011,1,0.800424,"Missing"
Q14-1040,W14-5201,1,0.76658,"Missing"
Q14-1040,E12-1059,1,0.751574,"ompound (e.g. coastline) because the prefix only provides information about the first part of the word. In our approach, compounds are detected using a word splitting algorithm with an English dictionary.8 Another issue are polysemous words, as learners might know one sense of a word but not be aware of the existence of a second sense. Polysemy interferes with frequency, e.g. the word well has a high frequency, but it occurs only rarely in its sense fountain. In order to account for polysemy, we count the number of represented word senses for the solution in the lexical-semantic resource UBY (Gurevych et al., 2012). The two senses of well also differ in their word class. The word class has been studied as a difficulty indicator by several researchers but with mixed results. Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners. Sigott (1995) could not confirm any effect of the word class on C-test difficulty. 6 In all examples, we only highlight a single gap to illustrate a certain phenomenon. 7 http://www.grsampson.net/RSue.html 8 http://www.danielnaber.de/jwordsplitter/index en.html 521 The word class is determined"
Q14-1040,N07-1058,0,0.0251899,"n test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search fo"
Q14-1040,W12-2016,0,0.135917,"ing ability examining only recognition. However, Jakschik et al. (2010) transform the C-test 518 into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the d"
Q14-1040,P11-1027,0,0.0135175,"e student. This feature is comparable to the semantic cache used by Brown (1989). Phonetic complexity Wrong answer variants for C-test gaps are often rooted in phonetic problems. The spelling of a word is more difficult, if it contains a rare sequence of characters. The word appropriate, for example, triggers 69 different answers, 40 of them were provided only once. In addition, a spelling error is more likely to occur, in words with rare grapheme-phoneme mapping as in Wednesday. We build a character-based language model that indicates the probability of a character sequence using BerkeleyLM (Pauls and Klein, 2011). In addition, we build a phonetic model using phonetisaurus, a statistical alignment algorithm that maps characters onto phonemes.14 Both models are trained only on words from the Basic English list in order to reflect the knowledge of a language learner.15 Based on this scarce data, the phonetic model only learns the most frequent character-tophoneme mappings and assigns higher phonetic scores to less general letter sequences. We use this score as a feature and additionally calculate the string similarity between the output and the correct pronunciation in the CMU dictionary.16 Another sourc"
Q14-1040,W12-2017,0,0.102172,"riant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected"
Q14-1040,P13-2043,0,0.0718477,"ty: the application of relaxed scoring schemes and the use of distractors. In relaxed scoring, teachers accept all tolerable candidates for a gap and not only the intended solution as in exact scoring. Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002). The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked. Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language"
Q14-1040,W10-1007,0,0.159893,"518 into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific"
Q14-1040,W14-1817,1,0.850075,"elaxed scoring schemes and the use of distractors. In relaxed scoring, teachers accept all tolerable candidates for a gap and not only the intended solution as in exact scoring. Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002). The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked. Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary,"
Q16-1015,D14-1159,0,0.0192613,"o sense of guiltEmotion in the betrayal of personal confidence. Introduction In this work, we present a novel approach to automatically generate training data for semantic role labeling (SRL). It follows the distant supervision paradigm and performs knowledge-based label transfer from rich external knowledge sources to large corpora. SRL has been shown to improve many NLP applications that rely on a deeper understanding of semantics, such as question answering, machine translation or recent work on classifying stance and reason in online debates (Hasan and Ng, 2014) and reading comprehension (Berant et al., 2014). Even though unsupervised approaches continue to gain popularity, SRL is typically still solved using supervised training on labeled data. Creating such labeled data requires manual annotations by experts,1 1 Even though crowdsourcing has been used, it is still probOur novel approach to training data generation for FrameNet SRL uses the paradigm of distant supervision (Mintz et al., 2009) which has become popular in relation extraction. In distant supervision, the overall goal is to align text and a knowledge base, using some notion of similarity. Such an alignment allows us to transfer infor"
Q16-1015,W13-5503,0,0.0690203,"Missing"
Q16-1015,burchardt-pennacchiotti-2008-fate,0,0.0235989,"a brief description of verbs senses s/v inst(s) inst(r) Fate MASC Semeval FNFT-test 526 44 278 424 725 143 335 527 1.4 3.3 1.2 1.2 1,326 2,012 644 1,235 3,490 4,142 1,582 3,078 FNFT-dev 490 598 1.2 1,450 3,857 Table 3: Test dataset statistics on verbs; inst(s/r): number of ambiguous sense and role instances in the datasets. each dataset follows. We use the frame and role annotations in the Semeval 2010 task 10 evaluation and trial dataset (Ruppenhofer et al., 2010). It consists of literature texts. The Fate corpus contains frame annotations on the RTE-2 textual entailment challenge test set (Burchardt and Pennacchiotti, 2008). It is based on newspaper texts, texts from information extraction datasets such as ACE, MUC-4, and texts from question answering datasets such as CLEF and TREC. These two datasets were created prior to the release of FrameNet 1.5. For those sets, only senses (verb-frame combinations) that still occur in FrameNet 1.5 and their roles were included in the evaluation. The MASC WordSense sentence corpus (Passonneau et al., 2012) is a balanced corpus that contains sense annotations for 1000 instances of 100 words from the MASC corpus. It contains WordNet sense labels, we use a slightly smaller sub"
Q16-1015,burchardt-etal-2006-salsa,0,0.111474,"Missing"
Q16-1015,E14-1008,1,0.141826,"meNet. For the task of lematic for SRL labeling: the task is very complex, which results in manually adapted definitions (Fossati et al., 2013), or constrained role sets (Feizabadi and Pad´o, 2014). 197 Transactions of the Association for Computational Linguistics, vol. 4, pp. 197–213, 2016. Action Editor: Yuji Matsumoto. Submission batch: 9/2015; Revision batch: 1/2016; Published 5/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. word sense disambiguation (WSD), recent work on automatic training data generation based on LLR has only used WordNet (Cholakov et al., 2014), not considering other sense inventories such as FrameNet. Our distant supervision approach for automatic training data generation employs two types of knowledge sources: LLRs and linguistic knowledge formalized as rules to create data labeled with FrameNet senses and roles. It relies on large corpora, because we attach labels to corpus instances only sparsely. We generate training data for two commonly distinguished subtasks of SRL: first, for disambiguation of the frame-evoking lexical element relative to the FrameNet sense inventory, a WSD task; and second, for argument identification and"
Q16-1015,P11-1144,0,0.382253,".605 instances senses verbs for threshold t, and filter f result in a high-quality training corpus, we perform an extrinsic evaluation on a development set: we use a set of automatically labeled corpora based on ukWAC section 1 generated with different threshold values to train a verb sense disambiguation (VSD) system. We evaluate precision P (the number of correct instances/number of labeled instances), recall R (the number of labeled instances/all instances), and F1 (harmonic mean of P and R) of the systems on the development-split FNFTdev of the FrameNet 1.5 fulltext corpus (FNFT), used by Das and Smith (2011). A detailed description of the VSD system follows in the next section. We varied the thresholds of the discriminating filter f (Step 1A) and the threshold t (Step 1B) on the values (0.07, 0.1, 0.14, 0.2), as was suggested by Cholakov et al. (2014) for t. We also compare corpora with and without the discriminating filter f . To save space, we only report results with f for t = 0.2 in Table 1. As expected, increasing the pattern similarity threshold t at which a corpus sentence is labeled with a sense increases the precision at the cost of recall. Similarly, employing a discriminating filter f"
Q16-1015,N10-1138,0,0.0448314,"Missing"
Q16-1015,J14-1002,0,0.234409,"Missing"
Q16-1015,de-marneffe-etal-2006-generating,0,0.0608477,"Missing"
Q16-1015,W14-5201,1,0.872272,"Missing"
Q16-1015,E14-4044,0,0.0448908,"Missing"
Q16-1015,D15-1112,0,0.0343097,"Missing"
Q16-1015,P13-2130,0,0.0296987,"otstrap learning (Yarowsky, 1995), distant supervision creates training data in a single run. A particular type of knowledge base relevant for distant supervision are linked lexical resources (LLRs): integrated lexical resources that combine several resources (e.g., WordNet, FrameNet, Wiktionary) by linking them on the sense or on the role level. Previous approaches to generating training data for SRL (F¨urstenau and Lapata, 2012) do not use lexical resources apart from FrameNet. For the task of lematic for SRL labeling: the task is very complex, which results in manually adapted definitions (Fossati et al., 2013), or constrained role sets (Feizabadi and Pad´o, 2014). 197 Transactions of the Association for Computational Linguistics, vol. 4, pp. 197–213, 2016. Action Editor: Yuji Matsumoto. Submission batch: 9/2015; Revision batch: 1/2016; Published 5/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. word sense disambiguation (WSD), recent work on automatic training data generation based on LLR has only used WordNet (Cholakov et al., 2014), not considering other sense inventories such as FrameNet. Our distant supervision approach for automatic training data"
Q16-1015,J12-1005,0,0.221023,"Missing"
Q16-1015,W97-0802,0,0.186487,"he WaSR corpora shown in Table 7. The evaluation shows that our approach can be applied to German. For VSD, the automatically labeled 207 data can be used to improve on using S-train alone; improvements in precision are not significant, which has several potential causes, e.g., the smaller set of LLRs used for seed pattern extraction compared to English, and the smaller size of the resulting corpora. The smaller corpora also result in very low recall for the role classification. Future work could be to extend the German dataset by adding additional resources to the LLR, for instance GermaNet (Hamp and Feldweg, 1997). Extending the SemLink mapping to frames unique to SALSA should additionally contribute to an improved role coverage. 6 Related Work Relevant related work is research on (i) the automatic acquisition of sense-labeled data for verbs, (ii) the automatic acquisition of role-labeled data for FrameNet SRL, and (iii) approaches to FrameNet SRL using lexical resources and LLRs, including rule-based and knowledge-based approaches. Automatic acquisition of sense-labeled data. Most previous work on automatically sense-labeling corpora for WSD focussed on nouns and WordNet as a sense inventory, e.g., Le"
Q16-1015,P13-1134,1,0.84126,"d test instances is 0.6, 0.2, 0.2; data statistics are shown in Table 7. The unlabeled corpus used is based on deWAC sections 1-5 (Baroni et al., 2009). VSD corpus and evaluation. The LLR used to generate more than 22,000 seed patterns consists verbs senses roles inst(s) inst(r) S-test S-dev S-train 390 390 458 684 678 1,167 1,045 1,071 1,511 3,414 3,516 9460 8,010 8,139 22,669 WaS-de (t=0.07) WaSR-de-set WaSR-de-uni 333 920 - 602,207 - 193 172 277 241 155 210 80,370 115,332 51,241 57,822 Table 7: German dataset statistics on verbs. of S-train and the German Wiktionary based on the linking by Hartmann and Gurevych (2013). DeWAC is labeled based on those patterns, and the thresholds t and f are determined in a VSD task on S-dev using a subset of the corpus based on sections 1-3. The threshold t=0.07 together with a discriminating filter of f =0.07 result in the best precision, and t=0.07 in the best F1 score. Therefore, we perform extrinsic evaluation in a VSD task on S-test with WaS-de (t=0.07) and on the combinations U-WaS-de (union with S-train) and B-WaS-de (backoff-variant). The results in Table 8 show that the performance of the WaS-de-based system is worse than the Strain-based one, but the backoff vers"
Q16-1015,D14-1083,0,0.0347127,"ervised setting. 1 HeExperiencer feltF eeling no sense of guiltEmotion in the betrayal of personal confidence. Introduction In this work, we present a novel approach to automatically generate training data for semantic role labeling (SRL). It follows the distant supervision paradigm and performs knowledge-based label transfer from rich external knowledge sources to large corpora. SRL has been shown to improve many NLP applications that rely on a deeper understanding of semantics, such as question answering, machine translation or recent work on classifying stance and reason in online debates (Hasan and Ng, 2014) and reading comprehension (Berant et al., 2014). Even though unsupervised approaches continue to gain popularity, SRL is typically still solved using supervised training on labeled data. Creating such labeled data requires manual annotations by experts,1 1 Even though crowdsourcing has been used, it is still probOur novel approach to training data generation for FrameNet SRL uses the paradigm of distant supervision (Mintz et al., 2009) which has become popular in relation extraction. In distant supervision, the overall goal is to align text and a knowledge base, using some notion of similarit"
Q16-1015,D15-1076,0,0.0472722,"h as deep learning are used. In section 7, we discussed in detail how our method relates to and complements the most recent developments in FrameNet SRL. It would be interesting to evaluate the benefits that our automatically labeled data can add to an advanced SRL system. We expect particularly strong benefits in the context of domain adaptation: currently, FrameNet SRL systems are only evaluted on in-domain test data. Our method can be adapted to other sense and role inventories covered by LLRs (e.g., VerbNet and PropBank) and to related approaches to SRL and semantic parsing (e.g., QA-SRL (He et al., 2015)); the latter requires a mapping of the role inventory to a suitable LLR, for instance mapping the role labels in QA-SRL to SemLink. We would also like to evaluate our approach in comparison to other methods for training data generation, for instance methods based on alignments (F¨urstenau and Lapata, 2012), or paraphrasing (Woodsend and Lapata, 2014). 9 Conclusion We presented a novel approach to automatically generate training data for FrameNet SRL. It follows the distant supervision paradigm and performs knowledge-based label transfer from rich external knowledge sources to large-scale corp"
Q16-1015,P14-1136,0,0.0266025,"Missing"
Q16-1015,P15-2036,0,0.0976962,"Missing"
Q16-1015,R09-1037,0,0.0726162,"Missing"
Q16-1015,J98-1006,0,0.0824107,"Missing"
Q16-1015,S10-1066,0,0.0198187,"Missing"
Q16-1015,P09-1113,0,0.0464934,"rely on a deeper understanding of semantics, such as question answering, machine translation or recent work on classifying stance and reason in online debates (Hasan and Ng, 2014) and reading comprehension (Berant et al., 2014). Even though unsupervised approaches continue to gain popularity, SRL is typically still solved using supervised training on labeled data. Creating such labeled data requires manual annotations by experts,1 1 Even though crowdsourcing has been used, it is still probOur novel approach to training data generation for FrameNet SRL uses the paradigm of distant supervision (Mintz et al., 2009) which has become popular in relation extraction. In distant supervision, the overall goal is to align text and a knowledge base, using some notion of similarity. Such an alignment allows us to transfer information from the knowledge base to the text, and this information can serve as labeling for supervised learning. Hence, unlike semi-supervised methods which typically employ a supervised classifier and a small number of seed instances to do bootstrap learning (Yarowsky, 1995), distant supervision creates training data in a single run. A particular type of knowledge base relevant for distant"
Q16-1015,passonneau-etal-2012-masc,0,0.0302933,"aset (Ruppenhofer et al., 2010). It consists of literature texts. The Fate corpus contains frame annotations on the RTE-2 textual entailment challenge test set (Burchardt and Pennacchiotti, 2008). It is based on newspaper texts, texts from information extraction datasets such as ACE, MUC-4, and texts from question answering datasets such as CLEF and TREC. These two datasets were created prior to the release of FrameNet 1.5. For those sets, only senses (verb-frame combinations) that still occur in FrameNet 1.5 and their roles were included in the evaluation. The MASC WordSense sentence corpus (Passonneau et al., 2012) is a balanced corpus that contains sense annotations for 1000 instances of 100 words from the MASC corpus. It contains WordNet sense labels, we use a slightly smaller subset of verbs annotated with FrameNet 1.5 labels.4 We also evaluate on the test-split FNFT-test of the FrameNet fulltext corpus used in Das and Smith (2011). 3.3 VSD results and analysis. Impact of pattern filters. A comparison of results between the WaS corpora (first block of Table 4) shows that the filters in WaS L improve precision for three out of four test sets, which shows that stronger filtering can benefit precision-o"
Q16-1015,P15-2010,0,0.0613969,"Missing"
Q16-1015,popescu-etal-2014-mapping,0,0.0306915,"s the quality of the automatically labeled corpus. Essentially, our discriminating filter integrates the goal of capturing sense distinctions into our approach. The same goal is pursued by Corpus Analysis Patterns (CPA patterns, Hanks (2013)), which have been created to capture sense distinctions in word usage by combining argument structures, collocations and an ontology of semantic types for arguments. In contrast to our fully automatic approach, developing CPA patterns based on corpus evidence was a lexicographic effort. The following example compares two ASP patterns to a CPA pattern from Popescu et al. (2014): 1. CPA: [[Human]] |[[Institution]] abandon [[Activity]] |[[Plan]] 2. ASP: JJ person abandon JJ cognition of JJ quantity 3. ASP: person abandon communication which VVD PP JJ in Our abstract ASP patterns look similar, as they also abstract argument fillers to semantic classes and preserve certain function words. 200 Step 1B: Sense label transfer. Using the approach of Cholakov et al. (2014), we create sense patterns ruj from all sentences uj of an unlabeled corpus that contain a target verb, for instance the sentence u1 : I feel strangely sad and low-spirited today for the verb feel. For every"
Q16-1015,Q15-1032,0,0.0290265,"Missing"
Q16-1015,S10-1008,0,0.0217389,"and Smith (2011). Test data. For evaluation, we used four different FrameNet-labeled datasets. The statistics of the test datasets are compiled in Table 3, a brief description of verbs senses s/v inst(s) inst(r) Fate MASC Semeval FNFT-test 526 44 278 424 725 143 335 527 1.4 3.3 1.2 1.2 1,326 2,012 644 1,235 3,490 4,142 1,582 3,078 FNFT-dev 490 598 1.2 1,450 3,857 Table 3: Test dataset statistics on verbs; inst(s/r): number of ambiguous sense and role instances in the datasets. each dataset follows. We use the frame and role annotations in the Semeval 2010 task 10 evaluation and trial dataset (Ruppenhofer et al., 2010). It consists of literature texts. The Fate corpus contains frame annotations on the RTE-2 textual entailment challenge test set (Burchardt and Pennacchiotti, 2008). It is based on newspaper texts, texts from information extraction datasets such as ACE, MUC-4, and texts from question answering datasets such as CLEF and TREC. These two datasets were created prior to the release of FrameNet 1.5. For those sets, only senses (verb-frame combinations) that still occur in FrameNet 1.5 and their roles were included in the evaluation. The MASC WordSense sentence corpus (Passonneau et al., 2012) is a b"
Q16-1015,seeker-kuhn-2012-making,0,0.0150657,"de (union with S-train) and B-WaS-de (backoff-variant). The results in Table 8 show that the performance of the WaS-de-based system is worse than the Strain-based one, but the backoff version reaches best scores allover, indicating that our WaS-de corpora are complementary to S-train. P R F1 WaS-de 0.672* 0.912* B-WaS-de 0.711 0.958* U-WaS-de 0.676* 0.961* 0.773 0.816 0.794 S-train 0.809 0.707 0.946 Table 8: German VSD P, R, F1; * marks significant differences to S-train. SRL corpus and evaluation. We adapt the rulebased VerbNet role-labeling to German dependencies from the mate-tools parser (Seeker and Kuhn, 2012), and perform Steps 2A and 2B on WaS-de, resulting in WaSR-de-set/uni (see Table 7). We train our role classification system on the corpora in order to evaluate them extrinsically. Training on WaSR-de-uni results in precision of 0.69 – better than for English, but still significantly lower than for the S-train system with 0.828. Recall is very low at 0.17. This is due to the low role coverage of the WaSR corpora shown in Table 7. The evaluation shows that our approach can be applied to German. For VSD, the automatically labeled 207 data can be used to improve on using S-train alone; improvemen"
Q16-1015,N04-3006,0,0.155911,"Missing"
Q16-1015,Q15-1003,0,0.157247,"Missing"
Q16-1015,P95-1026,0,0.414379,"ll probOur novel approach to training data generation for FrameNet SRL uses the paradigm of distant supervision (Mintz et al., 2009) which has become popular in relation extraction. In distant supervision, the overall goal is to align text and a knowledge base, using some notion of similarity. Such an alignment allows us to transfer information from the knowledge base to the text, and this information can serve as labeling for supervised learning. Hence, unlike semi-supervised methods which typically employ a supervised classifier and a small number of seed instances to do bootstrap learning (Yarowsky, 1995), distant supervision creates training data in a single run. A particular type of knowledge base relevant for distant supervision are linked lexical resources (LLRs): integrated lexical resources that combine several resources (e.g., WordNet, FrameNet, Wiktionary) by linking them on the sense or on the role level. Previous approaches to generating training data for SRL (F¨urstenau and Lapata, 2012) do not use lexical resources apart from FrameNet. For the task of lematic for SRL labeling: the task is very complex, which results in manually adapted definitions (Fossati et al., 2013), or constra"
Q16-1015,N10-1088,0,\N,Missing
Q16-1015,J13-3006,0,\N,Missing
Q18-1006,P14-2082,0,0.105578,"Missing"
Q18-1006,D08-1073,0,0.568025,"Missing"
Q18-1006,D16-1200,0,0.018124,"us with a specific time expression, we will output this date. If our system returns that it happened before and after a certain date, it will output the year and month if both dates are in the same month. If both dates are in the same year but in different months, it will output the year. Events with predicted timespans of over more than one year are rejected. For Multi-Day Events, we only use the begin point as only this information was annotated for this shared task. Two teams participated in the shared task (GPLSIUA and HeidelToul). Currently, the best published performance was achieved by Cornegruta and Vlachos (2016) with an F1 -score of 28.58. Our system was able to improve the total F1 -score by 4.01 points as depicted in Table 7. A challenge for our system is the different anchoring of events in time: while our system can anchor events at two arbitrary dates, the SemEval-2015 Task 4 only anchors events either at a specific day, month 87 System Our approach Cornegruta GPLSIUA 1 HeidelToul 2 Airbus 30.37 25.65 22.35 16.50 GM 28.83 26.64 19.28 10.94 Stock 38.01 32.35 33.59 25.89 Total 32.59 28.58 25.36 18.34 Table 7: Performance of our system on the SemEval-2015 Task 4 Track B for the topics Airbus, Gener"
Q18-1006,D12-1062,0,0.0688778,"Missing"
Q18-1006,P15-1061,0,0.0160795,"Neural Networks Architecture (Lecun, 1989) depicted in Figure 2. The Narrow Down classifier is a simple, hand-crafted, rule-based classifier described in Section 4.2.6. 4.2.1 Neural Network Architecture We use the same neural network architecture with slightly different configurations for the different local classifiers. The architecture is depicted in Figure 2 and is described in the following sections. The neural network architecture is based on the design proposed by Zeng et al. (2014), which can achieve state-of-the-art performance on relation classification tasks (Zeng et al., 2014; dos Santos et al., 2015). The neural network applies a convolution over the word representations and position embeddings of the input text followed by a max-over-time pooling layer. We call the output of this layer Input Text Features. Those Input Text Features are merged with the word embedding for the event and time expression token. The merged input is fed into a hidden layer using either the hyperbolic tangent tanh(·) or a rectified linear unit (ReLU) as activation function. The choice of the activation function is a hyperparameter and was optimized on a development set. The final layer is either a single sigmoid"
Q18-1006,P12-1010,0,0.593059,"Missing"
Q18-1006,P14-2050,0,0.012282,"sing either the hyperbolic tangent tanh(·) or a rectified linear unit (ReLU) as activation function. The choice of the activation function is a hyperparameter and was optimized on a development set. The final layer is either a single sigmoid neuron, in the case of binary classification, or a softmax layer. To avoid overfitting, we used two dropout layers (Srivastava et al., 2014), the first before the dense hidden layer and the second after the dense hidden layer. The percentages of the dropouts were set as hyperparameters. Word Embeddings. We used the pre-trained word embeddings presented by Levy and Goldberg (2014). The embedding layer of our neural networks maps each token from the input text to their respective word embedding. Out-of-vocabulary tokens are Figure 1: Tree structure used to extract the temporal information for an event. Rectangles are local classifiers based on deep convolutional neural networks except for the Narrow Down rectangles, which are simple rule based classifiers. Figure 2: The neural network architecture used for the different local classifiers. replaced with a special UNKNOWN token, for which the word embedding was randomly initialized. Position Embeddings. Collobert et al. ("
Q18-1006,D17-1035,1,0.862572,"Missing"
Q18-1006,P16-1207,1,0.918425,"ent must have happened after the date. A combination of before and after is possible. For Multi-Day Events, the annotators were asked to provide the begin and the end point of the event. As for Single Day Events, they were allowed to use the before and after notation in the case the explicit begin/end point is not mentioned in the document. The annotated corpus contains news articles and TV broadcast transcripts from various sources written mainly between January and April 1998. The shortest document has five sentences, while the longest has 63 sentences. A label distribution can be found in (Reimers et al., 2016). 3 http://www.usna.edu/Users/cs/nchamber/ caevo/ 79 Event Time Annotation 4 Automatic Event Time Extraction In this section we first present our hierarchical tree approach to automatically infer the event times in 4 The most informative temporal expression is defined as the temporal expression giving the reader the information at which date, or in which time frame, the event happened. a document. In Section 4.3 we present two baselines that we use for comparison: the first uses dense TLINKs extracted by the CAEVO system and the second baseline is a reduced version of the presented tree approa"
Q18-1006,D15-1063,0,0.0604913,"Missing"
Q18-1006,S10-1062,0,0.288251,"Missing"
Q18-1006,S13-2001,0,0.159741,"Missing"
Q18-1006,S07-1014,0,0.290865,"Missing"
Q18-1006,P09-1046,0,0.173865,"Missing"
Q18-1006,C14-1220,0,0.0364976,"ferent local classifiers applied in our tree structure. For all except the Narrow Down classifier, we used the Convolutional Neural Networks Architecture (Lecun, 1989) depicted in Figure 2. The Narrow Down classifier is a simple, hand-crafted, rule-based classifier described in Section 4.2.6. 4.2.1 Neural Network Architecture We use the same neural network architecture with slightly different configurations for the different local classifiers. The architecture is depicted in Figure 2 and is described in the following sections. The neural network architecture is based on the design proposed by Zeng et al. (2014), which can achieve state-of-the-art performance on relation classification tasks (Zeng et al., 2014; dos Santos et al., 2015). The neural network applies a convolution over the word representations and position embeddings of the input text followed by a max-over-time pooling layer. We call the output of this layer Input Text Features. Those Input Text Features are merged with the word embedding for the event and time expression token. The merged input is fed into a hidden layer using either the hyperbolic tangent tanh(·) or a rectified linear unit (ReLU) as activation function. The choice of"
Q18-1006,Q14-1022,0,\N,Missing
Q18-1026,D14-1190,0,0.0596433,"make use of Gaussian processes (GPs), which are distributions over functions of input features. GPs are nonparametric, meaning they can model highly nonlinear functions by allowing function complexity to grow with the amount of data (Rasmussen and 359 Williams, 2006). They account for model uncertainty when extrapolating from sparse training data and can be incorporated into larger graphical models. Example applications include analyzing the relationship between a user’s impact on Twitter and the textual features of their tweets (Lampos et al., 2014), predicting the level of emotion in text (Beck et al., 2014), and estimating the quality of machine translations given source and translated texts (Cohn and Specia, 2013). 4 Preference Learning Our aim is to develop a Bayesian method for identifying convincing arguments given their features, which can be trained on noisy pairwise labels. Each label, i  j, states that an argument, i, is more convincing than argument, j. This learning task is a form of preference learning, which can be addressed in several ways. A simple approach is to use a generic classifier by obtaining a single feature vector for each pair in the training and test datasets, either b"
Q18-1026,P13-1004,0,0.0559098,"onparametric, meaning they can model highly nonlinear functions by allowing function complexity to grow with the amount of data (Rasmussen and 359 Williams, 2006). They account for model uncertainty when extrapolating from sparse training data and can be incorporated into larger graphical models. Example applications include analyzing the relationship between a user’s impact on Twitter and the textual features of their tweets (Lampos et al., 2014), predicting the level of emotion in text (Beck et al., 2014), and estimating the quality of machine translations given source and translated texts (Cohn and Specia, 2013). 4 Preference Learning Our aim is to develop a Bayesian method for identifying convincing arguments given their features, which can be trained on noisy pairwise labels. Each label, i  j, states that an argument, i, is more convincing than argument, j. This learning task is a form of preference learning, which can be addressed in several ways. A simple approach is to use a generic classifier by obtaining a single feature vector for each pair in the training and test datasets, either by concatenating the feature vectors of the items in the pair, or by computing the difference of the two featur"
Q18-1026,C16-1168,0,0.160934,"Missing"
Q18-1026,P16-1150,1,0.368137,"binary votes from multiple people (Wei et al., 2016a; Tan et al., 2016). However, these approaches are limited by the cost of training annotators, a highly restricted set of categories, or the need for multiple annotators per document. 357 Transactions of the Association for Computational Linguistics, vol. 6, pp. 357–371, 2018. Action Editor: Daichi Mochihashi. Submission batch: 9/2017; Revision batch: 1/2018; Published 6/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. An alternative way to judge arguments is to compare them against one another (Habernal and Gurevych, 2016). When comparing the arguments in Figure 1, we may judge that argument 1 is less convincing due to its writing style, whereas argument 2 presents evidence in the form of historical events. Pairwise comparisons such as this are known to place less cognitive burden on human annotators than choosing a numerical rating and allow fine-grained sorting of items that is not possible with categorical labels (Kendall, 1948; Kingsley and Brown, 2010). Unlike numerical ratings, pairwise comparisons are not affected by different annotators’ biases toward high, low or middling values, or an individual’s bia"
Q18-1026,N13-1132,0,0.0459081,"Tan et al., 2016; Rosenfeld and Kraus, 2016; Monteserin and Amandi, 2013), but this work focuses on predicting salience of an argument given the state of the debate, rather than the qualities of arguments. Wachsmuth et al. (2017) recently showed that relative comparisons of argument convincingness correlate with theory-derived quality ratings. Habernal and Gurevych (2016) established datasets containing crowdsourced pairwise judgments of convincingness for arguments taken from online discussions. Errors in the crowdsourced data were handled by determining gold labels using the MACE algorithm (Hovy et al., 2013). The gold labels were then used to train SVM and bi-directional long short-term memory (BiLSTM) classifiers to predict pairwise labels for new arguments. The gold labels were also used to construct a directed graph of convincingness, which was input to PageRank to produce scores for each argument. These scores were then used to train SVM and BiLSTM regression models. A drawback of such pipeline approaches is that they are prone to error propagation (Chen and Ng, 2016), and consensus algorithms, such as MACE, require multiple crowdsourced labels for each argument pair, which increases annotati"
Q18-1026,P16-1089,0,0.0695974,"Missing"
Q18-1026,E14-1043,0,0.117571,"Missing"
Q18-1026,E17-1070,0,0.0600489,"58 learning set-up. Our software is publicly available1 . The rest of the paper is structured as follows. Section 2 reviews related work on argumentation, then Section 3 motivates the use of Bayesian methods by discussing their successful applications in NLP. In Section 4, we review preference learning methods and then Section 5 describes our scalable Gaussian process-based approach. Section 6 presents our evaluation, comparing our method to the state-of-the art and testing with noisy data and active learning. Finally, we present conclusions and future work. 2 Identifying Convincing Arguments Lukin et al. (2017) demonstrated that an audience’s personality and prior stance affect an argument’s persuasiveness, but they were unable to predict belief change to a high degree of accuracy. Related work has shown how persuasiveness is also affected by the sequence of arguments in a discussion (Tan et al., 2016; Rosenfeld and Kraus, 2016; Monteserin and Amandi, 2013), but this work focuses on predicting salience of an argument given the state of the debate, rather than the qualities of arguments. Wachsmuth et al. (2017) recently showed that relative comparisons of argument convincingness correlate with theory"
Q18-1026,E12-1003,0,0.042481,"uce performance (Xiong et al., 2011). Bayesian methods can be trained using unsupervised or semi-supervised learning to take advantage of structure in unlabeled data when labeled data is in short supply. Popular examples in NLP are Latent Dirichlet Allocation (LDA) (Blei et al., 2003), which is used for topic modelling, and its extension, the hierarchical Dirichlet process (HDP) (Teh et al., 2005), which learns the number of topics rather than requiring it to be fixed a priori. Semisupervised Bayesian learning has also been used to achieve state-of-the-art results for semantic role labelling (Titov and Klementiev, 2012). We can combine independent pieces of weak evidence using Bayesian methods through the likelihood. For instance, a Bayesian network can be used to infer attack relations between arguments by combining votes for acceptable arguments from different people (Kido and Okamoto, 2017). Other Bayesian approaches combine crowdsourced annotations to train a sentiment classifier without a separate quality control step (Simpson et al., 2015; Felt et al., 2016). Several successful Bayesian approaches in NLP make use of Gaussian processes (GPs), which are distributions over functions of input features. GPs"
Q18-1026,P17-2039,1,0.87416,"learning. Finally, we present conclusions and future work. 2 Identifying Convincing Arguments Lukin et al. (2017) demonstrated that an audience’s personality and prior stance affect an argument’s persuasiveness, but they were unable to predict belief change to a high degree of accuracy. Related work has shown how persuasiveness is also affected by the sequence of arguments in a discussion (Tan et al., 2016; Rosenfeld and Kraus, 2016; Monteserin and Amandi, 2013), but this work focuses on predicting salience of an argument given the state of the debate, rather than the qualities of arguments. Wachsmuth et al. (2017) recently showed that relative comparisons of argument convincingness correlate with theory-derived quality ratings. Habernal and Gurevych (2016) established datasets containing crowdsourced pairwise judgments of convincingness for arguments taken from online discussions. Errors in the crowdsourced data were handled by determining gold labels using the MACE algorithm (Hovy et al., 2013). The gold labels were then used to train SVM and bi-directional long short-term memory (BiLSTM) classifiers to predict pairwise labels for new arguments. The gold labels were also used to construct a directed g"
Q18-1026,P16-2032,0,0.0462736,"s of users. Automated methods could help readers overcome this challenge by identifying highquality, persuasive arguments from both sides of a debate. Theoretical approaches for assessing argument quality have proved difficult to apply to everyday arguments (Boudry et al., 2015). Empirical machine learning approaches instead train models using example judgments of arguments, such as those shown in Figure 1. Previous approaches to obtaining such judgments include training annotators to assign scores from 1-6 (Persing and Ng, 2017), asking annotators for simple binary or three-class categories (Wei et al., 2016b), and aggregating binary votes from multiple people (Wei et al., 2016a; Tan et al., 2016). However, these approaches are limited by the cost of training annotators, a highly restricted set of categories, or the need for multiple annotators per document. 357 Transactions of the Association for Computational Linguistics, vol. 6, pp. 357–371, 2018. Action Editor: Daichi Mochihashi. Submission batch: 9/2017; Revision batch: 1/2018; Published 6/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. An alternative way to judge arguments is to compare them ag"
Q18-1026,W16-2820,0,0.0214215,"s of users. Automated methods could help readers overcome this challenge by identifying highquality, persuasive arguments from both sides of a debate. Theoretical approaches for assessing argument quality have proved difficult to apply to everyday arguments (Boudry et al., 2015). Empirical machine learning approaches instead train models using example judgments of arguments, such as those shown in Figure 1. Previous approaches to obtaining such judgments include training annotators to assign scores from 1-6 (Persing and Ng, 2017), asking annotators for simple binary or three-class categories (Wei et al., 2016b), and aggregating binary votes from multiple people (Wei et al., 2016a; Tan et al., 2016). However, these approaches are limited by the cost of training annotators, a highly restricted set of categories, or the need for multiple annotators per document. 357 Transactions of the Association for Computational Linguistics, vol. 6, pp. 357–371, 2018. Action Editor: Daichi Mochihashi. Submission batch: 9/2017; Revision batch: 1/2018; Published 6/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. An alternative way to judge arguments is to compare them ag"
R09-1086,P05-1045,0,0.00307033,"mework for the comprehensive analysis of keyphrase extraction as shown in Figure 1. It was designed to be as language-independent as possible using either no language dependent information at all, or components that are already available for most languages (like tokenizers or chunkers). The preprocessing pipeline is based on the DKPro UIMA component repository [7]. Pre-Processing and Candidate Selection For preprocessing, we tokenize the documents, and split them into sentences. We integrated the TreeTagger for lemmatization, POS-tagging, and NP chunking [20], as well as the Stanford NER tool [5] for named entity recognition. From this pool of preprocessed data, we select as candidates Tokens, Lemmas, N-grams, Noun Phrases, and Named Entities. Following [15], we additionally use the restricted set of tokens Tokens (N,A) and lemmas Lemmas (N,A). Candidate Ranking The unsupervised graphbased methods (e.g. TextRank ) build a co-occurence graph using the candidates. The final candidate ranking is determined by computing the centrality scores of the graph nodes using PageRank. For tf.idf ranking, 3 It is formally defined as: Approx(kgold , kext ) = Exact ∨ Morph ∨ Includes. KEA TextRank tf"
R09-1086,N04-4005,0,0.141934,"se a corpus of training data to learn a keyphrase extraction model that is able to classify candidates as keyphrases. A well known supervised system is Kea [6] that uses all n-grams of a certain length as candidates, and ranks them using the probability of being a keyphrase. Kea is based on a Na¨ıve Bayes classifier using tf.idf and position as its main features. Extractor [22] is another supervised system that uses stems and stemmed n-grams as candidates. Its features are tuned using a genetic algorithm. Kea and Extractor are known to achieve roughly the same level of performance [23]. Hulth [10] uses a combination of lexical and syntactic features adding more linguistic knowledge which outperforms Kea. Medelyan and Witten [13] present the improved Kea++ that selects candidates with reference to a controlled vocabulary from a thesaurus or Wikipedia [14]. Turney [23] augments Kea with a feature set based on statistical word association to ensure that the returned keyphrase set is coherent. However, this assumption might not hold if a document covers different topics. Nguyen and Kan [16] augment Kea with features tailored towards scientific publications such as section information and c"
R09-1086,W08-1404,0,0.00452995,"on the properties of the evaluation dataset. Keywords keyphrase extraction; approximate matching 1 Introduction Keyphrases are small sets of expressions representing a document’s content. Keyphrase extraction is the task of automatically extracting such keyphrases from a document. The extracted phrases have to be present in the document itself, in contrast to keyphrase assignment (a multi-class text classification problem) where a fixed set of keyphrases is used which are not necessarily contained in the document. Keyphrase extraction has important applications in NLP including summarization [4, 11], clustering [9], as well as indexing and browsing [8], highlighting [22] and searching [2]. Despite the importance of the task, the evaluation of keyphrase extraction has not received much research attention in the past. In this paper, we address three core problems with the evaluation of keyphrase extraction: (i) the evaluation metric, (ii) the evaluation datasets, and (iii) the evaluation framework. The performance of most keyphrase extraction algorithms is evaluated by comparing whether the extracted keyphrases exactly match the human assigned gold standard keyphrases. However, this is kno"
R09-1086,C02-1142,0,0.0610057,"is section, we give an overview of (i) existing approaches to keyphrase extraction, (ii) the different ways to evaluate keyphrase extraction, and (iii) the datasets that have been used for evaluation. Keyphrase Extraction Approaches Existing methods for keyphrase extraction can be categorized into supervised and unsupervised approaches.1 1 Note that unsupervised approaches might use tools like NP chunkers relying on supervised approaches. However, as such 484 International Conference RANLP 2009 - Borovets, Bulgaria, pages 484–489 Closely related to keyphrase extraction are glossary extraction [17] and back-of-the-book indexing [3]. Unsupervised approaches usually select quite general sets of candidates (e.g. all tokens in a document), and use a subsequent ranking step to limit the selection to the most important candidates. For example, Barker and Cornacchia [1] restrict candidates to noun phrases, and rank them using heuristics based on length, term frequency, and head noun frequency. Bracewell et al. [2] also restrict candidates to noun phrases, and cluster them if they share a term. The clusters are ranked according to the noun phrase and token frequencies in the document. Finally,"
R09-1086,C08-2021,0,0.0295365,"anked using PageRank, and longer keyphrases can be reconstructed in a post-processing step merging adjacent keywords. The method was found to yield competitive results with state-of-theart supervised systems [15]. Wan and Xiao [24] expand TextRank by augmenting the graph with highly similar documents, which improves results compared with standard TextRank and a tf.idf baseline. Another branch of unsupervised approaches is based on statistical analysis. Tomokiyo and Hurst [21] use pointwise KL-divergence between language models derived from the documents and a reference corpus. Paukkeri et al. [18] use a similar method based on likelihood ratios. Matsuo and Ishizuka [12] present a statistical keyphrase extraction approach that does not make use of a reference corpus, but is based on co-occurrences of terms in a single document. Supervised approaches use a corpus of training data to learn a keyphrase extraction model that is able to classify candidates as keyphrases. A well known supervised system is Kea [6] that uses all n-grams of a certain length as candidates, and ranks them using the probability of being a keyphrase. Kea is based on a Na¨ıve Bayes classifier using tf.idf and positio"
R09-1086,W03-1805,0,0.0466009,"here the graph nodes are tokens and the edges reflect cooccurrence relations between tokens in the document. The nodes are ranked using PageRank, and longer keyphrases can be reconstructed in a post-processing step merging adjacent keywords. The method was found to yield competitive results with state-of-theart supervised systems [15]. Wan and Xiao [24] expand TextRank by augmenting the graph with highly similar documents, which improves results compared with standard TextRank and a tf.idf baseline. Another branch of unsupervised approaches is based on statistical analysis. Tomokiyo and Hurst [21] use pointwise KL-divergence between language models derived from the documents and a reference corpus. Paukkeri et al. [18] use a similar method based on likelihood ratios. Matsuo and Ishizuka [12] present a statistical keyphrase extraction approach that does not make use of a reference corpus, but is based on co-occurrences of terms in a single document. Supervised approaches use a corpus of training data to learn a keyphrase extraction model that is able to classify candidates as keyphrases. A well known supervised system is Kea [6] that uses all n-grams of a certain length as candidates, a"
R09-1086,W04-3252,0,\N,Missing
R11-1071,E09-1065,0,0.320234,"udgments across the whole similarity range. We then asked three annotators: “How similar are the given texts?” We then computed the Spearman correlation of each annotator’s ratings with the gold standard: ρA1 = 0.83, ρA2 = 0.65, and ρA3 = 0.85. The much lower correlation of 2 Articles written in Simple English use a limited vocabulary and easier grammar than the standard Wikipedia. 3 The last step requires all measures to be normalized. 516 Dataset Text Type / Domain Length in Terms () # Pairs 30 Sentence Pairs (Li et al., 2006) 50 Short Texts (Lee et al., 2005) Computer Science Assignments (Mohler and Mihalcea, 2009) Microsoft Paraphrase Corpus (Dolan et al., 2004) Concept Definitions News (Politics) 5–33 (11) 45–126 (80) 30 1,225 0–4 1–5 32 8–12 1–173 (18) 630 0–5 2 5–31 (19) 5,801 Computer Science News Rating Scale binary # Judges per Pair 2–3 Aggr. Similarity Table 2: Statistics for text similarity evaluation datasets 2 {ABC} {ABC} 1.5 3.1 Content Style +{D} Li et al. (2006) introduced 65 sentence pairs which are based on the noun pairs by Rubenstein and Goodenough (1965). Each noun was replaced by its definition from Collins Cobuild English Dictionary (Sinclair, 2001). The dataset contains judgments f"
R11-1071,W06-1603,0,0.0238547,"Missing"
R11-1071,W09-3204,0,0.0336359,"Missing"
R11-1071,U06-1019,0,0.037165,"Missing"
R11-1071,W09-3206,0,0.064296,"Missing"
R11-1071,U05-1023,0,0.0607695,"Missing"
R11-1071,C04-1051,0,0.0195908,"Missing"
R11-1071,I05-5003,0,0.0678646,"Missing"
R11-1071,P08-1048,0,\N,Missing
R13-1033,P07-1069,0,0.319718,"even wrong titles. Figure 2: TOC of this paper Supervised approaches learn a model of which document segments usually have a certain title. They are highly precise, but require training data and are limited to an a priori determined set of titles for which the model is trained. preprocessing including named entity recognition (Finkel et al., 2005), keyphrase extraction (Mihalcea and Tarau, 2004), and chunking (Schmid, 1994) which are then used as features for machine learning. To foster future research, we present two new datasets and compare results on these datasets and the one presented by Branavan et al. (2007). Our research contribution is to develop new algorithms for segment hierarchy identification, to present new evaluation datasets for all subtasks, and to compare our newly developed methods with the state of the art. We also provide a comprehensive analysis of the benefits and shortcomings of the applied methods. Figure 2 gives an overview of the paper’s organization (and at the same time highlights the usefulness of a TOC for the reader). Thus, we may safely skip the enumeration of paper sections and their content that usually concludes the introduction. 2 In the following, we organize the f"
R13-1033,H01-1011,0,0.0388201,"ear periods. A flexible system for generating segment titles enables the user to decide on which titles are more interesting and thus increasing the user’s benefit. each of the most frequent titles in each dataset. In Wikipedia, most articles have sections like See also, References, or External links, while books usually start with a chapter Preface. We restrict the list of title candidates to those appearing at least twice in the training data. We use a statistical model for predicting the title of a segment In contrast to previous approaches (Branavan et al., 2007; Nguyen and Shimazu, 2009; Jin and Hauptmann, 2001), we do not train on parts of the same document for which we want to predict titles, but rather on full documents of the same type (Wikipedia articles and books). This is an important difference, as in our usage scenario we need to generate full TOCs for previously unseen documents. On the Cormen dataset we cannot perform a trainings phase as it consists of one book. Evaluation Metrics We evaluated all approaches using two evaluation metrics. We propose accuracy as evaluation metric. A generated title is counted as correct only if it exactly matches the correct title. Hence, methods that gener"
R13-1033,R11-1106,0,0.0555659,"Missing"
R13-1033,W04-3252,0,0.00697675,"owing classes: Text-based approaches make use of only the text in the corresponding segment. Therefore, titles are limited to words appearing in the text. They can be applied in all situations, but will often create trivial or even wrong titles. Figure 2: TOC of this paper Supervised approaches learn a model of which document segments usually have a certain title. They are highly precise, but require training data and are limited to an a priori determined set of titles for which the model is trained. preprocessing including named entity recognition (Finkel et al., 2005), keyphrase extraction (Mihalcea and Tarau, 2004), and chunking (Schmid, 1994) which are then used as features for machine learning. To foster future research, we present two new datasets and compare results on these datasets and the one presented by Branavan et al. (2007). Our research contribution is to develop new algorithms for segment hierarchy identification, to present new evaluation datasets for all subtasks, and to compare our newly developed methods with the state of the art. We also provide a comprehensive analysis of the benefits and shortcomings of the applied methods. Figure 2 gives an overview of the paper’s organization (and"
R13-1033,D11-1071,0,0.0214634,"s. Furthermore, we want to continue develop better features for the task of hierarchy identification, and want to create methods for postprocessing a TOC in order to generate a coherent table-of-contents. We made the newly created evaluation datasets and our experimental framework publicly available in order to foster future research in table-ofcontents generation.8 the performance on some datasets. As these approaches typically use an independent set of title candidates, they can potentially achieve a higher performance. Commonly used combination strategies like voting or complex strategies (Chen, 2011) can only be applied within approaches from the same class, as different classes will output different titles. Besides, it is desirable to create a diversity of candidates without ignoring titles generated by only one approach. Results in Table 7 reveals that a combination of approaches provides the highest accuracy of all approaches. We cannot compare a list of generated titles to a gold title with Rouge, thus not presenting any numbers (n/a). We utilize the benefit of accuracy allowing to compare a set of generated titles to a gold title. In a real-world setting, a user selects the best titl"
R13-1033,P04-1015,0,0.22484,"Missing"
R13-1033,R09-1057,0,0.0897158,"of the dates of the fouryear periods. A flexible system for generating segment titles enables the user to decide on which titles are more interesting and thus increasing the user’s benefit. each of the most frequent titles in each dataset. In Wikipedia, most articles have sections like See also, References, or External links, while books usually start with a chapter Preface. We restrict the list of title candidates to those appearing at least twice in the training data. We use a statistical model for predicting the title of a segment In contrast to previous approaches (Branavan et al., 2007; Nguyen and Shimazu, 2009; Jin and Hauptmann, 2001), we do not train on parts of the same document for which we want to predict titles, but rather on full documents of the same type (Wikipedia articles and books). This is an important difference, as in our usage scenario we need to generate full TOCs for previously unseen documents. On the Cormen dataset we cannot perform a trainings phase as it consists of one book. Evaluation Metrics We evaluated all approaches using two evaluation metrics. We propose accuracy as evaluation metric. A generated title is counted as correct only if it exactly matches the correct title."
R13-1033,P05-1045,0,0.19965,"eration we divide related work into the following classes: Text-based approaches make use of only the text in the corresponding segment. Therefore, titles are limited to words appearing in the text. They can be applied in all situations, but will often create trivial or even wrong titles. Figure 2: TOC of this paper Supervised approaches learn a model of which document segments usually have a certain title. They are highly precise, but require training data and are limited to an a priori determined set of titles for which the model is trained. preprocessing including named entity recognition (Finkel et al., 2005), keyphrase extraction (Mihalcea and Tarau, 2004), and chunking (Schmid, 1994) which are then used as features for machine learning. To foster future research, we present two new datasets and compare results on these datasets and the one presented by Branavan et al. (2007). Our research contribution is to develop new algorithms for segment hierarchy identification, to present new evaluation datasets for all subtasks, and to compare our newly developed methods with the state of the art. We also provide a comprehensive analysis of the benefits and shortcomings of the applied methods. Figure 2 gi"
R13-1042,D10-1038,0,0.0675434,"Missing"
R13-1042,R11-1071,1,0.616101,"Missing"
R13-1042,J10-3004,0,0.0318242,"or temporal thread disentanglement, we perform pairwise classification experiments on texts in emails using no MIME headers or quoted previous emails. We have found that content-based text similarity metrics outperform a Dice baseline, and that structural and style text similarity features do not; adding these latter feature groups does not significantly improve total performance. We also found that contentbased features continue to outperform the others in both a class-balanced and class-imbalanced setting, as well as with semantically controlled or non-controlled negative instances. In NLP, Elsner and Charniak (2010) described the task of thread disentanglement as “the clustering task of dividing a transcript into a set of distinct conversations,” in which extrinsic thread delimitation is unavailable and the threads must be disentangled using only intrinsic information. In addition to emails with missing or incorrect MIME headers, entangled electronic conversations occur in environments such as interspersed Internet Relay Chat conversations, web 2.0 article response conversations that do not have a hierarchical display order, and misplaced comments in Wiki Talk discussions. Research on disentanglement of"
R13-1042,P11-1118,0,0.0510501,"Missing"
R13-1042,N04-4027,0,0.708763,"; and removing quoted material from earlier in the thread. 327 Proceedings of Recent Advances in Natural Language Processing, pages 327–335, Hissar, Bulgaria, 7-13 September 2013. Previous researchers have used a number of email corpora with high-precision (nonSubject-clustered) thread marking. Joti et al. (2010) used the BC3 corpus of 40 email threads and 3222 emails for topic segmentation. Carenini et al. (2008) annotated 39 email “conversations” from the Enron Email Corpus for email summariation. Wan and McKeown (2004) used a privatelyavailable corpus of 300 threads for summary generation. Rambow et al. (2004) used a privatelyavailable corpus of 96 email threads for thread summarization. 2 fixes such as Re: and Fwd:) and shared participants. To determine whether emails were among the same users, we split a Subject-created email proto-thread apart into any necessary threads, such that the split threads had no senders or recipients (including To, CC, and BCC) in common. The resulting email clusters had a number of problems. Clusters tended to over-group, because a single user included as a recipient for two different threads with the Subject “Monday Meeting” would cause the threads to be merged into"
R13-1042,C04-1079,0,0.173705,"s for one person; sharing one email account among multiple persons; changing the Subject header; and removing quoted material from earlier in the thread. 327 Proceedings of Recent Advances in Natural Language Processing, pages 327–335, Hissar, Bulgaria, 7-13 September 2013. Previous researchers have used a number of email corpora with high-precision (nonSubject-clustered) thread marking. Joti et al. (2010) used the BC3 corpus of 40 email threads and 3222 emails for topic segmentation. Carenini et al. (2008) annotated 39 email “conversations” from the Enron Email Corpus for email summariation. Wan and McKeown (2004) used a privatelyavailable corpus of 300 threads for summary generation. Rambow et al. (2004) used a privatelyavailable corpus of 96 email threads for thread summarization. 2 fixes such as Re: and Fwd:) and shared participants. To determine whether emails were among the same users, we split a Subject-created email proto-thread apart into any necessary threads, such that the split threads had no senders or recipients (including To, CC, and BCC) in common. The resulting email clusters had a number of problems. Clusters tended to over-group, because a single user included as a recipient for two d"
R13-1042,W99-0625,0,0.153747,"Missing"
R13-1042,P08-1041,0,\N,Missing
ruckle-gurevych-2017-real,P15-1155,0,\N,Missing
S10-1046,S10-1006,0,0.112078,"Missing"
S10-1046,W09-2415,0,\N,Missing
S12-1059,S12-1051,0,0.406442,": Official results on the test data for the top 5 participating runs out of 89 which were achieved on the known datasets MSRpar, MSRvid, and SMTeuroparl, as well as on the surprise datasets OnWN and SMTnews. We report the ranks (#1 : ALL, #2 : ALLnrm, #3 : Mean) and the corresponding Pearson correlation r according to the three offical evaluation metrics (see Sec. 6). The provided baseline is shown at the bottom of this table. metrics ALL (r = .823)4 and Mean (r = .677), and #2 for ALLnrm (r = .857). An exhaustive overview of all participating systems can be found in the STS task description (Agirre et al., 2012). 7 Conclusions and Future Work In this paper, we presented the UKP system, which performed best across the three official evaluation metrics in the pilot Semantic Textual Similarity (STS) task at SemEval-2012. While we did not reach the highest scores on any of the single datasets, our system was most robust across different data. In future work, it would be interesting to inspect the performance of a system that combines the output of all participating systems in a single linear model. We also propose that two major issues with the datasets are tackled in future work: (a) It is unclear how t"
S12-1059,C10-1005,0,0.0431732,"Missing"
S12-1059,P02-1020,0,0.0272336,"antic resources. Further, we employ a lexical substitution system and statistical machine translation to add additional lexemes, which alleviates lexical gaps. Our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on su"
S12-1059,C04-1051,0,0.0935903,"per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (Mihalcea et al., 2006; Landauer et al., 1998; Gabrilovich and Markovitch, 2007). The existing measures exhibit two major limitations,"
S12-1059,W99-0625,0,0.381092,"ifier. Nonetheless, we briefly list them for completeness. Structural similarity between texts can be detected by computing stopword n-grams (Stamatatos, 2011). Thereby, all content-bearing words are removed while stopwords are preserved. Stopword n-grams of both texts are compared using the containment measure (Broder, 1997). In our experiments, we tested n-gram sizes for n = 2, 3, . . . , 10. We also compute part-of-speech n-grams for various POS tags which we then compare using the containment measure and the Jaccard coefficient. We also used two similarity measures between pairs of words (Hatzivassiloglou et al., 1999): Word pair order tells whether two words occur in the same order in both texts (with any number of words in between), word pair distance counts the number of words which lie between those of a given pair. To compare texts along the stylistic dimension, we further use a function word frequencies measure (Dinu and Popescu, 2009) which operates on a set of 70 function words identified by Mosteller and Wallace (1964). Function word frequency vectors are computed and compared by Pearson correlation. We also include a number of measures which capture statistical properties of texts, such as typetok"
S12-1059,O97-1002,0,0.48473,"the original trigram variant to n = 2, 3, . . . , 15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Sema"
S12-1059,P07-2045,0,0.00189328,"n cardinal numbers (CD) was selected in the final models. 2.3 Text Expansion Mechanisms Lexical Substitution System We used the lexical substitution system based on supervised word sense disambiguation (Biemann, 2012). This system automatically provides substitutions for a set of about 1,000 frequent English nouns with high precision. For each covered noun, we added the substitutions to the text and computed the pairwise word similarity for the texts as described above. This feature alleviates the lexical gap for a subset of words. Statistical Machine Translation We used the Moses SMT system (Koehn et al., 2007) to translate the original English texts via three bridge languages (Dutch, German, Spanish) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. The system was trained on Europarl made available by Koehn (2005), using the following configuration which was not optimized for this task: WMT112 baseline without tuning, with MGIZA alignment. The largest improvement was reached for computing pairwise word similarity (as described above) on the concatenation of the original text and the three back-translation"
S12-1059,2005.mtsummit-papers.11,0,0.00230974,"nouns with high precision. For each covered noun, we added the substitutions to the text and computed the pairwise word similarity for the texts as described above. This feature alleviates the lexical gap for a subset of words. Statistical Machine Translation We used the Moses SMT system (Koehn et al., 2007) to translate the original English texts via three bridge languages (Dutch, German, Spanish) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. The system was trained on Europarl made available by Koehn (2005), using the following configuration which was not optimized for this task: WMT112 baseline without tuning, with MGIZA alignment. The largest improvement was reached for computing pairwise word similarity (as described above) on the concatenation of the original text and the three back-translations. 2.4 Measures Related to Structure and Style In our system, we also used measures which go beyond content and capture similarity along the structure and style dimensions inherent to texts. However, as we report later on, for this content1 www.wiktionary.org 0-5-grams, grow-diag-final-and alignment, m"
S12-1059,P98-2127,0,0.0293197,"nt to n = 2, 3, . . . , 15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analys"
S12-1059,W01-0515,0,0.305307,"nes a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementation by Barr´onCede˜no et al. (2010), thereby generalizing the original trigram variant to n = 2, 3, . . . , 15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the t"
S12-1059,R11-1063,0,0.0123301,"counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). Besides WordNet, we used two additional lexical-semantic resources for the construction of the ESA vector space: Wikipedia and Wiktionary1 . 436 Textual Entailment We experimented with using the BIUTEE textual entailment system (Stern and Dagan, 2011) for generating entailment scores to serve as features for the classifier. However, these features were not selected by the classifier. Distributional Thesaurus We used similarities from a Distributional Thesaurus (similar to Lin (1998b)) computed on 10M dependency-parsed sentences of English newswire as a source for pairwise word similarity, one additional feature per POS tag. However, only the feature based on cardinal numbers (CD) was selected in the final models. 2.3 Text Expansion Mechanisms Lexical Substitution System We used the lexical substitution system based on supervised word sense"
S12-1059,W07-1401,0,\N,Missing
S12-1059,C98-2122,0,\N,Missing
S13-2038,P11-1148,0,0.0245691,"Missing"
S13-2038,S12-1059,1,0.891497,"Missing"
S13-2038,E06-1018,0,0.0161675,"n and then builds groups of highly connected words called committees. It then iteratively assigns the remaining words to one of the committee clusters by comparing them to the averaged the com1 http://code.google.com/p/dkpro-core-asl/ 212 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 212–216, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics mittee feature vectors. This exploits the assumption that two or more words together disambiguate each other, Bordag (2006) extends on this idea by using word triples to form non-ambiguous seed-clusters. Many approaches use a variety of graph clustering algorithms for WSI: Others (Klapaftis and Manandhar, 2010) use hierarchical agglomerative clustering on hierarchical random graphs created from word co-occurrences. Di Marco and Navigli (2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-"
S13-2038,E09-1013,0,0.0203295,"(2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-order co-occurrences as well as singular value decomposition on the cooccurrence matrix, which is clustered using repeated bisections. Jurgens (2011) employ a graph-based community detection algorithm on a co-occurrence graph. Distributional approaches for WSI include LSA Apidianaki and Van de Cruys (2011) or LDA (Brody and Lapata, 2009). 3 Our Approach Our system consists of two independent parts. The first is a batch process that creates database containing co-occurrence statistics derived from a background corpus. The second is the actual WSI and WSD pipeline doing the result clustering. Both parts include identical preprocessing steps for segmentation and lemmatization. The pipeline (Figure 1) first performs Word Sense Induction, resulting in an induced sense inventory. A WSD algorithm then uses this inventory to disambiguate all instances of the search query within a web-page. A majority voting finally assigns a sense to"
S13-2038,J90-1003,0,0.181382,"3 Wikipedia ukWaC # words 3,011,397 8,687,711 # co-occurrences 96,979,920 441,005,478 Table 1: Size of co-occurrence databases 3.1 Preprocessing The pipeline first reads topics and snippets. If the web-page can be downloaded at the URL that corresponds to the result, it is cleaned by an HTML parser and the plain text is appended to the snippet. As further steps we segment and lemmatize the input. We apply the same preprocessing to snippets, queries and the corpora. 3.2 Co-occurrence Extraction We calculate the log-likelihood ratio (LLR) (Dunning, 1993) and point-wise mutual information (PMI) (Church and Hanks, 1990) of a word pair cooccurring at sentence level using a modified version of the collocation statistics implemented in Apache Mahout 2 . Even when sorting the co-occurrences by PMI, we employ a minimum support cut-off based on the LLR, which is based on significance. All pairs with a log-likelihood ratio < 1 are discarded. ˜ This value is lower than the significance level of 3.8 we found in the literature, but because in the expand step (see algorithm 2) we require more than two words to co-occur with the target word, we used a lower value. We use the English Wikipedia 3 and ukWaC (Baroni et al.,"
S13-2038,J13-3008,0,0.0339525,"Missing"
S13-2038,E03-1020,0,0.0354088,"esource that enumerates possible senses for each word is called a sense inventory. Manually created inventories come usually in form of lexical semantic resources, such as WordNet or more specifically created inventories such as OntoNotes (Hovy et al., 2006). Word sense induction (WSI) on the other hand aims to create such an inventory from a corpus in 2 Related Work One of the early approaches to WSI (Sch¨utze, 1998) maps words into a vector space and represents word contexts as vector-sums and use cosine vector similarity, clustering is performed by expectation maximization (EM) clustering. Dorow and Widdows (2003) use the BNC to build a co-occurrence graph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph. Pantel and Lin (2002) proposes a clustering approach called clustering by committee (CBC). This algorithm first selects the words with the highest similarity based on mutual information and then builds groups of highly connected words called committees. It then iteratively assigns the remaining words to one of the committee clusters by comparing them to the averaged the com1 http://code.google.com/p/dkpro-core-asl/ 212 Second Joint Conference on Lex"
S13-2038,J93-1003,0,0.0984487,"is for experimentation and evaluation of WSI systems. 213 Wikipedia ukWaC # words 3,011,397 8,687,711 # co-occurrences 96,979,920 441,005,478 Table 1: Size of co-occurrence databases 3.1 Preprocessing The pipeline first reads topics and snippets. If the web-page can be downloaded at the URL that corresponds to the result, it is cleaned by an HTML parser and the plain text is appended to the snippet. As further steps we segment and lemmatize the input. We apply the same preprocessing to snippets, queries and the corpora. 3.2 Co-occurrence Extraction We calculate the log-likelihood ratio (LLR) (Dunning, 1993) and point-wise mutual information (PMI) (Church and Hanks, 1990) of a word pair cooccurring at sentence level using a modified version of the collocation statistics implemented in Apache Mahout 2 . Even when sorting the co-occurrences by PMI, we employ a minimum support cut-off based on the LLR, which is based on significance. All pairs with a log-likelihood ratio < 1 are discarded. ˜ This value is lower than the significance level of 3.8 we found in the literature, but because in the expand step (see algorithm 2) we require more than two words to co-occur with the target word, we used a lowe"
S13-2038,N06-2015,0,0.0158953,"o WSI evaluation. The task requires building a WSI system and combining it with a WSD step to assign the induced sentences to example instances. Word sense disambiguation (WSD) is the task of determining the correct meaning for an ambiguous word from its context. WSD algorithms usually choose one sense out of a given set of possible senses for each word. A resource that enumerates possible senses for each word is called a sense inventory. Manually created inventories come usually in form of lexical semantic resources, such as WordNet or more specifically created inventories such as OntoNotes (Hovy et al., 2006). Word sense induction (WSI) on the other hand aims to create such an inventory from a corpus in 2 Related Work One of the early approaches to WSI (Sch¨utze, 1998) maps words into a vector space and represents word contexts as vector-sums and use cosine vector similarity, clustering is performed by expectation maximization (EM) clustering. Dorow and Widdows (2003) use the BNC to build a co-occurrence graph for nouns, based on a co-occurrence frequency threshold. They perform Markov clustering on this graph. Pantel and Lin (2002) proposes a clustering approach called clustering by committee (CB"
S13-2038,W11-1104,0,0.0161564,"rithms for WSI: Others (Klapaftis and Manandhar, 2010) use hierarchical agglomerative clustering on hierarchical random graphs created from word co-occurrences. Di Marco and Navigli (2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-order co-occurrences as well as singular value decomposition on the cooccurrence matrix, which is clustered using repeated bisections. Jurgens (2011) employ a graph-based community detection algorithm on a co-occurrence graph. Distributional approaches for WSI include LSA Apidianaki and Van de Cruys (2011) or LDA (Brody and Lapata, 2009). 3 Our Approach Our system consists of two independent parts. The first is a batch process that creates database containing co-occurrence statistics derived from a background corpus. The second is the actual WSI and WSD pipeline doing the result clustering. Both parts include identical preprocessing steps for segmentation and lemmatization. The pipeline (Figure 1) first performs Word Sense Induction, resul"
S13-2038,kilgarriff-rosenzweig-2000-english,0,0.025036,"adding edges between vertices representing the expansion terms and Csmall . Algorithm 2 expandJoin Require: G is a minimum spanning forest for s = Smax → 1 do for all Csmall (G), |Csmall |= s do E ← querys (v1 , .., vi ) for all Clarge ∈ G, |Clarge |> s do if |Clarge ∩ E|/|Clarge |> t then Clarge ← Clarge ∪ Csmall else Csmall ← Csmall ∪ E end if end for end for end for 3.4 Word Sense Disambiguation We use the DKPro WSD framework, which implements various WSD algorithms, with the same system configuration as reported by Miller et al. (2012). It uses a variant of the Simplified Lesk Algorithm (Kilgarriff et al., 2000). This algorithm measures the overlap between a words context and the textual descriptions of senses within a machine readable dictionary, such as WordNet. The senses that have been induced in the previous step are provided to the framework as a sense inventory. Instead of using sense descriptions, we now compute the overlap between the sense clusters and the context of the target word. The WSD system expands both the word context and the sense clusters with synonyms from a distributional thesaurus (DT), similar to Lin (1998). The DT has been created from 10M dependency-parsed sentences of Eng"
S13-2038,D10-1073,0,0.01548,"the averaged the com1 http://code.google.com/p/dkpro-core-asl/ 212 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 212–216, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics mittee feature vectors. This exploits the assumption that two or more words together disambiguate each other, Bordag (2006) extends on this idea by using word triples to form non-ambiguous seed-clusters. Many approaches use a variety of graph clustering algorithms for WSI: Others (Klapaftis and Manandhar, 2010) use hierarchical agglomerative clustering on hierarchical random graphs created from word co-occurrences. Di Marco and Navigli (2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-order co-occurrences as well as singular value decomposition on the cooccurrence matrix, which is clustered using repeated bisections. Jurgens (2011) employ a graph-based community detectio"
S13-2038,P98-2127,0,0.27187,"). It uses a variant of the Simplified Lesk Algorithm (Kilgarriff et al., 2000). This algorithm measures the overlap between a words context and the textual descriptions of senses within a machine readable dictionary, such as WordNet. The senses that have been induced in the previous step are provided to the framework as a sense inventory. Instead of using sense descriptions, we now compute the overlap between the sense clusters and the context of the target word. The WSD system expands both the word context and the sense clusters with synonyms from a distributional thesaurus (DT), similar to Lin (1998). The DT has been created from 10M dependency-parsed sentences of English newswire Run wacky-llr wp-llr wp-pmi F1 0.5826 0.5864 0.6048 ARI 0.0253 0.0377 0.0364 RI 0.5002 0.5109 0.5050 JI 0.3394 0.3177 0.2932 # clusters 3.6400 4.1700 5.8600 avg cl. size 32.3434 21.8702 30.3098 Table 2: Results for the submitted runs from the Leipzig Corpora Collection (Biemann et al., 2007) for word similarity5 . Besides knowledgebased WSD, the DT also has been successfully used for improving the performance of semantic text similarity (B¨ar et al., 2012). The WSD component disambiguates each instance of the se"
S13-2038,C12-1109,1,0.825575,"assume that Csmall represents a sense of its own extend the clus214 ter by adding edges between vertices representing the expansion terms and Csmall . Algorithm 2 expandJoin Require: G is a minimum spanning forest for s = Smax → 1 do for all Csmall (G), |Csmall |= s do E ← querys (v1 , .., vi ) for all Clarge ∈ G, |Clarge |> s do if |Clarge ∩ E|/|Clarge |> t then Clarge ← Clarge ∪ Csmall else Csmall ← Csmall ∪ E end if end for end for end for 3.4 Word Sense Disambiguation We use the DKPro WSD framework, which implements various WSD algorithms, with the same system configuration as reported by Miller et al. (2012). It uses a variant of the Simplified Lesk Algorithm (Kilgarriff et al., 2000). This algorithm measures the overlap between a words context and the textual descriptions of senses within a machine readable dictionary, such as WordNet. The senses that have been induced in the previous step are provided to the framework as a sense inventory. Instead of using sense descriptions, we now compute the overlap between the sense clusters and the context of the target word. The WSD system expands both the word context and the sense clusters with synonyms from a distributional thesaurus (DT), similar to L"
S13-2038,S13-2035,0,0.0221699,"URL of the corresponding web page. In this paper, we describe the UKP Lab system participating in the Semeval-2013 task “Word Sense Induction and Disambiguation within an End-User Application”. Our approach uses preprocessing, co-occurrence extraction, graph clustering, and a state-of-theart word sense disambiguation system. We developed a configurable pipeline which can be used to integrate and evaluate other components for the various steps of the complex task. 1 Introduction The task “Evaluating Word Sense Induction and Word Sense Disambiguation in an End-User Application” of SemEval-2013 (Navigli and Vannella, 2013) aims at an extrinsic evaluation scheme for WSI to overcome the difficulties inherent to WSI evaluation. The task requires building a WSI system and combining it with a WSD step to assign the induced sentences to example instances. Word sense disambiguation (WSD) is the task of determining the correct meaning for an ambiguous word from its context. WSD algorithms usually choose one sense out of a given set of possible senses for each word. A resource that enumerates possible senses for each word is called a sense inventory. Manually created inventories come usually in form of lexical semantic"
S13-2038,S10-1081,0,0.0287902,"words together disambiguate each other, Bordag (2006) extends on this idea by using word triples to form non-ambiguous seed-clusters. Many approaches use a variety of graph clustering algorithms for WSI: Others (Klapaftis and Manandhar, 2010) use hierarchical agglomerative clustering on hierarchical random graphs created from word co-occurrences. Di Marco and Navigli (2013) use word sense induction for web search result clustering. They introduce a maximum spanning tree algorithm that operates on co-occurrence graphs built from large corpora, such as ukWaC (Baroni et al., 2009). The system by Pedersen (2010) employs clustering first- and second-order co-occurrences as well as singular value decomposition on the cooccurrence matrix, which is clustered using repeated bisections. Jurgens (2011) employ a graph-based community detection algorithm on a co-occurrence graph. Distributional approaches for WSI include LSA Apidianaki and Van de Cruys (2011) or LDA (Brody and Lapata, 2009). 3 Our Approach Our system consists of two independent parts. The first is a batch process that creates database containing co-occurrence statistics derived from a background corpus. The second is the actual WSI and WSD pi"
S13-2038,J98-1004,0,0.50548,"Missing"
S13-2038,N10-1010,0,\N,Missing
S13-2038,C98-2122,0,\N,Missing
S13-2048,S12-1059,1,0.886472,"Missing"
S13-2048,C12-1011,1,0.819518,"Missing"
S13-2048,S13-2045,1,0.847683,"tment Technische Universit¨at Darmstadt § Natural Language Processing Lab Computer Science Department Bar-Ilan University Abstract Our system combines text similarity measures with a textual entailment system. In the main task, we focused on the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline. 1 Ido Dagan§ Introduction The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013) brings together two important dimensions of Natural Language Processing: real-world applications and semantic inference technologies. The challenge focuses on the domain of middleschool quizzes, and attempts to emulate the meticulous marking process that teachers do on a daily basis. Given a question, a reference answer, and a student’s answer, the task is to determine whether the student answered correctly. While this is not a new task in itself, the challenge focuses on employing textual entailment technologies as the backbone of this educational application. As a consequence, we formalize"
S13-2048,W01-0515,0,0.0300847,"use semantic similarity measures in order to bridge a possible vocabulary gap between the student and reference answer. We use the ESA measure (Gabrilovich 3 code.google.com/p/dkpro-core-asl/ DKPro Core v1.4.0, TreeTagger models v20130204.0, Stanford parser PCFG model v20120709.0. 5 Using the 750 most frequent n-grams gave good results on the training set, so we also used this number for the test runs. 6 As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and longest common substring (Allison and Dix, 1986), and word n-gram containment(Lyon et al., 2001) with n = 2. 4 and Markovitch, 2007) based on concept vectors build from WordNet, Wiktionary, and Wikipedia. Spelling Features As spelling errors might be indicative of the answer quality, we use the number of spelling errors normalized by the text length as an additional feature. Entailment Features We run BIUTEE (Stern and Dagan, 2011) on the test instance (as T ) with each reference answer (as H), which results in an array of numerical entailment confidence values. If there is more than one reference answer, we compute all pairwise confidence scores and add the minimum, maximum, average, an"
S13-2048,R11-1063,1,0.940093,"provide a robust architecture for student response analysis, that can generalize and perform well in multiple domains. Moreover, we are interested in evaluating how well general-purpose technologies will perform in this setting. We therefore approach the challenge by combining two such technologies: DKPro Similarity –an extensive suite of text similarity measures– that has been successfully applied in other settings like the SemEval 2012 task on semantic textual similarity (B¨ar et al., 2012a) or reuse detection (B¨ar et al., 2012b). BIUTEE, the Bar-Ilan University Textual Entailment Engine (Stern and Dagan, 2011), which has shown state-of-the-art performance on recognizing textual entailment challenges. Our systems use both technologies to extract features, and combine them in a supervised model. Indeed, this approach works relatively well (with respect to other entries in the challenge), especially in unseen domains. 2 2.1 Background Text Similarity Text similarity is a bidirectional, continuous function which operates on pairs of texts of any length and returns a numeric score of how similar one text is to the other. In previous work (Mihalcea et al., 285 Second Joint Conference on Lexical and Compu"
S14-1004,R11-1071,1,0.8995,"Missing"
S14-1004,J06-1003,0,0.0796174,"ned by the term frequency in the corresponding dimension, i.e. in a certain Wikipedia article. The similarity of two words is then computed as the inner product (usually the cosine) of the two word vectors. We now show how ESA can be adapted successfully to work on the sense-level, too. Jaguar (animal) .0000 .0000 .0341 Zoo Figure 2: Similarity between senses. sure (Milne, 2007) and Lin (Lin, 1998) as examples of sense-level similarity measures2 and ESA as the prototypical word-level measure.3 The Lin measure is a widely used graph-based similarity measure from a family of similar approaches (Budanitsky and Hirst, 2006; Seco et al., 2004; Banerjee and Pedersen, 2002; Resnik, 1999; Jiang and Conrath, 1997; Grefenstette, 1992). It computes the similarity between two senses based on the information content (IC) of the lowest common subsumer (lcs) and both senses (see Formula 1). simlin = 2 IC(lcs) IC(sense1) + IC(sense2) 2.1 In the standard definintion, ESA computes the term frequency based on the number of times a term—usually a word—appears in a document. In order to make it work on the sense level, we will need a large sense-disambiguated corpus. Such a corpus could be obtained by performing word sense disa"
S14-1004,P92-1052,0,0.0407679,"o words is then computed as the inner product (usually the cosine) of the two word vectors. We now show how ESA can be adapted successfully to work on the sense-level, too. Jaguar (animal) .0000 .0000 .0341 Zoo Figure 2: Similarity between senses. sure (Milne, 2007) and Lin (Lin, 1998) as examples of sense-level similarity measures2 and ESA as the prototypical word-level measure.3 The Lin measure is a widely used graph-based similarity measure from a family of similar approaches (Budanitsky and Hirst, 2006; Seco et al., 2004; Banerjee and Pedersen, 2002; Resnik, 1999; Jiang and Conrath, 1997; Grefenstette, 1992). It computes the similarity between two senses based on the information content (IC) of the lowest common subsumer (lcs) and both senses (see Formula 1). simlin = 2 IC(lcs) IC(sense1) + IC(sense2) 2.1 In the standard definintion, ESA computes the term frequency based on the number of times a term—usually a word—appears in a document. In order to make it work on the sense level, we will need a large sense-disambiguated corpus. Such a corpus could be obtained by performing word sense disambiguating (Agirre and Edmonds, 2006; Navigli, 2009) on all words. However, as this is an error-prone task a"
S14-1004,O97-1002,0,0.190707,"cle. The similarity of two words is then computed as the inner product (usually the cosine) of the two word vectors. We now show how ESA can be adapted successfully to work on the sense-level, too. Jaguar (animal) .0000 .0000 .0341 Zoo Figure 2: Similarity between senses. sure (Milne, 2007) and Lin (Lin, 1998) as examples of sense-level similarity measures2 and ESA as the prototypical word-level measure.3 The Lin measure is a widely used graph-based similarity measure from a family of similar approaches (Budanitsky and Hirst, 2006; Seco et al., 2004; Banerjee and Pedersen, 2002; Resnik, 1999; Jiang and Conrath, 1997; Grefenstette, 1992). It computes the similarity between two senses based on the information content (IC) of the lowest common subsumer (lcs) and both senses (see Formula 1). simlin = 2 IC(lcs) IC(sense1) + IC(sense2) 2.1 In the standard definintion, ESA computes the term frequency based on the number of times a term—usually a word—appears in a document. In order to make it work on the sense level, we will need a large sense-disambiguated corpus. Such a corpus could be obtained by performing word sense disambiguating (Agirre and Edmonds, 2006; Navigli, 2009) on all words. However, as this is"
S14-1004,C12-1108,1,0.926356,"orpus. Such a corpus could be obtained by performing word sense disambiguating (Agirre and Edmonds, 2006; Navigli, 2009) on all words. However, as this is an error-prone task and we are more interested to showcase the overall principle, we rely on Wikipedia as an already manually disambiguated corpus. Wikipedia is a highly linked resource and articles can be considered as senses.4 We extract all links from all articles, with the link target as the term. This approach is not restricted to Wikipedia, but can be applied to any resource containing connections between articles, such as Wiktionary (Meyer and Gurevych, 2012b). Another reason to select Wikipedia as a corpus is that it will allow us to directly compare similarity values with the Wikipedia Link Measure as described above. After this more high-level introduction, we now focus on the mathematical foundation of ESA and disambiguated ESA (called ESA on senses). ESA and ESA on senses count the frequency of each term (or sense) in each document. Table 1 shows the corresponding term-document matrix for the example in Figure 1. The term Jaguar appears in all shown documents, but the term Zoo appears in the articles Dublin Zoo and Wildlife Park.5 A manual a"
S14-2126,S13-2055,0,0.023939,"Missing"
S14-2126,P03-1054,0,0.012131,"ocessing the data, we use components from DKPro Core3 . Preprocessing is the same for subtasks A and B, with the only difference that in the subtask A the target expression is additionally annotated as text classification unit, while the rest of the tweet is considered to be a document context. We first segment the data with the Stanford Segmenter4 , apply the Stanford POS Tagger with a Twitter-trained model (Derczynski et al., 2013), and subsequently apply the Stanford Lemmatizer4 , TreeTagger Chunker (Schmid, 1994), Stanford Named Entity Recognizer (Finkel et al., 2005) and Stanford Parser (Klein and Manning, 2003) to each tweet. After this linguistic preprocessing, the token segmentation of the Stanford tools is removed and overwritten by the ArkTweet Tagger (Gimpel et al., 2011), which is more suitable for recognizing hashtags and smileys as one particular token. Finally, we expand the tweet and proceed to feature extraction as described in detail in Section 3. 2.2 3.1 3 4 Lexical features As a basis for our similarity and expansion experiments (sections 3.4 and 3.5), we use the binary sentiment polarity lexicon by Liu (2012) augmented with the smiley polarity lexicon by Becker et al. (2013) and an ad"
S14-2126,S13-2053,0,0.018373,"list5 [further as Liuaugmented ]. We selected this augmented lexicon for two reasons: firstly, it was the highest ranked lexical feature on the developmenttest and crossvalidation experiments, secondly it consists of two plain word lists and therefore does not introduce another complexity dimension for advanced feature calculations. We further measure lexicon hits normalized per number of tweet tokens for the following lexicons: Pennebaker’s Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), the NRC Emotion Lexicon (Mohammad and Turney, 2013), the NRC Hashtag Emotion Lexicon (Mohammad et al., 2013) and the Sentiment140 lexicon (Mohammad et al., 2013). We use an additional lexicon of positive, negative, very positive and very negative words, diminishers, intensifiers and negations composed by Steinberger et al. (2012), where we calculate the polarity score as described in their paper. In a complementary set of features we combine each of the lexicons above with a list of weighted intensifying expressions as published by Brooke (2009). The intensity of any polar word found in any of the emotion lexicons used is intensified or diminished by a given weight if an intensifier (a Classificatio"
S14-2126,P14-5011,1,0.834169,"oter are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.lt.informatik.tu-darmstadt.de 704 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 704–710, Dublin, Ireland, August 23-24, 2014. stadt. 2 and finally set to C=1 and G=0.01. The resulting model was wrapped in a cost sensitive meta classifier from the WEKA toolkit with the error costs set to reflect the class imbalance in the training set. Experimental setup Our experimental setup is based on an open-source text classification framework DKPro TC2 (Daxenberger et al., 2014), which allows to combine NLP pipelines into a configurable and modular system for preprocessing, feature extraction and classification. We use the unit classification mode of DKPro TC for Subtask A and the document classification mode for Subtask B. 2.1 3 We now describe the features used in our experiments. For Subtask A (contextual polarity), we extracted each feature twice - once on the tweet level and once on the focus expression level. Only n-gram features were extracted solely from the expressions. For Subtask B (tweet polarity), we extracted features on tweet level only. In both cases,"
S14-2126,S13-2052,0,0.0398493,"stock market analysis (Bollen et al., 2011) or the health sector (Culotta, 2010). Due to its large number of applications, sentiment analysis on Twitter is a very popular task. Challenges arise both from the character of the task and from the language specifics of Twitter messages. Messages are normally very short and informal, frequently using slang, alternative spelling, neologism and links, and mostly ignoring the punctuation. Our experiments have been carried out as part of the SemEval 2014 Task 9 - Sentiment Analysis on Twitter (Rosenthal et al., 2014), a rerun of a SemEval-2013 Task 2 (Nakov et al., 2013). The datasets are thus described in detail in the overview papers. The rerun uses the same training and development data, but new test data from Twitter and a “surprise domain”. The task consists of two subtasks: an expression-level subtask (Subtask A) and a message-level subtask (Subtask B). In subtask A, each tweet in a corpus contained a marked instance of a word or phrase. The goal is to determine whether that instance is positive, negative or neutral in that context. In subtask B, the goal is to classify whether the entire message is of positive, negative, or neutral sentiment. For messa"
S14-2126,R13-1026,0,0.0230076,"st part of the tweet when the word but is found. This approach helps to reduce the misleading positive hits when a negative message is introduced positively (It’d be good, but). For preprocessing the data, we use components from DKPro Core3 . Preprocessing is the same for subtasks A and B, with the only difference that in the subtask A the target expression is additionally annotated as text classification unit, while the rest of the tweet is considered to be a document context. We first segment the data with the Stanford Segmenter4 , apply the Stanford POS Tagger with a Twitter-trained model (Derczynski et al., 2013), and subsequently apply the Stanford Lemmatizer4 , TreeTagger Chunker (Schmid, 1994), Stanford Named Entity Recognizer (Finkel et al., 2005) and Stanford Parser (Klein and Manning, 2003) to each tweet. After this linguistic preprocessing, the token segmentation of the Stanford tools is removed and overwritten by the ArkTweet Tagger (Gimpel et al., 2011), which is more suitable for recognizing hashtags and smileys as one particular token. Finally, we expand the tweet and proceed to feature extraction as described in detail in Section 3. 2.2 3.1 3 4 Lexical features As a basis for our similarit"
S14-2126,P05-1045,0,0.00645162,"positively (It’d be good, but). For preprocessing the data, we use components from DKPro Core3 . Preprocessing is the same for subtasks A and B, with the only difference that in the subtask A the target expression is additionally annotated as text classification unit, while the rest of the tweet is considered to be a document context. We first segment the data with the Stanford Segmenter4 , apply the Stanford POS Tagger with a Twitter-trained model (Derczynski et al., 2013), and subsequently apply the Stanford Lemmatizer4 , TreeTagger Chunker (Schmid, 1994), Stanford Named Entity Recognizer (Finkel et al., 2005) and Stanford Parser (Klein and Manning, 2003) to each tweet. After this linguistic preprocessing, the token segmentation of the Stanford tools is removed and overwritten by the ArkTweet Tagger (Gimpel et al., 2011), which is more suitable for recognizing hashtags and smileys as one particular token. Finally, we expand the tweet and proceed to feature extraction as described in detail in Section 3. 2.2 3.1 3 4 Lexical features As a basis for our similarity and expansion experiments (sections 3.4 and 3.5), we use the binary sentiment polarity lexicon by Liu (2012) augmented with the smiley pola"
S14-2126,pak-paroubek-2010-twitter,0,0.0496104,"g alternatives, resulting in a noisy, little valuable feature (expansion full in Table 3). Setting up the threshold to 50 and cleaning up both the tweet and the expansion with Snowball stopword list (expansion clean in Table 3), the performance increased remarkably. Amongst other prominent features were parts of lexicons such as LIWC Positive emotions, LIWC Affect, LIWC Negative emotions, NRC Joy, NRC Anger and NRC Disgust. Informative were also the proportions of nouns, verbs and adverbs, the exclamation ratio or number of positive and negative smileys at the end of the tweet. Other features Pak and Paroubek (2010) pointed out a relation between the presence of different part-of-speech types and sentiment polarity. We measure the ratio of each part-of-speech type to each chunk. We furthermore count the occurrences of the dependency tag for negation. We use the Stanford Named Entity Recognizer to count occurrence of persons, organizations and locations in the tweet. Additionaly, beside basic surface metrics, such as the number of tokens, characters and sentences, we measure the number of elongated words (such as coool) in a tweet, ratio of sentences ending with exclamation, ratio of questions and number"
S14-2126,P11-2008,0,0.0641383,"Missing"
S14-2126,W02-1011,0,0.0161569,"features we consider only lexicons clearly meant for binary polarity, while a second set of features also includes other emotions, such as fear or anger, from the NRC and the LIWC corpora. 3.2 Negation We handle negation in two ways. On the expression level (Subtask A) we rely on the negation dependency tag provided by the Stanford Dependency Parser. This one captures verb negations rather precisely and thus helps to handle emotional verb expressions such as like vs don’t like. On the tweet level (all features of Subtask B and entiretweet-level features of Subtask A) we adopt the approach of Pang et al. (2002), considering as a negation context any sequence of tokens between a negation expression and the end of a sentence segment as annotated by the Stanford Segmenter. The negation expressions (don’t, can’t...) are represented by the list of invertors from Steinberger’s lexicon (Steinberger et al., 2012). We first assign polarity score to each word in the tweet based on the lexicon hits and then revert it for the words lying in the negation context. This approach is more robust than the one of the dependency governor but is error-prone in the area of overlapping (cascaded) negation contexts. 3.3 Sc"
S14-2126,W11-1704,0,\N,Missing
S14-2126,S14-2009,0,\N,Missing
S14-2126,P13-4021,1,\N,Missing
S17-2005,S17-2011,0,0.057962,"erographic puns, a list of candidate target words is produced using Damerau-Levenshtein (1964) distance. Among their corresponding WordNet senses, the system selects the one whose definition has the highest lexical overlap with the second partition. Fermi’s approach to pun location is a knowledge-based approach similar to that of ELiRF-UPV. For every pair of words in the context, a similarity score is calculated on the basis of the maximum pairwise similarity of their WordNet synsets. In the highest-scoring pair, the word closest to the end of the context is selected as the pun. Idiom Savant (Doogan et al., 2017). Idiom Savant uses a variety of different methods depending on the subtask and pun type, but which are generally based on Google n-grams and word2vec. Target recovery in heterographic puns involves computing phonetic distance with the aid of the CMU Pronouncing Dictionary. Uniquely among participating systems, Idiom Savant attempts to flag and specially process Tom Swifties, a genre of punning jokes commonly seen in the test data. UWaterloo (Vechtomova, 2017). UWaterloo is a rule-based pun locator that scores candidate words according to eleven simple heuristics. These heuristics involve the"
S17-2005,W09-2004,0,0.368595,"e document. However, there exists a class of language constructs known as puns, in which lexical-semantic ambiguity is a deliberate effect of the communication act. That is, the speaker or writer intends for a certain word or other lexical item to be interpreted as simultaneously carrying two or more separate meanings. Though puns are a recurrent and expected feature in many discourse 58 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 58–68, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 1997; Ritchie, 2005; Hong and Ong, 2009; Waller et al., 2009; Kawahara, 2010) or modelling their phonological properties (Hempelmann, 2003a,b). However, several studies have explored the detection and interpretation of puns (Yokogawa, 2002; Taylor and Mazlack, 2004; Miller and Gurevych, 2015; Kao et al., 2015; Miller and Turković, 2016; Miller, 2016); the most recent of these focus squarely on computational semantics. In this paper, we present the first organized public evaluation for the computational processing of puns. We believe computational interpretation of puns to be an important research question with a number of real-worl"
S17-2005,S17-2072,0,0.134221,"Missing"
S17-2005,H93-1061,0,0.361657,"dNet Baselines For each subtask, we provide results for various baselines: Pun detection. The only baseline we use for this subtask is a random classifier. It makes no assumption about the underlying class distribution, labelling each context as “pun” or “non-pun” with equal probability. On average, its recall and accuracy will therefore be 0.5, and its precision equal to the proportion of contexts containing puns. Pun location. For this subtask we present the results of three naïve baselines. The first simply selects one of the context words at random. The 3These counts come from the SemCor (Miller et al., 1993) corpus. 61 with the minimum Levenshtein (1966) distance to the lemmas in the first list. The most frequent sense of the lemmas in the second list is selected as the second meaning of the pun. In addition to the two naïve baselines, we also provide scores for the homographic pun interpretation system described by Miller and Gurevych (2015). This system works by running each pun through a variation of the Lesk (1986) algorithm that scores each candidate sense according to the lexical overlap with the pun’s context. The two top-scoring senses are then selected; in case of ties, the system attemp"
S17-2005,S17-2079,0,0.0742493,"Missing"
S17-2005,P15-1070,1,0.633835,"o be interpreted as simultaneously carrying two or more separate meanings. Though puns are a recurrent and expected feature in many discourse 58 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 58–68, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 1997; Ritchie, 2005; Hong and Ong, 2009; Waller et al., 2009; Kawahara, 2010) or modelling their phonological properties (Hempelmann, 2003a,b). However, several studies have explored the detection and interpretation of puns (Yokogawa, 2002; Taylor and Mazlack, 2004; Miller and Gurevych, 2015; Kao et al., 2015; Miller and Turković, 2016; Miller, 2016); the most recent of these focus squarely on computational semantics. In this paper, we present the first organized public evaluation for the computational processing of puns. We believe computational interpretation of puns to be an important research question with a number of real-world applications. For example: we are freely releasing to the research community.1 Our first data set, containing English homographic puns, is based on the one described by Miller and Turković (2016) and Miller (2016).2 It contains punning and non-punning"
S17-2005,W05-1614,0,0.262887,"each word in the document. However, there exists a class of language constructs known as puns, in which lexical-semantic ambiguity is a deliberate effect of the communication act. That is, the speaker or writer intends for a certain word or other lexical item to be interpreted as simultaneously carrying two or more separate meanings. Though puns are a recurrent and expected feature in many discourse 58 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 58–68, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 1997; Ritchie, 2005; Hong and Ong, 2009; Waller et al., 2009; Kawahara, 2010) or modelling their phonological properties (Hempelmann, 2003a,b). However, several studies have explored the detection and interpretation of puns (Yokogawa, 2002; Taylor and Mazlack, 2004; Miller and Gurevych, 2015; Kao et al., 2015; Miller and Turković, 2016; Miller, 2016); the most recent of these focus squarely on computational semantics. In this paper, we present the first organized public evaluation for the computational processing of puns. We believe computational interpretation of puns to be an important research question with a"
S17-2005,S17-2076,0,0.0492953,"(Xiu et al., 2017). ECNU uses a supervised approach to pun detection. The authors collected a training set of 60 homographic and 60 heterographic puns, plus 60 proverbs and famous sayings, from various Web sources. The data is then used to train a classifier, using features derived from WordNet and word2vec embeddings. The ECNU pun locator is knowledge-based, determining each context word’s likelihood of being the pun on the basis of the distance between its sense vectors, or between its senses and the context. Participating systems Our shared task saw participation from ten systems: BuzzSaw (Oele and Evang, 2017). BuzzSaw assumes that each meaning of the pun will exhibit high semantic similarity with one and only one part of the context. The system’s approach to homographic pun interpretation is to compute the semantic similarity between the two halves of every possible contiguous, binary partitioning of the context, retaining the partitioning with the lowest similarity between the two parts. A Lesk-like WSD algorithm based on word and sense embeddings is then used to disambiguate the pun word separately with respect to each part of the context. ELiRF-UPV (Hurtado et al., 2017). This system’s approach"
S17-2005,S17-2074,0,0.0826747,"Missing"
S17-2005,S17-2077,0,0.0888212,"a reference set of homophones and similar-sounding words. Only words in the second half of the context are scored; in the event of a tie, the system chooses the word closer to the end of the context. JU_CSE_NLP (Pramanick and Das, 2017). As a supervised approach, JU_CSE_NLP relies on a manually annotated data set of 413 puns sourced by the authors from Project Gutenberg. The data is used to train a hidden Markov model and cyclic dependency network, using features from a part-of-speech tagger and a syntactic parser. The classifiers are applied to the pun detection and location subtasks. UWAV (Vadehra, 2017). UWAV participated in the pun detection and location subtasks. The detection component is another supervised system, taking the votes of three classifiers (support vector machine, naïve Bayes, and logistic regression) trained on lexical-semantic and word embedding features of a manually annotated data set. PunFields (Mikhalkova and Karyakin, 2017). PunFields uses separate methods for pun 63 For pun location, UWAV splits the context in half and checks whether any word in the second half is in some predefined lists of homonyms, homophones, and antonyms. If so, one of those words is selected as"
S17-2005,passonneau-2006-measuring,0,0.0232294,"Missing"
S17-2005,S17-2071,0,0.0721381,"their WordNet synsets. In the highest-scoring pair, the word closest to the end of the context is selected as the pun. Idiom Savant (Doogan et al., 2017). Idiom Savant uses a variety of different methods depending on the subtask and pun type, but which are generally based on Google n-grams and word2vec. Target recovery in heterographic puns involves computing phonetic distance with the aid of the CMU Pronouncing Dictionary. Uniquely among participating systems, Idiom Savant attempts to flag and specially process Tom Swifties, a genre of punning jokes commonly seen in the test data. UWaterloo (Vechtomova, 2017). UWaterloo is a rule-based pun locator that scores candidate words according to eleven simple heuristics. These heuristics involve the position of the word within the context or relative to certain punctuation or function words, the word’s inverse document frequency in a large reference corpus, normalized pointwise mutual information (PMI) with other words in the context, and whether the word exists in a reference set of homophones and similar-sounding words. Only words in the second half of the context are scored; in the event of a tie, the system chooses the word closer to the end of the co"
S17-2005,S17-2070,0,0.0527715,"pun will be located near the end of the sentence. The system therefore calculates the similarity between every pair of non-adjacent words in the context using word2vec, retaining the pair with the highest similarity. The word in the pair that is closer to the end of the context is selected as the pun. The pun interpretation system is also used for homographic pun location. First, the interpretation system is run once for each polysemous word in the context. The word whose two disambiguated senses have maximum cosine distance between their sense embeddings is selected as the pun word. Duluth (Pedersen, 2017). For pun detection, the Duluth system assumes that all-words WSD systems will have difficulties in consistently assigning sense labels to contexts containing To interpret homographic puns, ELiRF-UPV first finds the two context words whose word embeddings are closest to that of the pun. 62 Then, for each context word, the system builds a bag-of-words representation for each of its candidate senses, and for each of the pun word’s candidate senses, using information from WordNet. The lexical overlap between every pair of pun and context senses is calculated, and the pun sense with the highest ov"
S17-2005,S17-2073,0,0.202653,"Missing"
S17-2005,S17-2078,0,0.0558615,"to the end of the context. Homographic pun interpretation is carried out by running various configurations of a WSD algorithm on the pun word and selecting the two most frequently returned senses. For heterographic puns, the system attempts to recover the target form either by generating a list of WordNet lemmas with minimal edit distance to the pun word, or by querying the Datamuse API for words with similar spellings, pronunciations, and meanings. WSD algorithms are then run separately on the pun and the set of target candidates, with the best matching pun and target senses retained. ECNU (Xiu et al., 2017). ECNU uses a supervised approach to pun detection. The authors collected a training set of 60 homographic and 60 heterographic puns, plus 60 proverbs and famous sayings, from various Web sources. The data is then used to train a classifier, using features derived from WordNet and word2vec embeddings. The ECNU pun locator is knowledge-based, determining each context word’s likelihood of being the pun on the basis of the distance between its sense vectors, or between its senses and the context. Participating systems Our shared task saw participation from ten systems: BuzzSaw (Oele and Evang, 20"
S17-2163,S17-2091,0,0.0764103,"Missing"
S17-2163,N16-1175,0,0.0333031,"Missing"
S17-2163,P14-2050,0,0.0187058,"we frame our classification problem as a mapping fθ (θ represents model parameters) from concatenated word embeddings to one of the three classes MATERIAL, PROCESS, and TASK: fθ : R`·d × Rc·d × Rr·d → {M, P, T}. Next, we describe the embeddings that we used and subsequently the machine learning models fθ . Word Embeddings We experimented with three kinds of word embeddings. We use the popular Glove embeddings (Pennington et al., 2014) (6B) of dimensions 50, 100, and 300, which largely capture semantic information. Further we employ the more syntactically oriented 300-dimensional embeddings of Levy and Goldberg (2014), as well as the 300dimensional embeddings of Komninos and Manandhar (2016), which are trained to predict both dependency- and standard window-based context. Deep Learning models Table 1: Class distribution in the datasets. Our first model is a character-level convolutional neural network (char-CNN) illustrated in Figure 1. This model (A) considers each of the three contexts (left, center, right) independently, representing them by a 100-dimensional vector as follows. Each character is represented by a 1-hot vector, which is then mapped to a 32-dimensional Inter-annotator agreement for the dat"
S17-2163,D14-1162,0,0.0792416,"Missing"
S17-2163,L16-1294,0,0.0161386,"nd standard window-based context. Deep Learning models Table 1: Class distribution in the datasets. Our first model is a character-level convolutional neural network (char-CNN) illustrated in Figure 1. This model (A) considers each of the three contexts (left, center, right) independently, representing them by a 100-dimensional vector as follows. Each character is represented by a 1-hot vector, which is then mapped to a 32-dimensional Inter-annotator agreement for the dataset was published to be between 0.45 and 0.85 (Cohen’s κ) (Augenstein et al., 2017). Reviewing similar annotation efforts (QasemiZadeh and Schumann, 2016) already shows that despite the seemingly simple annotation task, usually annotators do not reach high agreement neither on span of annotations nor the class assigned to each span3 . 3 100-d Implemented Approaches In this section, we describe the individual systems that form the basis of our experiments (see §4). Our basic setup for all of our systems was as follows. For each keyphrase we extracted its left context, right context and the keyphrase itself (center). We represent each of the three contexts as the concatenation of their word tokens: to have fixed-size representations, we limit the"
S18-2007,D13-1160,0,0.21072,", these systems have certain drawbacks in the QA setting: they are targeted at long well-formed documents, such as news texts, and are less suited for typically short and noisy question data. Other EL systems focus on noisy data (e.g. S-MART, Yang and Chang, 2015), but are not openly available and hence limited in their usage and application. Multiple error analyses of QA systems point to entity linking as a major external source of error (Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015). The QA datasets are normally collected from the web and contain very noisy and diverse data (Berant et al., 2013), which poses a number of challenges for EL. First, many common features used in EL systems, such as capitalization, are not meaningful on noisy data. Moreover, a question is a short text snippet that does not contain broader context that is helpful for entity disambiguation. The QA data also features many entities of various categories and differs in this respect from the Twitter datasets that are often used to evaluate EL systems. Introduction Knowledge base question answering (QA) requires a precise modeling of the question semantics through the entities and relations available in the knowl"
S18-2007,P14-1133,0,0.0493659,"model delivers a strong performance across different entity categories. 1 album Q24951125 for question answering (e.g. DBPedia Spotlight1 , AIDA2 ). However, these systems have certain drawbacks in the QA setting: they are targeted at long well-formed documents, such as news texts, and are less suited for typically short and noisy question data. Other EL systems focus on noisy data (e.g. S-MART, Yang and Chang, 2015), but are not openly available and hence limited in their usage and application. Multiple error analyses of QA systems point to entity linking as a major external source of error (Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015). The QA datasets are normally collected from the web and contain very noisy and diverse data (Berant et al., 2013), which poses a number of challenges for EL. First, many common features used in EL systems, such as capitalization, are not meaningful on noisy data. Moreover, a question is a short text snippet that does not contain broader context that is helpful for entity disambiguation. The QA data also features many entities of various categories and differs in this respect from the Twitter datasets that are often used to evaluate EL systems. Introduct"
S18-2007,D13-1085,0,0.134673,"is more diverse in this case. The WebQuestions dataset includes the Fictional Character and Thing categories which are almost absent from the NEEL dataset. A more even distribution can be observed in the GraphQuestion dataset that features many Events, Fictional Characters and Professions. This means that a successful system for EL on question data needs to be able to recognize and to link all categories of entities. Thus, we aim to show that comprehensive modeling of different context levels will result in a better generalization and performance across various entity categories. on QA data. Guo et al. (2013b) have created a new dataset of around 1500 tweets and suggested a Structured SVM approach that handled mention detection and entity disambiguation together. Chang et al. (2014) describe the winning system of the NEEL 2014 competition on EL for short texts: The system adapts a joint approach similar to Guo et al. (2013b), but uses the MART gradient boosting algorithm instead of the SVM and extends the feature set. The current state-of-the-art system for EL on noisy data is S-MART (Yang and Chang, 2015) which extends the approach from Chang et al. (2014) to make structured predictions. The sam"
S18-2007,E06-1002,0,0.100866,"Kr¨otzsch, 2014). Among the previous work, the common choices of a KB include Wikipedia, DBPedia and Freebase. The entities in Wikidata directly correspond to the Wikipedia articles, which enables us to work with data that was previously annotated with DBPedia. Freebase was discontinued and is no longer up-todate. However, most entities in Wikidata have been annotated with identifiers from other knowledge sources and databases, including Freebase, which establishes a link between the two KBs. Existing Solutions The early machine learning approaches to EL focused on long well-formed documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Han and Sun, 2012; Francis-Landau et al., 2016). These systems usually rely on an off-theshelf named entity recognizer to extract entity mentions in the input. As a consequence, such approaches can not handle entities of types other than those that are supplied by the named entity recognizer. Named entity recognizers are normally trained to detect mentions of Locations, Organizations and Person names, whereas in the context of QA, the system also needs to cover movie titles, songs, common nouns such as ‘president’ etc. To mitigate this, Cucerzan (2012) has introduced the idea"
S18-2007,D12-1010,0,0.0504679,"Missing"
S18-2007,D11-1072,0,0.130435,"Missing"
S18-2007,D15-1104,0,0.0189026,"amed entity recognizer to extract entity mentions in the input. As a consequence, such approaches can not handle entities of types other than those that are supplied by the named entity recognizer. Named entity recognizers are normally trained to detect mentions of Locations, Organizations and Person names, whereas in the context of QA, the system also needs to cover movie titles, songs, common nouns such as ‘president’ etc. To mitigate this, Cucerzan (2012) has introduced the idea to perform mention detection and entity linking jointly using a linear combination of manually defined features. Luo et al. (2015) have adopted the same idea and suggested a probabilistic graphical model for the joint prediction. This is essential for linking entities in questions. For example in “who does maggie grace play in taken?”, it is hard to distinguish between the usage of the word ‘taken’ and the title of a movie ‘Taken’ without consulting a knowledge base. Sun et al. (2015) were among the first to use neural networks to embed the mention and the entity for a better prediction quality. Later, FrancisLandau et al. (2016) have employed convolutional neural networks to extract features from the document context an"
S18-2007,D07-1074,0,0.256975,"he previous work, the common choices of a KB include Wikipedia, DBPedia and Freebase. The entities in Wikidata directly correspond to the Wikipedia articles, which enables us to work with data that was previously annotated with DBPedia. Freebase was discontinued and is no longer up-todate. However, most entities in Wikidata have been annotated with identifiers from other knowledge sources and databases, including Freebase, which establishes a link between the two KBs. Existing Solutions The early machine learning approaches to EL focused on long well-formed documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Han and Sun, 2012; Francis-Landau et al., 2016). These systems usually rely on an off-theshelf named entity recognizer to extract entity mentions in the input. As a consequence, such approaches can not handle entities of types other than those that are supplied by the named entity recognizer. Named entity recognizers are normally trained to detect mentions of Locations, Organizations and Person names, whereas in the context of QA, the system also needs to cover movie titles, songs, common nouns such as ‘president’ etc. To mitigate this, Cucerzan (2012) has introduced the idea to perform ment"
S18-2007,P14-5010,0,0.00370296,"eve ocati izati per arac prod thi ons e n i h l ga s . c fes or fic pro NEEL WebQSP GraphQuestions Figure 2: Distribution of entity categories in the NEEL 2014, WebQSP and GraphQuestions datasets 2 Motivation and Related Work Several benchmarks exist for EL on Wikipedia texts and news articles, such as ACE (Bentivogli et al., 2010) and CoNLL-YAGO (Hoffart et al., 2011). These datasets contain multi-sentence documents and largely cover three types of entities: Location, Person and Organization. These types are commonly recognized by named entity recognition systems, such as Stanford NER Tool (Manning et al., 2014). Therefore in this scenario, an EL system can solely focus on entity disambiguation. In the recent years, EL on Twitter data has emerged as a branch of entity linking research. In particular, EL on tweets was the central task of the NEEL shared task from 2014 to 2016 (Rizzo et al., 2017). Tweets share some of the challenges with QA data: in both cases the input data is short and noisy. On the other hand, it significantly differs with respect to the entity types covered. The data for the NEEL shared task was annotated with 7 broad entity categories, that besides Location, Organization and Pers"
S18-2007,N16-1150,0,0.0882436,"s of a KB include Wikipedia, DBPedia and Freebase. The entities in Wikidata directly correspond to the Wikipedia articles, which enables us to work with data that was previously annotated with DBPedia. Freebase was discontinued and is no longer up-todate. However, most entities in Wikidata have been annotated with identifiers from other knowledge sources and databases, including Freebase, which establishes a link between the two KBs. Existing Solutions The early machine learning approaches to EL focused on long well-formed documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Han and Sun, 2012; Francis-Landau et al., 2016). These systems usually rely on an off-theshelf named entity recognizer to extract entity mentions in the input. As a consequence, such approaches can not handle entities of types other than those that are supplied by the named entity recognizer. Named entity recognizers are normally trained to detect mentions of Locations, Organizations and Person names, whereas in the context of QA, the system also needs to cover movie titles, songs, common nouns such as ‘president’ etc. To mitigate this, Cucerzan (2012) has introduced the idea to perform mention detection and entity linking jointly using a"
S18-2007,D14-1162,0,0.0813015,"a fully-connected layer. x = what are taylor swift’s albums? Step 1. consider all n-grams N = ngrams(x), i = 0 i &lt; |N |, n = N [i] wikidata i=i+1 Step 2. entity candidates for an n-gram Full text search Context Components The token component corresponds to sentence-level features normally defined for EL and encodes the list of question tokens x into a fixed size vector. It maps the tokens in x to dw -dimensional pre-trained word embeddings, using a matrix W ∈ R|Vw |×dw , where |Vw |is the size of the vocabulary. We use 50-dimensional GloVe embeddings pre-trained on a 6 billion tokens corpus (Pennington et al., 2014). The word embeddings are concatenated with d p -dimensional position embeddings Pw ∈ R3×d p that are used to denote the tokens that are part of the target n-gram. The concatenated embeddings are processed by DCNNw to get a vector os . C = entity candidates(n) Step 3. score the n-gram with the model pn , pc = M(x, n, C) Step 4. compute the global assignment of entities G = global assignment(pn , pc , n, x|n ∈ N ) Figure 3: Architecture of the entity linking system certain length as entity mention candidates (Step 1). For each n-gram n, we look it up in the knowledge base using a full text sear"
S18-2007,Q14-1030,0,0.018581,"performance across different entity categories. 1 album Q24951125 for question answering (e.g. DBPedia Spotlight1 , AIDA2 ). However, these systems have certain drawbacks in the QA setting: they are targeted at long well-formed documents, such as news texts, and are less suited for typically short and noisy question data. Other EL systems focus on noisy data (e.g. S-MART, Yang and Chang, 2015), but are not openly available and hence limited in their usage and application. Multiple error analyses of QA systems point to entity linking as a major external source of error (Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015). The QA datasets are normally collected from the web and contain very noisy and diverse data (Berant et al., 2013), which poses a number of challenges for EL. First, many common features used in EL systems, such as capitalization, are not meaningful on noisy data. Moreover, a question is a short text snippet that does not contain broader context that is helpful for entity disambiguation. The QA data also features many entities of various categories and differs in this respect from the Twitter datasets that are often used to evaluate EL systems. Introduction Knowledge base q"
S18-2007,P15-1128,0,0.46205,"ifferent entity categories. 1 album Q24951125 for question answering (e.g. DBPedia Spotlight1 , AIDA2 ). However, these systems have certain drawbacks in the QA setting: they are targeted at long well-formed documents, such as news texts, and are less suited for typically short and noisy question data. Other EL systems focus on noisy data (e.g. S-MART, Yang and Chang, 2015), but are not openly available and hence limited in their usage and application. Multiple error analyses of QA systems point to entity linking as a major external source of error (Berant and Liang, 2014; Reddy et al., 2014; Yih et al., 2015). The QA datasets are normally collected from the web and contain very noisy and diverse data (Berant et al., 2013), which poses a number of challenges for EL. First, many common features used in EL systems, such as capitalization, are not meaningful on noisy data. Moreover, a question is a short text snippet that does not contain broader context that is helpful for entity disambiguation. The QA data also features many entities of various categories and differs in this respect from the Twitter datasets that are often used to evaluate EL systems. Introduction Knowledge base question answering ("
S18-2007,P16-2033,0,0.014348,"ty of each combination. The combination with the highest probability is selected as the final set of entity mentions. We have observed in practice a similar effect as descirbed by Strubell et al. (2017), namely that DCNNs are able to capture dependencies between different entity mentions in the same context and do not tend to produce overlapping mentions. 3.3 #Questions ∑ ∑ M (tn , pn ) n∈N c∈Cn + tn D(tc , pc ), 4 3794 2002 GraphQuestions Test 2608 4680 Datasets We compile two new datasets for entity linking on questions that we derive from publicly available question answering data: WebQSP (Yih et al., 2016) and GraphQuestions (Su et al., 2016). WebQSP contains questions that were originally collected for the WebQuestions dataset from web search logs (Berant et al., 2013). They were manually annotated with SPARQL queries that can be executed to retrieve the correct answer to each question. Additionally, the annotators have also selected the main entity in the question that is central to finding the answer. The annotations and the query use identifiers from the Freebase knowledge base. We extract all entities that are mentioned in the question from the SPARQL query. For the main entity, we also st"
S18-2007,P17-1053,0,0.0281173,"Missing"
S18-2007,D17-1283,0,0.0644028,"Missing"
S18-2007,D16-1054,0,0.0261704,"Missing"
S18-2007,P15-1049,0,\N,Missing
S18-2027,D15-1082,0,0.034942,"nd relations are learned by minimizing a margin-based ranking objective that aims to score positive triples higher than negative triples based on their energies and a predefined margin. TransE is a simple and effective method, however, the simple translational assumption constrains the performance when dealing with complex relations, such as one-to-many or many-toone. To address this limitation, some extensions of TransE have been proposed. Wang et al. (2014) introduced TransH, which uses translations on relation-specific hyperplanes and applies advanced methods for sampling negative triples. Lin et al. (2015b) proposed TransR, which uses separate spaces for modeling entities and relations. Entities are projected from their space to the corresponding relation space by relation-specific matrices. Moreover, they propose an extension called CTransR, in which instances of pairs of head and tail for a specific relation are clustered such that the members of the clusters exhibit similar meanings of this relation. Lin et al. (2015a) proposed another extension of TransE, called PTransE, that leverages multi-step relation path information in the process of representation learning. The above models rely onl"
S18-2027,D14-1162,0,0.0927826,"al., 2013). The provided embeddings are 1000 dimensional and are trained using the skipgram model over the Google 100B token news dataset. We applied L2 normalization on the generated embeddings. The entities of the WN9-IMG dataset correspond to word senses rather than to individual words. In order to create embeddings for the synsets, we used the AutoExtend framework (Rothe and Sch¨utze, 2015), which enables creating sense embeddings for a given sense based on the embeddings of the contained lemmas. For this purpose, we initialized AutoExtend with pretrained 300-dimensional GloVe embeddings (Pennington et al., 2014). In case where no pre-trained embeddings are found for the sense lemmas, AutoExtend generates zero initialized vectors for the corresponding synsets. In order to provide better representations, we define the embeddings of such synsets by copying the embeddings of the first hyperonym synset that has non-zero AutoExtend embeddings. The linguistic embeddings of WN9-IMG entities (synsets) are 300-dimensional vectors, which were also L2 -normalized. 4.3 Experimental Setup We investigated different sets of hyperparameters for training the model. The best results were obtained using the Adam optimiz"
S18-2027,P15-1173,0,0.0603149,"Missing"
S18-2027,N16-1020,0,0.0315456,"al representations; (2) we investigate different methods for combining multimodal representations and evaluate their performance; (3) we introduce a new large-scale dataset for multimodal KGC based on Freebase; (4) we experimentally demonstrate that our approach outperforms baseline approaches including the state-of-the-art method of Xie et al. (2017) on the link prediction and triple classification tasks. 2.2 Multimodal Methods Recent advances in natural language processing have witnessed a greater interest in leveraging 226 3.1 multimodal information for a wide range of tasks. For instance, Shutova et al. (2016) showed that better metaphor identification can be achieved by fusing linguistic and visual representations. Collell et al. (2017) demonstrated the effectiveness of combining linguistic and visual embeddings in the context of word relatedness and similarity tasks. Regarding KG representation learning, the first and, to the best of our knowledge, only attempt that considers multimodal data is the work of Xie et al. (2017). Their IKRL approach extends TransE based on visual representations extracted from images that correspond to the KG entities. In IKRL, the energy of a triple is defined in ter"
W02-0207,J96-2004,0,0.201028,"Missing"
W02-0207,W01-1612,1,0.888456,"Missing"
W02-0207,rapp-strube-2002-iterative,1,0.78873,"Missing"
W02-0220,W01-1614,0,\N,Missing
W02-0220,W01-0902,0,\N,Missing
W02-0220,W01-1608,0,\N,Missing
W03-0809,N03-1012,1,\N,Missing
W03-0809,W03-0903,1,\N,Missing
W03-0809,W03-0811,1,\N,Missing
W03-0903,P96-1009,0,0.0431571,"nd choices helping to construct an ontology, which is shared by multiple components of the system, can be re-used in different projects and applied to various tasks. Finally, examples highlighting the usefulness of our approach are given. 1 Introduction The ways in which knowledge has been represented in multi-modal dialogue systems (MMDS) show that individual representations with different semantics and heterogeneously structured content can be found in various formats within single natural language processing (NLP) systems and applications. For example, a typical NLP system, such as TRAINS (Allen et al., 1996), employs different knowledge representations for parsing, action planning and generation, despite the fact that what is being represented is common to all those representations, e. g., the parser representation for going from A to B has no similarity to the action planner’s representation thereof (Ferguson et al., 1996). Also central concepts, for example city, are represented in multiple ways throughout the system. The origin for this state of affairs is that the respective knowledge stores are hand-crafted individually for each task. Sometimes they are compiled into code and cease to be ext"
W03-0903,P98-1013,0,0.068935,"way, the basic top-level ontological categorization in our system divides all concepts into two classes Type and Role (see Figure 1). As the class Type includes concepts with primary ontological status independent of the particular application, every system using the ontology for its specific purposes deals with the class Role. Top Role Event Physical Object Process Abstract Object Abstract Event Type 4.3 Representing Processes The modeling of Process as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the F RAME N ET data (Baker et al., 1998). Based on the analysis of our dialogue data, we developed the following classification of processes (see Figure 2): GeneralProcess, a set of the most general processes such as duplication, imitation or repetition processes; MentalProcess, a set of processes such as cognitive, emotional or perceptual processes; PhysicalProcess, a set of processes such as motion, transaction or controlling processes; Type Abstract Event Event’s are further classified in PhysicalObject and Process. In contrast to abstract objects, they have a location in space and time. The class PhysicalObject describes any kin"
W03-0903,W02-0207,1,0.508983,"for various processing modules of the system is additionally used as the basis for evaluating the semantic coherence of sets of concepts. The scoring software performs a number of processing steps: converting each SRH into a concept representation. For this purpose, each entry of the system’s lexicon was augmented with zero, one or multiple ontology concepts; converting the domain model, i.e. an ontology, into a directed graph with concepts as nodes and relations as edges; scoring concept representations using the shortest path between concepts based scoring metric. For example, in our data (Gurevych et al., 2002) a user expressed the wish to get more information about a specific church, as: (3) Kann ich bitte Informationen zur May I please Information about the Heiliggeistkirche bekommen Church of Holy Spirit get Looking at two SRHs from the ensuing n-best list we found that Example (5) constituted a suitable representation of the utterance, whereas Example (4) constituted a less adequate representation thereof, labeled accordingly by the human annotators: (4) Kann ich Information zur May I Information about the Heiliggeistkirche kommen Church of Holy Spirit come (5) Kann ich Information zur May I Inf"
W03-0903,N03-1012,1,0.725314,"of the ontology and its portability with respect to new applications and NLP tasks. The ontology described here is used by the complete core of the system (L¨ockelt et al., 2002). In the next sections we give some examples of the usage within the project. 5 All examples are displayed with the Germano riginal on top and a glossed translation below. 5.1 Semantic Coherence Scoring We introduced the notion of semantic coherence as a special measurement which can be applied to estimate how well a given speech recognition hypothesis (SRH) fits with respect to the existing knowledge representation (Gurevych et al., 2003). This provides a mechanism increasing the robustness and reliability of multi-modal dialogue systems. 5.1.1 Challenge One of the major challenges in making an MMDS reliable enough to be deployed in more complex real world applications is an accurate recognition of the users’ input. In many cases both correct and incorrect representations of the users’ utterances are contained in the automatic speech recognizer’s n-best lists. Facing multiple representations of a single utterance poses the question, which of the different hypotheses corresponds most likely to the user’s utterance. Different me"
W03-0903,P92-1004,0,0.0490998,"ution There are several advantages for using an ontology. First, it enables a convenient way for interpreting common phenomena like partial utterances and ellipses. Second, and most notably, using overlay (Alexandersson and Becker, 2003) we can straightforwardly inherit information from one discourse state to another, even if the focussed instance of the ontology is from a different but related type than the one of the current hypothesis. The advantage of this technique becomes evident in the dialogue excerpt below. The data structure of the discourse memory is based on the ideas presented in LuperFoy (1992), Salmon-Alt (2000). A three-tiered partition of a modality, discourse and domain layer is connected with a double threaded focus structure. A non-monotonic unification-like operation called overlay serves as the main algorithm for manipulating instances of the ontology. It combines new information (cover) with old context information (background) by unifying where possible, and overwriting where unification would fail. Additionally, the operation does not fail if the types differ, but assimilates the background to the type of the cover - thereby possibly deleting information of the background"
W03-0903,C98-1013,0,\N,Missing
W03-2115,P98-1013,0,0.0311643,"d their slots, i.e. the named edges of the graph corresponding to the class properties, constraints and restrictions. The ontology employed for the evaluation has about 730 concepts and 200 relations. It includes a generic top-level ontology whose purpose is to provide a basic structure of the world, i.e. abstract classes to divide the universe in distinct parts as resulting from the ontological analysis.6 The modeling of Processes and Physical Objects as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the F RAME N ET data (Baker et al., 1998). The hierarchy of Processes is connected to the hierarchy of Physical Objects via slot-constraint definitions. See also (Gurevych et al., 2003b) for a further description of the ontology. O NTO S CORE performs a number of processing steps. A first preprocessing step is to convert each SRH into a concept representation (CR). For that purpose we augmented the system’s lexicon with specific concept mappings. That is, for each entry in the lexicon either zero, one or many corresponding concepts where added. A simple vector of concepts corresponding to the words in the SRH for which entries in the"
W03-2115,J96-2004,0,0.0645589,"Missing"
W03-2115,W02-0207,1,0.867112,"Missing"
W03-2115,N03-1012,1,0.573867,"or the evaluation has about 730 concepts and 200 relations. It includes a generic top-level ontology whose purpose is to provide a basic structure of the world, i.e. abstract classes to divide the universe in distinct parts as resulting from the ontological analysis.6 The modeling of Processes and Physical Objects as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the F RAME N ET data (Baker et al., 1998). The hierarchy of Processes is connected to the hierarchy of Physical Objects via slot-constraint definitions. See also (Gurevych et al., 2003b) for a further description of the ontology. O NTO S CORE performs a number of processing steps. A first preprocessing step is to convert each SRH into a concept representation (CR). For that purpose we augmented the system’s lexicon with specific concept mappings. That is, for each entry in the lexicon either zero, one or many corresponding concepts where added. A simple vector of concepts corresponding to the words in the SRH for which entries in the lexicon exist - constitutes each resulting CR. All other words with empty concept mappings, e.g. articles and aspectual markers, are ignored i"
W03-2115,W03-0903,1,0.852999,"or the evaluation has about 730 concepts and 200 relations. It includes a generic top-level ontology whose purpose is to provide a basic structure of the world, i.e. abstract classes to divide the universe in distinct parts as resulting from the ontological analysis.6 The modeling of Processes and Physical Objects as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the F RAME N ET data (Baker et al., 1998). The hierarchy of Processes is connected to the hierarchy of Physical Objects via slot-constraint definitions. See also (Gurevych et al., 2003b) for a further description of the ontology. O NTO S CORE performs a number of processing steps. A first preprocessing step is to convert each SRH into a concept representation (CR). For that purpose we augmented the system’s lexicon with specific concept mappings. That is, for each entry in the lexicon either zero, one or many corresponding concepts where added. A simple vector of concepts corresponding to the words in the SRH for which entries in the lexicon exist - constitutes each resulting CR. All other words with empty concept mappings, e.g. articles and aspectual markers, are ignored i"
W03-2115,C98-1013,0,\N,Missing
W04-3012,P98-1013,0,0.0298857,"component (Gurevych et al., 2003a; Gurevych and Porzel, 2003). In any of these cases, it is necessary to provide a systematic account of domain and world knowledge. These types of knowledge have largely been ignored so far in ASR research. The reason for this state of affairs lies in the fact that the manual construction of appropriate knowledge sources for broad domains is extremely costly. Also, easy domain portability is an important requirement for any ASR system. The emergence of wide coverage linguistic knowledge bases for multiple languages, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998; Baker et al., 2003), PropBank (Palmer et al., 2003; Xue et al., 2004) is likely to change this situation. Domain recognition, which is the central topic of this paper, can be thought of as high-level semantic tagging of utterances. We expect significant improvements in the performance of the ASR component of the system if information about the current domain of discourse is available. An obvious intuition behind this expectation is that knowing the current domain of discourse narrows down the search space of the speech recognizer. It also allows to rule out incoherent speech recognition hypo"
W04-3012,N03-1012,1,0.907939,"y of natural language processing tasks, e.g. Mahesh and Nirenburg (1995), Flycht-Eriksson (2003). Our ontology models the domains Electronic Program Guide, Interaction Management, Cinema Information, Personal Assistance, Route Planning, Sights, Home Appliances Control and Off Talk. The hierarchically structured ontology consists of ca. 720 concepts and 230 properties specifying relations between concepts. For example every instance of the concept Process features the relations hasBeginTime, hasEndTime and hasState. A detailed description of the ontology employed in our experiments is given in Gurevych et al. (2003b). Ontological concepts are high-level units. They allow to reduce the amount of information needed to represent relations existing between individual lexemes and to effectively incorporate this knowledge into automatic language processing. E.g., there may exist a large number of movies in a cinema reservation system. All of them will be represented by the concept Movie, thus allowing to map a variety Domain models For scoring high-level linguistic representations of utterances we use a domain model. A domain model is a two-dimensional matrix DM with the dimensions (#d × #c), where #d and #c"
W04-3012,W03-0903,1,0.836408,"y of natural language processing tasks, e.g. Mahesh and Nirenburg (1995), Flycht-Eriksson (2003). Our ontology models the domains Electronic Program Guide, Interaction Management, Cinema Information, Personal Assistance, Route Planning, Sights, Home Appliances Control and Off Talk. The hierarchically structured ontology consists of ca. 720 concepts and 230 properties specifying relations between concepts. For example every instance of the concept Process features the relations hasBeginTime, hasEndTime and hasState. A detailed description of the ontology employed in our experiments is given in Gurevych et al. (2003b). Ontological concepts are high-level units. They allow to reduce the amount of information needed to represent relations existing between individual lexemes and to effectively incorporate this knowledge into automatic language processing. E.g., there may exist a large number of movies in a cinema reservation system. All of them will be represented by the concept Movie, thus allowing to map a variety Domain models For scoring high-level linguistic representations of utterances we use a domain model. A domain model is a two-dimensional matrix DM with the dimensions (#d × #c), where #d and #c"
W04-3012,W03-2117,0,0.0269202,"Missing"
W04-3012,rapp-strube-2002-iterative,0,0.0147629,"individual domains. In the case of human annotations, we deal with binary values, whereas tf*idf scores range over the interval [0,1]. 3 Data and Annotation Experiments We performed a number of annotation experiments. The purpose of these experiments was to: • investigate the reliability of the annotations; • create a domain model based on human annotations; • produce a training dataset for statistical classifiers; • set a Gold Standard as a test dataset for the evaluation. All annotation experiments were conducted on data collected in hidden-operator tests following the paradigm described in Rapp and Strube (2002). Subjects were asked to verbalize a predefined intention in each of their turns, the system’s reaction was simulated by a human operator. We collected utterances from 29 subjects in 8 dialogues with the system each. All user turns were recorded in separate audio files. These audio files were processed by two versions of our dialogue system with different speech recognition modules. Data describing our corpora is given in Table 1. The first and the second system’s runs are referred to as Dataset 1 and Dataset 2 respectively. Number of dialogues Number of utterances Number of SRHs Number of coh"
W04-3012,J00-3003,0,0.0738581,"Missing"
W04-3012,C98-1013,0,\N,Missing
W04-3012,J05-1004,0,\N,Missing
W06-1104,I05-1067,1,0.724261,", word sense disambiguation, text summarization or spelling correction. It is defined on different kinds of textual units, e.g. documents, parts of a document (e.g. words and their surrounding context), words or concepts (Lebart and Rajman, 2000).2 Linguistic distance between words is inverse to their semantic similarity or relatedness. Semantic similarity is typically defined via the lexical relations of synonymy (automobile – car) and hypernymy (vehicle – car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist between two words (Gurevych, 2005).3 Dissimilar words can be semantically related, e.g. via functional relationships (night – dark) or when they are antonyms (high – low). Many NLP applications require knowledge about semantic relatedness rather than just similarity (Budanitsky and Hirst, 2006). A number of competing approaches for computing semantic relatedness of words have been developed (see Section 2). A commonly accepted method for evaluating these approaches is to compare their results with a gold standard based on human judgments on word pairs. For that purpose, relatedness scores for each word pair have to be determin"
W06-1104,O97-1002,0,0.0826444,"edness in mind. Word pairs consisted only of nounnoun combinations and only general terms were included. Polysemous and homonymous words were not disambiguated to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether"
W06-1104,W04-2607,0,0.0254114,"to cope with domain-specific or technical terms. This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car). Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of. Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations. Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e. other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity. Previous studies only considered semantic relatedness (or similarity) of words rather than concepts. However, polysemous or homonymous words should be annotated on the level of concepts. If we assume that bank has two meanings (“financial institution” vs. “river bank”)5 and it is paired with money, the result is two sense quali5 4 4.1 Experiment System architecture Figure 1 gives an overview of our"
W06-1104,H05-1077,0,0.03395,"odenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005). Results of such experiments are hard to quantify and cannot easily serve as the basis for evaluating SR measures. Rubenstein and Goodenough selected word pairs analytically to cover the whole spectrum of 4 That means, whether it fulfills some mathematical criteria: d(x, y) ≥ 0; d(x, y) = 0 ⇔ x = y; d(x, y) = d(y, x); d(x, z) ≤ d(x, y) + d(y, z). 17 PAPER R/G (1965) M/C (1991) Res (1995) Fin (2002) Gur (2005) Gur (2006) Z/G (2006) L ANGUAGE English English English English German German German PAIRS 65 30 30 353 65 350 328 POS N N N N, V, A N N, V, A N, V, A R EL - TYPE sim sim sim relat sim r"
W06-1104,J05-4002,0,0.0115492,"nounnoun combinations and only general terms were included. Polysemous and homonymous words were not disambiguated to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain"
W06-1104,P94-1019,0,0.0168389,"mber of word pairs was manually selected, with semantic similarity instead of relatedness in mind. Word pairs consisted only of nounnoun combinations and only general terms were included. Polysemous and homonymous words were not disambiguated to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematica"
W06-1104,J06-1003,0,\N,Missing
W07-0201,I05-1067,1,0.716974,"d. More problematic is that Fin353 consists of two subsets, which have been annotated by a different number of annotators. We performed further analysis of their dataset and found that the inter-annotator agreement7 differs considerably. These results suggest that further evaluation based on this data should actually regard it as two independent datasets. As Wikipedia is a multi-lingual resource, we are not bound to English datasets. Several German datasets are available that are larger than the existing English datasets and do not share the problems of the Finkelstein datasets (see Table 2). Gurevych (2005) conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965), but argued that the dataset is too small and only contains noun-noun pairs connected 6 Note that we do not use multiple-choice synonym question datasets (Jarmasz and Szpakowicz, 2003), as this is a different task, which is not addressed in this paper. 7 We computed the correlation for all annotators pairwise and summarized the values using a Fisher Z-value transformation. DATASET RG65 MC30 Res30 Fin353 Y EAR 1965 1991 1995 2002 L ANGUAGE English English English English Gur65 Gur350 ZG222 2"
W07-0201,P94-1019,0,0.0154613,"e, we cannot use the WCG directly to compute SR. Additionally, the WCG would not provide sufficient coverage, as it is relatively small. Thus, transferring SR measures to the WCG requires some modifications. The task of estimating SR between terms is casted to the task of SR between Wikipedia articles devoted to these terms. SR between articles is measured via the categories assigned to these articles. Leacock and Chodorow (1998, LC) normalize the path-length with the depth of the graph, simLC (n1 , n2 ) = − log l(n1 , n2 ) 2 × depth where depth is the length of the longest path in the graph. Wu and Palmer (1994, WP) introduce a measure that uses the notion of a lowest common subsumer of 4 3.2 Adapting SR Measures to Wikipedia X simply delete one of the links running on the same level. This strategy never disconnects any nodes from a connected component. X X+1 X+1 X+1 4 Semantic Relatedness Experiments X X+1 X X OR X+1 X+1 A commonly accepted method for evaluating SR measures is to compare their results with a gold standard dataset based on human judgments on word pairs.6 X+1 X+1 4.1 Figure 3: Breaking cycles in the WCG. We define C1 and C2 as the set of categories assigned to article ai and aj , res"
W07-0201,O97-1002,0,0.0560794,"ber of competing approaches for computing semantic relatedness between words using a graph structure, and then discuss the changes that are necessary to adapt semantic relatedness algorithms to work on the WCG. 3.1 two nodes lcs(n1 , n2 ). In a directed graph, a lcs is the parent of both child nodes with the largest depth in the graph. simW P = 2 depth(lcs) l(n1 , lcs) + l(n2 , lcs) + 2 depth(lcs) Resnik (1995, Res), defines semantic similarity between two nodes as the information content (IC) value of their lcs. He used the relative corpus frequency to estimate the information content value. Jiang and Conrath (1997, JC) additionally use the IC of the nodes. distJC (n1 , n2 ) = IC(n1 ) + IC(n2 ) − 2IC(lcs) Note that JC returns a distance value instead of a similarity value. Lin (1998, Lin) defined semantic similarity using a formula derived from information theory. simLin (n1 , n2 ) = 2 × IC(lcs) IC(n1 ) + IC(n2 ) Because polysemous words may have more than one corresponding node in a semantic wordnet, the resulting semantic relatedness between two words w1 and w2 can be calculated as   min dist(n1 , n2 ) path  n1 ∈s(w1 ),n2 ∈s(w2 ) SR =  max sim(n1 , n2 ) IC  n1 ∈s(w1 ),n2 ∈s(w2 ) Wordnet Based Mea"
W07-0201,W06-1104,1,0.380253,",1,2,3,4} discrete {0,1,2,3,4} discrete {0,1,2,3,4} # S UBJECTS 51 38 10 13/16 13 16 24 8 21 C ORRELATION r I NTER I NTRA .850 .903 .731 .549 .810 .690 .490 .647 Table 2: Comparison of German datasets used for evaluating semantic relatedness. by either synonymy or hyponymy. Thus, she created a larger German dataset containing 350 word pairs (Gur350). It contains nouns, verbs and adjectives that are connected by classical and nonclassical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected. Thus, Zesch and Gurevych (2006) used a semi-automatic process to create word pairs from domain-specific corpora. The resulting ZG222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations. Hence, it is particularly suited for analyzing the capability of a measure to estimate SR. 0.80 Gur65 0.60 0.40 0.20 0.00 GN Res JC Lin PL WP LC Correlation r 0.75 0.50 0.42 0.51 0.34 0.45 0.35 0.80 Gur350 0.60 4.2 Results and Discussion Figure 4 gives an overview of our experimental results of evaluating SR measures based on the WCG on three German datasets. We use Pearson’s product moment correlati"
W07-0201,N07-2052,1,0.779025,"al., 2006) have not treated them separately. However, the WCG should be treated separately, as it differs from the article graph. Article links are established because of any kind of relation between 3 Wikipedia can be downloaded //download.wikimedia.org/ from http: 2 articles, while links between categories are typically established because of hyponymy or meronymy relations. Holloway et al. (2005) create and visualize a category map based on co-occurrence of categories. Voss (2006) pointed out that the WCG is a kind of thesaurus that combines collaborative tagging and hierarchical indexing. Zesch et al. (2007a) identified the WCG as a valueable source of lexical semantic knowledge, but did not analytically analyze its properties. However, even if the WCG seems to be very similar to other semantic wordnets, a graph-theoretic analysis of the WCG is necessary to substantiate this claim. It is carried out in the next section. 2 Graph-theoretic Analysis of the WCG A graph-theoretic analysis of the WCG is required to estimate, whether graph based semantic relatedness measures developed for semantic wordnets can be transferred to the WCG. This is substantiated in a case study on computing semantic relate"
W07-0201,W04-2607,0,\N,Missing
W07-0201,J06-1003,0,\N,Missing
W08-0906,N03-1003,0,0.0446956,"estion paraphrasing have also already been investigated (section 2.3). 2.1 Sentence Paraphrase Identification Paraphrases are alternative ways to convey the same information (Barzilay and McKeown, 2001). Paraphrases can be found at different levels of linguistic structure: words, phrases and whole sentences. While word and phrasal paraphrases can be assimilated to the well-studied notion of syn45 onymy, sentence level paraphrasing is more difficult to grasp and cannot be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being compared. String similarity metrics, when applied to sentences, consist in comparing the words contained in the sentences. There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al., 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Semantic similarity measures are obtained by first com"
W08-0906,P01-1008,0,0.0461068,"es. Our methods to identify question paraphrases are detailed in section 4. Finally, we present and analyse the experimental results obtained in section 5 and conclude in section 6. 2 Related Work The identification of question paraphrases in question and answer repositories is related to research focusing on sentence paraphrase identification (section 2.1) and query paraphrasing (section 2.2). The specific features of question paraphrasing have also already been investigated (section 2.3). 2.1 Sentence Paraphrase Identification Paraphrases are alternative ways to convey the same information (Barzilay and McKeown, 2001). Paraphrases can be found at different levels of linguistic structure: words, phrases and whole sentences. While word and phrasal paraphrases can be assimilated to the well-studied notion of syn45 onymy, sentence level paraphrasing is more difficult to grasp and cannot be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being com"
W08-0906,C04-1051,0,0.122559,"word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being compared. String similarity metrics, when applied to sentences, consist in comparing the words contained in the sentences. There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al., 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Semantic similarity measures are obtained by first computing the semantic similarity of the words contained in the sentences being compared. Mihalcea et al. (2006) use both corpusbased and knowledge-based measures of the semantic similarity between words. Both string similarity and semantic similarity might be combined: for instance, Islam and Inkpen (2007) combine semantic similarity with longest common subsequence string similarity, while Li et al. (2006) make additional use of word order similarity. 2.2 Query Paraphrasing In Information Re"
W08-0906,P07-2032,1,0.593697,"Missing"
W11-0122,J08-4004,0,0.0479438,"e provided sense alignment candidate pairs, each consisting of a WordNet synset and a Wikipedia article. The annotation task was to label each sense pair either as alignment or not. Table 2 outlines the class distribution for three annotators and the majority decision. The most sense alignment candidates were annotated as non-alignments; only between 210 and 244 sense pairs were considered as alignments (extracted for 320 WordNet synsets). To assess the reliability of the annotators’ decision, we computed the pairwise observed inter-annotator agreement AO and the chance-corrected agreement κ (Artstein and Poesio, 2008)5 . The agreement values are shown in Table 3. The average observed agreement AO is 0.9721, while the multi-κ is 0.8727 indicating high reliability. The final dataset was compiled by means of a majority decision. Given 1,815 sense alignment candidate pairs, 1,588 were annotated as non-alignments, while 227 were annotated as alignments. 215 synsets were aligned with one article, while 6 synsets were aligned with two articles. Interesting to note is that the aligned samples are uniformly distributed among the different sampling dimensions as shown in Table 1 (right column). It demonstrates that"
W11-0122,N07-1025,0,0.112424,"010). Second, word senses contained in both resources can then be represented by relational information from WordNet and encyclopedic information from Wikipedia in a multilingual manner yielding an enriched knowledge representation. And finally, the third major benefit of the alignment is the ability to automatically acquire sense-tagged corpora in a mono- and multilingual fashion. For each WordNet synset, the text of the aligned Wikipedia article (or all sentences or paragraphs in Wikipedia that contain a link to the article) can be automatically extracted similar to the approach proposed by Mihalcea (2007). Automatically generated sense-tagged corpora can be used to, e.g., counter the bottleneck of supervised WSD methods that rely on such sense-tagged text collections, which are rare. Further, due to the cross-lingual links in Wikipedia, also corpora in different languages can be constructed easily. Our contribution to this paper is two-fold. First, we propose a novel two-step approach to align WordNet synsets and Wikipedia articles. We model the task as a word sense disambiguation problem applying the Personalized PageRank algorithm proposed by Agirre and Soroa (2009) as it is state-of-the-art"
W11-0122,P10-1154,0,0.104847,"es similar to our approach. A Wikipedia category is thereby represented by its main article or an article, which has the same title string as the category. Wu and Weld (2008) integrate the Wikipedia’s infobox information with WordNet to build a rich ontology using statistical-relational learning. Ruiz-Casado et al. (2005) proposed a method to align WordNet synsets and Wikipedia articles (instead of categories). They align articles of the Simple English Wikipedia to their most similar WordNet synsets depending on the vector-based similarity of the synset’s gloss and the article text. Recently, Ponzetto and Navigli (2010) presented a method based on a conditional probability p(s|w) of selecting the WordNet sense s given the Wikipedia article w, whereas the conditional probability relies on a normalized word overlap measure of the textual sense representation. Both approaches, however, have the following two major drawbacks: first, the algorithms are modeled such that they always assume a counterpart in WordNet for a given Wikipedia article, which does not hold for the English Wikipedia (see Section 4). Second, the algorithms always assign the most likely WordNet synset to a Wikipedia article, not allowing mult"
W11-0122,R09-1080,0,0.115242,"enrich WordNet with named entities mined from Wikipedia. Therefore, the noun is-a hierarchy of WordNet is mapped to the Wikipedia categories determining the overlap of articles belonging to the category and the instances for each of the senses of a polysemous word in WordNet. Ponzetto and Navigli (2009) applied a knowledge-rich method which maximizes the structural overlap between the WordNet taxonomy and the category graph extracted from Wikipedia. Based on the mapping information, the taxonomy automatically generated from the Wikipedia category graph is restructured to enhance the quality. Toral et al. (2009) disambiguate WordNet noun synsets and Wikipedia categories using multiple text similarity measures similar to our approach. A Wikipedia category is thereby represented by its main article or an article, which has the same title string as the category. Wu and Weld (2008) integrate the Wikipedia’s infobox information with WordNet to build a rich ontology using statistical-relational learning. Ruiz-Casado et al. (2005) proposed a method to align WordNet synsets and Wikipedia articles (instead of categories). They align articles of the Simple English Wikipedia to their most similar WordNet synset"
W11-0122,toral-etal-2008-named,0,0.146035,"it is hard to cope with neologisms, named entities, or rare usages on a large scale (Agirre and Edmonds, 2006; Meyer and Gurevych, 2010). In order to compensate for WordNet’s lack of coverage, Wikipedia has turned out to be a valuable resource in the NLP community. Wikipedia has the advantage of being constantly updated by thousands of voluntary contributors. It is multilingual and freely available containing a tremendous amount of encyclopedic knowledge enriched with hyperlink information. In the past, researchers have explored the alignment of Wikipedia categories and WordNet synsets (e.g., Toral et al. (2008); Ponzetto and Navigli (2009)). However, using the categories instead of the articles causes three limitations: First, the number of Wikipedia categories (about 0.5 million in the English edition) is much smaller compared to the number of articles (about 3.35 million). Secondly, the category system in Wikipedia is not structured consistently (Ponzetto and Navigli, 2009). And finally, disregarding the article level neglects the huge amount of textual content provided by the articles. Therefore, attempts to align WordNet synsets and Wikipedia articles (instead of categories) have been recently m"
W11-0122,zesch-etal-2008-extracting,1,0.105909,"Missing"
W11-0122,E09-1005,0,\N,Missing
W14-5201,P02-1022,0,0.304575,"ents centered around a particular combination of processing framework and type system (e.g. Buyko and Hahn (2008), Kano et al. (2011), Wu et al. (2013)). Each of these defines its own concepts of tokens, sentence, syntactic structures, discourse structures, etc. Yet, even these type systems leave certain aspects underspecified, e.g. the various tagsets used to categorize parts-of-speech, syntactic constituents, etc. 2.1 Sharing pipelines Processing frameworks offer a way to construct pipeline descriptions that instruct the framework to configure and execute a pipeline of NLP components. GATE (Cunningham et al., 2002) and Apache UIMA (Ferrucci and Lally, 2004) are currently the most prominent processing frameworks. Both describe pipelines by means of XML documents. These refer to 2 user has taken precautions that these components are individual components by name and expect that the accessible by the framework. Neither framework includes provisions to automatically obtain the components or resources they require, e.g. from a repository (Section 2.2). In fact, the pipeline descriptions do not contain sufficient information to uniquely identify components. Components are referred to only by name, but not by"
W14-5201,P13-1166,0,0.0271647,"Missing"
W14-5201,P07-2053,0,0.048557,"Missing"
W14-5201,hinrichs-etal-2010-weblicht,0,0.0718759,"are plug-in repositories, such as those used by GATE (Cunningham et al., 2002). From these, the user can conveniently download and install components within the GATE workbench. These plug-in repositories are specific to GATE, whereas the Maven repositories are a generic infrastructure widely used by the Java community and that is supported by many tools and applications. Online Workbenches While many NLP tools are offered as portable software for offline use, we observe a trend in recent years towards offering NLP tools as web-services for online use, sometimes as the only way to access them. Hinrichs et al. (2010) cite incompatibilities between the software and the user’s machine and insufficiently powerful workstations as reasons for this approach. Another reason may be the ability to set up a walled garden in which the service provider is able to control the use of services, e.g. to academic researchers or to paying commercial customers. Argo (Rak et al., 2013) is a web-based workbench. It offers access to a collection of UIMA-based NLP-services that can be executed in different environments. Rak et al. mention in particular a cluster environment but also plan support for a number of cloud platforms."
W14-5201,P14-5010,0,0.0244706,"d the software or resources, or to access them as services. Repositories Repositories are online services from which components and resources can be obtained. The Central Repository (2014) is a repository within the Java-ecosystem used to distribute Java libraries and resources they require, so-called artifacts. It relies on concepts that have evolved around the Maven project (Sonatype Company, 2008). Meanwhile, these are supported by many build tools, development environments, and even by some programming languages (cf. Section 3.3). Several NLP tools (e.g. ClearNLP (2014), Stanford CoreNLP (Manning et al., 2014), MaltParser (Nivre et al., 2007), ClearTK (Ogren et al., 2009)) are already distributed via this medium, some including their resources. There are many Maven repositories on the internet. They are organized as a loosely federated network. The Central Repository merely serves as the default point of contact built into clients. Repositories have the ability to access each other and to cache those artifacts required by their immediate users. This provides resilience against network failures or remote data loss. Artifacts can be addressed across the federation by a set of coordinates (groupId, ar"
W14-5201,P13-4020,0,0.02505,"pplications. Online Workbenches While many NLP tools are offered as portable software for offline use, we observe a trend in recent years towards offering NLP tools as web-services for online use, sometimes as the only way to access them. Hinrichs et al. (2010) cite incompatibilities between the software and the user’s machine and insufficiently powerful workstations as reasons for this approach. Another reason may be the ability to set up a walled garden in which the service provider is able to control the use of services, e.g. to academic researchers or to paying commercial customers. Argo (Rak et al., 2013) is a web-based workbench. It offers access to a collection of UIMA-based NLP-services that can be executed in different environments. Rak et al. mention in particular a cluster environment but also plan support for a number of cloud platforms. For this reason, we assume that most of the components are integrated into Argo as portable software that can be deployed to these platforms on-demand. Yet, it appears that the components are only accessible through Argo and that they are not distributed separately for use in other UIMA-based environments. U-Compare (Kano et al., 2011) is a Java applica"
W14-5201,S10-1071,0,0.0132562,"Missing"
W14-5201,P13-4004,0,0.0167953,". While some components accessible through the workbench run locally, many components are only stubs calling out to web-services running at different remote locations. WebLicht (Hinrichs et al., 2010) is a distributed infrastructure of NLP services hosted at different lo3 cations. They exchange data in an XML format called TCF (Text Corpus Format). Pipelines can be built and run using the web-based WebLicht workbench. Within this walled garden platform, authenticated academic users have access to resources that are free for academic research, but not otherwise. Online Marketplaces AnnoMarket (Tablan et al., 2013) is another distributed infrastructure of NLP services based on GATE. It does not seem to offer a workbench to compose custom pipelines. Instead, it offers a set of pre-configured components and exposes them as web-services to be programmatically accessed. It is the only commercial offering in this overview that the user has to pay for. Note on service-based approaches Service-based approaches have also been taken in other scientific domains to facilitate the creation of shareable and repeatable experiments, e.g. on platforms such as myExperiment (Goble et al., 2010). However, G´omez-P´erez et"
W14-5201,W11-3307,0,0.331696,"s to the document, e.g. sentence and token boundaries, POS tags, syntactic constituents, or dependency relations, etc. These steps build upon each other, e.g. a component for dependency parsing requires at least sentences, tokens, and POS tags. Many components are generic engines that require some resource (e.g. a probabilistic model or knowledge base) that configures them for a specific language, tagset, domain, etc. We use the term resource selection for the task of choosing a resource and configuring a component to use it. The task of obtaining the resource we call resource acquisition. As Thompson et al. (2011) point out, achieving a consensus on the exact representation of different linguistic theories as annotations and thereby attaining full conceptual interoperability between components from different vendors is currently not considered feasible. Thus, frameworks leave the type system (kinds of annotations) unspecified. Therefore, the technical integration with a framework alone does not make the tools interoperable on the conceptual level (cf. Chiarcos et al. (2008)). As a consequence, multiple component collections exist, each providing interoperable components centered around a particular com"
W14-5201,van-uytvanck-etal-2010-virtual,0,0.0607334,"Missing"
W15-0501,U12-1003,0,0.0216484,". According to the extended annotation scheme, each sentence in a scientific publication is annotated with exactly one of 15 categories (e.g. Background or Aim), reflecting the argumentative role the sentence has in the text. Mapping this scheme to our terminology (see section 1), a sentence corresponds to an argument component. The aim of this annotation scheme is to improve information access and to support applications like automatic text summarization (Teufel and Moens, 2002; Ruch et al., 2007; Contractor et al., 2012). While the authors themselves do not consider argumentative relations, Angrosh et al. (2012) transfer the argumentation inherent in the categories of the Argumentative Zoning to the Toulmin model (Toulmin, 1958) and therefore describe how argument components of several types relate to each other. For example, research findings are used to support “statements referring to the problems solved by an article” and “statements referring to current work shortcomings” support “statements referring to future work”. However, the paper focuses on citation contexts and considers relations only on a coarse-grained level. Several similar annotation schemes for scientific publications exist. For in"
W15-0501,W01-1605,0,0.0215119,"ions that hold between adjacent text units, e.g. sentences, clauses or nominalizations (Webber et al., 2012). Often, the text units considered in discourse analysis correspond to argument components, and discourse relations are closely related to argumentative relations. Most previous work in automated discourse analysis is based on corpora annotated with discourse relations, most notably the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) and 1 For a comparison between Argumentative Zoning and CoreSC, see Liakata et al. (2010). 3 the Rhetorical Structure Theory (RST) Discourse Treebank (Carlson et al., 2001). However, the data consists of newspaper articles (no scientific articles), and only relations between adjacent text units are identified. In addition, it is still an open question how the proposed discourse relations relate to argumentative relations (the difference of the relations is best illustrated by the work of Biran and Rambow (2011)). Nevertheless, annotated corpora like this can be valuable resources for training automatic classifiers later. IAA Metrics Current state-of-the-art annotation studies use chance corrected measures to compute IAA, i.e., random agreement is included in the"
W15-0501,C12-1041,0,0.0354894,"the work called Argumentative Zoning by Teufel (1999) which was extended by Teufel et al. (2009). According to the extended annotation scheme, each sentence in a scientific publication is annotated with exactly one of 15 categories (e.g. Background or Aim), reflecting the argumentative role the sentence has in the text. Mapping this scheme to our terminology (see section 1), a sentence corresponds to an argument component. The aim of this annotation scheme is to improve information access and to support applications like automatic text summarization (Teufel and Moens, 2002; Ruch et al., 2007; Contractor et al., 2012). While the authors themselves do not consider argumentative relations, Angrosh et al. (2012) transfer the argumentation inherent in the categories of the Argumentative Zoning to the Toulmin model (Toulmin, 1958) and therefore describe how argument components of several types relate to each other. For example, research findings are used to support “statements referring to the problems solved by an article” and “statements referring to current work shortcomings” support “statements referring to future work”. However, the paper focuses on citation contexts and considers relations only on a coars"
W15-0501,P11-1099,0,0.0398671,"arguments as small graph structures. We developed an annotation tool that supports the annotation of such graphs and carried out an annotation study with four annotators on 24 scientific articles from the domain of educational research. For calculating the inter-annotator agreement, we adapted existing measures and developed a novel graphbased agreement measure which reflects the semantic similarity of different annotation graphs. 1 Introduction Argumentation mining aims at automatically identifying arguments and argumentative relations in argumentative discourse, e.g., in newspaper articles (Feng and Hirst, 2011; Florou et al., 2013), legal documents (Mochales-Palau and Moens, 2011), or scientific publications. Many applications, such as text summarization, information retrieval, or faceted search could benefit from a fine-grained analysis of the argumentation structure, making the reasoning process directly visible. Such an enhanced information access would be particularly important for scientific publications, where the rapidly increasing amount of documents available in digital form makes it more and more difficult for users to find We observe worse performance for children with migration backgrou"
W15-0501,W13-2707,0,0.461161,"ph structures. We developed an annotation tool that supports the annotation of such graphs and carried out an annotation study with four annotators on 24 scientific articles from the domain of educational research. For calculating the inter-annotator agreement, we adapted existing measures and developed a novel graphbased agreement measure which reflects the semantic similarity of different annotation graphs. 1 Introduction Argumentation mining aims at automatically identifying arguments and argumentative relations in argumentative discourse, e.g., in newspaper articles (Feng and Hirst, 2011; Florou et al., 2013), legal documents (Mochales-Palau and Moens, 2011), or scientific publications. Many applications, such as text summarization, information retrieval, or faceted search could benefit from a fine-grained analysis of the argumentation structure, making the reasoning process directly visible. Such an enhanced information access would be particularly important for scientific publications, where the rapidly increasing amount of documents available in digital form makes it more and more difficult for users to find We observe worse performance for children with migration background in primary school."
W15-0501,liakata-etal-2010-corpora,0,0.0101459,"is the annotation of discourse structure which aims at identifying discourse relations that hold between adjacent text units, e.g. sentences, clauses or nominalizations (Webber et al., 2012). Often, the text units considered in discourse analysis correspond to argument components, and discourse relations are closely related to argumentative relations. Most previous work in automated discourse analysis is based on corpora annotated with discourse relations, most notably the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) and 1 For a comparison between Argumentative Zoning and CoreSC, see Liakata et al. (2010). 3 the Rhetorical Structure Theory (RST) Discourse Treebank (Carlson et al., 2001). However, the data consists of newspaper articles (no scientific articles), and only relations between adjacent text units are identified. In addition, it is still an open question how the proposed discourse relations relate to argumentative relations (the difference of the relations is best illustrated by the work of Biran and Rambow (2011)). Nevertheless, annotated corpora like this can be valuable resources for training automatic classifiers later. IAA Metrics Current state-of-the-art annotation studies use"
W15-0501,W04-1205,0,0.177437,"efore describe how argument components of several types relate to each other. For example, research findings are used to support “statements referring to the problems solved by an article” and “statements referring to current work shortcomings” support “statements referring to future work”. However, the paper focuses on citation contexts and considers relations only on a coarse-grained level. Several similar annotation schemes for scientific publications exist. For instance, Liakata et al. (2012) proposed CoreSC (“Core Scientific Concepts“), an annotation scheme consisting of 11 categories1 . Mizuta and Collier (2004) provide a scheme consisting of 7 categories (plus 5 subcategories) for the biology domain. In addition Yepes et al. (2013) provide a scheme to categorize sentences in abstracts of articles from biomedicine with 5 categories. Furthermore, Blake (2010) describes approaches to identify scientific claims or comparative claim sentences in scientific articles (Park and Blake, 2012). Again these works do not consider argumentative relations on a fine-grained level, but focus on the classification of argument components. While all of these works use data from the natural sciences, there are only few"
W15-0501,W12-4301,0,0.0112479,"ained level. Several similar annotation schemes for scientific publications exist. For instance, Liakata et al. (2012) proposed CoreSC (“Core Scientific Concepts“), an annotation scheme consisting of 11 categories1 . Mizuta and Collier (2004) provide a scheme consisting of 7 categories (plus 5 subcategories) for the biology domain. In addition Yepes et al. (2013) provide a scheme to categorize sentences in abstracts of articles from biomedicine with 5 categories. Furthermore, Blake (2010) describes approaches to identify scientific claims or comparative claim sentences in scientific articles (Park and Blake, 2012). Again these works do not consider argumentative relations on a fine-grained level, but focus on the classification of argument components. While all of these works use data from the natural sciences, there are only few works in the domain of social sciences (e.g. Ahmed et al. (2013)), and to the best of our knowledge no previous work has addressed scientific publications in the educational domain. A field that is closely related to the annotation of argumentation structures is the annotation of discourse structure which aims at identifying discourse relations that hold between adjacent text"
W15-0501,prasad-etal-2008-penn,0,0.019342,"nal domain. A field that is closely related to the annotation of argumentation structures is the annotation of discourse structure which aims at identifying discourse relations that hold between adjacent text units, e.g. sentences, clauses or nominalizations (Webber et al., 2012). Often, the text units considered in discourse analysis correspond to argument components, and discourse relations are closely related to argumentative relations. Most previous work in automated discourse analysis is based on corpora annotated with discourse relations, most notably the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) and 1 For a comparison between Argumentative Zoning and CoreSC, see Liakata et al. (2010). 3 the Rhetorical Structure Theory (RST) Discourse Treebank (Carlson et al., 2001). However, the data consists of newspaper articles (no scientific articles), and only relations between adjacent text units are identified. In addition, it is still an open question how the proposed discourse relations relate to argumentative relations (the difference of the relations is best illustrated by the work of Biran and Rambow (2011)). Nevertheless, annotated corpora like this can be valuable resources for training"
W15-0501,sonntag-stede-2014-grapat,0,0.0899591,"icts) B. The detail relation is used, if A is a detail of B and gives more information or defines something stated in B without argumentative reasoning. Finally, we link two argument components with the sequence relation, if two (or more) argument components belong together and only make sense in combination, i.e., they form a multi-sentence argument component.5 Annotation Tool We developed our own webbased annotation tool DiGAT which we think is better suited for annotating relations in long texts than existing tools like WebAnno (Yimam et al., 2013), brat (Stenetorp et al., 2012) or GraPAT (Sonntag and Stede, 2014). Although all of them allow to annotate relations between sentences, the view quickly becomes confusing when annotating relations. In WebAnno and brat, the relations are drawn with arrows directly in the text. Only GraPAT visualizes the annotations in a graph. However, the text is included in the nodes directly in the graph, which again becomes confusing for texts with multiple long sentences. DiGAT has several advantages over existing tools. First, the full-text with its layout structure (e.g., headings, paragraphs) is displayed without any relation annotations on the left-hand side of the s"
W15-0501,C14-1142,1,0.781355,"annotation schemes for annotating argumentation and discourse structure, (ii) inter-annotator agreement (IAA) metrics suitable for this annotation task, (iii) previous annotation studies. Annotation Schemes Previously, annotation schemes and approaches for identifying arguments in different domains have been developed. For instance, Mochales-Palau and Moens (2011) identify arguments in legal documents, Feng and Hirst (2011) focus on the identification of argumentation schemes (Walton, 1996) in newspapers and court cases, Florou et al. (2013) apply argumentation mining in policy modeling, and Stab and Gurevych (2014) present an approach to model arguments in persuasive essays. Most of the approaches focus on the identification and classification of argument components. There are only few works which aim at identifying argumentative relations and consequently argumentation structures. Furthermore it is important to note that the texts from those domains differ considerably from scientific publications regarding their length, complexity, purpose and language use. Regarding argumentation mining in scientific publications, one of the first approaches is the work called Argumentative Zoning by Teufel (1999) wh"
W15-0501,E12-2021,0,0.128209,"Missing"
W15-0501,J02-4002,0,0.083375,"blications, one of the first approaches is the work called Argumentative Zoning by Teufel (1999) which was extended by Teufel et al. (2009). According to the extended annotation scheme, each sentence in a scientific publication is annotated with exactly one of 15 categories (e.g. Background or Aim), reflecting the argumentative role the sentence has in the text. Mapping this scheme to our terminology (see section 1), a sentence corresponds to an argument component. The aim of this annotation scheme is to improve information access and to support applications like automatic text summarization (Teufel and Moens, 2002; Ruch et al., 2007; Contractor et al., 2012). While the authors themselves do not consider argumentative relations, Angrosh et al. (2012) transfer the argumentation inherent in the categories of the Argumentative Zoning to the Toulmin model (Toulmin, 1958) and therefore describe how argument components of several types relate to each other. For example, research findings are used to support “statements referring to the problems solved by an article” and “statements referring to current work shortcomings” support “statements referring to future work”. However, the paper focuses on citation con"
W15-0501,D09-1155,0,0.518217,"to model arguments in persuasive essays. Most of the approaches focus on the identification and classification of argument components. There are only few works which aim at identifying argumentative relations and consequently argumentation structures. Furthermore it is important to note that the texts from those domains differ considerably from scientific publications regarding their length, complexity, purpose and language use. Regarding argumentation mining in scientific publications, one of the first approaches is the work called Argumentative Zoning by Teufel (1999) which was extended by Teufel et al. (2009). According to the extended annotation scheme, each sentence in a scientific publication is annotated with exactly one of 15 categories (e.g. Background or Aim), reflecting the argumentative role the sentence has in the text. Mapping this scheme to our terminology (see section 1), a sentence corresponds to an argument component. The aim of this annotation scheme is to improve information access and to support applications like automatic text summarization (Teufel and Moens, 2002; Ruch et al., 2007; Contractor et al., 2012). While the authors themselves do not consider argumentative relations,"
W15-0501,W10-1814,0,0.0726868,"Missing"
W15-0501,W13-1913,0,0.0214284,"Missing"
W15-0501,P13-4001,1,0.768859,"Missing"
W15-0601,W11-1407,0,0.0160074,"distinguished into open and closed answer formats. In open formats, the learner has to actually produce the solution, while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gaps as the mutilated context"
W15-0601,Q14-1040,1,0.76714,"ting institutions and conducted a learner study to obtain error rates for an additional test type.1 For a better understanding of the differences between test types, we first calculate the candidate space of potential answers and compare it to learner answers. We assume that higher answer ambiguity leads to higher difficulty. As all datasets allow binary scoring (correct/wrong), the difficulty of an item is interpreted as the proportion of wrong answers, also referred to as the error rate. We then build a generalized difficulty prediction framework based on an earlier approach we presented in Beinborn et al. (2014a) which was limited to English and to one specific test type. We evaluate the prediction for different test types and languages and obtain remarkable results for French and German. Many language tests are designed as multiple choice questions. The generalized prediction approach lacks predictive power for this format because the evaluation strategy for the answer candidates is solely based on word frequency. We develop two strategies for more sophisticated candidate ranking that are inspired by automatic solving methods based on language models and semantic relatedness. We show that the candi"
W15-0601,H05-1025,0,0.036304,"ambiguity. 5 Candidate evaluation strategies The main challenge for solving a reduced redundancy test consists in identifying the most suitable candidate in the candidate space. The context fitness of a candidate can be evaluated based on language model probabilities and on semantic relatedness between the candidate and the context. LM-based approach A probabilistic language model (LM) calculates the probability of a phrase based on the frequencies of lower order n-grams extracted from training data (Stolcke, 1994). This can be used to predict the fitness of a word for the sentential context. Bickel et al. (2005), for example, evaluate the use of probabilistic language models to support auto-completion of sentences in writing editors. In the completion scenario, only the left context is available, while the learner can also consider the right context in language tests. Zweig et al. (2012) thus model the problem of solving cloze tests by applying methods from lexical substitution to evaluate and rank the candidates. The part to be substituted is a gap and the set of “substitution candidates” is already provided by the answer options. Unfortunately, we cannot rely on static sentences for the open test f"
W15-0601,H05-1103,0,0.097309,"while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gaps as the mutilated context of a single gap can only be recreated by solving the surrounding gaps. 2.1 Cloze test Cloze tests have been intr"
W15-0601,P14-5011,1,0.788352,"word class, inflection, compound structure, spelling difficulty, etc.)4 , the readability of the text (typetoken-ratio, number of clauses, average word and sentence length, etc.) and the test parameters (number of candidates, position of gap, etc). We also adapt the pipelines to include proper German and French pre-processing using DkPro (de Castilho and Gurevych, 2014) and adapt the candidate calculation to the different test types. In order to assure comparability to previous work, we also use support vector machines for regression in Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014). 4.3 Table 2 provides an overview of the test datasets used in this paper. It consists of four open formats and one closed format with multiple choice options. The number of participants is averaged over the texts because each participant worked with 5 texts in the open formats.5 The error rates should not be compared across test types because the participants had different levels of language proficiency. The high standard deviations indicate that each test contains gaps that are rather easy and others that are extremely difficult. In Beinborn et al. (2014a) we have shown that the error rate"
W15-0601,W14-5201,1,0.595942,"Missing"
W15-0601,N07-1058,0,0.0240565,"produce the solution, while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gaps as the mutilated context of a single gap can only be recreated by solving the surrounding gaps. 2.1 Cloze test Cloze"
W15-0601,W14-3503,1,0.853128,"context of a single gap can only be recreated by solving the surrounding gaps. 2.1 Cloze test Cloze tests have been introduced by Taylor (1953) and have become the most popular form of reduced redundancy testing. In cloze tests, full words are deleted from a text. This strategy requires compre2 Figure 1: Example for a cloze question, the solution is observance. hensive context, so the deletion rate is usually every 7th word or higher (Brown, 1989). The main problem with cloze tests is that the gaps are usually highly ambiguous and the set of potential solutions cannot be exactly anticipated (Horsmann and Zesch, 2014). Therefore, most cloze tests are designed as closed formats, so that the correct solution can be selected from a set of distractors (see Figure 1 for an example). 2.2 C-test Although the cloze test is widely used, the setup contains several weaknesses such as the small number of gaps and the ambiguity of the solution. The C-test is an alternative of the cloze test that has been developed by Klein-Braley and Raatz (1982). The C-test construction principle enables a higher number of gaps on less text, every second word of a short paragraph is transformed into a gap. As this high deletion rate w"
W15-0601,W12-2016,0,0.0241095,"redundancy tests can be distinguished into open and closed answer formats. In open formats, the learner has to actually produce the solution, while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gap"
W15-0601,W12-2017,0,0.0227427,"d set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gaps as the mutilated context of a single gap can only be recreated by solving the surrounding gaps. 2.1 Cloze test Cloze tests have been introduced by Taylor (1953) and have become the most popula"
W15-0601,quasthoff-etal-2006-corpus,0,0.025272,"ap that scores the generated sub-sentences using a language model and only keeps the n best. For the closed cloze test, the number of generated sentences is of course limited to the number of candidates (5) because each sentence contains only one gap. We use 5-gram language models that are trained on monolingual news corpora using berkeleylm with Kneser-Ney smoothing.8 Zweig et al. (2012) trained their models explicitly on training data only from Sherlock Holmes novels. In order to better simulate learner knowledge, we use rather small and controlled training data from the Leipzig collection (Quasthoff et al., 2006) consisting of one million sentences for each language. For solving the test, we then select the generated sentence with the highest log-probability in the language model and count how many gaps are solved correctly. If several sentences obtain the same probability, we pick one at random. We run this strategy ten times and average the results. For comparison, we implement a baseline that always selects the most frequent candidate without considering the context. Semantic relatedness approach Language models cannot capture relations between distant words in the sentence. To account for this con"
W15-0601,W10-1007,0,0.163459,"he learner has to actually produce the solution, while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gaps as the mutilated context of a single gap can only be recreated by solving the surrounding gaps"
W15-0601,W14-1817,1,0.853652,"e missing words. Reduced redundancy tests can be distinguished into open and closed answer formats. In open formats, the learner has to actually produce the solution, while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the"
W15-0601,W12-2704,0,0.0471167,"Missing"
W15-0601,P12-1063,0,0.394108,"c relatedness between the candidate and the context. LM-based approach A probabilistic language model (LM) calculates the probability of a phrase based on the frequencies of lower order n-grams extracted from training data (Stolcke, 1994). This can be used to predict the fitness of a word for the sentential context. Bickel et al. (2005), for example, evaluate the use of probabilistic language models to support auto-completion of sentences in writing editors. In the completion scenario, only the left context is available, while the learner can also consider the right context in language tests. Zweig et al. (2012) thus model the problem of solving cloze tests by applying methods from lexical substitution to evaluate and rank the candidates. The part to be substituted is a gap and the set of “substitution candidates” is already provided by the answer options. Unfortunately, we cannot rely on static sentences for the open test formats as the context needs to be determined by solving the surrounding gaps. For each gap, we take all candidates into account and generate all possible sentences resulting from the combinations with the candidates of subsequent gaps. This can lead to strong dependencies between"
W15-3603,P14-1119,0,0.0245233,"ng architecture and analyze the performance of four state-of-the-art algorithms. We then perform experiments on three German datasets, of which two have been created particularly for these experiments, in order to analyze the impact of decompounding on standard keyphrase extraction approaches. Decompounding has previously been successfully used in other applications, e.g. in machine translation (Koehn and Knight, 2003), information retrieval (Hollink et al., 2004; Alfonseca et al., 2008b; Alfonseca et al., 2008a), speech recognition (Ordelman, 2003), and word prediction (Baroni et al., 2002). Hasan and Ng (2014) have shown that infrequency errors are a major cause for lower keyphrase extraction results . To the best of our knowledge, we are the first to examine the influence of decompounding on keyphrase extraction. Introduction Most approaches for automatic extraction of keyphrases are based on the assumption that the more frequent a term or phrase is mentioned, the more important it is. Consequently, most extraction algorithms apply some kind of normalization, e.g. lemmatization or noun chunking (Hulth, 2003; Mihalcea and Tarau, 2004), in order to arrive with accurate counts. However, especially in"
W15-3603,P08-2064,0,0.141717,"er consists of the parts Deutsch (Engl.: German) and Lehrer (Engl.: teacher). In this paper, we propose a comprehensive decompounding architecture and analyze the performance of four state-of-the-art algorithms. We then perform experiments on three German datasets, of which two have been created particularly for these experiments, in order to analyze the impact of decompounding on standard keyphrase extraction approaches. Decompounding has previously been successfully used in other applications, e.g. in machine translation (Koehn and Knight, 2003), information retrieval (Hollink et al., 2004; Alfonseca et al., 2008b; Alfonseca et al., 2008a), speech recognition (Ordelman, 2003), and word prediction (Baroni et al., 2002). Hasan and Ng (2014) have shown that infrequency errors are a major cause for lower keyphrase extraction results . To the best of our knowledge, we are the first to examine the influence of decompounding on keyphrase extraction. Introduction Most approaches for automatic extraction of keyphrases are based on the assumption that the more frequent a term or phrase is mentioned, the more important it is. Consequently, most extraction algorithms apply some kind of normalization, e.g. lemmati"
W15-3603,W03-1028,0,0.104841,"a), speech recognition (Ordelman, 2003), and word prediction (Baroni et al., 2002). Hasan and Ng (2014) have shown that infrequency errors are a major cause for lower keyphrase extraction results . To the best of our knowledge, we are the first to examine the influence of decompounding on keyphrase extraction. Introduction Most approaches for automatic extraction of keyphrases are based on the assumption that the more frequent a term or phrase is mentioned, the more important it is. Consequently, most extraction algorithms apply some kind of normalization, e.g. lemmatization or noun chunking (Hulth, 2003; Mihalcea and Tarau, 2004), in order to arrive with accurate counts. However, especially in Germanic languages the frequent use of noun compounds has an adverse effect on the reliability of frequency counts. Consider for example a German document that talks about Lehrer (Engl.: teacher) without ever mentioning the word “Lehrer” at all, because it is always part of compounds like Deutschlehrer (Engl.: German teacher) or Gymnasiallehrer (Engl.: grammar school teacher). Thus, we argue that the problem can be solved by splitting noun compounds in meaningful parts, i.e. by performing decompounding"
W15-3603,S10-1040,0,0.0504271,"Missing"
W15-3603,E03-1076,0,0.161055,"schlehrer (Engl.: German teacher). in German. The compound Deutschlehrer consists of the parts Deutsch (Engl.: German) and Lehrer (Engl.: teacher). In this paper, we propose a comprehensive decompounding architecture and analyze the performance of four state-of-the-art algorithms. We then perform experiments on three German datasets, of which two have been created particularly for these experiments, in order to analyze the impact of decompounding on standard keyphrase extraction approaches. Decompounding has previously been successfully used in other applications, e.g. in machine translation (Koehn and Knight, 2003), information retrieval (Hollink et al., 2004; Alfonseca et al., 2008b; Alfonseca et al., 2008a), speech recognition (Ordelman, 2003), and word prediction (Baroni et al., 2002). Hasan and Ng (2014) have shown that infrequency errors are a major cause for lower keyphrase extraction results . To the best of our knowledge, we are the first to examine the influence of decompounding on keyphrase extraction. Introduction Most approaches for automatic extraction of keyphrases are based on the assumption that the more frequent a term or phrase is mentioned, the more important it is. Consequently, most"
W15-3603,biemann-etal-2008-asv,0,0.0281646,"ight), it creates a split and stops. Banana Splitter3 searches for the word from the right to the left, and if there is more than one possibility, the one with the longest split on the right side is taken as candidate. Data Driven counts the number of words in a dictionary, which contain a split at this position as prefix or suffix for every position in the input. A split is made at the position with the largest difference between prefix and suffix counts (Larson et al., 2000). ASV Toolbox4 uses a trained Compact Patricia Tree to recursively split parts from the beginning and end of the word (Biemann et al., 2008). Unlike the other algorithms, it generates only a single split candidate at each recursive step. For that reason, it does not need a ranker. It is also the only supervised (using lists of existing compounds) approach tested. 2.2 Freq. M.I. .64 .26 .58 .08 .71 .33 JWord Splitter Freq. M.I. .67 .59 .63 .20 .79 .73 Banana Splitter Freq. M.I. .70 .66 .40 .16 .83 .81 Data Driven Freq. M.I. .49 .40 .18 .04 .70 .58 .80 .75 .87 In these equations, N is the number of fragments the candidate has, w is the fragment itself, f (w) is the relative unigram frequency for that fragment w, bigr(wi , wj ) is th"
W15-3603,W14-5201,1,0.887716,"Missing"
W15-3603,R09-1086,1,0.898436,"perform stopword removal and decompounding as described in Section 2. It should be noted that in most preprocessing pipelines, decompounding should be the last step, as it heavily influences POS-tagging. We extract all lemmas in the document as keyphrase candidates and rank them according to basic ranking approaches based on frequency counts and the position in the document. We do not use more sophisticated extraction approaches, as we want to examine the influence of decompounding as directly as possible. However, it has been shown that frequency-based heuristics are a very strong baseline (Zesch and Gurevych, 2009), and even supervised keyphrase extraction methods such as KEA (Witten et al., 1999) use term frequency and position as the most important features and will be 7 www.medizin-forum.de/ www.dipf.de/en/research/projects/ pythagoras 8 9 nlp.stanford.edu/software/segmenter. shtml 12 heavily influenced by decompounding. We evaluate the following ranking methods: tfidfconstant ranks candidates according to their term frequency f (t, d) in the document. tf-idf decreases the impact of words that occur in most documents. The term frequency count is normalized with the inverse document frequency in the t"
W15-3603,W04-3252,0,\N,Missing
W16-0508,I13-1112,1,0.787621,"ateness because the Spanish spelling would be ingeniero. The spelling of marmalade with an e seems to be idiosyncratic to German learners. The above analyses are only performed on an anecdotal basis and need to be backed up with more thorough experimental studies. The examples support the intuitive assumption that cognates are particularly prone to spelling errors due to the diﬀerent orthographic and phonetic patterns in the L1 of the learner. The cognateness of words can be determined automatically using string similarity measures (Inkpen et al., 2005) or character-based machine translation (Beinborn et al., 2013). The learners in the EFC corpus also diﬀer in proﬁciency (e.g. German learners seem to be more advanced than Brazilian learners) which might also have an inﬂuence on the spelling error probability of words. However, it is complicated to disentangle the inﬂuence of the L1 and Correct Brazilian Mexican Chinese Russian German attention atention(27) attencion (10) atencion (3) atention (13) attencion(1) attentio (1) attaention (1) atttention (1) - attantion (5) atantion (1) atention (1) - departmental departament (10) departamente (1) departaments (1) department (1) - deparment (2) deparmental (1"
W16-0508,N09-3006,0,0.0220173,"discussed psycholinguistic analyses of spelling diﬃculty. In natural language processing, related work in the ﬁeld of spelling has focused on error correction (Ng et al., 2013; Ng et al., 2014). For ﬁnding the right correction, Deorowicz and Ciura (2005) analyze probable causes for spelling errors. They identify three types of causes (mistyping, misspelling and vocabulary incompetence) and model them using substitution rules. Toutanova and Moore (2002) use the similarity of pronunciations to pick the best correction for an error resulting in an improvement over state-of-the-art spellcheckers. Boyd (2009) build on their work but model the pronunciation of non-native speakers, leading to slight improvements in the pronunciationbased model. Modeling the spelling diﬃculty of words could also have a positive eﬀect on spelling correction because spelling errors would be easier to anticipate. Another important line of research is the development of spelling exercises. A popular recent example is the game Phontasia (Berkling et al., 2015a). It has been developed for L1 learners but could probably also be used for L2 learners. In this case, the ﬁndings on cross-lingual transfer could be integrated to"
W16-0508,P14-5011,1,0.842772,"Missing"
W16-0508,C12-1049,0,0.105468,"of the error frequency over all occurrences of the word (including the erroneous occurrences). perr (w) = EFC Table 2: Examples for high and low spelling error probsi ∈ D SD Error Probability high low (1) = (e, c) X Corpus ferr (w) f (w) (4) 77 The words are then ranked by their error probability to quantify spelling diﬃculty.10 This is only a rough approximation that ignores other factors such as repetition errors and learner ability because detailed learner data was not available for all corpora. In future work, more elaborate measures of spelling diﬃculty could be analyzed (see for example Ehara et al. (2012)). 3.3 Training and Test Data An inspection of the ranked probabilities indicates that the spelling diﬃculty of a word is a continuous variable which points to a regression problem. However, the number of spelling errors is too small to distinguish between a spelling error probability of 0.2 and 0.3, for example. Instead, we only focus on the extremes of the scale. 10 In the case of tied error probability, the word with the higher error frequency is ranked higher. In the case of an error frequency of zero for both words, the word with the lower correct frequency is ranked higher. Individual Fe"
W16-0508,W13-1718,0,0.030946,"et al. (2014), we analyzed that words with high spelling error probability lead to more diﬃcult exercises. This indicates, that spelling diﬃculty should also be considered in exercise generation. In text simpliﬁcation tasks (Specia et al., 2012), a quantiﬁcation of spelling diﬃculty could lead to more focused, learner-oriented lexical simpliﬁcation. Spelling problems are often inﬂuenced by cross-lingual transfer because learners apply patterns from their native language (Ringbom and Jarvis, 2009). Spelling errors can therefore be a good predictor for automatic natural language identiﬁcation (Nicolai et al., 2013). Language teachers are not always aware of these processes because they are often not familiar with the native language of their learners. Automatic prediction methods for L1-speciﬁc spelling diﬃculties can lead to a better understanding of cross-lingual transfer and support the development of individualized exercises. In this paper, we take an empirical approach and approximate spelling diﬃculty based on error frequencies in learner corpora. We extract more than 140,000 spelling errors by more than 85,000 learners from three learner corpora. Two corpora cover essays by learners of English an"
W16-0508,P11-1027,0,0.0122416,"wels to consonants. Both extremes—words with high density (e.g. aerie) and very low density (e.g. strength)—are likely to cause spelling problems. 3 4 grapheme length: 9, phoneme length: 5 http://www.speech.cs.cmu.edu/cgi-bin/cmudict 75 Character Sequence Probability We assume, that the grapheme–phoneme correspondence of a word is less intuitive, if the word contains a rare sequence of characters (e.g. gardener vs guarantee). To approximate this, we build a language model of character trigrams that indicates the probability of a character sequence using the framework Berkeleylm version 1.1.2 (Pauls and Klein, 2011). The quality of a language model is usually measured as the perplexity, i.e. the ability of the model to deal with unseen data. The perplexity can often be improved by using more training data. However, in this scenario, the model is supposed to perform worse on unseen data because it should model human learners. In order to reﬂect the sparse knowledge of a language learner, the model is trained only on the 800–1000 most frequent words from each language. We refer to these words as the Basic Vocabulary.5 Pronunciation Diﬃculty Furthermore, we try to capture the assumption that a spelling erro"
W16-0508,W11-1409,0,0.0298869,"L1 adults, L2 adults) are due to ambiguous sound–letter correspondences. Berkling et al. (2015b) study the interplay between graphemes and phonotactics in German in detail and developed a game to teach orthographic patterns to children. Peereman et al. (2007) provide a very good overview of factors inﬂuencing word diﬃculty and also highlight the importance of consistent grapheme–phoneme correspondence. It thus seems justiﬁed to focus on the phonetic problems. The features described below try to approximate the relationship between graphemes and phonemes from various angles. Orthographic Depth Rosa and Eskenazi (2011) analyze the inﬂuence of word complexity features on the vocabulary acquisition of L2 learners and show that words which follow a simple one-to-one mapping of graphemes to phonemes are considered to be easier than one-to-many or many-to-one mappings as in knowledge.3 The orthographic depth can be expressed as the grapheme-to-phoneme ratio (the word length in characters divided by the number of phonemes). For English, we calculate the number of phonemes based on the phonetic representation in the Carnegie Mellon University Pronouncing Dictionary.4 For Italian and German, a comparable pronunciat"
W16-0508,S12-1046,0,0.0287817,"ew sounds and their mapping to graphemes. English is a well-known example for a particularly inconsistent grapheme-to-phoneme mapping. For example, the sequence ough can be pronounced in six diﬀerent ways as in though, through, rough, cough, thought and bough.1 In many language learning scenarios, it is important to be aware of the spelling diﬃculty of a word. In Beinborn et al. (2014), we analyzed that words with high spelling error probability lead to more diﬃcult exercises. This indicates, that spelling diﬃculty should also be considered in exercise generation. In text simpliﬁcation tasks (Specia et al., 2012), a quantiﬁcation of spelling diﬃculty could lead to more focused, learner-oriented lexical simpliﬁcation. Spelling problems are often inﬂuenced by cross-lingual transfer because learners apply patterns from their native language (Ringbom and Jarvis, 2009). Spelling errors can therefore be a good predictor for automatic natural language identiﬁcation (Nicolai et al., 2013). Language teachers are not always aware of these processes because they are often not familiar with the native language of their learners. Automatic prediction methods for L1-speciﬁc spelling diﬃculties can lead to a better"
W16-0508,P02-1019,0,0.0318911,"of the L2 proﬁciency based on the current data and we leave this analysis to future work. phenomena occurring with L2 learners. 6 7 Related work In section 2, we already discussed psycholinguistic analyses of spelling diﬃculty. In natural language processing, related work in the ﬁeld of spelling has focused on error correction (Ng et al., 2013; Ng et al., 2014). For ﬁnding the right correction, Deorowicz and Ciura (2005) analyze probable causes for spelling errors. They identify three types of causes (mistyping, misspelling and vocabulary incompetence) and model them using substitution rules. Toutanova and Moore (2002) use the similarity of pronunciations to pick the best correction for an error resulting in an improvement over state-of-the-art spellcheckers. Boyd (2009) build on their work but model the pronunciation of non-native speakers, leading to slight improvements in the pronunciationbased model. Modeling the spelling diﬃculty of words could also have a positive eﬀect on spelling correction because spelling errors would be easier to anticipate. Another important line of research is the development of spelling exercises. A popular recent example is the game Phontasia (Berkling et al., 2015a). It has"
W16-0508,P11-1019,0,0.0376541,"corrections provided by teachers. We extract 167,713 annotations with the tag SP for spelling error.8 To our knowledge, this is by far the biggest available corpus with spelling errors from language learners. FCE The second corpus is part of the Cambridge Learner Corpus and consists of learner 7 It also contains essays by Czech learners, but this subset is signiﬁcantly smaller than the ones for the other two languages and is therefore not used here. 8 Some corrections have two diﬀerent tags; we only extract those with a single SP tag. 76 answers for the First Certiﬁcate in English (FCE) exam (Yannakoudakis et al., 2011). It contains 2,488 essays by 1,244 learners (each learner had to answer two tasks) from 16 nationalities. The essays have been corrected by oﬃcial examiners. We extract 4,074 annotations with the tag S for spelling error. Merlin The third corpus has been developed within the EU-project MERLIN (Boyd et al., 2014) and contains learner essays graded according to the Common European Reference Framework. The 813 Italian and the 1,033 German samples have been obtained as part of a test for the European language certiﬁcate (TELC). 752 of the German essays and 754 of the Italian essays were annotated"
W16-1104,W14-2302,0,0.153974,"eural network. In Section 4, we describe the used resources and training data and present the results of our method. Concluding this paper, in Section 5 we also give an outlook for future work. 28 Proceedings of The Fourth Workshop on Metaphor in NLP, pages 28–33, c San Diego, CA, 17 June 2016. 2016 Association for Computational Linguistics 2 Related Work While some approaches aim at inferring conceptual metaphors, here we mainly discuss works which detect linguistic metaphors, although differing in classification granularity (i.e. detection on token, construction, or sentence level). Beigman Klebanov et al. (2014) used logistic regression to assess (binary) metaphoricity on the token level, considering only content words. They built upon their work which used unigram, POS, topic model, and concreteness features from a concreteness ratings list, by implementing a reweighing scheme to correct for the imbalanced class distribution in metaphor data (Beigman Klebanov et al., 2015). Re-weighting significantly improved results on their test data, and allows for task-dependent tuning to focus on precision or recall. They also experiment with differences between concreteness ratings in certain grammatical const"
W16-1104,W15-1402,0,0.135359,"approaches aim at inferring conceptual metaphors, here we mainly discuss works which detect linguistic metaphors, although differing in classification granularity (i.e. detection on token, construction, or sentence level). Beigman Klebanov et al. (2014) used logistic regression to assess (binary) metaphoricity on the token level, considering only content words. They built upon their work which used unigram, POS, topic model, and concreteness features from a concreteness ratings list, by implementing a reweighing scheme to correct for the imbalanced class distribution in metaphor data (Beigman Klebanov et al., 2015). Re-weighting significantly improved results on their test data, and allows for task-dependent tuning to focus on precision or recall. They also experiment with differences between concreteness ratings in certain grammatical constructions, interpreting the rather small performance increases as an indicator of concreteness possibly not being a defining factor of metaphoricity. An ensemble approach to detect sentences containing metaphors has been implemented by Dunn et al. (2014). Their system extracts candidate token pairs from parsed text using manually defined syntactic patterns, before app"
W16-1104,W13-0908,0,0.158533,"Missing"
W16-1104,C14-1165,0,0.0258257,"mies to extend the system to different languages. Tsvetkov et al. (2013) employed random forests for metaphor detection on adjective-noun constructions. Database-provided features such as concreteness and imageability values are complemented by low-dimensional word embeddings and supersenses. By means of bilingual dictionaries they test their system on datasets in different languages, allowing for the continued use of English resources for feature extraction (e.g. concreteness and imageability databases, WordNet for supersense extraction, etc.). As an exception to the other discussed systems, Mohler et al. (2014) present a complex system for 29 conceptual metaphor detection, which requires linguistic metaphors identified by a separate system as the input data. For clustering words into conceptual classes, they compare the usage of dependency based vector representations, traditional LSA, and dense word embeddings. Their concept-mapping approach yields significant improvements in accuracy for three languages when using word embeddings; however, for English, LSA produced the best results. Schulder and Hovy (2014) provide an inherently resource and language independent method by using tf.idf for metaphor"
W16-1104,W14-2303,0,0.035187,"ases, WordNet for supersense extraction, etc.). As an exception to the other discussed systems, Mohler et al. (2014) present a complex system for 29 conceptual metaphor detection, which requires linguistic metaphors identified by a separate system as the input data. For clustering words into conceptual classes, they compare the usage of dependency based vector representations, traditional LSA, and dense word embeddings. Their concept-mapping approach yields significant improvements in accuracy for three languages when using word embeddings; however, for English, LSA produced the best results. Schulder and Hovy (2014) provide an inherently resource and language independent method by using tf.idf for metaphor detection, requiring only large background corpora and a small set of training sentences. Employing a bootstrapping approach with manually selected seed terms, they achieve rather modest results based on tf.idf alone, but emphasize its potential usefulness as a feature in more advanced multi-feature detection systems. Closely related, work in metonymy identification also has made use of word embeddings. Among other features commonly used for metaphor detection, such as abstractness or selectional restr"
W16-1104,S13-1040,0,0.265123,"ble results to other systems, while removing the need for additional resources. 1 Introduction According to Lakoff and Johnson (1980), metaphors are cognitive mappings of concepts from a source to a target domain. While in some works identifying those mappings (conceptual metaphors) themselves is the subject of analysis, we concern ourselves with detecting their manifestations in text (linguistic metaphors). Various features have been designed to model either representation of metaphor, prominently e.g. violations of (generalized) selectional preferences in grammatical relations (Wilks, 1978; Shutova, 2013), concreteness ratings to model the difference between source and target concepts (Tsvetkov et al., 2014; Turney and Assaf, 2011), supersenses and In this paper, we present a novel approach for metaphor detection using neural networks on the token-level in running text, relying solely on word embeddings used in context. In recent years, neural networks have been used to solve natural language processing tasks with great effect, but so far have not been applied to metaphor detection. While our approach still has to be tested on data in other languages, it already shows promising results on Engl"
W16-1104,W13-0906,0,0.550747,"r Educational Research and Educational Information http://www.ukp.tu-darmstadt.de † Abstract hypernym relations (Tsvetkov et al., 2013), or topic models (Heintz et al., 2013; Beigman Klebanov et al., 2014). Some of these features can be obtained in an unsupervised way, but many require additional resources such as concreteness databases or word taxonomies. While this is a good approach for resource-rich languages, this poses problems for languages where such resources are not readily available. Approaches to alleviate this issue often make use of bilingual dictionaries or machine translation (Tsvetkov et al., 2013; Dunn et al., 2014), in itself introducing the need for a new resource resp. introducing a possible new source for misclassification. Automatic metaphor detection usually relies on various features, incorporating e.g. selectional preference violations or concreteness ratings to detect metaphors in text. These features rely on background corpora, hand-coded rules or additional, manually created resources, all specific to the language the system is being used on. We present a novel approach to metaphor detection using a neural network in combination with word embeddings, a method that has alrea"
W16-1104,P14-1024,0,0.24907,"cording to Lakoff and Johnson (1980), metaphors are cognitive mappings of concepts from a source to a target domain. While in some works identifying those mappings (conceptual metaphors) themselves is the subject of analysis, we concern ourselves with detecting their manifestations in text (linguistic metaphors). Various features have been designed to model either representation of metaphor, prominently e.g. violations of (generalized) selectional preferences in grammatical relations (Wilks, 1978; Shutova, 2013), concreteness ratings to model the difference between source and target concepts (Tsvetkov et al., 2014; Turney and Assaf, 2011), supersenses and In this paper, we present a novel approach for metaphor detection using neural networks on the token-level in running text, relying solely on word embeddings used in context. In recent years, neural networks have been used to solve natural language processing tasks with great effect, but so far have not been applied to metaphor detection. While our approach still has to be tested on data in other languages, it already shows promising results on English data, all the more considering it is not using an elaborate feature set, deriving the representation"
W16-1104,D11-1063,0,0.233688,"ohnson (1980), metaphors are cognitive mappings of concepts from a source to a target domain. While in some works identifying those mappings (conceptual metaphors) themselves is the subject of analysis, we concern ourselves with detecting their manifestations in text (linguistic metaphors). Various features have been designed to model either representation of metaphor, prominently e.g. violations of (generalized) selectional preferences in grammatical relations (Wilks, 1978; Shutova, 2013), concreteness ratings to model the difference between source and target concepts (Tsvetkov et al., 2014; Turney and Assaf, 2011), supersenses and In this paper, we present a novel approach for metaphor detection using neural networks on the token-level in running text, relying solely on word embeddings used in context. In recent years, neural networks have been used to solve natural language processing tasks with great effect, but so far have not been applied to metaphor detection. While our approach still has to be tested on data in other languages, it already shows promising results on English data, all the more considering it is not using an elaborate feature set, deriving the representation only from distributed an"
W16-1306,bauer-etal-2012-dependency,0,0.0191404,"-FE Alignment After identifying property-frame correspondences, property arguments are mapped to FEs as follows (Figure 3): 1) Creating Argument/FE Context: Regarding property arguments, we apply the procedure described in Section 3 to create two contexts for each argument: semantic type and filler contexts. Similarly, we create for each FE two contexts: 1) semantic type context which consists of the label and the semantic type of that FE as defined in FN and 2) filler context which contains the headwords of the fillers of that FE which were obtained from the FN annotated corpus according to (Bauer et al., 2012). 2) Generating Word Embedding Vectors: Next, the embedding vector for each word in the argument/FE context are retrieved from a word embedding space that was trained on the Google News dataset as provided by the word2vec framework (Mikolov et al., 2013a). We chose this embedding space due to its high coverage of three million words and phrases. Indeed, phrases are crucial in our case, especially, since the majority of argument fillers correspond to named entities. Subsequently, the embedding vectors are summed to produced one final vector per context, i.e., one for the semantic type context a"
W16-1306,W08-2208,0,0.0452064,"Missing"
W16-1306,P14-5004,0,0.0137989,"frame based on its lexical units and frame label. In contrast to the rich frame context (each frame is associated with 13 lexical units on average), property context is rather poor. This is because a considerable part of WD properties has few to no aliases. Therefore, we expand the property context with additional words based the technique of word embedding (Mikolov et al., 2013b). Word embedding is a technique for representing words as vectors of real numbers in a low-dimensional space. It has gained much attention recently and has been successfully applied to a wide range of semantic tasks (Faruqui and Dyer, 2014). (Levy and Goldberg, 2014) presented a word embedding approach in which the context of a given word is created based on the dependency graph of that word over large collection of sentences. According to this approach, words with similar functionality, such as co-hyponyms lay close to each other in the embedding space. This type of embedding is good candidate for our case because we assume that words of similar functionality would evoke the same frame. Therefore, we use the pre-calculated word vectors provided by (Levy and Goldberg, 2014) to expand the context of WD properties. First, we ident"
W16-1306,P13-1134,1,0.861231,"2: The accuracy of argument-FE alignments 33 6 Related Work The problem of aligning expert lexical resources in order to increase their coverage was the topic of several research efforts (Shi and Mihalcea, 2005; Chow and Webster, 2007; Johansson and Nugues, 2007; De Cao et al., 2008; Lacalle et al., 2014). Another line of research considered aligning communitycreated resources like Wikipedia and Wiktionary to lexical resources like FrameNet. (Tonelli and Giuliano, 2009; Tonelli et al., 2013) presented an approach for extending FN by linking its LUs to Wikipedia articles using supervised WSD. (Hartmann and Gurevych, 2013) presented an approach for linking FN with Wiktionary in order to build a FrameNet-like resource for German. While our work consider the alignment on the relation level, the mentioned efforts focus on extending the coverage of FN by inducing new LUs using word-sense alignment techniques. In fact, the problem of aligning FN frames with knowledge base relations is new. An initial attempt with a similar goal as ours was presented by sar-graph (Krause et al., 2015). sargraph is a graph that connects different contractions of a given relation. The nodes correspond to words or arguments of that rela"
W16-1306,W15-4204,0,0.0616115,"Missing"
W16-1306,lopez-de-lacalle-etal-2014-predicate,0,0.05168,"Missing"
W16-1306,P14-2050,0,0.0346254,"units and frame label. In contrast to the rich frame context (each frame is associated with 13 lexical units on average), property context is rather poor. This is because a considerable part of WD properties has few to no aliases. Therefore, we expand the property context with additional words based the technique of word embedding (Mikolov et al., 2013b). Word embedding is a technique for representing words as vectors of real numbers in a low-dimensional space. It has gained much attention recently and has been successfully applied to a wide range of semantic tasks (Faruqui and Dyer, 2014). (Levy and Goldberg, 2014) presented a word embedding approach in which the context of a given word is created based on the dependency graph of that word over large collection of sentences. According to this approach, words with similar functionality, such as co-hyponyms lay close to each other in the embedding space. This type of embedding is good candidate for our case because we assume that words of similar functionality would evoke the same frame. Therefore, we use the pre-calculated word vectors provided by (Levy and Goldberg, 2014) to expand the context of WD properties. First, we identify for each label and alia"
W16-1306,D09-1029,0,0.024684,"his task. Overlap Cosine Jaccard Our Method ARG 1 Accuracy 0.55 0.51 0.53 0.70 ARG 2 Accuracy 0.56 0.62 0.63 0.68 AVG Accuracy 0.56 0.57 0.58 0.69 Table 2: The accuracy of argument-FE alignments 33 6 Related Work The problem of aligning expert lexical resources in order to increase their coverage was the topic of several research efforts (Shi and Mihalcea, 2005; Chow and Webster, 2007; Johansson and Nugues, 2007; De Cao et al., 2008; Lacalle et al., 2014). Another line of research considered aligning communitycreated resources like Wikipedia and Wiktionary to lexical resources like FrameNet. (Tonelli and Giuliano, 2009; Tonelli et al., 2013) presented an approach for extending FN by linking its LUs to Wikipedia articles using supervised WSD. (Hartmann and Gurevych, 2013) presented an approach for linking FN with Wiktionary in order to build a FrameNet-like resource for German. While our work consider the alignment on the relation level, the mentioned efforts focus on extending the coverage of FN by inducing new LUs using word-sense alignment techniques. In fact, the problem of aligning FN frames with knowledge base relations is new. An initial attempt with a similar goal as ours was presented by sar-graph ("
W16-2808,W14-2109,0,0.0192301,"n mining in various genres. Moens et al. (2007) identify argumentative sentences in newspapers, parliamentary records, court reports and online discussions. Mochales-Palau and Moens (2009) identify argumentation structures including claims and premises in court cases. Other approaches focus on online comments and recognize argument components (Habernal and Gurevych, 2015), justifications (Biran and Rambow, 2011) or different types of claims (Kwon et al., 2007). Work in the context of the IBM Debater project deals with identifying claims and evidence in Wikipedia articles (Rinott et al., 2015; Aharoni et al., 2014). Peldszus and Stede (2015) identify argumentation structures in microtexts (similar to essays). They rely on several base classifiers and minimum spanning trees to recognize argumentative tree structures. Stab and Gurevych (2016) extract argument structures from essays by recognizing argument components and jointly modeling their types and relations between them. Both approaches focus on the structure and neglect the content of arguments. Persing and Ng (2015) annotate argument strength, which is related to content, yet what it is that makes an argument strong has not been made explicit in th"
W16-2808,W14-2104,0,0.322564,"mentative writing, is receiving increased attention, along with a demand for argumentation-aware Automated Writing Evaluation (AWE) systems. However, current AWE systems typically do not consider argumentation (Lim and Kahng, 2012), and employ features that address grammar, mechanics, discourse structure, syntactic and lexical richness (Burstein et al., 2013). Developments in Computational Argumentation (CA) could bridge this gap. Recently, progress has been made towards a more detailed understanding of argumentation in essays (Song et al., 2014; Stab and Gurevych, 2014; Persing and Ng, 2015; Ong et al., 2014). An important distinction emerging from the relevant work is that between argumentative structure and argumentative content. Facility with the argumentation structure underlies the contrast between (1) and (2) below: In (1), claims are made without support; relationships between claims are not explicit; there is intervening irrelevant material. In (2), the argumentative structure is clear – there is a critical claim supported by a specific reason. Yet, 1 (2) “The mayor’s policy of switching to a new trash collector service is flawed because he failed to consider the ugly color of the trucks u"
W16-2808,D15-1110,0,0.0488105,"res. Moens et al. (2007) identify argumentative sentences in newspapers, parliamentary records, court reports and online discussions. Mochales-Palau and Moens (2009) identify argumentation structures including claims and premises in court cases. Other approaches focus on online comments and recognize argument components (Habernal and Gurevych, 2015), justifications (Biran and Rambow, 2011) or different types of claims (Kwon et al., 2007). Work in the context of the IBM Debater project deals with identifying claims and evidence in Wikipedia articles (Rinott et al., 2015; Aharoni et al., 2014). Peldszus and Stede (2015) identify argumentation structures in microtexts (similar to essays). They rely on several base classifiers and minimum spanning trees to recognize argumentative tree structures. Stab and Gurevych (2016) extract argument structures from essays by recognizing argument components and jointly modeling their types and relations between them. Both approaches focus on the structure and neglect the content of arguments. Persing and Ng (2015) annotate argument strength, which is related to content, yet what it is that makes an argument strong has not been made explicit in the rubric and the annotation"
W16-2808,P15-1053,0,0.378947,"ore specifically, argumentative writing, is receiving increased attention, along with a demand for argumentation-aware Automated Writing Evaluation (AWE) systems. However, current AWE systems typically do not consider argumentation (Lim and Kahng, 2012), and employ features that address grammar, mechanics, discourse structure, syntactic and lexical richness (Burstein et al., 2013). Developments in Computational Argumentation (CA) could bridge this gap. Recently, progress has been made towards a more detailed understanding of argumentation in essays (Song et al., 2014; Stab and Gurevych, 2014; Persing and Ng, 2015; Ong et al., 2014). An important distinction emerging from the relevant work is that between argumentative structure and argumentative content. Facility with the argumentation structure underlies the contrast between (1) and (2) below: In (1), claims are made without support; relationships between claims are not explicit; there is intervening irrelevant material. In (2), the argumentative structure is clear – there is a critical claim supported by a specific reason. Yet, 1 (2) “The mayor’s policy of switching to a new trash collector service is flawed because he failed to consider the ugly co"
W16-2808,D15-1050,0,0.0564987,"cuses on argumentation mining in various genres. Moens et al. (2007) identify argumentative sentences in newspapers, parliamentary records, court reports and online discussions. Mochales-Palau and Moens (2009) identify argumentation structures including claims and premises in court cases. Other approaches focus on online comments and recognize argument components (Habernal and Gurevych, 2015), justifications (Biran and Rambow, 2011) or different types of claims (Kwon et al., 2007). Work in the context of the IBM Debater project deals with identifying claims and evidence in Wikipedia articles (Rinott et al., 2015; Aharoni et al., 2014). Peldszus and Stede (2015) identify argumentation structures in microtexts (similar to essays). They rely on several base classifiers and minimum spanning trees to recognize argumentative tree structures. Stab and Gurevych (2016) extract argument structures from essays by recognizing argument components and jointly modeling their types and relations between them. Both approaches focus on the structure and neglect the content of arguments. Persing and Ng (2015) annotate argument strength, which is related to content, yet what it is that makes an argument strong has not b"
W16-2808,W14-2110,1,0.927507,"ndards for Education,1 argumentation, and, more specifically, argumentative writing, is receiving increased attention, along with a demand for argumentation-aware Automated Writing Evaluation (AWE) systems. However, current AWE systems typically do not consider argumentation (Lim and Kahng, 2012), and employ features that address grammar, mechanics, discourse structure, syntactic and lexical richness (Burstein et al., 2013). Developments in Computational Argumentation (CA) could bridge this gap. Recently, progress has been made towards a more detailed understanding of argumentation in essays (Song et al., 2014; Stab and Gurevych, 2014; Persing and Ng, 2015; Ong et al., 2014). An important distinction emerging from the relevant work is that between argumentative structure and argumentative content. Facility with the argumentation structure underlies the contrast between (1) and (2) below: In (1), claims are made without support; relationships between claims are not explicit; there is intervening irrelevant material. In (2), the argumentative structure is clear – there is a critical claim supported by a specific reason. Yet, 1 (2) “The mayor’s policy of switching to a new trash collector service is f"
W16-2808,D15-1255,1,0.837525,"and content are conceptually distinct, they might in reality go together. We therefore evaluate the ability of the structurebased system to deal with content-based annotations of argumentation. 2 Related Work 3 Existing work in CA focuses on argumentation mining in various genres. Moens et al. (2007) identify argumentative sentences in newspapers, parliamentary records, court reports and online discussions. Mochales-Palau and Moens (2009) identify argumentation structures including claims and premises in court cases. Other approaches focus on online comments and recognize argument components (Habernal and Gurevych, 2015), justifications (Biran and Rambow, 2011) or different types of claims (Kwon et al., 2007). Work in the context of the IBM Debater project deals with identifying claims and evidence in Wikipedia articles (Rinott et al., 2015; Aharoni et al., 2014). Peldszus and Stede (2015) identify argumentation structures in microtexts (similar to essays). They rely on several base classifiers and minimum spanning trees to recognize argumentative tree structures. Stab and Gurevych (2016) extract argument structures from essays by recognizing argument components and jointly modeling their types and relations"
W16-2808,C14-1142,1,0.829606,"n,1 argumentation, and, more specifically, argumentative writing, is receiving increased attention, along with a demand for argumentation-aware Automated Writing Evaluation (AWE) systems. However, current AWE systems typically do not consider argumentation (Lim and Kahng, 2012), and employ features that address grammar, mechanics, discourse structure, syntactic and lexical richness (Burstein et al., 2013). Developments in Computational Argumentation (CA) could bridge this gap. Recently, progress has been made towards a more detailed understanding of argumentation in essays (Song et al., 2014; Stab and Gurevych, 2014; Persing and Ng, 2015; Ong et al., 2014). An important distinction emerging from the relevant work is that between argumentative structure and argumentative content. Facility with the argumentation structure underlies the contrast between (1) and (2) below: In (1), claims are made without support; relationships between claims are not explicit; there is intervening irrelevant material. In (2), the argumentative structure is clear – there is a critical claim supported by a specific reason. Yet, 1 (2) “The mayor’s policy of switching to a new trash collector service is flawed because he failed t"
W16-2808,J17-3005,1,\N,Missing
W16-2813,D14-1083,0,0.016996,"its using conditional random fields (CRF). Second, they jointly model the argument component types and argumentative relations using integer linear programming (ILP) and finally they distinguish between supporting and opposing arguments. We employ this parser as a structural approach and compare it to our document classification approach for recognizing the absence of opposing arguments in persuasive essays. Another related area is stance recognition that aims at identifying the author’s stance on a controversy by labeling a document as either “for” or “against” (Somasundaran and Wiebe, 2009; Hasan and Ng, 2014). Consequently, stance recognition systems are designed to identify the predominant stance of a text instead of recognizing the presence of less conspicuous opposing arguments. Other approaches on argumentation in essays focus on thesis clarity (Persing and Ng, 2013), argumentation schemes (Song et al., 2014) or argumentation strength (Persing and Ng, 2015). We are not aware of any approach that focuses on recognizing the absence of opposing arguments. 3 3.1 Inter-Annotator Agreement To verify that the derived document-level annotations are reliable, we compare the annotations derived from the"
W16-2813,P03-1054,0,0.045146,"Missing"
W16-2813,Q13-1034,0,0.0517905,"Missing"
W16-2813,C14-1141,0,0.0287302,"tomatically recognizes the absence of opposing arguments effectively guides students to improve their argumentation. For the same reason, the writing standards of the common core standard1 require that students are able to clarify the relation between their own standpoint and opposing arguments on a controversial topic. Existing structural approaches on argument analysis like the argumentation structure parser 1 2 Related Work Existing approaches in computational argumentation focus primarily on the identification of arguments, their components (e.g. claims and premises) (Rinott et al., 2015; Levy et al., 2014) and structures (Mochales-Palau and Moens, 2011; Stab and Gurevych, 2014b). Among these, there www.corestandards.org 113 Proceedings of the 3rd Workshop on Argument Mining, pages 113–118, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics and class distribution for detecting the absence of opposing arguments at the document-level. Each essay in this corpus is annotated with argumentation structures that allow to derive documentlevel annotations. The argumentation structures include arguments supporting or opposing the author’s stance. Accordingly, we consider"
W16-2813,D09-1036,0,0.101049,"Missing"
W16-2813,P14-5011,1,0.825922,"Missing"
W16-2813,W14-5201,1,0.874473,"Missing"
W16-2813,D15-1110,0,0.0571093,"includes supporting arguments and as positive if it includes at least one opposing argument. Note that the manual identification of opposing arguments is a subtask of the argumentation structure identification. Both require that the annotators identify the author’s stance, the individual arguments and if an argument supports or opposes the author’s stance. Thus, deriving document-level annotations from argumentation structures is a valid approach since the decisions of the annotators in both tasks are equivalent. are few approaches which distinguish between supporting and opposing arguments. Peldszus and Stede (2015b) use lexical, contextual and syntactic features to classify argument components as support or oppose. They experiment with pro/contra columns of a German newspaper and German microtexts. Similarly, their minimum spanning tree (MST) approach identifies the structure of arguments and recognizes if an argument component belongs to the proponent or opponent (Peldszus and Stede, 2015a). However, both approaches presuppose that the components of an argument are already known. Thus, they omit important analysis steps and cannot be applied directly for recognizing the absence of opposing arguments."
W16-2813,W15-0513,0,0.0167686,"includes supporting arguments and as positive if it includes at least one opposing argument. Note that the manual identification of opposing arguments is a subtask of the argumentation structure identification. Both require that the annotators identify the author’s stance, the individual arguments and if an argument supports or opposes the author’s stance. Thus, deriving document-level annotations from argumentation structures is a valid approach since the decisions of the annotators in both tasks are equivalent. are few approaches which distinguish between supporting and opposing arguments. Peldszus and Stede (2015b) use lexical, contextual and syntactic features to classify argument components as support or oppose. They experiment with pro/contra columns of a German newspaper and German microtexts. Similarly, their minimum spanning tree (MST) approach identifies the structure of arguments and recognizes if an argument component belongs to the proponent or opponent (Peldszus and Stede, 2015a). However, both approaches presuppose that the components of an argument are already known. Thus, they omit important analysis steps and cannot be applied directly for recognizing the absence of opposing arguments."
W16-2813,P13-1026,0,0.172923,"Missing"
W16-2813,P15-1053,0,0.239957,"Missing"
W16-2813,D15-1050,0,0.0532095,"hat a system which automatically recognizes the absence of opposing arguments effectively guides students to improve their argumentation. For the same reason, the writing standards of the common core standard1 require that students are able to clarify the relation between their own standpoint and opposing arguments on a controversial topic. Existing structural approaches on argument analysis like the argumentation structure parser 1 2 Related Work Existing approaches in computational argumentation focus primarily on the identification of arguments, their components (e.g. claims and premises) (Rinott et al., 2015; Levy et al., 2014) and structures (Mochales-Palau and Moens, 2011; Stab and Gurevych, 2014b). Among these, there www.corestandards.org 113 Proceedings of the 3rd Workshop on Argument Mining, pages 113–118, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics and class distribution for detecting the absence of opposing arguments at the document-level. Each essay in this corpus is annotated with argumentation structures that allow to derive documentlevel annotations. The argumentation structures include arguments supporting or opposing the author’s stance. Accor"
W16-2813,D13-1170,0,0.0126956,"Missing"
W16-2813,P09-1026,0,0.241727,"owledge Processing Lab (UKP-DIPF) German Institute for Educational Research www.ukp.tu-darmstadt.de Abstract presented by Stab and Gurevych (2016) or the approach introduced by Peldszus and Stede (2015a) recognize the internal microstructure of arguments. Although these approaches can be exploited for identifying opposing arguments, they require several consecutive analysis steps like separating argumentative from non-argumentative text units (Moens et al., 2007), recognizing the boundaries of argument components (Goudas et al., 2014) and classifying individual arguments as support or oppose (Somasundaran and Wiebe, 2009). Certainly, an advantage of structural approaches is that they recognize the position of opposing arguments in text. However, knowing the position of opposing arguments is only relevant for positive feedback to the author and irrelevant for negative feedback, i.e. pointing out that opposing arguments are missing. Therefore, it is reasonable to model the recognition of missing opposing arguments as a document classification task. The contributions of this paper are the following: first, we introduce a corpus for detecting the absence of opposing arguments that we derive from argument structure"
W16-2813,W14-2110,0,0.0798374,"Missing"
W16-2813,C14-1142,1,0.829858,"y guides students to improve their argumentation. For the same reason, the writing standards of the common core standard1 require that students are able to clarify the relation between their own standpoint and opposing arguments on a controversial topic. Existing structural approaches on argument analysis like the argumentation structure parser 1 2 Related Work Existing approaches in computational argumentation focus primarily on the identification of arguments, their components (e.g. claims and premises) (Rinott et al., 2015; Levy et al., 2014) and structures (Mochales-Palau and Moens, 2011; Stab and Gurevych, 2014b). Among these, there www.corestandards.org 113 Proceedings of the 3rd Workshop on Argument Mining, pages 113–118, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics and class distribution for detecting the absence of opposing arguments at the document-level. Each essay in this corpus is annotated with argumentation structures that allow to derive documentlevel annotations. The argumentation structures include arguments supporting or opposing the author’s stance. Accordingly, we consider an essay as negative if it solely includes supporting arguments and as p"
W16-2813,D14-1006,1,0.870647,"y guides students to improve their argumentation. For the same reason, the writing standards of the common core standard1 require that students are able to clarify the relation between their own standpoint and opposing arguments on a controversial topic. Existing structural approaches on argument analysis like the argumentation structure parser 1 2 Related Work Existing approaches in computational argumentation focus primarily on the identification of arguments, their components (e.g. claims and premises) (Rinott et al., 2015; Levy et al., 2014) and structures (Mochales-Palau and Moens, 2011; Stab and Gurevych, 2014b). Among these, there www.corestandards.org 113 Proceedings of the 3rd Workshop on Argument Mining, pages 113–118, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics and class distribution for detecting the absence of opposing arguments at the document-level. Each essay in this corpus is annotated with argumentation structures that allow to derive documentlevel annotations. The argumentation structures include arguments supporting or opposing the author’s stance. Accordingly, we consider an essay as negative if it solely includes supporting arguments and as p"
W16-2813,J17-3005,1,\N,Missing
W16-4011,P98-1013,0,0.0649142,"syntactic structures. This includes, but is not limited to the tools mentioned in Section 2. WebAnno 3 was developed and implemented in close coordination with users in the context of an annotation project (cf. M´ujdricza-Maydt et al. (2016)) for word sense disambiguation (WSD) and SRL on German texts and driven by its practical requirements. SRL is the task of identifying semantic predicates, their arguments, and assigning roles to these arguments. It is a difficult task usually performed by experts. Examples of well-known SRL schemes motivated by different linguistic theories are FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbNet (Kipper Schuler, 2005). SRL annotation is typically based on syntactic structures obtained from treebanks, such as the constituent-based Penn Treebank (for PropBank annotation), or the German TIGER treebank for FrameNet-style annotation (Burchardt et al., 2009). An argument is typically identified by the span of its syntactic head or syntactic constituent. For some annotation schemes (e.g. FrameNet), the task also includes WSD. In this case, the sense label typically determines the available argument slots. The example below shows an annotation usi"
W16-4011,burchardt-etal-2006-salto,1,0.893566,"Missing"
W16-4011,N13-3004,0,0.131331,"ject in Section 4. While joint WSD and SRL annotations are conveniently supported and facilitated using constraints, they can also be performed separately and independently of one another. 2 Related Work We briefly review presently available annotation tools that could be used for annotating semantic structures. Since we aim to support geographically distributed annotation teams, we consider recent generic webbased annotation tools. Additionally, we examine tools from earlier semantic annotation projects that are specialised for SRL but not web-based. 2.1 Web-based Annotation Tools Anafora by Chen and Styler (2013) is a recent web-based annotation tool for event-like structures. Specifically, it supports the annotation of spans and n-ary relations. Spans are anchored on text while relations exist independently from the text and consist of slots that can be filled with spans. Annotations are visualised using a coloured text background. Selecting a relation highlights the participating spans by placing boxes around them. Anafora is not suited for annotation tasks that require an alignment of the semantic structures with syntactic structures such as constituent or dependency parse trees. brat by Stenetorp"
W16-4011,N10-2004,0,0.0183683,"s with FrameNet categories. Once a frame for a predicate has been selected, applicable roles from FrameNet can be assigned to nodes in the parse tree by drag-and-drop. SALTO supports discontinuous annotations, multi-token annotations, and cross-sentence annotations. It also offers basic team management functionalities including workload assignment and curation. However, annotators cannot correct mistakes in the underlying treebank because the parse tree is not editable. This is problematic for automatically preprocessed input. The final release of SALTO was in 2012. Jubilee and Cornerstone by Choi et al. (2010) are tools for annotating PropBank. Jubilee supports the annotation of PropBank instances, while its sister tool Cornerstone allows editing the frameset XML files that provide the annotation scheme to Jubilee. The user interface (UI) of Jubilee displays a treebank view and allows annotating nodes in the parse tree with frameset senses and roles. Jubilee supports annotation and adjudication of annotations in small teams. It is a Java application that stores all data on the file system. Thus, the annotation team needs to be able to access a common file system, which does not meet our needs for a"
W16-4011,W97-0802,0,0.114672,"since 2014. 2.3 Requirements of Semantic Annotation The annotation of semantic structures imposes two main requirements on annotation tools: 1) the flexibility to support multiple layers of annotation including syntactic and semantic layers using freely configurable annotation schemes and 2) the ability to handle large, interdependent tagsets. Flexible multi-layer annotation. While the usage-driven design of dedicated SRL annotation tools allows for a very efficient annotation, users face a serious lack of flexibility when trying to combine different annotation schemes (e.g. GermaNet senses (Hamp and Feldweg, 1997) and VerbNet roles), or when trying to use data preprocessed in different ways (e.g. for a crowdsourcing approach, automatically pre-annotating predicate and argument spans can be helpful, while experts may find pre-annotated dependency relations beneficial). This is not supported by current web-based annotation tools. Handling rich annotation schemes. Tools need to specifically support rich semantic annotation schemes—like FrameNet—with interdependent labels (i.e. sense labels determine available argument roles). Manually typing sense and role labels is error-prone, and selecting them from a"
W16-4011,L16-1484,1,0.824737,"Missing"
W16-4011,J05-1004,0,0.026006,"ludes, but is not limited to the tools mentioned in Section 2. WebAnno 3 was developed and implemented in close coordination with users in the context of an annotation project (cf. M´ujdricza-Maydt et al. (2016)) for word sense disambiguation (WSD) and SRL on German texts and driven by its practical requirements. SRL is the task of identifying semantic predicates, their arguments, and assigning roles to these arguments. It is a difficult task usually performed by experts. Examples of well-known SRL schemes motivated by different linguistic theories are FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbNet (Kipper Schuler, 2005). SRL annotation is typically based on syntactic structures obtained from treebanks, such as the constituent-based Penn Treebank (for PropBank annotation), or the German TIGER treebank for FrameNet-style annotation (Burchardt et al., 2009). An argument is typically identified by the span of its syntactic head or syntactic constituent. For some annotation schemes (e.g. FrameNet), the task also includes WSD. In this case, the sense label typically determines the available argument slots. The example below shows an annotation using FrameNet; the predicate ask r"
W16-4011,E12-2021,0,0.0315475,"Missing"
W16-4011,P13-4001,1,0.642633,"Missing"
W16-4011,P14-5016,1,0.655131,"Missing"
W16-4011,C98-1013,0,\N,Missing
W16-4318,P16-1150,1,0.762232,"c composition (Coca et al., 2013) and phoneme classification (Graves and Schmidhuber, 2005). They have also been applied in opinion mining on text (Wang et al., 2016), but have not yet been explored for opinion mining on speech. 3.2 Debating Technologies The field of debating technologies is a newly developing research area that focuses on computational methods to support human argumentation and debating (Gurevych et al., 2016). In recent work, claim identification for controversial topics (Roitman et al., 2016), evidence detection (Rinott et al., 2015) and argument convincingness prediction (Habernal and Gurevych, 2016) have been tackled. These works focus on analyzing the content, but Hosman et al. (2002) showed that paralinguistic features are very informative to detect credibility and persuasiveness of speakers. This observation has been used in the work by Lippi and Torroni (2016) who combine paralinguistic features with textual features to detect claims in political debates. They represent the input signal by mel-frequency cepstral coefficients and find that the combination of text and audio modalities yields the best results. Brilman and Scherer (2015) also apply a multi-modal approach and combine text"
W16-4318,P13-1096,0,0.0734062,"Missing"
W16-4318,D15-1303,0,0.144256,"eering. In order to test whether our approach can compete with state-of-the-art methods, we focus on two interesting tasks concerning speech: opinion mining and persuasiveness prediction. For both tasks, the goal can be framed as opinion prediction, but the perspective differs. In the first task, our goal is to predict the opinion of a user speaking about a product. In the second task, we aim at predicting the influence of a speaker on the opinion of an audience. Previous approaches to these tasks developed a sophisticated feature set to capture the recognition of emotions for opinion mining (Poria et al., 2015) and the characteristics of voice quality for persuasiveness prediction (Brilman and Scherer, 2015). We find that the results of our domain-agnostic approach come close to the performance of domainspecific ones that apply thorough feature engineering. As we use the same features for different tasks, we minimize the risk of overfitting to the data. Our error analysis explain in more details the issues with our approach in both datasets, but also highlight how far a generic computational method based solely on speech can go in tasks related to opinion prediction. 2 Tasks For the evaluation of ou"
W16-4318,D15-1050,0,0.0256646,"obtained good results for audio processing tasks such as music composition (Coca et al., 2013) and phoneme classification (Graves and Schmidhuber, 2005). They have also been applied in opinion mining on text (Wang et al., 2016), but have not yet been explored for opinion mining on speech. 3.2 Debating Technologies The field of debating technologies is a newly developing research area that focuses on computational methods to support human argumentation and debating (Gurevych et al., 2016). In recent work, claim identification for controversial topics (Roitman et al., 2016), evidence detection (Rinott et al., 2015) and argument convincingness prediction (Habernal and Gurevych, 2016) have been tackled. These works focus on analyzing the content, but Hosman et al. (2002) showed that paralinguistic features are very informative to detect credibility and persuasiveness of speakers. This observation has been used in the work by Lippi and Torroni (2016) who combine paralinguistic features with textual features to detect claims in political debates. They represent the input signal by mel-frequency cepstral coefficients and find that the combination of text and audio modalities yields the best results. Brilman"
W16-4318,P16-2037,0,0.02156,"ance of temporal aspects for speech perception (Rosen, 1992), we model the speech signal as a time series. In previous work on opinion mining in speech, complex functions had been calculated over the features extracted at the frame level to account for the temporal dependencies. Recent progress in modeling time series data has been achieved with long short term memory networks. They have obtained good results for audio processing tasks such as music composition (Coca et al., 2013) and phoneme classification (Graves and Schmidhuber, 2005). They have also been applied in opinion mining on text (Wang et al., 2016), but have not yet been explored for opinion mining on speech. 3.2 Debating Technologies The field of debating technologies is a newly developing research area that focuses on computational methods to support human argumentation and debating (Gurevych et al., 2016). In recent work, claim identification for controversial topics (Roitman et al., 2016), evidence detection (Rinott et al., 2015) and argument convincingness prediction (Habernal and Gurevych, 2016) have been tackled. These works focus on analyzing the content, but Hosman et al. (2002) showed that paralinguistic features are very info"
W17-0814,C16-1058,0,0.0139423,"33 145 444 466 456 221 8 216 25 176 165 1,022 1,025 1,017 Table 1: Data statistics for SR3de (PB, VN, FN). Training data generation In this work, we use a corpus-based, monolingual approach to training data expansion. F¨urstenau and Lapata (2012) propose monolingual annotation projection for lowerresourced languages: they create data labeled with FrameNet frames and roles based on a small set of labeled seed sentences in the target language. We apply their approach to the different SRL frameworks, and for the first time to VerbNet-style labels. Other approaches apply cross-lingual projection (Akbik and Li, 2016) or paraphrasing, replacing FrameNet predicates (Pavlick et al., 2015) or PropBank arguments (Woodsend and Lapata, 2014) in labeled texts. We do not employ these approaches, because they assume large role-labeled corpora. 3 dev Datasets and Data Expansion Method SR3de: a German parallel SRL dataset The VerbNet-style dataset by M´ujdricza-Maydt et al. (2016) covers a subset of the PropBank-style CoNLL 2009 annotations, which are based on the German FrameNet-style SALSA corpus. This allowed us to create SR3de, the first corpus with parallel sense and role labels from SALSA, PropBank, and GermaNe"
W17-0814,W09-1206,0,0.0447772,"Missing"
W17-0814,C10-1011,0,0.0161969,"cate lemma and align dependency graphs of seeds and expansions based on lexical similarity of the graph nodes and syntactic similarity of the edges. The alignment is then used to map predicate and role labels from the seed sentences to the expansion sentences. For each seed instance, the k best-scoring expansions are selected. Given a seed set of size n and the maximal number of expansions per seed k, we get up to n · k additional training instances. Lexical and syntactic similarity are balanced using the weight parameter α. Our adjusted re-implementation uses the matetools dependency parser (Bohnet, 2010) and word2vec embeddings (Mikolov et al., 2013) trained on deWAC (Baroni et al., 2009) for word similarity calculation. We tune the parameter α via intrinsic evaluation on the SR3de dev set. We project the seed set SR3de-train directly to SR3dedev and compare the labels from the k=1 best seeds for a dev sentence to the gold label, measuring F1 for all projections. Then we use the best-scoring α value for each framework to project annotations from the SR3de training set to deWAC for predicate lemmas occurring at least 10 times. We vary the number of expansions k, selecting k from {1, 3, 5, 10,"
W17-0814,burchardt-etal-2006-salsa,1,0.888828,"Missing"
W17-0814,J12-1005,0,0.319427,"Missing"
W17-0814,W97-0802,0,0.170745,"style roles using labels A0 and A1 for Agent- and Patient-like roles, and continuing up to A9 for other arguments. Instead of spans, arguments were defined by their dependency heads for CoNLL. The resulting dataset was used as a benchmark dataset in the CoNLL 2009 shared task. For VerbNet, M´ujdricza-Maydt et al. (2016) recently published a small subset of the CoNLL shared task corpus with VerbNet-style roles. It contains 3,500 predicate instances for 275 predicate lemma types. Since there is no taxonomy of verb classes for German corresponding to original VerbNet classes, they used GermaNet (Hamp and Feldweg, 1997) to label predicate senses. GermaNet provides a fine-grained sense inventory similar to the English WordNet (Fellbaum, 1998). Comparison of SRL frameworks Previous experimental work compares VerbNet and PropBank: Zapirain et al. (2008) find that PropBank SRL is more robust than VerbNet SRL, generalizing better to unseen or rare predicates, and relying less on predicate sense. Still, they aspire to use more mean2 Cf. Hajiˇc et al. (2009), Sun et al. (2010), Boas (2009). 116 Automatic SRL systems for German State-ofthe-art SRL systems for German are only available for PropBank labels: Bj¨orkelun"
W17-0814,P09-1033,0,0.491813,"Missing"
W17-0814,L16-1484,1,0.871771,"Missing"
W17-0814,J05-1004,0,0.189767,"te and role labels from the different frameworks and applying the same conditions and criteria for training and testing. Previous work comparing these frameworks either provides theoretical investigations, for instance for the pair PropBank–FrameNet (Ellsworth et al., 2004), or presents experimental investigations for the pair PropBank–VerbNet (Zapirain et al., 2008; Merlo and van der Plas, 2009). Theoretical analyses contrast the richness of the semantic model of FrameNet with efficient annotation of PropBank labels and their suitability for system training. Verb1 See Fillmore et al. (2003), Palmer et al. (2005) and KipperSchuler (2005), respectively. 115 Proceedings of the 11th Linguistic Annotation Workshop, pages 115–121, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Our work explores the generalization properties of three SRL frameworks in a contrastive setup, assessing SRL performance when training and evaluating on a dataset with parallel annotations for each framework in a uniform SRL system architecture. We also explore to what extent the frameworks benefit from training data generation via annotation projection (F¨urstenau and Lapata, 2012). Since all three"
W17-0814,P15-2067,0,0.116907,"Missing"
W17-0814,Q15-1032,0,0.0355932,"Missing"
W17-0814,D14-1045,0,0.156842,"Missing"
W17-0814,C10-1119,0,0.0636362,"Missing"
W17-0814,P08-1063,0,0.650213,"Missing"
W17-0902,J14-2004,0,0.0128507,"etween predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-ali"
W17-0902,araki-etal-2014-detecting,0,0.0144858,"tracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between pre"
W17-0902,D08-1031,1,0.548822,"reras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (O"
W17-0902,P10-1124,1,0.91175,"ic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+"
W17-0902,J12-1003,1,0.927138,"rbal performance (accuracy of 0.73 vs. 0.25). Finally, argument identification was hard mainly because of inconsistencies in verbal versus nominal predicate-argument structure in dependency trees.4 The low performance in predicate coreference compared to entity coreference can be explained by the higher variability of predicate terms. The argument co-reference task becomes easy given gold predicate-argument structures, as most arguments are singletons (i.e. composed of a single element). Finally, while the performance of the predicate entailment component reflects the current stateof-the-art (Berant et al., 2012; Han and Sun, 2016), the performance on entity entailment is much worse than the current state-of-the-art in this task as measured on common lexical inference test sets. We conjecture that this stems from the nature of the entities in our dataset, consisting of both named entities and common nouns, many of which are multi-word expressions, whereas most work in entity entailment is focused on single word common nouns. Furthermore, it is worth noting that our annotations are of naturally occurring texts, and represent lexical entailment in real world coreference chains, as opposed to synthetica"
W17-0902,W05-0620,0,0.162639,"Missing"
W17-0902,W99-0201,0,0.0935993,"on Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference"
W17-0902,W09-4303,0,0.0115663,") is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining en"
W17-0902,W13-2322,0,0.107609,"le texts, and in specifying how such representation can be created based on entity and event coreference and lexical entailment. An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al.,"
W17-0902,P15-1136,0,0.0225471,"Missing"
W17-0902,E12-1004,0,0.0217269,"tractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art"
W17-0902,cybulska-vossen-2014-using,0,0.0126286,"achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can h"
W17-0902,P99-1071,0,0.536493,"on. We can expect the use of OKR structures in MDS to shift the research efforts in this task to other components, e.g. generation, and eventually contribute to improving state of the art on this task. Similarly, an algorithm creating the ECKG structure can benefit from building upon a consolidated structure such as OKR, rather than working directly on free text. systematic solution, and the burden of integrating information across multiple texts is delegated to downstream applications, leading to partial solutions which are geared to specific applications. Multi-Document Summarization (MDS) (Barzilay et al., 1999) is a task whose goal is to produce a concise summary from a set of related text documents, such that it includes the most important information in a non-redundant manner. While extractive summarization selects salient sentences from the document collection, abstractive summarization generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a sing"
W17-0902,P13-2080,1,0.82203,"ment annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual information. In discourse representation theory (DRT), a proposition applies to all co-referring entities (Kamp et al., 2011). In recognizing textual entailment (Dagan et al., 2013), lexical substitution of co-referring elements is useful (Stern and Dagan, 2012). For example, in Figure 1, sentence (1) together with the coreference relation between plane and jet entail that “Turkey forces down Syrian jet.” 2.3 Proposed Representation Our Open Knowledge Representation (OKR) aims to capture the consolidated information expressed jointly in a set of texts. In some analogy to structured knowledge bases, we would like the elements of our representation to correspond to entities in the described scenario and to statements (propositions) that relate them. Still, in the spirit of"
W17-0902,bejan-harabagiu-2008-linguistic,0,0.0326074,"1; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align"
W17-0902,duclaye-etal-2002-using,0,0.0512409,"implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 20"
W17-0902,D12-1045,0,0.0269834,"2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing t"
W17-0902,D11-1142,0,0.0614886,"ference and lexical entailment. An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments a"
W17-0902,D14-1168,0,0.0154703,"selects salient sentences from the document collection, abstractive summarization generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entitie"
W17-0902,C16-1273,0,0.0306302,"Missing"
W17-0902,liu-etal-2014-supervised,0,0.0231139,"Missing"
W17-0902,D15-1076,0,0.0606389,"on in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use"
W17-0902,N15-1114,0,0.026058,"ng each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entities, and collapse two entities based on their names rather than on coreference. 7 Conclusions In this paper we advocate the development of representation frameworks for the consolidated information expressed in a set of texts. The key ingredients of our approach are the extraction of pr"
W17-0902,C92-2082,0,0.446376,"wo candidate sentences for the summary differ only in terms that hold a lexical inference relation (e.g. “the plane landed in Ankara” and “the plane landed in Turkey”). Recognizing the inference direction, e.g. that Ankara is more specific than Turkey, can help in selecting the desired granularity level of the description. There has been consistent attention to recognizing lexical inference between terms. Some methods aim to recognize a general lexical inference relation (e.g. (Kotlerman et al., 2010; Turney and Mohammad, 2015)), others focus on a specific semantic relation, mostly hypernymy (Hearst, 1992; Snow et al., 2005; Santus et al., 2014; Shwartz et al., 2016), while recent methods classify a pair of 3.1 Entities To represent entities, we first annotate the text by entity mentions and coreference. Following the typical notion for these tasks, an entity mention corresponds to a word or multi-word expression that refers to an object or concept in the described scenario (in the broader sense of “entity”). Ac14 Original texts: (1) Turkey forces down Syrian plane. (2) Syrian jet grounded by Turkey carried munitions from Moscow. (3) Intercepted Syrian plane carried ammunition from Russia. Ent"
W17-0902,W97-1311,0,0.0722333,"rior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to"
W17-0902,N15-1050,1,0.790932,"event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored"
W17-0902,P13-1048,0,0.0297432,"zation generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entities, and collapse two entities based on their names rather than on corefere"
W17-0902,W16-5304,1,0.885624,"Missing"
W17-0902,K15-1018,1,0.833569,"m for improvement. These bottle-necks are bound to hinder the performance of a pipeline end-to-end system. Future research into OKR should first target these areas; either as a pipeline or in a joint learning framework. 3 For Argument Mention detection we attach the components (entities and propositions) as arguments of predicates when the components are syntactically dependent on them. Argument Coreference is simply predicted by marking coreference if and only if the arguments are both mentions of the same entity co-reference chain. For Entity Entailment purposes we used knowledge resources (Shwartz et al., 2015) and a pretrained model for HypeNET (Shwartz et al., 2016) to obtain a score for all pairs of Wikipedia common words (unigrams, bigrams, and trigrams). A threshold for the binary entailment decision was then calibrated on a held out development set. Finally, for Predicate Entailment we used the entailment rules extracted by Berant et al. (2012). 5.1 Results and Error Analysis Using the same metrics used for measuring interannotator agreement, we evaluated how well the presented models were able to recover the different facets of the OKR gold annotations. The performance on the different subtas"
W17-0902,P15-1146,0,0.0257952,"Missing"
W17-0902,P16-1226,1,0.873666,"Missing"
W17-0902,K15-1002,1,0.834714,"ile the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al.,"
W17-0902,D16-1038,1,0.86973,"Missing"
W17-0902,P11-1080,0,0.0191905,"ddle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankar"
W17-0902,S12-1030,0,0.0281997,"ms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee e"
W17-0902,J01-4004,0,0.174444,"s like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolutio"
W17-0902,P16-1119,1,0.813405,"cly available tools and simple baselines which approximate the current state-of-the-art in each of these subtasks. For brevity sake, in the rest of the section we briefly describe each of these baselines. For a more detailed technical description see the OKR repository (https://github.com/vered1986/ OKR). For Entity Mention extraction we use the spaCy NER model2 in addition to annotating all of the nouns and adjectives as entities. For Proposition Mention detection we use Open IE propositions extracted from PropS (Stanovsky et al., 2016), where non-restrictive arguments were reduced following Stanovsky and Dagan (2016). For ProposiFor entity, predicate, and argument co-reference we calculated coreference resolution metrics: the link-based MUC (Vilain et al., 1995), the mentionbased B 3 (Bagga and Baldwin, 1998), the entitybased CEAF, and the widely adopted CoNLL F1 measure which is an average of the three. For entity and proposition entailment we compute the F1 score over the annotated directed edges in each entailment graph, as is common for entailment agreement metrics (Berant et al., 2010). We macro-averaged these scores to obtain an overall agreement on the 5 events annotated by both annotators. The agr"
W17-0902,E14-4008,0,0.0143178,"mmary differ only in terms that hold a lexical inference relation (e.g. “the plane landed in Ankara” and “the plane landed in Turkey”). Recognizing the inference direction, e.g. that Ankara is more specific than Turkey, can help in selecting the desired granularity level of the description. There has been consistent attention to recognizing lexical inference between terms. Some methods aim to recognize a general lexical inference relation (e.g. (Kotlerman et al., 2010; Turney and Mohammad, 2015)), others focus on a specific semantic relation, mostly hypernymy (Hearst, 1992; Snow et al., 2005; Santus et al., 2014; Shwartz et al., 2016), while recent methods classify a pair of 3.1 Entities To represent entities, we first annotate the text by entity mentions and coreference. Following the typical notion for these tasks, an entity mention corresponds to a word or multi-word expression that refers to an object or concept in the described scenario (in the broader sense of “entity”). Ac14 Original texts: (1) Turkey forces down Syrian plane. (2) Syrian jet grounded by Turkey carried munitions from Moscow. (3) Intercepted Syrian plane carried ammunition from Russia. Entities: E1 = {Turkey}, E2 = {Syrian}, E3"
W17-0902,P15-2050,1,0.81398,"test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are rep"
W17-0902,D10-1106,0,0.0195202,"ub.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity c"
W17-0902,P12-3013,1,0.834165,"ss document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual information. In discourse representation theory (DRT), a proposition applies to all co-referring entities (Kamp et al., 2011). In recognizing textual entailment (Dagan et al., 2013), lexical substitution of co-referring elements is useful (Stern and Dagan, 2012). For example, in Figure 1, sentence (1) together with the coreference relation between plane and jet entail that “Turkey forces down Syrian jet.” 2.3 Proposed Representation Our Open Knowledge Representation (OKR) aims to capture the consolidated information expressed jointly in a set of texts. In some analogy to structured knowledge bases, we would like the elements of our representation to correspond to entities in the described scenario and to statements (propositions) that relate them. Still, in the spirit of Open IE, we would like the representation to be open, while relying only on the"
W17-0902,W04-3206,1,0.412965,"ailable at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions,"
W17-0902,C16-1183,1,0.835662,"on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual"
W17-0902,M95-1005,0,0.294299,"ction we briefly describe each of these baselines. For a more detailed technical description see the OKR repository (https://github.com/vered1986/ OKR). For Entity Mention extraction we use the spaCy NER model2 in addition to annotating all of the nouns and adjectives as entities. For Proposition Mention detection we use Open IE propositions extracted from PropS (Stanovsky et al., 2016), where non-restrictive arguments were reduced following Stanovsky and Dagan (2016). For ProposiFor entity, predicate, and argument co-reference we calculated coreference resolution metrics: the link-based MUC (Vilain et al., 1995), the mentionbased B 3 (Bagga and Baldwin, 1998), the entitybased CEAF, and the widely adopted CoNLL F1 measure which is an average of the three. For entity and proposition entailment we compute the F1 score over the annotated directed edges in each entailment graph, as is common for entailment agreement metrics (Berant et al., 2010). We macro-averaged these scores to obtain an overall agreement on the 5 events annotated by both annotators. The agreement scores for the two annotators are shown in Table 1, and overall show high levels of agreement. A qualitative analysis of the more common disa"
W17-0902,C14-1212,0,0.0125443,"t Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task"
W17-0902,P13-2012,0,0.0402841,"Missing"
W17-0902,N15-1002,0,0.0522403,"Missing"
W17-0902,P02-1014,0,\N,Missing
W17-0902,P15-1034,0,\N,Missing
W17-0908,P14-5011,1,0.837512,"een a story context and an ending. This helps to detect cases where the wrong ending is written in first person although the story context is written in third person. SentiContrast: Disagreement in the sentiment value between the third and fourth sentences of a context and an ending. The sentiment value was computed based on a sentiment word list manually extracted from the ROCStories dataset. For each of the features listed above, an additional feature was added to model the feature value difference between the two endings of a story. All features were extracted using the DKPro TC Framework (Daxenberger et al., 2014). 3.2 Neural Network The overall architecture of our neural network model is shown in Figure 1. First, the tokens of the context sentences s1 , . . . , s4 and of ending e1 are looked up in the word embedding matrix to obtain vectorized representations. The embeddings of the context sentences are concatenated into the vector vc , whereas Features We defined a set of linguistic features in order to profit from the discoveries of our dataset exploration (see Section 2). The features are: NGramOverlap: The number of overlapping ngrams between an ending and a story context 57 those of the ending fo"
W17-0908,D13-1170,0,0.00362496,", 2016; dos Santos et al., 2016), we chose a dimensionality of 141 for the LSTM output vectors and hidden layer H. We employ zero padding for sentences longer than 20 tokens. The network was trained in batches (size 40) for 30 epochs with the Adam optimizer (Kingma and Ba, 2014), starting with a learning rate of 10−4 . We apply dropout (p = 0.3) after computing the BiLSTM output. Finally, ReLu (Glorot et al., 2011) is used as an activation function in layer H. The system was implemented using the TensorFlow library (Abadi et al., 2016). 3.3 employed the state-of-the-art sentiment annotator by Socher et al. (2013) for this purpose. This system does not take the story context into account. The happy ending baseline reached an accuracy of 0.616 on the development split which is substantially higher than random guessing. BiLSTM-V scored 0.708 whereas BiLSTM-VF reached a slightly higher accuracy of 0.712. 3.4 Because our presented approach conducts supervised learning, it requires training data in the same form as testing data. Up to this point, the only dataset available in this form is the Story Cloze validation set which consists of comparably few instances for training. We experimented with ways to aut"
W17-0908,J12-1005,0,0.0760275,"Missing"
W17-0908,P16-1044,0,0.0179567,"output layer, O. Afterwards, the softmax function is applied on the output of O to obtain a score representing the probability of ending e1 being correct. The same procedure is applied for ending e2 in place of ending e1 . Then, the highest-scoring ending is chosen as the correct ending of the story. Due to time constraints and system complexity we decided against hyper-parameter optimization and chose parameter values we deemed reasonable: We used pretrained 100-dimensional GloVe embeddings (Pennington et al., 2014)8 . Following previous work on using BiLSTMs in NLP tasks (Tan et al., 2015; Tan et al., 2016; dos Santos et al., 2016), we chose a dimensionality of 141 for the LSTM output vectors and hidden layer H. We employ zero padding for sentences longer than 20 tokens. The network was trained in batches (size 40) for 30 epochs with the Adam optimizer (Kingma and Ba, 2014), starting with a learning rate of 10−4 . We apply dropout (p = 0.3) after computing the BiLSTM output. Finally, ReLu (Glorot et al., 2011) is used as an activation function in layer H. The system was implemented using the TensorFlow library (Abadi et al., 2016). 3.3 employed the state-of-the-art sentiment annotator by Socher"
W17-0908,N16-1098,0,0.445147,"oesn’t match the story context4 or where the wrong ending contradicts the story context5 . For some stories, the correct ending cannot be identified rationally, but rather according to commonly accepted moral values6 or based on the reader’s expectation of a positive mood in a story7 . Regarding sentiments, we generally noticed a bias towards stories with “happy endings”, i. e. stories where the sentiment expressed in the correct ending is more positive than for the wrong ending. We infer from these observations that an approach focusing exclusively on text understanding The Story Cloze test (Mostafazadeh et al., 2016) is a recent effort in providing a common test scenario for text understanding systems. As part of the LSDSem 2017 shared task, we present a system based on a deep learning architecture combined with a rich set of manually-crafted linguistic features. The system outperforms all known baselines for the task, suggesting that the chosen approach is promising. We additionally present two methods for generating further training data based on stories from the ROCStories corpus. Our system and generated data are publicly available on GitHub1 . 1 Introduction The goal of the Story Cloze test is to pro"
W17-0908,D14-1162,0,0.084556,"e concatenate ic , ie and if and feed them to the dense hidden layer H. The output of H is fed to the output layer, O. Afterwards, the softmax function is applied on the output of O to obtain a score representing the probability of ending e1 being correct. The same procedure is applied for ending e2 in place of ending e1 . Then, the highest-scoring ending is chosen as the correct ending of the story. Due to time constraints and system complexity we decided against hyper-parameter optimization and chose parameter values we deemed reasonable: We used pretrained 100-dimensional GloVe embeddings (Pennington et al., 2014)8 . Following previous work on using BiLSTMs in NLP tasks (Tan et al., 2015; Tan et al., 2016; dos Santos et al., 2016), we chose a dimensionality of 141 for the LSTM output vectors and hidden layer H. We employ zero padding for sentences longer than 20 tokens. The network was trained in batches (size 40) for 30 epochs with the Adam optimizer (Kingma and Ba, 2014), starting with a learning rate of 10−4 . We apply dropout (p = 0.3) after computing the BiLSTM output. Finally, ReLu (Glorot et al., 2011) is used as an activation function in layer H. The system was implemented using the TensorFlow"
W17-0908,W10-3504,0,\N,Missing
W17-2213,Q16-1022,0,0.0852854,"Missing"
W17-2213,W14-5302,1,0.904875,"Missing"
W17-2213,L16-1234,1,0.875346,"Missing"
W17-2213,P11-1061,0,0.174795,"Missing"
W17-2213,W14-5201,1,0.809288,"Missing"
W17-2213,P12-1018,0,0.0132619,"suffixes. As the main purpose of the stemmer is to reduce data sparsity rather provide a morphological analyzer the stemmer does not split prefixes or suffixes e.g. if a word has several suffixes the stemmer treat it as one suffix. Further on, the paper will refer to such complex affixes as “prefix” or “suffix”. First, the parallel German texts were stemmed. For this purpose, we used Snowball stemmer for German.4 Then, we split all the Hittite words into characters and word boundaries were marked with a special character. To create a character-based (CB) alignment we used Phrasal ITG Aligner (Neubig et al., 2012). Figure 2 shows a character alignment of Hittite phrase “KUR URU Mizri” to the German stems land and agypt.5 Both the Hittite noun KUR meaning land and the Hittite determiner URU are aligned to the German stem land while Mizri is aligned to the German stem agypt. This example shows the basic principals of how the stemmer works: Hittite substring aligned with German stems are likely to be stems themselves. It is particularly effective in Hittite because of the abundance of noun determiners that are frequently translated by a separate German word. The resulting CB alignment was processed as fol"
W17-2213,petrov-etal-2012-universal,0,0.128733,"or magic rituals, but also historic documents like treaties, annals, etc. have been found. As every genre is associated with genre-specific vocabulary and syntactic constructions, this genre variety can negatively affect the performance of the POS tagger. Furthermore, diachronic variations in spelling, morphology and syntax can have a negative impact on the tagging accuracy. The texts cover the whole of Hittite history, from OH 1 2.3 POS Annotation of Hittite In order to evaluate our pipeline, a hittitologist and co-author of this paper annotated selected documents with Universal POS tagset (Petrov et al., 2012). These were only used for the evaluation. As the pipeline was trained on a diachronic corpus containing various genres, we balanced the evaluation set and included texts that represent all the time periods. Table 1 shows the list of the texts included in the evaluation set. It totals 969 tokens and has proportionally balanced texts from NH, MH and OH. The complexity of the annotation process varied based on the period. While MH and NH are well-researched and there are many available texts in MH and NH, OH is very complicated and has words whose translation is not known. We decided to create a"
W17-2213,P03-1050,0,0.142877,"Missing"
W17-2213,rognvaldsson-etal-2012-icelandic,0,0.0629405,"Missing"
W17-2618,P10-2045,0,0.0119288,"p wake cycle” has no LU). In fact, such frames are used as meta-frames for abstraction purposes, thus, they exist only to participate in F2F relations with other frames (Ruppenhofer et al., 2006). In general, each frame pair is connected via only one F2F relation with occasional exceptions and the F2F relations situate the frames in semantic space (Ruppenhofer et al., 2006). F2F relations are used in the context of other tasks, such as text understanding (Fillmore and Baker, 2001), paraphrase rule generation for the system LexPar (Coyne and Rambow, 2009) and recognition of textual entailment (Aharon et al., 2010). Furthermore, F2F can be used as a form of commonsense knowledge (Rastogi and Van Durme, 2014). The incompleteness of the FN hierarchy is a known issue not only at the frame level (Rastogi and Van Durme, 2014; Pavlick et al., 2015; Hartmann and Gurevych, 2013) but also at the Automatic completion of frame-to-frame (F2F) relations in the FrameNet (FN) hierarchy has received little attention, although they incorporate meta-level commonsense knowledge and are used in downstream approaches. We address the problem of sparsely annotated F2F relations. First, we examine whether the manually defined"
W17-2618,bauer-etal-2012-dependency,0,0.054297,"Missing"
W17-2618,P16-1191,1,0.838438,"thod based on cosine distance to solve these analogy tasks. This assumes that relationships are expressed by vector offsets: given two word pairs (a, b) and (c, d), the question is to what extent the relations within the pairs are similar. We will apply this method to frame pairs that are connected via F2F relations in order to find out whether the frame embeddings incorporate F2F relations. There is an interest in abstracting away from word embeddings towards embeddings for more coarse grained units: Word2Vec is used to learn embeddings for senses (Iacobacci et al., 2015) or for supersenses (Flekova and Gurevych, 2016). Iacobacci (2015) use the CBOW model on texts annotated with BabelNet senses (Navigli and Ponzetto, 2012). Flekova (2016) use the skipgram model on texts with mapped WordNet supersenses (Miller, 1990; Fellbaum, 1990). For evaluation both works are oriented towards Mikolov’s (2013b) analogy tasks and perform qualitative analyses for the top k most similar embeddings for (super)senses or visualize the embeddings in vector space. To have text-based frame embeddings in line with related work, we will also use the Word2Vec algorithm to learn an additional version for frame embeddings. 2.2 3 Data T"
W17-2618,P13-1134,1,0.821623,"Missing"
W17-2618,E17-1045,1,0.793155,"Missing"
W17-2618,P14-1136,0,0.0229147,"ations can naturally emerge from text. We find that, concerning the methods we explored, the embeddings have difficulties in showing structures directly corresponding to F2F relations. 2) We transfer the relation prediction task from research on Knowledge Graph Completion to the 2 2.1 Related Work Frame Embeddings Frame embeddings for frame identification To our knowledge, the only approach learning frame embeddings is a matrix factorization approach in the context of the task of Frame Identification (FrameId), which is the first step in FN Semantic Role Labeling. The state-of-the-art system (Hermann et al., 2014) for FrameId projects frames and predicates with their context words into the same latent space by using the WSABIE algorithm (Weston et al., 2011). Two projection matrices (one for frames and one for predicates) are learned using WARP loss and gradient-based updates such that the distance between the predicate’s latent representation and that of the correct frame are minimized. Consequently, latent representations of frames will end up close to each other if they are evoked by similar predicates and context words. As the focus of such systems is on the FrameId task, the latent representations"
W17-2618,P15-1010,0,0.0279749,". Mikolov (2013b) suggest a vector offset method based on cosine distance to solve these analogy tasks. This assumes that relationships are expressed by vector offsets: given two word pairs (a, b) and (c, d), the question is to what extent the relations within the pairs are similar. We will apply this method to frame pairs that are connected via F2F relations in order to find out whether the frame embeddings incorporate F2F relations. There is an interest in abstracting away from word embeddings towards embeddings for more coarse grained units: Word2Vec is used to learn embeddings for senses (Iacobacci et al., 2015) or for supersenses (Flekova and Gurevych, 2016). Iacobacci (2015) use the CBOW model on texts annotated with BabelNet senses (Navigli and Ponzetto, 2012). Flekova (2016) use the skipgram model on texts with mapped WordNet supersenses (Miller, 1990; Fellbaum, 1990). For evaluation both works are oriented towards Mikolov’s (2013b) analogy tasks and perform qualitative analyses for the top k most similar embeddings for (super)senses or visualize the embeddings in vector space. To have text-based frame embeddings in line with related work, we will also use the Word2Vec algorithm to learn an addit"
W17-2618,P14-2050,0,0.0331546,"ce. 4 4.1 Methods WSABIE frame embeddings Concerning the matrix factorization approach for learning text-based frame embeddings, we use the code provided by (Hartmann et al., 2017) as it is publically available. It is leaned on Hermann’s (2014) description of their state-of-the-art system and achieves comparable results on FrameId. Our hyperparameter choices are oriented towards (Hartmann et al., 2017): embedding dimension 100, maximum number of negative samples: 100, epochs: 1000 and initial representation of predicate and context: concatenation of pretrained dependencybased word embeddings (Levy and Goldberg, 2014). Word2Vec frame embeddings Concerning the NN approach for learning text-based frame embeddings, we use the Word2Vec implementation ˇ uˇrek and Sojka, in the python library gensim (Reh˚ 2010). To obtain frame embeddings we follow the same steps as if we would learn word embeddings on FN sentences plus we replace all predicates with their frames. For instance, in the sequence “Officials claim that Iran has produced bombs” the predicates “claim” and “bombs” are replaced by “STATEMENT” and “WEAPON” respectively. This procedure corresponds to Flekova’s (2016) setup for learning supersense embeddin"
W17-2618,N15-1098,0,0.0391327,"Missing"
W17-2618,N13-1090,0,0.375061,"istance between the predicate’s latent representation and that of the correct frame are minimized. Consequently, latent representations of frames will end up close to each other if they are evoked by similar predicates and context words. As the focus of such systems is on the FrameId task, the latent representations of the frames are rather a sub-step contributing to FrameId but not studied further or applied to other tasks. We will extract these frame embeddings and explore them with respect to F2F relations. Word2Vec embeddings The Neural Network (NN) architecture of the Word2Vec algorithm (Mikolov et al., 2013a) learns word embeddings by either predicting a target word given its context words (CBOW model) or by predicting context words given their target word (skip-gram model). 147 eling relations as translations that operate on the embeddings of the entities. The model is formulated to minimize |h + r − t |for a training set, with randomly initialized embeddings. The function to minimize resembles the idea of the vector offset by Mikolov (2013b). Answer selection model Link Prediction is methodologically related to the key-task of Answer Selection from Question Answering (QA). The task is to rank"
W17-2618,P15-2067,0,0.0780085,"Missing"
W17-2618,W14-2901,0,0.0245606,"Missing"
W17-4510,W04-1013,0,0.465618,"imilarity score between the candidate summary and a pool of reference summaries previously written by human annotators (Lin, 2004; Yang et al., 2016; Ng and Abrecht, 2015). Some variants rely only on the source documents and the candidate summary ignoring the reference summaries (Louis and Nenkova, 2013; Steinberger and Jeˇzek, 2012). In order to select the best automatic metric, we typically consider manual evalution metrics as our gold standard, then a good automatic metric should reliably predict how well a summarizer would perform if human evaluation was conducted (Owczarzak et al., 2012; Lin, 2004; Rankel et al., 2013). In practice, we use the human judgment datasets like the ones constructed during the manual evaluation of the Text Analysis Conference (TAC). The system summaries submitted to the shared tasks were manually scored by trained human annotators following the Responsiveness and/or the Pyramid schemes. An automatic metric is considered good if it ranks the system summaries similarly as humans did. Currently, ROUGE (Lin, 2004) is the accepted standard for automatic evaluation of content selection because of its simplicity and its good correlation with human judgments. However"
W17-4510,P98-1013,0,0.0587346,"ent tool. We now briefly present the selected features. JSref (s, θi ) = 1 X JS(s, ref ) |θi | (4) ref ∈θi ROUGE-WE (Ng and Abrecht, 2015) is the variant of ROUGE-N replacing the hard lexical matching by a soft matching based on the cosine similarity of word embeddings. We use ROUGEWE-1 and ROUGE-WE-2 as part of our features. FrameNet-based metrics ROUGE-WE proposes a statistical approach (word embeddings) to alleviate the hard lexical matching of ROUGE. We also include a linguistically motivated one. We replace all nouns and verbs of the reference and candidate summaries with their FrameNet (Baker et al., 1998) frames. This frame annotation is done with the best-performing system configuration from Hartmann et al. (2017) pre-trained on all FrameNet data. It assigns a frame to a word based on the word itself and the surrounding context in the sentence. Frames are more abstract than words, thus different but related words might be associated with the same frames depending on the meaning of the words in the respective context. ROUGE-N can now match related words through their frames. We also use the unigram and bigram variants (FrameN). Semantic Vector Space Similarities In general, automatic evaluatio"
W17-4510,N06-1059,0,0.644625,"h aimed at creating strong automatic metrics by automating the Pyramid scoring scheme (Harnly et al., 2005). Yang et al. (2016) proposed PEAK, a metric where the components requiring human input in the original Pyramid annotation scheme are replaced by stateof-the-art NLP tools. It is more semantically motivated than ROUGE and approximates correctly the manual Pyramid scores but it is computationally expensive making it difficult to use in practice. Some other metrics do not make use of the reference summaries, they compute a score based only on the candidate summary and the source documents (Lin et al., 2006; Louis and Nenkova, 2013). One representative of this class is the Jensen Shannon (JS) divergence, an informationtheoretic measure comparing system summaries and source documents with their underlying probability distributions of n-grams. JS divergence is simply the symmetric version of the well-known Kullback-Leibler (KL) divergence (Haghighi and Vanderwende, 2009). Little work has been done on the topic of learning an evaluation metric. Conroy and Dang (2008) previously investigated the performances of ROUGE metrics in comparison with human judgments and proposed ROSE (ROUGE Optimal Summari"
W17-4510,C16-1099,1,0.885819,"Missing"
W17-4510,J13-2002,0,0.463135,"Peyrard and Teresa Botschen and Iryna Gurevych Research Training Group AIPHES and UKP Lab Computer Science Department, Technische Universit¨at Darmstadt www.aiphes.tu-darmstadt.de, www.ukp.tu-darmstadt.de Abstract was dedicated to the study of automatic evaluation metrics. Automatic metrics aim to produce a semantic similarity score between the candidate summary and a pool of reference summaries previously written by human annotators (Lin, 2004; Yang et al., 2016; Ng and Abrecht, 2015). Some variants rely only on the source documents and the candidate summary ignoring the reference summaries (Louis and Nenkova, 2013; Steinberger and Jeˇzek, 2012). In order to select the best automatic metric, we typically consider manual evalution metrics as our gold standard, then a good automatic metric should reliably predict how well a summarizer would perform if human evaluation was conducted (Owczarzak et al., 2012; Lin, 2004; Rankel et al., 2013). In practice, we use the human judgment datasets like the ones constructed during the manual evaluation of the Text Analysis Conference (TAC). The system summaries submitted to the shared tasks were manually scored by trained human annotators following the Responsiveness"
W17-4510,C08-1019,0,0.0336777,"ther metrics do not make use of the reference summaries, they compute a score based only on the candidate summary and the source documents (Lin et al., 2006; Louis and Nenkova, 2013). One representative of this class is the Jensen Shannon (JS) divergence, an informationtheoretic measure comparing system summaries and source documents with their underlying probability distributions of n-grams. JS divergence is simply the symmetric version of the well-known Kullback-Leibler (KL) divergence (Haghighi and Vanderwende, 2009). Little work has been done on the topic of learning an evaluation metric. Conroy and Dang (2008) previously investigated the performances of ROUGE metrics in comparison with human judgments and proposed ROSE (ROUGE Optimal Summarization Evaluation) a linear combination of ROUGE metrics to maximize correlation with human responsiveness. We also look for a combination of features which correlates well with human judgements but, in contrast to Conroy and Dang (2008), we include a wider set of metrics: ROUGE scores, other evaluation metrics (like Jensen-Shannon divergence) and features typically used by summarization systems. Related Work Hirao et al. (2007) also proposed a related approach."
W17-4510,W09-1802,0,0.244634,"n K are lists of size n. The scores for the summaries of the l-th summarizer are aggregated to form the l-th element of the lists. The correlation is computed on the two aggregated sys lists. Therefore, Kavg only indicates whether the evaluation metrics can rank systems correctly after aggregation of many summary scores but it ignores individual summaries. It has been used before because evaluation metrics were initially tasked to compare systems. 76 candidate summary and the reference summaries. If θi is the set of reference summaries for the i-th topic, then we compute the following score: (Gillick and Favre, 2009; Haghighi and Vanderwende, 2009). Optimization-based systems have recently become popular (McDonald, 2007). Such features score the candidate summary based only on the document sources and the summary itself. The last category contains the metrics producing a score based only on the summary. Examples of such metrics include readability or redundancy. Clearly, features using reference summaries (existing automatic metrics) are expected to be more useful for our task. However, it has been shown that some metrics of the second category (like JS divergence) also contain useful signal to approxima"
W17-4510,N09-1041,0,0.215616,"e manual Pyramid scores but it is computationally expensive making it difficult to use in practice. Some other metrics do not make use of the reference summaries, they compute a score based only on the candidate summary and the source documents (Lin et al., 2006; Louis and Nenkova, 2013). One representative of this class is the Jensen Shannon (JS) divergence, an informationtheoretic measure comparing system summaries and source documents with their underlying probability distributions of n-grams. JS divergence is simply the symmetric version of the well-known Kullback-Leibler (KL) divergence (Haghighi and Vanderwende, 2009). Little work has been done on the topic of learning an evaluation metric. Conroy and Dang (2008) previously investigated the performances of ROUGE metrics in comparison with human judgments and proposed ROSE (ROUGE Optimal Summarization Evaluation) a linear combination of ROUGE metrics to maximize correlation with human responsiveness. We also look for a combination of features which correlates well with human judgements but, in contrast to Conroy and Dang (2008), we include a wider set of metrics: ROUGE scores, other evaluation metrics (like Jensen-Shannon divergence) and features typically"
W17-4510,D15-1222,0,0.226803,"es on a LIKERT scale ranging from 1 to 5. Later, the Pyramid scheme was introduced to evaluate content selection with high inter-annotator agreement (Nenkova et al., 2007). Manual evalations are meaningful and reliable but are also expensive and not reproducible. This makes them unfit for systematic comparison. Due to the necessity of having cheap and reproducible metrics, a significant body of research 74 Proceedings of the Workshop on New Frontiers in Summarization, pages 74–84 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics system-level. Afterwards, Ng and Abrecht (2015) extended ROUGE with word embeddings. Instead of hard lexical matching of n-grams, ROUGE-WE uses soft matching based on the cosine similarity of word embedding. candidate metric and human judgments for each topic indivually and then average these correlations over topics. In this scenario, which we call summary-level correlation analysis, the performance of ROUGE significantly drops meaning that on average ROUGE does not really identify summary quality, it can only rank systems after aggregation of many topics. In order to advance the field of summarization we need to have more consistent metr"
W17-4510,E17-1045,1,0.779752,"UGE-WE (Ng and Abrecht, 2015) is the variant of ROUGE-N replacing the hard lexical matching by a soft matching based on the cosine similarity of word embeddings. We use ROUGEWE-1 and ROUGE-WE-2 as part of our features. FrameNet-based metrics ROUGE-WE proposes a statistical approach (word embeddings) to alleviate the hard lexical matching of ROUGE. We also include a linguistically motivated one. We replace all nouns and verbs of the reference and candidate summaries with their FrameNet (Baker et al., 1998) frames. This frame annotation is done with the best-performing system configuration from Hartmann et al. (2017) pre-trained on all FrameNet data. It assigns a frame to a word based on the word itself and the surrounding context in the sentence. Frames are more abstract than words, thus different but related words might be associated with the same frames depending on the meaning of the words in the respective context. ROUGE-N can now match related words through their frames. We also use the unigram and bigram variants (FrameN). Semantic Vector Space Similarities In general, automatic evaluation metrics comparing system summaries with reference summaries propose a kind of semantic similarity between summ"
W17-4510,W12-2601,0,0.228652,"to produce a semantic similarity score between the candidate summary and a pool of reference summaries previously written by human annotators (Lin, 2004; Yang et al., 2016; Ng and Abrecht, 2015). Some variants rely only on the source documents and the candidate summary ignoring the reference summaries (Louis and Nenkova, 2013; Steinberger and Jeˇzek, 2012). In order to select the best automatic metric, we typically consider manual evalution metrics as our gold standard, then a good automatic metric should reliably predict how well a summarizer would perform if human evaluation was conducted (Owczarzak et al., 2012; Lin, 2004; Rankel et al., 2013). In practice, we use the human judgment datasets like the ones constructed during the manual evaluation of the Text Analysis Conference (TAC). The system summaries submitted to the shared tasks were manually scored by trained human annotators following the Responsiveness and/or the Pyramid schemes. An automatic metric is considered good if it ranks the system summaries similarly as humans did. Currently, ROUGE (Lin, 2004) is the accepted standard for automatic evaluation of content selection because of its simplicity and its good correlation with human judgmen"
W17-4510,P14-2050,0,0.0215766,"ded in this analysis and serve as baselines. Identifying which metrics have high correlation with human judgments constitutes an initial feature analysis. Most of the features do not need language dependent information, except those requiring word embeddings or frame identification based on a frame inventory. We do not include the frame identification features when experimenting with the German DBS-corpus. However, for the other language dependent features, we used the German word embeddings developed by Reimers et al. (2014). For the English datasets, we use dependency-based word embeddings (Levy and Goldberg, 2014). The performances of the baselines on TAC2008 and TAC-2009 are displayed in Table 1, and Table 2 depicts scores for the DBS-corpus. In order to have an insightful view, we report the scores for the three correlation metrics presented in the previous section: Pearson’s r, Spearman’s ρ and Ndcg. Our Models For each dataset, we trained two models. The first model (Sf3ull for Supervised Summarization Scorer) uses all the available features for training. However, the previous feature analysis revealed that some features are poor. We hypothesized that they might harm the learning process. Therefore"
W17-4510,C16-1024,1,0.926737,"ole scoring spectrum. To summarize our contributions: We performed a summary-level correlation analysis to compare a large set of existing evaluation metrics. We learned a new evaluation metric as a combination of existing ones to maximize the summary-level correlation with human judgments. We conducted a manual evaluation to test whether learning from available human judgment datasets yields a reliable metric accross its whole scoring spectrum. 2 Recently, a line of research aimed at creating strong automatic metrics by automating the Pyramid scoring scheme (Harnly et al., 2005). Yang et al. (2016) proposed PEAK, a metric where the components requiring human input in the original Pyramid annotation scheme are replaced by stateof-the-art NLP tools. It is more semantically motivated than ROUGE and approximates correctly the manual Pyramid scores but it is computationally expensive making it difficult to use in practice. Some other metrics do not make use of the reference summaries, they compute a score based only on the candidate summary and the source documents (Lin et al., 2006; Louis and Nenkova, 2013). One representative of this class is the Jensen Shannon (JS) divergence, an informat"
W17-4510,P17-2005,1,0.786607,"ub.com/UKPLab/ coling2016-genetic-swarm-MDS 81 r human judgment datasets are somehow limited. While it is possible to learn a reliable combination of existing metrics, one would need better and bigger human judgment datasets to really get strong improvements. In particular, it is important to extend the coverage of these datasets because we rely on them to compare evaluation metrics. These annotations are the key to understand what humans consider to be good summaries. Statistical analysis on such datasets will likely be beneficial to develop both evaluation metrics and summarization systems (Peyrard and Eckle-Kohler, 2017). The metric was evaluated on English news datasets and on a German dataset of heterogeneous sources but a wider study might be needed in order to measure the generalization of the learned metric to other datasets and domains. Such generalization capabilities would be interesting because one would not need to re-train a new metric for every domain. We believe it is important to develop evaluation metrics correlating well with human judgments at the summary-level. This gives a more insightful and reliable metric. If the metric is reliable enough, one can use it as a target to train supervised s"
W17-4510,P13-2024,0,0.152807,"core between the candidate summary and a pool of reference summaries previously written by human annotators (Lin, 2004; Yang et al., 2016; Ng and Abrecht, 2015). Some variants rely only on the source documents and the candidate summary ignoring the reference summaries (Louis and Nenkova, 2013; Steinberger and Jeˇzek, 2012). In order to select the best automatic metric, we typically consider manual evalution metrics as our gold standard, then a good automatic metric should reliably predict how well a summarizer would perform if human evaluation was conducted (Owczarzak et al., 2012; Lin, 2004; Rankel et al., 2013). In practice, we use the human judgment datasets like the ones constructed during the manual evaluation of the Text Analysis Conference (TAC). The system summaries submitted to the shared tasks were manually scored by trained human annotators following the Responsiveness and/or the Pyramid schemes. An automatic metric is considered good if it ranks the system summaries similarly as humans did. Currently, ROUGE (Lin, 2004) is the accepted standard for automatic evaluation of content selection because of its simplicity and its good correlation with human judgments. However, previous works on ev"
W17-4510,E12-1023,0,0.0217472,"s and on a German dataset of heterogeneous sources but a wider study might be needed in order to measure the generalization of the learned metric to other datasets and domains. Such generalization capabilities would be interesting because one would not need to re-train a new metric for every domain. We believe it is important to develop evaluation metrics correlating well with human judgments at the summary-level. This gives a more insightful and reliable metric. If the metric is reliable enough, one can use it as a target to train supervised summarization systems (Takamura and Okumura, 2010; Sipos et al., 2012) and approach summarization as a principled machine learning task. Responsiveness ρ Ndcg Best baseline .6945 .6701 .9210 Sf3ull 3 Sbest .7198 .7318 .6818 .6936 .9323 .9355 Table 4: Correlation of automatic metrics with hu3 . man accross the whole scoring spectrum of Sbest guidelines used during DUC and TAC for assessing responsiveness. To select the summaries, we 3 ranked them according to their Sbest scores and for a population of 100 we picked 10 evenly spaced summaries (the first, the tenth and so on). We observe an inter-annotator agreement of 0.74 Cohen’s κ. The results are displayed in T"
W17-4510,C98-1013,0,\N,Missing
W17-6909,W09-1206,0,0.0606452,"Missing"
W17-6909,W08-1301,0,0.136894,"Missing"
W17-6909,W14-5201,1,0.88223,"Missing"
W17-6909,D17-1320,1,0.537363,"Missing"
W17-6909,D17-2004,1,0.302587,"Missing"
W17-6909,W11-1414,0,0.45573,"et al., 2010; Zubrinic et al., 2012). Relation Extraction During relation extraction, phrases describing the relationship between pairs of concept candidates have to be identified in the text. Following their concept extraction approach, Valerio and Leake (2006) identify verb phrases that connect two concept candidates, whereas Villalon (2012) uses tokens on the shortest path between the candidates in a simplified dependency graph. A patternbased extraction from dependencies and part-of-speech sequences was also suggested (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002). Olney et al. (2011) use the output of an SRL system and are thus close to this work. However, they focus on a variant of the task where relations are restricted to a fixed domain-specific set of 30 labels, creating less expressive concept maps. In the next section, we show how the definition of these patterns for both concept and relation extraction can be made obsolete by leveraging existing predicate-argument analysis tools. 4 Concept and Relation Extraction from Predicate-Argument Structures The intuition for using predicate-argument structures is that a proposition formed by a relation and its concepts in a"
W17-6909,J05-1004,0,0.101354,"nected argument nodes. We remove unary predicates and break down higher-arity predicates by creating all possible pairs except if they have the same edge label (e.g. two objects). Thus, we obtain the following binary predicates for (3): 1. invented (Joseph Novak; concept maps) 2. represent (concept maps; concepts) 3. represent (concept maps; their relationships) SRL As the third method, we apply semantic role labeling using Mate Tools (Bj¨orkelund et al., 2009), which is freely available and was one of the best in the CoNLL 2009 shared task (Hajiˇc et al., 2009). We prefer PropBank-style SRL (Palmer et al., 2005) over FrameNet and VerbNet because of its robustness and maturity. In a sentence, it marks verbs with their PropBank frame, identifies subtrees in the dependency representation of the sentence as arguments and labels them with a role: nsubj amod rcmod nsubjpass auxpass prep pobj nn conj dobj cc poss Concept maps, which were invented by Joseph Novak, represent concepts and their relationships. invent.01 represent.01 relationship.01 R-A1 A1 A0 A1 A0 A0 This approach is the most sophisticated type of analysis in our study, trying to map a natural language sentence to another layer of abstract sem"
W17-6909,L16-1294,0,0.0159164,"ntly high number of tokens as the input for a single concept map. For the second dataset, B IOLOGY, we followed Olney et al. (2011) and used a collection of 464 concept maps produced by experts as teaching materials for biology.2 The maps were created independently from a text. We matched them automatically with corresponding articles in Wikipedia and manually corrected wrong assignments. Every map is a star-like graph centered around a central concept, e.g. protein, and hence has a similar topical focus as an encyclopedic article. As the third dataset, ACL, we used the ACL RD-TEC 2.0 corpus (QasemiZadeh and Schumann, 2016). It consists of 300 abstracts taken from papers in the ACL Anthology in which two annotators marked terms with a specialized meaning. As abstracts are usually good summaries of a paper, these terms tend to be the central concepts discussed in the papers. We used Apache Tika3 to extract the full texts, excluding the abstracts, from the PDF version of the corresponding papers. These texts were then used with the annotated concepts as the gold concepts. Note that we cannot use this corpus to evaluate relation extraction, as such annotations are not available. Table 1 compares the introduced data"
W17-6909,P16-1119,0,0.0336296,"Missing"
W17-6909,D16-1252,0,0.266711,"Missing"
W17-6909,P16-2077,0,0.171298,"nvented(Joseph Novak; concept maps) is a semantic representation for both (1) and (2), requiring no separate handling of the cases. Using such a unified representation based on predicates and arguments, concept map mining approaches no longer need to carefully define large sets of patterns, but can instead make use of existing semantic analysis tools. In this work, we analyze three different representations that improve upon pure syntactic representations by different degrees: Open Information Extraction (OpenIE) (Banko et al., 2007), identifying predicates and arguments in a sentence, PropS (Stanovsky et al., 2016), which additionally unifies a certain amount of syntactic variations, and Semantic Role Labeling (SRL) (Hajiˇc et al., 2009), mapping predicates and arguments to an abstract, semantic representation. We describe how these representations can be used and present several experiments comparing them to previous work. As a result, we find that the suggested methods are competitive or even better while requiring no patterns to be defined. This finding has the potential to drastically simplify the development of concept map mining systems in the future, because they can rely on readily available too"
W17-6927,E17-1104,0,0.0510913,"Missing"
W17-6927,W03-1210,0,0.0911189,"rpus (Hidey and McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of propositional causality in Japanese. The system incorporates background knowledge for enhancing the learning process through the use of multi-column convolutional neural networks. Regarding the classification of explicit causality, Khoo et al. (1998) proposed a rule-based system grounded in regular expressions for the classification of explicit causal relations, whereas Mirza and Tonelli (2016) pre"
W17-6927,P16-1135,0,0.345848,"of their disambiguation. Due to the lack of unambiguous linguistic construction of causality, we claim that the use of linguistic features may restrict the representation of causality meaning. Also, the use of dense vector spaces, roughly speaking word embeddings, can provide a better representation of causality, and can improve the disambiguation of the causal meaning of lexical markers. Hence, we propose a neural network architecture with two inputs as sequences of word embeddings, encoding the left and the right context of the lexical marker. We evaluate our proposal on the AltLex corpus (Hidey and McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju ("
W17-6927,K16-1006,1,0.831429,"the inputs (see Equation 1). For each word, its corresponding word vector of 300 com- Figure 1: Neural model, where e1 is ponents (d) was looked up in the 840b cased Glove embeddings the first event, l is the lexical marker (Pennington et al., 2014). Subsequently, the concatenated word and e2 is the second event. embeddings get passed through an encoding Long Short-Term 1 1,1 1,2 1,3 2 1,t 2,1 2,2 2,3 1 1 2,t 2 2 Memory (LSTM) recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) layer. We decided to use LSTM because of its ability to encode sequential and contextual information (Melamud et al., 2016). We assume some sort of relation exists between e1 and e2 , so we first evaluated the performance of the connection of the two LSTM layers through the initialization of the second LSTM with the end state of the first one (dashed arrow in Figure 1). We call this model “Stated Pair LSTM”. We assessed the same model but without the connection of the two LSTMs for evaluating our assumption. We call it “Pair LSTM” (no dashed arrow in Figure 1). The two outputs of the encoding layer are transformed to a vector of length 100 by a dense layer with a tanh activation function. The context of the causal"
W17-6927,C16-1007,0,0.126855,"s. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of propositional causality in Japanese. The system incorporates background knowledge for enhancing the learning process through the use of multi-column convolutional neural networks. Regarding the classification of explicit causality, Khoo et al. (1998) proposed a rule-based system grounded in regular expressions for the classification of explicit causal relations, whereas Mirza and Tonelli (2016) presented a supervised system based on the use of lexical, syntactic and semantic features from WordNet. The proposal of Bethard and Martin (2008) is similar to that of Mirza and Tonelli (2016), but it was focused only on conjunction constructions, namely conjoined events. The three approaches suffer from the ambiguity of the lexical markers, the limited coverage of the linguistic resources and the constraint to a specific syntactic construction. In contrast, our proposal tries to cover lexical and propositional causality independently of whether it is explicit or implicit, and we do not rest"
W17-6927,D14-1162,0,0.0802783,"o in... ... puts. The lengths (n, m) of the instances of each input are not Embed. Lookup Embed. Lookup necessarily the same, so in order to make their lengths equal, Token. & w ... w ... w w w w w w Padding: three zero-padding strategies were assessed, namely the maxie e Input: l * mum, the mean and the mode of the lengths (t) of the compo|input |= n |input |= m nents of the inputs (see Equation 1). For each word, its corresponding word vector of 300 com- Figure 1: Neural model, where e1 is ponents (d) was looked up in the 840b cased Glove embeddings the first event, l is the lexical marker (Pennington et al., 2014). Subsequently, the concatenated word and e2 is the second event. embeddings get passed through an encoding Long Short-Term 1 1,1 1,2 1,3 2 1,t 2,1 2,2 2,3 1 1 2,t 2 2 Memory (LSTM) recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) layer. We decided to use LSTM because of its ability to encode sequential and contextual information (Melamud et al., 2016). We assume some sort of relation exists between e1 and e2 , so we first evaluated the performance of the connection of the two LSTM layers through the initialization of the second LSTM with the end state of the first one (dashed"
W17-6927,prasad-etal-2008-penn,0,0.0817483,". Also, the use of dense vector spaces, roughly speaking word embeddings, can provide a better representation of causality, and can improve the disambiguation of the causal meaning of lexical markers. Hence, we propose a neural network architecture with two inputs as sequences of word embeddings, encoding the left and the right context of the lexical marker. We evaluate our proposal on the AltLex corpus (Hidey and McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of proposi"
W17-6927,W13-4004,0,0.177091,"d McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of propositional causality in Japanese. The system incorporates background knowledge for enhancing the learning process through the use of multi-column convolutional neural networks. Regarding the classification of explicit causality, Khoo et al. (1998) proposed a rule-based system grounded in regular expressions for the classification of explicit causal relations, whereas Mirza and Tonelli (2016) presented a supervised sy"
W17-6927,W14-4322,0,0.0256738,"Missing"
W17-6927,P08-2045,0,\N,Missing
W17-6935,N10-1145,0,0.0335086,"sults and significantly outperforms various strong baselines. An additional evaluation on the factoid QA dataset WikiQA demonstrates that our approach is well-suited for other scenarios that deal with shorter texts. In general, we show that it is possible to perform effective importance weighting in non-factoid answer selection without relying on the relatedness of questions and candidate answers. 2 Related Work Earlier work in answer selection relies on handcrafted features based on semantic role annotations (Shen and Lapata, 2007; Surdeanu et al., 2011), parse trees (Wang and Manning, 2010; Heilman and Smith, 2010), tree kernels (Moschitti et al., 2007; Severyn and Moschitti, 2012), discourse structures (Jansen et al., 2014), and external resources (Yih et al., 2013). More recently, researchers started using deep neural networks for answer selection. Yu et al. (2014), for example, propose a convolutional bigram model to classify a candidate answer as correct or incorrect. Similar but more enhanced, Severyn and Moschitti (2015) use a CNN with additional dense layers to capture interactions between questions and candidate answers, a model that is also part of a combined approach with tree kernels (Tymoshe"
W17-6935,P14-1092,0,0.0189228,"iQA demonstrates that our approach is well-suited for other scenarios that deal with shorter texts. In general, we show that it is possible to perform effective importance weighting in non-factoid answer selection without relying on the relatedness of questions and candidate answers. 2 Related Work Earlier work in answer selection relies on handcrafted features based on semantic role annotations (Shen and Lapata, 2007; Surdeanu et al., 2011), parse trees (Wang and Manning, 2010; Heilman and Smith, 2010), tree kernels (Moschitti et al., 2007; Severyn and Moschitti, 2012), discourse structures (Jansen et al., 2014), and external resources (Yih et al., 2013). More recently, researchers started using deep neural networks for answer selection. Yu et al. (2014), for example, propose a convolutional bigram model to classify a candidate answer as correct or incorrect. Similar but more enhanced, Severyn and Moschitti (2015) use a CNN with additional dense layers to capture interactions between questions and candidate answers, a model that is also part of a combined approach with tree kernels (Tymoshenko et al., 2016). And Wang and Nyberg (2015) incorporate stacked BiLSTMs to learn a joint feature vector of a q"
W17-6935,P07-1098,0,0.0471649,"ous strong baselines. An additional evaluation on the factoid QA dataset WikiQA demonstrates that our approach is well-suited for other scenarios that deal with shorter texts. In general, we show that it is possible to perform effective importance weighting in non-factoid answer selection without relying on the relatedness of questions and candidate answers. 2 Related Work Earlier work in answer selection relies on handcrafted features based on semantic role annotations (Shen and Lapata, 2007; Surdeanu et al., 2011), parse trees (Wang and Manning, 2010; Heilman and Smith, 2010), tree kernels (Moschitti et al., 2007; Severyn and Moschitti, 2012), discourse structures (Jansen et al., 2014), and external resources (Yih et al., 2013). More recently, researchers started using deep neural networks for answer selection. Yu et al. (2014), for example, propose a convolutional bigram model to classify a candidate answer as correct or incorrect. Similar but more enhanced, Severyn and Moschitti (2015) use a CNN with additional dense layers to capture interactions between questions and candidate answers, a model that is also part of a combined approach with tree kernels (Tymoshenko et al., 2016). And Wang and Nyberg"
W17-6935,D16-1244,0,0.0637892,"Missing"
W17-6935,D14-1162,0,0.0971293,"BiLSTM , but use a different network structure. Neural Network Setup We performed grid search over several hyperparameter combinations and found the optimal choices to be similar to hyperparameters of previous work. The cell size of all LSTMs is 141 (each direction), and the number of filters for all CNNs is 400 with size 3. The only exception is CNN +BiLSTM with 282 filters and a cell size of 282. We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 4 · 10−4 and a margin m = 0.2. We initialize the word embeddings with off-the-shelf 100-dimensional uncased GloVe embeddings (Pennington et al., 2014) and optimize them further during training. Dropout of 0.3 was applied on the representations before comparison. We chose different hyperparameters for WikiQA, which we do not list here due to space restrictions. Details can be found in our public source code repository. 5 Experimental Results InsuranceQA v1 Our evaluation on InsuranceQA v1 allows us to compare our approach against a broad list of recently published attention-based models. Table 2 shows the results of our evaluation where we measure the ratio of correctly selected answers (accuracy). We observe that by adding LW to either CNN"
W17-6935,P17-4004,1,0.877596,"Missing"
W17-6935,D07-1002,0,0.0594042,"representation learning approaches with attention, our approach achieves the best results and significantly outperforms various strong baselines. An additional evaluation on the factoid QA dataset WikiQA demonstrates that our approach is well-suited for other scenarios that deal with shorter texts. In general, we show that it is possible to perform effective importance weighting in non-factoid answer selection without relying on the relatedness of questions and candidate answers. 2 Related Work Earlier work in answer selection relies on handcrafted features based on semantic role annotations (Shen and Lapata, 2007; Surdeanu et al., 2011), parse trees (Wang and Manning, 2010; Heilman and Smith, 2010), tree kernels (Moschitti et al., 2007; Severyn and Moschitti, 2012), discourse structures (Jansen et al., 2014), and external resources (Yih et al., 2013). More recently, researchers started using deep neural networks for answer selection. Yu et al. (2014), for example, propose a convolutional bigram model to classify a candidate answer as correct or incorrect. Similar but more enhanced, Severyn and Moschitti (2015) use a CNN with additional dense layers to capture interactions between questions and candida"
W17-6935,J11-2003,0,0.0309588,"approaches with attention, our approach achieves the best results and significantly outperforms various strong baselines. An additional evaluation on the factoid QA dataset WikiQA demonstrates that our approach is well-suited for other scenarios that deal with shorter texts. In general, we show that it is possible to perform effective importance weighting in non-factoid answer selection without relying on the relatedness of questions and candidate answers. 2 Related Work Earlier work in answer selection relies on handcrafted features based on semantic role annotations (Shen and Lapata, 2007; Surdeanu et al., 2011), parse trees (Wang and Manning, 2010; Heilman and Smith, 2010), tree kernels (Moschitti et al., 2007; Severyn and Moschitti, 2012), discourse structures (Jansen et al., 2014), and external resources (Yih et al., 2013). More recently, researchers started using deep neural networks for answer selection. Yu et al. (2014), for example, propose a convolutional bigram model to classify a candidate answer as correct or incorrect. Similar but more enhanced, Severyn and Moschitti (2015) use a CNN with additional dense layers to capture interactions between questions and candidate answers, a model that"
W17-6935,P16-1044,0,0.416205,"complex answer texts (e.g. descriptions, opinions, or explanations). Answer selection for non-factoid QA is especially difficult because we usually deal with user-generated content, for example questions and answers extracted from community question answering platforms or FAQ websites. As a consequence, candidate answers are complex multi-sentence texts with detailed information. Two examples are shown in Figures 2 and 3. To deal with this challenge, recent approaches employ attention-based neural networks to focus on segments within the candidate answer that are most related to the question (Tan et al., 2016; Wang et al., 2016). For scoring, dense vector representations of the question and the candidate answer are learned and the distance between the vectors is measured. With attention-based models, segments with a stronger focus are treated as more important and have more influence on the resulting representations. Using the relatedness between a candidate answer and the question to determine the importance is intuitive for correct candidate answers because the most important segments of both texts are expected to be strongly related. However, we also deal with a large number of incorrect candid"
W17-6935,N16-1152,0,0.0113953,", 2010), tree kernels (Moschitti et al., 2007; Severyn and Moschitti, 2012), discourse structures (Jansen et al., 2014), and external resources (Yih et al., 2013). More recently, researchers started using deep neural networks for answer selection. Yu et al. (2014), for example, propose a convolutional bigram model to classify a candidate answer as correct or incorrect. Similar but more enhanced, Severyn and Moschitti (2015) use a CNN with additional dense layers to capture interactions between questions and candidate answers, a model that is also part of a combined approach with tree kernels (Tymoshenko et al., 2016). And Wang and Nyberg (2015) incorporate stacked BiLSTMs to learn a joint feature vector of a question and a candidate answer for classification. Answer selection can also be formulated as a ranking task where we learn dense vector representations of questions and candidate answers and measure the distance between them for scoring. Feng et al. (2015) use such an approach and compare different models based on CNN with different similarity measures. Based on that, models with attention mechanisms were proposed. Tan et al. (2016) apply an attentive BiLSTM component that performs importance weight"
W17-6935,P16-1122,0,0.601298,"ts (e.g. descriptions, opinions, or explanations). Answer selection for non-factoid QA is especially difficult because we usually deal with user-generated content, for example questions and answers extracted from community question answering platforms or FAQ websites. As a consequence, candidate answers are complex multi-sentence texts with detailed information. Two examples are shown in Figures 2 and 3. To deal with this challenge, recent approaches employ attention-based neural networks to focus on segments within the candidate answer that are most related to the question (Tan et al., 2016; Wang et al., 2016). For scoring, dense vector representations of the question and the candidate answer are learned and the distance between the vectors is measured. With attention-based models, segments with a stronger focus are treated as more important and have more influence on the resulting representations. Using the relatedness between a candidate answer and the question to determine the importance is intuitive for correct candidate answers because the most important segments of both texts are expected to be strongly related. However, we also deal with a large number of incorrect candidate answers where th"
W17-6935,P15-2116,0,0.277635,"show the effectiveness of our approach, which outperforms several state-of-the-art attention-based models on the recent non-factoid answer selection datasets InsuranceQA v1 and v2. We show that it is possible to perform effective importance weighting for answer selection without relying on the relatedness of questions and answers. The source code of our experiments is publicly available.1 1 Introduction Answer selection is an important subtask of question answering (QA) that enables choosing one final answer from a list of candidate answers in regard to the input question (Feng et al., 2015; Wang and Nyberg, 2015). QA itself can be divided into factoid QA, which enables the retrieval of facts, and nonfactoid QA, which enables finding of complex answer texts (e.g. descriptions, opinions, or explanations). Answer selection for non-factoid QA is especially difficult because we usually deal with user-generated content, for example questions and answers extracted from community question answering platforms or FAQ websites. As a consequence, candidate answers are complex multi-sentence texts with detailed information. Two examples are shown in Figures 2 and 3. To deal with this challenge, recent approaches e"
W17-6935,C10-1131,0,0.0318934,"ach achieves the best results and significantly outperforms various strong baselines. An additional evaluation on the factoid QA dataset WikiQA demonstrates that our approach is well-suited for other scenarios that deal with shorter texts. In general, we show that it is possible to perform effective importance weighting in non-factoid answer selection without relying on the relatedness of questions and candidate answers. 2 Related Work Earlier work in answer selection relies on handcrafted features based on semantic role annotations (Shen and Lapata, 2007; Surdeanu et al., 2011), parse trees (Wang and Manning, 2010; Heilman and Smith, 2010), tree kernels (Moschitti et al., 2007; Severyn and Moschitti, 2012), discourse structures (Jansen et al., 2014), and external resources (Yih et al., 2013). More recently, researchers started using deep neural networks for answer selection. Yu et al. (2014), for example, propose a convolutional bigram model to classify a candidate answer as correct or incorrect. Similar but more enhanced, Severyn and Moschitti (2015) use a CNN with additional dense layers to capture interactions between questions and candidate answers, a model that is also part of a combined approach"
W17-6935,D15-1237,0,0.0178786,"d from the same community question answering website, they model different setups due to a different sampling strategy that was used to create the candidate answer pools. Whereas in InsuranceQA v1 the pools were created randomly (plus the correct answers), the pools in InsuranceQA v2 were created by querying a search engine to retrieve candidate answers that are lexically similar to the question.2 In addition, we also test our approaches on the factoid answer selection dataset WikiQA, which was constructed by means of crowd-sourcing through the extraction of sentences from Wikipedia articles (Yang et al., 2015). We use this dataset to test our models within the different scenario of factoid answer selection that deals with significantly shorter texts. The dataset statistics are listed in Table 1. 2 Since the correct answers were not separately inserted in InsuranceQA v2, the pools are not guaranteed to contain a correct answer. We discard all questions without any correct answer in the associated pool of candidate answers. Model Valid Test AttentiveBiLSTM (Tan et al., 2016) IABRNN (Wang et al., 2016) APBiLSTM (Dos Santos et al., 2016) 68.9 69.1 68.4 66,9 67.0 69.1 Model Valid Test APBiLSTM (reimplem"
W17-6935,P13-1171,0,0.0219888,"ted for other scenarios that deal with shorter texts. In general, we show that it is possible to perform effective importance weighting in non-factoid answer selection without relying on the relatedness of questions and candidate answers. 2 Related Work Earlier work in answer selection relies on handcrafted features based on semantic role annotations (Shen and Lapata, 2007; Surdeanu et al., 2011), parse trees (Wang and Manning, 2010; Heilman and Smith, 2010), tree kernels (Moschitti et al., 2007; Severyn and Moschitti, 2012), discourse structures (Jansen et al., 2014), and external resources (Yih et al., 2013). More recently, researchers started using deep neural networks for answer selection. Yu et al. (2014), for example, propose a convolutional bigram model to classify a candidate answer as correct or incorrect. Similar but more enhanced, Severyn and Moschitti (2015) use a CNN with additional dense layers to capture interactions between questions and candidate answers, a model that is also part of a combined approach with tree kernels (Tymoshenko et al., 2016). And Wang and Nyberg (2015) incorporate stacked BiLSTMs to learn a joint feature vector of a question and a candidate answer for classifi"
W17-6935,Q16-1019,0,0.0516684,"Missing"
W18-3602,P17-1183,0,0.438095,"ion procedure in an end-to-end fashion. Recent work (Faruqui et al., 2016) proposed to model the problem as a sequence-to-sequence learning task, using the encoder-decoder neural network architecture developed in the machine translation community (Cho et al., 2014; Sutskever et al., 2014). This approach showed an improvement over conventional machine learning models, but failed to address the issue of poor sample complexity of complex neural networks – in practice, the approach did not perform well on low-resource or morphologically rich languages. An attempt to address this issue was made by Aharoni and Goldberg (2017), who proposed to directly model an almost monotonic alignment between the input and output character sequences by using a controllable hard attention mechanism which allows the network to jointly align and transduce, while maintaining a focused representation at Syntactic Ordering Given a bag of input words, a syntactic ordering algorithm constructs an output sentence. Prior work explored a range of approaches to syntactic ordering: grammar-based methods (Elhadad and Robin, 1992; Carroll et al., 1999; White et al., 2007), generate-and-rerank approaches (Bangalore and Rambow, 2000; Langkilde-G"
W18-3602,N15-1107,0,0.0276731,"tactic attributes (Number=Sing, Person=3, Tense=Past). Early work proposed to approach the task with finite state transducers (Koskenniemi, 1983; Kaplan and Kay, 1994). While being accurate, these systems require a lot of time and linguistic expertise to construct and maintain. With the advance of machine learning, the community mostly shifted towards data-driven methods of automatic morphological paradigm induction and string transduction as the method of morphological inflection generation (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2015). In comparison with their rule-based counterparts, these approaches scale better across languages and domains, but require manually-defined comprehensive feature representation of the inputs. Current research focuses on data-driven models which learn a high-dimensional feature representation of the input data during the optimization procedure in an end-to-end fashion. Recent work (Faruqui et al., 2016) proposed to model the problem as a sequence-to-sequence learning task, using the encoder-decoder neural network architecture developed in the machine translation community (Cho et al., 2014; Su"
W18-3602,C00-1007,0,0.0341818,"was made by Aharoni and Goldberg (2017), who proposed to directly model an almost monotonic alignment between the input and output character sequences by using a controllable hard attention mechanism which allows the network to jointly align and transduce, while maintaining a focused representation at Syntactic Ordering Given a bag of input words, a syntactic ordering algorithm constructs an output sentence. Prior work explored a range of approaches to syntactic ordering: grammar-based methods (Elhadad and Robin, 1992; Carroll et al., 1999; White et al., 2007), generate-and-rerank approaches (Bangalore and Rambow, 2000; Langkilde-Geary, 2002), tree linearization using probabilistic language models (Guo et al., 2008), inter alia. Depending on how much syntactic information is available as input, the research on syntactic ordering can be categorized into (1) free word ordering, (2) full tree linearization and (3) partial tree linearization (Liu et al., 2015). The setup of the Surface Realization Task corresponds to the full tree linearization case, since the dependency tree information is provided. Conceptually, the problem of tree linearization is simple. However, given no constraints, the search space is ex"
W18-3602,W11-2832,0,0.297561,"ils of our system architecture are specified in Introduction Natural Language generation (NLG) is the task of generating natural language utterances from textual inputs or structured data representations. For many years one of the research foci in the NLG community has been Surface Realization (SR) – the process of transforming a sentence plan into a linearly-ordered, grammatical string of morphologically inflected words (Langkilde-Geary, 2002). The SR Shared Task is aimed at developing a common input representation that could be used by a variety of NLG systems to generate realizations from (Belz et al., 2011). In the case of the Surface Realization Shared Task 2018 (Mille et al., 2018) there are two different representations the contestants can use, depending on the track they participate in: Shallow Track: unordered dependency trees consisting of lemmatized nodes with part-ofspeech (POS) tags and morphological information as found in the Universal Dependencies (UD) annotations (version 2.0).1 1 2 http://universaldependencies.org/ format.html http://universaldependencies.org/ 13 Proceedings of the First Workshop on Multilingual Surface Realisation, pages 13–28 c Melbourne, Australia, July 19, 2018"
W18-3602,N16-1077,0,0.0185169,"igm induction and string transduction as the method of morphological inflection generation (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2015). In comparison with their rule-based counterparts, these approaches scale better across languages and domains, but require manually-defined comprehensive feature representation of the inputs. Current research focuses on data-driven models which learn a high-dimensional feature representation of the input data during the optimization procedure in an end-to-end fashion. Recent work (Faruqui et al., 2016) proposed to model the problem as a sequence-to-sequence learning task, using the encoder-decoder neural network architecture developed in the machine translation community (Cho et al., 2014; Sutskever et al., 2014). This approach showed an improvement over conventional machine learning models, but failed to address the issue of poor sample complexity of complex neural networks – in practice, the approach did not perform well on low-resource or morphologically rich languages. An attempt to address this issue was made by Aharoni and Goldberg (2017), who proposed to directly model an almost mono"
W18-3602,N10-1115,0,0.142177,".64 9.40 9.58 14.47 10.23 10.21 12.44 9.35 9.36 13.63 8.68 7.21 12.48 7.73 7.60 11.85 8.61 8.64 12.78 7.76 7.54 15.5 12.94 13.06 Table 5: Final metric evaluation results of the system pipeline. Dev-UD denotes the development set of the original UD dataset. Dev-SR and Test-SR is the data provided by the organizers (with scrambled lemmas). A more principled approach would be to define an adaptive model which encodes some notion of processing preference: given a set of tokens, the system should first make predictions it is most confident about, similar to easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) or imitation learning methods (Lampouras and Vlachos, 2016). been previously explored, but we found no easy way of doing so for the proposed approach. Nevertheless, it seems like a promising direction to pursue, and we plan to investigate it further. 7 Conclusion In this paper, we have presented the results of our participation in the Surface Realization Shared Task 2018. We developed a promising method of syntactic ordering; evaluation results on the development data indicate that once the problem of order-sensitivity is solved, it can be successfully applied as a component in the syntactic"
W18-3602,C10-1012,0,0.133017,"ble. This stimulated the line of research focusing on the development of approximate search methods. Current state-of-the-art (evaluated on the English data only) belongs to the system of Puduppully et al. (2016) who extended the work of Liu et al. (2015) on developing a transition-based generator. The authors treated language generation process as a generalized form of dependency parsing with unordered token sequences, and used a learning and search framework of Zhang and Clark (2011) to keep the decoding process tractable. A similar approach to dependency tree linearization was explored in (Bohnet et al., 2010), who approximated exact decoding with a beam search. Our method of syntactic ordering is also based on search ap14 Language Property unique features OOV lemmas OOV forms OOV chars ar 37 1056 1745 0 cs 112 3299 8070 2 en 36 1180 1313 3 es 56 1368 2131 1 fi 89 1598 3666 5 fr 35 1895 2387 12 it 41 439 683 2 nl 66 973 1131 0 pt 48 535 785 0 ru 40 2723 8190 0 Table 1: Cross-lingual data analysis. min = 1, max = 18 mean = 1.24, std = 0.64 each step. The authors proposed to utilize independently learned character-level alignments instead of the weighted sum of representations (as done in the soft at"
W18-3602,C08-1038,0,0.058646,"Missing"
W18-3602,P09-1091,0,0.129706,"tree and a binary tree, constructed according to Algorithm 1. Reference sentence: “I like fresh juicy Egle apples”. one needs to take into account the complexity of the tree structures. We found the branching factor to be very informative in this regard: for each node in each tree we counted the number of children the node has. Most nodes in the dependency trees of all examined languages have one to three children (Figure 2 shows the distribution of branching factor values for English). This solicits decomposition of the syntactic ordering procedure over subtrees, similar to what was done in (He et al., 2009). 4 (b) Binary tree 4.1 Syntactic Component The first step of the proposed pipeline orders the nodes of the dependency tree into a sequence which ideally mirrors the order of words in the reference sentence. The main difficulty of this step is finding a sorting or ranking method which avoids making many node comparisons or scoring decisions. We propose an ordering procedure which uses a given dependency tree and constructs a binary tree storing the original dependency nodes (lemmas) in a sorted order (Algorithm 1) . As input, the algorithm takes a dependency tree and a classifier trained to ma"
W18-3602,D14-1179,0,0.00749316,"Missing"
W18-3602,J94-3001,0,0.0947827,"the lemmas and inflecting them to the correct surface forms. Past research work proposed both joint and pipeline solutions for the problem. Taking into consideration the pipeline nature of our system, we separate the related work stage-wise. 2.1 Word Inflection Word inflection in the context of the Surface Realization Task can be defined as the subtask of generating a surface form (was) from a given source lemma (be) and additional morphological/syntactic attributes (Number=Sing, Person=3, Tense=Past). Early work proposed to approach the task with finite state transducers (Koskenniemi, 1983; Kaplan and Kay, 1994). While being accurate, these systems require a lot of time and linguistic expertise to construct and maintain. With the advance of machine learning, the community mostly shifted towards data-driven methods of automatic morphological paradigm induction and string transduction as the method of morphological inflection generation (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2015). In comparison with their rule-based counterparts, these approaches scale better across languages and domains, but require manually-defined compr"
W18-3602,N16-1058,0,0.173385,", (2) full tree linearization and (3) partial tree linearization (Liu et al., 2015). The setup of the Surface Realization Task corresponds to the full tree linearization case, since the dependency tree information is provided. Conceptually, the problem of tree linearization is simple. However, given no constraints, the search space is exponential in the number of tokens, which makes exhaustive search intractable. This stimulated the line of research focusing on the development of approximate search methods. Current state-of-the-art (evaluated on the English data only) belongs to the system of Puduppully et al. (2016) who extended the work of Liu et al. (2015) on developing a transition-based generator. The authors treated language generation process as a generalized form of dependency parsing with unordered token sequences, and used a learning and search framework of Zhang and Clark (2011) to keep the decoding process tractable. A similar approach to dependency tree linearization was explored in (Bohnet et al., 2010), who approximated exact decoding with a beam search. Our method of syntactic ordering is also based on search ap14 Language Property unique features OOV lemmas OOV forms OOV chars ar 37 1056"
W18-3602,C16-1105,0,0.0408172,"Missing"
W18-3602,W02-2103,0,0.142212,"structure. Section 2 describes related work done in the past. Section 3 presents the results of the exploratory data analysis conducted prior to system development. The details of our system architecture are specified in Introduction Natural Language generation (NLG) is the task of generating natural language utterances from textual inputs or structured data representations. For many years one of the research foci in the NLG community has been Surface Realization (SR) – the process of transforming a sentence plan into a linearly-ordered, grammatical string of morphologically inflected words (Langkilde-Geary, 2002). The SR Shared Task is aimed at developing a common input representation that could be used by a variety of NLG systems to generate realizations from (Belz et al., 2011). In the case of the Surface Realization Shared Task 2018 (Mille et al., 2018) there are two different representations the contestants can use, depending on the track they participate in: Shallow Track: unordered dependency trees consisting of lemmatized nodes with part-ofspeech (POS) tags and morphological information as found in the Universal Dependencies (UD) annotations (version 2.0).1 1 2 http://universaldependencies.org/"
W18-3602,N15-1012,0,0.263889,"yntactic ordering algorithm constructs an output sentence. Prior work explored a range of approaches to syntactic ordering: grammar-based methods (Elhadad and Robin, 1992; Carroll et al., 1999; White et al., 2007), generate-and-rerank approaches (Bangalore and Rambow, 2000; Langkilde-Geary, 2002), tree linearization using probabilistic language models (Guo et al., 2008), inter alia. Depending on how much syntactic information is available as input, the research on syntactic ordering can be categorized into (1) free word ordering, (2) full tree linearization and (3) partial tree linearization (Liu et al., 2015). The setup of the Surface Realization Task corresponds to the full tree linearization case, since the dependency tree information is provided. Conceptually, the problem of tree linearization is simple. However, given no constraints, the search space is exponential in the number of tokens, which makes exhaustive search intractable. This stimulated the line of research focusing on the development of approximate search methods. Current state-of-the-art (evaluated on the English data only) belongs to the system of Puduppully et al. (2016) who extended the work of Liu et al. (2015) on developing a"
W18-3602,2007.mtsummit-ucnlg.4,0,0.0763496,"lly rich languages. An attempt to address this issue was made by Aharoni and Goldberg (2017), who proposed to directly model an almost monotonic alignment between the input and output character sequences by using a controllable hard attention mechanism which allows the network to jointly align and transduce, while maintaining a focused representation at Syntactic Ordering Given a bag of input words, a syntactic ordering algorithm constructs an output sentence. Prior work explored a range of approaches to syntactic ordering: grammar-based methods (Elhadad and Robin, 1992; Carroll et al., 1999; White et al., 2007), generate-and-rerank approaches (Bangalore and Rambow, 2000; Langkilde-Geary, 2002), tree linearization using probabilistic language models (Guo et al., 2008), inter alia. Depending on how much syntactic information is available as input, the research on syntactic ordering can be categorized into (1) free word ordering, (2) full tree linearization and (3) partial tree linearization (Liu et al., 2015). The setup of the Surface Realization Task corresponds to the full tree linearization case, since the dependency tree information is provided. Conceptually, the problem of tree linearization is s"
W18-3602,W04-0109,0,0.0188287,"(was) from a given source lemma (be) and additional morphological/syntactic attributes (Number=Sing, Person=3, Tense=Past). Early work proposed to approach the task with finite state transducers (Koskenniemi, 1983; Kaplan and Kay, 1994). While being accurate, these systems require a lot of time and linguistic expertise to construct and maintain. With the advance of machine learning, the community mostly shifted towards data-driven methods of automatic morphological paradigm induction and string transduction as the method of morphological inflection generation (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2015). In comparison with their rule-based counterparts, these approaches scale better across languages and domains, but require manually-defined comprehensive feature representation of the inputs. Current research focuses on data-driven models which learn a high-dimensional feature representation of the input data during the optimization procedure in an end-to-end fashion. Recent work (Faruqui et al., 2016) proposed to model the problem as a sequence-to-sequence learning task, using the encoder-decoder neural network archite"
W18-3602,W18-3601,0,0.0721816,"generation (NLG) is the task of generating natural language utterances from textual inputs or structured data representations. For many years one of the research foci in the NLG community has been Surface Realization (SR) – the process of transforming a sentence plan into a linearly-ordered, grammatical string of morphologically inflected words (Langkilde-Geary, 2002). The SR Shared Task is aimed at developing a common input representation that could be used by a variety of NLG systems to generate realizations from (Belz et al., 2011). In the case of the Surface Realization Shared Task 2018 (Mille et al., 2018) there are two different representations the contestants can use, depending on the track they participate in: Shallow Track: unordered dependency trees consisting of lemmatized nodes with part-ofspeech (POS) tags and morphological information as found in the Universal Dependencies (UD) annotations (version 2.0).1 1 2 http://universaldependencies.org/ format.html http://universaldependencies.org/ 13 Proceedings of the First Workshop on Multilingual Surface Realisation, pages 13–28 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics Section 4 which is followed b"
W18-3602,P00-1027,0,0.120497,"ask of generating a surface form (was) from a given source lemma (be) and additional morphological/syntactic attributes (Number=Sing, Person=3, Tense=Past). Early work proposed to approach the task with finite state transducers (Koskenniemi, 1983; Kaplan and Kay, 1994). While being accurate, these systems require a lot of time and linguistic expertise to construct and maintain. With the advance of machine learning, the community mostly shifted towards data-driven methods of automatic morphological paradigm induction and string transduction as the method of morphological inflection generation (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2015). In comparison with their rule-based counterparts, these approaches scale better across languages and domains, but require manually-defined comprehensive feature representation of the inputs. Current research focuses on data-driven models which learn a high-dimensional feature representation of the input data during the optimization procedure in an end-to-end fashion. Recent work (Faruqui et al., 2016) proposed to model the problem as a sequence-to-sequence learning task, using the encoder-decoder neu"
W18-3602,E06-1010,0,0.0558971,"Missing"
W18-3602,P02-1040,0,0.109202,"s the words to finish the surface realization process. Our contribution is a novel sequential method of ordering lemmas, which, despite its simplicity, achieves promising results. We demonstrate the effectiveness of the proposed approach, describe its limitations and outline ways to improve it. 1 We participated in the shallow track, and therefore our task was to generate a sentence by ordering the lemmas and inflecting them to the correct surface forms. The outputs of the participating systems are assessed using both automatic and manual evaluation. The former is performed by computing BLEU (Papineni et al., 2002), NIST (Doddington, 2002), CIDEr (Vedantam et al., 2015) scores and normalized string edit distance (EDIST) between the reference sentence and a system output. Manual evaluation is based on preference judgments: third-year undergraduate students from Cambridge, Oxford and Edinburgh rate pairs of candidate outputs (including the target sentence), scoring them for Clarity, Fluency and Meaning Similarity. The data used for the task is the UD treebanks distributed in the 10-column CoNLL-U format.2 The data is available for Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russia"
W18-4508,W14-2302,0,0.0211778,"ployed in previous work unnecessary. This includes e.g., abstractness values to model source and target concepts (Tsvetkov This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 70 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 70–80 Santa Fe, New Mexico, USA, August 25, 2018. et al., 2014; Turney and Assaf, 2011), selectional preference violations (Wilks, 1978; Shutova, 2013) or topic modeling (Heintz et al., 2013; Beigman Klebanov et al., 2014). In contrast, because they are the only external resource we utilize, we investigate the influence of an important hyper-parameter of our network—different pre-trained embeddings—on the token-level metaphor detection task and show the genre-specific effects of these embedding models. 2 Related work Classification and detection of non-literal language has largely focused on metaphor detection. Another prominent task is the detection of idiomatic language. Similar features have been employed in those tasks, even though the specific phenomena differ. However, since the datasets used for these ta"
W18-4508,W16-1104,1,0.799586,"tasets created and used in these works often follow no unified annotation guidelines (compare Steen et al. (2010) and Tsvetkov et al. (2014)), or even no disclosed guidelines at all, e.g., Heintz et al. (2013), or annotate metaphors at different levels of granularity (Steen et al., 2010; Gutierrez et al., 2016). This is also true for many works in more general non-literal language detection. Consequently, methods are seldom compared on related tasks. Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection (Do Dinh and Gurevych, 2016; Rei et al., 2017) or detection of non-literal and figurative language in general. In this paper, we test whether the same simple generic neural network approach is effective for four different non-literal language detection tasks: token and construction level metaphor detection, idiom classification and classification of literal and non-literal German particle verbs. We train a neural model using LSTMs to encode the context of a metaphor candidate or non-literal compound. We show that our approach outperforms existing state-of-the-art models on two tasks, while producing competitive results"
W18-4508,C18-1132,1,0.767991,"del using LSTMs to encode the context of a metaphor candidate or non-literal compound. We show that our approach outperforms existing state-of-the-art models on two tasks, while producing competitive results on another task, independent of the mode of classification (e.g., token vs. construction classification). In demonstrating the applicability of the same, simple neural network architecture to different non-literal language tasks, we lay the foundation for a more integrative approach. A joint modeling of these tasks, through data concatenation and multi-task learning, is investigated in Do Dinh et al. (2018). Given enough training data, our model renders many of the handcrafted features employed in previous work unnecessary. This includes e.g., abstractness values to model source and target concepts (Tsvetkov This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 70 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 70–80 Santa Fe, New Mexico, USA, August 25, 2018. et al., 2014; Turney and Assaf, 2011), selectional preference viola"
W18-4508,P16-1018,0,0.0195188,"nently metaphor detection (Veale et al., 2016), has been plentiful. For metaphor detection, most works name the Conceptual Metaphor Theory (Lakoff and Johnson, 1980) as their underlying framework, in which metaphors are modeled as cognitive mappings of concepts from a source to a target domain. However, the datasets created and used in these works often follow no unified annotation guidelines (compare Steen et al. (2010) and Tsvetkov et al. (2014)), or even no disclosed guidelines at all, e.g., Heintz et al. (2013), or annotate metaphors at different levels of granularity (Steen et al., 2010; Gutierrez et al., 2016). This is also true for many works in more general non-literal language detection. Consequently, methods are seldom compared on related tasks. Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection (Do Dinh and Gurevych, 2016; Rei et al., 2017) or detection of non-literal and figurative language in general. In this paper, we test whether the same simple generic neural network approach is effective for four different non-literal language detection tasks: token and construction level metaphor detection, idiom c"
W18-4508,W13-0908,0,0.0888009,"na. 1 Introduction Computational research of non-literal phenomena, e.g., metonymy, idiom, and prominently metaphor detection (Veale et al., 2016), has been plentiful. For metaphor detection, most works name the Conceptual Metaphor Theory (Lakoff and Johnson, 1980) as their underlying framework, in which metaphors are modeled as cognitive mappings of concepts from a source to a target domain. However, the datasets created and used in these works often follow no unified annotation guidelines (compare Steen et al. (2010) and Tsvetkov et al. (2014)), or even no disclosed guidelines at all, e.g., Heintz et al. (2013), or annotate metaphors at different levels of granularity (Steen et al., 2010; Gutierrez et al., 2016). This is also true for many works in more general non-literal language detection. Consequently, methods are seldom compared on related tasks. Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection (Do Dinh and Gurevych, 2016; Rei et al., 2017) or detection of non-literal and figurative language in general. In this paper, we test whether the same simple generic neural network approach is effective for four d"
W18-4508,L16-1135,0,0.382951,"015) investigate metonymy identification, i.e. identification of instances where entities replace other associated entities. For example in the sentence “Washington and Beijing enter new trade talks”, Washington and Beijing are used to refer to the US and Chinese governments. Zhang and Gelernter (2015) reuse many features commonly used for the metaphor detection task, such as imageability and abstractness ratings. They further test different word representations—word embeddings, LSA, and one-hot-encoding—to detect metonymy using an SVM. A different non-literal language task is investigated by Horbach et al. (2016), in which they classify literal and idiomatic use of different German infinitive-verb compounds based on their context. They employ Naive Bayes and various features—including local skip-n-grams, POS tags, automatically obtained subject and object information, selectional preferences, and manually annotated topic information. K¨oper and Schulte im Walde (2016) classify literally and non-literally used German particle verbs across 10 particles. Using a random forest classifier and various features (e.g., unigrams, affective ratings, distributed word representations), they achieve an accuracy of"
W18-4508,N16-1175,0,0.144717,"ng on the task. For example, for the infinitive-verb classification, the annotated instance can consist of two tokens, thus we can have two center embeddings. To illustrate, consider the example: “Kinder sollten nicht mehr sitzen bleiben m¨ussen, sondern gef¨ordert werden.” In this sentence, we use (Kinder,sollten,nicht,mehr) as left context, (sitzen,bleiben) as center, and (m¨ussen,sondern,gef¨ordert,werden) as right context (see Figure 1). For the tasks with German data we use the word embeddings of Reimers et al. (2014). For construction level metaphor detection we employ the embeddings of Komninos and Manandhar (2016) as preliminary cross validation experiments on the training set show that they work well. On the other hand, preliminary experiments on the development set for token level metaphor detection show an advantage of the Google News word2vec embeddings (Mikolov et al., 2013) for this task, which is why we use them to work on the VUAMC (a more in-depth analysis validates our decision, Section 6). We conduct our experiments using Keras1 and Theano2 . We make our code publicly available3 . 5 Results The main results are laid out in Table 2, results broken down into subcorpora are shown in Table 3 (to"
W18-4508,N16-1039,0,0.0401523,"Missing"
W18-4508,E17-2086,0,0.0460612,"Missing"
W18-4508,P14-2050,0,0.0274791,"cles that the embeddings are trained on. Independently of the concrete embeddings used, the network performs consistently best on the news subcorpus, followed by the academic texts. Looking more closely into the classifications on the fiction subcorpus, we observe a large performance difference between Glove and the remaining embeddings. This is mainly due to low recall (0.356, see also Table 6), especially compared to the word2vec embeddings (0.710). The results on the conversation subcorpus are similarly noteworthy, because here both embedding models that encode dependency information, from Levy and Goldberg (2014) and Komninos and Manandhar (2016), perform worse than the remaining models (also due to lower recall). This is in line with our findings from Section 5.1 where we note that our network struggles with omissions or ungrammatical sentences—as the structure of the conversation sentences is more likely to be irregular, including “correct” syntactic information can apparently be detrimental. P word2vec GloVe ConceptNet Levy Komninos .576 .544 .604 .652 .634 academic R F1 .706 .594 .654 .535 .628 .634 .568 .628 .588 .631 P conversation R F1 .567 .470 .595 .629 .652 .518 .584 .478 .439 .396 .541 .521"
W18-4508,D14-1162,0,0.0797149,"Missing"
W18-4508,D17-1162,0,0.251401,"n these works often follow no unified annotation guidelines (compare Steen et al. (2010) and Tsvetkov et al. (2014)), or even no disclosed guidelines at all, e.g., Heintz et al. (2013), or annotate metaphors at different levels of granularity (Steen et al., 2010; Gutierrez et al., 2016). This is also true for many works in more general non-literal language detection. Consequently, methods are seldom compared on related tasks. Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection (Do Dinh and Gurevych, 2016; Rei et al., 2017) or detection of non-literal and figurative language in general. In this paper, we test whether the same simple generic neural network approach is effective for four different non-literal language detection tasks: token and construction level metaphor detection, idiom classification and classification of literal and non-literal German particle verbs. We train a neural model using LSTMs to encode the context of a metaphor candidate or non-literal compound. We show that our approach outperforms existing state-of-the-art models on two tasks, while producing competitive results on another task, in"
W18-4508,schafer-bildhauer-2012-building,0,0.0372383,"Missing"
W18-4508,S13-1040,0,0.0220763,"ing data, our model renders many of the handcrafted features employed in previous work unnecessary. This includes e.g., abstractness values to model source and target concepts (Tsvetkov This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 70 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 70–80 Santa Fe, New Mexico, USA, August 25, 2018. et al., 2014; Turney and Assaf, 2011), selectional preference violations (Wilks, 1978; Shutova, 2013) or topic modeling (Heintz et al., 2013; Beigman Klebanov et al., 2014). In contrast, because they are the only external resource we utilize, we investigate the influence of an important hyper-parameter of our network—different pre-trained embeddings—on the token-level metaphor detection task and show the genre-specific effects of these embedding models. 2 Related work Classification and detection of non-literal language has largely focused on metaphor detection. Another prominent task is the detection of idiomatic language. Similar features have been employed in those tasks, even though the s"
W18-4508,P14-1024,0,0.426115,"different languages, (iii) over different non-literal language phenomena. 1 Introduction Computational research of non-literal phenomena, e.g., metonymy, idiom, and prominently metaphor detection (Veale et al., 2016), has been plentiful. For metaphor detection, most works name the Conceptual Metaphor Theory (Lakoff and Johnson, 1980) as their underlying framework, in which metaphors are modeled as cognitive mappings of concepts from a source to a target domain. However, the datasets created and used in these works often follow no unified annotation guidelines (compare Steen et al. (2010) and Tsvetkov et al. (2014)), or even no disclosed guidelines at all, e.g., Heintz et al. (2013), or annotate metaphors at different levels of granularity (Steen et al., 2010; Gutierrez et al., 2016). This is also true for many works in more general non-literal language detection. Consequently, methods are seldom compared on related tasks. Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection (Do Dinh and Gurevych, 2016; Rei et al., 2017) or detection of non-literal and figurative language in general. In this paper, we test whether th"
W18-4508,D11-1063,0,0.0197682,"sk learning, is investigated in Do Dinh et al. (2018). Given enough training data, our model renders many of the handcrafted features employed in previous work unnecessary. This includes e.g., abstractness values to model source and target concepts (Tsvetkov This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 70 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 70–80 Santa Fe, New Mexico, USA, August 25, 2018. et al., 2014; Turney and Assaf, 2011), selectional preference violations (Wilks, 1978; Shutova, 2013) or topic modeling (Heintz et al., 2013; Beigman Klebanov et al., 2014). In contrast, because they are the only external resource we utilize, we investigate the influence of an important hyper-parameter of our network—different pre-trained embeddings—on the token-level metaphor detection task and show the genre-specific effects of these embedding models. 2 Related work Classification and detection of non-literal language has largely focused on metaphor detection. Another prominent task is the detection of idiomatic language. Simil"
W18-5211,P18-1224,0,0.0961507,"ibility, Wang et al. (2018) reveal the failure of models only relying on distributional data, whilst the injection of world knowledge helps. Glockner et al. (2018) point out the deficiency of state-ofthe-art approaches for understanding entailment on ∗ First and second authors contributed equally to this work. 1 SemEval-2018 Task 12: https://competitions. codalab.org/competitions/17327 the large-scale SNLI corpus (Stanford Natural Language Inference) (Bowman et al., 2015). In their study, the model incorporating external lexical information from WordNet, KIM (Knowledge-based Inference Model) (Chen et al., 2018), does not yield the awaited improvements — where the crucial point might be WordNet (Miller, 1995) which does not contain explicit world knowledge in the form of event- and fact-based knowledge. Previous work argues that information in WordNet overlaps with word embeddings (Zhai et al., 2016), therefore we focus on other types of knowledge in our work. Complementary sources of external knowledge: we experiment using the lexical-semantic resource FrameNet (FN) and the knowledge base Wikidata (WD). These resources provide information beyond the lexical relations encoded in WordNet and thus have"
W18-5211,N18-1076,0,0.0549577,"Missing"
W18-5211,S12-1051,0,0.0182003,"Missing"
W18-5211,S18-1122,0,0.121184,"ment on their complementarity (cf. Sec. 2.2, 2.3, 2.4). Finally, we present our approach with preprocessing and the actual model enrichment (cf. Sec. 2.5, 2.6). 2.1 understanding of the context of both, claim and reason, is crucial – for which Habernal et al. (2018) recommend the inclusion of external knowledge. Baseline The baseline provided by Habernal et al. (2018) is an intra-warrant attention model that reads in Word2Vec vectors (Mikolov et al., 2013) of all words in (a-c) and adapts attention weights for the decision between (i) and (ii). Shared task winner The shared task winner, GIST (Choi and Lee, 2018), transfers inference knowledge (SNLI, Bowman et al., 2015) to the task of ARC and benefits from similar information in both datasets. Our approach in contrast to GIST Our approach extends the baseline model with two external knowledge schemata, FN and WD, to explore their effects. The intuition can be explained with the instance in Figure 1: FN could be helpful by disambiguating ‘companies’ and ‘corporations’ to the same frame with meta-knowledge how it relates to other frames and WD could be of additional help by mapping them to entities with detailed information and examples for such instit"
W18-5211,N16-1067,0,0.0461453,"Missing"
W18-5211,P98-1013,0,0.316405,"h the instance in Figure 1: FN could be helpful by disambiguating ‘companies’ and ‘corporations’ to the same frame with meta-knowledge how it relates to other frames and WD could be of additional help by mapping them to entities with detailed information and examples for such institutions. We focus on utilizing the two knowledge schemata of FN and WD and thus, our interest is orthogonal to GIST. The advantage of our approach is independence of domain and task, which becomes especially relevant in scenarios lacking large-scale support data. 2.2 FrameNet’s Event Knowledge The Berkeley FrameNet (Baker et al., 1998; Ruppenhofer et al., 2016) is an ongoing project for manually building a large lexical-semantic resource with expert annotations. It embodies the theory of frame semantics (Fillmore, 1976): frames capture units of meaning corresponding to prototypical situations. It consists of two parts, a lexicon that maps predicates to frames they can evoke, and fully annotated texts. For example, the verb buy can evoke either the frame Commerce buy or Fall for, depending on the context (buying goods versus buying a lie). Furthermore, the lexicon gives access to frame-specific role-labels (e.g., Buyer, Goo"
W18-5211,P18-2103,0,0.0305966,"ion (ARC) used in the SemEval-2018 shared task. After reviewing the participating systems, they hypothesize that external world knowledge may be essential for ARC.1 We explore enriching models with event and fact knowledge on ARC to investigate into this hypothesis. Semantic tasks profit from external knowledge: language understanding requires more complex knowledge than that contained in current systems and word embeddings. For the task of semantic plausibility, Wang et al. (2018) reveal the failure of models only relying on distributional data, whilst the injection of world knowledge helps. Glockner et al. (2018) point out the deficiency of state-ofthe-art approaches for understanding entailment on ∗ First and second authors contributed equally to this work. 1 SemEval-2018 Task 12: https://competitions. codalab.org/competitions/17327 the large-scale SNLI corpus (Stanford Natural Language Inference) (Bowman et al., 2015). In their study, the model incorporating external lexical information from WordNet, KIM (Knowledge-based Inference Model) (Chen et al., 2018), does not yield the awaited improvements — where the crucial point might be WordNet (Miller, 1995) which does not contain explicit world knowled"
W18-5211,C16-1120,0,0.0308206,"tities to a knowledge base (EntLink) disambiguates the participants. Furthermore, both resources provide meta-knowledge about how their frames or entries relate to each other. Multiple levels of knowledge processing help: combining several kinds of annotations benefits question answering (Khashabi et al., 2018), external knowledge about synonyms enhances inference (Chen et al., 2018), and jointly modeling several tasks (e.g., frame-semantic parsing and dependency parsing) is fruitful (Peng et al., 2018). In particular, the idea of connecting event semantics and fact knowledge was confirmed by Guo et al. (2016): they jointly formalize semantic role labeling and relation classification and thereby improve upon PropBank semantic role labeling. 90 Proceedings of the 5th Workshop on Argument Mining, pages 90–96 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Outline In this paper, we investigate whether external information in terms of event-based frames (FN) and fact-based entities (WD) can contribute to holistic understanding of the argumentation in the ARC task. First, we examine the effect of both annotations separately and second, we explore whether a joint ann"
W18-5211,N18-1175,1,0.810725,"vent knowledge about prototypical situations from FrameNet and fact knowledge about concrete entities from Wikidata to solve the task. We find that both resources can contribute to an improvement over the non-enriched approach and point out two persisting challenges: first, integration of many annotations of the same type, and second, fusion of complementary annotations. After our explorations, we question the key role of external world knowledge with respect to the argumentative reasoning task and rather point towards a logic-based analysis of the chain of reasoning. 1 Introduction Recently, Habernal et al. (2018) introduced a challenging dataset for Argument Reasoning Comprehension (ARC) used in the SemEval-2018 shared task. After reviewing the participating systems, they hypothesize that external world knowledge may be essential for ARC.1 We explore enriching models with event and fact knowledge on ARC to investigate into this hypothesis. Semantic tasks profit from external knowledge: language understanding requires more complex knowledge than that contained in current systems and word embeddings. For the task of semantic plausibility, Wang et al. (2018) reveal the failure of models only relying on d"
W18-5211,W17-2618,1,0.770172,"iching with Semantics Preprocessing - Obtaining Annotations We use two freely available systems to obtain semantic annotations for the claim (b), the reason (c) and the alternative warrants (i, ii): the frame identifier by Botschen et al. (2018) for frame annotations and the entity linker by Sorokin and Gurevych (2018). We employ pre-trained vector representations to encode information from FN and WD. We use the pre-trained frame embeddings (50-dim.) that are learned with TransE (Bordes et al., 2013) on the structure of the FN hierarchy with the collection of (frame, relation, frame)-triples (Botschen et al., 2017). We also use TransE to pre-train entity embeddings (100-dim.) on the WD graph. The an3 Results In Table 1 we report our results on the ARC task. Our extended approaches ‘+FN’ and ‘+WD’ for semantic enrichment with information about frames and entities increase the averaged performance by more than one percentage point against the baseline. For the best run, the advantage of ‘+FN’ and ‘+WD’ becomes even clearer (+2.2 pp.). On the other hand, the straightforward combination of the two external knowledge source, ‘+FN/WD’, does not lead to further improvements. This points out the 3 www.wikidata."
W18-5211,D15-1075,0,0.215621,"anguage understanding requires more complex knowledge than that contained in current systems and word embeddings. For the task of semantic plausibility, Wang et al. (2018) reveal the failure of models only relying on distributional data, whilst the injection of world knowledge helps. Glockner et al. (2018) point out the deficiency of state-ofthe-art approaches for understanding entailment on ∗ First and second authors contributed equally to this work. 1 SemEval-2018 Task 12: https://competitions. codalab.org/competitions/17327 the large-scale SNLI corpus (Stanford Natural Language Inference) (Bowman et al., 2015). In their study, the model incorporating external lexical information from WordNet, KIM (Knowledge-based Inference Model) (Chen et al., 2018), does not yield the awaited improvements — where the crucial point might be WordNet (Miller, 1995) which does not contain explicit world knowledge in the form of event- and fact-based knowledge. Previous work argues that information in WordNet overlaps with word embeddings (Zhai et al., 2016), therefore we focus on other types of knowledge in our work. Complementary sources of external knowledge: we experiment using the lexical-semantic resource FrameNe"
W18-5211,N18-2049,0,0.0232934,"ain of reasoning. 1 Introduction Recently, Habernal et al. (2018) introduced a challenging dataset for Argument Reasoning Comprehension (ARC) used in the SemEval-2018 shared task. After reviewing the participating systems, they hypothesize that external world knowledge may be essential for ARC.1 We explore enriching models with event and fact knowledge on ARC to investigate into this hypothesis. Semantic tasks profit from external knowledge: language understanding requires more complex knowledge than that contained in current systems and word embeddings. For the task of semantic plausibility, Wang et al. (2018) reveal the failure of models only relying on distributional data, whilst the injection of world knowledge helps. Glockner et al. (2018) point out the deficiency of state-ofthe-art approaches for understanding entailment on ∗ First and second authors contributed equally to this work. 1 SemEval-2018 Task 12: https://competitions. codalab.org/competitions/17327 the large-scale SNLI corpus (Stanford Natural Language Inference) (Bowman et al., 2015). In their study, the model incorporating external lexical information from WordNet, KIM (Knowledge-based Inference Model) (Chen et al., 2018), does no"
W18-5211,N16-1098,0,0.0338127,"Missing"
W18-5211,W16-1306,1,0.890874,"Missing"
W18-5211,N18-1135,0,0.0440844,"Missing"
W18-5211,S18-2007,1,0.88111,"Missing"
W18-5216,D17-1078,0,0.0242199,"k Our work connects to different strands of research. Multi-Task Learning MTL was shown to be particularly beneficial when tasks stand in a natural hierarchy and when they are syntactic in nature (Søgaard and Goldberg, 2016). Moreover, it has been claimed that further main benefits for MTL are observed when data for the main task is sparse, in which case the auxiliary tasks may act as regularizers that prevent overfitting (Ruder et al., 2017). The latter is the case for PD3-MTL with little available parallel data. MTL has also been made use of for supervised cross-lingual transfer techniques (Cotterell and Heigold, 2017; Yang et al., 2017; Kim et al., 2017; Dinh et al., 2018). These assume small training sets in L2, and a system trained on them is regularized by a larger amount of training data in L1. In contrast to these, we assume no gold labels in L2 (unsupervised transfer), which necessitates a projection step. Our approach could also be combined with these supervised ones, by adding this small gold data to the three different datasets that we use in PD3. Argumentation Mining ArgMin is a fastgrowing field in NLP with applications in decision making and the legal domain (Palau and Moens, 2009) and can be"
W18-5216,N13-1073,0,0.0615894,"Missing"
W18-5216,P17-1002,1,0.846487,"em trained on them is regularized by a larger amount of training data in L1. In contrast to these, we assume no gold labels in L2 (unsupervised transfer), which necessitates a projection step. Our approach could also be combined with these supervised ones, by adding this small gold data to the three different datasets that we use in PD3. Argumentation Mining ArgMin is a fastgrowing field in NLP with applications in decision making and the legal domain (Palau and Moens, 2009) and can be solved on sentence-level (Daxenberger et al., 2017; Niculae et al., 2017; Stab et al., 2018) or token-level (Eger et al., 2017; Schulz et al., 2018). Cross-lingual ArgMin has recently attracted interest (Aker and Zhang, 2017; Eger et al., 2018). The proposed approaches mostly used machine translation, which is unavailable for the vast majority of the world’s languages. Low-resource transfer Low-resource language transfer has recently become very popular, e.g., when relying on only very few translation pairs for bilingual embedding space induction (Artetxe et al., 2017; Zhang et al., 2016) or in unsupervised machine translation using no parallel sources at all (Artetxe et al., 2018; Lample et al., 2018). Lowresource t"
W18-5216,C18-1071,1,0.769487,"labels in L2 (unsupervised transfer), which necessitates a projection step. Our approach could also be combined with these supervised ones, by adding this small gold data to the three different datasets that we use in PD3. Argumentation Mining ArgMin is a fastgrowing field in NLP with applications in decision making and the legal domain (Palau and Moens, 2009) and can be solved on sentence-level (Daxenberger et al., 2017; Niculae et al., 2017; Stab et al., 2018) or token-level (Eger et al., 2017; Schulz et al., 2018). Cross-lingual ArgMin has recently attracted interest (Aker and Zhang, 2017; Eger et al., 2018). The proposed approaches mostly used machine translation, which is unavailable for the vast majority of the world’s languages. Low-resource transfer Low-resource language transfer has recently become very popular, e.g., when relying on only very few translation pairs for bilingual embedding space induction (Artetxe et al., 2017; Zhang et al., 2016) or in unsupervised machine translation using no parallel sources at all (Artetxe et al., 2018; Lample et al., 2018). Lowresource transfer (on a level of domains rather than languages) has also been considered in ArgMin (Schulz et al., 2018), assumi"
W18-5216,J17-1004,1,0.807192,"ur focus is particularly on argumentation mining (ArgMin), a rapidly growing research field in NLP. Cross131 Proceedings of the 5th Workshop on Argument Mining, pages 131–143 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics lingual transfer is majorly important for ArgMin because it is inherently costly to get high-quality annotations for ArgMin due to: (i) subjectivity of argumentation as well as divergent and competing ArgMin theories (Daxenberger et al., 2017; Schulz et al., 2018), leading to disagreement among crowdworkers as well as expert annotators (Habernal and Gurevych, 2017), (ii) dependence of argument annotations on background knowledge and parsing of complex pragmatic relations (Moens, 2017). Thus, in order not to reproduce the same annotation costs for new languages, cross-lingual ArgMin methods are required. These techniques should both perform well with little available parallel data, to address many languages, and with general (nonargumentative) parallel data, because this is much more likely to be available. Our experiments address both of these requirements.1 2 PD3 Let LS = {(xS , y S )} denote a set of L1 data points in which each xS is an instance and"
W18-5216,P16-1101,0,0.0408731,"We do not apply dropout or `2 regularization. We report average macro F1 scores over 20 runs with different random initializations. For PD3merge, we shuffle the merged data before training— ˆ data. ˆ S , and D i.e., mini-batches can contain LS , D T For PD3-MTL, we shuffle L1 and L2 data individually and during training we sample each mini-batch from either task according to its size. In the MTL setup, we share the CNN layer across tasks and use task-specific softmax regression layers. Sequence tagging network architecture: For token-level POS tagging, we implement a bidirectional LSTM as in Ma and Hovy (2016) and Lample et al. (2016) with a CRF output layer. This is a state-of-the-art system for sequence tagging tasks such as POS and NER. Our model uses pretrained word embeddings and optionally concatenates these with a learned character-level representation. For all experiments, we use the same network topology: we use two hidden layers with 100 hidden units each, applying dropout on the hidden units and on the word embeddings. We use Adam as optimizer. Our network uses a CRF output layer rather than a softmax classifier to account for dependencies between successive labels. In the MTL setup, we"
W18-5216,D11-1006,0,0.0516663,"Missing"
W18-5216,P17-1091,0,0.0142348,"2018). These assume small training sets in L2, and a system trained on them is regularized by a larger amount of training data in L1. In contrast to these, we assume no gold labels in L2 (unsupervised transfer), which necessitates a projection step. Our approach could also be combined with these supervised ones, by adding this small gold data to the three different datasets that we use in PD3. Argumentation Mining ArgMin is a fastgrowing field in NLP with applications in decision making and the legal domain (Palau and Moens, 2009) and can be solved on sentence-level (Daxenberger et al., 2017; Niculae et al., 2017; Stab et al., 2018) or token-level (Eger et al., 2017; Schulz et al., 2018). Cross-lingual ArgMin has recently attracted interest (Aker and Zhang, 2017; Eger et al., 2018). The proposed approaches mostly used machine translation, which is unavailable for the vast majority of the world’s languages. Low-resource transfer Low-resource language transfer has recently become very popular, e.g., when relying on only very few translation pairs for bilingual embedding space induction (Artetxe et al., 2017; Zhang et al., 2016) or in unsupervised machine translation using no parallel sources at all (Art"
W18-5216,P14-1006,0,0.0732009,"Missing"
W18-5216,D17-1302,0,0.0230469,"arch. Multi-Task Learning MTL was shown to be particularly beneficial when tasks stand in a natural hierarchy and when they are syntactic in nature (Søgaard and Goldberg, 2016). Moreover, it has been claimed that further main benefits for MTL are observed when data for the main task is sparse, in which case the auxiliary tasks may act as regularizers that prevent overfitting (Ruder et al., 2017). The latter is the case for PD3-MTL with little available parallel data. MTL has also been made use of for supervised cross-lingual transfer techniques (Cotterell and Heigold, 2017; Yang et al., 2017; Kim et al., 2017; Dinh et al., 2018). These assume small training sets in L2, and a system trained on them is regularized by a larger amount of training data in L1. In contrast to these, we assume no gold labels in L2 (unsupervised transfer), which necessitates a projection step. Our approach could also be combined with these supervised ones, by adding this small gold data to the three different datasets that we use in PD3. Argumentation Mining ArgMin is a fastgrowing field in NLP with applications in decision making and the legal domain (Palau and Moens, 2009) and can be solved on sentence-level (Daxenberger"
W18-5216,2005.mtsummit-papers.11,0,0.0324484,"tions. The difference between MTL and single-task learning (STL) is illustrated in Figure 1. STL is a network with only one task, as in PD3-merge, direct transfer and standard projection. We report average accuracy over five (or 10, in case of very little data) random weight matrix initializations. In the MTL setup, we choose a mini-batch randomly in each iteration (containing instances from only one of the tasks as in our sentence-level ArgMin experiments). Cross-lingual Embeddings: For token-level experiments, we initially train 100-d BIVCD embeddings (Vuli´c and Moens, 2015) from Europarl (Koehn, 2005) (for EN-DE) and the UN corpus (Ziemski et al., 2016) (for EN-FR), respectively. For sentence-level experiments, we use 300-d BIVCD embeddings. This means that we initially assume that high-quality bilingual word embeddings are readily available for the two languages involved. At first sight, this appears a realistic assumption since high-quality bilingual embeddings can already be obtained with very little available bilingual data (Zhang et al., 2016; Artetxe et al., 2017). In low-resource settings, however, even little monolingual data is typically available for L2 and we address this setup"
W18-5216,N16-1030,0,0.0083579,"t or `2 regularization. We report average macro F1 scores over 20 runs with different random initializations. For PD3merge, we shuffle the merged data before training— ˆ data. ˆ S , and D i.e., mini-batches can contain LS , D T For PD3-MTL, we shuffle L1 and L2 data individually and during training we sample each mini-batch from either task according to its size. In the MTL setup, we share the CNN layer across tasks and use task-specific softmax regression layers. Sequence tagging network architecture: For token-level POS tagging, we implement a bidirectional LSTM as in Ma and Hovy (2016) and Lample et al. (2016) with a CRF output layer. This is a state-of-the-art system for sequence tagging tasks such as POS and NER. Our model uses pretrained word embeddings and optionally concatenates these with a learned character-level representation. For all experiments, we use the same network topology: we use two hidden layers with 100 hidden units each, applying dropout on the hidden units and on the word embeddings. We use Adam as optimizer. Our network uses a CRF output layer rather than a softmax classifier to account for dependencies between successive labels. In the MTL setup, we use the same architecture"
W18-5216,N16-1164,0,0.0603917,"Missing"
W18-5216,N18-2006,1,0.911452,"available, the most realistic scenario for many L2 languages. While our approach is general, our focus is particularly on argumentation mining (ArgMin), a rapidly growing research field in NLP. Cross131 Proceedings of the 5th Workshop on Argument Mining, pages 131–143 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics lingual transfer is majorly important for ArgMin because it is inherently costly to get high-quality annotations for ArgMin due to: (i) subjectivity of argumentation as well as divergent and competing ArgMin theories (Daxenberger et al., 2017; Schulz et al., 2018), leading to disagreement among crowdworkers as well as expert annotators (Habernal and Gurevych, 2017), (ii) dependence of argument annotations on background knowledge and parsing of complex pragmatic relations (Moens, 2017). Thus, in order not to reproduce the same annotation costs for new languages, cross-lingual ArgMin methods are required. These techniques should both perform well with little available parallel data, to address many languages, and with general (nonargumentative) parallel data, because this is much more likely to be available. Our experiments address both of these requirem"
W18-5216,P16-2038,0,0.434073,"from projection. Finally, we combine our original dataset LS with ˆ and ˆ S and D the two pseudo-labeled dataset D T train our classifier C on it; after training, our goal in cross-lingual transfer is to apply the trained classifiers to L2 data. We denote this combination operation by ~. A simple approach is to let ~ be the “merging” (or, concatenation) of both datasets (PD3-merge). In ˆ are merged ˆ S and D this variant of PD3, LS , D T into one big dataset on which training takes place. A more sophisticated approach is to let ~ represent a multi-task learning (MTL) scenario (Caruana, 1993; Søgaard and Goldberg, 2016) in which L1 and L2 instances represent one task each (PD3ˆ , ˆ S and D MTL). Here, rather than merging LS , D T ˆ S ) as we treat source language datasets (LS and D ˆ ) as anone task and target language datasets (D T other task, each having a dedicated output layer. This leads to a different network architecture than in PD3-merge, in which we now have two separate output layers (i.e., one for each language); this distinction is also illustrated in Figure 1 below. Thus, for each input instance, we predict two outputs (e.g., two ArgMin labels), one in the source language and one in the target l"
W18-5216,N18-5005,1,0.881065,"Missing"
W18-5216,C14-1142,1,0.89661,"Missing"
W18-5216,J17-3005,1,0.895422,"Missing"
W18-5216,W12-1908,0,0.0234812,"Missing"
W18-5216,Q13-1001,0,0.0516857,"Missing"
W18-5216,N13-1126,0,0.0617233,"Missing"
W18-5216,P15-2118,0,0.056319,"Missing"
W18-5216,P95-1026,0,0.486951,", standard projection ignores the available data in L1 once the L2 dataset has been created and standard direct transfer does not use any L2 information. In this work, we investigate whether the inclusion of both L1 and L2 data outperforms transfer approaches that exploit only one type of such information, and if so, under what conditions. More precisely, we first train a system on shared features as in standard direct transfer on labeled L1 data. Then, we make use of two further datasets. One is based on the source side of parallel unlabeled data; it is derived similarly as in self-training (Yarowsky, 1995) by applying the trained system to unlabeled data, from which a pseudo-labeled dataset is derived. The other is based on its target side—using annotation projections—as in standard projection. Thus, we explore the effects of combining Projection and Direct transfer using three datasets (PD3). Our approach is detailed in §2. We report results for two L2 languages (French, German) on one sentence-level problem (argumentation mining) and one token-level problem (POS tagging). We find that our suggested approach PD3 substantially outperforms both direct transfer and projection when little parallel"
W18-5216,H01-1035,0,0.245235,"ould not only suit English, an arguably particularly simple exemplar of the world’s roughly 7,000 languages. A further motivation for cross-lingual approaches is the fact that many labeled datasets are to this date only available in English and labeled data is generally costly to obtain—be it via expert annotators or through crowd-sourcing. Therefore, methods which are capable of training on labeled data in a resource-rich language such as English and which can then be applied to typically resourcepoor other languages are highly desirable. Two standard cross-lingual approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Agic et al., 2016) and direct transfer (McDonald et al., 2011). Direct transfer trains, in the source language L1, on language-independent or shared features and then directly applies the trained system to the target language of interest L2. In contrast, projection trains and evaluates on L2 itself. To do so, it uses parallel data, applies a system trained on L1 to its source side and then projects the inferred labels to the parallel L2 side. This projection step may involve word alignment information. After projection, an annotated L2 dataset"
W18-5216,N16-1156,0,0.0198624,"Moens, 2009) and can be solved on sentence-level (Daxenberger et al., 2017; Niculae et al., 2017; Stab et al., 2018) or token-level (Eger et al., 2017; Schulz et al., 2018). Cross-lingual ArgMin has recently attracted interest (Aker and Zhang, 2017; Eger et al., 2018). The proposed approaches mostly used machine translation, which is unavailable for the vast majority of the world’s languages. Low-resource transfer Low-resource language transfer has recently become very popular, e.g., when relying on only very few translation pairs for bilingual embedding space induction (Artetxe et al., 2017; Zhang et al., 2016) or in unsupervised machine translation using no parallel sources at all (Artetxe et al., 2018; Lample et al., 2018). Lowresource transfer (on a level of domains rather than languages) has also been considered in ArgMin (Schulz et al., 2018), assuming little annotated data in a new target domain due to annotation costs of ArgMin as a subjective high-level task. 138 7 Concluding Remarks We combined direct transfer with annotation projection, addressing short-comings of both methods and combining their strengths. We saw consistent gains over either of the two methods in isolation, particularly i"
W18-5216,L16-1561,0,0.0222929,"-task learning (STL) is illustrated in Figure 1. STL is a network with only one task, as in PD3-merge, direct transfer and standard projection. We report average accuracy over five (or 10, in case of very little data) random weight matrix initializations. In the MTL setup, we choose a mini-batch randomly in each iteration (containing instances from only one of the tasks as in our sentence-level ArgMin experiments). Cross-lingual Embeddings: For token-level experiments, we initially train 100-d BIVCD embeddings (Vuli´c and Moens, 2015) from Europarl (Koehn, 2005) (for EN-DE) and the UN corpus (Ziemski et al., 2016) (for EN-FR), respectively. For sentence-level experiments, we use 300-d BIVCD embeddings. This means that we initially assume that high-quality bilingual word embeddings are readily available for the two languages involved. At first sight, this appears a realistic assumption since high-quality bilingual embeddings can already be obtained with very little available bilingual data (Zhang et al., 2016; Artetxe et al., 2017). In low-resource settings, however, even little monolingual data is typically available for L2 and we address this setup subsequently. 3 We choose only 800 sentences in order"
W18-5217,D15-1075,0,0.508697,"aim of cross-language learning is to develop ML techniques that exploit annotated resources in a source language to solve tasks in a target language. Eger et al. (2018) propose the first attempt 144 Proceedings of the 5th Workshop on Argument Mining, pages 144–154 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics lated to our task. For instance, recognizing textual entailment (RTE) also focuses on pair classification (Sammons et al., 2012). State-of-theart systems explore complex sentence encoding techniques using a variety of approaches, such as recurrent (Bowman et al., 2015a) and recursive (Bowman et al., 2015b) neural networks, followed by a set of hidden layers (including aggregation functions (Chen et al., 2017; Peters et al., 2018) and attention mechanisms (Rockt¨aschel et al., 2015)). In another line of work, discourse parsing approaches aim to identify the structure of the text in terms of discourse or rhetorical relations between elementary units of text (e.g. propositions). Recent work focuses on building good representations of text relying on neural network architectures (Braud et al., 2017). Some attempts exist to address these related tasks in cross-"
W18-5217,W15-4002,0,0.419905,"aim of cross-language learning is to develop ML techniques that exploit annotated resources in a source language to solve tasks in a target language. Eger et al. (2018) propose the first attempt 144 Proceedings of the 5th Workshop on Argument Mining, pages 144–154 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics lated to our task. For instance, recognizing textual entailment (RTE) also focuses on pair classification (Sammons et al., 2012). State-of-theart systems explore complex sentence encoding techniques using a variety of approaches, such as recurrent (Bowman et al., 2015a) and recursive (Bowman et al., 2015b) neural networks, followed by a set of hidden layers (including aggregation functions (Chen et al., 2017; Peters et al., 2018) and attention mechanisms (Rockt¨aschel et al., 2015)). In another line of work, discourse parsing approaches aim to identify the structure of the text in terms of discourse or rhetorical relations between elementary units of text (e.g. propositions). Recent work focuses on building good representations of text relying on neural network architectures (Braud et al., 2017). Some attempts exist to address these related tasks in cross-"
W18-5217,E17-1028,0,0.0209766,"techniques using a variety of approaches, such as recurrent (Bowman et al., 2015a) and recursive (Bowman et al., 2015b) neural networks, followed by a set of hidden layers (including aggregation functions (Chen et al., 2017; Peters et al., 2018) and attention mechanisms (Rockt¨aschel et al., 2015)). In another line of work, discourse parsing approaches aim to identify the structure of the text in terms of discourse or rhetorical relations between elementary units of text (e.g. propositions). Recent work focuses on building good representations of text relying on neural network architectures (Braud et al., 2017). Some attempts exist to address these related tasks in cross-lingual settings. For RTE there has been work using parallel corpora (Mehdad et al., 2011) and lexical resources (Castillo, 2011), as well as shared tasks (Camacho-Collados et al., 2017). Typically, these systems explore projection approaches and abstract representations that do not require prior translation, namely bilingual dictionaries, syntactic information, statistical knowledge and external knowledge from lexical resources (e.g. ConceptNet, WordNet, BabelNet). More recently, Agic and Schluter (2018) provide multilingual test d"
W18-5217,S17-2002,0,0.0308744,"l., 2018) and attention mechanisms (Rockt¨aschel et al., 2015)). In another line of work, discourse parsing approaches aim to identify the structure of the text in terms of discourse or rhetorical relations between elementary units of text (e.g. propositions). Recent work focuses on building good representations of text relying on neural network architectures (Braud et al., 2017). Some attempts exist to address these related tasks in cross-lingual settings. For RTE there has been work using parallel corpora (Mehdad et al., 2011) and lexical resources (Castillo, 2011), as well as shared tasks (Camacho-Collados et al., 2017). Typically, these systems explore projection approaches and abstract representations that do not require prior translation, namely bilingual dictionaries, syntactic information, statistical knowledge and external knowledge from lexical resources (e.g. ConceptNet, WordNet, BabelNet). More recently, Agic and Schluter (2018) provide multilingual test data for four major languages (Arabic, French, Spanish and Russian) and baseline cross-language RTE models. Preliminary work shows that projection approaches work better in cross-lingual settings than direct transfer. be obtained for less-resourced"
W18-5217,P17-1152,0,0.407793,"age. Eger et al. (2018) propose the first attempt 144 Proceedings of the 5th Workshop on Argument Mining, pages 144–154 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics lated to our task. For instance, recognizing textual entailment (RTE) also focuses on pair classification (Sammons et al., 2012). State-of-theart systems explore complex sentence encoding techniques using a variety of approaches, such as recurrent (Bowman et al., 2015a) and recursive (Bowman et al., 2015b) neural networks, followed by a set of hidden layers (including aggregation functions (Chen et al., 2017; Peters et al., 2018) and attention mechanisms (Rockt¨aschel et al., 2015)). In another line of work, discourse parsing approaches aim to identify the structure of the text in terms of discourse or rhetorical relations between elementary units of text (e.g. propositions). Recent work focuses on building good representations of text relying on neural network architectures (Braud et al., 2017). Some attempts exist to address these related tasks in cross-lingual settings. For RTE there has been work using parallel corpora (Mehdad et al., 2011) and lexical resources (Castillo, 2011), as well as s"
W18-5217,L18-1614,0,0.0150239,"on neural network architectures (Braud et al., 2017). Some attempts exist to address these related tasks in cross-lingual settings. For RTE there has been work using parallel corpora (Mehdad et al., 2011) and lexical resources (Castillo, 2011), as well as shared tasks (Camacho-Collados et al., 2017). Typically, these systems explore projection approaches and abstract representations that do not require prior translation, namely bilingual dictionaries, syntactic information, statistical knowledge and external knowledge from lexical resources (e.g. ConceptNet, WordNet, BabelNet). More recently, Agic and Schluter (2018) provide multilingual test data for four major languages (Arabic, French, Spanish and Russian) and baseline cross-language RTE models. Preliminary work shows that projection approaches work better in cross-lingual settings than direct transfer. be obtained for less-resourced languages. For that, we propose employing cross-language learning techniques, such as projection (Yarowsky et al., 2001) and direct transfer (McDonald et al., 2011). We show promising results following the approach presented in this paper, by obtaining performance scores in an unsupervised cross-language setting that are c"
W18-5217,D17-1144,0,0.0259419,"5), namely: text segmentation, identification of ADUs, ADU type classification, relation identification, and relation type classification. Addressing argumentative relation identification in isolation, Nguyen and Litman (2016) adopt a feature-based approach including lexical (unigrams), syntactic (part-of-speech, production rules), discourse indicators (PDTB relations) and topic-context features. Recent works address the task through deep learning architectures. Bosc et al. (2016) employ an encoder-decoder architecture and two distinct LSTMs to identify support and attack relations on tweets. Cocarascu and Toni (2017) follow architectures used for the recognizing textual entailment task, reporting results that substantially improve accuracy as compared to a feature-based ML approach on the same corpus. Other approaches model the problem jointly with previous subtasks of AM. Stab and Gurevych (2017) follow a feature-based approach employing features at different levels of abstraction and integer linear programming for joint optimization of the subtasks. Eger et al. (2017) propose an end-to-end AM system by framing the task as a token-level dependency parser and sequence tagging problem. Potash et al. (2017)"
W18-5217,D17-1070,0,0.0245765,"g et al., 2016). This attention mechanism uses the information encoded in LST M (ADUs ) to inform which of the words in ADUt the model should pay more attention to, given ADUs . By employing this attention mechanism, we obtain a weighted input embeddings repet The final resentation of ADUt , represented as x hidden state used as the encoding of the tuple is obtained by applying a LSTM over the weighted et ). representation of ADUt : LST M (x Methods Similarly to approaches that aim to learn universal sentence representations able to capture the semantics of the sentence (Bowman et al., 2015b; Conneau et al., 2017), we explore different deep learning architectures to encode the meaning of ADUs for the task of argumentative relation identification. To help replicate our results, we publish the code used in this work1 . We propose five neural network architectures that differ in the sentence encoding techniques employed (as described in Section 4.1), to which we add a fully-connected hidden layer with the same dimension as the output of the sentence encoding component, followed by a softmax layer to obtain the final predictions. To prevent the model from overfitting, we apply dropout (Srivastava et al., 2"
W18-5217,P17-1002,1,0.77864,"sc et al. (2016) employ an encoder-decoder architecture and two distinct LSTMs to identify support and attack relations on tweets. Cocarascu and Toni (2017) follow architectures used for the recognizing textual entailment task, reporting results that substantially improve accuracy as compared to a feature-based ML approach on the same corpus. Other approaches model the problem jointly with previous subtasks of AM. Stab and Gurevych (2017) follow a feature-based approach employing features at different levels of abstraction and integer linear programming for joint optimization of the subtasks. Eger et al. (2017) propose an end-to-end AM system by framing the task as a token-level dependency parser and sequence tagging problem. Potash et al. (2017) use an encoder-decoder problem formulation by employing a pointer network based deep neural network architecture. The results reported by Potash et al. (0.767 macro F1-score) constitute the current state-of-the-art on the Persuasive Essays corpus (Stab and Gurevych, 2017) for the subtask of argumentative relation identification. Related work aiming to capture relations between elementary units of texts is closely reDespite the similarity between the tasks o"
W18-5217,D11-1006,0,0.180075,"dictionaries, syntactic information, statistical knowledge and external knowledge from lexical resources (e.g. ConceptNet, WordNet, BabelNet). More recently, Agic and Schluter (2018) provide multilingual test data for four major languages (Arabic, French, Spanish and Russian) and baseline cross-language RTE models. Preliminary work shows that projection approaches work better in cross-lingual settings than direct transfer. be obtained for less-resourced languages. For that, we propose employing cross-language learning techniques, such as projection (Yarowsky et al., 2001) and direct transfer (McDonald et al., 2011). We show promising results following the approach presented in this paper, by obtaining performance scores in an unsupervised cross-language setting that are competitive (and in some settings better) than fully-supervised in-language ML approaches. To the best of our knowledge, this is the first approach to consider the task of argumentative relation identification in a cross-lingual setting. 2 Related Work The full process of AM can be decomposed into several subtasks (Peldszus and Stede, 2015), namely: text segmentation, identification of ADUs, ADU type classification, relation identificati"
W18-5217,C18-1071,1,0.773268,"to identify argumentative content from manually annotated examples. Building a corpus with reliably annotated arguments is a challenging and time-consuming task, due to its complexity (Habernal et al., 2014). Consequently, training data for AM is scarce, in particular for less-resourced languages. To overcome the lack of annotated resources for AM in lessresourced languages, we explore cross-language learning approaches (Xiao and Guo, 2013). The aim of cross-language learning is to develop ML techniques that exploit annotated resources in a source language to solve tasks in a target language. Eger et al. (2018) propose the first attempt 144 Proceedings of the 5th Workshop on Argument Mining, pages 144–154 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics lated to our task. For instance, recognizing textual entailment (RTE) also focuses on pair classification (Sammons et al., 2012). State-of-theart systems explore complex sentence encoding techniques using a variety of approaches, such as recurrent (Bowman et al., 2015a) and recursive (Bowman et al., 2015b) neural networks, followed by a set of hidden layers (including aggregation functions (Chen et al., 2017; Pete"
W18-5217,P11-1134,0,0.0222076,"set of hidden layers (including aggregation functions (Chen et al., 2017; Peters et al., 2018) and attention mechanisms (Rockt¨aschel et al., 2015)). In another line of work, discourse parsing approaches aim to identify the structure of the text in terms of discourse or rhetorical relations between elementary units of text (e.g. propositions). Recent work focuses on building good representations of text relying on neural network architectures (Braud et al., 2017). Some attempts exist to address these related tasks in cross-lingual settings. For RTE there has been work using parallel corpora (Mehdad et al., 2011) and lexical resources (Castillo, 2011), as well as shared tasks (Camacho-Collados et al., 2017). Typically, these systems explore projection approaches and abstract representations that do not require prior translation, namely bilingual dictionaries, syntactic information, statistical knowledge and external knowledge from lexical resources (e.g. ConceptNet, WordNet, BabelNet). More recently, Agic and Schluter (2018) provide multilingual test data for four major languages (Arabic, French, Spanish and Russian) and baseline cross-language RTE models. Preliminary work shows that projection approa"
W18-5217,P16-1190,0,0.0268747,"projection of the labels is trivial (no token level alignment is required). Mandatory for the direct transfer approach is the existence of cross-lingual word embeddings, which are trained to obtain a shared embedding space representation of words in different languages. With them, we are able to employ techniques based on word embeddings across different languages. Similarly to monolingual word embeddings, various approaches for learning cross-lingual word embeddings have been proposed in recent years (Ruder, 2017). In this paper, we use pre-trained multilingual embeddings publicly available (Ferreira et al., 2016). The embeddings were obtained by combining parallel data from the TED Corpus with pretrained English GloVe embeddings6 . Each embedding contains 300 dimensions. Cross-Language Learning Techniques Several approaches have been presented for crosslanguage learning, including projection, direct transfer, and feature space analysis. As a convention, LS denotes the source language (in which most of the annotated data is available) and LT the target language (in which the capability of the system to perform cross-language adaptation will be evaluated, typically containing few or no labeled data). In"
W18-5217,P16-1107,0,0.022343,"scores in an unsupervised cross-language setting that are competitive (and in some settings better) than fully-supervised in-language ML approaches. To the best of our knowledge, this is the first approach to consider the task of argumentative relation identification in a cross-lingual setting. 2 Related Work The full process of AM can be decomposed into several subtasks (Peldszus and Stede, 2015), namely: text segmentation, identification of ADUs, ADU type classification, relation identification, and relation type classification. Addressing argumentative relation identification in isolation, Nguyen and Litman (2016) adopt a feature-based approach including lexical (unigrams), syntactic (part-of-speech, production rules), discourse indicators (PDTB relations) and topic-context features. Recent works address the task through deep learning architectures. Bosc et al. (2016) employ an encoder-decoder architecture and two distinct LSTMs to identify support and attack relations on tweets. Cocarascu and Toni (2017) follow architectures used for the recognizing textual entailment task, reporting results that substantially improve accuracy as compared to a feature-based ML approach on the same corpus. Other approa"
W18-5217,D16-1244,0,0.0919883,"Missing"
W18-5217,D15-1110,0,0.17017,"age learning setting. In this paper, we aim to employ existing state-of-theart cross-language learning techniques to address the task of argumentative relation identification, leveraging knowledge extracted from annotated corpora in English to address the task in a lessresourced language, such as Portuguese. As it may be costly to produce small amounts of training data in many different languages, we employ unsupervised language adaptation techniques, which do not require labeled data in the target language. The aim of argumentative relation identification, the last subtask of the AM process (Peldszus and Stede, 2015), is to classify each argumentative discourse unit (ADU) pair as argumentatively related or not. We assume that the subtask of text segmentation in ADUs is already solved (although no ADU classification is assumed). The task is formulated as a binary classification problem: given a tuple hADUs , ADUt i, we aim to classify the relation from ADUs to ADUt as “support” (where ADUs plays the role of premise and ADUt plays the role of conclusion), or “none” (unrelated ADUs). This is a consistent way of formulating the problem (i.e. the premise on the left and conclusion on the right side of the tupl"
W18-5217,N18-1202,0,0.364876,"018) propose the first attempt 144 Proceedings of the 5th Workshop on Argument Mining, pages 144–154 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics lated to our task. For instance, recognizing textual entailment (RTE) also focuses on pair classification (Sammons et al., 2012). State-of-theart systems explore complex sentence encoding techniques using a variety of approaches, such as recurrent (Bowman et al., 2015a) and recursive (Bowman et al., 2015b) neural networks, followed by a set of hidden layers (including aggregation functions (Chen et al., 2017; Peters et al., 2018) and attention mechanisms (Rockt¨aschel et al., 2015)). In another line of work, discourse parsing approaches aim to identify the structure of the text in terms of discourse or rhetorical relations between elementary units of text (e.g. propositions). Recent work focuses on building good representations of text relying on neural network architectures (Braud et al., 2017). Some attempts exist to address these related tasks in cross-lingual settings. For RTE there has been work using parallel corpora (Mehdad et al., 2011) and lexical resources (Castillo, 2011), as well as shared tasks (Camacho-C"
W18-5217,D17-1143,0,0.0212879,"ascu and Toni (2017) follow architectures used for the recognizing textual entailment task, reporting results that substantially improve accuracy as compared to a feature-based ML approach on the same corpus. Other approaches model the problem jointly with previous subtasks of AM. Stab and Gurevych (2017) follow a feature-based approach employing features at different levels of abstraction and integer linear programming for joint optimization of the subtasks. Eger et al. (2017) propose an end-to-end AM system by framing the task as a token-level dependency parser and sequence tagging problem. Potash et al. (2017) use an encoder-decoder problem formulation by employing a pointer network based deep neural network architecture. The results reported by Potash et al. (0.767 macro F1-score) constitute the current state-of-the-art on the Persuasive Essays corpus (Stab and Gurevych, 2017) for the subtask of argumentative relation identification. Related work aiming to capture relations between elementary units of texts is closely reDespite the similarity between the tasks of argumentative relation identification and RTE, since both tasks are grounded in different conceptual frameworks, the inherent semantic r"
W18-5217,H01-1035,0,0.592269,"require prior translation, namely bilingual dictionaries, syntactic information, statistical knowledge and external knowledge from lexical resources (e.g. ConceptNet, WordNet, BabelNet). More recently, Agic and Schluter (2018) provide multilingual test data for four major languages (Arabic, French, Spanish and Russian) and baseline cross-language RTE models. Preliminary work shows that projection approaches work better in cross-lingual settings than direct transfer. be obtained for less-resourced languages. For that, we propose employing cross-language learning techniques, such as projection (Yarowsky et al., 2001) and direct transfer (McDonald et al., 2011). We show promising results following the approach presented in this paper, by obtaining performance scores in an unsupervised cross-language setting that are competitive (and in some settings better) than fully-supervised in-language ML approaches. To the best of our knowledge, this is the first approach to consider the task of argumentative relation identification in a cross-lingual setting. 2 Related Work The full process of AM can be decomposed into several subtasks (Peldszus and Stede, 2015), namely: text segmentation, identification of ADUs, AD"
W18-5217,D17-1035,1,0.795764,"espond to the sum of the confusion matrix from the test set predictions in each fold (Forman and Scholz, 2010). Following this procedure, we obtain final evaluation metrics for the full dataset in LT that are directly comparable with the scores reported on the full dataset for LT in cross-language experiments, as the evaluation scores are obtained from exactly the same data in both settings. Cross-validation splits are also at the document-level and keep the original label distribution. Since reporting single performance scores is insufficient to compare non-deterministic learning approaches (Reimers and Gurevych, 2017), we report average scores of 10 runs with different random seeds. Due to the unbalanced nature of the datasets, evaluation metrics reported in the experiments are average macro F1-scores over all 10 runs. All models are trained using the Adam optimizer, using the default parameters suggested in the original paper (Kingma and Ba, 2014), and cross-entropy loss function. The activation function used in all the layers was ReLU (Glorot et al., 2011). To find the best model in each run, we stop training once the accuracy on the validation set does not improve for 5 epochs (early-stop criterion) or"
W18-5217,J17-3005,1,0.919369,"syntactic (part-of-speech, production rules), discourse indicators (PDTB relations) and topic-context features. Recent works address the task through deep learning architectures. Bosc et al. (2016) employ an encoder-decoder architecture and two distinct LSTMs to identify support and attack relations on tweets. Cocarascu and Toni (2017) follow architectures used for the recognizing textual entailment task, reporting results that substantially improve accuracy as compared to a feature-based ML approach on the same corpus. Other approaches model the problem jointly with previous subtasks of AM. Stab and Gurevych (2017) follow a feature-based approach employing features at different levels of abstraction and integer linear programming for joint optimization of the subtasks. Eger et al. (2017) propose an end-to-end AM system by framing the task as a token-level dependency parser and sequence tagging problem. Potash et al. (2017) use an encoder-decoder problem formulation by employing a pointer network based deep neural network architecture. The results reported by Potash et al. (0.767 macro F1-score) constitute the current state-of-the-art on the Persuasive Essays corpus (Stab and Gurevych, 2017) for the subt"
W18-5217,P16-1122,0,0.0338487,"using CNNs is the fact that they can model the sequence of words by processing subsequences in parallel to obtain a final higher-level representation of the sentence. This is a promising approach when dealing with text in different languages, where the order of words are different. Inner-Att. Inspired by previous successful work using attention (Bahdanau et al., 2014; Stab et al., 2018) in several NLP applications, we propose an attention-based sentence encoding that learns the importance of weighting ADUt depending on the content of ADUs . We adopt an innerattention mechanism as proposed by Wang et al. (2016). First, we encode ADUs using a LSTM. Then, we determine the importance weighting on the input sequence ADUt instead of on the hidden states of the LST M (ADUt ): this has been shown to prevent biased importance weights towards the end of a sequence (Wang et al., 2016). This attention mechanism uses the information encoded in LST M (ADUs ) to inform which of the words in ADUt the model should pay more attention to, given ADUs . By employing this attention mechanism, we obtain a weighted input embeddings repet The final resentation of ADUt , represented as x hidden state used as the encoding of"
W18-5516,D15-1075,0,0.0681706,"an entity. Candidate article search: We use the MediaWiki API3 to search through the titles of all Wikipedia articles for matches with the potential entity mentions found in the claim. The MediaWiki API uses the Wikipedia search engine to find matching articles. The top match is the article whose title has the largest overlap with the query. For each entity mention, we store the seven highest-ranked Wikipedia article matches. The MediaWiki API uses the online version of Wikipedia and since there are some discrepancies Enhanced Sequential Inference Model Originally developed for the SNLI task (Bowman et al., 2015) of determining entailment between two statements, the ESIM (Enhanced Sequential Inference Model) (Chen et al., 2016) creates a rich representation of statement-pairs. Since the FEVER task requires the handling of claimsentence pairs, we use the ESIM as the basis for both sentence selection and textual entailment. The ESIM solves the entailment problem in three consecutive steps, taking two statements as input. Input encoding: Using a bidirectional LSTM (BiLSTM) (Graves and Schmidhuber, 2005), representations of the individual tokens of the two input statements are computed. Local inference mo"
W18-5516,D07-1074,0,0.117948,"present underlying methods that we adopted for the development of our system. 2.1 3 Entity linking In this section, we describe the models that we developed for the three FEVER sub-tasks. The document retrieval step requires matching a given claim with the content of a Wikipedia article. A claim frequently features one or multiple entities that form the main content of the claim. Furthermore, Wikipedia can be viewed as a knowledge base, where each article describes a particular entity, denoted by the article title. Thus, the document retrieval step can be framed as an entity linking problem (Cucerzan, 2007). That is, identifying entity mentions in the claim and linking them to the Wikipedia articles of this entity. The linked Wikipedia articles can then be used as the set of the retrieved documents for the subsequent steps. 2.2 Our system for fact extraction and claim verification 3.1 Document retrieval As explained in Section 2.1, we propose an entity linking approach to the document retrieval subtask. That is, we find entities in the claims that match the titles of Wikipedia articles (documents). Following the typical entity linking pipeline, we develop a document retrieval component that has"
W18-5516,D14-1162,0,0.0809006,"tiveness of our ad-hoc entity linking approach (see Section 4). 3.2 3.3 Recognizing textual entailment In order to classify the claim as Supported, Ref uted or N otEnoughInf o, we use the five sentences retrieved by our sentence selection model described in the previous section. For the classification, we propose another extension to the ESIM, which can predict the entailment relation between multiple input sentences and the claim. Fig. 2 gives an overview of our extended ESIM for the FEVER textual entailment task. As word representation for both claim and sentences, we concatenate the Glove (Pennington et al., 2014) and FastText (Bojanowski et al., 2016) embeddings for each word. Since both types of embeddings are pretrained on Wikipedia, they are particularly suitable for our problem setting. Sentence selection In this step, we select candidate sentences as a potential evidence set for a claim from the Wikipedia articles retrieved in the previous step. This is achieved by extending the ESIM to generate a ranking score on the basis of two input statements, instead of predicting the entailment relation between these two statements. Architecture: The modified ESIM takes as input a claim and a sentence. To"
W18-5516,S18-2007,1,0.829524,"ference Local Inference Input Encoding Input Encoding claim evidence sentences claim sampled sentences Figure 1: Sentence selection model between the 2017 dump used in the shared task and the latest version, we also perform an exact search over all Wikipedia article titles in the dump. We add these results to the set of the retrieved articles. Candidate filtering: The MediaWiki API retrieves articles whose title overlaps with the query. Thus, the results may contain articles with a title longer or shorter than the entity mention used in the query. Similarly to previous work on entity linking (Sorokin and Gurevych, 2018), we remove results that are longer than the entity mention and do not overlap with the rest of the claim. To check this overlap, we first remove the content in parentheses from the Wikipedia article titles and stem the remaining words in the titles and the claim. Then, we discard a Wikipedia article if its stemmed article title is not completely included in the stemmed claim. We collect all retrieved Wikipedia articles for all identified entity mentions in the claim after filtering and supply them to the next step in the pipeline. The evaluation of the document retrieval system on the develop"
W18-5516,N18-1074,0,0.0717481,"Missing"
W18-6557,W14-4012,0,0.144658,"Missing"
W18-6557,D14-1179,0,0.0230283,"Missing"
W18-6557,P16-2008,0,0.158496,"Missing"
W18-6557,C16-1105,0,0.0584027,"Missing"
W18-6557,D17-1238,0,0.185773,"Missing"
W18-6557,W17-5525,0,0.140711,"Missing"
W18-6557,P02-1040,0,0.10106,"estaurant domain (Novikova et al., 2017b). Each training instance consists of a dialogue act-based meaning representation (MR) and up to 16 references in natural language (Figure 1). The data was collected using pictorial representations as stimuli, with the intention of creating more natural, informative and diverse human references compared to the ones one might generate from textual inputs. The task is to generate an utterance from a given MR, which is both similar to human-generated reference texts and highly rated by humans. Similarity is assessed using standard evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Lavie and Agarwal, 2007), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015). However, the final assessment is done via human ratings obtained using a mixture of crowdsourcing and expert annotations. To facilitate a better assessment of the proposed approaches, the organizing team used TGen (Duˇsek and Jurcicek, 2016), one of the recent E2E datadriven systems, as a baseline. It is a sequence-tosequence neural system with attention (Bahdanau et al., 2014). TGen uses beam search for decod463 Proceedings of The 11th International Natural Language Generation Conf"
W18-6557,D17-1035,1,0.826581,"hich activate each component depending on whether an MR attribute is part of the input. For example, if TGen Finally, we also add a simple post-processing step to handle specific punctuation and article choices. 3 Metric Evaluation Table 2 shows the results of metric evaluation of the systems. Since we were provided with only one TGen prediction file and a single performance score, comparing score distributions is not possible and statistical significance tests are not meaningful due to the non-deterministic nature of the approaches based on neural networks and randomized training procedures (Reimers and Gurevych, 2017). In order to facilitate a fair comparison with other competing systems, we report the mean development score of Model-D (averaged across twenty runs with different random seeds) and performance variance for each automatic metric. Model-T is a deterministic system, so it is sufficient to report the results of a single run. The results show that Model-D outperforms 465 Error type dropped contents punctuation errors modified contents bad grammar TGen Model-D Model-T 9 1 4 4 49 12 4 1 0 0 0 0 training data and find out if the discrepancies of Model-D were learned from the data. 4 Table 3: Common"
W18-6557,J09-4008,0,0.108056,"Missing"
W18-6557,W07-0734,0,0.0270477,"instance consists of a dialogue act-based meaning representation (MR) and up to 16 references in natural language (Figure 1). The data was collected using pictorial representations as stimuli, with the intention of creating more natural, informative and diverse human references compared to the ones one might generate from textual inputs. The task is to generate an utterance from a given MR, which is both similar to human-generated reference texts and highly rated by humans. Similarity is assessed using standard evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Lavie and Agarwal, 2007), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015). However, the final assessment is done via human ratings obtained using a mixture of crowdsourcing and expert annotations. To facilitate a better assessment of the proposed approaches, the organizing team used TGen (Duˇsek and Jurcicek, 2016), one of the recent E2E datadriven systems, as a baseline. It is a sequence-tosequence neural system with attention (Bahdanau et al., 2014). TGen uses beam search for decod463 Proceedings of The 11th International Natural Language Generation Conference, pages 463–471, c Tilburg, The Netherlands, November"
W18-6557,W14-3301,0,0.0370992,"Missing"
W18-6557,W04-1013,0,0.024654,"-based meaning representation (MR) and up to 16 references in natural language (Figure 1). The data was collected using pictorial representations as stimuli, with the intention of creating more natural, informative and diverse human references compared to the ones one might generate from textual inputs. The task is to generate an utterance from a given MR, which is both similar to human-generated reference texts and highly rated by humans. Similarity is assessed using standard evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Lavie and Agarwal, 2007), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015). However, the final assessment is done via human ratings obtained using a mixture of crowdsourcing and expert annotations. To facilitate a better assessment of the proposed approaches, the organizing team used TGen (Duˇsek and Jurcicek, 2016), one of the recent E2E datadriven systems, as a baseline. It is a sequence-tosequence neural system with attention (Bahdanau et al., 2014). TGen uses beam search for decod463 Proceedings of The 11th International Natural Language Generation Conference, pages 463–471, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Asso"
W18-6557,N16-1086,0,0.065489,"e Eagle near Burger King.” Figure 1: E2E NLG Challenge data specification. 1.1 Introduction Natural Language Generation (NLG) is the task of generating natural language utterances from structured data representations. The E2E NLG Challenge2 is a shared task which focuses on end-toend data-driven NLG methods. These approaches attract a lot of attention, because they perform joint learning of textual structure and surface realization patterns from non-aligned data, which allows for a significant reduction of the amount of human annotation effort needed for NLG corpus creation (Wen et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Lampouras and Vlachos, 2016). The contribution of our submission to the challenge can be summarized as follows: (1) we show how exploiting data properties allows us to design more accurate neural architectures; (2) we develop a simple template-based system which achieves performance comparable to neural approaches. 1 https://github.com/UKPLab/e2e-nlgchallenge-2017 2 http://www.macs.hw.ac.uk/ InteractionLab/E2E eatType[coffee shop] priceRange[moderate] area[riverside] near[Burger King] Task Definition The organizers of the shared task provided a crowdsourced data se"
W18-6557,D15-1199,0,0.189668,"Missing"
W19-4308,D18-2029,0,0.0448298,"Missing"
W19-4308,L18-1269,0,0.0526912,"Conneau et al., 2018). Our motivation is to assemble diverse observations from different published works regarding problematic aspects of the emerging field of sentence encoders. We do so in order to provide future research with an easy-to-access reference about issues that may not (yet) be widely known. We also want to provide the newcomer to sentence encoders a guide for avoiding pitfalls that even experienced researchers have fallen prey to. We also recommend best practices, from our viewpoint. 2 Setup We compare several freely available sentence encoders (listed in Table 1) with SentEval (Conneau and Kiela, 2018), using its default settings. SentEval trains a logistic regression classifier for specific downstream tasks with the sentence embeddings as the input. We compare 6 downstream tasks from the fields of sentiment analysis (MR, SST), product reviews (CR), subjectivity (SUBJ), opinion polarity (MPQA), and question-type classification (TREC). In these tasks, the goal is to label a single sentence with one of several classes. We also evaluate on the STSBenchmark (Cer et al., 2017), which evaluates semantic similarity of pairs of sentences. Sentence Encoder InferSent (Conneau et al., 2017) Sent2Vec ("
W19-4308,D17-1070,0,0.526995,"e in a variety of contexts: (i) clustering of sentences and short texts; (ii) retrieval tasks, e.g., retrieving answer passages for a question; and (iii) when task-specific training data is scarce—i.e., when the full potential of task-specific word-level representation approaches cannot be leveraged (Subramanian et al., 2018). The popularity of sentence encoders has led to a large variety of proposed techniques. These range from ‘complex’ unsupervised RNN models predicting context sentences (Kiros et al., 2015) to supervised RNN models predicting semantic relationships between sentence pairs (Conneau et al., 2017). Even more complex models learn sentence embeddings in a multi-task setup (Subramanian et al., 2018). In contrast, ‘simple’ encoders compute sentence embeddings as an elementary function of word embeddings. They compute a weighted average of word embeddings and then modify these representations via principal component analysis (SIF) (Arora et al., 2017); average n-gram embeddings (Sent2Vec) (Pagliardini et al., 2018); consider generalized pooling mechanisms (Shen et al., 2018; R¨uckl´e et al., 2018); or combine word embeddings via randomly initialized projection matrices (Wieting and Kiela, 2"
W19-4308,P18-1198,0,0.346707,"er average word embeddings. Therefore, we strongly encourage future research to compare embeddings of the same sizes to provide a fair evaluation (or at least similar sizes). We compile several pitfalls when evaluating and comparing sentence encoders. These relate to (i) the embedding sizes, (ii) normalization of embeddings before feeding them to classifiers, and (iii) unsupervised semantic similarity evaluation. We also discuss (iv) the choice of classifier used on top of sentence embeddings and (v) divergence in performance results which compare downstream tasks and so-called probing tasks (Conneau et al., 2018). Our motivation is to assemble diverse observations from different published works regarding problematic aspects of the emerging field of sentence encoders. We do so in order to provide future research with an easy-to-access reference about issues that may not (yet) be widely known. We also want to provide the newcomer to sentence encoders a guide for avoiding pitfalls that even experienced researchers have fallen prey to. We also recommend best practices, from our viewpoint. 2 Setup We compare several freely available sentence encoders (listed in Table 1) with SentEval (Conneau and Kiela, 20"
W19-4308,W16-2506,0,0.0170749,"at least logistic regression and MLP. 58 References et al. (2018), where WC had the lowest correlation. Thus, it remains unclear to which extent downstram tasks benefit from the different properties that are defined by many probing tasks. 4 Frank J. Anscombe. 1973. Graphs in Statistical Analysis. The American Statistician, 27(1):17–21. Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A Simple but Tough-to-Beat Baseline for Sentence Embeddings. In International Conference on Learning Representations (ICLR 2017). Conclusion Others have laid out problems with the evaluation of word embeddings (Faruqui et al., 2016) using word similarity tasks. They referred to the vagueness of the data underlying the tasks (as well as its annotations), the low correlations between extrinsic and intrinsic evaluations, and the lack of statistical tests. Our critique differs (in part) from this in that we also address extrinsic evaluation and the evaluation techniques themselves,3 and in that we believe that the comparison between sentence embeddings is not always fair, especially given the current evaluations using logistic regression. This implicitly favors larger embeddings, and may therefore result in misleading conclu"
W19-4308,D17-1035,1,0.858614,"ngs. 1 Introduction The field of natural language processing (NLP) is currently in upheaval. A reason for this is the success story of deep learning, which has led to ever better reported performances across many different NLP tasks, sometimes exceeding the scores achieved by humans. These fanfares of victory are echoed by isolated voices raising concern about the trustworthiness of some of the reported results. For instance, Melis et al. (2017) find that neural language models have been misleadingly evaluated and that, under fair conditions, standard LSTMs outperform more recent innovations. Reimers and Gurevych (2017) find that reporting single performance scores is insufficient for comparing nondeterministic approaches such as neural networks. Post (2018) holds that neural MT systems are unfairly compared in the literature using different variants of the BLEU score metric. In an even more general context, Lipton and Steinhardt (2018) detect several current “troubling trends” in machine learning scholarship, some of which refer to evaluation. 55 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 55–60 c Florence, Italy, August 2, 2019. 2019 Association for Computation"
W19-4308,D18-1512,0,0.0575982,"Missing"
W19-4308,P18-1041,0,0.0381135,"Missing"
W19-4308,N15-1028,0,0.0614376,"Missing"
W19-4308,D18-1425,0,0.0142195,"es when the ranking of systems is not stable across different classifiers. To our knowledge, this is an open issue. We are only aware of Subramanian et al. (2018), who evaluate a few setups both with logistic regression and using an MLP, and their results indicate that their own approach profits much more from the MLP than the InferSent embeddings they compare to (+3.4pp vs. +2.2pp). Thus, future research might focus on more suitable (difficult) datasets and sentence classification tasks for the evaluation of sentence embeddings, a lesson already learned in other fields (L¨aubli et al., 2018; Yu et al., 2018). Importantly, depending on the set of evaluated sentence encoders, such correlations can yield contradictory outcomes. For example, Conneau et al. (2018) evaluate more than 40 combinations of similar sentence encoder architectures and observe the strongest correlation with downstream task performances for WC (cf. their figure 2). This is in contrast to the correlations from the results of Perone Thus, it is not sufficient to only report results with logistic regression, and evaluations with betterperforming approaches would provide a more realistic comparison for actual use-case scenarios. We"
W19-4308,N18-1049,0,0.393004,"hese range from ‘complex’ unsupervised RNN models predicting context sentences (Kiros et al., 2015) to supervised RNN models predicting semantic relationships between sentence pairs (Conneau et al., 2017). Even more complex models learn sentence embeddings in a multi-task setup (Subramanian et al., 2018). In contrast, ‘simple’ encoders compute sentence embeddings as an elementary function of word embeddings. They compute a weighted average of word embeddings and then modify these representations via principal component analysis (SIF) (Arora et al., 2017); average n-gram embeddings (Sent2Vec) (Pagliardini et al., 2018); consider generalized pooling mechanisms (Shen et al., 2018; R¨uckl´e et al., 2018); or combine word embeddings via randomly initialized projection matrices (Wieting and Kiela, 2019). The embeddings of different encoders vary across various dimensions, the most obvious being their size. E.g., the literature has proposed embeddings ranging from 300d average word embeddings to 700d n-gram embeddings, to 4096d InferSent embeddings, to 24k dimensional random embeddings (Wieting and Kiela, 2019). Unsurprisingly, comparing embeddings of different sizes is unfair when size itself is crucially relate"
W19-4308,D14-1162,0,0.0942991,"downstream tasks with the sentence embeddings as the input. We compare 6 downstream tasks from the fields of sentiment analysis (MR, SST), product reviews (CR), subjectivity (SUBJ), opinion polarity (MPQA), and question-type classification (TREC). In these tasks, the goal is to label a single sentence with one of several classes. We also evaluate on the STSBenchmark (Cer et al., 2017), which evaluates semantic similarity of pairs of sentences. Sentence Encoder InferSent (Conneau et al., 2017) Sent2Vec (Pagliardini et al., 2018) PMeans (R¨uckl´e et al., 2018) USE (Cer et al., 2018) Avg. Glove (Pennington et al., 2014) Avg. Word2Vec (Mikolov et al., 2013) SIF-Glove (Arora et al., 2017) Problems Cosine similarity and Pearson correlation may give misleading results. The following evaluation scenario is common when testing for semantic similarity: given two inputs (words or sentences), embed each of them, (i) compute the cosine similarity of the pairs of vectors, and then (ii) calculate the Pearson (or Spearman) correlation with human judgments for the same pairs. Both steps are problematic: (i) it is unclear whether cosine similarity is better suited to measure semantic similarity than other similarity functi"
W19-4308,W18-6319,0,0.0149364,"as led to ever better reported performances across many different NLP tasks, sometimes exceeding the scores achieved by humans. These fanfares of victory are echoed by isolated voices raising concern about the trustworthiness of some of the reported results. For instance, Melis et al. (2017) find that neural language models have been misleadingly evaluated and that, under fair conditions, standard LSTMs outperform more recent innovations. Reimers and Gurevych (2017) find that reporting single performance scores is insufficient for comparing nondeterministic approaches such as neural networks. Post (2018) holds that neural MT systems are unfairly compared in the literature using different variants of the BLEU score metric. In an even more general context, Lipton and Steinhardt (2018) detect several current “troubling trends” in machine learning scholarship, some of which refer to evaluation. 55 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 55–60 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 3 in downstream tasks, as has been highlighted before (R¨uckl´e et al., 2018; Wieting and Kiela, 2019). Size matters. Currentl"
W19-4308,S17-2001,0,\N,Missing
W19-8635,W18-3606,0,0.230378,"inlin 268 Proceedings of The 12th International Conference on Natural Language Generation, pages 268–278, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Data split Train Dev Test ar cs en es 6,016 897 676 66,485 9,016 9,876 12,375 1,978 2,061 14,289 1,651 1,719 Language fi fr 12,030 1,336 1,525 14,529 1,473 416 it nl pt ru 12,796 562 480 12,318 720 685 8,325 559 476 48,119 6,441 6,366 Table 1: Number of sentences in SR’18 datasets (Mille et al., 2018). 2 Task Description techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). The former approaches traverse the input tree, encode nodes using sparse manually defined feature sets as input representations and generate a sentence by extending a candidate hypothesis with an input word that has the highest score among other input words that have not yet been processed. These approaches rely on the observation that natural language production has a preference for shorter dependencies (Gibson, 2000; White and Rajkumar, 2012; King and White, 2018), which facilitates building sentences incrementally. The NLP community organized two Surface Realization Shared Tasks (in 2011"
W19-8635,P17-1183,0,0.167773,"-resource scenario: Table 1 shows that the treebanks are rather small, which poses a challenge for training complex neural models. 3 Related Work The two best-performing approaches in the task of generating sentences from dependency trees have been feature-based incremental text generation (Bohnet et al., 2010; Liu et al., 2015; Puduppully et al., 2016; King and White, 2018) and 2 http://universaldependencies.org/ 269 Property Data efficiency Rich context representation Interpretability Language coverage form much better than similar systems which learn the alignment information from scratch (Aharoni and Goldberg, 2017). The success of the encoder-decoder paradigm has given birth to a prominent research trend of finding various ways of utilizing the abundant data on the web. While looking for ways to acquire more data for training even larger models is a promising research topic, an orthogonal direction is pursuing the question of how to design and train more data-efficient models. Our work focuses on this latter point and attempts to address it via data analysis and algorithm design. Taking this into consideration, we build upon the work done by Puzikov and Gurevych (2018), and attempt to improve their meth"
W19-8635,W17-3518,1,0.855413,"of encoderdecoder (Cho et al., 2014) and sequence-tosequence (seq2seq) (Sutskever et al., 2014) neural architectures, this line of work has gained a lot of popularity due to the method’s simplicity: the input string is encoded into a dense vector and a sentence is being generated token-by-token from the encoded input representation. From an NLP perspective, one of the main research problems in this paradigm has become the choice of the graph encoding strategy. The most popular method is linearizing it into a sequence of tokens and encoding using a variant of a recurrent neural network (RNN) (Gardent et al., 2017; Castro Ferreira et al., 2017; Konstas et al., 2017). Another prominent approach is using graph-to-text neural networks (Song et al., 2018; Trisedya et al., 2018). These methods have shown good results across various tasks, but in the context of surface realization they produced somewhat mixed results: the former ones were successfully used only when being trained on large amounts of data (Elder and Hokamp, 2018), while the latter ones have been only evaluated on the SR’11 Deep Track data and, while performing better than RNN-type encoders, fell short behind feature-based methods (MarchegShal"
W19-8635,W11-2832,0,0.0990045,"ering and morphological inflection steps of the surface linearization process. From a research perspective, this offers greater control over the problem-solving procedure. Introduction Natural Language Generation (NLG) is the task of generating natural language utterances from various data representations. In this work we consider lemmatized dependency trees as input and focus on the process of transforming a dependency tree into a linearly-ordered grammatical string of morphologically inflected words – the setup which is most commonly known as surface realization (SR) (Langkilde-Geary, 2002; Belz et al., 2011). Most surface realization approaches fall into two main groups: feature-based incremental generation pipelines and end-to-end neural approaches. To predict a correct token sequence, In this work we extend B IN L IN along two orthogonal directions. First, we propose a way to enrich the training data, which largely compensates for the small size of the datasets used in the task. Second, we propose a new input encoding strategy which incorporates both local and global prediction contexts. These modifications bridge the performance gap between B IN L IN and endto-end black-box approaches, while r"
W19-8635,W18-3605,0,0.359997,"Mille et al., 2018). 2 Task Description techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). The former approaches traverse the input tree, encode nodes using sparse manually defined feature sets as input representations and generate a sentence by extending a candidate hypothesis with an input word that has the highest score among other input words that have not yet been processed. These approaches rely on the observation that natural language production has a preference for shorter dependencies (Gibson, 2000; White and Rajkumar, 2012; King and White, 2018), which facilitates building sentences incrementally. The NLP community organized two Surface Realization Shared Tasks (in 2011 and 2018) which aimed at developing a common representation that could be used by a variety of NLG systems as input (Belz et al., 2011). They used almost identical task definitions, but different datasets. We focus on the latest task (SR’18 (Mille et al., 2018)), because the former was confined to using English data only, while the latter included Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish Universal Dependencies (UD, versi"
W19-8635,C10-1012,0,0.0214058,"on labels. We focus on the Shallow Track, because it covers more languages than the Deep Track (only three), and is therefore more interesting to study the problem of word ordering and morphological inflection as two steps of the surface realization process. The task can be considered as operating under low-resource scenario: Table 1 shows that the treebanks are rather small, which poses a challenge for training complex neural models. 3 Related Work The two best-performing approaches in the task of generating sentences from dependency trees have been feature-based incremental text generation (Bohnet et al., 2010; Liu et al., 2015; Puduppully et al., 2016; King and White, 2018) and 2 http://universaldependencies.org/ 269 Property Data efficiency Rich context representation Interpretability Language coverage form much better than similar systems which learn the alignment information from scratch (Aharoni and Goldberg, 2017). The success of the encoder-decoder paradigm has given birth to a prominent research trend of finding various ways of utilizing the abundant data on the web. While looking for ways to acquire more data for training even larger models is a promising research topic, an orthogonal dire"
W19-8635,W17-3204,0,0.0302366,"f the workshop. The authors identified the lack of sufficient training data as the major obstacle to training highperforming neural models and mentioned that the system trained only on the original dataset failed to deliver sensible outputs. These results are supported by the work done in other NLP fields. For example, in the machine translation community researchers have found that neural models have a much slower learning curve with respect to the amount of training data, which usually manifests itself as worse quality in low-resource settings, but better performance in high-resource cases (Koehn and Knowles, 2017). In morphological inflection, when trained on small datasets, seq2seq models with additional external (noisy) alignments per4 Approach Description Before explaining our work, we briefly recap how B IN L IN works. It is a pipeline system which generates a sentence from a dependency tree in two stages: 1. Syntactic ordering: convert dependency tree into a binary tree, then traverse the latter to obtain a sequence of lemmas. 2. Morphological inflection: conjugate each lemma into a surface form. Figure 1 shows a schematic view of the first stage. It relies on the procedure which first runs a brea"
W19-8635,W17-3501,0,0.0131435,"., 2014) and sequence-tosequence (seq2seq) (Sutskever et al., 2014) neural architectures, this line of work has gained a lot of popularity due to the method’s simplicity: the input string is encoded into a dense vector and a sentence is being generated token-by-token from the encoded input representation. From an NLP perspective, one of the main research problems in this paradigm has become the choice of the graph encoding strategy. The most popular method is linearizing it into a sequence of tokens and encoding using a variant of a recurrent neural network (RNN) (Gardent et al., 2017; Castro Ferreira et al., 2017; Konstas et al., 2017). Another prominent approach is using graph-to-text neural networks (Song et al., 2018; Trisedya et al., 2018). These methods have shown good results across various tasks, but in the context of surface realization they produced somewhat mixed results: the former ones were successfully used only when being trained on large amounts of data (Elder and Hokamp, 2018), while the latter ones have been only evaluated on the SR’11 Deep Track data and, while performing better than RNN-type encoders, fell short behind feature-based methods (MarchegShallow Track: unordered dependenc"
W19-8635,P17-1014,0,0.0227524,"osequence (seq2seq) (Sutskever et al., 2014) neural architectures, this line of work has gained a lot of popularity due to the method’s simplicity: the input string is encoded into a dense vector and a sentence is being generated token-by-token from the encoded input representation. From an NLP perspective, one of the main research problems in this paradigm has become the choice of the graph encoding strategy. The most popular method is linearizing it into a sequence of tokens and encoding using a variant of a recurrent neural network (RNN) (Gardent et al., 2017; Castro Ferreira et al., 2017; Konstas et al., 2017). Another prominent approach is using graph-to-text neural networks (Song et al., 2018; Trisedya et al., 2018). These methods have shown good results across various tasks, but in the context of surface realization they produced somewhat mixed results: the former ones were successfully used only when being trained on large amounts of data (Elder and Hokamp, 2018), while the latter ones have been only evaluated on the SR’11 Deep Track data and, while performing better than RNN-type encoders, fell short behind feature-based methods (MarchegShallow Track: unordered dependency trees consisting of l"
W19-8635,W18-3604,0,0.0995046,"/ inlg2019-revisiting-binlin 268 Proceedings of The 12th International Conference on Natural Language Generation, pages 268–278, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Data split Train Dev Test ar cs en es 6,016 897 676 66,485 9,016 9,876 12,375 1,978 2,061 14,289 1,651 1,719 Language fi fr 12,030 1,336 1,525 14,529 1,473 416 it nl pt ru 12,796 562 480 12,318 720 685 8,325 559 476 48,119 6,441 6,366 Table 1: Number of sentences in SR’18 datasets (Mille et al., 2018). 2 Task Description techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). The former approaches traverse the input tree, encode nodes using sparse manually defined feature sets as input representations and generate a sentence by extending a candidate hypothesis with an input word that has the highest score among other input words that have not yet been processed. These approaches rely on the observation that natural language production has a preference for shorter dependencies (Gibson, 2000; White and Rajkumar, 2012; King and White, 2018), which facilitates building sentences incrementally. The NLP community organized two Surface Realizati"
W19-8635,W02-2103,0,0.075576,"is of the syntactic ordering and morphological inflection steps of the surface linearization process. From a research perspective, this offers greater control over the problem-solving procedure. Introduction Natural Language Generation (NLG) is the task of generating natural language utterances from various data representations. In this work we consider lemmatized dependency trees as input and focus on the process of transforming a dependency tree into a linearly-ordered grammatical string of morphologically inflected words – the setup which is most commonly known as surface realization (SR) (Langkilde-Geary, 2002; Belz et al., 2011). Most surface realization approaches fall into two main groups: feature-based incremental generation pipelines and end-to-end neural approaches. To predict a correct token sequence, In this work we extend B IN L IN along two orthogonal directions. First, we propose a way to enrich the training data, which largely compensates for the small size of the datasets used in the task. Second, we propose a new input encoding strategy which incorporates both local and global prediction contexts. These modifications bridge the performance gap between B IN L IN and endto-end black-box"
W19-8635,D14-1179,0,0.0113457,"Missing"
W19-8635,N15-1012,0,0.105683,"n the Shallow Track, because it covers more languages than the Deep Track (only three), and is therefore more interesting to study the problem of word ordering and morphological inflection as two steps of the surface realization process. The task can be considered as operating under low-resource scenario: Table 1 shows that the treebanks are rather small, which poses a challenge for training complex neural models. 3 Related Work The two best-performing approaches in the task of generating sentences from dependency trees have been feature-based incremental text generation (Bohnet et al., 2010; Liu et al., 2015; Puduppully et al., 2016; King and White, 2018) and 2 http://universaldependencies.org/ 269 Property Data efficiency Rich context representation Interpretability Language coverage form much better than similar systems which learn the alignment information from scratch (Aharoni and Goldberg, 2017). The success of the encoder-decoder paradigm has given birth to a prominent research trend of finding various ways of utilizing the abundant data on the web. While looking for ways to acquire more data for training even larger models is a promising research topic, an orthogonal direction is pursuing"
W19-8635,D12-1023,0,0.0173889,"tences in SR’18 datasets (Mille et al., 2018). 2 Task Description techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). The former approaches traverse the input tree, encode nodes using sparse manually defined feature sets as input representations and generate a sentence by extending a candidate hypothesis with an input word that has the highest score among other input words that have not yet been processed. These approaches rely on the observation that natural language production has a preference for shorter dependencies (Gibson, 2000; White and Rajkumar, 2012; King and White, 2018), which facilitates building sentences incrementally. The NLP community organized two Surface Realization Shared Tasks (in 2011 and 2018) which aimed at developing a common representation that could be used by a variety of NLG systems as input (Belz et al., 2011). They used almost identical task definitions, but different datasets. We focus on the latest task (SR’18 (Mille et al., 2018)), because the former was confined to using English data only, while the latter included Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish Universal"
W19-8635,W18-6501,0,0.0361472,"Missing"
W19-8635,W18-3601,0,0.0156982,"-end black-box approaches, while retaining its interpretability advantages. 1 https://github.com/UKPLab/ inlg2019-revisiting-binlin 268 Proceedings of The 12th International Conference on Natural Language Generation, pages 268–278, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Data split Train Dev Test ar cs en es 6,016 897 676 66,485 9,016 9,876 12,375 1,978 2,061 14,289 1,651 1,719 Language fi fr 12,030 1,336 1,525 14,529 1,473 416 it nl pt ru 12,796 562 480 12,318 720 685 8,325 559 476 48,119 6,441 6,366 Table 1: Number of sentences in SR’18 datasets (Mille et al., 2018). 2 Task Description techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). The former approaches traverse the input tree, encode nodes using sparse manually defined feature sets as input representations and generate a sentence by extending a candidate hypothesis with an input word that has the highest score among other input words that have not yet been processed. These approaches rely on the observation that natural language production has a preference for shorter dependencies (Gibson, 2000; White and Rajkumar, 2012; King and White, 201"
W19-8635,P02-1040,0,0.104067,"85.6 92.85 85.56 Table 3: The distribution of left/right labels in the training data and the accuracy of predicting a node’s relative position with the binary classifier. Two cases are considered: predicting the position of a dependent w.r.t. its head (head-dep), and a sibling (dep-dep). BLEU EDIST NIST B IN L IN + data enrichment + new encoder + new features 24.92 48.47 50.67 51.15 35.91 62.04 64.05 64.78 9.55 10.72 10.82 10.82 Upper bound 65.31 85.52 11.38 dency locality hypothesis. We trained the syntactic ordering component and performed its automatic metric evaluation by computing BLEU (Papineni et al., 2002)3 , NIST (Doddington, 2002) and normalized string edit distance (EDIST) scores between the references and system outputs. Note that system outputs contain ordered lemmas, not surface forms, while the references are correctly ordered sequences of inflected surface forms given in the CONLL file. Table 4 shows the contribution of each of the modifications that we propose in this work; the results are computed on the English SR’18 development set. We also show the maximum metric scores that an ideal syntactic ordering component would get, i.e. an upper bound on its performance. We computed it by r"
W19-8635,N16-1058,0,0.0166486,"k, because it covers more languages than the Deep Track (only three), and is therefore more interesting to study the problem of word ordering and morphological inflection as two steps of the surface realization process. The task can be considered as operating under low-resource scenario: Table 1 shows that the treebanks are rather small, which poses a challenge for training complex neural models. 3 Related Work The two best-performing approaches in the task of generating sentences from dependency trees have been feature-based incremental text generation (Bohnet et al., 2010; Liu et al., 2015; Puduppully et al., 2016; King and White, 2018) and 2 http://universaldependencies.org/ 269 Property Data efficiency Rich context representation Interpretability Language coverage form much better than similar systems which learn the alignment information from scratch (Aharoni and Goldberg, 2017). The success of the encoder-decoder paradigm has given birth to a prominent research trend of finding various ways of utilizing the abundant data on the web. While looking for ways to acquire more data for training even larger models is a promising research topic, an orthogonal direction is pursuing the question of how to de"
W19-8635,W18-3602,1,0.824118,"ediction accuracy. We show how enriching the training data to better capture word order constraints almost doubles the performance of the system. We further demonstrate that encoding both local and global prediction contexts yields another considerable performance boost. With the proposed modifications, the system which ranked low in the latest shared task on multilingual surface realization now achieves best results in five out of ten languages, while being on par with the state-of-the-art approaches in others. 1 1 This work builds upon B IN L IN, a binary linearization technique proposed by Puzikov and Gurevych (2018). It is a hybrid approach which uses a feature-based neural word ordering module and a sequence-to-sequence morphological inflection component. In terms of prediction accuracy, B IN L IN falls short compared to end-to-end neural approaches, but has an advantage of being more intuitive and interpretable. It also supports separate analysis of the syntactic ordering and morphological inflection steps of the surface linearization process. From a research perspective, this offers greater control over the problem-solving procedure. Introduction Natural Language Generation (NLG) is the task of genera"
W19-8635,P18-1150,0,0.0204681,"ained a lot of popularity due to the method’s simplicity: the input string is encoded into a dense vector and a sentence is being generated token-by-token from the encoded input representation. From an NLP perspective, one of the main research problems in this paradigm has become the choice of the graph encoding strategy. The most popular method is linearizing it into a sequence of tokens and encoding using a variant of a recurrent neural network (RNN) (Gardent et al., 2017; Castro Ferreira et al., 2017; Konstas et al., 2017). Another prominent approach is using graph-to-text neural networks (Song et al., 2018; Trisedya et al., 2018). These methods have shown good results across various tasks, but in the context of surface realization they produced somewhat mixed results: the former ones were successfully used only when being trained on large amounts of data (Elder and Hokamp, 2018), while the latter ones have been only evaluated on the SR’11 Deep Track data and, while performing better than RNN-type encoders, fell short behind feature-based methods (MarchegShallow Track: unordered dependency trees consisting of lemmatized nodes with part-ofspeech (POS) tags and morphological information, as found"
W19-8635,P18-1151,0,0.0157503,"larity due to the method’s simplicity: the input string is encoded into a dense vector and a sentence is being generated token-by-token from the encoded input representation. From an NLP perspective, one of the main research problems in this paradigm has become the choice of the graph encoding strategy. The most popular method is linearizing it into a sequence of tokens and encoding using a variant of a recurrent neural network (RNN) (Gardent et al., 2017; Castro Ferreira et al., 2017; Konstas et al., 2017). Another prominent approach is using graph-to-text neural networks (Song et al., 2018; Trisedya et al., 2018). These methods have shown good results across various tasks, but in the context of surface realization they produced somewhat mixed results: the former ones were successfully used only when being trained on large amounts of data (Elder and Hokamp, 2018), while the latter ones have been only evaluated on the SR’11 Deep Track data and, while performing better than RNN-type encoders, fell short behind feature-based methods (MarchegShallow Track: unordered dependency trees consisting of lemmatized nodes with part-ofspeech (POS) tags and morphological information, as found in the UD annotations. D"
Y11-1034,W11-1104,0,0.0224433,"Garcia-Molina, 2006) and ontologies (Schmitz, 2006) from folksonomies based on co-occurrence patterns. Recently, work has been published on clustering and disambiguating tag similarity graphs. Yeung et al. (2009) employ the Girvan–Newman algorithm (Girvan and Newman, 2002) to optimize the betweenness of nodes by incrementally removing edges and recalculating betweenness. They only look at a small data set of ten exemplary tags. Our goal is to go beyond simple clustering and do actual disambiguation, meaning to instantiate the different readings of a tag as individual nodes within the network. Jurgens (2011) presents a way to use community detection to induce word senses from word collocations. He builds a graph from word co-occurrences and then applies a community detection algorithm. In contrast to the approach in this work, they do not split the nodes into different senses but use a clustering algorithm that deals with nodes belonging to multiple communities. The algorithm is a hierarchical agglomerative clustering operating on a similarity measure defined for edges, not vertices. The paper is structured as follows: First, we describe how to derive a tag similarity graph from folksonomies and"
Y11-1034,zesch-etal-2008-extracting,1,0.809552,"bSonomy dataset has a sense distribution that resembles that of WordNet and Wiktionary. If used with resource similarity, the average number of senses is much higher. Due to the sparseness of the resource space, many of a tag’s neighboring tags feature no similarity to each other and will therefore end up in separate clusters yielding more senses. Next, we turn to a qualitative analysis of polysemous lexemes found in the both generated disambiguated graphs from Section 2.3. Table 6 shows the number of senses for six lexemes 6 The English edition from June 5, 2011, accessed using JWKTL 0.14.1 (Zesch et al., 2008) Table 4: Examples of words found in the folksonomy but not in Wiktionary Category abbrevations concatenations meta-tags multiword expressions spelling errors neologisms named entities other Examples cvs, pda, tdd, dhtml, nyc real estate, bayarea, zipcode mek:untagged, firefox:import milkandcookies, home improvement, acadamia hometheater, pilates, howto, etext, bioinformatics, portscan junit hamradio 329 Table 5: Sense distribution for different tag similarity graphs and other resources in percent dr (1) dr (2) dr (3) dr (4) dr (≥ 5) bib-res 15.45 15.62 14.68 12.05 42.20 bib-user 83.75 13.74 2"
Y14-1030,R11-1071,1,0.866238,"Missing"
Y14-1030,W11-0408,0,0.0182551,"0) separately calculated bias and error, enabling better quality assessment of a worker. Some research explores the decision between obtaining more labels per instance or more labelled instances. Sheng et al. (2008) evaluated machine learning performance with different corpus sizes and label qualities. They evaluated four algorithms for use in deciding between redundant labelling and more labelled instances. Kumar and Lease (2011) built on the model by Sheng et al. (2008), adding knowledge of annotator quality for faster learning. Other work focuses on correcting labels at the instance level. Dligach and Palmer (2011) used annotation-error detection and ambiguity detection to identify instances in need of additional annotations. Hsueh et al. (2009) modelled annotator quality and ambiguity rating to select highly informative yet unambiguous training instances. Alternatively, class imbalance can be accommodated during machine learning, by resampling and cost-sensitive learning. Das et al. (2014) used density-based clustering to identify clusters in the instance space: if the clusters exceeded a threshold of majority-class dominance, they are undersampled to increase class-balance in the dataset. Batista et a"
Y14-1030,W09-1904,0,0.0257341,"ing more labels per instance or more labelled instances. Sheng et al. (2008) evaluated machine learning performance with different corpus sizes and label qualities. They evaluated four algorithms for use in deciding between redundant labelling and more labelled instances. Kumar and Lease (2011) built on the model by Sheng et al. (2008), adding knowledge of annotator quality for faster learning. Other work focuses on correcting labels at the instance level. Dligach and Palmer (2011) used annotation-error detection and ambiguity detection to identify instances in need of additional annotations. Hsueh et al. (2009) modelled annotator quality and ambiguity rating to select highly informative yet unambiguous training instances. Alternatively, class imbalance can be accommodated during machine learning, by resampling and cost-sensitive learning. Das et al. (2014) used density-based clustering to identify clusters in the instance space: if the clusters exceeded a threshold of majority-class dominance, they are undersampled to increase class-balance in the dataset. Batista et al. (2004) examined the effects of sampling for class-imbalance reduction on 13 datasets and found that oversampling is generally more"
Y14-1030,R13-1042,1,0.854674,"2011; Sheng et al., 2008). However, much previous work for quality maximization and cost limitation assumes that the dataset to be annotated is classbalanced. Content-based instance targeting can be used to select instances with a high probability of being rare-class. For example, in a binary class annotation task identifying pairs of emails from the same thread, where most instances are negative, cosine text similarity between the emails can be used to identify pairs of emails that are likely to be positive, so that they could be annotated and included in the resulting class-balanced corpus (Jamison and Gurevych, 2013). However, this technique renders the corpus useless for experiments including token similarity (or ngram similarity, semantic similarity, stopword distribution similarity, keyword similarity, etc) as a feature; a machine learner would be likely to learn the very same features for classification that were used to identify the rare-class instances during corpus construction. Even worse, Mikros and Argiri (2007) showed that many features besides ngrams are significantly correlated with topic, including sentence and token length, readability measures, and word length distributions. The proposed t"
Y14-1030,D08-1027,0,0.151338,"Missing"
Y14-1030,P11-1122,0,0.0273363,"itives (rare-class) discovered. The fewer TP’s discovered, the less likely the resulting corpus will represent the original data in an undistorted manner. P Prare is the precision over rare instances: T PT+F P Lower precision means lower confidence in the produced dataset, because the “rare” instances we found might have been misclassified. 7 crowdflower.com AvgA⇥annoCost ing a rare instance: classImbalance Recallrare Savings is the estimated cost saved when identifying rare instances, over the baseline. Includes Standard Deviation. 6 Supervised Cascading Classifier Experiments Previous work (Zaidan and Callison-Burch, 2011) used machine learners to predict which instances to annotate based on annotation metadata. In this section, we used crowdsourcing annotation metadata (such as time duration) as features for a cascading logistic regression classifier to choose whether or not an additional annotation is needed. In each of the five cascade rounds, an instance was classified as either potentially rare or common. Instances classified as potentially rare received another annotation and continued through the next cascade, while instances classified as common were discarded. Discarding instances before the end of the"
Y14-1055,P13-2013,0,0.0529521,"es. Lin et al. (2009) performed discourse relation recognition using lexical pairs as well as con!481 PACLIC 28 stituent and dependency information of relations in the Penn Discourse Treebank. They achieved 0.328 accuracy against a 0.261 most frequent class baseline, using 13,366 instances. Pitler et al. (2009) performed binary discourse relation prediction using lexical pairs, verb information, and linguisticallymotivated features, and achieve improvements of up to 0.60-0.62 accuracy, compared with a 0.50 baseline, on datasets sized 1,460 to 12,712 instances from the Penn Discourse Treebank. Biran and McKeown (2013) aggregated lexical pairs as clusters, to combat the feature sparsity problem. While improvements are modest, lexical pairs are helpful in these discourse tasks where useful linguisticallymotivated features have proven elusive. 4 Dataset Our dataset3 consists of discussion turn pairs from Ferschke’s (2014) English Wikipedia Discussions Corpus (EWDC). Discussion pages provide a forum for users to discuss edits to a Wikipedia article. We derived a class-balanced dataset of 26844 pairs of adjacent and non-adjacent discussion turn pairs from the EWDC. The pairs came from 550 discussions within 83"
Y14-1055,N07-1054,0,0.0117824,"riment combinations were p  0.05 significantly different (McNemar, 1947) from the CosineSim and MFC baselines. The highest performing feature combination was pair unigrams with stopwords removed (pair1grams+noSW), which had higher accuracy (.68±.02) than all other feature combinations, including pair1grams that included stopwords (.64±.01), and all of the stopword feature sets. Stopword removal increases the system performance for our task, which is unexpected because in other work on different discourse relation tasks, the removal of stopwords from lexical pairs has hurt system performance (Blair-Goldensohn et al., 2007; Marcu and Echihabi, 2002; Biran and McKeown, 2013). Longer ngrams did not increase performance: pair2grams (.57±.03) significantly underperformed pair1grams (.64±.01). We examined the performance curve using various n numbers of most frequent lexical pairs as features on a subset of our corpus (1,380 instances). We found that there was no sharp benefit from a few particularly useful pairs, but that performance continued to increase as n approached 5000. We found that the classifier performs better when the model learns turn pair order, and the reduced data sparsity from using symmetrical fea"
Y14-1055,P14-5011,1,0.745233,"the feature name. Adjacency pairs, by definition, are nonsymmetrical. To confirm this property, in some of our feature groups, we extract a reverse-ordered feature for each standard feature. An example with symmetrical and non-symmetrical features is shown below. Turn1: Why ? Turn2: Because . Non-Sym features: (why, because) Sym features: (why, because), (because, why) 7 Experiments without Topic Bias Control In our first set of experiments, we perform adjacency pair recognition without topic bias control (“nonTBC”). We use the SVM classifier SMO (Hall et al., 2009) in the DKPro TC framework (Daxenberger et al., 2014) for pairwise classification6 and 5-fold7 cross-validation (CV), in which all instances are randomly assigned to CV folds. These experiments do not control for any topic bias in the data. Previous work (Wang and Ros´e, 2010) has structured adjacency pair recognition as a ranking task, with the classifier choosing between one correct and one incorrect response to a given turn. In our experiments, we use pairwise binary classification, because the high indentation error rate and our EWDC instance selection method did not yield enough matched turn pairs for ranking. Feature parameters (such as to"
Y14-1055,E12-1079,1,0.85623,"m Ferschke’s (2014) English Wikipedia Discussions Corpus. Thread structure is indicated by tab indents. Turn pairs (1,2), (1,3), and (3,4) are adjacency pairs; pairs (2,3) and (1,4) are not. Adjacency pair recognition is the classification of a pair of turns as adjacent or nonadjacent. Although most previous work on thread reconstruction takes advantage of metadata such as user id, timestamp, and quoted material (Aumayr et al., 2011; Wang et al., 2011a), metadata is unreliable in some forums, such as Wikipedia Discussion page forums, where metadata and user contribution is difficult to align (Ferschke et al., 2012). Wang et al. (2011b) find that joint prediction of dialogue act labels and adjacency pair recognition improves accuracy when compared to separate classification; dialogue act classification does not require metadata. However, existing dialogue act typologies are unapplicable for some forums (see Section 2.2). In this paper, we perform adjacency pair recognition on pairs of turns extracted from the English Copyright 2014 by Emily K. Jamison and Iryna Gurevych 28th Pacific Asia Conference on Language, Information and Computation pages 479–488 !479 PACLIC 28 Wikipedia Discussions Corpus (EWDC)."
Y14-1055,P13-1071,1,0.851554,"tive. Such a model is recognizing adjacency by topic. To remove this topic bias, instances from a single article should never occur simultaneously in training and evaluation datasets. Topic bias is a pervasive problem. Mikros and Argiri (2007) have shown that many features besides ngrams are significantly correlated with topic, including sentence and token length, readability measures, and word length distributions. Topiccontrolled corpora have been used for authorship identification (Koppel and Schler, 2003), genre detection (Finn and Kushmerick, 2003), and Wikipedia quality flaw prediction (Ferschke et al., 2013). The class distribution by discussion in our dataset is shown in Figure 2; imbalance is shown by the percentage of positive pairs minus the percentage of negative pairs. Only 39 of 550 discussions contributed an approximately equal number of positive !485 Number of discussions PACLIC 28 Feature MFC CosineSim Nonpair1grams Stopwords+SB+noSym Stopwords+SB+Sym Stopwords+noSB+noSym Stopwords+SB+CA+SL+noSym DiscConn+SB+noSym And-as-for Pair1grams Pair2grams Pair1ngrams+noDC Pair1ngrams+noSW 40 20 0 50 0 Imbalance 50 Figure 2: Class imbalance by discussion, in percent. -20 means a discussion is 20"
Y14-1055,P13-1010,0,0.016494,"3 .64±.01 .57±.03 .65±.02 .68±.02 Table 2: Non-TBC adjacency pair recognition feature set descriptions and results. F1 results are shown by adjacent (+) and nonadjacent (-) classes. Accuracy is shown with cross-validation fold standard deviation. Human Upper Bound is calculated on a different dataset, which was also derived from the EWDC. stance, the first turn begins, “In languages with dynamic scoping, this is not the case,[...],” and the other turn replies, “I’ll readily admit that I have little experience with dynamic scoping[...]” This may be solvable with centering theoretic approaches (Guinaudeau and Strube, 2013), which probabilistically model the argument position of multiple sequential mentions of an entity such as “dynamic scoping”. The second instance consists of a deep disagreement between the two authors, in which they discuss a number of keywords and topic specific terms, disagree with each other, and make conclusions. This instance may need a combination of a centering theoretic approach, opinion mining, and topic modeling to solve. 7.3 Feature Analysis We examined the top-ranked features from our most accurate system, pair1grams+noSW (accuracy = .66±.01), as determined by Information Gain ran"
Y14-1055,W10-2923,0,0.0322754,"sations. Sacks et al. (1974) were discussing phone conversations when they observed that a speaker can select the next speaker by the use of adjacency pairs, and the subsequent speaker is obligated to give a response appropriate to and limited by the adjacency pair, such as answering a question. In a phone conversation, the participant set is fixed, and rules of the conversation permit the speaker to address other participants directly, and obligate a response. However, in other types of discussion, such as forum discussions, this is not the case. For example, in QA-style forums such as CNET (Kim et al., 2010), a user posts a question, and anyone in the community may respond; the user cannot select a certain participant as the next speaker. Wikipedia discussions vary even further from phone conversations: many threads are initiated by users interested in determining community opinion on a topic, who avoid asking direct questions. Wikipedia turns that might have required direct replies from a particular participant in a speaker-selecting (SS) phone conversation, are formulated to reduce or remove obligation of response in this non-speaker-selecting context. Some examples are below; NSS turns are act"
Y14-1055,D09-1036,0,0.0585584,"Missing"
Y14-1055,P02-1047,0,0.615011,"y between two turns and their titles, to identify thread-linking structure. Wang et al. (2011b) used a dependency parser, based on unweighted cosine similarity of titles and turn contents, as well as authorship and structural features, to learn a model for joint classification of Dialogue Acts and “inter-post links” between posts in the CNET forum dataset. 3.2 Lexical Pairs We use lexical pairs as features for adjacency pair recognition. Although not previously been used for this task, lexical pairs have been helpful for other discourse structure tasks such as recognising discourse relations. Marcu and Echihabi (2002) used lexical pairs from all words, nouns, verbs, and cuephrases, to recognise discourse relations. A binary relation/non-relation classifier achieves 0.64 to 0.76 accuracy against a 0.50 baseline, over approx. 1M instances. Lin et al. (2009) performed discourse relation recognition using lexical pairs as well as con!481 PACLIC 28 stituent and dependency information of relations in the Penn Discourse Treebank. They achieved 0.328 accuracy against a 0.261 most frequent class baseline, using 13,366 instances. Pitler et al. (2009) performed binary discourse relation prediction using lexical pairs"
Y14-1055,P09-1077,0,0.0141576,"urse structure tasks such as recognising discourse relations. Marcu and Echihabi (2002) used lexical pairs from all words, nouns, verbs, and cuephrases, to recognise discourse relations. A binary relation/non-relation classifier achieves 0.64 to 0.76 accuracy against a 0.50 baseline, over approx. 1M instances. Lin et al. (2009) performed discourse relation recognition using lexical pairs as well as con!481 PACLIC 28 stituent and dependency information of relations in the Penn Discourse Treebank. They achieved 0.328 accuracy against a 0.261 most frequent class baseline, using 13,366 instances. Pitler et al. (2009) performed binary discourse relation prediction using lexical pairs, verb information, and linguisticallymotivated features, and achieve improvements of up to 0.60-0.62 accuracy, compared with a 0.50 baseline, on datasets sized 1,460 to 12,712 instances from the Penn Discourse Treebank. Biran and McKeown (2013) aggregated lexical pairs as clusters, to combat the feature sparsity problem. While improvements are modest, lexical pairs are helpful in these discourse tasks where useful linguisticallymotivated features have proven elusive. 4 Dataset Our dataset3 consists of discussion turn pairs fro"
Y14-1055,prasad-etal-2008-penn,0,0.016864,"ase, adjacency) that holds between two text spans, N1 , N2 , is determined by the ngram pairs in the cartesian product defined over the words in the two text spans (ni , nj ) 2 N1 ⇥ N2 . The goal of using lexical pairs is to identify word pairs indicative of adjacency, such as (why, because) and (?, yes). These pairs cannot be identified using text similarity techniques used in previous work (Wang and Ros´e, 2010). In addition to lexical pairs created from document ngrams, lexical pairs were created from a list of 50 stopwords (Stamatatos, 2011), Penn Discourse Treebank discourse connectives (Prasad et al., 2008), and a particularly effective combination of just 3 stopwords: and, as, for. Other variables included the parameter ngram n, and removed stopwords, which skipped unallowed words in the text. Structural context information. Some of our feature groups include structural context information of the discussion turn codified as lexical items in the lexical pair string. We include sentence boundaries (SB), commas (CA), and sentence location (i.e., sentence occurs in first quarter, last quarter, or middle of the discussion turn). A sample lexical string representing text from the beginning of a turn"
Y14-1055,N10-1097,0,0.0687192,"Missing"
Y14-1055,D11-1002,0,0.299515,"nition of adjacency pairs is a critical step in thread reconstruction (Balali et al., 2014; Wang et al., 2008; Aumayr et al., 2011). Figure 1 shows an excerpt from Ferschke’s (2014) English Wikipedia Discussions Corpus. Thread structure is indicated by tab indents. Turn pairs (1,2), (1,3), and (3,4) are adjacency pairs; pairs (2,3) and (1,4) are not. Adjacency pair recognition is the classification of a pair of turns as adjacent or nonadjacent. Although most previous work on thread reconstruction takes advantage of metadata such as user id, timestamp, and quoted material (Aumayr et al., 2011; Wang et al., 2011a), metadata is unreliable in some forums, such as Wikipedia Discussion page forums, where metadata and user contribution is difficult to align (Ferschke et al., 2012). Wang et al. (2011b) find that joint prediction of dialogue act labels and adjacency pair recognition improves accuracy when compared to separate classification; dialogue act classification does not require metadata. However, existing dialogue act typologies are unapplicable for some forums (see Section 2.2). In this paper, we perform adjacency pair recognition on pairs of turns extracted from the English Copyright 2014 by Emily"
Y14-1055,U11-1011,0,0.280505,"nition of adjacency pairs is a critical step in thread reconstruction (Balali et al., 2014; Wang et al., 2008; Aumayr et al., 2011). Figure 1 shows an excerpt from Ferschke’s (2014) English Wikipedia Discussions Corpus. Thread structure is indicated by tab indents. Turn pairs (1,2), (1,3), and (3,4) are adjacency pairs; pairs (2,3) and (1,4) are not. Adjacency pair recognition is the classification of a pair of turns as adjacent or nonadjacent. Although most previous work on thread reconstruction takes advantage of metadata such as user id, timestamp, and quoted material (Aumayr et al., 2011; Wang et al., 2011a), metadata is unreliable in some forums, such as Wikipedia Discussion page forums, where metadata and user contribution is difficult to align (Ferschke et al., 2012). Wang et al. (2011b) find that joint prediction of dialogue act labels and adjacency pair recognition improves accuracy when compared to separate classification; dialogue act classification does not require metadata. However, existing dialogue act typologies are unapplicable for some forums (see Section 2.2). In this paper, we perform adjacency pair recognition on pairs of turns extracted from the English Copyright 2014 by Emily"
zesch-etal-2008-extracting,E06-1002,0,\N,Missing
zesch-etal-2008-extracting,W07-0201,1,\N,Missing
zesch-etal-2008-extracting,N07-2052,1,\N,Missing
zesch-etal-2008-extracting,P07-1130,1,\N,Missing
zesch-etal-2008-extracting,D07-1093,0,\N,Missing
zesch-gurevych-2010-better,W06-2501,0,\N,Missing
zesch-gurevych-2010-better,W04-2607,0,\N,Missing
zesch-gurevych-2010-better,P06-1040,0,\N,Missing
zesch-gurevych-2010-better,P99-1020,0,\N,Missing
zesch-gurevych-2010-better,N07-2052,1,\N,Missing
zesch-gurevych-2010-better,D07-1090,0,\N,Missing
zesch-gurevych-2010-better,I05-1067,1,\N,Missing
zesch-gurevych-2010-better,J06-1003,0,\N,Missing
zesch-gurevych-2010-better,zesch-etal-2008-extracting,1,\N,Missing
