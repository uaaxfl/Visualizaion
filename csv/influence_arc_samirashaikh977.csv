2019.ccnlg-1.3,N19-1409,0,0.0145527,"ting of an encoder and decoder. Table 3 provides a comparison of our approach with to the baseline approach. In Table 3, we refer our “Our Model Fine-Tuned” as the baseline fine-tuned GPT-2 model trained on the dialogue and “Our-model Emo-prepend” as the GPT-2 model that is fine-tuned on the dialogues but also conditioned on the emotion displayed in the conversation. We find that fine-tuning the GPT-2 language model using a transfer learning approach helps us achieve a lower perplexity and a higher BLEU scores. The results from our approach are consistent with the empirical study conducted by Edunov et al (2019) that demonstrate the effectiveness of the using pre-trained model diminishes when added to the decoder network in an seq2seq approach. We also perform a comparison between our two models on the metrics of length, diversity, readability and coherence. We find that our baseline model produces less diverse responses compared to when the model is conditioned on emotion. We find that the our emoprepend model also higher a slightly higher readability score that our baseline model. 4.2 Qualitative Evaluation To assess the quality of generations, we conducted a MTurk human evaluation. We recruited a"
2019.ccnlg-1.3,P17-1059,0,0.0635601,"e, 2015; Li et al., 2016c). These issues also affect engagement with the conversational agent, that leads to short conversations (Venkatesh et al., 2018). Apart from producing engaging responses, understanding the situation and producing the right emotional response to a that situation is another desirable trait (Rashkin et al., 2019). Emotions are intrinsic to humans and help in creation of a more engaging conversation (Poria et al., 2019). Recent work has focused on approaches towards incorporating emotion in conversational agents (Asghar et al., 2018; Zhou et al., 2018; Huang et al., 2018; Ghosh et al., 2017), however these approaches are focused towards seq2seq task. We approach this problem of emotional generation as a form of transfer learning, using large pretrained language models. These language models, including BERT, GPT-2 and XLNet, have helped achieve state of the art across several natural language understanding tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019). However, their success in language modeling tasks have been inconsistent (Ziegler et al., 2019). In our approach, we use these pretrained language models as the base model and perform transfer learning to fine"
2019.ccnlg-1.3,N18-2008,0,0.0813542,"ation (Vinyals and Le, 2015; Li et al., 2016c). These issues also affect engagement with the conversational agent, that leads to short conversations (Venkatesh et al., 2018). Apart from producing engaging responses, understanding the situation and producing the right emotional response to a that situation is another desirable trait (Rashkin et al., 2019). Emotions are intrinsic to humans and help in creation of a more engaging conversation (Poria et al., 2019). Recent work has focused on approaches towards incorporating emotion in conversational agents (Asghar et al., 2018; Zhou et al., 2018; Huang et al., 2018; Ghosh et al., 2017), however these approaches are focused towards seq2seq task. We approach this problem of emotional generation as a form of transfer learning, using large pretrained language models. These language models, including BERT, GPT-2 and XLNet, have helped achieve state of the art across several natural language understanding tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019). However, their success in language modeling tasks have been inconsistent (Ziegler et al., 2019). In our approach, we use these pretrained language models as the base model and perform tran"
2019.ccnlg-1.3,P19-1365,0,0.0123843,"1 4.36 4.31 Table 2: Statistics of Empathetic Dialogue dataset used in our experiments 3.2 Implementation In all our experiments, we use the GPT-2 pretrained language model. We use the publicly available model containing 117M parameters with 12 layers; each layer has 12 heads. We implemented our models using PyTorch Transformers.2 The input sentences are tokenized using byte-pair encoding(BPE) (Sennrich et al., 2016) (vocabulary size of 50263). While decoding, we use the nucleus sampling (p = 0.9) approach instead of beam-search to overcome the drawbacks of beam search (Holtzman et al., 2019; Ippolito et al., 2019). All our models are trained on a single TitanV GPU and takes around 2 hours to fine-tune the model. The fine-tuned models along with the configuration files and the code will be made available at: https://github.com/ sashank06/CCNLG-emotion. 3.3 Metrics Evaluating the quality of responses in open domain situations where the goal is not defined is an important area of research. Researchers have used methods such as BLEU , METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004) from machine translation and text summarization (Liu et al., 2016) tasks. BLEU and METEOR are based on word overlap betwe"
2019.ccnlg-1.3,N16-1014,0,0.414135,"ty metric by about 5 points and achieves a higher BLEU metric score. 1 Introduction Rapid advancement in the field of generative modeling through the use of neural networks has helped advance the creation of more intelligent conversational agents. Traditionally these conversational agents are built using seq2seq framework that is widely used in the field of machine translation (Vinyals and Le, 2015). However, prior research has shown that engaging with these agents produces dull and generic responses whilst also being inconsistent with the emotional tone of conversation (Vinyals and Le, 2015; Li et al., 2016c). These issues also affect engagement with the conversational agent, that leads to short conversations (Venkatesh et al., 2018). Apart from producing engaging responses, understanding the situation and producing the right emotional response to a that situation is another desirable trait (Rashkin et al., 2019). Emotions are intrinsic to humans and help in creation of a more engaging conversation (Poria et al., 2019). Recent work has focused on approaches towards incorporating emotion in conversational agents (Asghar et al., 2018; Zhou et al., 2018; Huang et al., 2018; Ghosh et al., 2017), how"
2019.ccnlg-1.3,P16-1094,0,0.229044,"ty metric by about 5 points and achieves a higher BLEU metric score. 1 Introduction Rapid advancement in the field of generative modeling through the use of neural networks has helped advance the creation of more intelligent conversational agents. Traditionally these conversational agents are built using seq2seq framework that is widely used in the field of machine translation (Vinyals and Le, 2015). However, prior research has shown that engaging with these agents produces dull and generic responses whilst also being inconsistent with the emotional tone of conversation (Vinyals and Le, 2015; Li et al., 2016c). These issues also affect engagement with the conversational agent, that leads to short conversations (Venkatesh et al., 2018). Apart from producing engaging responses, understanding the situation and producing the right emotional response to a that situation is another desirable trait (Rashkin et al., 2019). Emotions are intrinsic to humans and help in creation of a more engaging conversation (Poria et al., 2019). Recent work has focused on approaches towards incorporating emotion in conversational agents (Asghar et al., 2018; Zhou et al., 2018; Huang et al., 2018; Ghosh et al., 2017), how"
2019.ccnlg-1.3,D16-1127,0,0.634231,"ty metric by about 5 points and achieves a higher BLEU metric score. 1 Introduction Rapid advancement in the field of generative modeling through the use of neural networks has helped advance the creation of more intelligent conversational agents. Traditionally these conversational agents are built using seq2seq framework that is widely used in the field of machine translation (Vinyals and Le, 2015). However, prior research has shown that engaging with these agents produces dull and generic responses whilst also being inconsistent with the emotional tone of conversation (Vinyals and Le, 2015; Li et al., 2016c). These issues also affect engagement with the conversational agent, that leads to short conversations (Venkatesh et al., 2018). Apart from producing engaging responses, understanding the situation and producing the right emotional response to a that situation is another desirable trait (Rashkin et al., 2019). Emotions are intrinsic to humans and help in creation of a more engaging conversation (Poria et al., 2019). Recent work has focused on approaches towards incorporating emotion in conversational agents (Asghar et al., 2018; Zhou et al., 2018; Huang et al., 2018; Ghosh et al., 2017), how"
2019.ccnlg-1.3,D16-1230,0,0.134353,"Missing"
2019.ccnlg-1.3,Q18-1027,0,0.018413,"coder network in an seq2seq approach. We also perform a comparison between our two models on the metrics of length, diversity, readability and coherence. We find that our baseline model produces less diverse responses compared to when the model is conditioned on emotion. We find that the our emoprepend model also higher a slightly higher readability score that our baseline model. 4.2 Qualitative Evaluation To assess the quality of generations, we conducted a MTurk human evaluation. We recruited a total Related Work The area of dialogue systems has been studied extensively in both open-domain (Niu and Bansal, 2018) and goal-oriented (Lipton et al., 2018) situations. Extant approaches towards building dialogue systems has been done predominantly through the seq2seq framework (Vinyals and Le, 2015). However, prior research has shown that these systems are prone to producing dull and generic responses that causes engagement with the human to be affected (Vinyals and Le, 2015; Venkatesh et al., 2018). Researchers have tackled this problem of dull and generic responses through different optimization function such as MMI (Li et al., 2016b) and through reinforcement learning approaches(Li et al., 2016d). Alter"
2019.ccnlg-1.3,D17-1238,0,0.0225068,"Missing"
2019.ccnlg-1.3,P02-1040,0,0.103824,"rics Evaluating the quality of responses in open domain situations where the goal is not defined is an important area of research. Researchers have used methods such as BLEU , METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004) from machine translation and text summarization (Liu et al., 2016) tasks. BLEU and METEOR are based on word overlap between the proposed and ground truth responses; they do not adequately account for the diversity of responses that are possible for a given input utterance and show little to no correlation with human judgments (Liu et al., 2016). We report on the BLEU (Papineni et al., 2002) and Perplexity (PPL) metric to provide a comparison with the current state-of-the-art methods. We also report our performance using other metrics such as length of responses produced by the model. Following, Mei et al (2017), we also report the diversity metric that helps us measure the ability of the model to promote diversity in responses (Li et al., 2 https://github.com/huggingface/ pytorch-transformers 2016a). Diversity is calculated as the as the number of distinct unigrams in the generation scaled by the total number of generated tokens (Mei et al., 2017; Li et al., 2016c). We report on"
2019.ccnlg-1.3,P19-1534,0,0.426349,"seq2seq framework that is widely used in the field of machine translation (Vinyals and Le, 2015). However, prior research has shown that engaging with these agents produces dull and generic responses whilst also being inconsistent with the emotional tone of conversation (Vinyals and Le, 2015; Li et al., 2016c). These issues also affect engagement with the conversational agent, that leads to short conversations (Venkatesh et al., 2018). Apart from producing engaging responses, understanding the situation and producing the right emotional response to a that situation is another desirable trait (Rashkin et al., 2019). Emotions are intrinsic to humans and help in creation of a more engaging conversation (Poria et al., 2019). Recent work has focused on approaches towards incorporating emotion in conversational agents (Asghar et al., 2018; Zhou et al., 2018; Huang et al., 2018; Ghosh et al., 2017), however these approaches are focused towards seq2seq task. We approach this problem of emotional generation as a form of transfer learning, using large pretrained language models. These language models, including BERT, GPT-2 and XLNet, have helped achieve state of the art across several natural language understand"
2019.ccnlg-1.3,W19-8610,1,0.821571,"asets. Specifically, we look at conditioning the pre-trained model on the emotion of the situation produce more affective responses that are appropriate for a particular situation. We notice that our fine-tuned and emoprepend models outperform the current state of the art approach relative to the automated metrics such as BLEU and perplexity on the validation set. We also notice that the emo-prepend approach does not out perform a simple fine tuning approach on the dataset. We plan to investigate the cause of this in future work from the perspective of better experiment design for evaluation (Santhanam and Shaikh, 2019) and analyzing the models focus when emotion is prepended to the sequence (Clark et al., 2019). Along with this, we also notice other drawbacks in our work such as not having an emotional classifier to predict the outcome of the generated sentence, which we plan to address in future work. Acknowledgments This work was supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No FA8650-18-C-7881. All statements of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or policies of AFRL, DARP"
2019.ccnlg-1.3,P16-1162,0,0.0120425,"e corpus. 1 More information about the dataset made available on the (Rashkin et al., 2019) Train Valid. Test Num. Conversations 19433 2770 2547 Utterances 84324 12078 10973 Avg Length Conversations 4.31 4.36 4.31 Table 2: Statistics of Empathetic Dialogue dataset used in our experiments 3.2 Implementation In all our experiments, we use the GPT-2 pretrained language model. We use the publicly available model containing 117M parameters with 12 layers; each layer has 12 heads. We implemented our models using PyTorch Transformers.2 The input sentences are tokenized using byte-pair encoding(BPE) (Sennrich et al., 2016) (vocabulary size of 50263). While decoding, we use the nucleus sampling (p = 0.9) approach instead of beam-search to overcome the drawbacks of beam search (Holtzman et al., 2019; Ippolito et al., 2019). All our models are trained on a single TitanV GPU and takes around 2 hours to fine-tune the model. The fine-tuned models along with the configuration files and the code will be made available at: https://github.com/ sashank06/CCNLG-emotion. 3.3 Metrics Evaluating the quality of responses in open domain situations where the goal is not defined is an important area of research. Researchers have"
2019.ccnlg-1.3,P18-1205,0,0.0160307,"r research has shown that these systems are prone to producing dull and generic responses that causes engagement with the human to be affected (Vinyals and Le, 2015; Venkatesh et al., 2018). Researchers have tackled this problem of dull and generic responses through different optimization function such as MMI (Li et al., 2016b) and through reinforcement learning approaches(Li et al., 2016d). Alternative approaches towards generating more engaging responses is by grounding them in personality of the speakers that enables in creating more personalized and consistent responses (Li et al., 2016c; Zhang et al., 2018; Wolf et al., 2019). Several other works have focused on creating more engaging responses by producing affective responses. One of the earlier works to incorporate affect through language modeling is the work done by Ghosh et al. (Ghosh et al., 2017). This work leverages the LIWC (Pennebaker et al., 2001) text analysis platform for affective features. Alternative approaches of inducing emotion in generated responses from a seq2seq framework include the work done by Zhou et al(2018) that uses internal and external memory, Asghar et al. (2018) that models emotion through afExperiment Valid PPL"
2019.ccnlg-1.3,W05-0909,0,\N,Missing
2019.ccnlg-1.3,W04-1013,0,\N,Missing
2019.ccnlg-1.3,W19-4828,0,\N,Missing
2019.ccnlg-1.3,N19-1423,0,\N,Missing
2020.findings-emnlp.247,P17-1017,0,0.0224341,"find that the approach produces better responses per automated metrics and detailed human evaluations. • We propose the use of LCS-inspired representations based on asks and framings, which in turn are grounded in conversation analysis literature, to generate plans, instead of using dialogue acts. • We release corpora annotated with plans for all utterances, using three planners, including symbolic planners and attention-based planners. dialogue systems (He et al., 2018). Novikova et al. (2017) propose the E2E challenge and use MRs to show lexical richness and syntactic variation. Similarly, Gardent et al. (2017) focus on structured data (e.g. DBpedia) to generate text in the WebNLG framework. Moryossef et al. (2019) use an explicit symbolic component for planning in a neural data to text generation system that allows controllable generation. Along with conversational intents, dialogue acts are also used for natural language understanding (NLU) in task-oriented systems (Li et al., 2019; Peskov et al., 2019). In contrast to these prior approaches, our work uses more in-depth meaning representations for open-domain dialogue systems based on lexical conceptual structures (explained in Section 3.1). 2 3 R"
2020.findings-emnlp.247,W17-5525,0,0.0184266,"he following contributions: • We investigate the impact of separating planning and realization in open-domain dialogue and find that the approach produces better responses per automated metrics and detailed human evaluations. • We propose the use of LCS-inspired representations based on asks and framings, which in turn are grounded in conversation analysis literature, to generate plans, instead of using dialogue acts. • We release corpora annotated with plans for all utterances, using three planners, including symbolic planners and attention-based planners. dialogue systems (He et al., 2018). Novikova et al. (2017) propose the E2E challenge and use MRs to show lexical richness and syntactic variation. Similarly, Gardent et al. (2017) focus on structured data (e.g. DBpedia) to generate text in the WebNLG framework. Moryossef et al. (2019) use an explicit symbolic component for planning in a neural data to text generation system that allows controllable generation. Along with conversational intents, dialogue acts are also used for natural language understanding (NLU) in task-oriented systems (Li et al., 2019; Peskov et al., 2019). In contrast to these prior approaches, our work uses more in-depth meaning"
2020.findings-emnlp.247,N18-2012,0,0.0460113,"Missing"
2020.findings-emnlp.247,P02-1040,0,0.107449,"y in a ratio of 80/10/10 for the training, testing, and validation set. 4.1 Planning Phase Evaluation This evaluation focuses on investigating the efficacy of the two automated planners (Context Attention (CTX) and Pseudo-Self Attention (PSA)) in learning to generate response plans. 4.1.1 Automated Metrics Are the automated planners able to faithfully learn how to generate the response utterance plans? To investigate, we compare the performance of the CTX and the PSA planner with the symbolic planner output (which is our silver standard reference) using common automated metrics Table 2: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) on the test set. We use the library by Sharma et al. (2017). We find that PSA was able to achieve higher word overlap metrics with respect to the silver standard. We conducted an indepth analysis of the CTX and PSA planner output on the entire testing set. We found that the PSA model was more likely to produce ask actions that matched the ground truth, resulting in higher scores on the automated metrics. 4.1.2 Human Evaluation Evaluation using automated metrics provides limited evidence for the ability to aut"
2020.findings-emnlp.247,D19-1460,0,0.0215155,"planners and attention-based planners. dialogue systems (He et al., 2018). Novikova et al. (2017) propose the E2E challenge and use MRs to show lexical richness and syntactic variation. Similarly, Gardent et al. (2017) focus on structured data (e.g. DBpedia) to generate text in the WebNLG framework. Moryossef et al. (2019) use an explicit symbolic component for planning in a neural data to text generation system that allows controllable generation. Along with conversational intents, dialogue acts are also used for natural language understanding (NLU) in task-oriented systems (Li et al., 2019; Peskov et al., 2019). In contrast to these prior approaches, our work uses more in-depth meaning representations for open-domain dialogue systems based on lexical conceptual structures (explained in Section 3.1). 2 3 Related Work Open-Ended Dialogue Systems: Transformer models (Vaswani et al., 2017) and large transformer-based language models such as GPT, GPT-2, XLNet, BERT (Radford et al., 2018, 2019; Yang et al., 2019; Devlin et al., 2019) have helped achieve the SOTA performance across several natural language tasks. However, these models do not achieve the same level of consistent performance on generative mo"
2020.findings-emnlp.247,W94-0319,0,0.558422,"o Self Attention. Introduction Recent advancements in the area of generative modeling have helped increase the fluency of generative models. However, several issues persist: coherence of output and the semblance of mere repetition/hallucination of tokens from the training data (Moryossef et al., 2019; Wiseman et al., 2017). One reason could be that the generation task is typically construed as an end-to-end system. This is in contrast to traditional approaches, which incorporate a sequence of steps in the NLG system, including content determination, sentence planning, and surface realization (Reiter, 1994; Reiter and Dale, 2000). A review of literature from psycholinguistics and cognitive science also provides strong empirical evidence that the human language production process is not a monolith (Dell, 1985; Bock, 1996; Bock et al., 2007; Kennison, 2018). Prior approaches have indeed incorporated content planning into the NLG system, for example data-to-text generation problems (Puduppully et al., 2019; Moryossef et al., 2019) as well as classic works that include planning, based on speech acts (Cohen and Perrault, 1979) (for an in-depth review c.f. (Garoufi, 2014)). Our work closely follows t"
2020.findings-emnlp.247,W19-8610,1,0.842372,"92 0.0065 7.553 0.838 PSG 0.1253 0.0045 15.128 0.847 No Plan Symbolic Planner Table 5: Automated metric results on the responses generated on the test set of both corpora. input utterance but no plan as input; (2) Symbolic Planner based Generation: This model receives the plan from symbolic planner output; (3) CTX Planner-Based Generation: This model receives the CTX plan; (4) PSA Planner-Based Generation: This model receives the PSA plan. 4.2.1 Automated Metrics Prior research has shown that most automated metrics have little to no correlation to human ratings on NLG tasks (Liu et al., 2016; Santhanam and Shaikh, 2019); however, they may provide some standard of reference to evaluate performance. We report the following metrics: (i) BLEU (Papineni et al., 2002) (ii) length of responses, with the understanding that models that are able to generate longer responses are better (iii) following, Mei et al (2017), we report the diversity metric (Li et al., 2016a). Diversity is calculated as the number of distinct unigrams in the generation scaled by the total number of generated tokens (Mei et al., 2017; Li et al., 2016b). (iv) BERT-Score (Zhang* et al., 2020) metric, an embedding-based score which has shown grea"
2020.findings-emnlp.247,P16-1056,0,0.0435254,"Missing"
2020.findings-emnlp.247,J00-3003,0,0.725047,"Missing"
2020.findings-emnlp.247,D19-1062,0,0.0290866,"derstanding tasks (Ziegler et al., 2019; Edunov et al., 2019). Wolf et al. (2019) propose a transfer learning approach that fine tunes large pretrained language models and achieves SOTA scores on the PERSONA-chat dataset (Golovanov et al., 2019) and in the CONVAI2 competition (Dinan et al., 2019; Yusupov and Kuratov, 2018). Keskar et al. (2019) introduce a large-scale conditional transformer model that improves generation based on control codes. Our training paradigm is consistent with existing research that constrains large-scale language models across generation tasks (Rashkin et al., 2019; Urbanek et al., 2019) and yields controllable text generation (Shen et al., 2019; Zhou et al., 2017), with one key difference: we learn to plan and realize separately. Accordingly, we overview planning based approaches next. Planning-Based Approaches: A standard component of traditional NLG systems is a planner (Reiter and Dale, 2000). Prior work leverages intent and meaning representations (MR) to understand the content of the message (Young et al., 2013), but largely in task-oriented as opposed to open-ended 3.1 Approach NLU using Asks and Framing The representation we use to generate plans leverages asks and fr"
2020.findings-emnlp.247,C18-1312,0,0.0208261,", GPT-2, XLNet, BERT (Radford et al., 2018, 2019; Yang et al., 2019; Devlin et al., 2019) have helped achieve the SOTA performance across several natural language tasks. However, these models do not achieve the same level of consistent performance on generative modeling tasks as opposed to language understanding tasks (Ziegler et al., 2019; Edunov et al., 2019). Wolf et al. (2019) propose a transfer learning approach that fine tunes large pretrained language models and achieves SOTA scores on the PERSONA-chat dataset (Golovanov et al., 2019) and in the CONVAI2 competition (Dinan et al., 2019; Yusupov and Kuratov, 2018). Keskar et al. (2019) introduce a large-scale conditional transformer model that improves generation based on control codes. Our training paradigm is consistent with existing research that constrains large-scale language models across generation tasks (Rashkin et al., 2019; Urbanek et al., 2019) and yields controllable text generation (Shen et al., 2019; Zhou et al., 2017), with one key difference: we learn to plan and realize separately. Accordingly, we overview planning based approaches next. Planning-Based Approaches: A standard component of traditional NLG systems is a planner (Reiter and"
2020.findings-emnlp.247,P19-1566,0,0.0198767,"on of responses that are constrained by the response plan. In this phase, we only experiment with the Pseudo Self attention (PSA) model, based on Ziegler et al. (2019), who demonstrate that PSA outperforms other approaches on text generation tasks. We use nucleus sampling to overcome some of the drawbacks of beam search (Holtzman et al., 2020). 3.3 Corpora Our choice of corpora is driven by the presence of information elicitation and persuasive strategies in the utterances (i.e., asks and framings). Accordingly, we experiment with the AntiScam (Li et al., 2019) and Persuasion for Social Good (Wang et al., 2019) corpora. AntiScam contains dialogues about a customer service scenario and is specifically crowdsourced to understand human elicitation strategies. Persuasion for Social Good corpus contains interactions between workers who are assigned the roles of persuader and persuadee, 2738 AntiScam PSG 220 1017 Avg. Conversation Length 12.45 10.43 Avg. Utterance Length 11.13 19.36 Number of GIVE 2192 11587 Number of PERFORM 1681 7335 Number of GAIN 70 399 Number of LOSE 73 588 4376 8078 Number of Dialogues Number of RESPOND pling both in the planning and realization phase. All models are trained on two"
2020.findings-emnlp.247,D17-1239,0,0.0192667,"te that decoupling the process into planning and realization performs better than an end-to-end approach. 1 Figure 1: Example conversation between two speakers A & B where the response for the speaker B is generated based on the response plan from two learned planners: Context Attention and Pseudo Self Attention. Introduction Recent advancements in the area of generative modeling have helped increase the fluency of generative models. However, several issues persist: coherence of output and the semblance of mere repetition/hallucination of tokens from the training data (Moryossef et al., 2019; Wiseman et al., 2017). One reason could be that the generation task is typically construed as an end-to-end system. This is in contrast to traditional approaches, which incorporate a sequence of steps in the NLG system, including content determination, sentence planning, and surface realization (Reiter, 1994; Reiter and Dale, 2000). A review of literature from psycholinguistics and cognitive science also provides strong empirical evidence that the human language production process is not a monolith (Dell, 1985; Bock, 1996; Bock et al., 2007; Kennison, 2018). Prior approaches have indeed incorporated content planni"
2020.stoc-1.1,N03-1013,1,0.118022,"dentify the main action and arguments. For example, click here yields click as the ask and its argument here. Additional constraints are imposed through the use of a lexicon based on Lexical Conceptual Structure (LCS) (Dorr and Olsen, 2018; Dorr and Voss, 2018), derived from a pool of team members’ collected suspected scam/impersonation emails. Verbs from these emails were grouped as follows: • PERFORM: connect, copy, refer • GIVE: administer, contribute, donate • LOSE: deny, forget, surrender • GAIN: accept, earn, grab, win Additional linguistic processing includes: (1) categorial variation (Habash and Dorr, 2003) to map between different parts of speech, e.g., reference(N) → refer(V) enables detection of an explicit ask from you can reference your gift card; and (2) verbal processing to eliminate spurious asks containing verb forms such as sent or signing in sent you this email because you are signing up. 4.2.2 Motive Detection In addition to the use of distinct tools for detecting linguistic knowledge, Panacea extracts the attacker’s intention, or motive. Leveraging the attacker’s demands (asks), goals (framings) and message attack types (from the threat type classifier), the Motive Detection module"
2020.stoc-1.1,W18-3808,1,0.788683,"Missing"
2020.stoc-1.1,W18-1404,1,0.721686,"Missing"
2020.stoc-1.1,D09-1096,0,0.0526194,"Missing"
2020.stoc-1.1,P19-1247,0,0.0282452,"age the attacker to elicit identifying information, is the next advance in this arena. Prior work extracts information from email interactions (Dada et al., 2019), applies supervised learning to identify email signatures and forwarded messages (Carvalho and Cohen, 2004), and classifies email content into different structural sections (Lampert et al., 2009). Statistical and rulebased heuristics extract users’ names and aliases (Yin et al., 2011) and structured script representations determine whether an email resembles a password reset email typically sent from an organization’s IT department (Li and Goldwasser, 2019). Analysis of chatbot responses (Prakhar Gupta and Bigham, 2019) yields human-judgement correlation improvements. Approaches above differ from ours in that they require extensive model training. 1.1.1 Monitoring and Detection Panacea includes an initial protection layer based on the analysis of incoming messages. Conceptual users include end users and IT security professionals. Each message is processed and assigned a label of friend, foe, or unknown, taking into account headers and textual information of each message. The data obtained from this analysis is converted into threat intelligence"
2020.stoc-1.1,P14-5010,0,0.0108818,"d by state-ofthe-art systems in cyber threat intelligence. MISP (Wagner et al., 2016) focuses on information sharing from a community of trusted organizations. MITRE’s Collaborative Research Into Threats (CRITs) (Goffin, 2020) platform is, like Panacea, built on top of the Structured Threat Intelligence eXchange (STIX) specification. Panacea differs from these in that it is part of operational active defenses, rather than solely an analytical tool for incident response and threat reporting. 2 3 System Overview Panacea’s processing workflow is inspired by Stanford’s CoreNLP annotator pipeline (Manning et al., 2014a), but with a focus on using NLP to power active defenses against SE. A F3EAD-inspired phased analysis and engagement cycle is employed to conduct active defense operations. The cycle is triggered when a message arrives and is deconstructed into STIX threat intelligence objects. Object instances for the identities of the sender and all recipients are found or created in the knowledge base. Labeled relationships are created between those identity objects and the message itself. Once a message is ingested, plug-in components process the message in the find phase, yielding a response as a JSON o"
2020.stoc-1.1,W19-5944,0,0.0205723,"Missing"
2020.stoc-1.1,W18-5030,0,0.0283298,"ker, bulwarked by hopes of eventual payoff. Such requests are implemented as a collection of flag seeking strategies built on top of a conversational theory of asks. Flags are collected using information extraction techniques. Future work includes inferential logic and deception detection to unmask an attacker and separate them from feigned identities used to gain trust. 2 Our approach relates to work on conversational agents, e.g., response generation using neural models (Gao et al., 2019; Santhanam and Shaikh, 2019), topic models (Dziri et al., 2018), self-disclosure for targeted responses (Ravichander and Black, 2018), topic models (Bhakta and Harris, 2015), and other NLP analysis (Sawa et al., 2016). All such approaches are limited to a pre-defined set of topics, constrained by the training corpus. Other prior work focuses on persuasion detection/prediction (Hidey and McKeown, 2018) but for judging when a persuasive attempt might be successful, whereas Panacea aims to achieve effective dialogue for countering (rather than adopting) persuasive attempts. Text-based semantic analysis is also used for SE detection (Kim et al., 2018), but not for engaging with an attacker. Whereas a bot might be employed to wa"
2020.stoc-1.2,W18-1404,1,0.779842,"ain through compliance or lack thereof. It should be noted that there is no one-to-one ratio between ask and framing in the ask/framing detection output. Given the content, there may be none, one or more asks and/or framings in the output. Our lexical organization is based on Lexical Conceptual Structure (LCS), a formalism that supports resource construction and extensions to new applications such as SE detection and response generation. Semantic classes of verbs with similar meanings (give, donate) are readily augmented through adoption of the STYLUS variant of LCS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018). We derive LCS+ from asks/framings and employ CATVAR (Habash and Dorr, 2003) to relate word variants (e.g., reference and refer). Table 1 illustrates LCS+ Ask/Framing output for three (presumed) SE emails: two PERFORM asks and one GIVE ask.1 Parentheses () refer to ask arguments, often a link that the potential victim might choose to click. Ask/framing outputs are provided to downstream response generation. For example, a possible response for Table 1(a) is I will contact asap. A comparison of LCS+ to two related resources shows that our lexical organization supports refinements, improves ask"
2020.stoc-1.2,habash-dorr-2002-handling,1,0.360012,"the SE task is to waste the attacker’s time, play along, and possibly extract information that could unveil their identity. GAIN: 13.5.1 Get: You are a winner of 1M Eu. 13.5.2 Obtain: You can recover your credit rating 4. Table 2: Lexical organization of LCS+ relies on Ask Categories (PERFORM, GIVE) and Framing Categories (GIVE, LOSE). Italicized exemplars with boldfaced triggers illustrate usage for each class. Boldfaced class numbers indicate those STYLUS classes that were modified to yield the LCS+ resource. Related Work LCS is used in interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition 7 8 12 For brevity, excerpts are shown in lieu of full emails. LCS+ detects both GIVE/send and PERFORM/respond. Acknowledgments (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Ou"
2020.stoc-1.2,2006.amta-papers.7,1,0.45115,".2 Obtain: You can recover your credit rating 4. Table 2: Lexical organization of LCS+ relies on Ask Categories (PERFORM, GIVE) and Framing Categories (GIVE, LOSE). Italicized exemplars with boldfaced triggers illustrate usage for each class. Boldfaced class numbers indicate those STYLUS classes that were modified to yield the LCS+ resource. Related Work LCS is used in interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition 7 8 12 For brevity, excerpts are shown in lieu of full emails. LCS+ detects both GIVE/send and PERFORM/respond. Acknowledgments (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Our approach relates to work on conversational agents (CAs), where neural models automatically generate responses (Gao et al., 2019; Santhanam and Shaikh, 2019), topic model"
2020.stoc-1.2,W18-5030,0,0.0251272,"abash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Our approach relates to work on conversational agents (CAs), where neural models automatically generate responses (Gao et al., 2019; Santhanam and Shaikh, 2019), topic models produce focused responses (Dziri et al., 2018), self-disclosure yields targeted responses (Ravichander and Black, 2018), and SE detection employs topic models (Bhakta and Harris, 2015) and NLP of conversations (Sawa et al., 2016). However, all such approaches are limited to a pre-defined set of topics, constrained by the training corpus. Other prior work focuses on persuasion detection/ prediction (Hidey and McKeown, 2018) by leveraging argument structure, but for the purpose of judging when a persuasive attempt might be successful in subreddit discussions dedicated to changing opinions (ChangeMyView). Our work aims to achieve effective dialogue for countering (rather than adopting) persuasive attempts. Text-b"
2020.stoc-1.2,1985.tmi-1.17,0,0.202151,"Missing"
2020.stoc-1.2,W00-0207,0,0.0954748,"ategories (PERFORM, GIVE) and Framing Categories (GIVE, LOSE). Italicized exemplars with boldfaced triggers illustrate usage for each class. Boldfaced class numbers indicate those STYLUS classes that were modified to yield the LCS+ resource. Related Work LCS is used in interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition 7 8 12 For brevity, excerpts are shown in lieu of full emails. LCS+ detects both GIVE/send and PERFORM/respond. Acknowledgments (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Our approach relates to work on conversational agents (CAs), where neural models automatically generate responses (Gao et al., 2019; Santhanam and Shaikh, 2019), topic models produce focused responses (Dziri et al., 2018), self-disclosure yields targeted responses (Ravichander"
2020.stoc-1.2,N03-1013,1,0.592437,"one-to-one ratio between ask and framing in the ask/framing detection output. Given the content, there may be none, one or more asks and/or framings in the output. Our lexical organization is based on Lexical Conceptual Structure (LCS), a formalism that supports resource construction and extensions to new applications such as SE detection and response generation. Semantic classes of verbs with similar meanings (give, donate) are readily augmented through adoption of the STYLUS variant of LCS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018). We derive LCS+ from asks/framings and employ CATVAR (Habash and Dorr, 2003) to relate word variants (e.g., reference and refer). Table 1 illustrates LCS+ Ask/Framing output for three (presumed) SE emails: two PERFORM asks and one GIVE ask.1 Parentheses () refer to ask arguments, often a link that the potential victim might choose to click. Ask/framing outputs are provided to downstream response generation. For example, a possible response for Table 1(a) is I will contact asap. A comparison of LCS+ to two related resources shows that our lexical organization supports refinements, improves ask/framing detection and top ask identification, and yields qualitative improve"
2020.stoc-1.2,2003.mtsummit-systems.9,1,\N,Missing
2021.acl-srw.31,2020.lrec-1.242,0,0.017391,"ghest ranking frequent and exclusive words) or the highest probability words in each topic are correlated in any way with the concreteness values. We address this limitation as part of our future work. • Language Limitations: Our study is focused on an event that occurred in the United States. As such, all of our data are in English. As part of addressing the question of generalizability of findings, we further aim to replicate our findings in multiple languages given appropriate data. Concreteness lexicons now exist in multiple languages, including Dutch (Brysbaert et al., 2014b) and French (Bonin et al., 2020), which makes this future analysis a viable option. This suggests that the abstract construals are likely to appear both before and after the event but not during. This finding is consistent with prior research applying Construal Level Theory in lab settings. 6 Conclusion The protests that took place in Charlottesville in August of 2017 caused an outsize reaction on social media. We investigate how individuals perceive an event during its occurrence and after it ends, through the lens of Construal Level Theory. Our main finding is that adding concreteness values as covariates during topic mode"
2021.acl-srw.31,N19-1304,0,0.0198065,"deling techniques, based on probabilistic latent semantic analysis (Hofmann, 2001), latent Dirichlet allocation (LDA) (Blei and Lafferty, 2006) have been widely used to support quantitative and qualitative analysis of text data. While the topics are uncorrelated in the base LDA model, correlated topic models leverage the fact that certain topics may share words between them and thus be closer to one another (Blei et al., 2007). Topic models can be created using a variety of methods, and salient topics can be derived from tweets collected using both traditional LDA and non-traditional methods (Demszky et al., 2019). Topic models have also been used to study topics that analyze how human emotion is attached to text samples in context different than construal theory analysis (Kleinberg et al., 2020). Structured topic models (STM) (Wallach, 2008), treat the documents as sequences of segments, which can share the same prior distribution of topics. This allows the model to leverage the existing structure of documents from the given segmentation. The other advantage of using STM is that it allows for the inclusion of covariates into the prior distributions, so that variance of different topics of the variable"
2021.acl-srw.31,2020.nlpcovid19-acl.11,0,0.047484,"Missing"
2021.gem-1.10,2020.acl-main.424,0,0.0251404,"les WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling prog"
2021.gem-1.10,2020.acl-main.766,0,0.0426234,"sh datasets were ranked lower, with only WebNLG and MLSum among the top 15 datasets. We grouped all datasets by their high-level tasks and selected a group that would not violate the selection principles (e.g., only high-resource tasks). If two datasets fit, we picked the one with a higher interest rating. Among the 11 datasets, we have 18different languages, and the dataset sizes range from 5,000 examples to 1.5M, with most datasets between 50-150k examples. Two of them do not include English at all, which we hope reduces the dependence of the modeling approaches on anglocentric pretraining (Anastasopoulos and Neubig, 2020). The high-level tasks include Dialog, Summarization, Data-to-Text, and Simplification. About half of the datasets have multiple references and more than half had post-processing steps applied to them to ensure high data quality. 3.1 GEMifying the data We produce data cards (Bender and Friedman, 2018; Gebru et al., 2018) for all data sets in GEM, for which we developed an NLG-specific template.7 In addition to describing the data itself, the cards acknowledge potential limitations of a dataset regarding its creation process and describe its real-world use cases to ensure that the research is c"
2021.gem-1.10,W05-0909,0,0.165721,"otebook within 2-3 hours. 4.2 avoid overfitting to known metrics, we will use metrics on the test submissions that are not included in this initial writeup. Consequentially, the baseline results are an incomplete list which will be expanded upon the announcement of the test metrics. The set of metrics can be computed via the framework described at https://gem-benchmark. com/shared_task which comprises metrics in the following categories: Lexical Similarity. We include multiple “traditional” metrics as baseline metrics, notably BLEU (Papineni et al., 2002), ROUGE-1/2/L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). These metrics can often be gamed, for example, ROUGE can be improved by increased the output length of the model (Sun et al., 2019). Moreover, the reliability of these metrics depends on the quality and number of the references (Mathur et al., 2020a; Freitag et al., 2020). However, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 10"
2021.gem-1.10,W11-2832,0,0.0915033,"Missing"
2021.gem-1.10,2020.inlg-1.24,1,0.864971,"human evaluation standards. In recent work, Howcroft et al. (2020) investigated NLG papers from the last 2 For a more complete description of recent developments in NLG evaluation, we refer to the survey by Celikyilmaz et al. (2020). 99 twenty years and the evaluation methodologies differ drastically across papers. Moreover, in most cases, it is not even mentioned what the human evaluation aims to measure and that definitions of measures like “accuracy” or “fluency” are inconsistent. They thus suggest reporting standards for criteria and methods, following a classification system proposed by Belz et al. (2020). In addition, regularly scheduled shared tasks like WMT have lead to standardization of human evaluation setups and enabled controlled experimentation with them. GEM has the opportunity to develop reproducible standards for how human evaluation for NLG tasks beyond translation should be conducted while at the same time incorporating lessons from related work. Acting on the same need, the recently proposed GENIE (Khashabi et al., 2021) system aims to automate and standardize the human evaluation of different NLG systems, however with the contrasting goal of reducing the evaluating to a leaderb"
2021.gem-1.10,Q18-1041,0,0.236179,"han half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard ("
2021.gem-1.10,W17-4755,0,0.0383891,"Missing"
2021.gem-1.10,W16-2302,0,0.0489314,"Missing"
2021.gem-1.10,N18-2097,0,0.0498492,"Missing"
2021.gem-1.10,N19-1423,0,0.0210653,"provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation appr"
2021.gem-1.10,P19-1483,1,0.811795,"tions. 106 Figure 2: A screenshot of the interactive result exploration tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek"
2021.gem-1.10,K19-1037,0,0.027404,"Missing"
2021.gem-1.10,P17-1123,0,0.0287395,"Missing"
2021.gem-1.10,2020.acl-main.454,1,0.84177,"LEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen"
2021.gem-1.10,W19-8652,1,0.897973,"Missing"
2021.gem-1.10,C16-1191,0,0.0653297,"Missing"
2021.gem-1.10,P18-1082,0,0.0705277,"Missing"
2021.gem-1.10,W16-3622,1,0.897392,"Missing"
2021.gem-1.10,P16-2008,1,0.869379,"Missing"
2021.gem-1.10,W19-8670,1,0.884454,"Missing"
2021.gem-1.10,2020.emnlp-main.393,0,0.179031,"le to be able to address newly found limitations, and that the benchmark should focus on climbing a leaderboard. Instead, a living benchmark that can adjust its datasets and specific evaluation metrics can be much more powerful and long-lived. This can, for example, be seen in Dynabench,1 (Potts et al., 2020) which has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able mu"
2021.gem-1.10,N19-1395,0,0.0587235,"Missing"
2021.gem-1.10,2020.emnlp-main.751,0,0.503468,"nly on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover,"
2021.gem-1.10,P19-1346,1,0.897232,"Missing"
2021.gem-1.10,2020.webnlg-1.7,1,0.843551,"Missing"
2021.gem-1.10,2020.findings-emnlp.195,1,0.835947,"Missing"
2021.gem-1.10,2020.emnlp-main.5,0,0.126308,"e machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differently across tasks, setups, and languages, a multi-task NLG benchmark has the opportunity to act as a testbed to evaluate how the latest advances in automated metrics perform on these different tasks. The benchmark can facilitate this research through the release of system outputs and associated human annotations, which is what we are planning to do with GEM. Moreover, we allow the integrat"
2021.gem-1.10,2020.emnlp-main.489,0,0.0422168,"les 3 and 4. Our interactive system is centered around a parallel coordinates plot (Inselberg, 1985) which shows all results as lines through parallel axes. Every line intersects the axes at the corresponding mapped value. For instance, see the red line representing the results for task “ToTTo” of baseline “t5-small”. Filters can be applied along axes (see BLEURT axis in Figure 2) and the filtered selection is highlighted through bold lines. A selection can be a set of metrics, systems, or tasks. This style of presentation has not been used before for a benchmark. The closest prior work is by Fu et al. (2020) for namedentity recognition which allows similar filtering and sorting, but presents the results in a table. However, the parallel coordinates approach can scale to a much greater number of metrics than a table. Moreover, by using a parallel coordinates plot instead of a table, it is easy to spot patterns that span multiple metrics, systems, or tasks. For example, the highlighted line in Figure 2 uncovers that, for the T5 baseline on ToTTo, the diversity metrics score higher than other systems while scoring lower on reference-based metrics. Since we only have a single baseline for ToTTo, it i"
2021.gem-1.10,W17-3518,1,0.863605,"Missing"
2021.gem-1.10,N19-1169,1,0.923485,"eiter and Dale, 2000). These texts aim to fulfill an underlying communicative goal (e.g., to produce a summary of an article) while remaining faithful to the input information, fluent, grammatical, and natural-looking. An NLG system needs to be robust to shifts in the data distribution and be able to produce text in many different languages. Finally, it is often desired that repeated interactions with the model produce diverse outputs, for example, to explain concepts in multiple ways or to become a more interesting conversational agent. These optimization objectives can often be conflicting (Hashimoto et al., 2019) and, as a result, evaluations that focus only on a single aspect may fail to recognize the drawbacks of a particular method. To demonstrate this trade-off, consider an improvement on the CNN-DM summarization dataset (Hermann et al., 2015; Nallapati et al., 2016) measured by the ROUGE-L metWe introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent"
2021.gem-1.10,2020.ngt-1.1,0,0.0253511,"first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution"
2021.gem-1.10,2020.inlg-1.23,1,0.841029,"Missing"
2021.gem-1.10,D15-1229,0,0.0462438,"Missing"
2021.gem-1.10,2020.acl-main.709,1,0.887061,"Missing"
2021.gem-1.10,2020.acl-main.560,0,0.077305,"on does not consider differences between the languages that lead to higher modeling complexity, for example, a richer morphology or a flexible word-order. Still, the majority of work in NLP and almost all benchmarks exclusively focus on English (e.g., Wang et al., 2019b; Liu et al., 2020a; McCann et al., 2018). Even if multiple languages are considered, the availability of data in a language often does not represent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and Wiki"
2021.gem-1.10,2020.evalnlgeval-1.4,0,0.0163557,"ation tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These inclu"
2021.gem-1.10,D18-1208,0,0.0650198,"Missing"
2021.gem-1.10,Q18-1023,0,0.0642232,"Missing"
2021.gem-1.10,2020.findings-emnlp.360,1,0.934785,"e a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling progress. and conducting in-depth ana"
2021.gem-1.10,D16-1128,0,0.0680679,"Missing"
2021.gem-1.10,2020.acl-main.703,0,0.214186,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,2020.acl-main.653,0,0.233553,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,N16-1014,0,0.0410993,"t at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These include the Shannon Entropy (Shannon and Weaver, 1963) over unigrams and bigrams (H1 , H2 ), the mean segmented type token ratio over segment lengths of 100 (MSTTR, Johnson, 1944), the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that only appear once across the entire test output (Unique1,2 , Li et al., 2016). focus of this section will be on qualitative descriptions through model cards, we also gather quantitative information that is not necessarily associated with a judgment. As part of this, we collect the number of parameters of a system, as suggested by Ethayarajh and Jurafsky (2020). For each task, we additionally report the vocabulary size over the output (|V|) and the mean output length of a system (Sun et al., 2019). 5 Results One of the central aims of GEM is to measure the progress in NLG without misrepresenting the complex interactions between the sometimes contradicting measures. We t"
2021.gem-1.10,2020.findings-emnlp.165,0,0.0883829,"Missing"
2021.gem-1.10,W04-1013,0,0.355472,"n be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets"
2021.gem-1.10,2020.acl-main.465,0,0.123121,"; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard (Linzen, 2020), GEM focuses on an in-depth evaluation of model outputs across human and automatic evaluation that aims to uncover shortcomings and opportunities for progress. As datasets, metrics, and models improve, the benchmark environment will improve as well, replacing “solved” tasks with more challenging ones, incorporating newly developed metrics, and addressing discovered flaws in the experimental setup, as demonstrated in Figure 1. Making all model outputs available under an open-source license will support evaluation research and integrating new metrics will, in turn, help their adoption and incre"
2021.gem-1.10,2020.tacl-1.47,0,0.129414,"ich has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able much richer evaluation (as described in the next sections), and promote non-English datasets. In addition, it can ensure that the datasets created for those shared tasks continue being evaluated. Increasing multilingualism of NLG research. Another potentially harmful choice by benchmark creators is t"
2021.gem-1.10,2021.ccl-1.108,0,0.0502233,"Missing"
2021.gem-1.10,W15-4640,0,0.0777152,"Missing"
2021.gem-1.10,W18-6450,0,0.0170006,"ever, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automa"
2021.gem-1.10,W19-5302,0,0.0535855,"Missing"
2021.gem-1.10,2020.coling-main.420,0,0.022745,"s many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured quantities (Howcroft et al., 2020). Improving Data Improving Metrics Evaluation with gameable metrics Improving Models Consistent Human Eval Non-repeatable human evaluation Figure 1: The opportunities of living benchmarks and pitfalls of evaluation. As models improve, we need consistent evaluations such that models can be compared to each other. This can only happen if we develop robust human evaluation standards and improve our automated metrics. Ot"
2021.gem-1.10,2020.acl-main.448,0,0.212993,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.wmt-1.77,0,0.124839,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.acl-main.173,1,0.838186,"to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicti"
2021.gem-1.10,W18-3601,1,0.834682,"where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way"
2021.gem-1.10,2020.msr-1.1,1,0.821875,"Missing"
2021.gem-1.10,C18-1147,1,0.871186,"Missing"
2021.gem-1.10,2020.emnlp-main.466,0,0.0504036,"Missing"
2021.gem-1.10,D15-1238,0,0.013053,"d this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the in"
2021.gem-1.10,K16-1028,0,0.065799,"Missing"
2021.gem-1.10,D18-1206,1,0.894493,"Missing"
2021.gem-1.10,W17-5525,1,0.880333,"Missing"
2021.gem-1.10,P02-1040,0,0.114424,"n NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achi"
2021.gem-1.10,2020.emnlp-main.89,1,0.895725,"Missing"
2021.gem-1.10,W17-3537,1,0.910798,"trics saturate, we need to evaluate them on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmar"
2021.gem-1.10,2020.emnlp-main.185,0,0.0484252,"Missing"
2021.gem-1.10,2021.acl-long.186,0,0.0851423,"Missing"
2021.gem-1.10,P19-1195,0,0.0585453,"Missing"
2021.gem-1.10,N18-1012,0,0.0173282,"akers of low-resourced languages through a participatory research approach, as suggested by (∀ et al., 2020). Toward this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the result"
2021.gem-1.10,Q19-1016,0,0.0583227,"Missing"
2021.gem-1.10,J18-3002,0,0.0872491,"for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated"
2021.gem-1.10,2020.acl-main.442,0,0.137138,"hem on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generatio"
2021.gem-1.10,2020.emnlp-main.647,0,0.0370713,"Missing"
2021.gem-1.10,2021.emnlp-main.529,0,0.024648,"on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291 63.5 64.0 32.5 29.4 55.1 54.5 27.5 26.4 0.943 0.942 -0.400 -0.412 mT5-small mT5-base mT5-large mT5-XL TGen TGen+ TGen++ 0.229 0.23 0.233 0.229 0.152 0.151 0.167 47.3 48.1 51.3 52.1 13.6 13.8 9.7 28.6 28.8 30.0 31.3 0.0 0.0 0.0 43.0 44.2 46.4 47.3 13.6 13.8 9.7 17.9 17.1 17.5 17.0 0.03 0.03 0.03 0.895 0.898 0.902 0.905 0.6"
2021.gem-1.10,D19-1320,0,0.0210218,"ence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTSc"
2021.gem-1.10,2020.acl-main.704,1,0.824209,"er, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang e"
2021.gem-1.10,P19-1212,0,0.0526263,"Missing"
2021.gem-1.10,P19-1646,0,0.0614703,"Missing"
2021.gem-1.10,Q16-1029,1,0.821779,"in a news article en *25k Articles WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmar"
2021.gem-1.10,W15-3031,0,0.06329,"Missing"
2021.gem-1.10,W19-2303,0,0.167787,"of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured"
2021.gem-1.10,2020.nlp4convai-1.13,0,0.0861626,"Missing"
2021.gem-1.10,D16-1033,0,0.064867,"Missing"
2021.gem-1.10,2020.acl-main.450,0,0.015505,"2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291"
2021.gem-1.10,P18-1205,0,0.0245839,"nalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the input of the wider NLG research community. To do so, we will set up a yearly selec"
2021.ltedi-1.20,2020.peoples-1.5,0,0.589342,"pre-training on low-resources languages including Tamil and Malayalam. For our implementation, we use the Simple Transformers3 library which is built upon the transformers library by huggingface (Wolf et al., 2020). We use Adam (Kingma and Ba, 2014) as our optimizer. Our hyperparameters are presented in Table 1. Hyperparameter Value epochs bacth size α (English) α (Malayalam & Tamil) max length decay (L2) 6 8 0.00002 0.00001 256 0 Table 1: Hyperparameter values 3 Results We present detailed results for each language below. We also compare with baselines provided in the original dataset paper (Chakravarthi, 2020), and 3 https://simpletransformers.ai/ include them in the results in Tables 2, 3, and 4. We report results to 2 significant digits, since the baseline provided by the task is also limited to 2 significant digits. We also provide the dataset distribution in these tables, in the “Support” column. We discuss our findings further in Section 4. Evaluation for English. Table 2 overviews the results of detecting hope-speech, non-hope, and not-english. The baseline for English was weighted average F 1 = 0.90 (Chakravarthi, 2020). Our approach scored a weighted average F 1 = 0.93 on both the dev (N ="
2021.ltedi-1.20,2021.ltedi-1.8,0,0.099744,"ose HateXplain in which the span of text constituting the hate speech must also be detected. However, hate speech is just one facet of human behavior, especially on social media (Chakravarthi et al., 2020; Mandl et al., 2020; Chakravarthi et al., 2021; Suryawanshi and Chakravarthi, 2021). There are equally interesting studies on prosocial behaviors online including solidarity (Herrera-Viedma et al., 2015; Santhanam et al., 2021 (in press) and altruism (Althoff et al., 2014), as well as hope speech. The Hope Speech Detection for Equality, Diversity and Inclusion task at LT-EDI-2021 @ EACL2021 (Chakravarthi and Muralidaran, 2021) marks a step towards helping push for more positive, uplifting speech on social media. Exacerbated by the pandemic, and the subsequent need of communication to move to online communities, research in the detection of “hope speech”, which promotes positive, uplifting discussion to build support, is an important step (Puranik et al., 2021; Ghanghor et al., 2021). Findings from this effort could have an overall positive effect in the real world. The task mainly focuses on multilingual classification for identifying speech associated with promise, potential, support, reassurance, suggestions or i"
2021.ltedi-1.20,2021.dravidianlangtech-1.17,0,0.034336,"tiple SemEval2 tasks 1 2 https://tinyurl.com/teamuncc-hope https://semeval.github.io/ to detect hate speech, including binary (2 classes, (Basile et al., 2019)), and multi-label (>2 classes with overlaps (Mollas et al., 2020)) annotations. Hate speech detection has also further been expanded to cover explainability for the approaches used by Mathew et al. (2020), who propose HateXplain in which the span of text constituting the hate speech must also be detected. However, hate speech is just one facet of human behavior, especially on social media (Chakravarthi et al., 2020; Mandl et al., 2020; Chakravarthi et al., 2021; Suryawanshi and Chakravarthi, 2021). There are equally interesting studies on prosocial behaviors online including solidarity (Herrera-Viedma et al., 2015; Santhanam et al., 2021 (in press) and altruism (Althoff et al., 2014), as well as hope speech. The Hope Speech Detection for Equality, Diversity and Inclusion task at LT-EDI-2021 @ EACL2021 (Chakravarthi and Muralidaran, 2021) marks a step towards helping push for more positive, uplifting speech on social media. Exacerbated by the pandemic, and the subsequent need of communication to move to online communities, research in the detection o"
2021.ltedi-1.20,2020.acl-main.747,0,0.0887238,"Missing"
2021.ltedi-1.20,N19-1423,0,0.0308011,"assurance, suggestions or inspiration provided to participants by their peers during periods of illness, stress, loneliness and depression (Snyder et al., 2005). In addition to being a multi-class problem, each language (English, Tamil and Malayalam) also had differing amounts of class imbalance. We describe our approach in detail in Section 2 and present our results in Section 3. 136 Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion, pages 136–142 April 19, 2021. ©2020 Association for Computational Linguistics 2 Method RoBERTa is an improved BERT (Devlin et al., 2019) model by Facebook AI which achieves stateof-the-art results on several natural language understanding (NLU) tasks including GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016). RoBERTa is improved through training BERT for a longer duration on longer sequences, increasing the data quantity, removing the sentence prediction objective during pre-training, and changing the masking pattern applied during pre-training. With the aim of improving cross-lingual language understanding (XLU), XLM-RoBERTa was developed using a Transformer-based masked language model (MLM). XLM-RoBERTa was pre-tr"
2021.ltedi-1.20,2021.ccl-1.108,0,0.0709107,"Missing"
2021.ltedi-1.20,D16-1264,0,0.0322189,"g a multi-class problem, each language (English, Tamil and Malayalam) also had differing amounts of class imbalance. We describe our approach in detail in Section 2 and present our results in Section 3. 136 Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion, pages 136–142 April 19, 2021. ©2020 Association for Computational Linguistics 2 Method RoBERTa is an improved BERT (Devlin et al., 2019) model by Facebook AI which achieves stateof-the-art results on several natural language understanding (NLU) tasks including GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016). RoBERTa is improved through training BERT for a longer duration on longer sequences, increasing the data quantity, removing the sentence prediction objective during pre-training, and changing the masking pattern applied during pre-training. With the aim of improving cross-lingual language understanding (XLU), XLM-RoBERTa was developed using a Transformer-based masked language model (MLM). XLM-RoBERTa was pre-trained using 2 terabytes of CommonCrawl data (Wenzek et al., 2020) containing one hundred languages. XLM-RoBERTa outperforms its multilingual MLMs mBERT (Devlin et al., 2019) and XLM (L"
2021.ltedi-1.20,D19-1410,0,0.0202878,"lower than the our original submission results with the exception of the hopespeech class. On the Malayalam dev set, we scored weighted average F 1 = 0.84, F 1 = 0.64 for hope-speech, F 1 = 0.89 for non-hope, and F 1 = 0.75 for non-malayalam. We note that all of the individual class results are slightly lower than the our submission results. 4 Qualitative Evaluation In this section, we analytically go through sample records in the English test set to better understand the dataset, and to evaluate the strengths and weaknesses of our approach. Analytical Evaluation. We use sentencetransformers (Reimers and Gurevych, 2019, 2020) to further evaluate our approach. Sentence transformers were created by utilizing word-level representation models such as BERT and RoBERTa for better downstream computational performance on sentence-level tasks, since utilizing wordlevel representations for tasks such as determining the similarity of 2 sentences takes much longer than computing sentence-level representations. We use the paraphrase-xlm-rmultilingual-v1 pretrained model to get our sentence embeddings, and then use K-means clustering using scikit-learn (Pedregosa et al., 2011) to cluster the test set into 3 clusters, the"
2021.ltedi-1.20,2020.emnlp-main.365,0,0.0466726,"Missing"
2021.ltedi-1.20,2021.dravidianlangtech-1.16,0,0.0317537,"tps://tinyurl.com/teamuncc-hope https://semeval.github.io/ to detect hate speech, including binary (2 classes, (Basile et al., 2019)), and multi-label (>2 classes with overlaps (Mollas et al., 2020)) annotations. Hate speech detection has also further been expanded to cover explainability for the approaches used by Mathew et al. (2020), who propose HateXplain in which the span of text constituting the hate speech must also be detected. However, hate speech is just one facet of human behavior, especially on social media (Chakravarthi et al., 2020; Mandl et al., 2020; Chakravarthi et al., 2021; Suryawanshi and Chakravarthi, 2021). There are equally interesting studies on prosocial behaviors online including solidarity (Herrera-Viedma et al., 2015; Santhanam et al., 2021 (in press) and altruism (Althoff et al., 2014), as well as hope speech. The Hope Speech Detection for Equality, Diversity and Inclusion task at LT-EDI-2021 @ EACL2021 (Chakravarthi and Muralidaran, 2021) marks a step towards helping push for more positive, uplifting speech on social media. Exacerbated by the pandemic, and the subsequent need of communication to move to online communities, research in the detection of “hope speech”, which promotes posit"
2021.ltedi-1.20,W18-5446,0,0.0267203,"l., 2005). In addition to being a multi-class problem, each language (English, Tamil and Malayalam) also had differing amounts of class imbalance. We describe our approach in detail in Section 2 and present our results in Section 3. 136 Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion, pages 136–142 April 19, 2021. ©2020 Association for Computational Linguistics 2 Method RoBERTa is an improved BERT (Devlin et al., 2019) model by Facebook AI which achieves stateof-the-art results on several natural language understanding (NLU) tasks including GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016). RoBERTa is improved through training BERT for a longer duration on longer sequences, increasing the data quantity, removing the sentence prediction objective during pre-training, and changing the masking pattern applied during pre-training. With the aim of improving cross-lingual language understanding (XLU), XLM-RoBERTa was developed using a Transformer-based masked language model (MLM). XLM-RoBERTa was pre-trained using 2 terabytes of CommonCrawl data (Wenzek et al., 2020) containing one hundred languages. XLM-RoBERTa outperforms its multilingual MLMs mBE"
2021.ltedi-1.20,2020.lrec-1.494,0,0.0259685,"Missing"
2021.ltedi-1.20,2020.emnlp-demos.6,0,0.0611765,"Missing"
2021.nlp4prog-1.7,L18-1491,0,0.0135195,"of many small sub-components tuned separately, attempts to build and train a single, large neural network that reads a sentence and outputs a correct translation (Bahdanau et al., 2015). NMT has emerged as a promising machine translation approach, showing superior performance on public benchmarks (Bojar et al., 2016), and it is widely recognized as the premier method for the translation of different languages (Wu et al., 2016). NMT has also been used to perform complex tasks on the UNIX operating system shell (Lin et al., 2017) (e.g. file manipulation and search), by stating goals in English (Lin et al., 2018), to automatically generate commit messages (Liu et al., 2018), etc. However, the NMT techniques have not heretofore been adopted to automatically generate software exploits from natural language comments. Since NMT is a data-driven approach to code generation, we need a dataset of intents in natural language, and their corresponding translation (in our context, in assembly language) for shellcode generation. In this preliminary work, we address the lack of such a dataset by presenting Shellcode IA32, a dataset containing 3, 200 lines of assembly code extracted from real shellcodes and describ"
2021.nlp4prog-1.7,P16-1057,0,0.0380227,"Missing"
2021.nlp4prog-1.7,P16-1138,0,0.0276414,"neural machine translation (NMT) to establish baseline performance levels on this task. 1 Introduction and Related Work A growing body of research has dealt with automated code generation: given a natural language description, a code comment or intent, the task is to generate a piece of code in a programming language (Yin and Neubig, 2017; Ling et al., 2016). The task of generating programming code snippets, also referred to as semantic parsing (Yin and Neubig, 2019; Xu et al., 2020), has been previously addressed to generate executable snippets in domain-specific languages (Guu et al., 2017; Long et al., 2016), and several programming languages, including Python (Yin and Neubig, 2017) and Java (Ling et al., 2016). We consider the task of generating shellcodes, i.e., small pieces of code used as a payload to exploit software vulnerabilities. Shellcoding, in its most literal sense, means writing code that will return a remote shell when executed. It can represent any byte code that will be inserted into an exploit to accomplish the desired, malicious, task (Mason et al., 2009). An example of a shellcode program in assembly language and the corresponding natural language comments are shown in Listing"
2021.nlp4prog-1.7,W18-1818,0,0.0661108,"Missing"
2021.nlp4prog-1.7,P17-1097,0,0.0115166,"andard methods in neural machine translation (NMT) to establish baseline performance levels on this task. 1 Introduction and Related Work A growing body of research has dealt with automated code generation: given a natural language description, a code comment or intent, the task is to generate a piece of code in a programming language (Yin and Neubig, 2017; Ling et al., 2016). The task of generating programming code snippets, also referred to as semantic parsing (Yin and Neubig, 2019; Xu et al., 2020), has been previously addressed to generate executable snippets in domain-specific languages (Guu et al., 2017; Long et al., 2016), and several programming languages, including Python (Yin and Neubig, 2017) and Java (Ling et al., 2016). We consider the task of generating shellcodes, i.e., small pieces of code used as a payload to exploit software vulnerabilities. Shellcoding, in its most literal sense, means writing code that will return a remote shell when executed. It can represent any byte code that will be inserted into an exploit to accomplish the desired, malicious, task (Mason et al., 2009). An example of a shellcode program in assembly language and the corresponding natural language comments a"
2021.nlp4prog-1.7,P02-1040,0,0.113168,"Missing"
2021.nlp4prog-1.7,2020.acl-main.538,0,0.0118795,"f challenging but common assembly instructions with their natural language descriptions. We experiment with standard methods in neural machine translation (NMT) to establish baseline performance levels on this task. 1 Introduction and Related Work A growing body of research has dealt with automated code generation: given a natural language description, a code comment or intent, the task is to generate a piece of code in a programming language (Yin and Neubig, 2017; Ling et al., 2016). The task of generating programming code snippets, also referred to as semantic parsing (Yin and Neubig, 2019; Xu et al., 2020), has been previously addressed to generate executable snippets in domain-specific languages (Guu et al., 2017; Long et al., 2016), and several programming languages, including Python (Yin and Neubig, 2017) and Java (Ling et al., 2016). We consider the task of generating shellcodes, i.e., small pieces of code used as a payload to exploit software vulnerabilities. Shellcoding, in its most literal sense, means writing code that will return a remote shell when executed. It can represent any byte code that will be inserted into an exploit to accomplish the desired, malicious, task (Mason et al., 2"
2021.nlp4prog-1.7,P17-1041,0,0.02522,"xploitation of a software vulnerability, starting from natural language comments. We assemble and release a novel dataset (Shellcode IA32), consisting of challenging but common assembly instructions with their natural language descriptions. We experiment with standard methods in neural machine translation (NMT) to establish baseline performance levels on this task. 1 Introduction and Related Work A growing body of research has dealt with automated code generation: given a natural language description, a code comment or intent, the task is to generate a piece of code in a programming language (Yin and Neubig, 2017; Ling et al., 2016). The task of generating programming code snippets, also referred to as semantic parsing (Yin and Neubig, 2019; Xu et al., 2020), has been previously addressed to generate executable snippets in domain-specific languages (Guu et al., 2017; Long et al., 2016), and several programming languages, including Python (Yin and Neubig, 2017) and Java (Ling et al., 2016). We consider the task of generating shellcodes, i.e., small pieces of code used as a payload to exploit software vulnerabilities. Shellcoding, in its most literal sense, means writing code that will return a remote s"
2021.nlp4prog-1.7,P19-1447,0,0.0157348,"de IA32), consisting of challenging but common assembly instructions with their natural language descriptions. We experiment with standard methods in neural machine translation (NMT) to establish baseline performance levels on this task. 1 Introduction and Related Work A growing body of research has dealt with automated code generation: given a natural language description, a code comment or intent, the task is to generate a piece of code in a programming language (Yin and Neubig, 2017; Ling et al., 2016). The task of generating programming code snippets, also referred to as semantic parsing (Yin and Neubig, 2019; Xu et al., 2020), has been previously addressed to generate executable snippets in domain-specific languages (Guu et al., 2017; Long et al., 2016), and several programming languages, including Python (Yin and Neubig, 2017) and Java (Ling et al., 2016). We consider the task of generating shellcodes, i.e., small pieces of code used as a payload to exploit software vulnerabilities. Shellcoding, in its most literal sense, means writing code that will return a remote shell when executed. It can represent any byte code that will be inserted into an exploit to accomplish the desired, malicious, tas"
2021.sigdial-1.36,andreas-etal-2012-annotating,0,0.034396,"t al., 2013). Online forums such as Reddit, and Wikipedia have also contributed to such corpora. These notably include the Reddit (Chang et al., 2020) corpus which has also been extended into larger corpora (Baumgartner et al., 2020). There have also been argumentative corpora obtained from online interactions, like the Reddit Domestic Abuse Corpus (Schrading et al., 2015) taken from subreddits specific on domestic abuse, allowing for discourse analysis on this subject. Debate and agreement corpora such as the Internet Argument Corpus (Walker et al., 2012b), Agreement in Wikipedia Talk Pages (Andreas et al., 2012) and Agreement by Create Debaters (Rosenthal and McKeown, 2015), from debate and discussion forums online such as CreateDebate also contribute towards argumentation in dialogue research (Rakshit et al., 2018). Additionally, there have been corpora obtained from social media such as UseNet and Twitter. These include the UseNet Corpus (Shaoul and Westbury, 2007, 2011), a platform which is considered a precursor to more recent forums; and the Twitter Corpus (Ritter et al., 2010), which was intended to help model dialogue acts. 3.3 Special Mentions This section includes special mentions of corpora"
2021.sigdial-1.36,2020.sigdial-1.8,0,0.133892,"s, local topics and meso-topics, and has been used to understand user roles and modeling leadership and influence (Strzalkowski et al., 2012). Game-playing corpora such as the Settlers of Catan Corpus (Afantenos et al., 2012) and Cards Corpus (Djalali et al., 2011) are great informal additions to chatroom corpora, with a competitive environment albeit in an informal setting. They have been used for tasks such as training models for negotiation dialogues (Cadilhac et al., 2013). Online forums such as Reddit, and Wikipedia have also contributed to such corpora. These notably include the Reddit (Chang et al., 2020) corpus which has also been extended into larger corpora (Baumgartner et al., 2020). There have also been argumentative corpora obtained from online interactions, like the Reddit Domestic Abuse Corpus (Schrading et al., 2015) taken from subreddits specific on domestic abuse, allowing for discourse analysis on this subject. Debate and agreement corpora such as the Internet Argument Corpus (Walker et al., 2012b), Agreement in Wikipedia Talk Pages (Andreas et al., 2012) and Agreement by Create Debaters (Rosenthal and McKeown, 2015), from debate and discussion forums online such as CreateDebate al"
2021.sigdial-1.36,2020.coling-main.393,0,0.0325765,"ks such as summarization is the TVD Corpus (Roy et al., 2014). It has been used to build models for speaker identification (Knyazeva et al., 2015). The Serial Speakers (Bost et al., 2020) dataset supplements data from both the aforementioned TV serials by also including the House of Cards and additional annotations. Recently, the Multimodal EmotionLines Dataset (MELD) (Poria et al., 2019) corpus has been presented by extending the (ELD) (Hsu et al., 2018), with audio-visual modality along with text. It has been used as a resource for Dialogue Act Classification (Saha et al., 2020). The MEISD (Firdaus et al., 2020) dataset is build further with TV scripts 341 from 10 series, adding Friends, How I Met Your Mother, The Office, House M.D., Grey’s Anatomy, Castle, Breaking Bad to the aforementioned series. 3.2 Written Corpora Written corpora for multi-party have often resulted from online chatroom discussions, like the NPS Chat Corpus (Forsythand and Martell, 2007), which is shared as a part of the NLTK (Loper and Bird, 2002), and is one of the first ComputerMediated corpora. The Ubuntu IRC chatroom has also contributed to corpora such as the Ubuntu Dialogue Corpus (Lowe et al., 2015) and Ubuntu Chat Corpus"
2021.sigdial-1.36,2020.inlg-1.46,0,0.0110917,"ther towards advancing this area of dialogue research. 1 Introduction To say research in conversational agents and natural language generation has seen an explosive growth in recent years would be an understatement, as evidenced by the increasing number of papers published on this topic. However, most current research in this area has focused on two-party or dyadic conversations. This focus is important, since many open questions remain with dialogue systems in dyadic settings, such as modeling longterm dialogue context modeling and infusion of knowledge, persona and empathy (Li et al., 2016; Hedayatnia et al., 2020; Liu et al., 2020) Nevertheless, there is still a pressing need to focus on more naturally occurring conversations which consist of more than two speakers (Kirchhoff and Ostendorf, 2003), also known as multiparty dialogue. Humans naturally tend to work in groups and teams. Conversational agents capable of working in multi-party dialogue situations stand to advance the future of work, since they can be integrated into teams, e.g., in surgery, search and rescue, or manufacturing and design. The settings for such agents could be informal (e.g. chatroom assistants) or formal (e.g. meeting assista"
2021.sigdial-1.36,2020.inlg-1.23,0,0.0704776,"Missing"
2021.sigdial-1.36,L18-1252,0,0.0218622,"et al., 2019). A TV series corpus including data from shows like The Big Bang Theory and Game of Thrones, supplemented by crowd-sourced contributions for tasks such as summarization is the TVD Corpus (Roy et al., 2014). It has been used to build models for speaker identification (Knyazeva et al., 2015). The Serial Speakers (Bost et al., 2020) dataset supplements data from both the aforementioned TV serials by also including the House of Cards and additional annotations. Recently, the Multimodal EmotionLines Dataset (MELD) (Poria et al., 2019) corpus has been presented by extending the (ELD) (Hsu et al., 2018), with audio-visual modality along with text. It has been used as a resource for Dialogue Act Classification (Saha et al., 2020). The MEISD (Firdaus et al., 2020) dataset is build further with TV scripts 341 from 10 series, adding Friends, How I Met Your Mother, The Office, House M.D., Grey’s Anatomy, Castle, Breaking Bad to the aforementioned series. 3.2 Written Corpora Written corpora for multi-party have often resulted from online chatroom discussions, like the NPS Chat Corpus (Forsythand and Martell, 2007), which is shared as a part of the NLTK (Loper and Bird, 2002), and is one of the fir"
2021.sigdial-1.36,W03-0703,0,0.181465,"t years would be an understatement, as evidenced by the increasing number of papers published on this topic. However, most current research in this area has focused on two-party or dyadic conversations. This focus is important, since many open questions remain with dialogue systems in dyadic settings, such as modeling longterm dialogue context modeling and infusion of knowledge, persona and empathy (Li et al., 2016; Hedayatnia et al., 2020; Liu et al., 2020) Nevertheless, there is still a pressing need to focus on more naturally occurring conversations which consist of more than two speakers (Kirchhoff and Ostendorf, 2003), also known as multiparty dialogue. Humans naturally tend to work in groups and teams. Conversational agents capable of working in multi-party dialogue situations stand to advance the future of work, since they can be integrated into teams, e.g., in surgery, search and rescue, or manufacturing and design. The settings for such agents could be informal (e.g. chatroom assistants) or formal (e.g. meeting assistants) settings. Particularly, with conversational assistants such as Amazon Alexa, there is a push to develop AI to understand multiple users and act as teammates (Winkler et al., 2019; Se"
2021.sigdial-1.36,L18-1466,0,0.0281361,"wering task in multi-party dialogue is the recently introduced QAConv corpus (Wu et al., 2021), with 34k questions taken from about 28k dialogues, with around 26k words and 32 speakers consisting of conversations taken from email, panels and other formal communication channels. There are also several corpora, especially multimodal, which have been transcribed, but we could not find the statistics. These include the VACE multimodal meeting corpus (Chen et al., 2005), which investigates the interaction among speech, gesture, posture, and gaze in meetings. Another corpus is the MULTISIMO corpus (Koutsombogera and Vogel, 2018), towards modeling of collaborative aspects of multimodal behavior in groups that perform simple tasks between 2 people, supported by a facilitator. Mana et al. (2007) also present the Mission Survival Corpora (MSC) 1 and 2, a multi-modal corpus of multi-party meetings, automatically annotated using audio-visual cues (speech rate, pitch and energy, head orientation, hand and body fidgeting). Due to the limited information available, we do not add these corpora to the tables or the taxonomy. A variation of the Machines Talking to Machines framework (Shah et al., 2018) allows a simulated user bo"
2021.sigdial-1.36,N04-4034,0,0.0319782,"f English Dialogues (CED) (Kyt¨o and Walker, 2006), which has been used to study signalling function in discourse (Lenker, 2018). Supplementing formal discourse in debate corpora are formal meeting corpora, with 2 corpora that have become really important for studying multi-party decision-making and discussions of actions to take are the ICSI meeting corpus (Janin et al., 2003), which also has Meeting Recorder Dialogue Act (MRDA) annotations (Shriberg et al., 2004); and the multi-modal AMI meeting corpus (Renals et al., 2007). ICSI has been used to further study multi-party language modeling (Ji and Bilmes, 2004), and AMI has been used to build summarization for meetings (Zhu et al., 2020). Recent additions include data from interviews, such as the INTERVIEW (Majumder et al., 2020) and MediaSum (Zhu et al., 2021) corpora. They include transcripts from interviews on channels such as National Public Radio NPR and CNN. 3.1.2 Scripted Spoken Corpora Scripted spoken corpora consist of pre-defined scripts such as those for plays, movies, and TV series. These are inherently different as they are not spontaneous, and have pre-defined roles for speakers as well as information on when the dialogues turns are ta"
2021.sigdial-1.36,C94-1103,0,0.273416,"ovie scripts. The distinction between scripted and unscripted is made to allow for dif339 Figure 1: Taxonomy of available Multi-party Corpora, organized by source type. ferent modelling tasks, since scripted dialogue displays an absence of hesitations, repetitions and other normal non-fluency features. 3.1.1 Unscripted Spoken Corpora One of the earliest multi-party spoken corpora is the British National Corpus (BNC) (Leech, 1992), originally created by the Oxford University press in 1980s-1990s. Covering a wide range of genres, including some written conversations, as well as POS-tagged data (Leech et al., 1994), it is important as a generalized multi-party conversation corpus. It has been used to study social differentiation in the use of English vocabulary (Rayson et al., 1997), word frequency differences in spoken vs written text (Leech et al., 2001), and amplifiers such as “very” and “so” in the English language (Xiao and Tao, 2007). The Cambridge and Nottingham Corpus of Discourse in English (CANCODE) (McCarthy, 1998) focuses on interpersonal communication conversations in various settings such as hair salons and restaurants. It has been used to study language use for teaching in classrooms (O’k"
2021.sigdial-1.36,2020.coling-main.238,0,0.0471126,"Missing"
2021.sigdial-1.36,P16-1094,0,0.0116527,"to contribute further towards advancing this area of dialogue research. 1 Introduction To say research in conversational agents and natural language generation has seen an explosive growth in recent years would be an understatement, as evidenced by the increasing number of papers published on this topic. However, most current research in this area has focused on two-party or dyadic conversations. This focus is important, since many open questions remain with dialogue systems in dyadic settings, such as modeling longterm dialogue context modeling and infusion of knowledge, persona and empathy (Li et al., 2016; Hedayatnia et al., 2020; Liu et al., 2020) Nevertheless, there is still a pressing need to focus on more naturally occurring conversations which consist of more than two speakers (Kirchhoff and Ostendorf, 2003), also known as multiparty dialogue. Humans naturally tend to work in groups and teams. Conversational agents capable of working in multi-party dialogue situations stand to advance the future of work, since they can be integrated into teams, e.g., in surgery, search and rescue, or manufacturing and design. The settings for such agents could be informal (e.g. chatroom assistants) or for"
2021.sigdial-1.36,L16-1147,0,0.0161446,"e script, and has been used to generate emotionally aligned responses to dialogue (Asghar et al., 2020). The Character Style From Film Corpus (Walker et al., 2012a) is another resource contributing towards guided text generation by providing character styles, created from the archive IMSDB. It has been used to generate stylistic dialogue for narratives (Xu et al., 2018). Both the OpenSubtitles (Tiedemann, 2012) and SubTle corpus (Ameixa and Coheur, 2013) are based on the OpenSubtitles site. They are corpora of plain scripts, but the website continues to contribute as a resource for more data (Lison and Tiedemann, 2016; Lison et al., 2018). Bridging the sources of movie and TV scripts is the Corpus of American Soap Operas (Davies, 2013) which focuses on informal language, and has been used to study cultural representation differences in American soap operas (Khaghaninejad et al., 2019). A TV series corpus including data from shows like The Big Bang Theory and Game of Thrones, supplemented by crowd-sourced contributions for tasks such as summarization is the TVD Corpus (Roy et al., 2014). It has been used to build models for speaker identification (Knyazeva et al., 2015). The Serial Speakers (Bost et al., 20"
2021.sigdial-1.36,L18-1275,0,0.0126384,"to generate emotionally aligned responses to dialogue (Asghar et al., 2020). The Character Style From Film Corpus (Walker et al., 2012a) is another resource contributing towards guided text generation by providing character styles, created from the archive IMSDB. It has been used to generate stylistic dialogue for narratives (Xu et al., 2018). Both the OpenSubtitles (Tiedemann, 2012) and SubTle corpus (Ameixa and Coheur, 2013) are based on the OpenSubtitles site. They are corpora of plain scripts, but the website continues to contribute as a resource for more data (Lison and Tiedemann, 2016; Lison et al., 2018). Bridging the sources of movie and TV scripts is the Corpus of American Soap Operas (Davies, 2013) which focuses on informal language, and has been used to study cultural representation differences in American soap operas (Khaghaninejad et al., 2019). A TV series corpus including data from shows like The Big Bang Theory and Game of Thrones, supplemented by crowd-sourced contributions for tasks such as summarization is the TVD Corpus (Roy et al., 2014). It has been used to build models for speaker identification (Knyazeva et al., 2015). The Serial Speakers (Bost et al., 2020) dataset supplemen"
2021.sigdial-1.36,D16-1149,0,0.028611,"ced in multi-party dialogue such as turn-taking, and has been used to evaluate such systems (Raffensperger et al., 2012). The IDIAP Wolf corpus (Hung and Chittaranjan, 2010) focuses on group behavior in a competitive role-playing game setting, with a pre-condition of bad faith interactions similar to the “werewolf” or “mafia” game that makes it a unique corpus. It has been used in the AIWolfDial task to help train game-playing AI (Kano et al., 2019). While specific instances of lying are not annotated, the “werewolf” of each game is annotated in the corpus. On the flip side, the TEAMS corpus (Litman et al., 2016) where teams of three or four speakers play two rounds of a cooperative board game, provides a novel resource for studying team entrainment and participation dominance. Rahimi and Litman (2020) use it to build a novel graph-based vector representation of multi-party entrainment, gaining insights into the dynamics of the entrainment relations. Recently, the Critical Role Dungeons and Dragons Dataset (CRD3) (Rameshkumar and Bailey, 2020) was released, which is a game-based corpus set in an open-ended scenario. The paper also provides an abstractive summarization benchmark and evaluation, based o"
2021.sigdial-1.36,W02-0109,0,0.287835,"ed by extending the (ELD) (Hsu et al., 2018), with audio-visual modality along with text. It has been used as a resource for Dialogue Act Classification (Saha et al., 2020). The MEISD (Firdaus et al., 2020) dataset is build further with TV scripts 341 from 10 series, adding Friends, How I Met Your Mother, The Office, House M.D., Grey’s Anatomy, Castle, Breaking Bad to the aforementioned series. 3.2 Written Corpora Written corpora for multi-party have often resulted from online chatroom discussions, like the NPS Chat Corpus (Forsythand and Martell, 2007), which is shared as a part of the NLTK (Loper and Bird, 2002), and is one of the first ComputerMediated corpora. The Ubuntu IRC chatroom has also contributed to corpora such as the Ubuntu Dialogue Corpus (Lowe et al., 2015) and Ubuntu Chat Corpus (Uthus and Aha, 2013), which were collected as users asked questions relating to Ubuntu on the forum, and other users answered the questions. They have been used to train end-to-end dialogue systems (Lowe et al., 2017). The Molweni corpus (Li et al., 2020) builds on the Ubuntu Chat Dialogue corpus, and adds annotations for machine reading comprehension and dscourse parsing. Another corpus based on chatroom data"
2021.sigdial-1.36,W15-4640,0,0.0320232,", 2020). The MEISD (Firdaus et al., 2020) dataset is build further with TV scripts 341 from 10 series, adding Friends, How I Met Your Mother, The Office, House M.D., Grey’s Anatomy, Castle, Breaking Bad to the aforementioned series. 3.2 Written Corpora Written corpora for multi-party have often resulted from online chatroom discussions, like the NPS Chat Corpus (Forsythand and Martell, 2007), which is shared as a part of the NLTK (Loper and Bird, 2002), and is one of the first ComputerMediated corpora. The Ubuntu IRC chatroom has also contributed to corpora such as the Ubuntu Dialogue Corpus (Lowe et al., 2015) and Ubuntu Chat Corpus (Uthus and Aha, 2013), which were collected as users asked questions relating to Ubuntu on the forum, and other users answered the questions. They have been used to train end-to-end dialogue systems (Lowe et al., 2017). The Molweni corpus (Li et al., 2020) builds on the Ubuntu Chat Dialogue corpus, and adds annotations for machine reading comprehension and dscourse parsing. Another corpus based on chatroom data is the Multi-Party Chat (MPC) Corpus (Shaikh et al., 2010) which presents an annotated corpus based on four levels with communication links, dialogue acts, local"
2021.sigdial-1.36,2020.acl-main.131,0,0.0305929,"Missing"
2021.sigdial-1.36,liu-etal-2012-extending,1,0.854632,"Missing"
2021.sigdial-1.36,P19-1050,0,0.0201422,"ural representation differences in American soap operas (Khaghaninejad et al., 2019). A TV series corpus including data from shows like The Big Bang Theory and Game of Thrones, supplemented by crowd-sourced contributions for tasks such as summarization is the TVD Corpus (Roy et al., 2014). It has been used to build models for speaker identification (Knyazeva et al., 2015). The Serial Speakers (Bost et al., 2020) dataset supplements data from both the aforementioned TV serials by also including the House of Cards and additional annotations. Recently, the Multimodal EmotionLines Dataset (MELD) (Poria et al., 2019) corpus has been presented by extending the (ELD) (Hsu et al., 2018), with audio-visual modality along with text. It has been used as a resource for Dialogue Act Classification (Saha et al., 2020). The MEISD (Firdaus et al., 2020) dataset is build further with TV scripts 341 from 10 series, adding Friends, How I Met Your Mother, The Office, House M.D., Grey’s Anatomy, Castle, Breaking Bad to the aforementioned series. 3.2 Written Corpora Written corpora for multi-party have often resulted from online chatroom discussions, like the NPS Chat Corpus (Forsythand and Martell, 2007), which is shared"
2021.sigdial-1.36,2020.acl-main.459,0,0.031149,"AI (Kano et al., 2019). While specific instances of lying are not annotated, the “werewolf” of each game is annotated in the corpus. On the flip side, the TEAMS corpus (Litman et al., 2016) where teams of three or four speakers play two rounds of a cooperative board game, provides a novel resource for studying team entrainment and participation dominance. Rahimi and Litman (2020) use it to build a novel graph-based vector representation of multi-party entrainment, gaining insights into the dynamics of the entrainment relations. Recently, the Critical Role Dungeons and Dragons Dataset (CRD3) (Rameshkumar and Bailey, 2020) was released, which is a game-based corpus set in an open-ended scenario. The paper also provides an abstractive summarization benchmark and evaluation, based on each dialogue’s summary. Within formal settings, one of the oldest corpus is the Corpus of Spoken, Professional AmericanEnglish (CSPAE) (Barlow, 2000), consisting of two main components. The first is White House press conferences, and the second is transcripts of meetings on national tests involving statements, discussions, and questions. In the past, it has proved a valuable resource for studying idioms and their 340 usage (Liu, 200"
2021.sigdial-1.36,J18-3002,0,0.0127294,"ed for specialized equipment and environments. • providing recommendations for collecting new useful datasets, to advance research in this area. Our intent is that with an up-to-date synthesis of available resources, and by drawing attention to the challenges particular to multi-party dialogue, we can provide insights of exploiting recent datadriven techniques to address these challenges. 2 Method Selection Criteria: Similar to recent work in systematic review of relevant literature, we followed the PRISMA method to identify, screen and include articles for this survey (Howcroft et al., 2020; Reiter, 2018). We searched Google Scholar and Semantic Scholar for the keywords multi-party dialogue and variations thereof (e.g., multi-party, multiparty conversation). We began by considering all papers that appeared in conferences and journals which focus on NLP and NLG, including all × CL venues as well as AI conferences and venues (e.g., AAAI, IJCNLP, Interspeech). We then iterated through the references and citations of these papers, and included any relevant articles that were missed through keyword search. This identification step resulted in 362 papers overall. As part of our screening process, we"
2021.sigdial-1.36,N10-1020,0,0.0351766,"and agreement corpora such as the Internet Argument Corpus (Walker et al., 2012b), Agreement in Wikipedia Talk Pages (Andreas et al., 2012) and Agreement by Create Debaters (Rosenthal and McKeown, 2015), from debate and discussion forums online such as CreateDebate also contribute towards argumentation in dialogue research (Rakshit et al., 2018). Additionally, there have been corpora obtained from social media such as UseNet and Twitter. These include the UseNet Corpus (Shaoul and Westbury, 2007, 2011), a platform which is considered a precursor to more recent forums; and the Twitter Corpus (Ritter et al., 2010), which was intended to help model dialogue acts. 3.3 Special Mentions This section includes special mentions of corpora as well as frameworks and toolkits that do not fall under our previous categories. There are very few corpora which have focused on human-machine dialogue for multi-party interactions. The only such corpora existing to the best of our knowledge is the Mission Rehearsal Exercise (MRE) Corpus (Robinson et al., 2004), which presents a dataset built as audio face-toface sessions between human trainees and virtual agents. The main theme of the multimodal dataset is decision-makin"
2021.sigdial-1.36,robinson-etal-2004-issues,0,0.157254,"Twitter. These include the UseNet Corpus (Shaoul and Westbury, 2007, 2011), a platform which is considered a precursor to more recent forums; and the Twitter Corpus (Ritter et al., 2010), which was intended to help model dialogue acts. 3.3 Special Mentions This section includes special mentions of corpora as well as frameworks and toolkits that do not fall under our previous categories. There are very few corpora which have focused on human-machine dialogue for multi-party interactions. The only such corpora existing to the best of our knowledge is the Mission Rehearsal Exercise (MRE) Corpus (Robinson et al., 2004), which presents a dataset built as audio face-toface sessions between human trainees and virtual agents. The main theme of the multimodal dataset is decision-making for a platoon-leader in a peacekeeping mission, with the trainee acting as a lieutenant. The corpora has about 30K words, 2K utterances, and a total of 55 speakers. Traum et al. (2008) also introduce another 3-party negotiation dialogue corpus, called the Stabilization and Support Operations (SASO-EN) corpus, which grew out of experiments on the MRE corpus (Lee et al., 2007), focusing on eye-gaze behavior in 3-party negotiation. I"
2021.sigdial-1.36,W15-4625,0,0.0209257,"a have also contributed to such corpora. These notably include the Reddit (Chang et al., 2020) corpus which has also been extended into larger corpora (Baumgartner et al., 2020). There have also been argumentative corpora obtained from online interactions, like the Reddit Domestic Abuse Corpus (Schrading et al., 2015) taken from subreddits specific on domestic abuse, allowing for discourse analysis on this subject. Debate and agreement corpora such as the Internet Argument Corpus (Walker et al., 2012b), Agreement in Wikipedia Talk Pages (Andreas et al., 2012) and Agreement by Create Debaters (Rosenthal and McKeown, 2015), from debate and discussion forums online such as CreateDebate also contribute towards argumentation in dialogue research (Rakshit et al., 2018). Additionally, there have been corpora obtained from social media such as UseNet and Twitter. These include the UseNet Corpus (Shaoul and Westbury, 2007, 2011), a platform which is considered a precursor to more recent forums; and the Twitter Corpus (Ritter et al., 2010), which was intended to help model dialogue acts. 3.3 Special Mentions This section includes special mentions of corpora as well as frameworks and toolkits that do not fall under our"
2021.sigdial-1.36,roy-etal-2014-tvd,0,0.0312651,"tles site. They are corpora of plain scripts, but the website continues to contribute as a resource for more data (Lison and Tiedemann, 2016; Lison et al., 2018). Bridging the sources of movie and TV scripts is the Corpus of American Soap Operas (Davies, 2013) which focuses on informal language, and has been used to study cultural representation differences in American soap operas (Khaghaninejad et al., 2019). A TV series corpus including data from shows like The Big Bang Theory and Game of Thrones, supplemented by crowd-sourced contributions for tasks such as summarization is the TVD Corpus (Roy et al., 2014). It has been used to build models for speaker identification (Knyazeva et al., 2015). The Serial Speakers (Bost et al., 2020) dataset supplements data from both the aforementioned TV serials by also including the House of Cards and additional annotations. Recently, the Multimodal EmotionLines Dataset (MELD) (Poria et al., 2019) corpus has been presented by extending the (ELD) (Hsu et al., 2018), with audio-visual modality along with text. It has been used as a resource for Dialogue Act Classification (Saha et al., 2020). The MEISD (Firdaus et al., 2020) dataset is build further with TV script"
2021.sigdial-1.36,2020.acl-main.402,0,0.0168817,"d-sourced contributions for tasks such as summarization is the TVD Corpus (Roy et al., 2014). It has been used to build models for speaker identification (Knyazeva et al., 2015). The Serial Speakers (Bost et al., 2020) dataset supplements data from both the aforementioned TV serials by also including the House of Cards and additional annotations. Recently, the Multimodal EmotionLines Dataset (MELD) (Poria et al., 2019) corpus has been presented by extending the (ELD) (Hsu et al., 2018), with audio-visual modality along with text. It has been used as a resource for Dialogue Act Classification (Saha et al., 2020). The MEISD (Firdaus et al., 2020) dataset is build further with TV scripts 341 from 10 series, adding Friends, How I Met Your Mother, The Office, House M.D., Grey’s Anatomy, Castle, Breaking Bad to the aforementioned series. 3.2 Written Corpora Written corpora for multi-party have often resulted from online chatroom discussions, like the NPS Chat Corpus (Forsythand and Martell, 2007), which is shared as a part of the NLTK (Loper and Bird, 2002), and is one of the first ComputerMediated corpora. The Ubuntu IRC chatroom has also contributed to corpora such as the Ubuntu Dialogue Corpus (Lowe et"
2021.sigdial-1.36,D15-1309,0,0.0283152,"012) and Cards Corpus (Djalali et al., 2011) are great informal additions to chatroom corpora, with a competitive environment albeit in an informal setting. They have been used for tasks such as training models for negotiation dialogues (Cadilhac et al., 2013). Online forums such as Reddit, and Wikipedia have also contributed to such corpora. These notably include the Reddit (Chang et al., 2020) corpus which has also been extended into larger corpora (Baumgartner et al., 2020). There have also been argumentative corpora obtained from online interactions, like the Reddit Domestic Abuse Corpus (Schrading et al., 2015) taken from subreddits specific on domestic abuse, allowing for discourse analysis on this subject. Debate and agreement corpora such as the Internet Argument Corpus (Walker et al., 2012b), Agreement in Wikipedia Talk Pages (Andreas et al., 2012) and Agreement by Create Debaters (Rosenthal and McKeown, 2015), from debate and discussion forums online such as CreateDebate also contribute towards argumentation in dialogue research (Rakshit et al., 2018). Additionally, there have been corpora obtained from social media such as UseNet and Twitter. These include the UseNet Corpus (Shaoul and Westbur"
2021.sigdial-1.36,shaikh-etal-2010-mpc,1,0.7137,"ted corpora. The Ubuntu IRC chatroom has also contributed to corpora such as the Ubuntu Dialogue Corpus (Lowe et al., 2015) and Ubuntu Chat Corpus (Uthus and Aha, 2013), which were collected as users asked questions relating to Ubuntu on the forum, and other users answered the questions. They have been used to train end-to-end dialogue systems (Lowe et al., 2017). The Molweni corpus (Li et al., 2020) builds on the Ubuntu Chat Dialogue corpus, and adds annotations for machine reading comprehension and dscourse parsing. Another corpus based on chatroom data is the Multi-Party Chat (MPC) Corpus (Shaikh et al., 2010) which presents an annotated corpus based on four levels with communication links, dialogue acts, local topics and meso-topics, and has been used to understand user roles and modeling leadership and influence (Strzalkowski et al., 2012). Game-playing corpora such as the Settlers of Catan Corpus (Afantenos et al., 2012) and Cards Corpus (Djalali et al., 2011) are great informal additions to chatroom corpora, with a competitive environment albeit in an informal setting. They have been used for tasks such as training models for negotiation dialogues (Cadilhac et al., 2013). Online forums such as"
2021.sigdial-1.36,W04-2319,0,0.161574,"ns, and has been used in tasks such as conflict detection (Kim et al., 2012). A historic debate corpus is the Trial Proceedings component of the Corpus of English Dialogues (CED) (Kyt¨o and Walker, 2006), which has been used to study signalling function in discourse (Lenker, 2018). Supplementing formal discourse in debate corpora are formal meeting corpora, with 2 corpora that have become really important for studying multi-party decision-making and discussions of actions to take are the ICSI meeting corpus (Janin et al., 2003), which also has Meeting Recorder Dialogue Act (MRDA) annotations (Shriberg et al., 2004); and the multi-modal AMI meeting corpus (Renals et al., 2007). ICSI has been used to further study multi-party language modeling (Ji and Bilmes, 2004), and AMI has been used to build summarization for meetings (Zhu et al., 2020). Recent additions include data from interviews, such as the INTERVIEW (Majumder et al., 2020) and MediaSum (Zhu et al., 2021) corpora. They include transcripts from interviews on channels such as National Public Radio NPR and CNN. 3.1.2 Scripted Spoken Corpora Scripted spoken corpora consist of pre-defined scripts such as those for plays, movies, and TV series. These"
2021.sigdial-1.36,C12-1155,1,0.750286,"untu on the forum, and other users answered the questions. They have been used to train end-to-end dialogue systems (Lowe et al., 2017). The Molweni corpus (Li et al., 2020) builds on the Ubuntu Chat Dialogue corpus, and adds annotations for machine reading comprehension and dscourse parsing. Another corpus based on chatroom data is the Multi-Party Chat (MPC) Corpus (Shaikh et al., 2010) which presents an annotated corpus based on four levels with communication links, dialogue acts, local topics and meso-topics, and has been used to understand user roles and modeling leadership and influence (Strzalkowski et al., 2012). Game-playing corpora such as the Settlers of Catan Corpus (Afantenos et al., 2012) and Cards Corpus (Djalali et al., 2011) are great informal additions to chatroom corpora, with a competitive environment albeit in an informal setting. They have been used for tasks such as training models for negotiation dialogues (Cadilhac et al., 2013). Online forums such as Reddit, and Wikipedia have also contributed to such corpora. These notably include the Reddit (Chang et al., 2020) corpus which has also been extended into larger corpora (Baumgartner et al., 2020). There have also been argumentative co"
2021.sigdial-1.36,tiedemann-2012-parallel,0,0.0592593,"ts Online Series corpus includes British movie scripts, but is not available online. The Cornell Movie-Dialogue Corpus (Danescu-Niculescu-Mizil and Lee, 2011) contains metadata associated with each movie script, and has been used to generate emotionally aligned responses to dialogue (Asghar et al., 2020). The Character Style From Film Corpus (Walker et al., 2012a) is another resource contributing towards guided text generation by providing character styles, created from the archive IMSDB. It has been used to generate stylistic dialogue for narratives (Xu et al., 2018). Both the OpenSubtitles (Tiedemann, 2012) and SubTle corpus (Ameixa and Coheur, 2013) are based on the OpenSubtitles site. They are corpora of plain scripts, but the website continues to contribute as a resource for more data (Lison and Tiedemann, 2016; Lison et al., 2018). Bridging the sources of movie and TV scripts is the Corpus of American Soap Operas (Davies, 2013) which focuses on informal language, and has been used to study cultural representation differences in American soap operas (Khaghaninejad et al., 2019). A TV series corpus including data from shows like The Big Bang Theory and Game of Thrones, supplemented by crowd-so"
2021.sigdial-1.36,walker-etal-2012-annotated,0,0.127167,"plays (Demmen, 2012). The Movie-DiC Corpus (Banchs, 2012) consists of a wide range of American movie scripts, along with context descriptions. It has even been used to generate parallel corpora for dialogue translation (Wang et al., 2016). The Film Scripts Online Series corpus includes British movie scripts, but is not available online. The Cornell Movie-Dialogue Corpus (Danescu-Niculescu-Mizil and Lee, 2011) contains metadata associated with each movie script, and has been used to generate emotionally aligned responses to dialogue (Asghar et al., 2020). The Character Style From Film Corpus (Walker et al., 2012a) is another resource contributing towards guided text generation by providing character styles, created from the archive IMSDB. It has been used to generate stylistic dialogue for narratives (Xu et al., 2018). Both the OpenSubtitles (Tiedemann, 2012) and SubTle corpus (Ameixa and Coheur, 2013) are based on the OpenSubtitles site. They are corpora of plain scripts, but the website continues to contribute as a resource for more data (Lison and Tiedemann, 2016; Lison et al., 2018). Bridging the sources of movie and TV scripts is the Corpus of American Soap Operas (Davies, 2013) which focuses on"
2021.sigdial-1.36,walker-etal-2012-corpus,0,0.233319,"plays (Demmen, 2012). The Movie-DiC Corpus (Banchs, 2012) consists of a wide range of American movie scripts, along with context descriptions. It has even been used to generate parallel corpora for dialogue translation (Wang et al., 2016). The Film Scripts Online Series corpus includes British movie scripts, but is not available online. The Cornell Movie-Dialogue Corpus (Danescu-Niculescu-Mizil and Lee, 2011) contains metadata associated with each movie script, and has been used to generate emotionally aligned responses to dialogue (Asghar et al., 2020). The Character Style From Film Corpus (Walker et al., 2012a) is another resource contributing towards guided text generation by providing character styles, created from the archive IMSDB. It has been used to generate stylistic dialogue for narratives (Xu et al., 2018). Both the OpenSubtitles (Tiedemann, 2012) and SubTle corpus (Ameixa and Coheur, 2013) are based on the OpenSubtitles site. They are corpora of plain scripts, but the website continues to contribute as a resource for more data (Lison and Tiedemann, 2016; Lison et al., 2018). Bridging the sources of movie and TV scripts is the Corpus of American Soap Operas (Davies, 2013) which focuses on"
2021.sigdial-1.36,L16-1436,0,0.0229138,"aken. Some corpora are actually labelled with this information, while others are simply transcript-like (Table 1). One of the earliest available scripted spoken corpora is a second component of the Corpus of English Dialogue CED (Kyt¨o and Walker, 2006) focusing on Prose Fiction. It has been used to study language styles in Shakespeare’s plays in the context of contemporaneous plays (Demmen, 2012). The Movie-DiC Corpus (Banchs, 2012) consists of a wide range of American movie scripts, along with context descriptions. It has even been used to generate parallel corpora for dialogue translation (Wang et al., 2016). The Film Scripts Online Series corpus includes British movie scripts, but is not available online. The Cornell Movie-Dialogue Corpus (Danescu-Niculescu-Mizil and Lee, 2011) contains metadata associated with each movie script, and has been used to generate emotionally aligned responses to dialogue (Asghar et al., 2020). The Character Style From Film Corpus (Walker et al., 2012a) is another resource contributing towards guided text generation by providing character styles, created from the archive IMSDB. It has been used to generate stylistic dialogue for narratives (Xu et al., 2018). Both the"
2021.sigdial-1.36,N16-1017,0,0.0190825,"id resource. The Michigan Corpus of Academic Spoken English (MICASE) (Simpson-Vlach and Leicher, 2006) includes academic speech from university settings. It also comes with abstracts for each transcript, and has been used in online speech summarization (Murray and Renals, 2007). Debate-based settings are also ideal candidates for multi-party corpora building, and thus the Intelligence Squared Debates (IQ2US) (Yang et al., 2010) are an important source. They follow an Oxford-style debating structure, and contain structured data making for a great resource for debate and argumentation analysis (Zhang et al., 2016). Canal9 (Vinciarelli et al., 2009) is another debate corpus, consisting of political debates. It includes a rich set of socially relevant annotations, and has been used in tasks such as conflict detection (Kim et al., 2012). A historic debate corpus is the Trial Proceedings component of the Corpus of English Dialogues (CED) (Kyt¨o and Walker, 2006), which has been used to study signalling function in discourse (Lenker, 2018). Supplementing formal discourse in debate corpora are formal meeting corpora, with 2 corpora that have become really important for studying multi-party decision-making an"
2021.sigdial-1.36,2021.naacl-main.474,0,0.0411129,"with 2 corpora that have become really important for studying multi-party decision-making and discussions of actions to take are the ICSI meeting corpus (Janin et al., 2003), which also has Meeting Recorder Dialogue Act (MRDA) annotations (Shriberg et al., 2004); and the multi-modal AMI meeting corpus (Renals et al., 2007). ICSI has been used to further study multi-party language modeling (Ji and Bilmes, 2004), and AMI has been used to build summarization for meetings (Zhu et al., 2020). Recent additions include data from interviews, such as the INTERVIEW (Majumder et al., 2020) and MediaSum (Zhu et al., 2021) corpora. They include transcripts from interviews on channels such as National Public Radio NPR and CNN. 3.1.2 Scripted Spoken Corpora Scripted spoken corpora consist of pre-defined scripts such as those for plays, movies, and TV series. These are inherently different as they are not spontaneous, and have pre-defined roles for speakers as well as information on when the dialogues turns are taken. Some corpora are actually labelled with this information, while others are simply transcript-like (Table 1). One of the earliest available scripted spoken corpora is a second component of the Corpus"
C10-1117,J99-1001,0,0.241255,"and classify social language uses in multi-party dialogue. 2. Related Research Issues related to linguistic manifestation of social phenomena have not been systematically researched before in computational linguistics; indeed, most of the effort thus far was directed towards the communicative dimension of discourse. While the Speech Acts theory (Austin, 1962; Searle, 1969) provides a generalized framework for multiple levels of discourse analysis (locution, illocution and perlocution), most current approaches to dialogue focus on information content and structural components (Blaylock, 2002; Carberry & Lambert, 1999; Stolcke, et al., 2000) in dialogue; few take into account the effects that speech acts may have upon the social 1038 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1038–1046, Beijing, August 2010 roles of discourse participants. Also relevant is research on modeling sequences of dialogue acts – to predict the next one (Samuel et al. 1998; Ji & Bilmes, 2006 inter alia) – or to map them onto subsequences or “dialogue games” (Carlson 1983; Levin et al., 1998), which are attempts to formalize participants’ roles in conversation (e.g., Linell, 1"
C10-1117,P05-2014,0,0.0137442,"Missing"
C10-1117,N06-1036,0,0.051279,"Missing"
C10-1117,W03-2118,0,0.0235703,"Missing"
C10-1117,shaikh-etal-2010-mpc,1,0.886958,"Missing"
C10-1117,J00-3003,0,0.178483,"Missing"
C10-1117,webb-etal-2008-cross,1,0.880529,"Missing"
C10-1117,C10-2150,1,\N,Missing
C10-1117,H05-1044,0,\N,Missing
C10-1117,E03-1072,0,\N,Missing
C10-1117,P98-2188,0,\N,Missing
C10-1117,C98-2183,0,\N,Missing
C12-1155,J99-1001,0,0.0607346,"ons. Any member of the expert panel may exhibit the sociolinguistic behavior consistent with being an influencer. In a peer-oriented group discussion however, it could occur that the task and thought leader (leader and influencer) are the same person. Human-human interaction affords a rich resource for research. Much prior work has been done in communication that focuses on the communicative dimension of discourse. For example, the Speech Act theory (Austin, 1962; Searle 1969) provides a generalized framework of multiple levels of discourse analysis; work on dialogue analysis (Blaylock, 2002; Carberry and Lambert, 1999; Stolcke et al., 2000) focuses on information content and structure of dialogues. Somewhat more relevant to social roles is research that models sequences of dialogue acts (Bunt, 1994), in order to predict the next dialogue act (Samuel et al. 1998; Stolcke, et al., 2000; Ji & Bilmes, 2006, inter alia) or to map them onto subsequences or “dialogue games” (Carlson 1983; Levin et al., 1998), from which participants’ functional roles in conversation (though not social roles) may be extrapolated (e.g., Linell, 1990; Poesio and Mikheev, 1998; Field et al., 2008). However, the effects of speech acts"
C12-1155,H92-1086,0,0.379027,"Missing"
C12-1155,P11-2059,0,0.0128008,"ying online chat relies on the more explicit linguistic devices necessary to convey social and cultural nuances than is typical in face-to-face or telephonic conversations. The use of language by participants as a feature to determine interpersonal relations has been studied by Bracewell et al. (2011) who developed a learning framework to determine collegiality between discourse participants. Their approach, however, looks at singular instances of linguistic markers or single utterances rather than a sustained demonstration of sociolinguistic behavior over the 2536 course of entire discourse. Freedman et al. (2011) have developed an approach that takes into account the entire discourse to detect behaviors such as persuasion; however their analysis is conducted on and models developed upon online discussion threads where the social phenomena of interest may be rare. By contrast, we build our models based on analysis of a data corpus of online chat discourse, where data collection experiments were specifically designed so that the resulting corpus may be rich in sociolinguistic phenomena. Our research extends the work of Strzalkowski et al. (2010) and Broadwell et al. (2012), who first proposed the two-ti"
C12-1155,liu-etal-2012-extending,1,0.822134,"Missing"
C12-1155,P98-2188,0,0.0301205,"n-human interaction affords a rich resource for research. Much prior work has been done in communication that focuses on the communicative dimension of discourse. For example, the Speech Act theory (Austin, 1962; Searle 1969) provides a generalized framework of multiple levels of discourse analysis; work on dialogue analysis (Blaylock, 2002; Carberry and Lambert, 1999; Stolcke et al., 2000) focuses on information content and structure of dialogues. Somewhat more relevant to social roles is research that models sequences of dialogue acts (Bunt, 1994), in order to predict the next dialogue act (Samuel et al. 1998; Stolcke, et al., 2000; Ji & Bilmes, 2006, inter alia) or to map them onto subsequences or “dialogue games” (Carlson 1983; Levin et al., 1998), from which participants’ functional roles in conversation (though not social roles) may be extrapolated (e.g., Linell, 1990; Poesio and Mikheev, 1998; Field et al., 2008). However, the effects of speech acts on social behaviors and roles of conversation participants have not been systematically studied. Research in anthropology and communication has concentrated on how certain social norms and behaviors may be reflected in language (e.g., Scollon and"
C12-1155,shaikh-etal-2010-mpc,1,0.928668,"nor role to play in computing Influence, and hence we do not include them while combining behaviors. Similarly, while Task Control and Disagreement are most indicative of Task Leadership, other behaviours such as Network Centrality and Argument Diversity do not correlate with this role. Hence, we do not include them in computation of Leadership. We shall elaborate on this in Section 5, Evaluation and Results. 2540 3 Corpus, Annotation and Computational Modules The models described in this paper are derived from online chat dialogues. The corpus we use for this analysis is the MPC chat corpus (Shaikh et al., 2010, Liu et al., 2012). This is a corpus of over 90 hours of online chat dialogues in English, Urdu and Mandarin. Participants in these chats are native speakers of these languages. Each chat session is a task-oriented dialogue around 90 minutes in length, with at least 4 participants. This corpus is particularly useful for the type of sociolinguistic analysis we are interested in due to the characteristics of interaction in each chat session – the participants are focused on some task, they form a fairly stable group and the dynamics of conversation unfold naturally through discourse. Other corp"
C12-1155,W04-2319,0,0.0395466,"over 90 hours of online chat dialogues in English, Urdu and Mandarin. Participants in these chats are native speakers of these languages. Each chat session is a task-oriented dialogue around 90 minutes in length, with at least 4 participants. This corpus is particularly useful for the type of sociolinguistic analysis we are interested in due to the characteristics of interaction in each chat session – the participants are focused on some task, they form a fairly stable group and the dynamics of conversation unfold naturally through discourse. Other corpora exist such as the ICSI-MRDA corpus (Shriberg et al., 2004) and the AMI meeting corpus (Carletta, 2007), however these are spoken language resources rather than online chat. Where other corpora of online chat do exist, like the NPS Internet chat corpus (Forsyth and Martell, 2007) and StrikeCom corpus (Twitchell et al., 2004), they do not contain any information about the participants themselves or their reactions to the discussion. In order to create a ground truth of assessments of sociolinguistic behavior, we needed certain information to be captured through questionnaires or survey following each data collection session. In the data that comprise M"
C12-1155,J00-3003,0,0.308716,"Missing"
C12-1155,C10-1117,1,0.852338,"olinguistic behavior over the 2536 course of entire discourse. Freedman et al. (2011) have developed an approach that takes into account the entire discourse to detect behaviors such as persuasion; however their analysis is conducted on and models developed upon online discussion threads where the social phenomena of interest may be rare. By contrast, we build our models based on analysis of a data corpus of online chat discourse, where data collection experiments were specifically designed so that the resulting corpus may be rich in sociolinguistic phenomena. Our research extends the work of Strzalkowski et al. (2010) and Broadwell et al. (2012), who first proposed the two-tiered approach to sociolinguistic modeling and have demonstrated that a subset of mid-level sociolinguistic behaviors may be accurately inferred by a combination of low-level language features. We have adopted their approach and extended it to modeling of leadership and influence. Furthermore, we enhanced the method by adding the evidence learnt from correlations of indices and measures to compute weights through which sociolinguistic behaviors may be combined appropriately to infer higher-level social phenomena. In this paper, we descr"
C12-1155,C10-2150,0,0.023678,"Missing"
D19-5016,W17-4215,0,0.0908235,"sity of North Carolina and Technology and Technology and Technology at Charlotte Computer Science Department Computer Science Department Computer Science Department Computer Science Department Irbid, Jordan Irbid, Jordan Irbid, Jordan NC, USA alomarihani1997@gmail.com mabdullah@just.edu.jo oaaltiti18@cit.just.edu.jo samirashaikh@uncc.edu Abstract their minds with a certain idea, for political, ideological, or business motivations (Tandoc Jr et al., 2018; Brennen, 2017). Detecting fake news and propaganda is getting more attention recently (Jain and Kasbe, 2018; Helmstetter and Paulheim, 2018; Bourgonje et al., 2017), however, the limited resources and corpora is considered the biggest challenge for researchers in this field. In this work, we use the corpus provided by the shared task on fine-grained propaganda detection (NLP4IF 2019) (Da San Martino et al., 2019). The corpus consists of news articles in which the sentences are labeled as propagandistic or not. The goal of the challenge is to build automatic tools to detect propaganda. Knowing that deep learning is outperforming traditional machine learning techniques, we have proposed an ensemble deep learning model using BiLSTM, XGBoost, and BERT to add"
D19-5016,W17-5205,0,0.0877562,"Missing"
D19-5016,D19-1565,0,0.0816176,"Missing"
D19-5016,D14-1162,0,0.0821505,"There are 16975 sentences in the training data, where 12244 are non-propaganda and 4721 are propaganda. 3.1 Data preprocessing In our model, text preprocessing has been performed for each sentence of training and development set that includes: removing punctuation, cleaning text from special symbols, removing stop words, clean contractions, and correct some misspelled words. 3.2 Features In our approach, we have 449 dimensions for our extracted features that are obtained as the following: Each line of text is represented as a 300-dimensional vector using the pretrained Glove embedding model (Pennington et al., 2014). http://www.fakenewschallenge.org 114 Figure 1: The architecture of our approach 4.1 It is worth mentioning that we have also experimented word2vec embedding model that is trained on Google News (Mikolov et al., 2013) but the results were not promising. Our hypothesis is that emotional and affective words will characterize fake news more strongly than neutral words. Accordingly, each line of text is represented as 149-dimensional vector by concatenating three vectors obtained from AffectiveTweets Weka-package (Mohammad and Bravo-Marquez, 2017; Bravo-Marquez et al., 2014), 43 features were ext"
D19-5016,C18-1287,0,0.162645,"versity of Science Jordan University of Science University of North Carolina and Technology and Technology and Technology at Charlotte Computer Science Department Computer Science Department Computer Science Department Computer Science Department Irbid, Jordan Irbid, Jordan Irbid, Jordan NC, USA alomarihani1997@gmail.com mabdullah@just.edu.jo oaaltiti18@cit.just.edu.jo samirashaikh@uncc.edu Abstract their minds with a certain idea, for political, ideological, or business motivations (Tandoc Jr et al., 2018; Brennen, 2017). Detecting fake news and propaganda is getting more attention recently (Jain and Kasbe, 2018; Helmstetter and Paulheim, 2018; Bourgonje et al., 2017), however, the limited resources and corpora is considered the biggest challenge for researchers in this field. In this work, we use the corpus provided by the shared task on fine-grained propaganda detection (NLP4IF 2019) (Da San Martino et al., 2019). The corpus consists of news articles in which the sentences are labeled as propagandistic or not. The goal of the challenge is to build automatic tools to detect propaganda. Knowing that deep learning is outperforming traditional machine learning techniques, we have proposed an ensemble d"
L16-1180,bestgen-2008-building,0,0.785704,"(2004) used WordNet (Miller et al. 1995) to assign positive or negative polarity to words using synonyms and antonyms for a small set of seed words, however, such a method is limited by the set of seed words chosen. Esuli and Sebastini (2006) used semi-supervised learning to create SentiWordNet, where potentially every word in WordNet would be assigned a sentiment score, although many words actually may not be sentiment-bearing (cf. Taboada, 2011 for further discussion). A number of approaches use semantic proximity of words in variations of Latent Semantic Analysis (Turney and Littman, 2003; Bestgen, 2008; Bestgen and Vincze, 2012); however, their self-reported correlations of proposed expansions against human ratings are not sufficiently robust. Neilson (2011) created a new ANEW specifically geared towards detecting sentiment in microblog posts, but it only contains 2477 words scored manually on a scale of +2 (positive) to -2 (negative). 3. Approach Our expansion method follows that adopted by Liu et al. (2014) for their automated expansion of the MRC psycholinguistic database. We use WordNet (Miller, 1995), a large English lexical database with over 150,000 words, hierarchically organized in"
L16-1180,esuli-sebastiani-2006-sentiwordnet,0,0.112714,"Missing"
L16-1180,C04-1200,0,0.0155118,"e our method of automatically expanding an existing affective lexicon for English. Our approach is general enough to apply to any specialized lexicon in any language where a partial resource exists. We also describe a method of automatically creating lexicons for a new language where no prior resources exist or are very limited; specifically, for Spanish, Russian and Farsi. We also describe the validation procedures used to ensure validity of these lexicons1. 2. Related Work There has been prior work in the automatic construction and expansion of affective lexicons using different techniques. Kim and Hovy (2004) used WordNet (Miller et al. 1995) to assign positive or negative polarity to words using synonyms and antonyms for a small set of seed words, however, such a method is limited by the set of seed words chosen. Esuli and Sebastini (2006) used semi-supervised learning to create SentiWordNet, where potentially every word in WordNet would be assigned a sentiment score, although many words actually may not be sentiment-bearing (cf. Taboada, 2011 for further discussion). A number of approaches use semantic proximity of words in variations of Latent Semantic Analysis (Turney and Littman, 2003; Bestge"
L16-1180,J11-2001,0,0.0249183,"Missing"
L16-1180,1996.amta-1.36,0,0.492001,"Missing"
L16-1594,W13-0909,1,0.901166,"Missing"
lin-etal-2012-revealing,W99-0705,0,\N,Missing
lin-etal-2012-revealing,W00-1308,0,\N,Missing
lin-etal-2012-revealing,J95-4004,0,\N,Missing
liu-etal-2012-extending,shaikh-etal-2010-mpc,1,\N,Missing
liu-etal-2012-extending,W09-3404,0,\N,Missing
liu-etal-2012-extending,song-etal-2010-enhanced,0,\N,Missing
liu-etal-2014-automatic-expansion,W13-0909,1,\N,Missing
S15-1009,P98-1013,0,0.149614,"the syntactic head of the text passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals"
S15-1009,baker-etal-2010-modality,0,0.0482113,"Missing"
S15-1009,W13-2322,0,0.0158937,"xt passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals, groups, artifacts, etc., which"
S15-1009,W09-3012,1,0.690346,"space we do not provide an overview over all definitions. While at first the terms “belief” and “factuality” appear to relate to rather different things (a subjective state versus truth), in the NLP community they in fact refer to the same phenomenon, while having rather different connotations. The phenomenon is the communicative intention of a writer1 to present propositional content as something that she firmly believes is true, weakly believes is true, or has some other attitude towards, namely a wish or a reported belief. The term “belief” here describes the cognitive state of the writer (Diab et al., 2009), and comes from artificial intelligence and cognitive science, as in the Belief-Desire-Intention model of Bratman (1999 1987). The term “factuality” describes the communicative intention of the writer (Saur´ı and Pustejovsky, 2012, p. 263) (our emphasis): The fact that an eventuality is depicted as holding or not does not mean that this is the case in the world, but that this is how it is characterized by its informant. Similarly, it does not mean that this is the real knowledge that informant has (his true cognitive state regarding that event) but what he wants us to believe it is. We would"
S15-1009,doddington-etal-2004-automatic,1,0.834711,"sity/George Washington University, the Florida Institute for Human and Machine Cognition, and the University of Albany. The goal of our research project is not linguistic annotation, but the identification of meaning which is expressed in a non-linguistic manner. Such a meaning representation is useful for many applications; in our project we are specifically interested in knowledge base population. A different part of the DEFT program is concerned with the representation of propositional meaning, following the tradition of the ACE program in representing entities, relations and events (ERE) (Doddington et al., 2004). The work presented here is concerned with the attitude of agents towards propositional content: do the agents express a committed belief or a non-committed belief in the propositional content? Our work has several characteristics that set it apart from other work: we are interested in annotation which can be done fairly quickly; we are not interested in annotating linguistic elements (such as trigger words); and we are planning an integration with sentiment annotation. The structure of the paper is as follows: we start out by situating our notion of “belief” with respect to other notions of"
S15-1009,W10-3001,0,0.293703,"Missing"
S15-1009,P11-2102,0,0.0364363,"rotates around the earth, as was his (presumably) honest communicative intention. Therefore, to us as researchers interested in describing how language 2 Sarcasm and irony differ from lying in that the communicative intention and the cognitive state are aligned, but they do not align with the standard interpretation of the utterance. Here, the intention is that the reader recognizes that the form of the utterance does not literally express the cognitive state. We leave aside sarcasm and irony in this paper; for current computational work on sarcasm detection, see for example (Gonz´alez-Ib´an˜ ez et al., 2011). is used to communicate, it does not matter that astronomers now believe that Ptolemy was wrong, it does not change our account of communication and it does not change the communication that happened two millennia ago. And since we do not need to make the assumption that the writer knows what she is talking about, we choose not to make this assumption. In the case of Ptolemy, we leave this determination – what is actually true – to astronomers. In other cases, we typically have models of trustworthiness: if a writer sends her spouse a text message saying she is hungry, the spouse has no reaso"
S15-1009,P09-2078,0,0.0256017,"d, we could assume that the writer knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumptio"
S15-1009,P11-1032,0,0.0145471,"er knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumption may be false in c"
S15-1009,C10-2117,1,0.747479,"on-committed belief in the annotations, the heuristic rules (mainly based on the presence of modal auxiliaries) that we added for the purpose of classifying the beliefs (CB, NCB, ROB, NA) did not work reliably in all cases. 4.3 System C System C uses a supervised learning approach to identify tokens denoting the heads of propositions that denote author’s expressed beliefs. It approaches this problem as a 5-way (CB, NCB, ROB, NA, nil) multi-class classification task at the word level. System C is adapted from a previous system which uses an earlier, simpler definition and annotation of belief (Prabhakaran et al., 2010). The system uses lexical and syntactic features for this task, which are extracted using the part-of-speech tags and dependency parses obtained from the Stanford CoreNLP system. In addition to the features described in (Prabhakaran et al., 2010), System C uses a set of new features including features based on a dictionary of hedge-words (Prokofieva and Hirschberg, 2014). The hedge features improved the NCB Fmeasure by around 2.2 percentage points (an overall F-measure improvement of 0.25 percentage points) in experiments conducted on a separate development set. It uses a quadratic kernel SVM"
S15-1009,W12-3807,1,0.915772,"Missing"
S15-1009,J12-2002,0,0.157236,"Missing"
S15-1009,W15-1304,1,0.70443,"ore, the FactBank annotation is basically compatible with ours. Our annotation is much simpler than that of FactBank in order to allow for a quicker annotation. We summarize the main points of simplification here. • We have taken the source always to be the writer. As we will discuss in Section 7.1, we will adopt the FactBank annotation in the next iteration of our annotation. • We do not distinguish between possible and probable; this distinction may be hard to annotate and not too valuable. • We ignore negation. If present, we simply assume it is part of the proposition which is the target. Werner et al. (2015) study the relation between belief and factuality in more detail. They provide an automatic way of mapping the annotations in FactBank to the 4-way distinction of speaker/writer’s belief that we present in this paper. 3.3 Corpus and Annotation Results The annotation effort for this phase of belief annotation for DEFT produced a training corpus of 852,836 words and an evaluation corpus of 100,037 words. All annotated data consisted of English text from discussion forum threads. The discussion forum threads were originally collected for the DARPA BOLT program, and were harvested from a wide vari"
S15-1009,C98-1013,0,\N,Missing
S18-1053,W11-0705,0,0.0611139,"he system for all subtasks in both languages shows substantial improvements in Spearman correlation scores over the baseline models provided by Task 1 organizers, ranging from 0.03 to 0.23. Introduction The rise and diversity of social microblogging channels encourage people to express their feelings and opinions on a daily basis. Consequently, sentiment analysis and emotion detection have gained the interest of researchers in natural language processing and other fields that include political science, marketing, communication, social sciences, and psychology (Mohammad and BravoMarquez, 2017; Agarwal et al., 2011; Chin et al., 2016). Sentiment analysis refers to classifying a subjective text as positive, neutral, or negative; emotion detection recognizes types of feelings through the expression of texts, such as anger, joy, fear, and sadness (Agarwal et al., 2011; Ekman, 1993). SemEval is the International Workshop on Semantic Evaluation that has evolved from SensEThe remainder of this research paper is organized as follows: Section 2 gives a brief overview of existing work on social media emotion and sentiment analyses, including for English and Arabic languages. Section 3 presents the requirements o"
S18-1053,S12-1033,0,0.0354006,"t and Emotion Analysis: Sentiment analysis was first explored in 2003 by Nasukawa and Yi (Nasukawa and Yi, 2003). An interest in studying and building models for sentiment analysis and emotion detection for social microblogging platforms has increased significantly in recent years (Kouloumpis et al., 2011; Pak and Paroubek, 2010; Oscar et al., 2017; Jimenez-Zafra et al., 2017). Going beyond the task of mainly classifying tweets as positive or negative, several approaches to detect emotions were presented in previous research papers (Mohammad and Kiritchenko, 2015; Tromp and Pechenizkiy, 2014; Mohammad, 2012). Researchers (Mohammad and Bravo-Marquez, 2017) introduced the WASSA2017 shared task of detecting the intensity of emotion felt by the speaker of a tweet. The stateof-the-art system in that competition (Goel et al., 2017) used an approach of ensembling three different deep neural network-based models, representing tweets as word2vec embedding vectors. In our system, we add doc2vec embedding vectors and classify tweets to ordinal classes of emotions as well as multi-class labeling of emotions. Arabic Emotion Analysis: The growth of the Arabic language on social microblogging platforms, especia"
S18-1053,W17-5205,0,0.0376152,"Missing"
S18-1053,S18-1001,0,0.0697422,"Missing"
S18-1053,W17-5207,0,0.0199824,"blogging platforms has increased significantly in recent years (Kouloumpis et al., 2011; Pak and Paroubek, 2010; Oscar et al., 2017; Jimenez-Zafra et al., 2017). Going beyond the task of mainly classifying tweets as positive or negative, several approaches to detect emotions were presented in previous research papers (Mohammad and Kiritchenko, 2015; Tromp and Pechenizkiy, 2014; Mohammad, 2012). Researchers (Mohammad and Bravo-Marquez, 2017) introduced the WASSA2017 shared task of detecting the intensity of emotion felt by the speaker of a tweet. The stateof-the-art system in that competition (Goel et al., 2017) used an approach of ensembling three different deep neural network-based models, representing tweets as word2vec embedding vectors. In our system, we add doc2vec embedding vectors and classify tweets to ordinal classes of emotions as well as multi-class labeling of emotions. Arabic Emotion Analysis: The growth of the Arabic language on social microblogging platforms, especially on Twitter, and the significant role of the Arab region in international politics and in the global economy have led researchers to investigate the area of mining and analyzing sentiments and emotions of Arabic tweets"
S18-1053,L18-1030,0,0.0161368,"elated to the Arabic language. They also used a simplification of the SVM (known as SMO) and the NaiveBayes classifiers. Another two related works (Kiritchenko et al., 2016; Rosenthal et al., 2017) shared different tasks to identify the overall sentiments of the tweets or phrases taken from tweets in both English and Arabic. Our work uses the state-of-the3 Task Description and Datasets SemEval-2018 Task 1, Affect in Tweets, presents five subtasks (El-reg, El-oc, V-reg, V-oc, and E-c.) The subtasks provide training and testing for Twitter datasets in the English, Arabic, and Spanish languages (Mohammad and Kiritchenko, 2018). Task 1 asks the participants to predict the intensity of emotions and sentiments in the testing datasets. It also includes multi-label emotion classification subtask for tweets. This paper focuses on determining emotions in English and Arabic tweets. Figure 1 shows the number of tweets for both training and testing datasets for individual subtasks. We note that subtasks El-reg and El-oc share the same datasets with different annotations, and the same for subtasks V-reg and V-oc. El-‐reg El-‐oc Train Anger 2089 Joy 1906 Sadness 1930 Fear 2641 Test Anger 1002 Joy 1105 Sadness 975 Fear 986 Tr"
S18-1053,S16-1004,0,0.0311245,"rs in this area can be classified under two main areas: a lack of annotated resources and the challenges of the Arabic language’s complex morphology relative to other languages (Assiri et al., 2015). Although recent research has been dedicated to detect emotions for English content, to our knowledge, there are few studies for Arabic content. Researchers (Rabie and Sturm, 2014) collected and annotated data and applied different preprocessing steps related to the Arabic language. They also used a simplification of the SVM (known as SMO) and the NaiveBayes classifiers. Another two related works (Kiritchenko et al., 2016; Rosenthal et al., 2017) shared different tasks to identify the overall sentiments of the tweets or phrases taken from tweets in both English and Arabic. Our work uses the state-of-the3 Task Description and Datasets SemEval-2018 Task 1, Affect in Tweets, presents five subtasks (El-reg, El-oc, V-reg, V-oc, and E-c.) The subtasks provide training and testing for Twitter datasets in the English, Arabic, and Spanish languages (Mohammad and Kiritchenko, 2018). Task 1 asks the participants to predict the intensity of emotions and sentiments in the testing datasets. It also includes multi-label emot"
S18-1053,W16-1609,0,0.0321308,"Marquez, 2017; BravoMarquez et al., 2014), 43 features have been extracted using the TweetToLexiconFeatureVector attribute that calculates attributes for a tweet using a variety of lexical resources; two-dimensional vector using the Sentiment strength feature from the same package, and the final 100 dimensional vector is obtained by vectorizing the tweets to embeddings attribute also from the same package. Doc2Vec-300: Each tweet is represented as a 300 dimensional vector by concatenating two vectors of 150 dimensions each, using the documentlevel embeddings (’doc2vec’) (Le and Mikolov, 2014; Lau and Baldwin, 2016). The vector for each word in the tweet has been averaged to attain a 150 dimensional representation of the tweet. Word2Vec-300: Each tweet is represented as a 300 dimensional vector using the pretrained word2vec embedding model that is trained on Google News (Mikolov et al., 2013b), and for Arabic tweets, we use the pretrained embedding model that is trained on Arabic tweets (Twt-SG) (Soliman et al., 2017). PaddingWord2Vec-300: Each word in a tweet is represented as a 300 dimensional vector. The same pretraind word2vec embedding models that are used in Word2Vec-300 are also used in this featu"
S18-1053,pak-paroubek-2010-twitter,0,0.0581566,"s for this research. 350 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 350–357 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 2 Related work art approaches of deep learning and word/doc embedding. Sentiment and Emotion Analysis: Sentiment analysis was first explored in 2003 by Nasukawa and Yi (Nasukawa and Yi, 2003). An interest in studying and building models for sentiment analysis and emotion detection for social microblogging platforms has increased significantly in recent years (Kouloumpis et al., 2011; Pak and Paroubek, 2010; Oscar et al., 2017; Jimenez-Zafra et al., 2017). Going beyond the task of mainly classifying tweets as positive or negative, several approaches to detect emotions were presented in previous research papers (Mohammad and Kiritchenko, 2015; Tromp and Pechenizkiy, 2014; Mohammad, 2012). Researchers (Mohammad and Bravo-Marquez, 2017) introduced the WASSA2017 shared task of detecting the intensity of emotion felt by the speaker of a tweet. The stateof-the-art system in that competition (Goel et al., 2017) used an approach of ensembling three different deep neural network-based models, representin"
S18-1053,S17-2088,0,0.0327438,"ssified under two main areas: a lack of annotated resources and the challenges of the Arabic language’s complex morphology relative to other languages (Assiri et al., 2015). Although recent research has been dedicated to detect emotions for English content, to our knowledge, there are few studies for Arabic content. Researchers (Rabie and Sturm, 2014) collected and annotated data and applied different preprocessing steps related to the Arabic language. They also used a simplification of the SVM (known as SMO) and the NaiveBayes classifiers. Another two related works (Kiritchenko et al., 2016; Rosenthal et al., 2017) shared different tasks to identify the overall sentiments of the tweets or phrases taken from tweets in both English and Arabic. Our work uses the state-of-the3 Task Description and Datasets SemEval-2018 Task 1, Affect in Tweets, presents five subtasks (El-reg, El-oc, V-reg, V-oc, and E-c.) The subtasks provide training and testing for Twitter datasets in the English, Arabic, and Spanish languages (Mohammad and Kiritchenko, 2018). Task 1 asks the participants to predict the intensity of emotions and sentiments in the testing datasets. It also includes multi-label emotion classification subtas"
shaikh-etal-2010-mpc,W03-2118,0,\N,Missing
shaikh-etal-2010-mpc,P05-2014,0,\N,Missing
W10-2708,webb-etal-2010-evaluating,1,0.875558,"Missing"
W13-0909,W10-0303,0,0.588845,"Missing"
W13-0909,J91-1003,0,0.409834,"aphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method reli"
W13-0909,W06-3506,0,0.850487,"Missing"
W13-0909,P03-1054,0,0.00653924,"g is presented in a separate, future publication. Words that have an imageability rating lower than an experimentally determined threshold are further excluded from consideration. In the exam70 ple shown in Figure 1, words that have sufficiently high imageability scores are “labyrinthine”, “port”, “rail” and “airline”. We shall consider them as candidate relations, to be further investigated, as explained in the dependency parsing step described next. 3.3 Relation Extraction Dependency parsing reveals the syntactic structure of the sentence with the Target concept. We use the Stanford parser (Klein and Manning, 2003) for English language data. We identify candidate metaphorical relations to be any verbs that have the Target concept in direct dependency path (other than auxiliary and modal verbs). We exclude verbs of attitude (“think”, “say”, “consider”), since these have been found to be more indicative of metonymy than of metaphor. This list of attitude verbs is automatically derived from WordNet. From the example shown in Figure 1, one of the candidate relations extracted would be the verb “navigate”. In addition, we have a list of candidate relations from Step 3.2, which are the highly imageable nouns"
W13-0909,W07-0103,0,0.160031,"ig, 1995) who have focused on the 68 way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on refugees (1992), see metaphor as a tool to help outsiders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Political Science underpinning (Musolff, 2008; Lakoff, 2001). In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Naraya"
W13-0909,P80-1004,0,0.840031,"ed. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data,"
W13-0909,shutova-teufel-2010-metaphor,0,0.033737,"Net (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities"
W13-0909,C12-2109,0,0.305578,"Missing"
W13-0909,C10-1117,1,\N,Missing
W13-0909,D11-1063,0,\N,Missing
W13-1105,P03-1054,0,0.00392165,"in gaps. The lengths of these gaps are also important to measures for some behaviors. Meso-topics are the most persistent local topics, topics that are widely cited through long stretches of discourse. A selection of meso-topics is closely associated with the task in which the discourse participants are engaged. Short “gaps” in the chain are permitted (up to 10 turns, to accommodate digressions, obscure references, noise, etc.). Meso-topics can be distinguished from the local topics because the participants often make polarized statements about them. We use the Stanford part-of-speech tagger (Klein and Manning, 2003) to automatically detect nouns and noun phrases in dialogue and select those with subsequent mentions as local topics using a fairly simple pronoun resolution method based primarily on presence of specific lexical features as well as temporal distance between utterances. Princeton Wordnet (Fellbaum et al., 2006) is consulted to identify synonyms and other related words commonly used in co-references. The local topics that form sufficiently long co-reference chains are designated as meso-topics. 3.2 Topical Positioning Topical Positioning is defined as the attitude a speaker has towards the mes"
W13-1105,C10-1117,1,0.802734,"that constructs a model of a communitywide sentiment towards certain common issues discussed in social media, particularly forums and open blogs. This model is then used to assess whether a new post would fit into the targeted community by comparing the sentiment polarities about the concepts in the message and in the model. Potential posters are then guided in ways to shape their communication so that it minimizes the number of conflicting concept sentiments, while still preserving the intended message. 42 Another related research domain is about modeling the social phenomena in discourse. (Strzalkowski et al., 2010, Broadwell et al., 2012) proposed a two-tier approach that relies on extracting observable linguistic features of conversational text to detect mid-level social behaviors such as Topic Control, Disagreement and Involvement. These social behaviors are then used to infer higher-level social roles such as Leader and Influencer, which may have impact on how other participants’ opinions form and change. 3 System Modules In this section, we describe a series of modules in our system, which include meso-topic extraction, topical positioning and topical positioning map, and explain how we capture opi"
W14-2306,W10-0303,0,0.0169498,"age. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van der Sluis and Mellish 2008). It is expressed through multiple means, many"
W14-2306,P80-1004,0,0.702834,"rishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to"
W14-2306,J91-1003,0,0.690231,"Missing"
W14-2306,W06-3506,0,0.0751735,"Missing"
W14-2306,C04-1200,0,0.115008,"metimes referred to as intensity. Our approach to affect in metaphor has been vetted not only by our core linguistic team but also by an independent team of linguist-analysts with whom we work to understand metaphor across several language-culture groups. Our research continues to show no difficulties in comprehension or disagreement across languages concerning the concept of linguistic affect, of its application to metaphor, and of its having both polarity and intensity. 5 Related Research: sentiment and affect There is a relatively large volume of research on sentiment analysis in language (Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; inter alia) that aim at detecting polarity of text, but is not specifically concerned with metaphors. A number of systems were developed to automatically extract writer’s senti1 The First Workshop on Metaphor in NLP. http://aclweb.org/anthology//W/W13/W13-09.pdf 43 ment towards specific products or services such as movies or hotels, from online reviews (e.g., Turney, 2002; Pang and Lee, 2008) or social media messages (e.g., Thelwall et al., 2010). None of these techniques has been applied specifically to metaphorical language, and it is"
W14-2306,P13-1067,0,0.356958,"egative categories. However, the presence of largely negative concepts such as “poverty” in a given sentence overwhelms the sentiment for the sentence in their method. Other relevant efforts in sentence level sentiment analysis include Sem-Eval Task2. While presence of affect in metaphorical language is well documented in linguistic and psycholinguistic literature (e.g., Osgood, 1980; Pavio and Walsh, 1993; Caffi and Janney, 1994; Steen, 1994), relatively little work was done to detect affect automatically. Some notable recent efforts include Zhang and Barnden (2010), Veale and Li (2012), and Kozareva (2013), who proposed various models of metaphor affect classification based primarily on lexical features of the surrounding text: specifically the word polarity information. In these and other similar approaches, which are closely related to sentiment analysis, affect is attributed to the entire text fragment: a sentence or utterance containing a metaphor, or in some cases the immediate textual context around it. In contrast, our objective is to isolate affect due to the metaphor itself, independently of its particular context, and also to determine how various elements of the metaphoric expression"
W14-2306,W07-0103,0,0.0258652,"loran, 2007) that attempt to correlate metaphor semantics with their usage in naturally occurring text but generally lack robust tools to do so; and (3) social science approaches, particularly in psychology and anthropology that seek to explain how people produce and understand metaphors in interaction, but which lack the necessary computational tools to work with anything other than relatively isolated examples. In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & N"
W14-2306,S13-2053,0,0.216251,"m a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O’Halloran, 2007) that attempt to correlate metaphor semantics with their usag"
W14-2306,W13-0904,0,0.0153035,"interest are those that operate between the concepts within a Source domain and can be “borrowed” to link concepts within the Target domain, e.g., “Crime(TARGET) spread to(RELATION) previously safe areas” may be borrowing from a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors a"
W14-2306,shutova-teufel-2010-metaphor,0,0.0617997,"Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van d"
W14-2306,P10-1071,0,0.0174283,"between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van der Sluis and Mellish 2008). It i"
W14-2306,C12-2109,0,0.0359731,"Missing"
W14-2306,S07-1013,0,0.0701286,"Missing"
W14-2306,W13-0909,1,0.73941,"may be borrowing from a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O’Halloran, 2007) that attempt to correlate metaphor semantics with their usag"
W14-2306,P02-1053,0,0.0132888,"Missing"
W14-2306,P12-3002,0,0.0922262,"sentences into positive/negative categories. However, the presence of largely negative concepts such as “poverty” in a given sentence overwhelms the sentiment for the sentence in their method. Other relevant efforts in sentence level sentiment analysis include Sem-Eval Task2. While presence of affect in metaphorical language is well documented in linguistic and psycholinguistic literature (e.g., Osgood, 1980; Pavio and Walsh, 1993; Caffi and Janney, 1994; Steen, 1994), relatively little work was done to detect affect automatically. Some notable recent efforts include Zhang and Barnden (2010), Veale and Li (2012), and Kozareva (2013), who proposed various models of metaphor affect classification based primarily on lexical features of the surrounding text: specifically the word polarity information. In these and other similar approaches, which are closely related to sentiment analysis, affect is attributed to the entire text fragment: a sentence or utterance containing a metaphor, or in some cases the immediate textual context around it. In contrast, our objective is to isolate affect due to the metaphor itself, independently of its particular context, and also to determine how various elements of the"
W14-2306,W13-0905,0,0.020242,"at operate between the concepts within a Source domain and can be “borrowed” to link concepts within the Target domain, e.g., “Crime(TARGET) spread to(RELATION) previously safe areas” may be borrowing from a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language"
W14-4725,P80-1004,0,0.355181,"ed. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data,"
W14-4725,J91-1003,0,0.171713,"Missing"
W14-4725,W06-3506,0,0.0458493,"Missing"
W14-4725,W07-0103,0,0.0290505,"errig, 1995) who have focused on the way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on refugees (1992), see metaphor as a tool to help outsiders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Political Science underpinning (Musolff, 2008; Lakoff, 2001). In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Naraya"
W14-4725,shutova-teufel-2010-metaphor,0,0.0214182,"Net (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantitie"
W14-4725,P10-1071,0,0.014226,"tiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generate"
W14-4725,C12-2109,0,0.0320502,"Missing"
W14-4725,W13-0905,0,0.016931,"initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. More recently, several important approaches to metaphor extraction have emerged from the IARPA Metaphor program, including Broadwell et al (2013), Strzalkowski et al. (2014), Wilks et al (2013), Hovy et al (2013) inter alia. These papers concentrate on the algorithms for detection and classification of individual linguistic metaphors in text rather than formation of conceptual metaphors in a broader cultural context. Taylor et al (2014) outlines the rationale why conceptual level metaphors may provide important insights into cross-cultural contrasts. Our work described here is a first attempt at automatic discovery of conceptual metaphors operating within a culture directly from the linguistic evidence in language. 3 Our Approach The process of discovering conceptual metaphors is ne"
W14-4725,W13-0907,0,\N,Missing
W14-4725,shaikh-etal-2014-multi,1,\N,Missing
W14-4725,W13-0909,1,\N,Missing
W15-1408,W10-0303,0,0.0227642,"pages 67–76, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Understanding conflicts in this manner may allow policy-makers facilitate negotiations and discussions across different communities and help bridge contrasting viewpoints and cultural values. 2 Relevant Research The underlying core of our research is automated, large-scale metaphor extraction. Computational approaches to metaphor to date have yielded only limited scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our"
W15-1408,C12-2016,0,0.0515284,"Missing"
W15-1408,J91-1003,0,0.629185,"Missing"
W15-1408,W13-1105,1,0.941007,"ssessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant work includes Nguyen et al. (2013), who look at nonparametric topic modeling as a measure of influence; and Bracewell et al. (2012), who look at a category of social acts to determine measures of leadership; among others. Analysis of positions held by discourse participants has been studied in the realm of political science and computational sociolinguistics (Laver, Benoit & Garry, 2003; Slapin & Proksch, 2008; Lin et al., 2013; Pang & Lee, 2008) and our approach draws parallels from such prior work. Our topical positioning approach is a departure from existing approaches to sentiment analysis (Wiebe, Wilson and Cardie, 2005; Strapparava and Mihalcea, 2008) in looking at a larger context of discourse rather than individual utterances. 68 3 The Conflict – U.S. Gun Debate The main hypothesis, and an open research question, is then: can this new technology be effectively applied to understanding of a broad cultural conflict such as may arise in any society where potentially divisive issues exist? To answer this questio"
W15-1408,W13-0904,0,0.020107,"ave yielded only limited scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our approach builds upon their work. The use of metaphor, along with sociocultural aspects of language to understand cross-cultural conflict is novel to our approach. Recent research in computational sociolinguistic has developed methods for automatic assessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant"
W15-1408,shutova-teufel-2010-metaphor,0,0.0163255,"e Third Workshop on Metaphor in NLP, pages 67–76, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Understanding conflicts in this manner may allow policy-makers facilitate negotiations and discussions across different communities and help bridge contrasting viewpoints and cultural values. 2 Relevant Research The underlying core of our research is automated, large-scale metaphor extraction. Computational approaches to metaphor to date have yielded only limited scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extract"
W15-1408,C10-1117,1,0.822159,"approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our approach builds upon their work. The use of metaphor, along with sociocultural aspects of language to understand cross-cultural conflict is novel to our approach. Recent research in computational sociolinguistic has developed methods for automatic assessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant work includes Nguyen et al. (2013), who look at nonparametric topic modeling as a measure of influence; and Bracewell et al. (2012), who look at a category of social acts to determine measures of leadership; among others. Analysis of positions held by discourse participants has been studied in the realm of political science and computational sociolinguistics (Laver, Benoit & Garry, 2003; Slapin & Proksch, 2008; Lin et al., 2013; Pang & Lee, 2008) and our approach draws parallels from such prior work. Our topical positioning approach is a depa"
W15-1408,W13-0909,1,0.870627,"the listener (Perloff, 2014). Metaphors, which are mapping systems that allow the semantics of a familiar Source domain to be applied to a Target domain so that new frameworks of reasoning can emerge in the Target domain, are pervasive in discourse. Metaphorically rich language is considered highly influential. Persuasion and influence literature (Soppory and Dillard, 2002) indicates messages containing metaphorical language produce somewhat greater attitude change than messages that do not. Metaphors embody a number of elements of persuasive language, including concreteness and imageability (Strzalkowski et al., 2013, Broadwell et al., 2013, Charteris-Black, 2005). Using this line of investigation, we aim to understand the motivations of a group or of a political faction through their discourse, as part of the answer to such questions as: What are the key differences in protagonists’ positions? How extensive is a protagonists’ influence? Who dominates the discourse? Where is the core of the groups’ support? Our goal is to provide a basis for the analysis of cross-cultural conflicts by viewing the conflict as an ongoing debate or a “dialogue” between protagonists or participants. In this interpretation, ea"
W15-1408,W13-0905,0,0.027816,"ted scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our approach builds upon their work. The use of metaphor, along with sociocultural aspects of language to understand cross-cultural conflict is novel to our approach. Recent research in computational sociolinguistic has developed methods for automatic assessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant work includes Nguyen"
W19-8610,S17-1008,0,0.0129323,"a Shaikh Computer Science University of North Carolina at Charlotte Charlotte, NC, USA {ssantha1,sshaikh2}@uncc.edu Abstract poor correlation with human judgments. Despite their shortcomings, automated metrics like BLEU, ROUGE, and METEOR are used due to a lack of alternative metrics. This puts a major imperative on obtaining high-quality crowdsourced human judgments. Previous research which employs crowdsourced judgments has focused on metrics including ease of answering, information flow and coherence (Li et al., 2016; Dziri et al., 2018), naturalness (Asghar et al., 2018), interestingness (Asghar et al., 2017; Santhanam and Shaikh, 2019), fluency or readability (Zhang et al., 2018), engagement (Venkatesh et al., 2018). While experiment designs primarily use Likert scales, Belz and Kow (2010) argue that discrete scales, such as the Likert scales, can be unintuitive and certain individuals may avoid extreme values in their judgments. Prior research has also shown that use of continuous scales is more viable for language evaluation (Novikova et al., 2018; Belz and Kow, 2011). Such evidence places more emphasis on a careful study towards obtaining reliable and consistent human ratings for dialogue eva"
W19-8610,D16-1127,0,0.0429495,"s Best Experiment Design for Evaluating Dialogue System Output Sashank Santhanam and Samira Shaikh Computer Science University of North Carolina at Charlotte Charlotte, NC, USA {ssantha1,sshaikh2}@uncc.edu Abstract poor correlation with human judgments. Despite their shortcomings, automated metrics like BLEU, ROUGE, and METEOR are used due to a lack of alternative metrics. This puts a major imperative on obtaining high-quality crowdsourced human judgments. Previous research which employs crowdsourced judgments has focused on metrics including ease of answering, information flow and coherence (Li et al., 2016; Dziri et al., 2018), naturalness (Asghar et al., 2018), interestingness (Asghar et al., 2017; Santhanam and Shaikh, 2019), fluency or readability (Zhang et al., 2018), engagement (Venkatesh et al., 2018). While experiment designs primarily use Likert scales, Belz and Kow (2010) argue that discrete scales, such as the Likert scales, can be unintuitive and certain individuals may avoid extreme values in their judgments. Prior research has also shown that use of continuous scales is more viable for language evaluation (Novikova et al., 2018; Belz and Kow, 2011). Such evidence places more emphas"
W19-8610,D16-1230,0,0.143894,"Missing"
W19-8610,W10-4201,0,0.050865,"ings, automated metrics like BLEU, ROUGE, and METEOR are used due to a lack of alternative metrics. This puts a major imperative on obtaining high-quality crowdsourced human judgments. Previous research which employs crowdsourced judgments has focused on metrics including ease of answering, information flow and coherence (Li et al., 2016; Dziri et al., 2018), naturalness (Asghar et al., 2018), interestingness (Asghar et al., 2017; Santhanam and Shaikh, 2019), fluency or readability (Zhang et al., 2018), engagement (Venkatesh et al., 2018). While experiment designs primarily use Likert scales, Belz and Kow (2010) argue that discrete scales, such as the Likert scales, can be unintuitive and certain individuals may avoid extreme values in their judgments. Prior research has also shown that use of continuous scales is more viable for language evaluation (Novikova et al., 2018; Belz and Kow, 2011). Such evidence places more emphasis on a careful study towards obtaining reliable and consistent human ratings for dialogue evaluation. To address this research problem, we focus on a systematic comparison of four experimental conditions by incorporating continuous, relative and ranking scales for obtaining crow"
W19-8610,P11-2040,0,0.559722,"ring, information flow and coherence (Li et al., 2016; Dziri et al., 2018), naturalness (Asghar et al., 2018), interestingness (Asghar et al., 2017; Santhanam and Shaikh, 2019), fluency or readability (Zhang et al., 2018), engagement (Venkatesh et al., 2018). While experiment designs primarily use Likert scales, Belz and Kow (2010) argue that discrete scales, such as the Likert scales, can be unintuitive and certain individuals may avoid extreme values in their judgments. Prior research has also shown that use of continuous scales is more viable for language evaluation (Novikova et al., 2018; Belz and Kow, 2011). Such evidence places more emphasis on a careful study towards obtaining reliable and consistent human ratings for dialogue evaluation. To address this research problem, we focus on a systematic comparison of four experimental conditions by incorporating continuous, relative and ranking scales for obtaining crowdsourced human judgments. In this initial study, we evaluate the use of two metrics: Readability and Coherence. Our key findings are: 1. Use of Likert scales results in the lowest inter-rater consistency and agreement when compared to other experiment conditions 2. Use of continuous sc"
W19-8610,P17-1103,0,0.0203186,"tion history. 4 Experiment Designs In our study, we use three well-known question types of Likert Scale, Magnitude Estimation and Best-Worst Ranking. We chose these questions types to investigate as these are commonly used across various language evaluation tasks (Belz and Kow, 2011; Asghar et al., 2018; Novikova et al., 2018; Kiritchenko and Mohammad, 2017) . With the help of these three types of questions, we design four rating procedures that are explained below. Likert Scale (LS): is typically used in experiments for crowdsourcing human evaluation of dialogue systems (Asghar et al., 2018; Lowe et al., 2017). In our experiment, we ask the raters to rate the generated responses on a 6-point scale, following Novikova et al. (2018) (where 1 is the lowest and 6 is the highest on the metrics of readability and coherence). Rank-Based Magnitude Estimation (RME): Prior research by Belz and Kow (2011) demonstrates through six separate experiments that continuous scales are more viable and offer distinct advantages over discrete scales in evaluation tasks. Recently, Novikova et al. (2018) adopted magnitude estimation by providing the rater with a standard value for a reference sentence to evaluate output f"
W19-8610,W19-3646,0,0.15106,"nd Kow (2011) demonstrates through six separate experiments that continuous scales are more viable and offer distinct advantages over discrete scales in evaluation tasks. Recently, Novikova et al. (2018) adopted magnitude estimation by providing the rater with a standard value for a reference sentence to evaluate output from goal-oriented systems. Following Novikova et al. (2018), we also set the value of the For this initial study, we focus on two metrics, readability and coherence. These metrics are among those essential to evaluate the quality of generated responses (Novikova et al., 2017; Dziri et al., 2019). We describe an automated method to compute each metric. Readability or Fluency measures the linguistic quality of text and helps quantify the difficulty of understanding the text for a reader (Gatt and Krahmer, 2018; Novikova et al., 2017). We use the Flesch Reading Ease (FRE) (Kincaid et al., 1975) that counts the number of words, syllables and sentences in the text.2 Higher readability scores indicate that utterance is easier to read and comprehend. Coherence measures the ability of the dialogue system to produce responses consistent with the topic of conversation (Venkatesh et al., 2018)."
W19-8610,D17-1238,0,0.218011,"Missing"
W19-8610,N18-2012,0,0.172359,"Missing"
W19-8610,P18-1205,0,0.0213074,"tte, NC, USA {ssantha1,sshaikh2}@uncc.edu Abstract poor correlation with human judgments. Despite their shortcomings, automated metrics like BLEU, ROUGE, and METEOR are used due to a lack of alternative metrics. This puts a major imperative on obtaining high-quality crowdsourced human judgments. Previous research which employs crowdsourced judgments has focused on metrics including ease of answering, information flow and coherence (Li et al., 2016; Dziri et al., 2018), naturalness (Asghar et al., 2018), interestingness (Asghar et al., 2017; Santhanam and Shaikh, 2019), fluency or readability (Zhang et al., 2018), engagement (Venkatesh et al., 2018). While experiment designs primarily use Likert scales, Belz and Kow (2010) argue that discrete scales, such as the Likert scales, can be unintuitive and certain individuals may avoid extreme values in their judgments. Prior research has also shown that use of continuous scales is more viable for language evaluation (Novikova et al., 2018; Belz and Kow, 2011). Such evidence places more emphasis on a careful study towards obtaining reliable and consistent human ratings for dialogue evaluation. To address this research problem, we focus on a systematic compar"
