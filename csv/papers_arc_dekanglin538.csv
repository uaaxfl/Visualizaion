D18-1235,A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension,2018,0,4,6,0,25672,jiahua liu,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"The task of machine reading comprehension (MRC) has evolved from answering simple questions from well-edited text to answering real questions from users out of web data. In the real-world setting, full-body text from multiple relevant documents in the top search results are provided as context for questions from user queries, including not only questions with a single, short, and factual answer, but also questions about reasons, procedures, and opinions. In this case, multiple answers could be equally valid for a single question and each answer may occur multiple times in the context, which should be taken into consideration when we build MRC system. We propose a multi-answer multi-task framework, in which different loss functions are used for multiple reference answers. Minimum Risk Training is applied to solve the multi-occurrence problem of a single answer. Combined with a simple heuristic passage extraction strategy for overlong documents, our model increases the ROUGE-L score on the DuReader dataset from 44.18, the previous state-of-the-art, to 51.09."
N12-1095,Unsupervised Translation Sense Clustering,2012,28,22,3,0,717,mohit bansal,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common semantic sense. Words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using the soft K-Means algorithm. In addition to describing our approach, we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation. By comparing our induced clusters to reference clusters generated from WordNet, we demonstrate that our method effectively identifies sense-based translation clusters and benefits from both monolingual and parallel corpora. Finally, we describe a method for annotating clusters with usage examples."
W10-2921,Improved Natural Language Learning via Variance-Regularization Support Vector Machines,2010,46,8,2,1,39132,shane bergsma,Proceedings of the Fourteenth Conference on Computational Natural Language Learning,0,"We present a simple technique for learning better SVMs using fewer training examples. Rather than using the standard SVM regularization, we regularize toward low weight-variance. Our new SVM objective remains a convex quadratic function of the weights, and is therefore computationally no harder to optimize than a standard SVM. Variance regularization is shown to enable dramatic improvements in the learning rates of SVMs on three lexical disambiguation tasks."
P10-1089,Creating Robust Supervised Classifiers via Web-Scale N-Gram Data,2010,33,30,3,1,39132,shane bergsma,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance."
lin-etal-2010-new,New Tools for Web-Scale N-grams,2010,32,68,1,1,30549,dekang lin,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"While the web provides a fantastic linguistic resource, collecting and processing data at web-scale is beyond the reach of most academic laboratories. Previous research has relied on search engines to collect online information, but this is hopelessly inefficient for building large-scale linguistic resources, such as lists of named-entity types or clusters of distributionally similar words. An alternative to processing web-scale text directly is to use the information provided in an N-gram corpus. An N-gram corpus is an efficient compression of large amounts of text. An N-gram corpus states how often each sequence of words (up to length N) occurs. We propose tools for working with enhanced web-scale N-gram corpora that include richer levels of source annotation, such as part-of-speech tags. We describe a new set of search tools that make use of these tags, and collectively lower the barrier for lexical learning and ambiguity resolution at web-scale. They will allow novel sources of information to be applied to long-standing natural language challenges."
C10-1100,Using Web-scale N-grams to Improve Base {NP} Parsing Performance,2010,31,28,3,0,5826,emily pitler,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We use web-scale N-grams in a base NP parser that correctly analyzes 95.4% of the base NPs in natural text. Web-scale data improves performance. That is, there is no data like more data. Performance scales log-linearly with the number of parameters in the model (the number of unique N-grams). The web-scale N-grams are particularly helpful in harder cases, such as NPs that contain conjunctions."
Y09-1024,Gender and {A}nimacy Knowledge Discovery from Web-Scale N-Grams for Unsupervised Person Mention Detection,2009,15,35,2,0,716,heng ji,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"In this paper we present a simple approach to discover gender and animacy knowledge for person mention detection. We learn noun-gender and noun-animacy pair counts from web-scale n-grams using specific lexical patterns, and then apply confidence estimation metrics to filter noise. The selected informative pairs are then used to detect person mentions from raw texts in an unsupervised learning framework. Experiments showed that this approach can achieve high performance comparable to state-of-the-art supervised learning methods which require manually annotated corpora and gazetteers."
W09-1116,"Glen, Glenda or Glendale: Unsupervised and Semi-supervised Learning of {E}nglish Noun Gender",2009,30,6,2,1,39132,shane bergsma,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL}-2009),0,"English pronouns like he and they reliably reflect the gender and number of the entities to which they refer. Pronoun resolution systems can use this fact to filter noun candidates that do not agree with the pronoun gender. Indeed, broad-coverage models of noun gender have proved to be the most important source of world knowledge in automatic pronoun resolution systems.n n Previous approaches predict gender by counting the co-occurrence of nouns with pronouns of each gender class. While this provides useful statistics for frequent nouns, many infrequent nouns cannot be classified using this method. Rather than using co-occurrence information directly, we use it to automatically annotate training examples for a large-scale discriminative gender model. Our model collectively classifies all occurrences of a noun in a document using a wide variety of contextual, morphological, and categorical gender features. By leveraging large volumes of un-labeled data, our full semi-supervised system reduces error by 50% over the existing state-of-the-art in gender classification."
P09-1116,Phrase Clustering for Discriminative Learning,2009,25,182,1,1,30549,dekang lin,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers. To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification. Our results show that phrase clusters offer significant improvements over word clusters. Our NER system achieves the best current result on the widely used CoNLL benchmark. Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts."
E09-1076,Flexible Answer Typing with Discriminative Preference Ranking,2009,16,8,2,1,47395,christopher pinchak,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,An important part of question answering is ensuring a candidate answer is plausible as a response. We present a flexible approach based on discriminative preference ranking to determine which of a set of candidate answers are appropriate. Discriminative methods provide superior performance while at the same time allow the flexibility of adding new and diverse features. Experimental results on a set of focused What ...? and Which ...? questions show that our learned preference ranking methods perform better than alternative solutions to the task of answer typing. A gain of almost 0.2 in MRR for both the first appropriate and first correct answers is observed along with an increase in precision over the entire range of recall.
P08-1002,Distributional Identification of Non-Referential Pronouns,2008,27,25,2,1,39132,shane bergsma,Proceedings of ACL-08: HLT,1,"We present an automatic approach to determining whether a pronoun in text refers to a preceding noun phrase or is instead nonreferential. We extract the surrounding textual context of the pronoun and gather, from a large corpus, the distribution of words that occur within that context. We learn to reliably classify these distributions as representing either referential or non-referential pronoun instances. Despite its simplicity, experimental results on classifying the English pronoun it show the system achieves the highest performance yet attained on this important task."
P08-1061,Semi-Supervised Convex Training for Dependency Parsing,2008,35,17,3,0.740741,45770,qin wang,Proceedings of ACL-08: HLT,1,"We present a novel semi-supervised training algorithm for learning dependency parsers. By combining a supervised large margin loss with an unsupervised least squares loss, a discriminative, convex, semi-supervised learning algorithm can be obtained that is applicable to large-scale problems. To demonstrate the benefits of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora. Using a stochastic gradient descent algorithm, a parsing model can be efficiently learned from semi-supervised data that significantly outperforms corresponding supervised methods."
P08-1113,Mining Parenthetical Translations from the Web by Word Alignment,2008,13,49,1,1,30549,dekang lin,Proceedings of ACL-08: HLT,1,"Documents in languages such as Chinese, Japanese and Korean sometimes annotate terms with their translations in English inside a pair of parentheses. We present a method to extract such translations from a large collection of web documents by building a partially parallel corpus and use a word alignment algorithm to identify the terms being translated. The method is able to generalize across the translations for different terms and can reliably extract translations that occurred only once in the entire web. Our experiment on Chinese web pages produced more than 26 million pairs of translations, which is over two orders of magnitude more than previous results. We show that the addition of the extracted translation pairs as training data provides significant increase in the BLEU score for a statistical machine translation system."
D08-1007,Discriminative Learning of Selectional Preference from Unlabeled Text,2008,33,55,2,1,39132,shane bergsma,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We present a discriminative method for learning selectional preferences from unlabeled text. Positive examples are taken from observed predicate-argument pairs, while negatives are constructed from unobserved combinations. We train a Support Vector Machine classifier to distinguish the positive from the negative instances. We show how to partition the examples for efficient training with 57 thousand features and 6.5 million training instances. The model outperforms other recent approaches, achieving excellent correlation with human plausibility judgments. Compared to Mutual Information, it identifies 66% more verb-object pairs in unseen text, and resolves 37% more pronouns correctly in a pronoun resolution experiment."
W07-0403,Inversion Transduction Grammar for Joint Phrasal Translation Modeling,2007,20,57,2,1,3520,colin cherry,"Proceedings of {SSST}, {NAACL}-{HLT} 2007 / {AMTA} Workshop on Syntax and Structure in Statistical Translation",0,"We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models. This syntactic model is similar to its flat-string phrasal predecessors, but admits polynomial-time algorithms for Viterbi alignment and EM training. We demonstrate that the consistency constraints that allow flat phrasal models to scale also help ITG algorithms, producing an 80-times faster inside-outside algorithm. We also show that the phrasal translation tables produced by the ITG are superior to those of the flat joint phrasal model, producing up to a 2.5 point improvement in BLEU score. Finally, we explore, for the first time, the utility of a joint phrasal translation model as a word alignment method."
P06-2014,Soft Syntactic Constraints for Word Alignment through Discriminative Training,2006,19,47,2,1,3520,colin cherry,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree. However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex. We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser."
P06-1005,Bootstrapping Path-Based Pronoun Resolution,2006,19,101,2,0.952381,39132,shane bergsma,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We present an approach to pronoun resolution based on syntactic paths. Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities. This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints. Highly coreferent paths also allow mining of precise probabilistic gender/number information. We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier. Significant gains in performance are observed on several datasets."
P06-1102,Names and Similarities on the Web: Fact Extraction in the Fast Lane,2006,15,86,2,0,32671,marius pacsca,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In a new approach to large-scale extraction of facts from unstructured text, distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns, and the validation and ranking of candidate facts. The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents, starting from ten seed facts and using no additional knowledge, lexicons or complex tools."
E06-1019,A Comparison of Syntactically Motivated Word Alignment Spaces,2006,14,14,2,1,3520,colin cherry,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This work is concerned with the space of alignments searched by word alignment systems. We focus on situations where word re-ordering is limited by syntax. We present two new alignment spaces that limit an ITG according to a given dependency parse. We provide D-ITG grammars to search these spaces completely and without redundancy. We conduct a careful comparison of five alignment spaces, and show that limiting search with an ITG reduces error rate by 10%, while a D-ITG produces a 31% reduction."
E06-1050,A Probabilistic Answer Type Model,2006,19,32,2,1,47395,christopher pinchak,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"All questions are implicitly associated with an expected answer type. Unlike previous approaches that require a predefined set of question types, we present a method for dynamically constructing a probability-based answer type model for each different question. Our model evaluates the appropriateness of a potential answer by the probability that it fits into the question contexts. Evaluation is performed against manual and semiautomatic methods using a fixed set of answer labels. Results show our approach to be superior for those questions classified as having a miscellaneous answer type."
W05-1516,Strictly Lexical Dependency Parsing,2005,22,19,3,0.740741,45770,qin wang,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We present a strictly lexical parsing model where all the parameters are based on the words. This model does not rely on part-of-speech tags or grammatical categories. It maximizes the conditional probability of the parse tree given the sentence. This is in contrast with most previous models that compute the joint probability of the parse tree and the sentence. Although the maximization of joint and conditional probabilities are theoretically equivalent, the conditional model allows us to use distributional word similarity to generalize the observed frequency counts in the training corpus. Our experiments with the Chinese Treebank show that the accuracy of the conditional model is 13.6% higher than the joint model and that the strictly lexicalized conditional model outperforms the corresponding unlexicalized model based on part-of-speech tags."
C04-1090,A Path-based Transfer Model for Machine Translation,2004,14,52,1,1,30549,dekang lin,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,We propose a path-based transfer model for machine translation. The model is trained with a word-aligned parallel corpus where the source language sentences are parsed. The training algorithm extracts a set of transfer rules and their probabilities from the training corpus. A rule translates a path in the source language dependency tree into a fragment in the target dependency tree. The problem of finding the most probable translation becomes a graph-theoretic problem of finding the minimum path covering of the source language dependency tree.
W03-0302,{P}ro{A}lign: Shared Task System Description,2003,8,9,1,1,30549,dekang lin,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,0,"ProAlign combines several different approaches in order to produce high quality word word alignments. Like competitive linking, ProAlign uses a constrained search to find high scoring alignments. Like EM-based methods, a probability model is used to rank possible alignments. The goal of this paper is to give a bird's eye view of the ProAlign system to encourage discussion and comparison."
P03-1012,A Probability Model to Improve Word Alignment,2003,18,94,2,0.930233,3520,colin cherry,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,Word alignment plays a crucial role in statistical machine translation. Word-aligned corpora have been found to be an excellent source of translation-related knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment.
N03-4011,Automatically Discovering Word Senses,2003,10,10,2,1,34734,patrick pantel,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations,0,We will demonstrate the output of a distributional clustering algorithm called Clustering by Committee that automatically discovers word senses from text.
N03-2017,Word Alignment with Cohesion Constraint,2003,11,21,1,1,30549,dekang lin,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"We present a syntax-based constraint for word alignment, known as the cohesion constraint. It requires disjoint English phrases to be mapped to non-overlapping intervals in the French sentence. We evaluate the utility of this constraint in two different algorithms. The results show that it can provide a significant improvement in alignment quality."
C02-1144,Concept Discovery from Text,2002,15,157,1,1,30549,dekang lin,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Broad-coverage lexical resources such as WordNet are extremely useful. However, they often include many rare senses while missing domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers concepts from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning elements to their most similar cluster. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality."
H01-1046,{L}a{T}a{T}: Language and Text Analysis Tools,2001,10,30,1,1,30549,dekang lin,Proceedings of the First International Conference on Human Language Technology Research,0,"LaTaT is a Language and Text Analysis Toolset. This paper gives a brief description of the components comprising LaTaT, including a Minimalist parser and language and concept learning programs."
P00-1014,An Unsupervised Approach to Prepositional Phrase Attachment using Contextually Similar Words,2000,12,75,2,1,34734,patrick pantel,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"Prepositional phrase attachment is a common source of ambiguity in natural language processing. We present an unsupervised corpus-based approach to prepositional phrase attachment that achieves similar performance to supervised methods. Unlike previous unsupervised approaches in which training data is obtained by heuristic extraction of unambiguous examples from a corpus, we use an iterative process to extract training data from an automatically parsed corpus. Attachment decisions are made using a linear combination of features and low frequency events are approximated using contextually similar words."
dorr-etal-2000-chinese,{C}hinese-{E}nglish Semantic Resource Construction,2000,12,8,3,0.178571,14512,bonnie dorr,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Abstract : We describe an approach to large-scale construction of a semantic lexicon for Chinese verbs. We leverage off of three existing resources-- a classification of English verbs called EVCA (English Verbs Classes and Alternations), a Chinese conceptual database called HowNet, and a large-machine readable dictionary called Optilex. The resulting lexicon is used for determining appropriate word senses in applications such as machine translation and cross-language information retrieval."
A00-2011,Word-for-Word Glossing with Contextually Similar Words,2000,12,17,2,1,34734,patrick pantel,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Many corpus-based machine translation systems require parallel corpora. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. To gloss a word, we first identify its similar words that occurred in the same context in a large corpus. We then determine the gloss by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus."
dorr-etal-2000-building,Building a {C}hinese-{E}nglish mapping between verb concepts for multilingual applications,2000,19,8,3,0.178571,14512,bonnie dorr,Proceedings of the Fourth Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"This paper addresses the problem of building conceptual resources for multilingual applications. We describe new techniques for large-scale construction of a Chinese-English lexicon for verbs, using thematic-role information to create links between Chinese and English conceptual information. We then present an approach to compensating for gaps in the existing resources. The resulting lexicon is used for multilingual applications such as machine translation and cross-language information retrieval."
P99-1041,Automatic Identification of Non-compositional Phrases,1999,15,145,1,1,30549,dekang lin,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"Non-compositional expressions present a special challenge to NLP applications. We present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus. Our method is based on the hypothesis that when a phrase is non-composition, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word."
J99-2008,Book Reviews: {W}ord{N}et: An Electronic Lexical Database,1999,-1,-1,1,1,30549,dekang lin,Computational Linguistics,0,None
P98-2127,Automatic Retrieval and Clustering of Similar Words,1998,17,1462,1,1,30549,dekang lin,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.
M98-1006,Using Collocation Statistics in Information Extraction,1998,4,45,1,1,30549,dekang lin,"Seventh Message Understanding Conference ({MUC}-7): Proceedings of a Conference Held in Fairfax, Virginia, {A}pril 29 - May 1, 1998",0,"Our main objective in participating MUC-7 is to investigate and experiment with the use of collocation statistics in information extraction. A collocation is a habitual word combination, such as weather a storm,  le a lawsuit, and the falling yen. Collocation statistics refers to the frequency counts of the collocational relations extracted from a parsed corpus. For example, out of 6577 instances of addition in a corpus, 5190 was used as the object of in. Out of 3214 instances of hire, 12 of them take alien as the object."
C98-2122,Automatic Retrieval and Clustering of Similar Words,1998,17,1462,1,1,30549,dekang lin,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.
P97-1009,Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity,1997,20,237,1,1,30549,dekang lin,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,Most previous corpus-based algorithms disambiguate a word with a classifier trained from previous usages of the same word. Separate classifiers have to be trained for different words. We present an algorithm that uses the same knowledge sources to disambiguate different words. The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts.
A97-2010,A Broad-Coverage Word Sense Tagger,1997,7,0,1,1,30549,dekang lin,Fifth Conference on Applied Natural Language Processing: Descriptions of System Demonstrations and Videos,0,None
C96-2123,On the Structural Complexity of Natural Language Sentences,1996,5,30,1,1,30549,dekang lin,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,The objective of this paper is to formalize the intnition about the complexity of syntactic structures. We propose a definition of structural complexity such that sentences ranked by our definition as more complex are generally more difficult for humans to process. We justify the definition by showing how it is able to account for several seemingly unrelated phenomena in natural languages.
M95-1010,{U}niversity of {M}anitoba: Description of the {PIE} System Used for {MUC}-6,1995,9,13,1,1,30549,dekang lin,"Sixth Message Understanding Conference ({MUC}-6): Proceedings of a Conference Held in {C}olumbia, {M}aryland, November 6-8, 1995",0,"The PIE (Principar-driven Information Extraction) system takes a different approach to the problem of information extraction from the NUBA system that was used in MUC-5. The NUBA system did not have a parser and relies on an abductive reasoner to construct the semantic relationships between domain specific concepts mentioned in a sentence. The PIE system, on the other hand, relies heavily on a principle-based broad-coverage parser, called PRINCIPAR [2, 6, 8], that we have developed over the past three years. Most of the information extracted are directly read-off the parser outputs by a subtree pattern-matcher, bypassing the usual step of constructing semantic representations."
J95-2005,Squibs and Discussions: Efficient Parsing for {K}orean and {E}nglish: A Parameterized Message-Passing Approach,1995,11,6,3,0.178571,14512,bonnie dorr,Computational Linguistics,0,None
C94-1079,"{PRINCIPAR} - An Efficient, Broad-coverage, Principle-based Parser",1994,9,111,1,1,30549,dekang lin,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We present an efficient, broad-coverage, principle-based parser for English. The parser has been implemented in C and runs on SUN Sparcstations with X-windows. It contains a lexicon with over 90,000 entries, constructed automatically by applying a set of extraction and conversion rules to entries from machine readable dictionaries."
1994.amta-1.20,A Parameter-Based Message-Passing Parser for {MT} of {K}orean and {E}nglish,1994,-1,-1,1,1,30549,dekang lin,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
P93-1016,Principle-Based Parsing Without Overgeneration,1993,10,153,1,1,30549,dekang lin,31st Annual Meeting of the Association for Computational Linguistics,1,"Overgeneration is the main source of computational complexity in previous principle-based parsers. This paper presents a message passing algorithm for principle-based parsing that avoids the overgeneration problem. This algorithm has been implemented in C and successfully tested with example sentences from (van Riemsdijk and Williams, 1986)."
M93-1022,{U}niversity of {M}anitoba: Description of the {NUBA} System as Used for {MUC}-5,1993,9,5,1,1,30549,dekang lin,"Fifth Message Understanding Conference ({MUC}-5): Proceedings of a Conference Held in Baltimore, {M}aryland, August 25-27, 1993",0,"Abduction is the inference to the best explanation. Many tasks in natural language understanding such as word-sense disambiguity [1], local pragmatics [4], metaphor interpretation [3], and plan recognition [5, 8], can be viewed as abduction."
