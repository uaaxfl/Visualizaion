2020.cogalex-1.10,N18-2061,0,0.0852059,"set of encyclopedic definitions and distractors, and which we use in this paper. In a subsequent contribution, Espinosa-Anke and Saggion (2014) present a supervised approach in which only syntactic features derived from dependency relations are used, and whose results are reported higher to the WCL method. For identifying definitions with higher linguistic variability, a weakly supervised approach is presented in Espinosa-Anke et al. (2015). And finally, models based on neural networks have been leveraged for exploiting both long and short-range dependencies, either combining CNNs and LSTMS (Espinosa-Anke and Schockaert, 2018) or BERT (Veyseh et al., 2019), and which are currently the highest performing models on WCL. 3 Data In this section we present the datasets utilized for our analysis, namely WCL (Section 3.1) and DEFT (Section 3.2), and provide a descriptive analysis comparing both datasets (Section 3.3). 3.1 WCL dataset The WCL dataset (Navigli et al., 2010) contains 1,772 definitions and 2,847 non-definitions. Each instance is extracted from Wikipedia, and definitions follow a canonical structure following the genus et differentia model (i.e., ‘X is a Y which Z’). A preliminary (and shallow) analysis that c"
2020.cogalex-1.10,R15-1025,0,0.0213255,"and Monachesi, 2007) such as ‘refers to’ or ‘is a’. A notable contribution to DE is the Word Class Lattices model (Navigli and Velardi, 2010), which explores DE on the WCL dataset, a set of encyclopedic definitions and distractors, and which we use in this paper. In a subsequent contribution, Espinosa-Anke and Saggion (2014) present a supervised approach in which only syntactic features derived from dependency relations are used, and whose results are reported higher to the WCL method. For identifying definitions with higher linguistic variability, a weakly supervised approach is presented in Espinosa-Anke et al. (2015). And finally, models based on neural networks have been leveraged for exploiting both long and short-range dependencies, either combining CNNs and LSTMS (Espinosa-Anke and Schockaert, 2018) or BERT (Veyseh et al., 2019), and which are currently the highest performing models on WCL. 3 Data In this section we present the datasets utilized for our analysis, namely WCL (Section 3.1) and DEFT (Section 3.2), and provide a descriptive analysis comparing both datasets (Section 3.3). 3.1 WCL dataset The WCL dataset (Navigli et al., 2010) contains 1,772 definitions and 2,847 non-definitions. Each insta"
2020.cogalex-1.10,D13-1073,0,0.0162723,"us attention for its applications in Natural Language Processing, Computational Linguistics and Computational Lexicography (Espinosa-Anke and Saggion, 2014), as it has been proven to be applicable to glossary generation (Muresan and Klavans, 2002; Park et al., 2002), terminological databases (Nakamura and Nagao, 1988) or question answering systems (Saggion and Gaizauskas, 2004; Cui et al., 2005), among many others. Research on DE has seen contributions where the task is typically proposed as a binary classification problem (whether a sentence is a definition or not), although with exceptions (Jin et al., 2013). DE has also been studied in languages other than English, e.g., Slavic languages (Przepi´orkowski et al., 2007), Spanish (Sierra et al., 2008) or Portuguese (Del Gaudio et al., 2014). Many of these approaches use symbolic methods depending on manually crafted or semi-automatically learned lexico-syntactic patterns (Hovy et al., 2003; Westerhout and Monachesi, 2007) such as ‘refers to’ or ‘is a’. A notable contribution to DE is the Word Class Lattices model (Navigli and Velardi, 2010), which explores DE on the WCL dataset, a set of encyclopedic definitions and distractors, and which we use in"
2020.cogalex-1.10,N15-1169,0,0.0414329,"Missing"
2020.cogalex-1.10,muresan-klavans-2002-method,0,0.214537,"ble”). The syntactic patterns are simple and represent what we could refer to as canonical definitions. We will test the performance of a model trained on this dataset, and evaluate on the DEFT dataset, which contains a set of definitions and non-definitions from various topics such as biology, history and government. 2 Related Work Over the last years, DE has received notorious attention for its applications in Natural Language Processing, Computational Linguistics and Computational Lexicography (Espinosa-Anke and Saggion, 2014), as it has been proven to be applicable to glossary generation (Muresan and Klavans, 2002; Park et al., 2002), terminological databases (Nakamura and Nagao, 1988) or question answering systems (Saggion and Gaizauskas, 2004; Cui et al., 2005), among many others. Research on DE has seen contributions where the task is typically proposed as a binary classification problem (whether a sentence is a definition or not), although with exceptions (Jin et al., 2013). DE has also been studied in languages other than English, e.g., Slavic languages (Przepi´orkowski et al., 2007), Spanish (Sierra et al., 2008) or Portuguese (Del Gaudio et al., 2014). Many of these approaches use symbolic metho"
2020.cogalex-1.10,C88-2098,0,0.723149,"r to as canonical definitions. We will test the performance of a model trained on this dataset, and evaluate on the DEFT dataset, which contains a set of definitions and non-definitions from various topics such as biology, history and government. 2 Related Work Over the last years, DE has received notorious attention for its applications in Natural Language Processing, Computational Linguistics and Computational Lexicography (Espinosa-Anke and Saggion, 2014), as it has been proven to be applicable to glossary generation (Muresan and Klavans, 2002; Park et al., 2002), terminological databases (Nakamura and Nagao, 1988) or question answering systems (Saggion and Gaizauskas, 2004; Cui et al., 2005), among many others. Research on DE has seen contributions where the task is typically proposed as a binary classification problem (whether a sentence is a definition or not), although with exceptions (Jin et al., 2013). DE has also been studied in languages other than English, e.g., Slavic languages (Przepi´orkowski et al., 2007), Spanish (Sierra et al., 2008) or Portuguese (Del Gaudio et al., 2014). Many of these approaches use symbolic methods depending on manually crafted or semi-automatically learned lexico-syn"
2020.cogalex-1.10,P10-1134,0,0.220906,"r linguistic structures from canonical (the Aristotelian or genus et differentia model) can be leveraged to retrieve definitions from corpora in different domains of knowledge and textual genres alike. To this end, we develop a simple linear classifier and analyze the contribution of several (sets of) linguistic features. Finally, as a result of our experiments, we also shed light on the particularities of existing benchmarks as well as the most challenging aspects of the task. 1 Introduction Definition Extraction (DE) is the task to extract textual definitions from naturally occurring texts (Navigli and Velardi, 2010). The development of models able to identify definitions in freely occurring text has many applications such as the automatic generation of dictionaries, thesauri and glossaries, as well as e-learning materials and lexical taxonomies (Westerhout, 2009; Del Gaudio et al., 2014; Jurgens and Pilehvar, 2015; Espinosa-Anke et al., 2016). Moreover, definitional knowledge has proven to be a useful signal for improving language models in downstream NLP tasks (Joshi et al., 2020). The task of DE is currently approached almost unanimously as a supervised classification problem, and the latest methods ha"
2020.cogalex-1.10,navigli-etal-2010-annotated,0,0.0270644,"Missing"
2020.cogalex-1.10,C02-1142,0,0.108867,"ns are simple and represent what we could refer to as canonical definitions. We will test the performance of a model trained on this dataset, and evaluate on the DEFT dataset, which contains a set of definitions and non-definitions from various topics such as biology, history and government. 2 Related Work Over the last years, DE has received notorious attention for its applications in Natural Language Processing, Computational Linguistics and Computational Lexicography (Espinosa-Anke and Saggion, 2014), as it has been proven to be applicable to glossary generation (Muresan and Klavans, 2002; Park et al., 2002), terminological databases (Nakamura and Nagao, 1988) or question answering systems (Saggion and Gaizauskas, 2004; Cui et al., 2005), among many others. Research on DE has seen contributions where the task is typically proposed as a binary classification problem (whether a sentence is a definition or not), although with exceptions (Jin et al., 2013). DE has also been studied in languages other than English, e.g., Slavic languages (Przepi´orkowski et al., 2007), Spanish (Sierra et al., 2008) or Portuguese (Del Gaudio et al., 2014). Many of these approaches use symbolic methods depending on manu"
2020.cogalex-1.10,W19-4015,0,0.0466503,"le to learn a universal definition extraction system from canonical definitions, and to understand the core differences that currently exist in standard evaluation testbeds. In particular, we propose experiments where we develop a machine learning model able to distinguish definitions with high accuracy in a corpus of canonical definitions, and later evaluate such model in different (pertaining to different domains and genres) datasets. Our evaluation datasets are two, namely: the Word-Class Lattices (WCL) dataset from Navigli et al. (2010), and DEFT, from the SemEval 2020 Task 6 - Subtask 1 (Spala et al., 2019). The former provides an annotated set of definitions and non-definitions with syntactic patterns similar to those of definition sentences from Wikipedia (what the authors call syntactically plausible false definitions). The latter presents a robust English corpus that explores the less straightforward cases of term-definition structures in free and semistructured text from different domains (i.e., biology, history and government), and which is not limited to well-defined, structured, and narrow conditions. We include a detailed descriptive analysis of both corpora that identifies similarities"
2020.cogalex-1.10,2020.semeval-1.41,0,0.0237251,".9876 0.9732 0.9810 0.9796 0.9778 0.8686 0.5000 F1-Score 0.9797 0.9758 0.9816 0.9859 0.9710 0.9731 0.9888 0.9750 0.9819 0.9790 0.9792 0.8743 0.2768 Table 4: Results of the SVM model on the WCL dataset using 10-fold cross validation. Precision, recall and F1 are macro metrics. The last two rows include the average results of the two baselines considered. When testing the model on the DEFT corpus, the results are not close to being as satisfactory as they are in the WCL dataset, as we can see in Table 5. The model trained on the WCL dataset performs significantly worse than other recent models (Spala et al., 2020), which could be expected given the different nature of the definitions. In the following section we provide a more extensive analysis that also attempts at explaining the performance difference between the two datasets. Model SVM Naive Bayes Naive(all defs) Accuracy 0.7011 0.5909 0.3271 Precision 0.6573 0.5626 0.1635 Recall 0.5900 0.5689 0.5000 F1-Score 0.5872 0.5611 0.2465 Table 5: DEFT results of the SVM and baselines trained on the WCL corpus. 5 5.1 Analysis Feature analysis Figure 5 shows the features of the model with highest χ2 . Some of them are compositions extremely common in definit"
2020.cogalex-1.10,W09-4410,0,0.0713764,"Missing"
2020.coling-main.481,W19-1909,0,0.0389489,"Missing"
2020.coling-main.481,Q17-1010,0,0.293421,"fication. 2.2 Pre-trained word embeddings and language models Most state-of-the-art NLP models nowadays use unlabeled data in addition to labeled data to improve generalization (Goldberg, 2016). This comes in the form of word embeddings for fastText and a pretrained language model for BERT. Word embeddings. Word embeddings represent words in a vector space and are generally learned from shallow neural networks trained on text corpora, with Word2Vec (Mikolov et al., 2013) being one of the most popular and efficient approaches. A more recent model based on the Word2Vec architecture is fastText (Bojanowski et al., 2017), where words are additionally represented as the sum of character n-gram vectors. This allowed building vectors for rare words, misspelt words or concatenations of words. Language models. A limitation to the word embedding models described above is that they produce a single vector of a word despite the context in which it appears. In contrast, contextualized embeddings such as ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019) produce word representations that are dynamically informed by the words around them. The main drawback of these models, however, is that they are computationally"
2020.coling-main.481,N19-1213,0,0.023239,"ormance against most standard NLP benchmarks (Wang et al., 2019a; Wang et al., 2019b). However, these models require large computational resources that are not always available and have important environment implications (Strubell et al., 2019). Moreover, there is limited research in the applicability of pre-trained models in classification tasks with small amount of labelled data. Some related studies (Lee et al., 2020; Nguyen et al., 2020; Huang et al., 2019; Alsentzer et al., 2019) investigate whether it is helpful to tailor a pre-trained model to the domain while others (Sun et al., 2019; Chronopoulou et al., 2019; Radford et al., 2018) analyse methods for fine-tuning BERT to a given task. However, these studies perform evaluation on a limited range of datasets and classification models and do not consider scenarios with limited amounts of training data. In particular, this paper aims to estimate the role of labeled and unlabeled data for supervised text classification. Our study is similar to Gururangan et al. (2020) where they investigate whether it is still helpful to tailor a pre-trained model to the domain of a target task. In this paper, however, we focus our evaluation on text classification and"
2020.coling-main.481,N19-1423,0,0.605555,", United Kingdom {edwardsai,camachocolladosj,deribaupierreh,preecead}@cardiff.ac.uk Abstract Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including text classification. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on word embeddings, i.e., fastText (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data. 1 Introduction Language models pre-trained on large amounts of text corpora form the foundati"
2020.coling-main.481,2020.acl-main.740,0,0.035913,"Missing"
2020.coling-main.481,E17-2068,0,0.690821,"ited Kingdom ‡Crime and Security Research Institute, Cardiff University, United Kingdom {edwardsai,camachocolladosj,deribaupierreh,preecead}@cardiff.ac.uk Abstract Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including text classification. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on word embeddings, i.e., fastText (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data. 1 Introduction Languag"
2020.coling-main.481,P11-1015,0,0.0690603,"produce word representations that are dynamically informed by the words around them. The main drawback of these models, however, is that they are computationally very demanding, as they are generally based on large transformer-based language models (Strubell et al., 2019). 3 Experimental Setting Datasets. For our experiments we selected a suite of datasets with different domains and nature. These are: SemEval 2016 task on sentiment analysis (Nakov et al., 2019), SemEval 2018 task on emoji predic5523 tion (Barbieri et al., 2018), AG News (Zhang et al., 2015), Newsgroups (Lang, 1995) and IMDB (Maas et al., 2011). The main features and statistics of each dataset are summarized in Table 1.1 Dataset SemEval-16 (SA) SemEval-18 (EP) AG News 20 Newsgroups IMDB Task Sentiment analysis Emoji prediction Topic categorization Topic categorization Polarity detection Domain Twitter Twitter News Newsgroups Movie reviews Type Sentence Sentence Sentence Document Document Avg tokens 20 12 31 285 231 Labels 3 20 4 20 2 # Train 5,937 500,000 114,828 11,231 28,000 # Dev 1,386 1,000 624 748 2,560 # Test 20,806 49,998 5,612 6,728 23,041 Table 1: Overview of the classification datasets used in our experiments. Comparison m"
2020.coling-main.481,S18-1001,0,0.0280379,"owing we describe standard linear methods and explain recent techniques based on neural models that we compare in our quantitative evaluation. 2.1 Supervised machine learning models Linear models. Linear models such as SVMs or logistic regression coupled with frequency-based handcrafted features have been traditionally used for text classification. Despite their simplicity, they are considered a strong baseline for many text classification tasks (Joachims, 1998; McCallum et al., 1998; Fan et al., 2008), even more recently on noisy corpora such as social media text (C¸o¨ ltekin and Rama, 2018; Mohammad et al., 2018). In general, however, these methods tend to struggle with OOV (OutOf-Vocabulary) words, fine-grained distinctions and unbalanced datasets. FastText (Joulin et al., 2017), which is the model evaluated in this paper, partially addresses these issues by integrating a linear model with a rank constraint, allowing sharing parameters among features and classes, and integrates word embeddings that are then averaged into a text representation. Neural models. Neural models can learn non-linear and complex relationships which makes them a preferable method for many NLP tasks such as sentiment analysis"
2020.coling-main.481,2020.emnlp-demos.2,0,0.0171397,"e-trained on large amounts of text corpora form the foundation of today’s NLP (Gururangan et al., 2020; Rogers et al., 2020). They have proved to provide state-of-the-art performance against most standard NLP benchmarks (Wang et al., 2019a; Wang et al., 2019b). However, these models require large computational resources that are not always available and have important environment implications (Strubell et al., 2019). Moreover, there is limited research in the applicability of pre-trained models in classification tasks with small amount of labelled data. Some related studies (Lee et al., 2020; Nguyen et al., 2020; Huang et al., 2019; Alsentzer et al., 2019) investigate whether it is helpful to tailor a pre-trained model to the domain while others (Sun et al., 2019; Chronopoulou et al., 2019; Radford et al., 2018) analyse methods for fine-tuning BERT to a given task. However, these studies perform evaluation on a limited range of datasets and classification models and do not consider scenarios with limited amounts of training data. In particular, this paper aims to estimate the role of labeled and unlabeled data for supervised text classification. Our study is similar to Gururangan et al. (2020) where"
2020.coling-main.481,N18-1202,0,0.0534565,"s trained on text corpora, with Word2Vec (Mikolov et al., 2013) being one of the most popular and efficient approaches. A more recent model based on the Word2Vec architecture is fastText (Bojanowski et al., 2017), where words are additionally represented as the sum of character n-gram vectors. This allowed building vectors for rare words, misspelt words or concatenations of words. Language models. A limitation to the word embedding models described above is that they produce a single vector of a word despite the context in which it appears. In contrast, contextualized embeddings such as ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019) produce word representations that are dynamically informed by the words around them. The main drawback of these models, however, is that they are computationally very demanding, as they are generally based on large transformer-based language models (Strubell et al., 2019). 3 Experimental Setting Datasets. For our experiments we selected a suite of datasets with different domains and nature. These are: SemEval 2016 task on sentiment analysis (Nakov et al., 2019), SemEval 2018 task on emoji predic5523 tion (Barbieri et al., 2018), AG News (Zhang et al., 2015), News"
2020.coling-main.481,P17-1170,1,0.837645,"Text (Joulin et al., 2017), which is the model evaluated in this paper, partially addresses these issues by integrating a linear model with a rank constraint, allowing sharing parameters among features and classes, and integrates word embeddings that are then averaged into a text representation. Neural models. Neural models can learn non-linear and complex relationships which makes them a preferable method for many NLP tasks such as sentiment analysis or question answering (Sun et al., 2019). In particular, LSTMs, sometimes in combination with CNNs for text classification (Xiao and Cho, 2016; Pilehvar et al., 2017), enable capturing long-range dependencies in a sequential manner where data is read from only one direction (referred to as the ‘unidirectionality constraint’). Recent state-of-the-art language models, such as BERT (Devlin et al., 2019), overcome the unidirectionality constraint by using transformer-based masked language models to learn pre-trained deep bidirectional representations. These pre-trained models leverage generic knowledge on large unlabeled corpora that can then be fine-tuned on the specific task by using the pre-trained parameters. BERT, which is the pretrained language model te"
2020.coling-main.481,2020.tacl-1.54,0,0.0229032,"fication tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data. 1 Introduction Language models pre-trained on large amounts of text corpora form the foundation of today’s NLP (Gururangan et al., 2020; Rogers et al., 2020). They have proved to provide state-of-the-art performance against most standard NLP benchmarks (Wang et al., 2019a; Wang et al., 2019b). However, these models require large computational resources that are not always available and have important environment implications (Strubell et al., 2019). Moreover, there is limited research in the applicability of pre-trained models in classification tasks with small amount of labelled data. Some related studies (Lee et al., 2020; Nguyen et al., 2020; Huang et al., 2019; Alsentzer et al., 2019) investigate whether it is helpful to tailor a pre-trained m"
2020.coling-main.481,P19-1355,0,0.0561595,"sets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data. 1 Introduction Language models pre-trained on large amounts of text corpora form the foundation of today’s NLP (Gururangan et al., 2020; Rogers et al., 2020). They have proved to provide state-of-the-art performance against most standard NLP benchmarks (Wang et al., 2019a; Wang et al., 2019b). However, these models require large computational resources that are not always available and have important environment implications (Strubell et al., 2019). Moreover, there is limited research in the applicability of pre-trained models in classification tasks with small amount of labelled data. Some related studies (Lee et al., 2020; Nguyen et al., 2020; Huang et al., 2019; Alsentzer et al., 2019) investigate whether it is helpful to tailor a pre-trained model to the domain while others (Sun et al., 2019; Chronopoulou et al., 2019; Radford et al., 2018) analyse methods for fine-tuning BERT to a given task. However, these studies perform evaluation on a limited range of datasets and classification models and do not consider scenarios with limited"
2020.coling-main.481,P16-1128,0,0.0651443,"Missing"
2020.coling-tutorials.2,Q17-1010,0,0.0473549,"hniques in NLP, in the broad sense. We will start by conventional word embeddings (e.g., Word2Vec and GloVe) and then move to other types of embeddings, such as sense-specific and graph alternatives. We will finalize with an overview of the trending contextualized representations (e.g., ELMo and BERT) and explain their potential and impact in NLP. 1 Description In this tutorial we will start by providing a historical overview on word-level vector space models, and word embeddings in particular. Word embeddings (e.g. Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017)) have proven to be powerful keepers of prior knowledge to be integrated into downstream Natural Language Processing (NLP) applications. However, despite their flexibility and success in capturing semantic properties of words, the effectiveness of word embeddings are generally hampered by an important limitation, known as the meaning conflation deficiency: the inability to discriminate among different meanings of a word. A word can have one meaning (monosemous) or multiple meanings (ambiguous). For instance, the noun mouse can refer to two different meanings depending on the context: an animal"
2020.coling-tutorials.2,P12-1092,0,0.073822,"ated words rat and screen are pulled towards each other in the semantic space for their similarities to two different senses of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches have attempted to model individual word senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Rothe and Sch¨utze, 2015; Li and Jurafsky, 2015; Pilehvar and Collier, 2016; Mancini et al., 2017). Sense representation techniques, however, suffer from limitations which hinders their effective application in downstream NLP tasks: they either need vast amounts of training data to obtain reliable representations or require an additional sense disambiguation on the input text to make them integrable into NLP systems. This data is highly expensive to obtain in practice, which causes the so-called knowledge-acquisition bottleneck (Gale et al., 1992). As a practical wa"
2020.coling-tutorials.2,D15-1200,0,0.0243516,"space for their similarities to two different senses of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches have attempted to model individual word senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Rothe and Sch¨utze, 2015; Li and Jurafsky, 2015; Pilehvar and Collier, 2016; Mancini et al., 2017). Sense representation techniques, however, suffer from limitations which hinders their effective application in downstream NLP tasks: they either need vast amounts of training data to obtain reliable representations or require an additional sense disambiguation on the input text to make them integrable into NLP systems. This data is highly expensive to obtain in practice, which causes the so-called knowledge-acquisition bottleneck (Gale et al., 1992). As a practical way to deal with the knowledge-acquisition bottleneck, an emerging branch of"
2020.coling-tutorials.2,K17-1012,1,0.841481,"s of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches have attempted to model individual word senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Rothe and Sch¨utze, 2015; Li and Jurafsky, 2015; Pilehvar and Collier, 2016; Mancini et al., 2017). Sense representation techniques, however, suffer from limitations which hinders their effective application in downstream NLP tasks: they either need vast amounts of training data to obtain reliable representations or require an additional sense disambiguation on the input text to make them integrable into NLP systems. This data is highly expensive to obtain in practice, which causes the so-called knowledge-acquisition bottleneck (Gale et al., 1992). As a practical way to deal with the knowledge-acquisition bottleneck, an emerging branch of research has focused on directly integrating unsupe"
2020.coling-tutorials.2,K16-1006,0,0.130901,"n a 2D semantic space around the ambiguous word mouse. Having the word, with its different meanings, represented as a single point (vector) results in pulling together of semantically unrelated words, such as computer and rabbit. Figure 2: A general illustration of contextualized word embeddings and how they are integrated in NLP models. A language modelling component is responsible for analyzing the context of the target word (cell in the figure) and generating its dynamic embedding. i.e., their representations dynamically change depending on the context in which a word appears. Context2vec (Melamud et al., 2016) and ELMo (Peters et al., 2018a) are some of the early examples for this type of representation. These models represent the context of a target word by extracting the embedding of a word in context from a bi-directional LSTM language model. The latter further proposed a seamless integration into neural NLP systems, as depicted in Figure 2. More recently, Transformers (Vaswani et al., 2017) have proven very effective for encoding contextualized knowledge, thanks to their self-attention mechanism (Figure 3). BERT (Devlin et al., 2018), which is based on Transformers, has revolutionized the field"
2020.coling-tutorials.2,D14-1113,0,0.026328,"A word can have one meaning (monosemous) or multiple meanings (ambiguous). For instance, the noun mouse can refer to two different meanings depending on the context: an animal or a computer device. Hence, mouse is said to be ambiguous. In fact, according to the Principle of Economical Versatility of Words (Zipf, 1949), frequent words tend to have more senses. Moreover, this meaning conflation can have additional negative impacts on accurate semantic modeling, e.g., semantically unrelated words that are similar to different senses of a word are pulled towards each other in the semantic space (Neelakantan et al., 2014; Pilehvar and Collier, 2016). In our example, the two semantically-unrelated words rat and screen are pulled towards each other in the semantic space for their similarities to two different senses of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches hav"
2020.coling-tutorials.2,D14-1162,0,0.096322,"el synthesis of the main embedding techniques in NLP, in the broad sense. We will start by conventional word embeddings (e.g., Word2Vec and GloVe) and then move to other types of embeddings, such as sense-specific and graph alternatives. We will finalize with an overview of the trending contextualized representations (e.g., ELMo and BERT) and explain their potential and impact in NLP. 1 Description In this tutorial we will start by providing a historical overview on word-level vector space models, and word embeddings in particular. Word embeddings (e.g. Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017)) have proven to be powerful keepers of prior knowledge to be integrated into downstream Natural Language Processing (NLP) applications. However, despite their flexibility and success in capturing semantic properties of words, the effectiveness of word embeddings are generally hampered by an important limitation, known as the meaning conflation deficiency: the inability to discriminate among different meanings of a word. A word can have one meaning (monosemous) or multiple meanings (ambiguous). For instance, the noun mouse can refer to two different meanin"
2020.coling-tutorials.2,N18-1202,0,0.249928,"ambiguous word mouse. Having the word, with its different meanings, represented as a single point (vector) results in pulling together of semantically unrelated words, such as computer and rabbit. Figure 2: A general illustration of contextualized word embeddings and how they are integrated in NLP models. A language modelling component is responsible for analyzing the context of the target word (cell in the figure) and generating its dynamic embedding. i.e., their representations dynamically change depending on the context in which a word appears. Context2vec (Melamud et al., 2016) and ELMo (Peters et al., 2018a) are some of the early examples for this type of representation. These models represent the context of a target word by extracting the embedding of a word in context from a bi-directional LSTM language model. The latter further proposed a seamless integration into neural NLP systems, as depicted in Figure 2. More recently, Transformers (Vaswani et al., 2017) have proven very effective for encoding contextualized knowledge, thanks to their self-attention mechanism (Figure 3). BERT (Devlin et al., 2018), which is based on Transformers, has revolutionized the field of representation learning an"
2020.coling-tutorials.2,D18-1179,0,0.122379,"ambiguous word mouse. Having the word, with its different meanings, represented as a single point (vector) results in pulling together of semantically unrelated words, such as computer and rabbit. Figure 2: A general illustration of contextualized word embeddings and how they are integrated in NLP models. A language modelling component is responsible for analyzing the context of the target word (cell in the figure) and generating its dynamic embedding. i.e., their representations dynamically change depending on the context in which a word appears. Context2vec (Melamud et al., 2016) and ELMo (Peters et al., 2018a) are some of the early examples for this type of representation. These models represent the context of a target word by extracting the embedding of a word in context from a bi-directional LSTM language model. The latter further proposed a seamless integration into neural NLP systems, as depicted in Figure 2. More recently, Transformers (Vaswani et al., 2017) have proven very effective for encoding contextualized knowledge, thanks to their self-attention mechanism (Figure 3). BERT (Devlin et al., 2018), which is based on Transformers, has revolutionized the field of representation learning an"
2020.coling-tutorials.2,D16-1174,1,0.84262,"ng (monosemous) or multiple meanings (ambiguous). For instance, the noun mouse can refer to two different meanings depending on the context: an animal or a computer device. Hence, mouse is said to be ambiguous. In fact, according to the Principle of Economical Versatility of Words (Zipf, 1949), frequent words tend to have more senses. Moreover, this meaning conflation can have additional negative impacts on accurate semantic modeling, e.g., semantically unrelated words that are similar to different senses of a word are pulled towards each other in the semantic space (Neelakantan et al., 2014; Pilehvar and Collier, 2016). In our example, the two semantically-unrelated words rat and screen are pulled towards each other in the semantic space for their similarities to two different senses of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches have attempted to model individu"
2020.coling-tutorials.2,N10-1013,0,0.0548651,", the two semantically-unrelated words rat and screen are pulled towards each other in the semantic space for their similarities to two different senses of mouse (see Figure 1). This, in turn, contributes to the violation of the triangle inequality in euclidean spaces (Tversky and Gati, 1982; Neelakantan et al., 2014). Accurately capturing the meaning of words (both ambiguous and unambiguous) plays a crucial role in the language understanding of NLP systems. In order to deal with the meaning conflation deficiency, this tutorial covers approaches have attempted to model individual word senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Rothe and Sch¨utze, 2015; Li and Jurafsky, 2015; Pilehvar and Collier, 2016; Mancini et al., 2017). Sense representation techniques, however, suffer from limitations which hinders their effective application in downstream NLP tasks: they either need vast amounts of training data to obtain reliable representations or require an additional sense disambiguation on the input text to make them integrable into NLP systems. This data is highly expensive to obtain in practice, which causes the so-called knowledge-acquisition bottleneck (Gale et al., 1992"
2020.coling-tutorials.2,P15-1173,0,0.041538,"Missing"
2020.conll-1.9,Q16-1028,0,0.618621,"es, and provide a generalization boost to many NLP applications (Goldberg, 2017). Surprisingly, these representations have also been shown to exhibit linear relationships between words in the vector space, demonstrated by analogy. For example, Mikolov et al. (2013b) showed that a simple operation such as king-man+woman will result in a point near queen in the vector space1 . These word analogies have been extensively investigated in the literature, aiming to shed light on this surprising property. However, while there has been a body of research seeking to understand how these analogies work (Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019), and noting issues about their methodology (Linzen, 2016; Gladkova et al., 2016; Nissim et al., 2020), there has not been a specific analysis on the source of statistical cues that leads to their high performance on this task. Concurrently, a thread of research has focused on explicitly modeling lexical relationships of word pairs in text corpora (Jameel et al., 2018; EspinosaAnke and Schockaert, 2018; Washio and Kato, 2018; Joshi et al., 2019; Camacho-Collados et al., 2019). While these methods employ different means"
2020.conll-1.9,Q17-1010,0,0.746566,"rocessing (NLP). The main underlying principle is known for decades, as explained by Firth (1957). This principle was based on the idea that the meaning of a word can be understood by its surrounding company (i.e., the words in its context). Most modern representation learning theory in NLP is based on this assumption, with vector representation being the most successful area to date (Turney and Pantel, 2010). More recently, low-dimensional word representations learned from text corpora using neural networks (i.e., word embeddings) have emerged (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) stemming from cognitive frameworks based on distributed representation (Hinton et al., 1986; Feldman and Ballard, 1982). Neural word embeddings have been proven to contain useful information about concepts and entities, and provide a generalization boost to many NLP applications (Goldberg, 2017). Surprisingly, these representations have also been shown to exhibit linear relationships between words in the vector space, demonstrated by analogy. For example, Mikolov et al. (2013b) showed that a simple operation such as king-man+woman will result in a point near queen in the vector space1 . These"
2020.conll-1.9,C18-1138,0,0.287846,"milarity problems that can be solved through more appropriate operations. Linzen (2016) showed that simple baselines based on nearest neighbour searches are competitive in the analogy categories proposed by Mikolov et al. (2013b). Because of this, Gladkova et al. (2016) proposed a new dataset, partially addressing some of the previous shortcomings. Other works have shown that linear relationships, while being implicit, are not directly apparent in the word embedding space, and therefore word analogies may not be the best method to retrieve this information (Drozd et al., 2016; Schluter, 2018; Bouraoui et al., 2018). Finally, Gonen and Goldberg (2019) and Nissim et al. (2020) cautioned against over-reliance on analogies as a means to uncover and correct for biases in word embeddings. These methodological observations challenge the supremacy of analogy evaluations as the optimal proxy for downstream task performance of a word embedding. Nevertheless, they represent a valuable mechanism with which to compare the semantic regularities of two different neural embeddings. In particular, word analogies represent an ideal benchmark for our research questions, as the impact of co-occurrence statistics within wor"
2020.conll-1.9,C16-1332,0,0.475641,"hey are reduced to three separate similarity problems that can be solved through more appropriate operations. Linzen (2016) showed that simple baselines based on nearest neighbour searches are competitive in the analogy categories proposed by Mikolov et al. (2013b). Because of this, Gladkova et al. (2016) proposed a new dataset, partially addressing some of the previous shortcomings. Other works have shown that linear relationships, while being implicit, are not directly apparent in the word embedding space, and therefore word analogies may not be the best method to retrieve this information (Drozd et al., 2016; Schluter, 2018; Bouraoui et al., 2018). Finally, Gonen and Goldberg (2019) and Nissim et al. (2020) cautioned against over-reliance on analogies as a means to uncover and correct for biases in word embeddings. These methodological observations challenge the supremacy of analogy evaluations as the optimal proxy for downstream task performance of a word embedding. Nevertheless, they represent a valuable mechanism with which to compare the semantic regularities of two different neural embeddings. In particular, word analogies represent an ideal benchmark for our research questions, as the impac"
2020.conll-1.9,C18-1225,0,0.0404122,"Missing"
2020.conll-1.9,P19-1315,0,0.476488,"many NLP applications (Goldberg, 2017). Surprisingly, these representations have also been shown to exhibit linear relationships between words in the vector space, demonstrated by analogy. For example, Mikolov et al. (2013b) showed that a simple operation such as king-man+woman will result in a point near queen in the vector space1 . These word analogies have been extensively investigated in the literature, aiming to shed light on this surprising property. However, while there has been a body of research seeking to understand how these analogies work (Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019), and noting issues about their methodology (Linzen, 2016; Gladkova et al., 2016; Nissim et al., 2020), there has not been a specific analysis on the source of statistical cues that leads to their high performance on this task. Concurrently, a thread of research has focused on explicitly modeling lexical relationships of word pairs in text corpora (Jameel et al., 2018; EspinosaAnke and Schockaert, 2018; Washio and Kato, 2018; Joshi et al., 2019; Camacho-Collados et al., 2019). While these methods employ different means for learning relation vectors, they share a co"
2020.conll-1.9,P17-1007,0,0.0582129,"neralization boost to many NLP applications (Goldberg, 2017). Surprisingly, these representations have also been shown to exhibit linear relationships between words in the vector space, demonstrated by analogy. For example, Mikolov et al. (2013b) showed that a simple operation such as king-man+woman will result in a point near queen in the vector space1 . These word analogies have been extensively investigated in the literature, aiming to shed light on this surprising property. However, while there has been a body of research seeking to understand how these analogies work (Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019), and noting issues about their methodology (Linzen, 2016; Gladkova et al., 2016; Nissim et al., 2020), there has not been a specific analysis on the source of statistical cues that leads to their high performance on this task. Concurrently, a thread of research has focused on explicitly modeling lexical relationships of word pairs in text corpora (Jameel et al., 2018; EspinosaAnke and Schockaert, 2018; Washio and Kato, 2018; Joshi et al., 2019; Camacho-Collados et al., 2019). While these methods employ different means for learning relation"
2020.conll-1.9,N16-2002,0,0.279761,"ear relationships between words in the vector space, demonstrated by analogy. For example, Mikolov et al. (2013b) showed that a simple operation such as king-man+woman will result in a point near queen in the vector space1 . These word analogies have been extensively investigated in the literature, aiming to shed light on this surprising property. However, while there has been a body of research seeking to understand how these analogies work (Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019), and noting issues about their methodology (Linzen, 2016; Gladkova et al., 2016; Nissim et al., 2020), there has not been a specific analysis on the source of statistical cues that leads to their high performance on this task. Concurrently, a thread of research has focused on explicitly modeling lexical relationships of word pairs in text corpora (Jameel et al., 2018; EspinosaAnke and Schockaert, 2018; Washio and Kato, 2018; Joshi et al., 2019; Camacho-Collados et al., 2019). While these methods employ different means for learning relation vectors, they share a common initial premise: only co-occurring words in the corpus are considered2 . While this simplified assumptio"
2020.conll-1.9,W19-3621,0,0.0194048,"ved through more appropriate operations. Linzen (2016) showed that simple baselines based on nearest neighbour searches are competitive in the analogy categories proposed by Mikolov et al. (2013b). Because of this, Gladkova et al. (2016) proposed a new dataset, partially addressing some of the previous shortcomings. Other works have shown that linear relationships, while being implicit, are not directly apparent in the word embedding space, and therefore word analogies may not be the best method to retrieve this information (Drozd et al., 2016; Schluter, 2018; Bouraoui et al., 2018). Finally, Gonen and Goldberg (2019) and Nissim et al. (2020) cautioned against over-reliance on analogies as a means to uncover and correct for biases in word embeddings. These methodological observations challenge the supremacy of analogy evaluations as the optimal proxy for downstream task performance of a word embedding. Nevertheless, they represent a valuable mechanism with which to compare the semantic regularities of two different neural embeddings. In particular, word analogies represent an ideal benchmark for our research questions, as the impact of co-occurrence statistics within word relations can be evaluated directl"
2020.conll-1.9,S13-1005,0,0.0887114,"Missing"
2020.conll-1.9,2020.coling-main.112,1,0.789926,"y leverage the robustness of regularities, or features, learned about individual words to lay the structural foundation for inferences to then be made about a lexical relation. Removing co-occurrences of capitals and countries, for example, would not completely remove the concept of capitals and countries from the corpus. The embedding of ”Madrid” would likely still encode features associated with a busy city, government buildings, culture, and European regionality. This is also related to work that showed relations and relevant information from relations can be captured from word embeddings (Jadhav et al., 2020), even if the relation cannot be retrieved explicitly from linear transformations (Drozd et al., 2016; Bouraoui et al., 2018). Interestingly, however, our results indicate that the frequency of an individual word in a corpus is only weakly related to the robustness of features leveraged for successful analogy completion. Finally, even though co-occurrences of pairs from a specific relation are not necessary to learn the necessary features, word pairs still may play a critical role in regularity development. Most 126 word embedding models (including the Skip-Gram model of Word2Vec) are trained"
2020.conll-1.9,P18-1003,0,0.0788559,"rature, aiming to shed light on this surprising property. However, while there has been a body of research seeking to understand how these analogies work (Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019), and noting issues about their methodology (Linzen, 2016; Gladkova et al., 2016; Nissim et al., 2020), there has not been a specific analysis on the source of statistical cues that leads to their high performance on this task. Concurrently, a thread of research has focused on explicitly modeling lexical relationships of word pairs in text corpora (Jameel et al., 2018; EspinosaAnke and Schockaert, 2018; Washio and Kato, 2018; Joshi et al., 2019; Camacho-Collados et al., 2019). While these methods employ different means for learning relation vectors, they share a common initial premise: only co-occurring words in the corpus are considered2 . While this simplified assumption works well enough in practice, providing a useful signal even in downstream NLP applications, in this paper we find that valuable information is likely lost in the process. In fact, we 1 More information of how word analogies work can be found in Section 2.1. 2 Some of these methods also"
2020.conll-1.9,2020.tacl-1.28,0,0.0202393,", 2017) and a possible component of human creativity (Holyoak et al., 1996). 7 Moreover, the analysis could be extended to other types of relations, not only semantic. Further investigation could then focus on how the main sources of concepts and linguistic regularities in word embeddings are learned, and how they can be leveraged to improve unsupervised relation models, e.g., (Jameel et al., 2018; Joshi et al., 2019). Finally, as a follow-up to recent work aiming at understanding how language models and contextualized embeddings capture relations (Petroni et al., 2019; Bouraoui et al., 2020; Jiang et al., 2020), further research could be devoted to analyze the performance of such models with and without pairwise co-occurrence information. Conclusion and Future Work In this paper we have presented a large-scale analysis on the role of co-occurring relational word pairs in completing analogies. In the analyses we have measured to what extent the loss of co-occurrence information within relation types affects analogy completion using neural word embeddings. Perhaps surprisingly, this effect is quite small, to the point that word embeddings can complete analogies of a relationship in the vector space ev"
2020.conll-1.9,N19-1362,0,0.0743486,"has been a body of research seeking to understand how these analogies work (Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019), and noting issues about their methodology (Linzen, 2016; Gladkova et al., 2016; Nissim et al., 2020), there has not been a specific analysis on the source of statistical cues that leads to their high performance on this task. Concurrently, a thread of research has focused on explicitly modeling lexical relationships of word pairs in text corpora (Jameel et al., 2018; EspinosaAnke and Schockaert, 2018; Washio and Kato, 2018; Joshi et al., 2019; Camacho-Collados et al., 2019). While these methods employ different means for learning relation vectors, they share a common initial premise: only co-occurring words in the corpus are considered2 . While this simplified assumption works well enough in practice, providing a useful signal even in downstream NLP applications, in this paper we find that valuable information is likely lost in the process. In fact, we 1 More information of how word analogies work can be found in Section 2.1. 2 Some of these methods also provide tools to learn representations for out-of-vocabulary pairs (Joshi et"
2020.conll-1.9,W14-1618,0,0.0382036,"orous theoretical explanation on the linear algebraic structure of word embeddings. Their formalism is based on a latent variable model that makes assumptions on the nature of the vector space. Later works rely on the notion of paraphrasing (Gittens et al., 2017; Allen and Hospedales, 2019), based on the observation that different words can be used in similar contexts interchangeably, dropping some of the previous assumptions made by Arora et al. (2016). Concurrently, other works have attempted to provide explanations of the compositional properties of distributional models through additions (Levy and Goldberg, 2014a; Paperno and Baroni, 2016; Ethayarajh et al., 2019), which lie at the core of word analogy completion. While these works formalize word analogies and attempt to explain how they work mathematically, our empirical analysis is focused on understanding the source of signal in corpora that affect the performance of word analogy completion, without asserting any predefined assumption. In particular, we are mostly interested in determining whether relationship pair co-occurrence in sentences is necessary in order for a word embedding to succeed at analogy completion. 2.2 Issues in word analogies A"
2020.conll-1.9,Q15-1016,0,0.109363,"Missing"
2020.conll-1.9,W16-2503,0,0.586882,"to exhibit linear relationships between words in the vector space, demonstrated by analogy. For example, Mikolov et al. (2013b) showed that a simple operation such as king-man+woman will result in a point near queen in the vector space1 . These word analogies have been extensively investigated in the literature, aiming to shed light on this surprising property. However, while there has been a body of research seeking to understand how these analogies work (Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019), and noting issues about their methodology (Linzen, 2016; Gladkova et al., 2016; Nissim et al., 2020), there has not been a specific analysis on the source of statistical cues that leads to their high performance on this task. Concurrently, a thread of research has focused on explicitly modeling lexical relationships of word pairs in text corpora (Jameel et al., 2018; EspinosaAnke and Schockaert, 2018; Washio and Kato, 2018; Joshi et al., 2019; Camacho-Collados et al., 2019). While these methods employ different means for learning relation vectors, they share a common initial premise: only co-occurring words in the corpus are considered2 . While th"
2020.conll-1.9,N13-1090,0,0.499482,"s been a longstanding task in natural language processing (NLP). The main underlying principle is known for decades, as explained by Firth (1957). This principle was based on the idea that the meaning of a word can be understood by its surrounding company (i.e., the words in its context). Most modern representation learning theory in NLP is based on this assumption, with vector representation being the most successful area to date (Turney and Pantel, 2010). More recently, low-dimensional word representations learned from text corpora using neural networks (i.e., word embeddings) have emerged (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) stemming from cognitive frameworks based on distributed representation (Hinton et al., 1986; Feldman and Ballard, 1982). Neural word embeddings have been proven to contain useful information about concepts and entities, and provide a generalization boost to many NLP applications (Goldberg, 2017). Surprisingly, these representations have also been shown to exhibit linear relationships between words in the vector space, demonstrated by analogy. For example, Mikolov et al. (2013b) showed that a simple operation such as king-man+woman will resul"
2020.conll-1.9,D14-1162,0,0.111597,"ask in natural language processing (NLP). The main underlying principle is known for decades, as explained by Firth (1957). This principle was based on the idea that the meaning of a word can be understood by its surrounding company (i.e., the words in its context). Most modern representation learning theory in NLP is based on this assumption, with vector representation being the most successful area to date (Turney and Pantel, 2010). More recently, low-dimensional word representations learned from text corpora using neural networks (i.e., word embeddings) have emerged (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017) stemming from cognitive frameworks based on distributed representation (Hinton et al., 1986; Feldman and Ballard, 1982). Neural word embeddings have been proven to contain useful information about concepts and entities, and provide a generalization boost to many NLP applications (Goldberg, 2017). Surprisingly, these representations have also been shown to exhibit linear relationships between words in the vector space, demonstrated by analogy. For example, Mikolov et al. (2013b) showed that a simple operation such as king-man+woman will result in a point near queen in"
2020.conll-1.9,D19-1250,0,0.0952117,"Missing"
2020.conll-1.9,N18-2039,0,0.312809,"hree separate similarity problems that can be solved through more appropriate operations. Linzen (2016) showed that simple baselines based on nearest neighbour searches are competitive in the analogy categories proposed by Mikolov et al. (2013b). Because of this, Gladkova et al. (2016) proposed a new dataset, partially addressing some of the previous shortcomings. Other works have shown that linear relationships, while being implicit, are not directly apparent in the word embedding space, and therefore word analogies may not be the best method to retrieve this information (Drozd et al., 2016; Schluter, 2018; Bouraoui et al., 2018). Finally, Gonen and Goldberg (2019) and Nissim et al. (2020) cautioned against over-reliance on analogies as a means to uncover and correct for biases in word embeddings. These methodological observations challenge the supremacy of analogy evaluations as the optimal proxy for downstream task performance of a word embedding. Nevertheless, they represent a valuable mechanism with which to compare the semantic regularities of two different neural embeddings. In particular, word analogies represent an ideal benchmark for our research questions, as the impact of co-occurren"
2020.conll-1.9,N18-1102,0,0.0243922,". However, while there has been a body of research seeking to understand how these analogies work (Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019), and noting issues about their methodology (Linzen, 2016; Gladkova et al., 2016; Nissim et al., 2020), there has not been a specific analysis on the source of statistical cues that leads to their high performance on this task. Concurrently, a thread of research has focused on explicitly modeling lexical relationships of word pairs in text corpora (Jameel et al., 2018; EspinosaAnke and Schockaert, 2018; Washio and Kato, 2018; Joshi et al., 2019; Camacho-Collados et al., 2019). While these methods employ different means for learning relation vectors, they share a common initial premise: only co-occurring words in the corpus are considered2 . While this simplified assumption works well enough in practice, providing a useful signal even in downstream NLP applications, in this paper we find that valuable information is likely lost in the process. In fact, we 1 More information of how word analogies work can be found in Section 2.1. 2 Some of these methods also provide tools to learn representations for out-of-vocabul"
2020.emnlp-main.283,W04-3204,0,0.122764,"ous approaches relied on parallel corpora for two or more languages. The OMSTI corpus (Taghipour and Ng, 2015) was constructed by exploiting the alignments of an English-Chinese corpus. Similarly, Delli Bovi et al. (2017) presented EuroSense, a multilingual sense-annotated corpus using the Europarl parallel corpus for 21 languages as reference. In contrast to these approaches, we focus on unambiguous senses and, therefore, are not constrained to only nouns, knowledge from Wikipedia, or a specific type of corpus. Earlier works exploiting unambiguous words (Leacock et al., 1998; Mihalcea, 2002; Agirre and Martinez, 2004) and especially the subsequent extension by Martinez et al. (2008) are the most directly related to our paper. Martinez et al. (2008) retrieved example sentences with monosemous nouns from web search snippets and used them towards improved performance on WSD by leveraging WordNet relations. However, the WSD methods analyzed were sensitive to frequency bias, leading their collection effort to collect a large number of examples for fewer senses (and only nouns). In contrast, our solution is designed for all monosemous words, retrieving examples from web texts instead of snippets, attaining perfo"
2020.emnlp-main.283,P17-2094,1,0.863124,"owledge from Wikipedia, such as NASARI vectors (Camacho-Collados et al., 2016), for providing sense annotations for concepts and entities (Scarlini et al., 2019; Pasini and Navigli, 2019). In the case of Scarlini et al. (2019), and similarly to Raganato et al. (2016), their method requires hyperlinks and category information from Wikipedia, hence not extensible to other kinds of corpora.1 Previous approaches relied on parallel corpora for two or more languages. The OMSTI corpus (Taghipour and Ng, 2015) was constructed by exploiting the alignments of an English-Chinese corpus. Similarly, Delli Bovi et al. (2017) presented EuroSense, a multilingual sense-annotated corpus using the Europarl parallel corpus for 21 languages as reference. In contrast to these approaches, we focus on unambiguous senses and, therefore, are not constrained to only nouns, knowledge from Wikipedia, or a specific type of corpus. Earlier works exploiting unambiguous words (Leacock et al., 1998; Mihalcea, 2002; Agirre and Martinez, 2004) and especially the subsequent extension by Martinez et al. (2008) are the most directly related to our paper. Martinez et al. (2008) retrieved example sentences with monosemous nouns from web se"
2020.emnlp-main.283,N19-1423,0,0.0155915,"uous Word Annotations) dataset and show how a state-of-theart propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD. 1 Introduction There has been a lot of progress in word sense disambiguation (WSD) recently. This progress has been driven by two factors: (1) the introduction of large pre-trained Transformer-based language models and (2) propagation algorithms that extends the coverage of existing training sets. The gains due to pre-trained Neural Language Models (NLMs) such as BERT (Devlin et al., 2019) have been outstanding, helping reach levels close to human performance when training data is available. These models are generally based on a nearest neighbours strategy, where each sense is represented by a vector, exploiting the contextualized embeddings of these NLMs (Melamud et al., 2016; Peters et al., 2018; Loureiro and Jorge, 2019). However, training data for WSD is hard to obtain, and the most widely used training set nowadays, based on WordNet, dates back from the 90s (Miller et al., 1993, SemCor). This lack of curated data produces the so-called knowledge-acquisition bottleneck (Gal"
2020.emnlp-main.283,P16-1191,0,0.0670889,"Missing"
2020.emnlp-main.283,D19-1355,0,0.0419146,"019). In addition to the original version using BERT, we also provide results with RoBERTa (Liu et al., 2019) for completeness. We use the 24-layer models for both BERT and RoBERTa.3 4.1 Word Sense Disambiguation (WSD) Table 2 shows the WSD results on the standard evaluation framework of Raganato et al. (2017) for LMMS trained on the concatenation of SemCor and automatically-constructed corpora. In the table we include UWA with two different maximum number of examples per unambiguous word, i.e., 1 and 10. For comparison, we also include the results of EWISE (Kumar et al., 2019) and GlossBERT (Huang et al., 2019), which attempt to overcome the limited coverage of SemCor by exploiting textual definitions. As can be observed, the concatenation of our UWA corpus and SemCor provides the best overall results, regardless of the number of examples cut-off. Perhaps surprisingly, our corpus is the only one that provides improvements over the baseline (SemCor-only). These improvements are statistically significant on the full test set (i.e. ALL) for both BERT and RoBERTa with p &lt; 0.0005, based on a t-test with respect to the 3516 3 Commonly referred to as large models. Corpus SE-2 SE-3 SE07 SE13 SE15 ALL LMMS-B"
2020.emnlp-main.283,P19-1568,0,0.0466383,"D model LMMS (Loureiro and Jorge, 2019). In addition to the original version using BERT, we also provide results with RoBERTa (Liu et al., 2019) for completeness. We use the 24-layer models for both BERT and RoBERTa.3 4.1 Word Sense Disambiguation (WSD) Table 2 shows the WSD results on the standard evaluation framework of Raganato et al. (2017) for LMMS trained on the concatenation of SemCor and automatically-constructed corpora. In the table we include UWA with two different maximum number of examples per unambiguous word, i.e., 1 and 10. For comparison, we also include the results of EWISE (Kumar et al., 2019) and GlossBERT (Huang et al., 2019), which attempt to overcome the limited coverage of SemCor by exploiting textual definitions. As can be observed, the concatenation of our UWA corpus and SemCor provides the best overall results, regardless of the number of examples cut-off. Perhaps surprisingly, our corpus is the only one that provides improvements over the baseline (SemCor-only). These improvements are statistically significant on the full test set (i.e. ALL) for both BERT and RoBERTa with p &lt; 0.0005, based on a t-test with respect to the 3516 3 Commonly referred to as large models. Corpus"
2020.emnlp-main.283,J98-1006,0,0.487781,"Missing"
2020.emnlp-main.283,2021.ccl-1.108,0,0.0536693,"Missing"
2020.emnlp-main.283,P19-1569,1,0.928687,"as been driven by two factors: (1) the introduction of large pre-trained Transformer-based language models and (2) propagation algorithms that extends the coverage of existing training sets. The gains due to pre-trained Neural Language Models (NLMs) such as BERT (Devlin et al., 2019) have been outstanding, helping reach levels close to human performance when training data is available. These models are generally based on a nearest neighbours strategy, where each sense is represented by a vector, exploiting the contextualized embeddings of these NLMs (Melamud et al., 2016; Peters et al., 2018; Loureiro and Jorge, 2019). However, training data for WSD is hard to obtain, and the most widely used training set nowadays, based on WordNet, dates back from the 90s (Miller et al., 1993, SemCor). This lack of curated data produces the so-called knowledge-acquisition bottleneck (Gale et al., 1992; Navigli, 2009). However, there is a key source of information that has been neglected so far in existing senseannotated corpora and propagation methods, which is the presence of unambiguous words from the underlying knowledge resource. Strikingly, WordNet, which is known to be a comprehensive resource, is mostly composed of"
2020.emnlp-main.283,P14-5010,0,0.00437552,"Missing"
2020.emnlp-main.283,K16-1006,0,0.0453943,"ambiguation (WSD) recently. This progress has been driven by two factors: (1) the introduction of large pre-trained Transformer-based language models and (2) propagation algorithms that extends the coverage of existing training sets. The gains due to pre-trained Neural Language Models (NLMs) such as BERT (Devlin et al., 2019) have been outstanding, helping reach levels close to human performance when training data is available. These models are generally based on a nearest neighbours strategy, where each sense is represented by a vector, exploiting the contextualized embeddings of these NLMs (Melamud et al., 2016; Peters et al., 2018; Loureiro and Jorge, 2019). However, training data for WSD is hard to obtain, and the most widely used training set nowadays, based on WordNet, dates back from the 90s (Miller et al., 1993, SemCor). This lack of curated data produces the so-called knowledge-acquisition bottleneck (Gale et al., 1992; Navigli, 2009). However, there is a key source of information that has been neglected so far in existing senseannotated corpora and propagation methods, which is the presence of unambiguous words from the underlying knowledge resource. Strikingly, WordNet, which is known to be"
2020.emnlp-main.283,mihalcea-2002-bootstrapping,0,0.209656,"corpora.1 Previous approaches relied on parallel corpora for two or more languages. The OMSTI corpus (Taghipour and Ng, 2015) was constructed by exploiting the alignments of an English-Chinese corpus. Similarly, Delli Bovi et al. (2017) presented EuroSense, a multilingual sense-annotated corpus using the Europarl parallel corpus for 21 languages as reference. In contrast to these approaches, we focus on unambiguous senses and, therefore, are not constrained to only nouns, knowledge from Wikipedia, or a specific type of corpus. Earlier works exploiting unambiguous words (Leacock et al., 1998; Mihalcea, 2002; Agirre and Martinez, 2004) and especially the subsequent extension by Martinez et al. (2008) are the most directly related to our paper. Martinez et al. (2008) retrieved example sentences with monosemous nouns from web search snippets and used them towards improved performance on WSD by leveraging WordNet relations. However, the WSD methods analyzed were sensitive to frequency bias, leading their collection effort to collect a large number of examples for fewer senses (and only nouns). In contrast, our solution is designed for all monosemous words, retrieving examples from web texts instead"
2020.emnlp-main.283,H93-1061,0,0.788007,"xisting training sets. The gains due to pre-trained Neural Language Models (NLMs) such as BERT (Devlin et al., 2019) have been outstanding, helping reach levels close to human performance when training data is available. These models are generally based on a nearest neighbours strategy, where each sense is represented by a vector, exploiting the contextualized embeddings of these NLMs (Melamud et al., 2016; Peters et al., 2018; Loureiro and Jorge, 2019). However, training data for WSD is hard to obtain, and the most widely used training set nowadays, based on WordNet, dates back from the 90s (Miller et al., 1993, SemCor). This lack of curated data produces the so-called knowledge-acquisition bottleneck (Gale et al., 1992; Navigli, 2009). However, there is a key source of information that has been neglected so far in existing senseannotated corpora and propagation methods, which is the presence of unambiguous words from the underlying knowledge resource. Strikingly, WordNet, which is known to be a comprehensive resource, is mostly composed of unambiguous entries (30k lemmas are ambiguous, compared to 116k unambiguous). While the lack of unambiguous annotations does not have a direct effect in WSD, the"
2020.emnlp-main.283,2020.lrec-1.706,1,0.804564,"Missing"
2020.emnlp-main.283,N18-1202,0,0.0534957,"ntly. This progress has been driven by two factors: (1) the introduction of large pre-trained Transformer-based language models and (2) propagation algorithms that extends the coverage of existing training sets. The gains due to pre-trained Neural Language Models (NLMs) such as BERT (Devlin et al., 2019) have been outstanding, helping reach levels close to human performance when training data is available. These models are generally based on a nearest neighbours strategy, where each sense is represented by a vector, exploiting the contextualized embeddings of these NLMs (Melamud et al., 2016; Peters et al., 2018; Loureiro and Jorge, 2019). However, training data for WSD is hard to obtain, and the most widely used training set nowadays, based on WordNet, dates back from the 90s (Miller et al., 1993, SemCor). This lack of curated data produces the so-called knowledge-acquisition bottleneck (Gale et al., 1992; Navigli, 2009). However, there is a key source of information that has been neglected so far in existing senseannotated corpora and propagation methods, which is the presence of unambiguous words from the underlying knowledge resource. Strikingly, WordNet, which is known to be a comprehensive reso"
2020.emnlp-main.283,K15-1037,0,0.0271026,"uently addressed by automatically constructing sense-annotated corpora. Recent works propose methods that exploit knowledge from Wikipedia, such as NASARI vectors (Camacho-Collados et al., 2016), for providing sense annotations for concepts and entities (Scarlini et al., 2019; Pasini and Navigli, 2019). In the case of Scarlini et al. (2019), and similarly to Raganato et al. (2016), their method requires hyperlinks and category information from Wikipedia, hence not extensible to other kinds of corpora.1 Previous approaches relied on parallel corpora for two or more languages. The OMSTI corpus (Taghipour and Ng, 2015) was constructed by exploiting the alignments of an English-Chinese corpus. Similarly, Delli Bovi et al. (2017) presented EuroSense, a multilingual sense-annotated corpus using the Europarl parallel corpus for 21 languages as reference. In contrast to these approaches, we focus on unambiguous senses and, therefore, are not constrained to only nouns, knowledge from Wikipedia, or a specific type of corpus. Earlier works exploiting unambiguous words (Leacock et al., 1998; Mihalcea, 2002; Agirre and Martinez, 2004) and especially the subsequent extension by Martinez et al. (2008) are the most dire"
2020.emnlp-main.283,2019.gwc-1.14,0,0.0608217,"hey have 3515 # Instances Corpus Amb Unamb SemCor 198,153 27,883 OMSTI 909,830 1,304 T-o-M 719,888 114,580 UWA(1) 0 98,494 UWA(10) 0 804,861 UWA(all) 0 6,111,453 Avg # Exs 6.8 244.7 152.4 1.0 8.8 54.1 Coverage (w/ SC) Amb Unamb Total 26.2 7.4 16.1 26.8 7.4 16.4 28.5 7.5 17.2 26.2 82.9 56.7 26.2 82.9 56.7 26.2 82.9 56.7 Table 1: Number of instances, average number of examples per word sense, and coverage percentage (including SemCor) of various sense-annotated corpora. been used differently depending on the nature of the disambiguation task: as feature providers for other neural architectures (Vial et al., 2019), simple classifiers after fine-tuning (Wang et al., 2019), or as generators of contextual embeddings to be matched through nearest neighbours (Melamud et al., 2016; Peters et al., 2018; Loureiro and Jorge, 2019; Reif et al., 2019, 1NN). Our experiments in this paper will focus on improving the latter type of approach. In particular, we will investigate the state-of-the-art LMMS model (Loureiro and Jorge, 2019). This model learns sense embeddings based on BERT states. These embeddings are then propagated through WordNet’s ontology to infer additional senses, effectively providing a full covera"
2020.emnlp-main.283,P17-1170,1,0.882771,"Missing"
2020.emnlp-main.283,E17-1010,1,0.888671,"e are interested in verifying the impact of using UWA to improve WSD performance. In particular, we test the unambiguous annotations of UWA as a complement of existing sense-annotated training data. To this end, as explained in Section 3, we make use of the state-ofthe-art WSD model LMMS (Loureiro and Jorge, 2019). In addition to the original version using BERT, we also provide results with RoBERTa (Liu et al., 2019) for completeness. We use the 24-layer models for both BERT and RoBERTa.3 4.1 Word Sense Disambiguation (WSD) Table 2 shows the WSD results on the standard evaluation framework of Raganato et al. (2017) for LMMS trained on the concatenation of SemCor and automatically-constructed corpora. In the table we include UWA with two different maximum number of examples per unambiguous word, i.e., 1 and 10. For comparison, we also include the results of EWISE (Kumar et al., 2019) and GlossBERT (Huang et al., 2019), which attempt to overcome the limited coverage of SemCor by exploiting textual definitions. As can be observed, the concatenation of our UWA corpus and SemCor provides the best overall results, regardless of the number of examples cut-off. Perhaps surprisingly, our corpus is the only one t"
2020.emnlp-main.283,P19-1069,0,0.0117564,"nd, we show that by leveraging UWA, we can significantly improve a state-of-the-art WSD model. 3514 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3514–3520, c November 16–20, 2020. 2020 Association for Computational Linguistics 2 Related Work 3.1 The knowledge-acquisition bottleneck has been frequently addressed by automatically constructing sense-annotated corpora. Recent works propose methods that exploit knowledge from Wikipedia, such as NASARI vectors (Camacho-Collados et al., 2016), for providing sense annotations for concepts and entities (Scarlini et al., 2019; Pasini and Navigli, 2019). In the case of Scarlini et al. (2019), and similarly to Raganato et al. (2016), their method requires hyperlinks and category information from Wikipedia, hence not extensible to other kinds of corpora.1 Previous approaches relied on parallel corpora for two or more languages. The OMSTI corpus (Taghipour and Ng, 2015) was constructed by exploiting the alignments of an English-Chinese corpus. Similarly, Delli Bovi et al. (2017) presented EuroSense, a multilingual sense-annotated corpus using the Europarl parallel corpus for 21 languages as reference. In contrast to t"
2020.emnlp-main.584,P12-1092,0,0.16532,"nseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as restricting senses only to those defined by the inventory, or forcing the WSD system to explicitly model sense distinctions at the granularity level defined by the inventory. Stanford Contextual Word Similarity (Huang et al., 2012) is one of the first datasets that focuses on ambiguity but outside the boundaries of sense inventories, and as a similarity measurement between two words in their contexts. Pilehvar and Camacho-Collados (2019) highlighted some of the limitations of the dataset that prevent a reliable evaluation, and proposed the Word-in-Context (WiC) dataset. WiC is the closest dataset to ours, which provides around 10K instances (1400 instances for 1184 unique target nouns and verbs in the test set), but for the English language only. 2.2 Cross-lingual NLP A prerequisite for research on a language is the ava"
2020.emnlp-main.584,isahara-etal-2008-development,0,0.0482702,"on 3.2.2). 3.2.1 Multilingual WordNet WordNet (Miller, 1995) is the de facto sense inventory for English WSD. The resource was originally built as an English lexical database in 1995, but since then there have been many efforts to extend it to other languages (Bond and Paik, 2012). We took advantage of these extensions to construct XL-WiC. In particular, we processed the WordNet versions of Bulgarian (Simov and Osenova, 2010), Chinese (Huang et al., 2010), Croatian (Raffaelli et al., 2008), Danish (Pedersen et al., 2009), Dutch (Postma et al., 2016), Estonian (Vider and Orav, 2002), Japanese (Isahara et al., 2008), Korean (Yoon et al., 2009) and Farsi (Shamsfard et al., 2010).1 Farsi: Semi-automatic extraction. FarsNet v3.0 (Shamsfard et al., 2010) comprises 30K synsets with over 100K word entries. Many of these synsets are mapped to the English database; however, each synset provides just one example usage for a target word. This prevents us from applying the automatic extraction of positive examples. Therefore, we utilized a semi-automatic procedure for the construction of the Farsi set. To this end, for each word, we extracted all example usages from 1 We tried other WordNet versions such as Albania"
2020.emnlp-main.584,P18-4020,0,0.0372913,"Missing"
2020.emnlp-main.584,W17-3204,0,0.0247615,"Missing"
2020.emnlp-main.584,2020.acl-main.653,0,0.125474,"Missing"
2020.emnlp-main.584,P19-1124,0,0.0199364,"Missing"
2020.emnlp-main.584,S15-2049,0,0.612119,"ense Disambiguation The ability to identify the intended sense of a polysemous word in a given context is one of the fundamental problems in lexical semantics. It is usually addressed with two different kinds of approaches relying on either sense-annotated corpora (Bevilacqua and Navigli, 2020; Scarlini et al., 2020; Blevins and Zettlemoyer, 2020) or knowledge bases (Moro et al., 2014; Agirre et al., 2014; Scozzafava et al., 2020). Both are usually evaluated on dedicated benchmarks, including at least five WSD tasks in Senseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as restricting senses only to those defined by the inventory, or forcing the WSD system to explicitly model sense distinctions at the granularity level defined by the inventory. Stanford Contextual Word Similarity (Huang et al., 2012) is one of the first datasets that focuses on ambiguity but outside the boun"
2020.emnlp-main.584,Q14-1019,1,0.840733,"is a benchmark for inventory-independent evaluation of WSD models (Section 2.1), while the multilingual nature of the dataset makes it an interesting resource for experimenting with crosslingual transfer (Section 2.2). 2.1 Word Sense Disambiguation The ability to identify the intended sense of a polysemous word in a given context is one of the fundamental problems in lexical semantics. It is usually addressed with two different kinds of approaches relying on either sense-annotated corpora (Bevilacqua and Navigli, 2020; Scarlini et al., 2020; Blevins and Zettlemoyer, 2020) or knowledge bases (Moro et al., 2014; Agirre et al., 2014; Scozzafava et al., 2020). Both are usually evaluated on dedicated benchmarks, including at least five WSD tasks in Senseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as restricting senses only to those defined by the inventory, or forcing th"
2020.emnlp-main.584,S13-2040,0,0.39929,"und 10K instances (1400 instances for 1184 unique target nouns and verbs in the test set), but for the English language only. 2.2 Cross-lingual NLP A prerequisite for research on a language is the availability of relevant evaluation benchmarks. Given its importance, construction of multilingual datasets has always been considered as a key contribution in NLP research and numerous benchmarks exist for a wide range of tasks, such as semantic parsing (Hershcovich et al., 2019), word similarity (Camacho-Collados et al., 2017; Barzegar et al., 2018), sentence similarity (Cer et al., 2017), or WSD (Navigli et al., 2013; Moro and Navigli, 2015b). A more recent example is XTREME (Hu et al., 2020), a benchmark that covers around 40 languages in nine syntactic and semantic tasks. On the other hand, pre-trained language models have recently proven very effective in transferring knowledge in cross-lingual NLP tasks (Devlin et al., 2019; Conneau et al., 2020). This has further magnified the requirement for rigorous multilingual benchmarks that can be used as basis for this direction of research (Artetxe et al., 2020b). 3 XL-WiC: The Benchmark In this section, we describe the procedure we followed to construct the"
2020.emnlp-main.584,P02-1040,0,0.109067,"Missing"
2020.emnlp-main.584,N19-1128,1,0.608713,"Missing"
2020.emnlp-main.584,2016.gwc-1.43,0,0.0469731,"Missing"
2020.emnlp-main.584,W14-0105,0,0.0340426,"Missing"
2020.emnlp-main.584,2020.acl-demos.14,0,0.0324095,"Missing"
2020.emnlp-main.584,E17-1010,1,0.882283,"tended sense of a polysemous word in a given context is one of the fundamental problems in lexical semantics. It is usually addressed with two different kinds of approaches relying on either sense-annotated corpora (Bevilacqua and Navigli, 2020; Scarlini et al., 2020; Blevins and Zettlemoyer, 2020) or knowledge bases (Moro et al., 2014; Agirre et al., 2014; Scozzafava et al., 2020). Both are usually evaluated on dedicated benchmarks, including at least five WSD tasks in Senseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as restricting senses only to those defined by the inventory, or forcing the WSD system to explicitly model sense distinctions at the granularity level defined by the inventory. Stanford Contextual Word Similarity (Huang et al., 2012) is one of the first datasets that focuses on ambiguity but outside the boundaries of sense inventories, and as a similarity m"
2020.emnlp-main.584,2020.emnlp-main.285,1,0.825453,"g their multilingual counterparts by a large margin. 2 Related Work XL-WiC is a benchmark for inventory-independent evaluation of WSD models (Section 2.1), while the multilingual nature of the dataset makes it an interesting resource for experimenting with crosslingual transfer (Section 2.2). 2.1 Word Sense Disambiguation The ability to identify the intended sense of a polysemous word in a given context is one of the fundamental problems in lexical semantics. It is usually addressed with two different kinds of approaches relying on either sense-annotated corpora (Bevilacqua and Navigli, 2020; Scarlini et al., 2020; Blevins and Zettlemoyer, 2020) or knowledge bases (Moro et al., 2014; Agirre et al., 2014; Scozzafava et al., 2020). Both are usually evaluated on dedicated benchmarks, including at least five WSD tasks in Senseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as re"
2020.emnlp-main.584,N09-4007,0,0.0225585,"ur framework is based on the original WiC dataset, which we extend to multiple languages. 3.1 English WiC Each instance of the original WiC dataset (Pilehvar and Camacho-Collados, 2019) is composed of a target word (e.g., justify) and two sentences where the target word occurs (e.g., “Justify the margins” and “The end justifies the means”). The task is a binary classification: to decide whether the same sense of the target word (justify) was intended in the two contexts or not. The dataset was built using example sentences from resources such as Wiktionary, WordNet (Miller, 1995) and VerbNet (Schuler et al., 2009). 3.2 XL-WiC We followed Pilehvar and Camacho-Collados (2019) and constructed XL-WiC based on example usages of words in sense inventories. Example usages are curated in a way to be self contained and clearly distinguishable across different senses of a word; hence, they provide a reliable basis for the binary classification task. Specifically, for a word 7194 Lang. Target Word Sentence 1 Sentence 2 Label EN Beat We beat the competition. Agassi beat Becker in the tennis championship. True DA ET FR KO ZH FA Tro Ruum Causticit´e ᆯᄅ ᅳ ᄐ ᆷ ᅵ 發 Jeg tror p˚a det, min mor fortalte. ¨ Uhel hetkel olin"
2020.emnlp-main.584,2020.acl-demos.6,0,0.0235878,"t evaluation of WSD models (Section 2.1), while the multilingual nature of the dataset makes it an interesting resource for experimenting with crosslingual transfer (Section 2.2). 2.1 Word Sense Disambiguation The ability to identify the intended sense of a polysemous word in a given context is one of the fundamental problems in lexical semantics. It is usually addressed with two different kinds of approaches relying on either sense-annotated corpora (Bevilacqua and Navigli, 2020; Scarlini et al., 2020; Blevins and Zettlemoyer, 2020) or knowledge bases (Moro et al., 2014; Agirre et al., 2014; Scozzafava et al., 2020). Both are usually evaluated on dedicated benchmarks, including at least five WSD tasks in Senseval and SemEval series, from 2001 (Edmonds and Cotton, 2001) to 2015 (Moro and Navigli, 2015a) that are included in the Raganato et al. (2017)’s test suite. All these tasks are framed as classification problems, where disambiguation of a word is defined as selecting one of the predefined senses of the word listed by a sense inventory. This brings about different limitations such as restricting senses only to those defined by the inventory, or forcing the WSD system to explicitly model sense distinct"
2020.emnlp-main.584,simov-osenova-2010-constructing,0,0.0220283,"w i and sw where i = 6 j) are paired as a negative instance j (False label). We leveraged two main sense inventories for this extension: Multilingual WordNet (Section 3.2.1) and Wiktionary (Section 3.2.2). 3.2.1 Multilingual WordNet WordNet (Miller, 1995) is the de facto sense inventory for English WSD. The resource was originally built as an English lexical database in 1995, but since then there have been many efforts to extend it to other languages (Bond and Paik, 2012). We took advantage of these extensions to construct XL-WiC. In particular, we processed the WordNet versions of Bulgarian (Simov and Osenova, 2010), Chinese (Huang et al., 2010), Croatian (Raffaelli et al., 2008), Danish (Pedersen et al., 2009), Dutch (Postma et al., 2016), Estonian (Vider and Orav, 2002), Japanese (Isahara et al., 2008), Korean (Yoon et al., 2009) and Farsi (Shamsfard et al., 2010).1 Farsi: Semi-automatic extraction. FarsNet v3.0 (Shamsfard et al., 2010) comprises 30K synsets with over 100K word entries. Many of these synsets are mapped to the English database; however, each synset provides just one example usage for a target word. This prevents us from applying the automatic extraction of positive examples. Therefore,"
2020.emnlp-main.584,tiedemann-2012-parallel,0,0.0727061,"Missing"
2020.emnlp-main.584,2020.eamt-1.61,0,0.026207,"Missing"
2020.findings-emnlp.148,I13-1041,0,0.0172075,"w the effectiveness of starting off with existing pretrained generic language models, and continue training them on Twitter corpora. 1 Introduction Modern NLP systems are typically ill-equipped when applied to noisy user-generated text. The high-paced, conversational and idiosyncratic nature of social media, paired with platform-specific restrictions (e.g., Twitter’s character limit), requires tackling additional challenges, for example, POS tagging (Derczynski et al., 2013), lexical normalization (Han and Baldwin, 2011; Baldwin et al., 2015), or named entity recognition (Ritter et al., 2011; Baldwin et al., 2013). In other more generic contexts, these challenges can be considered solved or are simply non-existent. Moreover, other apparently simple tasks such as sentiment analysis have proven to be hard on Twitter data (Poria et al., 2020), among others, due to limited amount of contextual cues available in short texts (Kim et al., 2014). In addition to these and other inherent difficulties, advances in NLP for user-generated data are hindered by its highly fragmented landscape and the lack of a unified evaluation framework. In the current era of pretraining and Language Models (LMs), this is particula"
2020.findings-emnlp.148,W15-4319,0,0.0945742,"Missing"
2020.findings-emnlp.148,W14-3902,0,0.08212,"Missing"
2020.findings-emnlp.148,S19-2007,0,0.13315,"he specific statistics of each target domain in the stance detection task is included at the bottom. Irony Detection. This task consists of recognizing whether a tweet includes ironic intents or not. We use the Subtask A dataset of the SemEval2018 Irony Detection challenge (Van Hee et al., 2018). Note that this dataset was artificially balanced to make the task more accessible. Hate Speech Detection. This task consists in predicting whether a tweet is hateful or not against any of two target communities: immigrants and women. Our dataset of choice stems from the SemEval2019 Hateval challenge (Basile et al., 2019). Offensive Language Identification. This task consists in identifying whether some form of offensive language is present in a tweet. For our benchmark we rely on the SemEval2019 OffensEval dataset (Zampieri et al., 2019). Sentiment Analysis. The goal for the sentiment analysis task is to recognize if a tweet is positive, negative or neutral. We use the Semeval2017 dataset for Subtask A (Rosenthal et al., 2019), 1645 which includes data from previous runs (2013, 2014, 2015, and 2016) of the same SemEval task. Stance Detection. Stance detection is the task to determine, given a piece of text, w"
2020.findings-emnlp.148,Q17-1010,0,0.0162477,"lidation set, and use the corresponding model to evaluate on the test set. Baselines. FastText (Joulin et al., 2017) provides an efficient baseline based on standard features and subword units. We also include an SVM-based baseline with both word and character n-gram features, a model and feature set that has seen great success in recent Twitter-based shared tasks such as emoji prediction (C ¸ o¨ ltekin and Rama, 2018) and stance prediction (Mohammad et al., 2018). We finally report the results of a bi-directional LSTM.8 Both FastText and the LSTM use 100-dimensional FastText word embeddings (Bojanowski et al., 2017) trained on the 60M Twitter corpus for the 8 The LSTM has 128 cells, an embedding layer of 100 dimensions, dropout (0.5) and, similarly to the language models, the four learning rate values are tuned in the validation set. lookup table initialization. 4.2 Results Table 3 shows the results of all comparison systems on T WEET E VAL. Perhaps surprisingly, RoBERTaBase (RoB-Bs) performs well on all tasks, even outperforming the model trained on Twitter data only (RoB-Tw) in most tasks. This can also be attributed to the fact that Twitter is not only noisy text, and formal text can be also found reg"
2020.findings-emnlp.148,R13-1026,0,0.0527099,"provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pretrained generic language models, and continue training them on Twitter corpora. 1 Introduction Modern NLP systems are typically ill-equipped when applied to noisy user-generated text. The high-paced, conversational and idiosyncratic nature of social media, paired with platform-specific restrictions (e.g., Twitter’s character limit), requires tackling additional challenges, for example, POS tagging (Derczynski et al., 2013), lexical normalization (Han and Baldwin, 2011; Baldwin et al., 2015), or named entity recognition (Ritter et al., 2011; Baldwin et al., 2013). In other more generic contexts, these challenges can be considered solved or are simply non-existent. Moreover, other apparently simple tasks such as sentiment analysis have proven to be hard on Twitter data (Poria et al., 2020), among others, due to limited amount of contextual cues available in short texts (Kim et al., 2014). In addition to these and other inherent difficulties, advances in NLP for user-generated data are hindered by its highly fragm"
2020.findings-emnlp.148,N19-1423,0,0.0421313,"e anonymized and line breaks and website links are removed. Evaluation metrics. We use the same evaluation metric from the original tasks, which is macroaveraged F1 over all classes, in most cases. There are three exceptions: stance (macro-averaged of F1 of favor and against classes), irony (F1 of ironic class), and sentiment analysis (macro-averaged recall). Similar to GLUE (Wang et al., 2019b), we also introduce a global metric (TE) based on the average of all dataset-specific metrics. 3 Language Models for Tweet Classification Transformer-based LMs such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019) or XLNET (Yang et al., 2019) have taken the NLP field by storm, outperforming previous linear models and neural network methods based on LSTMs or CNNs in many tasks, including sentence and text classification (Wang et al., 2019b). The functioning of these language models for tweet classification is conceptually simple. First, they are trained on a large unlabeled corpus. Then, they are fine-tuned to the task for where an appropriate training set exists. For social media text, however, one may question whether existing pretrained models trained on standard corpora are optimal. We thus compare"
2020.findings-emnlp.148,P11-1038,0,0.0292102,"nt, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pretrained generic language models, and continue training them on Twitter corpora. 1 Introduction Modern NLP systems are typically ill-equipped when applied to noisy user-generated text. The high-paced, conversational and idiosyncratic nature of social media, paired with platform-specific restrictions (e.g., Twitter’s character limit), requires tackling additional challenges, for example, POS tagging (Derczynski et al., 2013), lexical normalization (Han and Baldwin, 2011; Baldwin et al., 2015), or named entity recognition (Ritter et al., 2011; Baldwin et al., 2013). In other more generic contexts, these challenges can be considered solved or are simply non-existent. Moreover, other apparently simple tasks such as sentiment analysis have proven to be hard on Twitter data (Poria et al., 2020), among others, due to limited amount of contextual cues available in short texts (Kim et al., 2014). In addition to these and other inherent difficulties, advances in NLP for user-generated data are hindered by its highly fragmented landscape and the lack of a unified eval"
2020.findings-emnlp.148,E17-2068,0,0.046211,"one dense layer to reduce the dimensions of the RoBERTa’s last layer to the number of labels in the classification task, and fine-tune the model on each classification task, training all the parameters simultaneously. We run a minimum parameter search on the starting learning rate (1.0e−3 , 1.0e−4 , 1.0e−5 , and 1.0e−6 ), use early stopping (5 epochs) on the validation set and run each experiment three times with different seeds (1,2,3). Then, we select the highest performing learning rate on the validation set, and use the corresponding model to evaluate on the test set. Baselines. FastText (Joulin et al., 2017) provides an efficient baseline based on standard features and subword units. We also include an SVM-based baseline with both word and character n-gram features, a model and feature set that has seen great success in recent Twitter-based shared tasks such as emoji prediction (C ¸ o¨ ltekin and Rama, 2018) and stance prediction (Mohammad et al., 2018). We finally report the results of a bi-directional LSTM.8 Both FastText and the LSTM use 100-dimensional FastText word embeddings (Bojanowski et al., 2017) trained on the 60M Twitter corpus for the 8 The LSTM has 128 cells, an embedding layer of 1"
2020.findings-emnlp.148,2021.ccl-1.108,0,0.116051,"Missing"
2020.findings-emnlp.148,P18-1185,1,0.902093,"Missing"
2020.findings-emnlp.148,S18-1001,0,0.193374,"relevant statistics and evaluation metrics. We also show, in Table 1, a sample tweet and its corresponding label from the original task. Emoji prediction Emotion det. Hate speech det. Irony detection Offensive lg. id. Sent. analysis Stance detection 20 4 2 2 2 3 3 45,000 3257 9,000 2,862 11,916 45,389 2620 2.1 Stance/Abortion Stance/Atheism Stance/Climate Stance/Feminism Stance/H. Clinton 3 3 3 3 3 587 461 355 597 620 Tasks Emotion Recognition. This task consists of recognizing the emotion evoked by a tweet. We use the dataset of the most participated task of SemEval2018, “Affects in Tweets” (Mohammad et al., 2018). The original competition was framed as a multi-label classification problem, including 11 emotions. The integration into T WEET E VAL consists of re-purposing this multi-label dataset into multi-class classification, keeping only the tweets labeled with a single emotion. Since the amount of tweets with single labels was scarce, we selected the most common four emotions (Anger, Joy, Sadness, Optimism)2 . Emoji Prediction. This task consists in, given a tweet, predicting its most likely emoji, and is based on the Emoji Prediction challenge at Semeval2018 (Barbieri et al., 2018). It only consid"
2020.findings-emnlp.148,S16-1003,0,0.0859026,"Missing"
2020.findings-emnlp.148,D11-1141,0,0.190405,"Missing"
2020.findings-emnlp.148,N19-1060,0,0.0484031,"Missing"
2020.findings-emnlp.148,W17-4901,0,0.0177824,"er corpus for the 8 The LSTM has 128 cells, an embedding layer of 100 dimensions, dropout (0.5) and, similarly to the language models, the four learning rate values are tuned in the validation set. lookup table initialization. 4.2 Results Table 3 shows the results of all comparison systems on T WEET E VAL. Perhaps surprisingly, RoBERTaBase (RoB-Bs) performs well on all tasks, even outperforming the model trained on Twitter data only (RoB-Tw) in most tasks. This can also be attributed to the fact that Twitter is not only noisy text, and formal text can be also found regularly (Hu et al., 2013; Xu, 2017). Using more Twitter data for training might further improve the results of RoB-Tw, but this would also translate into an even more expensive training. However, RoBERTaBase coupled with additional training on the same Twitter corpus (i.e. RoB-RT) proves more effective. The only task where a model trained from scratch on Twitter performs better is Irony detection, where RoB-Tw shows to better generalize (RoB-RT F1 drops 13 points from validation to test set, while Rob-Tw F1 5 points). This can be due to two factors: (1) irony used on social media might differ from irony on standard text, (2) tw"
2020.findings-emnlp.148,S19-2010,0,0.0318212,"A dataset of the SemEval2018 Irony Detection challenge (Van Hee et al., 2018). Note that this dataset was artificially balanced to make the task more accessible. Hate Speech Detection. This task consists in predicting whether a tweet is hateful or not against any of two target communities: immigrants and women. Our dataset of choice stems from the SemEval2019 Hateval challenge (Basile et al., 2019). Offensive Language Identification. This task consists in identifying whether some form of offensive language is present in a tweet. For our benchmark we rely on the SemEval2019 OffensEval dataset (Zampieri et al., 2019). Sentiment Analysis. The goal for the sentiment analysis task is to recognize if a tweet is positive, negative or neutral. We use the Semeval2017 dataset for Subtask A (Rosenthal et al., 2019), 1645 which includes data from previous runs (2013, 2014, 2015, and 2016) of the same SemEval task. Stance Detection. Stance detection is the task to determine, given a piece of text, whether the author has a favourable, neutral, or negative position towards a proposition or target. We use the SemEval2016 shared task on Detecting Stance in Tweets (Mohammad et al., 2016). In the original task, five targe"
2020.findings-emnlp.148,S17-2088,0,0.120045,"Missing"
2020.findings-emnlp.148,S18-1005,0,0.0563791,"Missing"
2020.findings-emnlp.148,L16-1655,0,0.0907058,"Missing"
2020.lrec-1.495,N19-1253,0,0.0241511,"ple, Artetxe et al. (2017) and Authors marked with an asterisk (*) contributed equally. Conneau et al. (2018a) achieved promising results in the word translation task (i.e., bilingual lexicon induction), but their experiments relied on the availability of high-quality monolingual source corpora, namely Wikipedia, which is also the case in a more recent analysis on cross-lingual embeddings performance (Glavas et al., 2019). In fact, there exists a significant number of settings which have been largely ignored, and which might challenge models that excel in idealized environments. For instance, Ahmad et al. (2019) found that for dissimilar languages with different word orderings than English, cross-lingual transfer is still challenging. Similarly, it remains unclear how well existing methods would perform on language pairs with significant differences in morphology (e.g., English-Finnish, the latter being an agglutinative language) or with different alphabets (e.g., English-Farsi or English-Russian). Moreover, settings with different kinds of corpora (e.g. noisy user-generated) have not been fully explored. This means, among others, that it is not clear how current cross-lingual embedding models would"
2020.lrec-1.495,W13-3520,0,0.211432,"Missing"
2020.lrec-1.495,P17-1042,0,0.310442,"f methods that only need comparable data (e.g., Wikipedia corpora in different languages) as the main source of supervision (Vuli´c and Moens, 2015). In a complementary direction, it has recently been shown that high-quality cross-lingual embeddings can be obtained by aligning two independently learned monolingual embedding spaces. This strategy is appealing, as it means that one only needs access to monolingual corpora and a bilingual dictionary as supervision signal. Surprisingly, perhaps, it turns out that dictionaries with less than 100 word pairs are sufficient to obtain good alignments (Artetxe et al., 2017). In fact, recent works have shown that cross-lingual embeddings can even be learned without any user-provided dictionary (Conneau et al., 2018a; Artetxe et al., 2018b; Xu et al., 2018). Despite the promising results reported in the literature, it remains unclear under which conditions the aforementioned methods succeed. For example, Artetxe et al. (2017) and Authors marked with an asterisk (*) contributed equally. Conneau et al. (2018a) achieved promising results in the word translation task (i.e., bilingual lexicon induction), but their experiments relied on the availability of high-quality"
2020.lrec-1.495,P18-1073,0,0.103938,"y direction, it has recently been shown that high-quality cross-lingual embeddings can be obtained by aligning two independently learned monolingual embedding spaces. This strategy is appealing, as it means that one only needs access to monolingual corpora and a bilingual dictionary as supervision signal. Surprisingly, perhaps, it turns out that dictionaries with less than 100 word pairs are sufficient to obtain good alignments (Artetxe et al., 2017). In fact, recent works have shown that cross-lingual embeddings can even be learned without any user-provided dictionary (Conneau et al., 2018a; Artetxe et al., 2018b; Xu et al., 2018). Despite the promising results reported in the literature, it remains unclear under which conditions the aforementioned methods succeed. For example, Artetxe et al. (2017) and Authors marked with an asterisk (*) contributed equally. Conneau et al. (2018a) achieved promising results in the word translation task (i.e., bilingual lexicon induction), but their experiments relied on the availability of high-quality monolingual source corpora, namely Wikipedia, which is also the case in a more recent analysis on cross-lingual embeddings performance (Glavas et al., 2019). In fact,"
2020.lrec-1.495,S18-2010,0,0.0191237,"RU 33.4 33.6 33.4 33.3 33.2 33.1 33.8 33.2 Avg 43.3 42.8 41.1 35.0 43.0 43.0 43.4 42.9 Web corpora Sup. Unsup Ident 8K Model VecMap MUSE VecMap MUSE VecMap MUSE MeemiVM MeemiMS EN-ES 48.5 47.7 45.5 35.2 48.4 47.3 47.8 47.3 EN-DE 47.9 47.1 44.4 36.6 47.5 48.6 48.6 48.2 Table 4: Accuracy in the cross-lingual natural language inference task (XNLI) using different cross-lingual word embedding models. Table 3: Spearman correlation performance of various cross-lingual word embedding models in the cross-lingual word similarity task. better with downstream performance than other intrinsic benchmarks (Bakarov et al., 2018). The results are reported in terms of the Pearson and Spearman correlation with respect to human similarity judgments. The cross-lingual word similarity results for all the systems are displayed in Table 3. The languages available for this dataset are English, Spanish, Italian, German and Farsi, hence Finnish and Russian were not evaluated in this task. 5.3. Cross-lingual natural language inference The task of natural language inference (NLI) consists in detecting entailment, contradiction and neutral relations between pairs of sentences. We test a zero-shot cross-lingual transfer setting whe"
2020.lrec-1.495,W16-1614,0,0.373114,"ights the embeddings based on the cross-correlation of their components, which makes it the only non-orthogonal method tested in this work. MUSE (Conneau et al., 2018a) obtains its transformation matrix in a similar way. In this case, the seed dictionary is used as-is (supervised setting) or obtained in a fully automatically way through an adversarial learning method (unsupervised setting). 3.2. Limitations and postprocessing By restricting transformations to orthogonal linear mappings, VecMap and MUSE rely on the assumption that the monolingual embeddings spaces are approximately isomorphic (Barone, 2016). However, it has been argued that this assumption is overly restrictive, as the isomorphism assumption is not always satisfied (Søgaard et al., 2018; Kementchedjhieva et al., 2018). For this reason, it has been proposed to go beyond orthogonal transformations by modifying the internal structure of the monolingual spaces, either by giving more weight to highly correlated embedding components, as is the case for the unsupervised variant of VecMap in this work (Artetxe et al., 2018a), or by complementing the orthogonal transformation with other forms of post-processing. As an example of this lat"
2020.lrec-1.495,Q17-1010,0,0.506354,"ictionaries and automatically-constructed dictionaries; and (4) we include a more exhaustive intrinsic evaluation (including cross-lingual semantic similarity). 3. Learning Cross-lingual Word Embeddings The focus of our evaluation is on methods that start off with monolingual embedding models and then integrate these in a shared cross-lingual space. Hence, given two monolingual corpora, a word vector space is first learned independently for each language. This can be achieved with common word embedding models, e.g., Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017). Second, a linear alignment strategy is used to map the monolingual embeddings to a common bilingual vector space (Section 3.1). In some cases, a third transformation is applied to already aligned embeddings so the word vectors from both languages are refined and further re-positioned (Section 3.2). Regardless of the overall methodology, however, these linear transformations are all learned based on a bilingual dictionary. This dictionary may be manually curated or, in some cases, automatically generated as part of the alignment process. 3.1. Alignment methods In this paper we analyze two wel"
2020.lrec-1.495,S17-2002,1,0.875224,"Missing"
2020.lrec-1.495,D18-1024,0,0.0605566,"orpora such as Wikipedia and non-comparable or noisy user-generated corpora) and (2) language pairs (distant language pairs still constitute a major challenge). We may also conclude that bilingual supervision signals constitute a key component for most models in non-ideal settings (i.e., non-comparable corpora or distant languages). In general, our analysis and the results show that supervised cross-lingual word embedding learning is more robust than purely unsupervised cross-lingual learning, challenging claims from previous works on this regard (Conneau et al., 2018a; Artetxe et al., 2018b; Chen and Cardie, 2018; Hoshen and Wolf, 2018; Xu et al., 2018) and in line with a concurring analysis showing a similar trend (Vuli´c et al., 2019). As future work, it would be interesting to analyze multilingual embeddings that involve more than two languages, along the lines of recent multilingual approaches (Chen and Cardie, 2018; Heyman et al., 2019; Doval et al., 2019). 8. Acknowledgments Steven Schockaert is supported by ERC Starting Grant 637277. Yerai Doval has been supported by the Spanish Ministry of Economy, Industry and Competitiveness (MINECO) through the ANSWER-ASAP project (TIN2017-85160-C2-2-R); by"
2020.lrec-1.495,L18-1269,0,0.023944,"t NLI system. Therefore, since this is a downstream task evaluated at the sentence level (and not at the word level as in dictionary induction and semantic word similarity), we develop a simple bag-of-words approach where a sentence embedding is obtained by word vector averaging. We then train a linear classifier10 to obtain the predicted label for each pair of sentences: entailment, contradiction or neutral. We use the full MultiNLI (Williams et al., 2018) English corpus for training and the Spanish, German and Russian test sets from 10 The codebase for these experiments is that of SentEval (Conneau and Kiela, 2018) Figure 1: P@1 performance of the unsupervised version of VecMap on dictionary induction across corpus types and language pairs. XNLI (Conneau et al., 2018b) for testing. Accuracy results are shown in Table 4.11 6. Analysis Supervision signals. Unsurprisingly, the best alignments of monolingual spaces tend to be obtained with the largest bilingual dictionaries. The unsupervised variants of VecMap (see Figure 1) and MUSE attain competitive performance in most cases, especially for comparable corpora where alignments are easier to obtain. However, they struggle in the case of noisy social media"
2020.lrec-1.495,D18-1269,0,0.0690237,"015). In a complementary direction, it has recently been shown that high-quality cross-lingual embeddings can be obtained by aligning two independently learned monolingual embedding spaces. This strategy is appealing, as it means that one only needs access to monolingual corpora and a bilingual dictionary as supervision signal. Surprisingly, perhaps, it turns out that dictionaries with less than 100 word pairs are sufficient to obtain good alignments (Artetxe et al., 2017). In fact, recent works have shown that cross-lingual embeddings can even be learned without any user-provided dictionary (Conneau et al., 2018a; Artetxe et al., 2018b; Xu et al., 2018). Despite the promising results reported in the literature, it remains unclear under which conditions the aforementioned methods succeed. For example, Artetxe et al. (2017) and Authors marked with an asterisk (*) contributed equally. Conneau et al. (2018a) achieved promising results in the word translation task (i.e., bilingual lexicon induction), but their experiments relied on the availability of high-quality monolingual source corpora, namely Wikipedia, which is also the case in a more recent analysis on cross-lingual embeddings performance (Glavas"
2020.lrec-1.495,D18-1027,1,0.925147,"016), which has been extended in a more recent survey by Ruder et al. (2018). In this paper, we complement those studies by analyzing and discussing empirical findings of the most recent state-of-the-art unsupervised and semi-supervised methods in a broader experimental setting, more in line with the recent concurrent analysis of Glavas et al. (2019). The main differences between this empirical evaluation and the contributions of our work lie in the scope of the survey, since: (1) they only consider Wikipedia data for training; (2) they do not consider postprocessing techniques such as Meemi (Doval et al., 2018), which we found to improve the performance of cross-lingual models, especially in the case of distant languages and non-comparable corpora; (3) in our analysis we also consider additional settings with scarce training data such as small seed dictionaries and automatically-constructed dictionaries; and (4) we include a more exhaustive intrinsic evaluation (including cross-lingual semantic similarity). 3. Learning Cross-lingual Word Embeddings The focus of our evaluation is on methods that start off with monolingual embedding models and then integrate these in a shared cross-lingual space. Henc"
2020.lrec-1.495,P19-1070,0,0.0797015,"Missing"
2020.lrec-1.495,W15-4321,0,0.0613617,"Missing"
2020.lrec-1.495,S13-1005,0,0.0217476,"sider a wide range of signals, including no supervision as well as automatically generated dictionaries of identical words. In the latter case, we rely on the assumption that words that occur in both of the monolingual corpora tend to have the same meaning. While this may seem naive, this strategy has been reported in the literature to perform well in practice (Smith et al., 2017; Søgaard et al., 2018). 1 All Wikipedia text dumps were downloaded from the Polyglot project (Al-Rfou et al., 2013): https://sites. google.com/site/rmyeid/projects/polyglot 2 The sources of the web-corpora are: UMBC (Han et al., 2013), 1-billion (Cardellino, 2016), itWaC and sdeWaC (Baroni et al., 2009), Hamshahri (AleAhmad et al., 2009), and Common Crawl downloaded from http://www.statmt.org/ wmt16/translation-task.html. 3 Social media corpora are based on Twitter, at different dates between 2015 and 2018 (Camacho-Collados et al., 2020). Monolingual embeddings were downloaded at https://github. com/pedrada88/crossembeddings-twitter 4 Due to some restrictions, we were not able to compile a reliable Twitter corpus for Russian. Corpus Web corpora Social media Language Size Words English Spanish Italian German Finnish Russian"
2020.lrec-1.495,D18-1056,0,0.0381738,"Missing"
2020.lrec-1.495,N19-1188,0,0.112384,"Missing"
2020.lrec-1.495,D18-1043,0,0.332224,"t generally high-quality corpora) and social media3 (as a prototypical example of noisy text). Statistics of these corpora are provided in Table 1.4 4.2. Bilingual supervision Early approaches for learning bilingual embeddings relied on large parallel corpora (Klementiev et al., 2012; Luong et al., 2015), which limited their applicability. More recent approaches instead rely on (often small) bilingual dictionaries as the only source of bilingual supervision. In fact, some methods remove the need for a user-supplied bilingual dictionary altogether (Conneau et al., 2018a; Artetxe et al., 2018b; Hoshen and Wolf, 2018; Xu et al., 2018), relying instead on synthetic dictionaries that are obtained fully automatically. In our experiments we consider a wide range of signals, including no supervision as well as automatically generated dictionaries of identical words. In the latter case, we rely on the assumption that words that occur in both of the monolingual corpora tend to have the same meaning. While this may seem naive, this strategy has been reported in the literature to perform well in practice (Smith et al., 2017; Søgaard et al., 2018). 1 All Wikipedia text dumps were downloaded from the Polyglot projec"
2020.lrec-1.495,K18-1021,0,0.108848,"18a) obtains its transformation matrix in a similar way. In this case, the seed dictionary is used as-is (supervised setting) or obtained in a fully automatically way through an adversarial learning method (unsupervised setting). 3.2. Limitations and postprocessing By restricting transformations to orthogonal linear mappings, VecMap and MUSE rely on the assumption that the monolingual embeddings spaces are approximately isomorphic (Barone, 2016). However, it has been argued that this assumption is overly restrictive, as the isomorphism assumption is not always satisfied (Søgaard et al., 2018; Kementchedjhieva et al., 2018). For this reason, it has been proposed to go beyond orthogonal transformations by modifying the internal structure of the monolingual spaces, either by giving more weight to highly correlated embedding components, as is the case for the unsupervised variant of VecMap in this work (Artetxe et al., 2018a), or by complementing the orthogonal transformation with other forms of post-processing. As an example of this latter strategy, Doval et al. (2018) fine-tune the initial alignment by learning an unconstrained linear transformation which aims to map each word vector onto the average of that vect"
2020.lrec-1.495,C12-1089,0,0.386432,"bedding model is learned for each language. There is an increasing interest, however, in learning crosslingual word embeddings, where words from different languages are mapped onto a single space. Such representations are attractive, for instance, for dealing with the multilingual nature of text on the Web, but also as a vehicle for transferring knowledge (e.g., labelled training data) from resource-rich languages such as English to other languages (Ruder et al., 2018). Initially, the main obstacle to learning such cross-lingual embeddings was the need for large multilingual parallel corpora (Klementiev et al., 2012; Chandar et al., 2014; Luong et al., 2015). This limitation, however, was alleviated by the development of methods that only need comparable data (e.g., Wikipedia corpora in different languages) as the main source of supervision (Vuli´c and Moens, 2015). In a complementary direction, it has recently been shown that high-quality cross-lingual embeddings can be obtained by aligning two independently learned monolingual embedding spaces. This strategy is appealing, as it means that one only needs access to monolingual corpora and a bilingual dictionary as supervision signal. Surprisingly, perhap"
2020.lrec-1.495,W15-1521,0,0.160807,"re is an increasing interest, however, in learning crosslingual word embeddings, where words from different languages are mapped onto a single space. Such representations are attractive, for instance, for dealing with the multilingual nature of text on the Web, but also as a vehicle for transferring knowledge (e.g., labelled training data) from resource-rich languages such as English to other languages (Ruder et al., 2018). Initially, the main obstacle to learning such cross-lingual embeddings was the need for large multilingual parallel corpora (Klementiev et al., 2012; Chandar et al., 2014; Luong et al., 2015). This limitation, however, was alleviated by the development of methods that only need comparable data (e.g., Wikipedia corpora in different languages) as the main source of supervision (Vuli´c and Moens, 2015). In a complementary direction, it has recently been shown that high-quality cross-lingual embeddings can be obtained by aligning two independently learned monolingual embedding spaces. This strategy is appealing, as it means that one only needs access to monolingual corpora and a bilingual dictionary as supervision signal. Surprisingly, perhaps, it turns out that dictionaries with less"
2020.lrec-1.495,D14-1162,0,0.0926226,"Missing"
2020.lrec-1.495,P18-1072,0,0.16944,"Missing"
2020.lrec-1.495,P14-1146,0,0.0480676,"perform on language pairs with significant differences in morphology (e.g., English-Finnish, the latter being an agglutinative language) or with different alphabets (e.g., English-Farsi or English-Russian). Moreover, settings with different kinds of corpora (e.g. noisy user-generated) have not been fully explored. This means, among others, that it is not clear how current cross-lingual embedding models would behave for transferring knowledge in environments such as social media centred tasks, given that such tasks usually benefit from embeddings that have been trained on social media corpora (Tang et al., 2014; Godin et al., 2015; Yang et al., 2018). In this work, we broaden the empirical evaluation of state-of-the-art techniques for learning cross-lingual embeddings, by using several types of training corpora, various amounts of supervision, languages from different families and different alignment strategies in three different tasks. The results obtained cast some doubt on the view that high-quality cross-lingual embeddings can always be learned without much supervision. 4013 2. Related Work Cross-lingual embeddings have become increasingly popular in the past few years (Smith et al., 2017; Artet"
2020.lrec-1.495,P16-1157,0,0.0383093,"Missing"
2020.lrec-1.495,P16-1024,0,0.129528,"Missing"
2020.lrec-1.495,P15-2118,0,0.0540452,"Missing"
2020.lrec-1.495,D19-1449,0,0.112614,"Missing"
2020.lrec-1.495,N18-1101,0,0.0130076,"important to highlight that in this evaluation our main aim is to compare the quality of the cross-lingual word embeddings, and not to develop a state-of-the-art NLI system. Therefore, since this is a downstream task evaluated at the sentence level (and not at the word level as in dictionary induction and semantic word similarity), we develop a simple bag-of-words approach where a sentence embedding is obtained by word vector averaging. We then train a linear classifier10 to obtain the predicted label for each pair of sentences: entailment, contradiction or neutral. We use the full MultiNLI (Williams et al., 2018) English corpus for training and the Spanish, German and Russian test sets from 10 The codebase for these experiments is that of SentEval (Conneau and Kiela, 2018) Figure 1: P@1 performance of the unsupervised version of VecMap on dictionary induction across corpus types and language pairs. XNLI (Conneau et al., 2018b) for testing. Accuracy results are shown in Table 4.11 6. Analysis Supervision signals. Unsurprisingly, the best alignments of monolingual spaces tend to be obtained with the largest bilingual dictionaries. The unsupervised variants of VecMap (see Figure 1) and MUSE attain compet"
2020.lrec-1.495,D18-1268,0,0.48549,"ently been shown that high-quality cross-lingual embeddings can be obtained by aligning two independently learned monolingual embedding spaces. This strategy is appealing, as it means that one only needs access to monolingual corpora and a bilingual dictionary as supervision signal. Surprisingly, perhaps, it turns out that dictionaries with less than 100 word pairs are sufficient to obtain good alignments (Artetxe et al., 2017). In fact, recent works have shown that cross-lingual embeddings can even be learned without any user-provided dictionary (Conneau et al., 2018a; Artetxe et al., 2018b; Xu et al., 2018). Despite the promising results reported in the literature, it remains unclear under which conditions the aforementioned methods succeed. For example, Artetxe et al. (2017) and Authors marked with an asterisk (*) contributed equally. Conneau et al. (2018a) achieved promising results in the word translation task (i.e., bilingual lexicon induction), but their experiments relied on the availability of high-quality monolingual source corpora, namely Wikipedia, which is also the case in a more recent analysis on cross-lingual embeddings performance (Glavas et al., 2019). In fact, there exists a sig"
2020.lrec-1.706,E09-1005,0,0.0651464,"tances were annotated manually while the remaining annotations (i.e. 118,856) were obtained automatically. This corpus of disambiguated glosses has already proved to be useful in 2 This corpus has also been annotated with other resources, such as FrameNet (Baker et al., 1998). A comparison between the sense annotations of WordNet and lexical units of FrameNet is provided in De Melo et al. (2012). 3 http://wordnet.princeton.edu/glosstag. shtml tasks such as semantic similarity (Pilehvar et al., 2013), domain labeling (Gonz´alez et al., 2012) and Word Sense Disambiguation (Baldwin et al., 2008; Agirre and Soroa, 2009; Camacho-Collados et al., 2015). OntoNotes. OntoNotes (Weischedel et al., 2013) is a corpus from the Linguistic Data Consortium which comprises different kinds of explicitly-tagged syntactic and semantic information, including annotations at the sense level. The OntoNotes corpus consists of documents from diverse genres such as news, weblogs and telephone conversation. Its 5.0 released version contains 264,622 sense annotations. OMSTI. The task of gathering sense annotations has proved expensive and not easily scalable. That is the reason why more recent approaches have attempted to exploit s"
2020.lrec-1.706,P15-1072,1,0.847074,"nually while the remaining annotations (i.e. 118,856) were obtained automatically. This corpus of disambiguated glosses has already proved to be useful in 2 This corpus has also been annotated with other resources, such as FrameNet (Baker et al., 1998). A comparison between the sense annotations of WordNet and lexical units of FrameNet is provided in De Melo et al. (2012). 3 http://wordnet.princeton.edu/glosstag. shtml tasks such as semantic similarity (Pilehvar et al., 2013), domain labeling (Gonz´alez et al., 2012) and Word Sense Disambiguation (Baldwin et al., 2008; Agirre and Soroa, 2009; Camacho-Collados et al., 2015). OntoNotes. OntoNotes (Weischedel et al., 2013) is a corpus from the Linguistic Data Consortium which comprises different kinds of explicitly-tagged syntactic and semantic information, including annotations at the sense level. The OntoNotes corpus consists of documents from diverse genres such as news, weblogs and telephone conversation. Its 5.0 released version contains 264,622 sense annotations. OMSTI. The task of gathering sense annotations has proved expensive and not easily scalable. That is the reason why more recent approaches have attempted to exploit semi-automatic or automatic techn"
2020.lrec-1.706,de-melo-etal-2012-empirical,0,0.0759947,"Missing"
2020.lrec-1.706,P17-2094,1,0.837334,"ormance when senseannotated corpora are available (Raganato et al., 2017a; Huang et al., 2019). Unfortunately, obtaining such data is heavily time-consuming and expensive (Schubert, 2006), and a reasonable amount of manually-annotated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017), as well as to learn effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Mancini et al., 2017). Nevertheless, each of the aforementioned datasets come with its own format, hence making it complicated to merge them or moving from"
2020.lrec-1.706,S01-1001,0,0.305236,"irre et al., 2009; 1 For a more specific survey on corpora annotated with language-specific versions of WordNet, please refer to Petrolito and Bond (2014). 5759 Figure 1: Overview of sense inventories with their corresponding sense-annotated corpora. Zhong and Ng, 2010; Raganato et al., 2017b; Luo et al., 2018; Loureiro and Jorge, 2019; Huang et al., 2019). SemEval. SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a). In particular, the datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 task 1 (Snyder and Palmer, 2004), SemEval-2007 task 17 (Pradhan et al., 2007), SemEval-2013 task 12 (Navigli et al., 2013), and SemEval-2015 task 13 (Moro and Navigli, 2015). These datasets, which have been mainly used as evaluation benchmarks for WSD systems, contain a total of 7,253 sense annotations. MASC-WSA. The MASC Word Sense Annotation (MASC-WSA) corpus (Ide et al., 2010)is an excerpt of the Manually Annotated Sub-Corpus of American English (Ide et al., 2008, MASC) and the Open American National Corpus (Ide et al., 2002, ANC) containing annotations for 45 distinct lexemes,"
2020.lrec-1.706,eisele-chen-2010-multiun,0,0.00913245,"blogs and telephone conversation. Its 5.0 released version contains 264,622 sense annotations. OMSTI. The task of gathering sense annotations has proved expensive and not easily scalable. That is the reason why more recent approaches have attempted to exploit semi-automatic or automatic techniques. OMSTI4 (Taghipour and Ng, 2015a, One Million Sense-Tagged Instances), which is a semi-automatically constructed corpus annotated with WordNet senses, is a prominent example. It was built by exploiting the alignment-based WSD approach of Chan and Ng (2005) on a large English-Chinese parallel corpus (Eisele and Chen, 2010, MultiUN corpus). OMSTI, coupled with SemCor, has already been successfully leveraged as training data for training supervised systems (Taghipour and Ng, 2015a; Iacobacci et al., 2016; Raganato et al., 2017a). 2.2. Wikipedia Wikipedia is a collaboratively-constructed encyclopedic resource representing concepts and entities with a so-called Wikipedia article. In addition to a large coverage of concepts and entities, Wikipedia provides multilinguality, as it covers over 250 languages and these languages are connected via interlingual links. In this Section we describe 4 http://www.comp.nus.edu."
2020.lrec-1.706,K17-1008,0,0.0638254,"Missing"
2020.lrec-1.706,P16-1191,0,0.142565,"notated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017), as well as to learn effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Mancini et al., 2017). Nevertheless, each of the aforementioned datasets come with its own format, hence making it complicated to merge them or moving from one to another. Vial et al. (2018) tackled this specific problem and, following Raganato et al. (2017a), proposed UFSAC, a unified repository of several senseannotated corpora all with the same format, thus making it easier develop and test WSD models on different datasets. In this survey we present the main approaches in the literature to build sense-annotated corpora, not only for WordNet but also for multilingual sense inventories, nam"
2020.lrec-1.706,H92-1045,0,0.593098,"Missing"
2020.lrec-1.706,E12-1039,0,0.0197157,"and, following Raganato et al. (2017a), proposed UFSAC, a unified repository of several senseannotated corpora all with the same format, thus making it easier develop and test WSD models on different datasets. In this survey we present the main approaches in the literature to build sense-annotated corpora, not only for WordNet but also for multilingual sense inventories, namely Wikipedia and BabelNet. There have been additional constructing sense-annotated data for other resources such as the New Oxford American Dictionary (Yuan et al., 2016) or other language-specific versions like GermaNet (Henrich et al., 2012). While these language-specific resources are certainly relevant, in this paper we have focused on English WordNet and multilingual resources with a higher coverage like Wikipedia and BabelNet.1 Finally, we provide a general overview and statistics of these sense-annotated resources, providing relevant details across resources and languages. 2. Sense-Annotated Corpora In this Section we describe the main efforts compiling sense-annotated corpora. We present currently available corpora for three resources: WordNet (Section 2.1), Wikipedia (Section 2.2) and BabelNet (Section 2.3). Figure 1 provi"
2020.lrec-1.706,D19-1355,0,0.0619756,"Missing"
2020.lrec-1.706,P15-1010,0,0.0256727,"ch contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017), as well as to learn effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Mancini et al., 2017). Nevertheless, each of the aforementioned datasets come with its own format, hence making it complicated to merge them or moving from one to another. Vial et al. (2018) tackled this specific problem and, following Raganato et al. (2017a), proposed UFSAC, a unified repository of several senseannotated corpora all with the same format, thus making it easier develop and test WSD models on different datasets. In this survey we present the main approaches in the literature to build sense-annotated corpora, not only for WordNet but also for multili"
2020.lrec-1.706,P16-1085,0,0.236099,"al statistics of each dataset and an analysis of their specific features. Keywords: Word Sense Disambiguation, Semantics, Corpus Creation, Multilinguality 1. Introduction Word Sense Disambiguation (WSD) is a key task in Natural Language Understanding. It consists in assigning the appropriate meaning from a pre-defined sense inventory to a word in context(Navigli, 2009). While knowledge-based approaches to this task have been proposed (Agirre et al., 2014; Moro et al., 2014; Butnaru et al., 2017; Chaplot and Salakhutdinov, 2018), supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018) have been more effective in terms of performance when senseannotated corpora are available (Raganato et al., 2017a; Huang et al., 2019). Unfortunately, obtaining such data is heavily time-consuming and expensive (Schubert, 2006), and a reasonable amount of manually-annotated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several s"
2020.lrec-1.706,W02-0808,0,0.314304,"Missing"
2020.lrec-1.706,ide-etal-2008-masc,0,0.0138488,"ons have been compiled and unified by Raganato et al. (2017a). In particular, the datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 task 1 (Snyder and Palmer, 2004), SemEval-2007 task 17 (Pradhan et al., 2007), SemEval-2013 task 12 (Navigli et al., 2013), and SemEval-2015 task 13 (Moro and Navigli, 2015). These datasets, which have been mainly used as evaluation benchmarks for WSD systems, contain a total of 7,253 sense annotations. MASC-WSA. The MASC Word Sense Annotation (MASC-WSA) corpus (Ide et al., 2010)is an excerpt of the Manually Annotated Sub-Corpus of American English (Ide et al., 2008, MASC) and the Open American National Corpus (Ide et al., 2002, ANC) containing annotations for 45 distinct lexemes, i.e., lemma-pos pairs, for a total of 441 distinct WordNet word senses.2 Each word occurrence has been manually annotated on Amazon Mechanichal Turk by roughly 25 persons for a total of 1M annotations. Princeton WordNet Gloss. The Princeton WordNet Gloss Corpus3 is a sense-annotated corpus of textual definitions (glosses) from WordNet synsets. The corpus was tagged semi-automatically: 330,499 instances were annotated manually while the remaining annotations (i.e. 118,856) were"
2020.lrec-1.706,P10-2013,0,0.0201918,"benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a). In particular, the datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 task 1 (Snyder and Palmer, 2004), SemEval-2007 task 17 (Pradhan et al., 2007), SemEval-2013 task 12 (Navigli et al., 2013), and SemEval-2015 task 13 (Moro and Navigli, 2015). These datasets, which have been mainly used as evaluation benchmarks for WSD systems, contain a total of 7,253 sense annotations. MASC-WSA. The MASC Word Sense Annotation (MASC-WSA) corpus (Ide et al., 2010)is an excerpt of the Manually Annotated Sub-Corpus of American English (Ide et al., 2008, MASC) and the Open American National Corpus (Ide et al., 2002, ANC) containing annotations for 45 distinct lexemes, i.e., lemma-pos pairs, for a total of 441 distinct WordNet word senses.2 Each word occurrence has been manually annotated on Amazon Mechanichal Turk by roughly 25 persons for a total of 1M annotations. Princeton WordNet Gloss. The Princeton WordNet Gloss Corpus3 is a sense-annotated corpus of textual definitions (glosses) from WordNet synsets. The corpus was tagged semi-automatically: 330,49"
2020.lrec-1.706,W16-5307,0,0.0524791,"Missing"
2020.lrec-1.706,2005.mtsummit-papers.11,0,0.0578916,"Missing"
2020.lrec-1.706,P19-1569,0,0.124242,"Cor (Miller et al., 1993b). SemCor was manually annotated and consists of 352 documents from the Brown Corpus (Kucera and Francis, 1979) and 226,040 sense annotations. SemCor is the largest manually-annotated corpus and the most used in the literature to train WSD supervised systems (Agirre et al., 2009; 1 For a more specific survey on corpora annotated with language-specific versions of WordNet, please refer to Petrolito and Bond (2014). 5759 Figure 1: Overview of sense inventories with their corresponding sense-annotated corpora. Zhong and Ng, 2010; Raganato et al., 2017b; Luo et al., 2018; Loureiro and Jorge, 2019; Huang et al., 2019). SemEval. SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a). In particular, the datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 task 1 (Snyder and Palmer, 2004), SemEval-2007 task 17 (Pradhan et al., 2007), SemEval-2013 task 12 (Navigli et al., 2013), and SemEval-2015 task 13 (Moro and Navigli, 2015). These datasets, which have been mainly used as evaluation benchmarks for WSD systems, contain a total of 7,253 sense annotat"
2020.lrec-1.706,P18-1230,0,0.0636521,"cific features. Keywords: Word Sense Disambiguation, Semantics, Corpus Creation, Multilinguality 1. Introduction Word Sense Disambiguation (WSD) is a key task in Natural Language Understanding. It consists in assigning the appropriate meaning from a pre-defined sense inventory to a word in context(Navigli, 2009). While knowledge-based approaches to this task have been proposed (Agirre et al., 2014; Moro et al., 2014; Butnaru et al., 2017; Chaplot and Salakhutdinov, 2018), supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018) have been more effective in terms of performance when senseannotated corpora are available (Raganato et al., 2017a; Huang et al., 2019). Unfortunately, obtaining such data is heavily time-consuming and expensive (Schubert, 2006), and a reasonable amount of manually-annotated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been"
2020.lrec-1.706,K17-1012,1,0.849722,"dNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017), as well as to learn effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Mancini et al., 2017). Nevertheless, each of the aforementioned datasets come with its own format, hence making it complicated to merge them or moving from one to another. Vial et al. (2018) tackled this specific problem and, following Raganato et al. (2017a), proposed UFSAC, a unified repository of several senseannotated corpora all with the same format, thus making it easier develop and test WSD models on different datasets. In this survey we present the main approaches in the literature to build sense-annotated corpora, not only for WordNet but also for multilingual sense inventories, namely Wikipedia and Babel"
2020.lrec-1.706,K16-1006,0,0.0472775,"the reader with general statistics of each dataset and an analysis of their specific features. Keywords: Word Sense Disambiguation, Semantics, Corpus Creation, Multilinguality 1. Introduction Word Sense Disambiguation (WSD) is a key task in Natural Language Understanding. It consists in assigning the appropriate meaning from a pre-defined sense inventory to a word in context(Navigli, 2009). While knowledge-based approaches to this task have been proposed (Agirre et al., 2014; Moro et al., 2014; Butnaru et al., 2017; Chaplot and Salakhutdinov, 2018), supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018) have been more effective in terms of performance when senseannotated corpora are available (Raganato et al., 2017a; Huang et al., 2019). Unfortunately, obtaining such data is heavily time-consuming and expensive (Schubert, 2006), and a reasonable amount of manually-annotated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998"
2020.lrec-1.706,H93-1061,0,0.634377,"approaches to this task have been proposed (Agirre et al., 2014; Moro et al., 2014; Butnaru et al., 2017; Chaplot and Salakhutdinov, 2018), supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018) have been more effective in terms of performance when senseannotated corpora are available (Raganato et al., 2017a; Huang et al., 2019). Unfortunately, obtaining such data is heavily time-consuming and expensive (Schubert, 2006), and a reasonable amount of manually-annotated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli"
2020.lrec-1.706,Q14-1019,0,0.160539,"eaturing distinct lexical resources as inventory of senses, i.e. WordNet, Wikipedia, BabelNet. Furthermore, we provide the reader with general statistics of each dataset and an analysis of their specific features. Keywords: Word Sense Disambiguation, Semantics, Corpus Creation, Multilinguality 1. Introduction Word Sense Disambiguation (WSD) is a key task in Natural Language Understanding. It consists in assigning the appropriate meaning from a pre-defined sense inventory to a word in context(Navigli, 2009). While knowledge-based approaches to this task have been proposed (Agirre et al., 2014; Moro et al., 2014; Butnaru et al., 2017; Chaplot and Salakhutdinov, 2018), supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018) have been more effective in terms of performance when senseannotated corpora are available (Raganato et al., 2017a; Huang et al., 2019). Unfortunately, obtaining such data is heavily time-consuming and expensive (Schubert, 2006), and a reasonable amount of manually-annotated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannota"
2020.lrec-1.706,S13-2040,0,0.145556,"Missing"
2020.lrec-1.706,L16-1483,0,0.0229738,"otated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017), as well as to learn effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Mancini et al., 2017). Nevertheless, each of the aforementioned datasets come with its own format, hence making it complicated to merge them or moving from one to another. Vial et al. (2018) tackled this specific problem and, following Raganato et al. (2017a), proposed UFSAC, a unified repository of several senseannotated corpora all with the same format, thus making it easier devel"
2020.lrec-1.706,D17-1008,1,0.798937,"t attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017), as well as to learn effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Mancini et al., 2017). Nevertheless, each of the aforementioned datasets come with its own format, hence making it complicated to merge them or moving from one to another. Vial et al. (2018) tackled this specific problem and, following Raganato et al. (2017a), proposed UFSAC, a unified repository of several senseannotated corpora all with the same format, thus making it easier develop and test WSD models on different datasets. In this survey we present the main approaches in"
2020.lrec-1.706,L18-1268,1,0.916813,"Missing"
2020.lrec-1.706,W14-0132,0,0.0656519,"follows we list the main WordNet sense-annotated corpora, using WordNet 3.0 as reference sense inventory. SemCor. The first and most prominent example of senseannotated corpora is SemCor (Miller et al., 1993b). SemCor was manually annotated and consists of 352 documents from the Brown Corpus (Kucera and Francis, 1979) and 226,040 sense annotations. SemCor is the largest manually-annotated corpus and the most used in the literature to train WSD supervised systems (Agirre et al., 2009; 1 For a more specific survey on corpora annotated with language-specific versions of WordNet, please refer to Petrolito and Bond (2014). 5759 Figure 1: Overview of sense inventories with their corresponding sense-annotated corpora. Zhong and Ng, 2010; Raganato et al., 2017b; Luo et al., 2018; Loureiro and Jorge, 2019; Huang et al., 2019). SemEval. SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a). In particular, the datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 task 1 (Snyder and Palmer, 2004), SemEval-2007 task 17 (Pradhan et al., 2007), SemEval-2013 task 12 (Navigli et al.,"
2020.lrec-1.706,P13-1132,0,0.0287373,"ed corpus of textual definitions (glosses) from WordNet synsets. The corpus was tagged semi-automatically: 330,499 instances were annotated manually while the remaining annotations (i.e. 118,856) were obtained automatically. This corpus of disambiguated glosses has already proved to be useful in 2 This corpus has also been annotated with other resources, such as FrameNet (Baker et al., 1998). A comparison between the sense annotations of WordNet and lexical units of FrameNet is provided in De Melo et al. (2012). 3 http://wordnet.princeton.edu/glosstag. shtml tasks such as semantic similarity (Pilehvar et al., 2013), domain labeling (Gonz´alez et al., 2012) and Word Sense Disambiguation (Baldwin et al., 2008; Agirre and Soroa, 2009; Camacho-Collados et al., 2015). OntoNotes. OntoNotes (Weischedel et al., 2013) is a corpus from the Linguistic Data Consortium which comprises different kinds of explicitly-tagged syntactic and semantic information, including annotations at the sense level. The OntoNotes corpus consists of documents from diverse genres such as news, weblogs and telephone conversation. Its 5.0 released version contains 264,622 sense annotations. OMSTI. The task of gathering sense annotations h"
2020.lrec-1.706,S07-1016,0,0.0347076,"rsions of WordNet, please refer to Petrolito and Bond (2014). 5759 Figure 1: Overview of sense inventories with their corresponding sense-annotated corpora. Zhong and Ng, 2010; Raganato et al., 2017b; Luo et al., 2018; Loureiro and Jorge, 2019; Huang et al., 2019). SemEval. SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a). In particular, the datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 task 1 (Snyder and Palmer, 2004), SemEval-2007 task 17 (Pradhan et al., 2007), SemEval-2013 task 12 (Navigli et al., 2013), and SemEval-2015 task 13 (Moro and Navigli, 2015). These datasets, which have been mainly used as evaluation benchmarks for WSD systems, contain a total of 7,253 sense annotations. MASC-WSA. The MASC Word Sense Annotation (MASC-WSA) corpus (Ide et al., 2010)is an excerpt of the Manually Annotated Sub-Corpus of American English (Ide et al., 2008, MASC) and the Open American National Corpus (Ide et al., 2002, ANC) containing annotations for 45 distinct lexemes, i.e., lemma-pos pairs, for a total of 441 distinct WordNet word senses.2 Each word occurr"
2020.lrec-1.706,E17-1010,1,0.936724,"on Word Sense Disambiguation (WSD) is a key task in Natural Language Understanding. It consists in assigning the appropriate meaning from a pre-defined sense inventory to a word in context(Navigli, 2009). While knowledge-based approaches to this task have been proposed (Agirre et al., 2014; Moro et al., 2014; Butnaru et al., 2017; Chaplot and Salakhutdinov, 2018), supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018) have been more effective in terms of performance when senseannotated corpora are available (Raganato et al., 2017a; Huang et al., 2019). Unfortunately, obtaining such data is heavily time-consuming and expensive (Schubert, 2006), and a reasonable amount of manually-annotated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts te"
2020.lrec-1.706,D17-1120,0,0.0299212,"Missing"
2020.lrec-1.706,P19-1069,1,0.900748,"Missing"
2020.lrec-1.706,W04-0811,0,0.0537943,"y on corpora annotated with language-specific versions of WordNet, please refer to Petrolito and Bond (2014). 5759 Figure 1: Overview of sense inventories with their corresponding sense-annotated corpora. Zhong and Ng, 2010; Raganato et al., 2017b; Luo et al., 2018; Loureiro and Jorge, 2019; Huang et al., 2019). SemEval. SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a). In particular, the datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 task 1 (Snyder and Palmer, 2004), SemEval-2007 task 17 (Pradhan et al., 2007), SemEval-2013 task 12 (Navigli et al., 2013), and SemEval-2015 task 13 (Moro and Navigli, 2015). These datasets, which have been mainly used as evaluation benchmarks for WSD systems, contain a total of 7,253 sense annotations. MASC-WSA. The MASC Word Sense Annotation (MASC-WSA) corpus (Ide et al., 2010)is an excerpt of the Manually Annotated Sub-Corpus of American English (Ide et al., 2008, MASC) and the Open American National Corpus (Ide et al., 2002, ANC) containing annotations for 45 distinct lexemes, i.e., lemma-pos pairs, for a total of 441 di"
2020.lrec-1.706,K15-1037,0,0.355055,"more effective in terms of performance when senseannotated corpora are available (Raganato et al., 2017a; Huang et al., 2019). Unfortunately, obtaining such data is heavily time-consuming and expensive (Schubert, 2006), and a reasonable amount of manually-annotated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017), as well as to learn effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Mancini et al., 2017). Nevertheless, each of the aforementioned datasets come with its own format, hence making it complicated to"
2020.lrec-1.706,N15-1035,0,0.0577105,"more effective in terms of performance when senseannotated corpora are available (Raganato et al., 2017a; Huang et al., 2019). Unfortunately, obtaining such data is heavily time-consuming and expensive (Schubert, 2006), and a reasonable amount of manually-annotated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017), as well as to learn effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Mancini et al., 2017). Nevertheless, each of the aforementioned datasets come with its own format, hence making it complicated to"
2020.lrec-1.706,L18-1166,0,0.0169028,"igli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017), as well as to learn effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Mancini et al., 2017). Nevertheless, each of the aforementioned datasets come with its own format, hence making it complicated to merge them or moving from one to another. Vial et al. (2018) tackled this specific problem and, following Raganato et al. (2017a), proposed UFSAC, a unified repository of several senseannotated corpora all with the same format, thus making it easier develop and test WSD models on different datasets. In this survey we present the main approaches in the literature to build sense-annotated corpora, not only for WordNet but also for multilingual sense inventories, namely Wikipedia and BabelNet. There have been additional constructing sense-annotated data for other resources such as the New Oxford American Dictionary (Yuan et al., 2016) or other language-sp"
2020.lrec-1.706,C16-1130,0,0.0138951,"only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from WordNet (Fellbaum, 1998). Since then, several semi-automatic and automatic approaches have also been proposed (Taghipour and Ng, 2015a; Delli Bovi et al., 2017; Pasini and Navigli, 2018). These automatic efforts tend to produce noisier annotations, but their coverage has been shown to lead to better supervised and semi-supervised WSD systems (Taghipour and Ng, 2015b; Otegi et al., 2016; Raganato et al., 2016; Yuan et al., 2016; Delli Bovi et al., 2017; Pasini and Navigli, 2017), as well as to learn effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Mancini et al., 2017). Nevertheless, each of the aforementioned datasets come with its own format, hence making it complicated to merge them or moving from one to another. Vial et al. (2018) tackled this specific problem and, following Raganato et al. (2017a), proposed UFSAC, a unified repository of several senseannotated corpora all with the same format, thus making it easier develop and test WSD models on different datase"
2020.lrec-1.706,P10-4014,0,0.721719,"thermore, we provide the reader with general statistics of each dataset and an analysis of their specific features. Keywords: Word Sense Disambiguation, Semantics, Corpus Creation, Multilinguality 1. Introduction Word Sense Disambiguation (WSD) is a key task in Natural Language Understanding. It consists in assigning the appropriate meaning from a pre-defined sense inventory to a word in context(Navigli, 2009). While knowledge-based approaches to this task have been proposed (Agirre et al., 2014; Moro et al., 2014; Butnaru et al., 2017; Chaplot and Salakhutdinov, 2018), supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018) have been more effective in terms of performance when senseannotated corpora are available (Raganato et al., 2017a; Huang et al., 2019). Unfortunately, obtaining such data is heavily time-consuming and expensive (Schubert, 2006), and a reasonable amount of manually-annotated instances are available for English only (Miller et al., 1993a). One of the first attempts towards building large senseannotated corpora was SemCor (Miller et al., 1993a) which contains instances annotated with senses from W"
2020.smm4h-1.12,S19-2007,0,0.0123332,"ers. SVMs have demonstrated effectiveness when used with Twitter datasets in healthcare contexts (Prieto et al., 2014; Han et al., 2020). For our experiments we used both a standard SVM classifier with TF-IDF features and a classifier based on the average of word embeddings within the tweet. With regards to pre-trained LMs, we used BERT (Bidirectional Encoder Representations from Transformers) and ALBERT (A Lite BERT) (Lan et al., 2019). These LMs have been deployed effectively in NLP tasks, leading to state-of-the-art results in most standard benchmarks (Wang et al., 2019) including Twitter (Basile et al., 2019; Roitero et al., 2020). In particular, ALBERT has been shown to provide competitive results despite being relatively light-weight compared to other LMs. Finally, for completeness we added a na¨ıve baseline that predicts positive instances in all cases. 4.1.3 Training details We used the scikit-learn SVM model (Pedregosa et al., 2011) as well as its TF-IDF (Term FrequencyInverse Document Frequency) vectorizer implementations.6 The word embeddings generated for each tweet were drawn from vectors trained on Twitter data (Pennington et al., 2014, GloVe). These vectors had a dimensionality of 200,"
2020.smm4h-1.12,N19-1423,0,0.011368,"at can automatically detect and flag such cases at a large-scale are highly desirable (Guntuku et al., 2017). They may enable prompt analysis and treatment, which is crucial in the early development of such conditions. Moreover, the interpersonal and economic effects of these illnesses may be mitigated with prompt intervention (Lexis et al., 2011). In this paper, we build a classification dataset2 to assist in the detection of depression and anxiety in Twitter, and compare several text classification baselines. The results show that state-of-the-art language models (LMs henceforth) like BERT (Devlin et al., 2019) unsurprisingly outperform competing This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 https://www.bupa.co.uk/newsroom/ourviews/2017/10/anxiety-depression 2 The datasets and code used in our experiments are available online at the following repository: https://bitbucket.org/nlpcardiff/preemptive-depression-anxiety-twitter. 82 th Proceedings of the 5 Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task, pages 82–89 Barcelona, Spain (Online), December 12, 2020. baselines. Ho"
2020.smm4h-1.12,S16-1003,0,0.0342178,"training sets (Table 4), SVM with word embeddings features perform similarly to BERT, being in fact slightly better overall. This result is perhaps surprising, but may be due to the relative robustness of SVMs with respect to unbalanced training sets, which seem to have a greater effect on the LMs. Another explanation may be that by concatenating TF-IDF features and word embeddings the classifier is effectively leveraging both global and local dependencies, which have been shown to be crucial in tweet classification tasks such as emoji prediction (Barbieri et al., 2018) and stance detection (Mohammad et al., 2016). More generally, the results in this setting are not hugely different from the first setting’s. This is encouraging, as it suggests that supervised models can also perform in a more realistic setting where the negative instances are more prevalent than the positive ones. 4.3 Analysis Perhaps the main highlights of our experiments are the results obtained by BERT and the concatenation of TF-IDF features and word embeddings in SVMs. BERT performs remarkably well despite not being trained on Twitter data. This could suggest that, although slang, jargon, misspellings, and emoji are typical in mic"
2020.smm4h-1.12,D14-1162,0,0.0834128,"ndard benchmarks (Wang et al., 2019) including Twitter (Basile et al., 2019; Roitero et al., 2020). In particular, ALBERT has been shown to provide competitive results despite being relatively light-weight compared to other LMs. Finally, for completeness we added a na¨ıve baseline that predicts positive instances in all cases. 4.1.3 Training details We used the scikit-learn SVM model (Pedregosa et al., 2011) as well as its TF-IDF (Term FrequencyInverse Document Frequency) vectorizer implementations.6 The word embeddings generated for each tweet were drawn from vectors trained on Twitter data (Pennington et al., 2014, GloVe). These vectors had a dimensionality of 200, and so did the averaged embedding generated. We performed tweet text preprocessing prior to their input to the SVM. In one series of SVM experiments all tweets underwent tokenization and lowercasing only, but in a second series all tweets also underwent tweet specific preprocessing7 (SVM+preproc henceforth). The preprocessing entailed the removal of hashtags, user mentions, reserved words (such as “RT” and “FAV”), emojis, and smileys. This enabled us to see how the presence of these common tweet features affected classification performance."
2020.smm4h-1.12,D17-1322,0,0.0976479,"including self-harm (Centers for Disease Control and Prevention, 2015), making timely diagnosis and treatment even more essential. However, sufferers of depression and anxiety can find that it takes great courage and strength to seek professional treatment (Dennis C Miller, 2016). They may also be afraid to confide in their peers due to the stigma of mental illness (Wasserman et al., 2012). With reluctance to seek professional treatment or rely on their peers, sufferers often turn to online resources for support. These include both specialised and general communities, with Twitter and Reddit (Yates et al., 2017) being paramount examples of the latter. Because of this, systems that can automatically detect and flag such cases at a large-scale are highly desirable (Guntuku et al., 2017). They may enable prompt analysis and treatment, which is crucial in the early development of such conditions. Moreover, the interpersonal and economic effects of these illnesses may be mitigated with prompt intervention (Lexis et al., 2011). In this paper, we build a classification dataset2 to assist in the detection of depression and anxiety in Twitter, and compare several text classification baselines. The results sho"
2021.acl-long.280,Q16-1028,0,0.0118848,"ueen). The motivation for this task dates back to the connectionism theory (Feldman and Ballard, 1982) in cognitive science. In particular, neural networks were thought to be able to model emergent concepts (Hopfield, 1982; Hinton, 1986) by learning distributed representations across an embedding space (Hinton et al., 1986), similar to the properties that word embeddings displayed in the analogy task. More recent works have proposed new mathematical theories and experiments to understand the analogical capabilities of word embeddings, attempting to understand their linear algebraic structure (Arora et al., 2016; Gittens et al., 2017; Allen and Hospedales, 2019) or by explicitly studying their compositional nature (Levy and 3610 Goldberg, 2014; Paperno and Baroni, 2016; Ethayarajh et al., 2019; Chiang et al., 2020). However, recent works have questioned the impressive results displayed by word embeddings in this task. In many cases simple baselines excluding the input pair (or query) were competitive (Linzen, 2016). Simultaneously, some researchers have found that many relationships may not be retrieved in the embedding space by simple linear transformations (Drozd et al., 2016; Bouraoui et al., 2018"
2021.acl-long.280,Q17-1010,0,0.0288523,"mPPL with the default configuration. Average accuracy (Avg) across datasets is included in the last column. sPMI and smPPL . Possible values for each hyperparameter (including the selection of six prompts and an ablation test on the scoring function) and the best configurations that were found by grid search are provided in the appendix. As baseline methods, we also consider three pre-trained word embedding models, which have been shown to provide competitive results in analogy tasks, as explained in Section 2.2: Word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). For the word embedding models, we simply represent word pairs by taking the difference between their embeddings4 . We then choose the answer candidate with the highest cosine similarity to the query in terms of this vector difference. To put the results into context, we also include two simple statistical baselines. First, we report the expected random performance. Second, we use a method based on each word pair’s PMI in a given corpus. We then select the answer candidate with the highest 4 Vector differences have been found to be the most robust encoding method in the context of word analog"
2021.acl-long.280,2020.conll-1.9,1,0.724661,"s (Hopfield, 1982; Hinton, 1986) by learning distributed representations across an embedding space (Hinton et al., 1986), similar to the properties that word embeddings displayed in the analogy task. More recent works have proposed new mathematical theories and experiments to understand the analogical capabilities of word embeddings, attempting to understand their linear algebraic structure (Arora et al., 2016; Gittens et al., 2017; Allen and Hospedales, 2019) or by explicitly studying their compositional nature (Levy and 3610 Goldberg, 2014; Paperno and Baroni, 2016; Ethayarajh et al., 2019; Chiang et al., 2020). However, recent works have questioned the impressive results displayed by word embeddings in this task. In many cases simple baselines excluding the input pair (or query) were competitive (Linzen, 2016). Simultaneously, some researchers have found that many relationships may not be retrieved in the embedding space by simple linear transformations (Drozd et al., 2016; Bouraoui et al., 2018) and others argued that the standard evaluation procedure has limitations (Schluter, 2018). New datasets and measures have also been introduced to address some of these issues (Gladkova et al., 2016; Fourni"
2021.acl-long.280,J90-1003,0,0.291793,"y in terms of this vector difference. To put the results into context, we also include two simple statistical baselines. First, we report the expected random performance. Second, we use a method based on each word pair’s PMI in a given corpus. We then select the answer candidate with the highest 4 Vector differences have been found to be the most robust encoding method in the context of word analogies (Hakami and Bollegala, 2017). PMI as the prediction. Note that the query word pair is completely ignored in this case. This PMI score is the well-known word-pair association metric introduced by Church and Hanks (1990) for lexicographic purposes (specifically, collocation extraction), which compares the probability of observing two words together with the probabilities of observing them independently (chance). The PMI scores in our experiments were computed using the English Wikipedia with a fixed window size 10. 5.2 Results Table 3 shows our main results. As far as the comparison among LMs is concerned, RoBERTa and GPT-2 consistently outperform BERT. Among the AP variants, smPPL achieves substantially better results than sPMI or sPPL in most cases. We also observe that word embeddings perform surprisingly"
2021.acl-long.280,D19-1109,0,0.0265794,"nal and marginal log-likelihood. In our case, we consider the conditional likelihood of ti given hi and the query pair (recall from Section 3.1 that h and t represent the head and tail of a given word pair, respectively), i.e. P (ti |hq , tq , hi ), and the marginal likelihood over hi , i.e. P (ti |hq , tq ). Subsequently, the PMI-inspired scoring function is defined as r(ti |hi , hq , tq ) = log P (ti |hi , hq , tq ) − α · log P (ti |hq , tq ) (2) where α is a hyperparameter to control the effect of the marginal likelihood. The PMI score corresponds to the specific case where α = 1. However, Davison et al. (2019) found that using a hyperparameter to balance the impact of the conditional and marginal probabilities can significantly improve the results. The probabilities in (2) are estimated by assuming that the answer candidates are the only possible word pairs that need to be considered. By relying on this closed-world assumption, we can estimate marginal probabilities based on perplexity, which we found to give better results than the masking based strategy from Davison et al. (2019). In particular, we estimate these probabilities as 3613 P (ti |hq , tq , hi ) = − f (Tt (hq , tq , hi , ti )) n P f (T"
2021.acl-long.280,N19-1423,0,0.431759,"ell as two hyperparameters in our scoring function, with the optimal choices not being consistent across different datasets. Moreover, using BERT leads to considerably weaker results, underperforming even standard word embeddings in all of the considered configurations. These findings suggest that while transformer-based LMs learn relational knowledge to a meaningful extent, more work is needed to understand how such knowledge is encoded, and how it can be exploited. 2 2.1 Related work Understanding Pre-trained LMs Since their recent dominance in standard NLP benchmarks (Peters et al., 2018a; Devlin et al., 2019; Liu et al., 2019), pre-trained language models have been extensively studied. This has mainly been done through probing tasks, which are aimed at understanding the knowledge that is implicitly captured by their parameters. After the initial focus on understanding pre-trained LSTM-based LMs (Peters et al., 2018b), attention has now shifted toward transformer-based models. The main aspects that have been studied in recent years are syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 2019b) and semantics (Ett"
2021.acl-long.280,C18-1138,1,0.830796,"e (Arora et al., 2016; Gittens et al., 2017; Allen and Hospedales, 2019) or by explicitly studying their compositional nature (Levy and 3610 Goldberg, 2014; Paperno and Baroni, 2016; Ethayarajh et al., 2019; Chiang et al., 2020). However, recent works have questioned the impressive results displayed by word embeddings in this task. In many cases simple baselines excluding the input pair (or query) were competitive (Linzen, 2016). Simultaneously, some researchers have found that many relationships may not be retrieved in the embedding space by simple linear transformations (Drozd et al., 2016; Bouraoui et al., 2018) and others argued that the standard evaluation procedure has limitations (Schluter, 2018). New datasets and measures have also been introduced to address some of these issues (Gladkova et al., 2016; Fournier et al., 2020). Finally, in the context of bias detection, for which analogies have been used as a proxy (Bolukbasi et al., 2016), it has also been found that word analogies may misguide or hide the real relationships existing in the vector space (Gonen and Goldberg, 2019; Nissim et al., 2020). As far as language models are concerned, word analogies have not been explored to the same exten"
2021.acl-long.280,C16-1332,0,0.0188537,"r algebraic structure (Arora et al., 2016; Gittens et al., 2017; Allen and Hospedales, 2019) or by explicitly studying their compositional nature (Levy and 3610 Goldberg, 2014; Paperno and Baroni, 2016; Ethayarajh et al., 2019; Chiang et al., 2020). However, recent works have questioned the impressive results displayed by word embeddings in this task. In many cases simple baselines excluding the input pair (or query) were competitive (Linzen, 2016). Simultaneously, some researchers have found that many relationships may not be retrieved in the embedding space by simple linear transformations (Drozd et al., 2016; Bouraoui et al., 2018) and others argued that the standard evaluation procedure has limitations (Schluter, 2018). New datasets and measures have also been introduced to address some of these issues (Gladkova et al., 2016; Fournier et al., 2020). Finally, in the context of bias detection, for which analogies have been used as a proxy (Bolukbasi et al., 2016), it has also been found that word analogies may misguide or hide the real relationships existing in the vector space (Gonen and Goldberg, 2019; Nissim et al., 2020). As far as language models are concerned, word analogies have not been ex"
2021.acl-long.280,D15-1075,0,0.0207254,"lts in the top row correspond to the full model without masking. figuration and tuning on these artificially-modified datasets.As can be seen in Table 5, a non-trivial performance is achieved for all datasets, which suggests that the words from the answer pair tend to be more similar to the words from the query than the words from negative examples. 7 Figure 4: Test accuracy in U2 and U4 per difficulty level. LMs use smPPL with the best configuration tuned in the corresponding validation sets. Hypothesis Only Recently, several researchers have found that standard NLP benchmarks, such as SNLI (Bowman et al., 2015) for language inference, contain several annotation artifacts that makes the task simpler for automatic models (Poliak et al., 2018; Gururangan et al., 2018). One of their most relevant findings is that models which do not even consider the premise can reach high accuracy. More generally, these issues have been found to be problematic in NLP models (Linzen, 2020) and neural networks more generally (Geirhos et al., 2020). According to the results shown in Table 3, we already found that the PMI baseline achieved a non-trivial performance, even outperforming BERT in a few settings and datasets. T"
2021.acl-long.280,P19-1315,0,0.0728109,"elf-explanatory, this is a small note explaining it. BERT is to NLP what AlexNet is to CV is making an analogy on what the BERT and AlexNet models represented for Natural Language Processing (NLP) and Computer Vision (CV), respectively. They both brought a paradigm shift in how research was undertaken in their corresponding disciplines and this is what the analogy refers to. 1 Source code and data to reproduce our experimental results are available in the following repository: https://github.com/asahi417/ analogy-language-model et al., 2013a; Vylomova et al., 2016; Allen and Hospedales, 2019; Ethayarajh et al., 2019). The underlying assumption is that when “a is to b what c is to d” the word vector differences b − a and d − c are expected to be similar, where we write x for the embedding of a word x. While this assumption holds for some types of syntactic relations, for semantic relations this holds to a much more limited degree than was suggested in early work (Linzen, 2016; Schluter, 2018). Moreover, the most commonly used benchmarks have focused on specific and well-defined semantic relations such as “capital of”, rather than the more abstract notion of relational similarity that is often needed for so"
2021.acl-long.280,W16-2503,0,0.0691785,"1 Source code and data to reproduce our experimental results are available in the following repository: https://github.com/asahi417/ analogy-language-model et al., 2013a; Vylomova et al., 2016; Allen and Hospedales, 2019; Ethayarajh et al., 2019). The underlying assumption is that when “a is to b what c is to d” the word vector differences b − a and d − c are expected to be similar, where we write x for the embedding of a word x. While this assumption holds for some types of syntactic relations, for semantic relations this holds to a much more limited degree than was suggested in early work (Linzen, 2016; Schluter, 2018). Moreover, the most commonly used benchmarks have focused on specific and well-defined semantic relations such as “capital of”, rather than the more abstract notion of relational similarity that is often needed for solving the kind of psychometric analogy problems that can be found in IQ tests and educational settings. An example of such a problem is shown in Table 1. Given the central role of analogy in human cognition, it is nonetheless important to understand the extent to which NLP models are able to solve these more abstract analogy problems. Besides its value as an intr"
2021.acl-long.280,2020.acl-main.465,0,0.0110037,"uracy in U2 and U4 per difficulty level. LMs use smPPL with the best configuration tuned in the corresponding validation sets. Hypothesis Only Recently, several researchers have found that standard NLP benchmarks, such as SNLI (Bowman et al., 2015) for language inference, contain several annotation artifacts that makes the task simpler for automatic models (Poliak et al., 2018; Gururangan et al., 2018). One of their most relevant findings is that models which do not even consider the premise can reach high accuracy. More generally, these issues have been found to be problematic in NLP models (Linzen, 2020) and neural networks more generally (Geirhos et al., 2020). According to the results shown in Table 3, we already found that the PMI baseline achieved a non-trivial performance, even outperforming BERT in a few settings and datasets. This suggests that several implausible negative examples are included in the analogy datasets. As a further exploration of such artifacts, here we analyse the analogue of a hypothesis-only baseline. In particular, for this analysis, we masked the head or tail of the candidate answer in all evaluation instances. Then, we test the masked language models with the sam"
2021.acl-long.280,2021.ccl-1.108,0,0.0621669,"Missing"
2021.acl-long.280,D14-1162,0,0.0955387,"val1 , and t = to-as. Note that sPPL = smPPL with the default configuration. Average accuracy (Avg) across datasets is included in the last column. sPMI and smPPL . Possible values for each hyperparameter (including the selection of six prompts and an ablation test on the scoring function) and the best configurations that were found by grid search are provided in the appendix. As baseline methods, we also consider three pre-trained word embedding models, which have been shown to provide competitive results in analogy tasks, as explained in Section 2.2: Word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). For the word embedding models, we simply represent word pairs by taking the difference between their embeddings4 . We then choose the answer candidate with the highest cosine similarity to the query in terms of this vector difference. To put the results into context, we also include two simple statistical baselines. First, we report the expected random performance. Second, we use a method based on each word pair’s PMI in a given corpus. We then select the answer candidate with the highest 4 Vector differences have been found to be the most robust encod"
2021.acl-long.280,N18-1202,0,0.0319787,"ce of the prompt, as well as two hyperparameters in our scoring function, with the optimal choices not being consistent across different datasets. Moreover, using BERT leads to considerably weaker results, underperforming even standard word embeddings in all of the considered configurations. These findings suggest that while transformer-based LMs learn relational knowledge to a meaningful extent, more work is needed to understand how such knowledge is encoded, and how it can be exploited. 2 2.1 Related work Understanding Pre-trained LMs Since their recent dominance in standard NLP benchmarks (Peters et al., 2018a; Devlin et al., 2019; Liu et al., 2019), pre-trained language models have been extensively studied. This has mainly been done through probing tasks, which are aimed at understanding the knowledge that is implicitly captured by their parameters. After the initial focus on understanding pre-trained LSTM-based LMs (Peters et al., 2018b), attention has now shifted toward transformer-based models. The main aspects that have been studied in recent years are syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 201"
2021.acl-long.280,D18-1179,0,0.0229833,"ce of the prompt, as well as two hyperparameters in our scoring function, with the optimal choices not being consistent across different datasets. Moreover, using BERT leads to considerably weaker results, underperforming even standard word embeddings in all of the considered configurations. These findings suggest that while transformer-based LMs learn relational knowledge to a meaningful extent, more work is needed to understand how such knowledge is encoded, and how it can be exploited. 2 2.1 Related work Understanding Pre-trained LMs Since their recent dominance in standard NLP benchmarks (Peters et al., 2018a; Devlin et al., 2019; Liu et al., 2019), pre-trained language models have been extensively studied. This has mainly been done through probing tasks, which are aimed at understanding the knowledge that is implicitly captured by their parameters. After the initial focus on understanding pre-trained LSTM-based LMs (Peters et al., 2018b), attention has now shifted toward transformer-based models. The main aspects that have been studied in recent years are syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 201"
2021.acl-long.280,D19-1250,0,0.050632,"Missing"
2021.acl-long.280,S18-2023,0,0.0539834,"Missing"
2021.acl-long.280,N19-1329,0,0.0237757,"elated work Understanding Pre-trained LMs Since their recent dominance in standard NLP benchmarks (Peters et al., 2018a; Devlin et al., 2019; Liu et al., 2019), pre-trained language models have been extensively studied. This has mainly been done through probing tasks, which are aimed at understanding the knowledge that is implicitly captured by their parameters. After the initial focus on understanding pre-trained LSTM-based LMs (Peters et al., 2018b), attention has now shifted toward transformer-based models. The main aspects that have been studied in recent years are syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 2019b) and semantics (Ettinger, 2019; Tenney et al., 2019a). For a more complete overview on analyses of the different properties of transformer-based LMs, we refer to Rogers et al. (2021). Despite the rise in probing analyses for LMs and the importance of analogical reasoning in human cognition, understanding the analogical capabilities of LMs remains understudied. The most similar works have focused on capturing relational knowledge from LMs (in particular the type of information available in knowled"
2021.acl-long.280,D19-1592,0,0.0552657,"Missing"
2021.acl-long.280,2020.emnlp-demos.6,0,0.0787475,"Missing"
2021.acl-long.280,N18-2039,0,0.110701,"and data to reproduce our experimental results are available in the following repository: https://github.com/asahi417/ analogy-language-model et al., 2013a; Vylomova et al., 2016; Allen and Hospedales, 2019; Ethayarajh et al., 2019). The underlying assumption is that when “a is to b what c is to d” the word vector differences b − a and d − c are expected to be similar, where we write x for the embedding of a word x. While this assumption holds for some types of syntactic relations, for semantic relations this holds to a much more limited degree than was suggested in early work (Linzen, 2016; Schluter, 2018). Moreover, the most commonly used benchmarks have focused on specific and well-defined semantic relations such as “capital of”, rather than the more abstract notion of relational similarity that is often needed for solving the kind of psychometric analogy problems that can be found in IQ tests and educational settings. An example of such a problem is shown in Table 1. Given the central role of analogy in human cognition, it is nonetheless important to understand the extent to which NLP models are able to solve these more abstract analogy problems. Besides its value as an intrinsic benchmark f"
2021.acl-long.280,2020.emnlp-main.346,0,0.0327053,"tonyms, synonyms, meronyms and hyponyms. 3612 and returns a sentence in which the placeholders were replaced by the words w1 , w2 , w3 , and w4 . For instance, given a query “word:language” and a candidate “note:music”, the prompting function produces Tto-as (“word”, “language”, “note”, “music”) = “word is to language as note is to music” where we use the template type to-as here. Using manually specified template types can result in a sub-optimal textual representation. For this reason, recent studies have proposed autoprompting strategies, which optimize the template type on a training set (Shin et al., 2020), paraphrasing (Jiang et al., 2020), additional prompt generation model (Gao et al., 2020), and corpus-driven template mining (Bouraoui et al., 2020). However, none of these approaches can be applied to unsupervised settings. Thus, we do not explore auto-prompting methods in this work. Instead, we will consider a number of different template types in the experiments, and assess the sensitivity of the results to the choice of template type. 4.2 Scoring Function Perplexity. We first define perplexity, which is widely used as a sentence re-ranking metric (Chan et al., 2016; Gulcehre et al., 2015)"
2021.acl-srw.13,Q17-1010,0,0.0107468,"types of lexical features: (1) Frequency features based on TF-IDF (TF)5 ; (2) semantic based on the average of word embeddings6 within the tweet (WE); and (3) the extra-linguistic features listed in Table 1 (EL). Models. As linear machine learning models exploiting the features, we used both Naive Bayes (as a baseline model) and SVM (as a non Deep Neural Network option) classifiers following their default implementations in scikit-learn. Moreover, 5 We considered the 500 most frequent words for the evaluation. 6 As pre-trained words embeddings, we used the 100dimensional fasttext embeddings (Bojanowski et al., 2017) trained on Twitter from Camacho-Collados et al. (2020). a Convolutional Neural Network (CNN) was implemented. Even though CNNs have been traditionally used in computer vision, they have proved to be effective for various NLP tasks, including text classification (Kim, 2014). In the present work, we trained a CNN with three layers of convolution using the same Twitter pre-trained word embeddings as initialisation. All models were evaluated using 10-fold cross validation. Finally, as current state-of-the-art NLP system we trained the base uncased version of BERT (Devlin et al., 2018) on our data"
2021.acl-srw.13,D14-1181,0,0.00494164,"both Naive Bayes (as a baseline model) and SVM (as a non Deep Neural Network option) classifiers following their default implementations in scikit-learn. Moreover, 5 We considered the 500 most frequent words for the evaluation. 6 As pre-trained words embeddings, we used the 100dimensional fasttext embeddings (Bojanowski et al., 2017) trained on Twitter from Camacho-Collados et al. (2020). a Convolutional Neural Network (CNN) was implemented. Even though CNNs have been traditionally used in computer vision, they have proved to be effective for various NLP tasks, including text classification (Kim, 2014). In the present work, we trained a CNN with three layers of convolution using the same Twitter pre-trained word embeddings as initialisation. All models were evaluated using 10-fold cross validation. Finally, as current state-of-the-art NLP system we trained the base uncased version of BERT (Devlin et al., 2018) on our dataset using the implementation provided in Simple Transformers (Rajapakse, 2019). 3.2 Results Table 5 shows the results of the classification models in our collected dataset. As expected, the CNN and BERT models perform better with BERT attaining the best results, with an ove"
2021.eacl-demos.7,2020.acl-main.536,0,0.0427237,"distant languages and do not share the alphabet. Second, Arabic also transfers well to Spanish which, despite the Arabic (lexical) influence on the Spanish language (Stewart et al., 1999), are still languages from distant families. Clearly, this is a shallow cross-lingual analysis, but it highlights the possibilities of our library for research in cross-lingual NER. Recently, (Hu et al., 2020a) proposed a compilation of multilingual benchmark tasks including the WikiAnn datasets as a part of it, and XLM-R proved to be a strong baseline on multilingual NER. This is in line with the results of Conneau et al. (2020), which showed a high capacity of zero-shot cross-lingual transferability. On this respect, Pfeiffer et al. (2020b) proposed a language/task specific adapter module that can further improve cross-lingual adaptation in NER. Given the possibilities and recent advances in cross-lingual language models in recent years, we expect our library to help practitioners to experiment and test these advances in NER. ar 53.2 55.4 54.9 57.2 62.1 90.3 Table 4: Cross-lingual type-aware F1 results on various languages for the WikiAnn dataset. that are more easily transferable, such as wnut and conll. The wnut-t"
2021.eacl-demos.7,W03-0419,0,0.454524,"Missing"
2021.eacl-main.140,W09-2420,0,0.113846,"Missing"
2021.eacl-main.140,N19-1423,0,0.0634982,"Missing"
2021.eacl-main.140,D12-1129,0,0.0223218,"el to (1) disambiguate the word in context without an external sense inventory, (2) deal with unseen instances and incomplete data, and (3) transfer the intrinsic knowledge (gained on general domain data) into a specific domain. 2 Related Work Word Sense Disambiguation. The task of WSD consists of associating a word in context with its most appropriate entry in a given sense inventory (Navigli, 2009), e.g., WordNet. For WSD there are many associated datasets (Raganato et al., 2017; Vial et al., 2018; R¨oder et al., 2018; Ling et al., 2015), including domain-specific ones (Agirre et al., 2009; Faralli and Navigli, 2012). The main difference between WSD and its re-formulation TSV is that for TSV the availability of a sense inventory is not required. Instead of associating a word in context with its most appropriate sense, the usage of a single given sense in the provided context is to be verified. Systems that aim to solve the proposed task are therefore not required to model all senses of the target word, but only a single sense instead. This facilitates the development of systems for specific domains or settings, as no general-domain knowledge resource is required to perform this task. For instance, an Indo"
2021.eacl-main.140,N18-2017,0,0.0532666,"Missing"
2021.eacl-main.140,D19-1355,0,0.118417,"an perform decently on the task, there remains a gap between machine and human performance, especially in outof-domain settings. WiC-TSV data is available at https://competitions.codalab. From 1970 to 2007, Apple’s chief executive was former Beatles road manager Neil Aspinall. org/competitions/23683 1 Introduction Word Sense Disambiguation (WSD) is a longstanding task in Natural Language Processing and Artificial Intelligence. While progress has been made in recent years, mainly thanks to the surge of transformer-based language models such as BERT (Loureiro and Jorge, 2019; Vial et al., 2019; Huang et al., 2019), the evaluation of WSD models has been limited to a set of (mostly SemEval-based) standard WSD datasets (Raganato et al., 2017). These datasets usually come in one of the two forms: lexical sample, in which a target word is placed in various contexts, triggering different senses, and all-words, in which all the content words in a given text are to be disambiguated. Both settings, however, come with a major restriction: word senses in the datasets are linked to external sense inventories such as WordNet (Fellbaum, Even when incorporating a general sense inventory (which would include senses fo"
2021.eacl-main.140,E17-2068,0,0.0180845,"Missing"
2021.eacl-main.140,Q15-1023,0,0.0278143,"domains. Therefore, this dataset aims at evaluating the ability of a model to (1) disambiguate the word in context without an external sense inventory, (2) deal with unseen instances and incomplete data, and (3) transfer the intrinsic knowledge (gained on general domain data) into a specific domain. 2 Related Work Word Sense Disambiguation. The task of WSD consists of associating a word in context with its most appropriate entry in a given sense inventory (Navigli, 2009), e.g., WordNet. For WSD there are many associated datasets (Raganato et al., 2017; Vial et al., 2018; R¨oder et al., 2018; Ling et al., 2015), including domain-specific ones (Agirre et al., 2009; Faralli and Navigli, 2012). The main difference between WSD and its re-formulation TSV is that for TSV the availability of a sense inventory is not required. Instead of associating a word in context with its most appropriate sense, the usage of a single given sense in the provided context is to be verified. Systems that aim to solve the proposed task are therefore not required to model all senses of the target word, but only a single sense instead. This facilitates the development of systems for specific domains or settings, as no general-"
2021.eacl-main.140,2020.acl-main.465,0,0.0138816,"biguation algorithms that do not require modeling the entirety of a sense inventory. This characteristic also provides a crucial advantage in enterprise and domain-specific settings as it facilitates the development of systems which are only aimed at modelling the domain at hand. Moreover, having these out-of-domain test instances makes our benchmark more robust and generalisable, preventing (or making it harder) for statistical models to learn spurious correlations from the training set, which has been proven to be an issue in standard NLP tasks (Poliak et al., 2018; Gururangan et al., 2018; Linzen, 2020). In our initial experiments we found that current state-of-the-art disambiguation techniques based on pre-trained language models such as BERT are very accurate at handling ambiguity, even in specialised domains. However, there is still room for improvement as highlighted by the gap with the human performance. This benchmark therefore opens up avenues for future research on domain-transfer and on developing general-purpose solutions which can perform well on a variety of domains without the need for large amounts of training data. As future work, we are planning to further investigate and ana"
2021.eacl-main.140,P19-1569,0,0.11729,"results show that even though these models can perform decently on the task, there remains a gap between machine and human performance, especially in outof-domain settings. WiC-TSV data is available at https://competitions.codalab. From 1970 to 2007, Apple’s chief executive was former Beatles road manager Neil Aspinall. org/competitions/23683 1 Introduction Word Sense Disambiguation (WSD) is a longstanding task in Natural Language Processing and Artificial Intelligence. While progress has been made in recent years, mainly thanks to the surge of transformer-based language models such as BERT (Loureiro and Jorge, 2019; Vial et al., 2019; Huang et al., 2019), the evaluation of WSD models has been limited to a set of (mostly SemEval-based) standard WSD datasets (Raganato et al., 2017). These datasets usually come in one of the two forms: lexical sample, in which a target word is placed in various contexts, triggering different senses, and all-words, in which all the content words in a given text are to be disambiguated. Both settings, however, come with a major restriction: word senses in the datasets are linked to external sense inventories such as WordNet (Fellbaum, Even when incorporating a general sense"
2021.eacl-main.140,N19-1128,1,0.828173,"Missing"
2021.eacl-main.140,S18-2023,0,0.0565736,"Missing"
2021.eacl-main.140,E17-1010,1,0.93906,"gs. WiC-TSV data is available at https://competitions.codalab. From 1970 to 2007, Apple’s chief executive was former Beatles road manager Neil Aspinall. org/competitions/23683 1 Introduction Word Sense Disambiguation (WSD) is a longstanding task in Natural Language Processing and Artificial Intelligence. While progress has been made in recent years, mainly thanks to the surge of transformer-based language models such as BERT (Loureiro and Jorge, 2019; Vial et al., 2019; Huang et al., 2019), the evaluation of WSD models has been limited to a set of (mostly SemEval-based) standard WSD datasets (Raganato et al., 2017). These datasets usually come in one of the two forms: lexical sample, in which a target word is placed in various contexts, triggering different senses, and all-words, in which all the content words in a given text are to be disambiguated. Both settings, however, come with a major restriction: word senses in the datasets are linked to external sense inventories such as WordNet (Fellbaum, Even when incorporating a general sense inventory (which would include senses for the fruit and the tree) and a technology-specific sense inventory (which would include the sense for Apple Inc. the technology"
2021.eacl-main.140,N04-1002,0,0.225153,"Missing"
2021.eacl-main.140,L18-1166,0,0.017952,"-specific instances from three different domains. Therefore, this dataset aims at evaluating the ability of a model to (1) disambiguate the word in context without an external sense inventory, (2) deal with unseen instances and incomplete data, and (3) transfer the intrinsic knowledge (gained on general domain data) into a specific domain. 2 Related Work Word Sense Disambiguation. The task of WSD consists of associating a word in context with its most appropriate entry in a given sense inventory (Navigli, 2009), e.g., WordNet. For WSD there are many associated datasets (Raganato et al., 2017; Vial et al., 2018; R¨oder et al., 2018; Ling et al., 2015), including domain-specific ones (Agirre et al., 2009; Faralli and Navigli, 2012). The main difference between WSD and its re-formulation TSV is that for TSV the availability of a sense inventory is not required. Instead of associating a word in context with its most appropriate sense, the usage of a single given sense in the provided context is to be verified. Systems that aim to solve the proposed task are therefore not required to model all senses of the target word, but only a single sense instead. This facilitates the development of systems for spe"
2021.eacl-main.140,2019.gwc-1.14,0,0.020377,"ough these models can perform decently on the task, there remains a gap between machine and human performance, especially in outof-domain settings. WiC-TSV data is available at https://competitions.codalab. From 1970 to 2007, Apple’s chief executive was former Beatles road manager Neil Aspinall. org/competitions/23683 1 Introduction Word Sense Disambiguation (WSD) is a longstanding task in Natural Language Processing and Artificial Intelligence. While progress has been made in recent years, mainly thanks to the surge of transformer-based language models such as BERT (Loureiro and Jorge, 2019; Vial et al., 2019; Huang et al., 2019), the evaluation of WSD models has been limited to a set of (mostly SemEval-based) standard WSD datasets (Raganato et al., 2017). These datasets usually come in one of the two forms: lexical sample, in which a target word is placed in various contexts, triggering different senses, and all-words, in which all the content words in a given text are to be disambiguated. Both settings, however, come with a major restriction: word senses in the datasets are linked to external sense inventories such as WordNet (Fellbaum, Even when incorporating a general sense inventory (which wo"
2021.emnlp-main.638,N18-2105,0,0.0165209,"2014; 2.2 Graph-based Methods Sterckx et al., 2015) introduces a topic distribution inferred by Latent Dirichlet Allocation (LDA) as a The basic idea behind graph-based methods is to identify the most relevant words from a graph con- pb (·), so that the estimation contains more semantic diversity across topics. TopicRank (Bougouin structed from a text document, where words are nodes and their connections are measured in differ- et al., 2013) clusters the candidates before running PageRank to group similar words together, and ent ways (Beliga et al., 2015). For this, PageRank MultipartiteRank (Boudin, 2018) extends it by em(Page et al., 1999) and its derivatives have proved to be highly successful (Mihalcea and Tarau, 2004; ploying a multipartite graph for a better candidate Wan and Xiao, 2008a; Florescu and Caragea, 2017; selection within a cluster. Sterckx et al., 2015; Bougouin et al., 2013). Finally, there are a few other works that directly Formally, let G = (V, E) be a graph where V and run graph clustering (Liu et al., 2009; Grineva et al., E are its associated set of vertices and edges. In a 2009), using edges to connect clusters instead of 8091 p(wi |wj ) = (1 − λ) P Data Size Domain Ty"
2021.emnlp-main.638,I13-1062,0,0.194968,"tic diversity across topics. TopicRank (Bougouin structed from a text document, where words are nodes and their connections are measured in differ- et al., 2013) clusters the candidates before running PageRank to group similar words together, and ent ways (Beliga et al., 2015). For this, PageRank MultipartiteRank (Boudin, 2018) extends it by em(Page et al., 1999) and its derivatives have proved to be highly successful (Mihalcea and Tarau, 2004; ploying a multipartite graph for a better candidate Wan and Xiao, 2008a; Florescu and Caragea, 2017; selection within a cluster. Sterckx et al., 2015; Bougouin et al., 2013). Finally, there are a few other works that directly Formally, let G = (V, E) be a graph where V and run graph clustering (Liu et al., 2009; Grineva et al., E are its associated set of vertices and edges. In a 2009), using edges to connect clusters instead of 8091 p(wi |wj ) = (1 − λ) P Data Size Domain Type KPCrowd Inspec Krapivin2009 Nguyen2007 PubMed Schutz2008 SemEval2010 SemEval2017 citeulike180 fao30 fao780 theses100 kdd wiki20 www 500 2000 2304 209 500 1231 243 493 183 30 779 100 755 20 1330 CS CS BM BM CS BI AG AG CS CS CS news abstract article article article article article paragraph"
2021.emnlp-main.638,N15-1059,1,0.746273,"Missing"
2021.emnlp-main.638,W03-1028,0,0.0533028,"uate the keyword extraction methods, we consider 15 different public datasets in English.6 Each entry in a dataset consists of a source document and a set of gold keyphrases, where the source document is processed through the pipeline described in Section 3 and the gold keyphrase set is filtered to include only phrases 5 All the details to reproduce our experiments are available at https://github.com/asahi417/kex 6 All the datasets were fetched from a public data repository for keyword extraction data: https://github.com/ LIAAD/KeywordExtractor-Datasets: KPCrowd (Marujo et al., 2013), Inspec (Hulth, 2003), Krapivin2009 (Krapivin et al., 2009), SemEval2017 (Augenstein et al., 2017), kdd (Gollapalli and Caragea, 2014), www (Gollapalli and Caragea, 2014), wiki20 (Medelyan and Witten, 2008), PubMed (Schutz et al., 2008), Schutz2008 (Schutz et al., 2008), citeulike180 (Medelyan et al., 2009), fao30 and fao780 (Medelyan and Witten, 2008), guyen2007 (Nguyen and Kan, 2007), and SemEval2010 (Kim et al., 2010). which appear in its candidate set. Table 1 provides high-level statistics of each dataset, including length and number of keyphrases7 (both average and standard deviation). Preprocessing. Before"
2021.emnlp-main.638,E14-1053,0,0.0295503,"ion, recent works proposed to use d∈D d∈D non-uniform distributions for pb (·). Florescu and Note also that, unlike in tf-idf, for lexical speci- Caragea (2017) observed that keywords are likely ficity a perfect partition of documents of D (refer- to occur very close to the first few sentences in a document in academic paper and proposed Posience corpus) is not required. This also opens up to tionRank in which pb (·) is defined as the inverse of other possibilities, such as using larger corpora as the absolute position of each word in a document. reference, for example. TopicalPageRank (TPR) (Jardine and Teufel, 2014; 2.2 Graph-based Methods Sterckx et al., 2015) introduces a topic distribution inferred by Latent Dirichlet Allocation (LDA) as a The basic idea behind graph-based methods is to identify the most relevant words from a graph con- pb (·), so that the estimation contains more semantic diversity across topics. TopicRank (Bougouin structed from a text document, where words are nodes and their connections are measured in differ- et al., 2013) clusters the candidates before running PageRank to group similar words together, and ent ways (Beliga et al., 2015). For this, PageRank MultipartiteRank (Boud"
2021.emnlp-main.712,W11-2501,0,0.0496097,"755/48/240 3,048/202/1,042 18,134/1,313/6,349 - 4,479/327/1,566 2,232/149/809 2,222/162/816 - Table 1: Number of instances for each relation type across training/validation/test sets of all lexical relation classification datasets. Lexical Relation Classification We consider the task of predicting which relation a given word pair belongs to. To solve this task, we train a multi-layer perceptron (MLP) which takes the (frozen) RelBERT embedding of the word pair as input. We consider the following widelyused multi-class relation classification benchmarks: K&H+N (Nec¸sulescu et al., 2015), BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016b), EVALution (Santus et al., 2015), and CogALex-V Subtask 2 (Santus et al., 2016a). Table 1 shows the size of the training, validation and test sets for each of the relation classification dataset. The hyperparameters of the MLP classifier are tuned on the validation set of each dataset. Concretely, we tune the learning rate from [0.001, 0.0001, 0.00001] and the hidden layer size from [100, 150, 200]. CogALex-V only has testing fragments so for this dataset we employ the default configuration of Scikit-Learn (Pedregosa et al., 2011), which uses a 100-dimensional h"
2021.emnlp-main.712,Q17-1010,0,0.0487999,"n layer size from [100, 150, 200]. CogALex-V only has testing fragments so for this dataset we employ the default configuration of Scikit-Learn (Pedregosa et al., 2011), which uses a 100-dimensional hidden layer and is optimized using Adam with a learning rate of 0.001. These datasets focus on the following lexical relations: co-hyponymy (cohyp), hypernymy (hyp), meronymy (mero), possession (poss), synonymy (syn), antonymy (ant), attribute (attr), event, and random (rand). 4.3 Baselines As baselines, we consider two standard word embedding models: GloVe (Pennington et al., 2014) and FastText (Bojanowski et al., 2017), where word pairs are represented by the vector difference of their word embeddings (diff ).7 For the classification experiments, we also consider the concatena7 Vector difference is the most common method for encoding relations, and has been shown to be the most reliable in the context of word analogies (Hakami and Bollegala, 2017). tion of the two word embeddings (cat) and their element-wise multiplication8 (dot). We furthermore experiment with two pre-trained word pair embedding models: pair2vec (Joshi et al., 2019) (pair) and RELATIVE (Camacho-Collados et al., 2019) (rel). For these word"
2021.emnlp-main.712,2020.acl-main.431,0,0.0381649,"extent the pre-trained LM already captures relational knowledge. We address this concern by including relation types in our evaluation which are different from the ones that have been used for fine-tuning. Probing LMs for Relational Knowledge Since the introduction of transformer-based LMs, a large number of works have focused on analysing the capabilities of such models, covering the extent to which they capture syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 2019), lexical semantics (Ethayarajh, 2019; Bommasani et al., 2020; Vulic et al., 2020), and various forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Unsupervised Relation Discovery Modelling Roberts et al., 2020), among others. The idea of how different words are related is a long-standing extracting relational knowledge from LMs, in par- challenge in NLP. An early approach is DIRT (Lin ticular, has also been studied. For instance, Petroni and Pantel, 2001), which encodes the relation beet al. (2019) use BERT for link prediction. To this tween two nouns as th"
2021.emnlp-main.712,J90-1003,0,0.558428,"ntext of word analogies (Hakami and Bollegala, 2017). tion of the two word embeddings (cat) and their element-wise multiplication8 (dot). We furthermore experiment with two pre-trained word pair embedding models: pair2vec (Joshi et al., 2019) (pair) and RELATIVE (Camacho-Collados et al., 2019) (rel). For these word pair embeddings, as well as for RelBERT, we concatenate the embeddings from both directions, i.e. (h, t) and (t, h). For the analogy questions, two simple statistical baselines are included: the expected random performance and a strategy based on point-wise mutual information (PMI) Church and Hanks (1990). In particular, the PMI score of a word pair is computed using the English Wikipedia, with a fixed window size of 10. We then choose the candidate pair with the highest PMI as the prediction. Note that this PMI-based method completely ignores the query pair. We also compare with the published results from Ushio et al. (2021), where a strategy is proposed to solve analogy questions by using LMs to compute an analogical proportion score. In particular, a four-word tuple (a, b, c, d) is encoded using a custom prompt and perplexity based scoring strategies are used to determine whether the word p"
2021.emnlp-main.712,D19-1109,0,0.0200772,"different from the ones that have been used for fine-tuning. Probing LMs for Relational Knowledge Since the introduction of transformer-based LMs, a large number of works have focused on analysing the capabilities of such models, covering the extent to which they capture syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 2019), lexical semantics (Ethayarajh, 2019; Bommasani et al., 2020; Vulic et al., 2020), and various forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Unsupervised Relation Discovery Modelling Roberts et al., 2020), among others. The idea of how different words are related is a long-standing extracting relational knowledge from LMs, in par- challenge in NLP. An early approach is DIRT (Lin ticular, has also been studied. For instance, Petroni and Pantel, 2001), which encodes the relation beet al. (2019) use BERT for link prediction. To this tween two nouns as the dependency path connectend, they use a manually defined prompt for each ing them. Their view is that two such depenrelation type, in which t"
2021.emnlp-main.712,N19-1423,0,0.48293,"r analogy) plays a central knowledge about named entities. Among othrole in computational creativity (Goel, 2019), leers, this makes it possible to distill high-quality gal reasoning (Ashley, 1988; Walton, 2010), onword vectors from pre-trained language models. However, it is currently unclear to what extology alignment (Raad and Evermann, 2015) and tent it is possible to distill relation embeddings, instance-based learning (Miclet et al., 2008). i.e. vectors that characterize the relationship Given the recent success of pre-trained language between two words. Such relation embeddings models (Devlin et al., 2019; Liu et al., 2019; Brown are appealing because they can, in principle, et al., 2020), we may wonder whether such modencode relational knowledge in a more finegrained way than is possible with knowledge els are able to capture lexical relations in a more graphs. To obtain relation embeddings from a faithful or fine-grained way than traditional word pre-trained language model, we encode word embeddings. However, for language models (LMs), pairs using a (manually or automatically genthere is no direct equivalent to the word vector erated) prompt, and we fine-tune the language difference. In this"
2021.emnlp-main.712,P19-1470,0,0.0200407,"distill relational knowledge that is encoded in the pre-trained LM, rather than merely generalising from the examples that were used for fine-tuning. 2 Related Work then, is whether relations themselves have an explicit representation, or whether transformer models essentially store a propositionalised knowledge graph. The results we present in this paper suggest that common lexical relations (e.g. hypernymy, meronymy, has-attribute), at least, must have some kind of explicit representation, although it remains unclear how they are encoded. Another notable work focusing on link prediction is (Bosselut et al., 2019), where GPT is fine-tuned to complete triples from commonsense knowledge graphs, in particular ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019). While their model was able to generate new knowledge graph triples, it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre-trained GPT model, or whether this rather comes from the ability to generalise from the training triples. For the ConceptNet dataset, for instance, Jastrz˛ebski et al. (2018) found that most test triples are in fact minor variations of training triples. I"
2021.emnlp-main.712,C18-1225,1,0.775645,"Missing"
2021.emnlp-main.712,K19-1049,0,0.0156912,"the full pre-trained LM’s vocabulary. as negative examples (while ensuring that these 9047 Figure 2: Batch augmentation where the original batch with m samples is augmented with 2m(m−1) samples. triples are from different relations). Figure 2 illustrates this idea. Note how the effective batch size thus increases quadratically, while the number of vectors that needs to be encoded by the LM remains unchanged. In our setting, this leads to an additional 13500 triples per relation. Similar in-batch negative sampling has been shown to be effective in information retrieval (Karpukhin et al., 2020; Gillick et al., 2019). Third, we also construct training triples by considering the 10 high-level categories as relation types. In this case, we choose two positive examples from different relations that belong to the same category, along with a positive example from a relation from a different category. We add 5040 triples of this kind for each of the 10 categories. ing rate 0.00002, batch size 64 and we fine-tune the model for 1 epoch. For AutoPrompt, the top-50 tokens are considered and the number of iterations is set to 50. In each iteration, one of the input tokens is re-sampled and the loss is re-computed ac"
2021.emnlp-main.712,N16-2002,0,0.0202721,"he model is required to select the relationally most similar word pair from a list of candidates. To solve this task, we simply choose the candidate whose RelBERT embedding has the highest cosine similarity with the RelBERT embedding of the query pair. Note that this task is completely unsupervised, without the need for any training or tuning. We use the five analogy datasets that were considered by Ushio et al. (2021): the SAT analogies dataset (Turney et al., 2003), the U2 and U4 analogy datasets, which were collected from an educational website5 , and datasets that were derived6 from BATS (Gladkova et al., 2016) and the Google analogy dataset (Mikolov et al., 2013b). These five datasets consist of tuning and testing fragments. In particular, they contain 37/337 (SAT), 24/228 (U2), 48/432 (U4), 50/500 (Google), and 199/1799 (BATS) questions for validation/testing. As there is no need to tune RelBERT on task-specific data, we only use the test fragments. For SAT, we will also report results on the full dataset (i.e. the testing fragment and tuning fragment combined), as this allows us to compare the performance with published results. We will refer to this full version of the SAT dataset as SAT†. Train"
2021.emnlp-main.712,P04-1053,0,0.412757,"Missing"
2021.emnlp-main.712,2021.eacl-main.316,0,0.0149656,"ionship between h and t. Moreover, we avoid minimal templates such as “[h] is the &lt;mask&gt; of [t]”, as LMs typically perform worse on such short inputs (Bouraoui et al., 2020; Jiang et al., 2020). Learned Prompts The choice of prompt can have a significant impact on an LM’s performance. Since it is difficult to generate manual prompts in a systematic way, several strategies for automated generation of task-specific prompts have been proposed, e.g. based on mining patterns from a corpus (Bouraoui et al., 2020), paraphrasing (Jiang et al., 2020), training an additional LM for template generation (Haviv et al., 2021; Gao et al., 2020), and prompt optimization (Shin et al., 2020; Liu et al., 2021). In our work, we focus on the 3.1 Prompt Generation latter strategy, given its conceptual simplicity and Manual Prompts A basic prompt generation its strong reported performance on various benchstrategy is to rely on manually created templates, marks. Specifically, we consider AutoPrompt (Shin et al., 2020) and P-tuning (Liu et al., 2021). Note 2 Interestingly, Roller and Erk (2016) showed that the direct that both methods rely on training data. We will concatenation of distributional word vectors in isolation c"
2021.emnlp-main.712,C92-2082,0,0.642137,"ion (Shin et al., 2020; Liu et al., 2021). In our work, we focus on the 3.1 Prompt Generation latter strategy, given its conceptual simplicity and Manual Prompts A basic prompt generation its strong reported performance on various benchstrategy is to rely on manually created templates, marks. Specifically, we consider AutoPrompt (Shin et al., 2020) and P-tuning (Liu et al., 2021). Note 2 Interestingly, Roller and Erk (2016) showed that the direct that both methods rely on training data. We will concatenation of distributional word vectors in isolation can effectively identify Hearst Patterns (Hearst, 1992). use the same training data and loss function that 9046 In this section, we describe our proposed relation embedding model (RelBERT henceforth). To obtain a relation embedding for given a word pair (h, t), we first convert it into a sentence s, called the prompt. We then feed the prompt through the LM and average the contextualized embeddings (i.e. the output vectors) to get the relation embedding of (h, t). These steps are illustrated in Figure 1 and explained in more detail in the following. where ε &gt; 0 is the margin and k · k is the l2 norm. we use for fine-tuning the LM; see Section 3.2."
2021.emnlp-main.712,N19-1419,0,0.0248576,"e in fact minor variations of training triples. In this paper, we also rely on fine-tuning, which makes it harder to determine to what extent the pre-trained LM already captures relational knowledge. We address this concern by including relation types in our evaluation which are different from the ones that have been used for fine-tuning. Probing LMs for Relational Knowledge Since the introduction of transformer-based LMs, a large number of works have focused on analysing the capabilities of such models, covering the extent to which they capture syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 2019), lexical semantics (Ethayarajh, 2019; Bommasani et al., 2020; Vulic et al., 2020), and various forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Unsupervised Relation Discovery Modelling Roberts et al., 2020), among others. The idea of how different words are related is a long-standing extracting relational knowledge from LMs, in par- challenge in NLP. An early approach is DIRT (Lin ticular, has also been studied. For instan"
2021.emnlp-main.712,P18-1003,1,0.634111,"Dante is born” is stored as al. (Riedel et al., 2013), which jointly models the a property of Florence. Dai et al. (2021) present contexts in which words appear in a corpus with a further evidence of this view. What is less clear, given set of relational facts. 9045 The aforementioned works essentially represent the relation between two words by summarising the contexts in which these words co-occur. In recent years, a number of strategies based on distributional models have been explored that rely on similar intuitions but go beyond simple vector operations of word embeddings.2 For instance, Jameel et al. (2018) introduced a variant of the GloVe word embedding model, in which relation vectors are jointly learned with word vectors. In SeVeN (Espinosa-Anke and Schockaert, 2018) and RELATIVE (Camacho-Collados et al., 2019), relation vectors are computed by averaging the embeddings of context words, while pair2vec (Joshi et al., 2019) uses an LSTM to summarise the contexts in which two given words occur, and Washio and Kato (2018) learn embeddings of dependency paths to encode word pairs. Another line of work is based on the idea that relation embeddings should facilitate link prediction, i.e. given the"
2021.emnlp-main.712,S12-1047,0,0.0342133,"Missing"
2021.emnlp-main.712,2020.emnlp-main.550,0,0.0201081,"sentences), we consider the full pre-trained LM’s vocabulary. as negative examples (while ensuring that these 9047 Figure 2: Batch augmentation where the original batch with m samples is augmented with 2m(m−1) samples. triples are from different relations). Figure 2 illustrates this idea. Note how the effective batch size thus increases quadratically, while the number of vectors that needs to be encoded by the LM remains unchanged. In our setting, this leads to an additional 13500 triples per relation. Similar in-batch negative sampling has been shown to be effective in information retrieval (Karpukhin et al., 2020; Gillick et al., 2019). Third, we also construct training triples by considering the 10 high-level categories as relation types. In this case, we choose two positive examples from different relations that belong to the same category, along with a positive example from a relation from a different category. We add 5040 triples of this kind for each of the 10 categories. ing rate 0.00002, batch size 64 and we fine-tune the model for 1 epoch. For AutoPrompt, the top-50 tokens are considered and the number of iterations is set to 50. In each iteration, one of the input tokens is re-sampled and the"
2021.emnlp-main.712,W16-2503,0,0.0222906,"l types of relacomparing the different prompting strategies, we again find that the manual prompts perform sur- tions that it has not seen during training. To answer this question, we conduct an additional experiment prisingly well, although the best results are now to evaluate RelBERT on lexical relation classificaobtained with learned prompts in a few cases. tion, using a version that was trained without the re9 The Google analogy dataset has been shown to be biased lations from the Class Inclusion category, which is toward word similarity and therefore to be well suited to word embeddings (Linzen, 2016; Rogers et al., 2017). the high-level category in the SemEval dataset that 9050 BLESS macro micro Model CogALexV macro micro EVALution macro micro K&H+N macro micro ROOT09 macro micro GloVe cat cat+dot cat+dot+pair cat+dot+rel diff diff+dot diff+dot+pair diff+dot+rel 92.9 93.1 91.8 91.1 91.0 92.3 91.3 91.1 93.3 93.7 92.6 92.0 91.5 92.9 92.2 91.8 42.8 51.9 56.4 53.2 39.2 50.6 55.5 52.8 73.5 79.2 81.1 79.2 70.8 78.5 80.2 78.6 56.9 55.9 58.1 58.4 55.6 56.5 56.0 56.9 58.3 57.3 59.6 58.6 56.9 57.9 57.4 57.9 88.8 89.6 89.4 89.3 87.0 88.3 88.0 87.4 94.9 95.1 95.7 94.9 94.4 94.8 95.5 94.6 86.3 88.8 8"
2021.emnlp-main.712,2021.ccl-1.108,0,0.0422191,"Missing"
2021.emnlp-main.712,Q16-1017,0,0.0249829,"rs are jointly learned with word vectors. In SeVeN (Espinosa-Anke and Schockaert, 2018) and RELATIVE (Camacho-Collados et al., 2019), relation vectors are computed by averaging the embeddings of context words, while pair2vec (Joshi et al., 2019) uses an LSTM to summarise the contexts in which two given words occur, and Washio and Kato (2018) learn embeddings of dependency paths to encode word pairs. Another line of work is based on the idea that relation embeddings should facilitate link prediction, i.e. given the first word and a relation vector, we should be able to predict the second word (Marcheggiani and Titov, 2016; Simon et al., 2019). This idea also lies at the basis of the approach from Soares et al. (2019), who train a relation encoder by fine-tuning BERT (Devlin et al., 2019) with a link prediction loss. However, it should be noted that they focus on learning relation vectors from individual sentences, as a pre-training task for applications such as few-shot relation extraction. In contrast, our focus in this paper is on characterising the overall relationship between two words. 3 RelBERT Figure 1: Pipeline to transform the word pair (h, t) to the relation embedding x. which has proven effective in"
2021.emnlp-main.712,N13-1090,0,0.37509,"the one hand, this competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, will allow us to gain a better understanding of how even without any task-specific fine-tuning.1 well lexical relations are captured by these models. On the other hand, this will also provide us with a 1 Introduction practical method for obtaining relation embeddings in applications such as the ones mentioned above. One of the most widely studied aspects of word Since it is unclear how LMs store relational embeddings is the fact that word vector differences capture lexical relations (Mikolov et al., 2013a). knowledge, rather than directly extracting relation embeddings, we first fine-tune the LM, such that reWhile not being directly connected to downstream performance on NLP tasks, this ability of word em- lation embeddings can be obtained from its output. beddings is nonetheless important. For instance, To this end, we need a prompt, i.e. a template to understanding lexical relations is an important pre- convert a given word pair into a sentence, and some requisite for understanding the meaning of com- training data to fine-tune the model. To illustrate the process, consider the word pair Pa"
2021.emnlp-main.712,S15-1021,0,0.0587386,"Missing"
2021.emnlp-main.712,D14-1162,0,0.0990184,"[0.001, 0.0001, 0.00001] and the hidden layer size from [100, 150, 200]. CogALex-V only has testing fragments so for this dataset we employ the default configuration of Scikit-Learn (Pedregosa et al., 2011), which uses a 100-dimensional hidden layer and is optimized using Adam with a learning rate of 0.001. These datasets focus on the following lexical relations: co-hyponymy (cohyp), hypernymy (hyp), meronymy (mero), possession (poss), synonymy (syn), antonymy (ant), attribute (attr), event, and random (rand). 4.3 Baselines As baselines, we consider two standard word embedding models: GloVe (Pennington et al., 2014) and FastText (Bojanowski et al., 2017), where word pairs are represented by the vector difference of their word embeddings (diff ).7 For the classification experiments, we also consider the concatena7 Vector difference is the most common method for encoding relations, and has been shown to be the most reliable in the context of word analogies (Hakami and Bollegala, 2017). tion of the two word embeddings (cat) and their element-wise multiplication8 (dot). We furthermore experiment with two pre-trained word pair embedding models: pair2vec (Joshi et al., 2019) (pair) and RELATIVE (Camacho-Collad"
2021.emnlp-main.712,D19-1250,0,0.145044,"relation types in our evaluation which are different from the ones that have been used for fine-tuning. Probing LMs for Relational Knowledge Since the introduction of transformer-based LMs, a large number of works have focused on analysing the capabilities of such models, covering the extent to which they capture syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 2019), lexical semantics (Ethayarajh, 2019; Bommasani et al., 2020; Vulic et al., 2020), and various forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Unsupervised Relation Discovery Modelling Roberts et al., 2020), among others. The idea of how different words are related is a long-standing extracting relational knowledge from LMs, in par- challenge in NLP. An early approach is DIRT (Lin ticular, has also been studied. For instance, Petroni and Pantel, 2001), which encodes the relation beet al. (2019) use BERT for link prediction. To this tween two nouns as the dependency path connectend, they use a manually defined prompt for each ing them. Their view is t"
2021.emnlp-main.712,D19-1410,0,0.0191922,"ile the LM’s weights are frozen. 3.2 Fine-tuning the LM To fine-tune the LM, we need training data and a loss function. As training data, we assume that, for a number of different relation types r, we have access to examples of word pairs (h, t) that are instances of that relation type. The loss function is based on the following intuition: the embeddings of word pairs that belong to the same relation type should be closer together than the embeddings of pairs that belong to different relations. In particular, we use the triplet loss from Schroff et al. (2015) and the classification loss from Reimers and Gurevych (2019), both of which are based on this intuition. Triplet Loss We draw a triplet from the relation dataset by selecting an anchor pair a = (ha , ta ), a positive example p = (hp , tp ) and a negative example n = (hn , tn ), i.e. we select word pairs a, p, n such that a and p belong to the same relation type while n belongs to a different relation type. Let us write xa , xp , xn for the corresponding relation embeddings. Each relation embedding is produced by the same LM, which is trained to make the distance between xa and xp smaller than the distance between xa and xn . Formally, this is accomplis"
2021.emnlp-main.712,N13-1008,0,0.0249668,"al knowl- topics in LDA). Turney (2005) proposed a method edge is encoded in the parameters of pre-trained called Latent Relational Analysis (LRA), which LMs. Some works have also looked at how such uses matrix factorization to learn relation embedknowledge is stored. Geva et al. (2020) argue that dings based on co-occurrences of word pairs and the feed-forward layers of transformer-based LMs dependency paths. Matrix factorization is also used act as neural memories, which would suggest that in the Universal Schema approach from Riedel et e.g. “the place where Dante is born” is stored as al. (Riedel et al., 2013), which jointly models the a property of Florence. Dai et al. (2021) present contexts in which words appear in a corpus with a further evidence of this view. What is less clear, given set of relational facts. 9045 The aforementioned works essentially represent the relation between two words by summarising the contexts in which these words co-occur. In recent years, a number of strategies based on distributional models have been explored that rely on similar intuitions but go beyond simple vector operations of word embeddings.2 For instance, Jameel et al. (2018) introduced a variant of the GloV"
2021.emnlp-main.712,2020.emnlp-main.437,0,0.0363797,"Missing"
2021.emnlp-main.712,S17-1017,0,0.0309151,"Missing"
2021.emnlp-main.712,L16-1722,0,0.021247,"/1,313/6,349 - 4,479/327/1,566 2,232/149/809 2,222/162/816 - Table 1: Number of instances for each relation type across training/validation/test sets of all lexical relation classification datasets. Lexical Relation Classification We consider the task of predicting which relation a given word pair belongs to. To solve this task, we train a multi-layer perceptron (MLP) which takes the (frozen) RelBERT embedding of the word pair as input. We consider the following widelyused multi-class relation classification benchmarks: K&H+N (Nec¸sulescu et al., 2015), BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016b), EVALution (Santus et al., 2015), and CogALex-V Subtask 2 (Santus et al., 2016a). Table 1 shows the size of the training, validation and test sets for each of the relation classification dataset. The hyperparameters of the MLP classifier are tuned on the validation set of each dataset. Concretely, we tune the learning rate from [0.001, 0.0001, 0.00001] and the hidden layer size from [100, 150, 200]. CogALex-V only has testing fragments so for this dataset we employ the default configuration of Scikit-Learn (Pedregosa et al., 2011), which uses a 100-dimensional hidden layer and is optimized"
2021.emnlp-main.712,W15-4208,0,0.0259614,"32/149/809 2,222/162/816 - Table 1: Number of instances for each relation type across training/validation/test sets of all lexical relation classification datasets. Lexical Relation Classification We consider the task of predicting which relation a given word pair belongs to. To solve this task, we train a multi-layer perceptron (MLP) which takes the (frozen) RelBERT embedding of the word pair as input. We consider the following widelyused multi-class relation classification benchmarks: K&H+N (Nec¸sulescu et al., 2015), BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016b), EVALution (Santus et al., 2015), and CogALex-V Subtask 2 (Santus et al., 2016a). Table 1 shows the size of the training, validation and test sets for each of the relation classification dataset. The hyperparameters of the MLP classifier are tuned on the validation set of each dataset. Concretely, we tune the learning rate from [0.001, 0.0001, 0.00001] and the hidden layer size from [100, 150, 200]. CogALex-V only has testing fragments so for this dataset we employ the default configuration of Scikit-Learn (Pedregosa et al., 2011), which uses a 100-dimensional hidden layer and is optimized using Adam with a learning rate of"
2021.emnlp-main.712,N19-1329,0,0.0235762,"hat most test triples are in fact minor variations of training triples. In this paper, we also rely on fine-tuning, which makes it harder to determine to what extent the pre-trained LM already captures relational knowledge. We address this concern by including relation types in our evaluation which are different from the ones that have been used for fine-tuning. Probing LMs for Relational Knowledge Since the introduction of transformer-based LMs, a large number of works have focused on analysing the capabilities of such models, covering the extent to which they capture syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 2019), lexical semantics (Ethayarajh, 2019; Bommasani et al., 2020; Vulic et al., 2020), and various forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Unsupervised Relation Discovery Modelling Roberts et al., 2020), among others. The idea of how different words are related is a long-standing extracting relational knowledge from LMs, in par- challenge in NLP. An early approach is DIRT (Lin ticular, has als"
2021.emnlp-main.712,2021.eacl-main.20,0,0.0222943,"m Soares et al. (2019), who train a relation encoder by fine-tuning BERT (Devlin et al., 2019) with a link prediction loss. However, it should be noted that they focus on learning relation vectors from individual sentences, as a pre-training task for applications such as few-shot relation extraction. In contrast, our focus in this paper is on characterising the overall relationship between two words. 3 RelBERT Figure 1: Pipeline to transform the word pair (h, t) to the relation embedding x. which has proven effective in factual knowledge probing (Petroni et al., 2019) and text classification (Schick and Schütze, 2021; Tam et al., 2021; Le Scao and Rush, 2021), among many others. To test whether manually generated templates can be effective for learning relation embeddings, we will consider the following five templates: 1. Today, I finally discovered the relation between [h] and [t] : [h] is the &lt;mask&gt; of [t] 2. Today, I finally discovered the relation between [h] and [t] : [t] is [h]’s &lt;mask&gt; 3. Today, I finally discovered the relation between [h] and [t] : &lt;mask&gt; 4. I wasn’t aware of this relationship, but I just read in the encyclopedia that [h] is the &lt;mask&gt; of [t] 5. I wasn’t aware of this relationshi"
2021.emnlp-main.712,2020.emnlp-main.346,0,0.0393817,"ch as “[h] is the &lt;mask&gt; of [t]”, as LMs typically perform worse on such short inputs (Bouraoui et al., 2020; Jiang et al., 2020). Learned Prompts The choice of prompt can have a significant impact on an LM’s performance. Since it is difficult to generate manual prompts in a systematic way, several strategies for automated generation of task-specific prompts have been proposed, e.g. based on mining patterns from a corpus (Bouraoui et al., 2020), paraphrasing (Jiang et al., 2020), training an additional LM for template generation (Haviv et al., 2021; Gao et al., 2020), and prompt optimization (Shin et al., 2020; Liu et al., 2021). In our work, we focus on the 3.1 Prompt Generation latter strategy, given its conceptual simplicity and Manual Prompts A basic prompt generation its strong reported performance on various benchstrategy is to rely on manually created templates, marks. Specifically, we consider AutoPrompt (Shin et al., 2020) and P-tuning (Liu et al., 2021). Note 2 Interestingly, Roller and Erk (2016) showed that the direct that both methods rely on training data. We will concatenation of distributional word vectors in isolation can effectively identify Hearst Patterns (Hearst, 1992). use the"
2021.emnlp-main.712,P16-1226,0,0.0379587,"Missing"
2021.emnlp-main.712,P19-1133,0,0.0188883,"rd vectors. In SeVeN (Espinosa-Anke and Schockaert, 2018) and RELATIVE (Camacho-Collados et al., 2019), relation vectors are computed by averaging the embeddings of context words, while pair2vec (Joshi et al., 2019) uses an LSTM to summarise the contexts in which two given words occur, and Washio and Kato (2018) learn embeddings of dependency paths to encode word pairs. Another line of work is based on the idea that relation embeddings should facilitate link prediction, i.e. given the first word and a relation vector, we should be able to predict the second word (Marcheggiani and Titov, 2016; Simon et al., 2019). This idea also lies at the basis of the approach from Soares et al. (2019), who train a relation encoder by fine-tuning BERT (Devlin et al., 2019) with a link prediction loss. However, it should be noted that they focus on learning relation vectors from individual sentences, as a pre-training task for applications such as few-shot relation extraction. In contrast, our focus in this paper is on characterising the overall relationship between two words. 3 RelBERT Figure 1: Pipeline to transform the word pair (h, t) to the relation embedding x. which has proven effective in factual knowledge pr"
2021.emnlp-main.712,P19-1279,0,0.0158348,"acho-Collados et al., 2019), relation vectors are computed by averaging the embeddings of context words, while pair2vec (Joshi et al., 2019) uses an LSTM to summarise the contexts in which two given words occur, and Washio and Kato (2018) learn embeddings of dependency paths to encode word pairs. Another line of work is based on the idea that relation embeddings should facilitate link prediction, i.e. given the first word and a relation vector, we should be able to predict the second word (Marcheggiani and Titov, 2016; Simon et al., 2019). This idea also lies at the basis of the approach from Soares et al. (2019), who train a relation encoder by fine-tuning BERT (Devlin et al., 2019) with a link prediction loss. However, it should be noted that they focus on learning relation vectors from individual sentences, as a pre-training task for applications such as few-shot relation extraction. In contrast, our focus in this paper is on characterising the overall relationship between two words. 3 RelBERT Figure 1: Pipeline to transform the word pair (h, t) to the relation embedding x. which has proven effective in factual knowledge probing (Petroni et al., 2019) and text classification (Schick and Schütze, 20"
2021.emnlp-main.712,2020.tacl-1.48,0,0.0386902,"sed for fine-tuning. Probing LMs for Relational Knowledge Since the introduction of transformer-based LMs, a large number of works have focused on analysing the capabilities of such models, covering the extent to which they capture syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 2019), lexical semantics (Ethayarajh, 2019; Bommasani et al., 2020; Vulic et al., 2020), and various forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Unsupervised Relation Discovery Modelling Roberts et al., 2020), among others. The idea of how different words are related is a long-standing extracting relational knowledge from LMs, in par- challenge in NLP. An early approach is DIRT (Lin ticular, has also been studied. For instance, Petroni and Pantel, 2001), which encodes the relation beet al. (2019) use BERT for link prediction. To this tween two nouns as the dependency path connectend, they use a manually defined prompt for each ing them. Their view is that two such depenrelation type, in which the tail entity is replaced dency paths a"
2021.emnlp-main.712,D19-1592,0,0.0232741,"Missing"
2021.emnlp-main.712,S18-2020,0,0.0169833,"taset, we compare with the published results from GPT-3 (Brown et al., 2020), LRA (Turney, 2005) and SuperSim (Turney, 2013); for relation classification we report the published results of the LexNet (Shwartz et al., 2016) and SphereRE (Wang et al., 2019) relation classification models, taking the results from the latter publication. We did not reproduce these latter methods in similar conditions as our work, and hence they are not fully comparable. More8 Multiplicative features have been shown to provide consistent improvements for word embeddings in supervised relation classification tasks (Vu and Shwartz, 2018). 9049 over, these approaches are a different nature, as the aim of our work is to provide universal relation embeddings instead of task-specific models. 5 Results In this section, we present our main experimental results, testing the relation embeddings learned by RelBERT on analogy questions (Section 5.1) and relation classification (Section 5.2). 5.1 Analogy Questions Model SAT† SAT U2 U4 Google BATS Random PMI LRA SuperSim GPT-3 (zero) GPT-3 (few) RELATIVE pair2vec GloVe FastText 20.0 23.3 56.4 54.8 53.7 65.2* 24.9 33.7 48.9 49.7 20.0 23.1 24.6 34.1 47.8 47.8 23.6 32.9 32.5 25.4 46.5 43.0"
2021.emnlp-main.712,2020.emnlp-main.586,0,0.0352537,"M already captures relational knowledge. We address this concern by including relation types in our evaluation which are different from the ones that have been used for fine-tuning. Probing LMs for Relational Knowledge Since the introduction of transformer-based LMs, a large number of works have focused on analysing the capabilities of such models, covering the extent to which they capture syntax (Goldberg, 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019; van Schijndel et al., 2019; Jawahar et al., 2019; Tenney et al., 2019), lexical semantics (Ethayarajh, 2019; Bommasani et al., 2020; Vulic et al., 2020), and various forms of factual and commonsense knowledge (Petroni et al., 2019; Forbes et al., 2019; Davison et al., 2019; Zhou et al., 2020; Talmor et al., 2020; Unsupervised Relation Discovery Modelling Roberts et al., 2020), among others. The idea of how different words are related is a long-standing extracting relational knowledge from LMs, in par- challenge in NLP. An early approach is DIRT (Lin ticular, has also been studied. For instance, Petroni and Pantel, 2001), which encodes the relation beet al. (2019) use BERT for link prediction. To this tween two nouns as the dependency path con"
2021.emnlp-main.712,P19-1169,0,0.0360728,"Missing"
2021.emnlp-main.712,2020.emnlp-main.16,0,0.0407248,"e 3 compares the performance of RelBERT with that of the vanilla pre-trained RoBERTa model (i.e. when only the prompt is optimized). As can be seen, the fine-tuning process is critical for achieving good results. In Figure 3, we also compare the performance of our main RelBERT model, which is based on RoBERTa, with versions that were instead initialized with BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2019).11 RoBERTa clearly outperforms the other two LMs, which is in accordance with findings from the literature suggesting that RoBERTa captures more semantic knowledge (Li et al., 2020; Warstadt et al., 2020). 6.3 Qualitative Analysis To give further insight into the nature of RelBERT embeddings, Table 6 shows the nearest neighbors of some selected word pairs from the evaluation datasets. To this end, we computed RelBERT relation vectors for all pairs in the Wikipedia pretrained RELATIVE vocabulary (over 1M pairs).12 The neighbors are those word pairs whose RelBERT embedding has the highest cosine similarity within the full pair vocabulary. As can be seen, the neighbors mostly represent word pairs that are relationally similar, even for morphological relations (e.g. dog:dogs), which are not presen"
2021.emnlp-main.712,D11-1135,0,0.105766,"Missing"
2021.mrl-1.10,P19-1070,0,0.060336,"Missing"
2021.mrl-1.10,P16-1085,1,0.809987,"226,036 92,202 1,175 1,151 1,155 1,931 1,656 1,467 1,481 1,706 4,272 Nouns Verbs Adj. Adv. EN Test - SemEval-13 FA Table 1: Number of sense-annotated instances in the benchmark datasets after cleaning and unification. RAW counts correspond to number of instances in the original datasets before cleaning and unification. ventories such as WordNet (e.g. semantic relations, sense glosses, distributions, etc.) for inference. For the last decade, the supervised approach has been the dominant paradigm for WSD (Raganato et al., 2017), either the conventional featurebased systems (Zhong and Ng, 2010; Iacobacci et al., 2016), LSTM-driven techniques (Melamud et al., 2016; Yuan et al., 2016), or the more recent trend empowered by pre-trained language models (Loureiro and Jorge, 2019; Scarlini et al., 2020b). In the latter approaches, feature extraction strategies where sense embeddings are determined by averaging a word’s contextualised representations have proven surprisingly effective (Loureiro et al., 2021), even in multilingual settings (Bevilacqua and Navigli, 2020; Raganato et al., 2020). We extend this simple idea to the cross-lingual setting, showing that the vanilla contextualized sense embeddings achievin"
2021.mrl-1.10,S07-1004,0,0.152133,"Missing"
2021.mrl-1.10,P19-1569,1,0.886816,"bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et al., 2020; Cao et al., 2020). Therefore, cross-lingual transfer has opened new opportunities to circumvent the knowledge acquisition bottleneck for less-resourced languages. In this paper, we aim at investigating this opportunity. To this end, we build upon recent research on cross-lingual transfer to compute contextualized sense embeddings and verify if semantic distinctions in the English language are transferable to oth"
2021.mrl-1.10,K16-1006,0,0.2984,"67 1,481 1,706 4,272 Nouns Verbs Adj. Adv. EN Test - SemEval-13 FA Table 1: Number of sense-annotated instances in the benchmark datasets after cleaning and unification. RAW counts correspond to number of instances in the original datasets before cleaning and unification. ventories such as WordNet (e.g. semantic relations, sense glosses, distributions, etc.) for inference. For the last decade, the supervised approach has been the dominant paradigm for WSD (Raganato et al., 2017), either the conventional featurebased systems (Zhong and Ng, 2010; Iacobacci et al., 2016), LSTM-driven techniques (Melamud et al., 2016; Yuan et al., 2016), or the more recent trend empowered by pre-trained language models (Loureiro and Jorge, 2019; Scarlini et al., 2020b). In the latter approaches, feature extraction strategies where sense embeddings are determined by averaging a word’s contextualised representations have proven surprisingly effective (Loureiro et al., 2021), even in multilingual settings (Bevilacqua and Navigli, 2020; Raganato et al., 2020). We extend this simple idea to the cross-lingual setting, showing that the vanilla contextualized sense embeddings achieving outstanding results in the monolingual setti"
2021.mrl-1.10,W04-0807,0,0.318541,"Missing"
2021.mrl-1.10,W04-0800,0,0.416298,"Missing"
2021.mrl-1.10,H93-1061,0,0.495972,"Missing"
2021.mrl-1.10,Q14-1019,0,0.0939957,"Missing"
2021.mrl-1.10,S13-2040,0,0.021605,"an one person) Cross-Lingual Neural Language Model Input Most Similar Figure 1: Overview of the proposed method for multilingual zero-shot word sense disambiguation. The example sentence presented (‘There is a bench in the hall’, in English) is using a different language (target) than our sense inventory (source), but using multilingual language models and lemma mappings, we demonstrate how it’s still possible to perform disambiguation, using either variations of our method (sense and synset strategies). 3.2 Multilingual SemEval test sets We considered two multilingual datasets: SemEval 2013 (Navigli et al., 2013a) available for English, French and Italian, and SemEval 2015 (Moro and Navigli, 2015), available for English, French, Italian, Spanish and German. These datasets were annotated with BabelNet (Navigli and Ponzetto, 2012), a resource that contains WordNet, among other linked sense inventories. Therefore, from each dataset we simply considered those disambiguated instances that could be mapped to PWN 3.0, while the rest of instances were removed. We also rely on BabelNet to gather a representative set of candidate senses for any given target word. 3.3 FarsNet To extend the evaluation set beyond"
2021.mrl-1.10,S07-1011,0,0.160123,"Missing"
2021.mrl-1.10,N19-4009,0,0.0133653,"age, we first datasets from the unified WSD evaluation framegather all the candidate synsets in the source lanwork of Raganato et al. (2017).7 As expected, the guage from Babelnet. Then each candidate synset 4 Following Loureiro and Jorge (2019), we consider tokenis associated with one or more senses. For examlevel embeddings as the average of sub-token embeddings, ple, we can find two candidate PWN senses for which is computed as the sum of embeddings from the last 4 the word presente (present in Spanish). The first layers of the corresponding NLM. 5 Our code is based on the Fairseq toolkit (Ott et al., 2019). sense corresponds to the PWN synset “intermeWe run our experiments on a single RTX 2070, with a runtime diate between past and future&quot; and the second to under 2 hours for generating all embeddings used in this work. 6 “being or existing in a specified place&quot;. Finally, We experimented with NLMs trained exclusively on Italwe compute the cosine distance from the contex- ian (i.e. UmBERTo-CC and dbmdz-IT-XXL), but found that the senses learned using those models do not consistently tualized embeddings of target word (presente) to outperform the MFS baseline on both test sets. 7 all the candidate"
2021.mrl-1.10,W14-0132,0,0.218083,"n lexical semantics. Currently, the dominant WSD paradigm is the supervised approach (Raganato et al., 2017), which highly relies on sense-annotated data. Similarly to many other supervised tasks, the amount of labeled (sense-annotated) data for WSD highly determines downstream performance. One of the factors that make WSD a challenging problem is that creating sense-annotated data is an expensive and arduous process, i.e., the so-called knowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred a"
2021.mrl-1.10,P19-1493,0,0.0537941,"Missing"
2021.mrl-1.10,E17-1010,1,0.78396,"e across languages and test them on the benchmark. Experimental results show that this contextualized knowledge can be effectively transferred to similar languages through pre-trained multilingual language models, to the extent that they can outperform monolingual representations learned from existing language-specific data. 1 Introduction Word Sense Disambiguation (WSD) is an indispensable component of language understanding (Navigli, 2009); hence, it has been one of the most studied long-standing problems in lexical semantics. Currently, the dominant WSD paradigm is the supervised approach (Raganato et al., 2017), which highly relies on sense-annotated data. Similarly to many other supervised tasks, the amount of labeled (sense-annotated) data for WSD highly determines downstream performance. One of the factors that make WSD a challenging problem is that creating sense-annotated data is an expensive and arduous process, i.e., the so-called knowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated ("
2021.mrl-1.10,2020.emnlp-main.584,1,0.853486,"Missing"
2021.mrl-1.10,2020.lrec-1.706,1,0.733327,"tly, the dominant WSD paradigm is the supervised approach (Raganato et al., 2017), which highly relies on sense-annotated data. Similarly to many other supervised tasks, the amount of labeled (sense-annotated) data for WSD highly determines downstream performance. One of the factors that make WSD a challenging problem is that creating sense-annotated data is an expensive and arduous process, i.e., the so-called knowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et"
2021.mrl-1.10,2020.lrec-1.723,0,0.141421,"Similarly to many other supervised tasks, the amount of labeled (sense-annotated) data for WSD highly determines downstream performance. One of the factors that make WSD a challenging problem is that creating sense-annotated data is an expensive and arduous process, i.e., the so-called knowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et al., 2020; Cao et al., 2020). Therefore, cross-lingual transfer has opened new opportunities to circumvent the kn"
2021.mrl-1.10,L18-1268,0,0.0164441,"sense-annotated data. Similarly to many other supervised tasks, the amount of labeled (sense-annotated) data for WSD highly determines downstream performance. One of the factors that make WSD a challenging problem is that creating sense-annotated data is an expensive and arduous process, i.e., the so-called knowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et al., 2020; Cao et al., 2020). Therefore, cross-lingual transfer has opened new opportuniti"
2021.mrl-1.10,N18-1202,0,0.350205,"nowledge-acquisition bottleneck (Gale et al., 1992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et al., 2020; Cao et al., 2020). Therefore, cross-lingual transfer has opened new opportunities to circumvent the knowledge acquisition bottleneck for less-resourced languages. In this paper, we aim at investigating this opportunity. To this end, we build upon recent research on cross-lingual transfer to compute contextualized sense embeddings and verify if semantic distinctions in the English langua"
2021.mrl-1.10,2019.gwc-1.14,0,0.0154849,"992). Moreover, WSD research often focuses on the English language. While datasets for other languages exist (Petrolito and Bond, 2014a; Pasini and Camacho-Collados, 2020), these are generally automatically generated (Delli Bovi et al., 2017; Pasini et al., 2018; Scarlini et al., 2020a; Barba et al., 2020) or not large enough for training supervised WSD models (Navigli et al., Authors marked with a star (? ) contributed equally. 2013a; Moro and Navigli, 2015).1 However, recent contextualized embeddings have proven highly effective in English WSD (Peters et al., 2018; Loureiro and Jorge, 2019; Vial et al., 2019; Loureiro et al., 2021), as well as in capturing high-level linguistic knowledge that can be shared or transferred across different languages (Conneau et al., 2020; Cao et al., 2020). Therefore, cross-lingual transfer has opened new opportunities to circumvent the knowledge acquisition bottleneck for less-resourced languages. In this paper, we aim at investigating this opportunity. To this end, we build upon recent research on cross-lingual transfer to compute contextualized sense embeddings and verify if semantic distinctions in the English language are transferable to other languages. The c"
2021.mrl-1.10,D17-1207,0,0.0294596,"annotations. However, as usual in NLP, most of As training corpora we used SemCor (Miller et al., the sense-annotated corpora are dedicated to the 1993), which consists of a collection of English English language only. Nonetheless, recent work documents annotated with PWN senses. Despite on cross-lingual word embeddings has shown that its age, SemCor remains the standard training corit is possible to reliably align monolingual seman- pus for WSD due to its large number of manual tic spaces with minimal or no supervision (Artetxe sense annotations. There have been several efforts et al., 2017; Zhang et al., 2017; Conneau et al., towards providing sense annotations for translated 2018). Moreover, pre-trained language models, versions of SemCor (Petrolito and Bond, 2014a). like BERT (Devlin et al., 2019), have been shown Consequently, we also considered the Italian verto be effective in transferring knowledge across lan- sion of SemCor included in MultiSemCor (Benguages (Lample and Conneau, 2019; Pires et al., tivogli and Pianta, 2005), which is the language 2019; Artetxe et al., 2020). In this paper we build with most PWN annotations available. MultiSemon these ideas to take the best of both worlds. I"
2021.mrl-1.10,P10-4014,0,0.0465707,",176 1,448 3,498 RAW 226,036 92,202 1,175 1,151 1,155 1,931 1,656 1,467 1,481 1,706 4,272 Nouns Verbs Adj. Adv. EN Test - SemEval-13 FA Table 1: Number of sense-annotated instances in the benchmark datasets after cleaning and unification. RAW counts correspond to number of instances in the original datasets before cleaning and unification. ventories such as WordNet (e.g. semantic relations, sense glosses, distributions, etc.) for inference. For the last decade, the supervised approach has been the dominant paradigm for WSD (Raganato et al., 2017), either the conventional featurebased systems (Zhong and Ng, 2010; Iacobacci et al., 2016), LSTM-driven techniques (Melamud et al., 2016; Yuan et al., 2016), or the more recent trend empowered by pre-trained language models (Loureiro and Jorge, 2019; Scarlini et al., 2020b). In the latter approaches, feature extraction strategies where sense embeddings are determined by averaging a word’s contextualised representations have proven surprisingly effective (Loureiro et al., 2021), even in multilingual settings (Bevilacqua and Navigli, 2020; Raganato et al., 2020). We extend this simple idea to the cross-lingual setting, showing that the vanilla contextualized"
C16-1323,J75-4040,0,0.7351,"Missing"
C16-1323,S15-2151,0,0.0388232,"database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g."
C16-1323,P10-2020,0,0.0292908,"ore valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g. for automatically retrieving them from corpora (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgariff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Gao, 2013), and for their semantic classification according to different typologies (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, no previous work attempted the automatic enrichment of WordNet with collocational information. The only related attempt consisted in designing a schema for This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 3422 Proceedings of COLING 2016, the 26th International Conference on Compu"
C16-1323,D14-1110,0,0.0460532,"m a 3B-word corpus extracted from the web (Han et al., 2013),5 arguably richer in collocations than the encyclopedic style of Wikipedia. Similarly to S ENS E MBED, this model is based on a pre-disambiguation of text corpora using BabelNet as sense inventory. However, unlike S ENS E MBED, which learns vector representations for individual word senses, for this model we are interested in obtaining fine-grained information in the form of both plain text words and synsets6 in a shared vector space (see Section 3.2 for the motivation behind this choice, and its application). To this end, we follow Chen et al. (2014) and modify the objective function of Word2Vec,7 so that words and synsets can be learned jointly in a single training. The output is a vector space of word and synset embeddings that we use as collocates model. 3 Methodology In this section, we provide a detailed description of the algorithm behind the construction of CWN. The system takes as input the WordNet lexical database and a set of collocation lists pertaining to predefined semantic categories, and outputs CWN. First, we collect training data and perform automatic disambiguation (Section 3.1). Then, we use this disambiguated data for"
C16-1323,P89-1010,0,0.824988,"erally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g. for automatically retrieving them from corpora (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgariff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Gao, 2013), and for their semantic classification according to different typologies (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, no previous work attempted the automatic enrichment of WordNet with collocational information. The only related attempt consisted in designing a schema for This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 3422"
C16-1323,D15-1084,1,0.857019,"the idea of ‘intense’ and ‘perform’ than ‘begin to perform’), the number of instances per category in our training data also varies significantly (see Table 1). Our training dataset consists at this stage of pairs of plain words, with the inherent ambiguity this gives raise to. We surmount this challenge by applying a disambiguation strategy based on the notion that, from all the available senses for a collocation’s base and collocate, their correct senses are those which are most similar. This is a strategy that has been proved effective in previous concept-level disambiguation tasks (Delli Bovi et al., 2015). Formally, let us denote the S ENS E MBED vector space as S, and our original text-based training data as T. For each training collocation hb, ci ∈ T we consider all the available lexicalizations (i.e., senses) for both the base b and the collocate c in S, namely Lb = {lb1 ...lbn }, and Lc = {lc1 ...lcm }, and their corresponding set of sense embeddings Vb = {~v 1b , ..., ~v nb } and Vc = 4 We downloaded the pre-trained sense embeddings at http://lcl.uniroma1.it/sensembed/. ebiquity.umbc.edu/blogger/2013/05/01/umbc-webbase-corpus-of-3b-english-words/ 6 As explained above, a synset is a set co"
C16-1323,D16-1041,1,0.848512,".g., modeling analogies or projecting similar words nearby in the vector space), the most pertinent to this work is the linear relation that holds between semantically similar words in two analogous spaces (Mikolov et al., 2013b). Mikolov et al.’s original work learned a linear projection between two monolingual embeddings models to train a word-level machine translation system between English and Spanish. Other examples include the exploitation of this property for language normalization, i.e. finding regular English counterparts of Twitter language (Tan et al., 2015), or hypernym discovery (Espinosa-Anke et al., 2016). In our specific case, we learn a linear transformation from ~v 0b to ~v 0c , aiming at reflecting an inherent condition of collocations. Since collocations are a linguistic phenomenon that is more frequent in the narrative discourse than in formal essays, they are less likely to appear in an encyclopedic corpus (recall that S ENS E MBED vectors, which we use, are trained on a dump of the English Wikipedia). This motivates the use of S as our base space, and our S HARED E MBED X as the collocate model, as it was trained over more varied language such as blog posts or news items. Then, we cons"
C16-1323,esuli-sebastiani-2006-sentiwordnet,0,0.0248628,"var, 2015). It is general practice to identify and formalize conceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information ha"
C16-1323,P08-1017,0,0.0329205,") applications (Jurgens and Pilehvar, 2015). It is general practice to identify and formalize conceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and"
C16-1323,N15-1184,0,0.145399,"et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations ha"
C16-1323,P14-1113,0,0.0269,"eves the highest MRR score, which we claim to be the most relevant measure, as it rewards cases where the first ranked returned collocation is correct without measuring in the retrieved collocates at other positions. Moreover, let us highlight the importance of two main factors. First, the need for a well-defined semantic relation between bases and collocates. It has been shown in other tasks that exploit linear transformations between embeddings models that even for one single relation there may be clusters that require certain specificity in the domain or semantic of the data (see Fu et al. Fu et al. (2014) for a discussion of this phenomenon in the task of taxonomy learning). Second, the importance of having a reasonable amount of training pairs so that the model can learn the idiosyncrasies of the semantic relation that is being encoded (e.g., Mikolov et al. (2013b) report a major increase in performance as training data increases in several orders of magnitude). This is reinforced in our experiments, where we obtain the highest MAP score for ‘intense’, the semantic category for which we have the largest training data available. 13 See Bian et al. (2008) for an in-depth analysis of these metri"
C16-1323,Y13-2006,0,0.0206674,"resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g. for automatically retrieving them from corpora (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgariff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Gao, 2013), and for their semantic classification according to different typologies (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, no previous work attempted the automatic enrichment of WordNet with collocational information. The only related attempt consisted in designing a schema for This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 3422 Proceedings of COLING 2016, the 26th International Conference on Computational Lin"
C16-1323,S13-1005,0,0.0203441,"al word senses based on Word2Vec (Mikolov et al., 2013a). Unlike other sense-based embeddings approaches, such as, e.g., Huang et al. (2012), which address the inherent polysemy of word-level representations relying solely on text corpora, S ENS E MBED exploits the structured knowledge of BabelNet along with distributional information gathered from the Wikipedia corpus. In this paper, we used S ENS E MBED for automatically disambiguating our training data, and as our bases model. S HARED E MBED. For this model we exploit distributional information from a 3B-word corpus extracted from the web (Han et al., 2013),5 arguably richer in collocations than the encyclopedic style of Wikipedia. Similarly to S ENS E MBED, this model is based on a pre-disambiguation of text corpora using BabelNet as sense inventory. However, unlike S ENS E MBED, which learns vector representations for individual word senses, for this model we are interested in obtaining fine-grained information in the form of both plain text words and synsets6 in a shared vector space (see Section 3.2 for the motivation behind this choice, and its application). To this end, we follow Chen et al. (2014) and modify the objective function of Word"
C16-1323,P12-1092,0,0.364618,"ry, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, h"
C16-1323,P15-1010,0,0.0362667,"In its 3.6 release version, BabelNet is composed of 6.1M concepts and 7.7M named entities. 3 For example, the concept defined as principal activity in your life that you do to earn money is represented by the synset {occupation, business, job, line of work, line}, where occupation, business, job, line of work, and line are senses/lexicalizations of the given synset. 2 3423 its corresponding synsetnwn , provided there exists one. In what follows, we briefly describe two different vector space models that are used in this paper for the task of synset-level collocation discovery. S ENS E MBED4 (Iacobacci et al., 2015) is a knowledge-based approach for obtaining latent continuous representations of individual word senses based on Word2Vec (Mikolov et al., 2013a). Unlike other sense-based embeddings approaches, such as, e.g., Huang et al. (2012), which address the inherent polysemy of word-level representations relying solely on text corpora, S ENS E MBED exploits the structured knowledge of BabelNet along with distributional information gathered from the Wikipedia corpus. In this paper, we used S ENS E MBED for automatically disambiguating our training data, and as our bases model. S HARED E MBED. For this"
C16-1323,N15-1169,0,0.0589463,"grained collocational information, automatically introduced thanks to a method exploiting linear relations between analogous sense-level embeddings spaces. We perform both intrinsic and extrinsic evaluations, and release CWN for the use and scrutiny of the community. 1 Introduction The embedding of cues about how we perceive concepts and how these concepts relate and generalize across different domains gives knowledge resources the capacity of generalization, which lies at the core of human cognition (Yu et al., 2015) and is also central to many Natural Language Processing (NLP) applications (Jurgens and Pilehvar, 2015). It is general practice to identify and formalize conceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and"
C16-1323,N13-1090,0,0.716301,"rence on Computational Linguistics: Technical Papers, pages 3422–3432, Osaka, Japan, December 11-17 2016. the manual inclusion of lexical functions from Explanatory Combinatorial Lexicology (ECL) (Mel’ˇcuk, 1996) into the Spanish EuroWordNet (Wanner et al., 2004). Given the importance of collocations for a series of NLP applications (e.g. machine translation, text generation or paraphrasing), we propose to fill this gap by putting forward a new methodology which exploits intrinsic properties of state-of-the-art semantic vector space models and leverages the transformation matrix introduced by Mikolov et al. (2013b) in a word-level machine translation task. As a result, we release an extension of WordNet with detailed collocational information, named ColWordNet (CWN). This extension is carried out by means of the inclusion of novel edges, where each edge encodes a collocates-with relation, as well as the semantics of the collocation itself. For example, given the pair col:intense of synsets desire.n.01 and ardent.a.01, a novel relation −−−−−−−→ is introduced, where ‘inx tense’ is the semantic category denoting intensification, and x is the confidence score assigned by our algorithm. The remainder of th"
C16-1323,N16-1018,0,0.0583307,"Missing"
C16-1323,P16-2074,0,0.0357694,"ocation clusters by extracting all the synsets associated lemmas (e.g. for heavy.a.01 rain.n.01, we would extract the cluster [heavy, rain, rainfall]). These are used as input for the Retrofitting Word Vectors algorithm (Faruqui et al., 2015).15 This algorithm takes as input a vector space and a semantic lexicon which may encode any semantic relation, and puts closer in the vector space words that are related in the lexicon. Previous approaches have encoded semantic relations by introducing some kind of bias into a vector space model (Yu et al., 2015; Pham et al., 2015; Mrkˇsi´c et al., 2016; Nguyen et al., 2016). For instance, Yu et al. (2015) encode (term, hypernym) relations by grouping together terms and their hypernyms, rather than semantically related items. In this way, their biased model puts closer to jaguar terms like animal or vehicle, while an unbiased model would put nearby terms such as lion, bmw or jungle. We aim at introducing a similar bias, but in terms of collocational information. This is achieved, for each lexical function and each synset in CWN-st, by obtaining its top 3 collocate candidates and incorporate information on their collocationality into the model. 4.2.1 Collocational"
C16-1323,P15-2004,0,0.0213646,"ings model.14 To this end, we extract collocation clusters by extracting all the synsets associated lemmas (e.g. for heavy.a.01 rain.n.01, we would extract the cluster [heavy, rain, rainfall]). These are used as input for the Retrofitting Word Vectors algorithm (Faruqui et al., 2015).15 This algorithm takes as input a vector space and a semantic lexicon which may encode any semantic relation, and puts closer in the vector space words that are related in the lexicon. Previous approaches have encoded semantic relations by introducing some kind of bias into a vector space model (Yu et al., 2015; Pham et al., 2015; Mrkˇsi´c et al., 2016; Nguyen et al., 2016). For instance, Yu et al. (2015) encode (term, hypernym) relations by grouping together terms and their hypernyms, rather than semantically related items. In this way, their biased model puts closer to jaguar terms like animal or vehicle, while an unbiased model would put nearby terms such as lion, bmw or jungle. We aim at introducing a similar bias, but in terms of collocational information. This is achieved, for each lexical function and each synset in CWN-st, by obtaining its top 3 collocate candidates and incorporate information on their colloca"
C16-1323,P13-1132,0,0.0216348,"onceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idio"
C16-1323,L16-1367,1,0.874671,"Missing"
C16-1323,P15-2108,0,0.102421,"been explored so far in the literature (e.g., modeling analogies or projecting similar words nearby in the vector space), the most pertinent to this work is the linear relation that holds between semantically similar words in two analogous spaces (Mikolov et al., 2013b). Mikolov et al.’s original work learned a linear projection between two monolingual embeddings models to train a word-level machine translation system between English and Spanish. Other examples include the exploitation of this property for language normalization, i.e. finding regular English counterparts of Twitter language (Tan et al., 2015), or hypernym discovery (Espinosa-Anke et al., 2016). In our specific case, we learn a linear transformation from ~v 0b to ~v 0c , aiming at reflecting an inherent condition of collocations. Since collocations are a linguistic phenomenon that is more frequent in the narrative discourse than in formal essays, they are less likely to appear in an encyclopedic corpus (recall that S ENS E MBED vectors, which we use, are trained on a dump of the English Wikipedia). This motivates the use of S as our base space, and our S HARED E MBED X as the collocate model, as it was trained over more varied lang"
C16-1323,wanner-etal-2004-enriching,1,0.827897,"Missing"
C16-1323,S16-1169,0,\N,Missing
D16-1041,P14-1098,0,0.0292126,"Missing"
D16-1041,W11-2501,0,0.0222365,"information is also used for supervised definition and hypernym extraction (Navigli and Velardi, 2010; Boella and Di Caro, 2013), or together with Wikipedia-specific heuristics (Flati et al., 2014). One of the main drawbacks of these methods is that they require both term and hypernym to co-occur in text within a certain window, which strongly hinders their recall. Higher recall can be achieved thanks to distributional methods, as they do not have co-occurrence requirements. In addition, they can be tailored to cover any number of predefined semantic relations such as cohyponymy or meronymy (Baroni and Lenci, 2011), but also cause-effect or entity-origin (Hendrickx et al., 2009). However, they are often more imprecise and seem to perform best in discovering broader semantic relations (Shwartz et al., 2016). One way to surmount the issue of generality was proposed by Fu et al. (2014), who explored the possibility to learn a hypernymic transformation matrix over a word embeddings space. As shown empirically in Fu et al.’s original work, the hypernymic relation that holds for the pair (dragonfly, insect) differs from the one of e.g. (carpenter, man). Prior to training, their system addresses this discrepan"
D16-1041,E12-1004,0,0.0546851,"d Di Caro, 2013; Espinosa-Anke et al., 2016), clustering (Yang and Callan, 2009) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013). Moreover, work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings (Mikolov et al., 2013d). In this area, supervised approaches, arguably the most popular nowadays, learn a feature vector between term-hypernym vector pairs and train classifiers to predict hypernymic relations. These pairs may be represented either as a concatenation of both vectors (Baroni et al., 2012), difference (Roller et al., 2014), dot-product (Mikolov et al., 2013c), or including additional linguistic information for LSTMbased learning (Shwartz et al., 2016). In this paper we propose TAXO E MBED2 , a hypernym detection algorithm based on sense embeddings, which can be easily applied to the construction of lexical taxonomies. It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces (Mikolov et al., 2013b) and, unlike previous approaches, leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain o"
D16-1041,S15-2151,0,0.0812627,"dings space. As shown empirically in Fu et al.’s original work, the hypernymic relation that holds for the pair (dragonfly, insect) differs from the one of e.g. (carpenter, man). Prior to training, their system addresses this discrepancy via k-means clustering using a held-out development set for tuning. The previously described methods for hypernym and taxonomy learning operate inherently at the surface level. This is partly due to the way evaluation is conducted, which is often limited to very specific domains with no integrative potential (e.g. taxonomies in food, science or equipment from Bordea et al. (2015)), or restricted to lists of word pairs. Hence, a drawback of surface-level taxonomy learning, apart from ambiguity issues, is that they require additional and error-prone steps to identify semantic clusters (Fu et al., 2014). Alternatively, recent advances in OIE based on disambiguation and deeper semantic analysis (Nakashole et al., 2012; Grycner and Weikum, 2014; Delli Bovi et al., 2015b) have shown their potential to construct taxonomized disambiguated resources both at node and at relation level. However, in addition to their inherently broader scope, OIE approaches are designed to achiev"
D16-1041,P15-1072,1,0.827607,"a predefined knowledge domain, under the assumption that vectors clustered with this criterion are likely to exhibit similar semantic properties (e.g. similarity). First, we allocate each synset into its most representative domain, which is achieved by exploiting the set of thirty four domains available in the Wikipedia featured articles page9 . Warfare, transport, or music are some of these domains. In the Wikipedia featured articles page each domain is composed of 128 Wikipedia pages on average. Then, in order to expand the set of concepts associated with each domain, we leverage NASARI10 (Camacho-Collados et al., 2015), a distributional approach that has been used to construct explicit vector representations of BabelNet synsets. Our goal is to associate BabelNet synsets with domains. To this end, we follow Camacho-Collados et al. (2016) and build a lexical vector for each Wikipedia domain by concatenating all Wikipedia pages representing the given domain into a single text. Finally, given a BabelNet synset b, we calculate the similarity between its corresponding NASARI lexical vector and all the domain vectors, selecting the domain leading to the highest similarity score: ˆ = max W O(d, ~ ~b) d(b) d∈D (1) w"
D16-1041,D15-1084,1,0.712095,"f lexical taxonomies. It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces (Mikolov et al., 2013b) and, unlike previous approaches, leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge. Our best configuration (ranking first in two thirds of the experiments conducted) considers two training sources: (1) Manually curated pairs from Wikidata (Vrandeˇci´c and Krötzsch, 2014); and (2) Hypernymy relations from a KB which integrates several Open Information Extraction (OIE) systems (Delli Bovi et al., 2015a). Since our method uses a very large semantic network as reference sense inventory, we are able to perform jointly hypernym extraction and disambiguation, from which 1 The terminology is not entirely unified in this respect. In addition to pattern-based (Fountain and Lapata, 2012; Bansal et al., 2014; Yu et al., 2015), other terms like path-based (Shwartz et al., 2016) or rule-based (Navigli and Velardi, 2010) are also used. 2 Data and source code available from the following link: www.taln.upf.edu/taxoembed. 425 expanding existing ontologies becomes a trivial task. Compared to word-level ta"
D16-1041,Q15-1038,1,0.689134,"f lexical taxonomies. It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces (Mikolov et al., 2013b) and, unlike previous approaches, leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge. Our best configuration (ranking first in two thirds of the experiments conducted) considers two training sources: (1) Manually curated pairs from Wikidata (Vrandeˇci´c and Krötzsch, 2014); and (2) Hypernymy relations from a KB which integrates several Open Information Extraction (OIE) systems (Delli Bovi et al., 2015a). Since our method uses a very large semantic network as reference sense inventory, we are able to perform jointly hypernym extraction and disambiguation, from which 1 The terminology is not entirely unified in this respect. In addition to pattern-based (Fountain and Lapata, 2012; Bansal et al., 2014; Yu et al., 2015), other terms like path-based (Shwartz et al., 2016) or rule-based (Navigli and Velardi, 2010) are also used. 2 Data and source code available from the following link: www.taln.upf.edu/taxoembed. 425 expanding existing ontologies becomes a trivial task. Compared to word-level ta"
D16-1041,P14-1089,0,0.076031,"minent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as N ELL (Carlson et al., 2010) rely crucially on taxonomized concepts and their relations within their learning process. Taxonomy learning is roughly based on a twostep process, namely is-a (hypernymic) relation de424 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 424–435, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics tection, and graph induction. The hypernym detection phase has gathered m"
D16-1041,N12-1051,0,0.0843009,"criterion (Hwang et al., 2012). By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as N ELL (Carlson et al., 2010) rely crucially on taxonomized concepts"
D16-1041,P14-1113,0,0.221824,"pernym to co-occur in text within a certain window, which strongly hinders their recall. Higher recall can be achieved thanks to distributional methods, as they do not have co-occurrence requirements. In addition, they can be tailored to cover any number of predefined semantic relations such as cohyponymy or meronymy (Baroni and Lenci, 2011), but also cause-effect or entity-origin (Hendrickx et al., 2009). However, they are often more imprecise and seem to perform best in discovering broader semantic relations (Shwartz et al., 2016). One way to surmount the issue of generality was proposed by Fu et al. (2014), who explored the possibility to learn a hypernymic transformation matrix over a word embeddings space. As shown empirically in Fu et al.’s original work, the hypernymic relation that holds for the pair (dragonfly, insect) differs from the one of e.g. (carpenter, man). Prior to training, their system addresses this discrepancy via k-means clustering using a held-out development set for tuning. The previously described methods for hypernym and taxonomy learning operate inherently at the surface level. This is partly due to the way evaluation is conducted, which is often limited to very specifi"
D16-1041,C14-1207,0,0.0192907,"y learning operate inherently at the surface level. This is partly due to the way evaluation is conducted, which is often limited to very specific domains with no integrative potential (e.g. taxonomies in food, science or equipment from Bordea et al. (2015)), or restricted to lists of word pairs. Hence, a drawback of surface-level taxonomy learning, apart from ambiguity issues, is that they require additional and error-prone steps to identify semantic clusters (Fu et al., 2014). Alternatively, recent advances in OIE based on disambiguation and deeper semantic analysis (Nakashole et al., 2012; Grycner and Weikum, 2014; Delli Bovi et al., 2015b) have shown their potential to construct taxonomized disambiguated resources both at node and at relation level. However, in addition to their inherently broader scope, OIE approaches are designed to achieve high coverage, and hence they tend to produce noisier data compared to taxonomy learning systems. In our sense-based approach, instead, not only do we leverage an unambiguous vector representation for hypernym discovery, but we also take advantage of a domain-wise clustering strategy to directly obtain specific term-hypernym training pairs, thereby substantially"
D16-1041,C92-2082,0,0.616227,"lson et al., 2010) rely crucially on taxonomized concepts and their relations within their learning process. Taxonomy learning is roughly based on a twostep process, namely is-a (hypernymic) relation de424 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 424–435, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics tection, and graph induction. The hypernym detection phase has gathered much interest not only for taxonomy learning but also for lexical semantics. It has been addressed by means of pattern-based methods1 (Hearst, 1992; Snow et al., 2004; Kozareva and Hovy, 2010; Carlson et al., 2010; Boella and Di Caro, 2013; Espinosa-Anke et al., 2016), clustering (Yang and Callan, 2009) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013). Moreover, work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings (Mikolov et al., 2013d). In this area, supervised approaches, arguably the most popular nowadays, learn a feature vector between term-hypernym vector pairs and train classifiers to predict hypernymic relation"
D16-1041,W09-2415,0,0.00699351,"extraction (Navigli and Velardi, 2010; Boella and Di Caro, 2013), or together with Wikipedia-specific heuristics (Flati et al., 2014). One of the main drawbacks of these methods is that they require both term and hypernym to co-occur in text within a certain window, which strongly hinders their recall. Higher recall can be achieved thanks to distributional methods, as they do not have co-occurrence requirements. In addition, they can be tailored to cover any number of predefined semantic relations such as cohyponymy or meronymy (Baroni and Lenci, 2011), but also cause-effect or entity-origin (Hendrickx et al., 2009). However, they are often more imprecise and seem to perform best in discovering broader semantic relations (Shwartz et al., 2016). One way to surmount the issue of generality was proposed by Fu et al. (2014), who explored the possibility to learn a hypernymic transformation matrix over a word embeddings space. As shown empirically in Fu et al.’s original work, the hypernymic relation that holds for the pair (dragonfly, insect) differs from the one of e.g. (carpenter, man). Prior to training, their system addresses this discrepancy via k-means clustering using a held-out development set for tu"
D16-1041,P12-1092,0,0.0162966,"er. Initially, |W |= 5,301,867 and |K |= 1,358,949. 3.2 Sense vectors S ENS E MBED (Iacobacci et al., 2015)8 constitutes the sense embeddings space that we use for training our hypernym detection algorithm. Vectors in the S ENS E MBED space, denoted as S, are latent continuous representations of word senses based on the Word2Vec architecture (Mikolov et al., 2013a), which was applied on a disambiguated Wikipedia corpus. Each vector ~v ∈ S represents a BabelNet sense, i.e. a synset along with one of its lexicalizations (e.g. album_chart_bn:00002488n). This differs from unsupervised approaches (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) that learn sense representations from text corpora only and are not mapped to any lexical resource, limiting their application in our task. 4 Methodology Our approach can be summarized as follows. First, we take advantage of a clustering algorithm for allocating each BabelNet synset of the training set into a domain cluster C (Section 4.1). Then, we expand the training set by exploiting the different lexicalizations available for each BabelNet synset (Section 4.2). Finally, we learn a cluster-wise linear projection (a hypernym transformation matri"
D16-1041,P15-1010,0,0.0659627,"ures its own manually-built taxonomic structure and relation type inventory (hence its own is-a relation type), we identified the relation synset containing N ELL’s is-a7 and then drew from the unified KB all the corresponding triples, which we denote as K. These triples constitute, similarly as in the previous case, a set of term-hypernym pairs automatically extracted from OIE-derived resources, with a disambiguation confidence of above 0.9 according to the disambiguation strategy described in the original paper. Initially, |W |= 5,301,867 and |K |= 1,358,949. 3.2 Sense vectors S ENS E MBED (Iacobacci et al., 2015)8 constitutes the sense embeddings space that we use for training our hypernym detection algorithm. Vectors in the S ENS E MBED space, denoted as S, are latent continuous representations of word senses based on the Word2Vec architecture (Mikolov et al., 2013a), which was applied on a disambiguated Wikipedia corpus. Each vector ~v ∈ S represents a BabelNet sense, i.e. a synset along with one of its lexicalizations (e.g. album_chart_bn:00002488n). This differs from unsupervised approaches (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) that learn sense representations from text"
D16-1041,D10-1108,0,0.221822,"nomies henceforth) are graph-like hierarchical structures where terms are nodes, and are typically organized over a predefined merging or splitting criterion (Hwang et al., 2012). By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy le"
D16-1041,N15-1098,0,0.0797191,"Missing"
D16-1041,D14-1088,0,0.0134584,"zed over a predefined merging or splitting criterion (Hwang et al., 2012). By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as N ELL (Carlson e"
D16-1041,D15-1117,0,0.010831,"generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as N ELL (Carlson et al., 2010) rely crucially on taxonomized concepts and their relations within their learning process. Taxonomy learning is roughly based on a twos"
D16-1041,N13-1090,0,0.635083,"graph induction. The hypernym detection phase has gathered much interest not only for taxonomy learning but also for lexical semantics. It has been addressed by means of pattern-based methods1 (Hearst, 1992; Snow et al., 2004; Kozareva and Hovy, 2010; Carlson et al., 2010; Boella and Di Caro, 2013; Espinosa-Anke et al., 2016), clustering (Yang and Callan, 2009) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013). Moreover, work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings (Mikolov et al., 2013d). In this area, supervised approaches, arguably the most popular nowadays, learn a feature vector between term-hypernym vector pairs and train classifiers to predict hypernymic relations. These pairs may be represented either as a concatenation of both vectors (Baroni et al., 2012), difference (Roller et al., 2014), dot-product (Mikolov et al., 2013c), or including additional linguistic information for LSTMbased learning (Shwartz et al., 2016). In this paper we propose TAXO E MBED2 , a hypernym detection algorithm based on sense embeddings, which can be easily applied to the construction of"
D16-1041,D12-1104,0,0.0559942,"Missing"
D16-1041,P10-1134,0,0.428627,"ures where terms are nodes, and are typically organized over a predefined merging or splitting criterion (Hwang et al., 2012). By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Info"
D16-1041,D14-1113,0,0.0139555,"= 1,358,949. 3.2 Sense vectors S ENS E MBED (Iacobacci et al., 2015)8 constitutes the sense embeddings space that we use for training our hypernym detection algorithm. Vectors in the S ENS E MBED space, denoted as S, are latent continuous representations of word senses based on the Word2Vec architecture (Mikolov et al., 2013a), which was applied on a disambiguated Wikipedia corpus. Each vector ~v ∈ S represents a BabelNet sense, i.e. a synset along with one of its lexicalizations (e.g. album_chart_bn:00002488n). This differs from unsupervised approaches (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) that learn sense representations from text corpora only and are not mapped to any lexical resource, limiting their application in our task. 4 Methodology Our approach can be summarized as follows. First, we take advantage of a clustering algorithm for allocating each BabelNet synset of the training set into a domain cluster C (Section 4.1). Then, we expand the training set by exploiting the different lexicalizations available for each BabelNet synset (Section 4.2). Finally, we learn a cluster-wise linear projection (a hypernym transformation matrix) over all pairs (term-hypernym) of the expan"
D16-1041,P13-1132,0,0.0162698,"ollow Camacho-Collados et al. (2016) and build a lexical vector for each Wikipedia domain by concatenating all Wikipedia pages representing the given domain into a single text. Finally, given a BabelNet synset b, we calculate the similarity between its corresponding NASARI lexical vector and all the domain vectors, selecting the domain leading to the highest similarity score: ˆ = max W O(d, ~ ~b) d(b) d∈D (1) where D is the set of all thirty-three domains, d~ is the vector of the domain d ∈ D, ~b is the vector of the BabelNet synset b, and WO refers to the Weighted Overlap comparison measure (Pilehvar et al., 2013), which is defined as follows: v uP −1 u w∈O rankw,v~1 + rankw,v~2 t W O(v~1 , v~2 ) = P|O| −1 i=1 (2i) (2) where rankw,v~i is the rank of the word w in the vector v~i according to its weight, and O is the set of overlapping words between the two vectors. In order to have a highly reliable set of domain labels, those 9 https://en.wikipedia.org/wiki/ Wikipedia:Featured_articles 10 http://lcl.uniroma1.it/nasari synsets whose maximum similarity score is below a certain threshold are not annotated with any domain. We fixed the threshold to 0.35, which provided a fine balance between precision (es"
D16-1041,C14-1097,0,0.121493,"Missing"
D16-1041,P16-1226,0,0.119326,"Missing"
D16-1041,P06-1101,0,0.0214229,"Missing"
D16-1041,P15-2108,0,0.0629219,"Missing"
D16-1041,C14-1016,0,0.0205832,"5,301,867 and |K |= 1,358,949. 3.2 Sense vectors S ENS E MBED (Iacobacci et al., 2015)8 constitutes the sense embeddings space that we use for training our hypernym detection algorithm. Vectors in the S ENS E MBED space, denoted as S, are latent continuous representations of word senses based on the Word2Vec architecture (Mikolov et al., 2013a), which was applied on a disambiguated Wikipedia corpus. Each vector ~v ∈ S represents a BabelNet sense, i.e. a synset along with one of its lexicalizations (e.g. album_chart_bn:00002488n). This differs from unsupervised approaches (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) that learn sense representations from text corpora only and are not mapped to any lexical resource, limiting their application in our task. 4 Methodology Our approach can be summarized as follows. First, we take advantage of a clustering algorithm for allocating each BabelNet synset of the training set into a domain cluster C (Section 4.1). Then, we expand the training set by exploiting the different lexicalizations available for each BabelNet synset (Section 4.2). Finally, we learn a cluster-wise linear projection (a hypernym transformation matrix) over all pairs ("
D16-1041,J13-3007,0,0.223644,"12). By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as N ELL (Carlson et al., 2010) rely crucially on taxonomized concepts and their relations w"
D16-1041,P09-1031,0,0.0118506,"a twostep process, namely is-a (hypernymic) relation de424 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 424–435, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics tection, and graph induction. The hypernym detection phase has gathered much interest not only for taxonomy learning but also for lexical semantics. It has been addressed by means of pattern-based methods1 (Hearst, 1992; Snow et al., 2004; Kozareva and Hovy, 2010; Carlson et al., 2010; Boella and Di Caro, 2013; Espinosa-Anke et al., 2016), clustering (Yang and Callan, 2009) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013). Moreover, work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings (Mikolov et al., 2013d). In this area, supervised approaches, arguably the most popular nowadays, learn a feature vector between term-hypernym vector pairs and train classifiers to predict hypernymic relations. These pairs may be represented either as a concatenation of both vectors (Baroni et al., 2012), difference (Roller et al., 2014), dot-product (Mikolov et"
D16-1041,P14-2089,0,0.0250439,"Missing"
D18-1027,J82-2005,0,0.591385,"Missing"
D18-1027,S18-1116,0,0.0241775,"Missing"
D18-1027,Q17-1010,0,0.638374,"e train a regression model to predict this average word vector from the vector representation of the given word only, i.e., without using the vector representation of its translation. 3 Methodology Our approach for improving cross-lingual embeddings consists of three main steps, where the first two steps are the same as in existing methods. In particular, given two monolingual corpora, a word vector space is first learned independently for each language. This can be achieved with common word embedding models, e.g., Word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017). Second, a linear alignment strategy is used to map the monolingual embeddings to a common bilingual vector space (Section 3.1). Third, a final transformation is applied on the aligned embeddings so the word vectors from both languages are refined and further integrated with each other (Section 3.2). This third step is the main contribution of our paper. 3.1 3.2 Meeting in the middle After the initial alignment of the monolingual word embeddings, our proposed method leverages an additional linear model to refine the resulting bilingual word embeddings. This is because the methods presented in"
D18-1027,S16-1168,0,0.0126182,"gether the most tend to be semantically very similar and belong to the same domain, e.g., car-bicycle, opera-cinema, or snow-ice. 4.2.3 Cross-lingual hypernym discovery Modeling hypernymy is a crucial task in NLP, with direct applications in diverse areas such as semantic search (Hoffart et al., 2014; Roller and Erk, 2016), question answering (Prager et al., 2008; Yahya et al., 2013) or textual entailment (Geffet and Dagan, 2005). Hypernyms, in addition, are the backbone of lexical ontologies (Yu et al., 2015), which are in turn useful for organizing, navigating and retrieving online content (Bordea et al., 2016). Thus, we propose to evaluate the contribution of cross-lingual embeddings towards the task of hypernym discovery, i.e., given an input word (e.g., cat), retrieve or discover its most likely (set of) valid hypernyms (e.g., animal, mammal, feline, and so on). Intuitively, by leveraging a bilingual vector space condensing the semantics of two languages, one of them being English, the need for large amounts of training data in the target language may be reduced. Results The results listed in Table 4 indicate several trends.12 First and foremost, in terms of 12 Note that this task is harder than"
D18-1027,S17-2002,1,0.894504,"Missing"
D18-1027,P15-2001,1,0.910552,"Missing"
D18-1027,D16-1250,0,0.594854,"tutes the main focus of this paper, attempts to learn bilingual embeddings via a two-step process: first, word embeddings are trained on monolingual corpora and then the resulting monolingual spaces are aligned by taking advantage of bilingual dictionaries (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xing et al., 2015). These alignments are generally modeled as linear transformations, which are constrained such that the structure of the initial monolingual spaces is left unchanged. This can be achieved by imposing an orthogonality constraint on the linear transformation (Xing et al., 2015; Artetxe et al., 2016). Our hypothesis in this paper is that such approaches can be further improved, as they rely on the assumption that the internal structure of the two monolingual spaces is identical. In reality, however, this structure is influenced by languagespecific phenomena, e.g., the fact that Spanish distinguishes between masculine and feminine nouns (Davis, 2015) as well as the specific biases of the different corpora from which the monolingual spaces were learned. Because of this, monolingual embedding spaces are not isomorphic (Søgaard et al., 2018; Kementchedjhieva et al., 2018). On the other hand,"
D18-1027,K18-1021,0,0.223682,"formation (Xing et al., 2015; Artetxe et al., 2016). Our hypothesis in this paper is that such approaches can be further improved, as they rely on the assumption that the internal structure of the two monolingual spaces is identical. In reality, however, this structure is influenced by languagespecific phenomena, e.g., the fact that Spanish distinguishes between masculine and feminine nouns (Davis, 2015) as well as the specific biases of the different corpora from which the monolingual spaces were learned. Because of this, monolingual embedding spaces are not isomorphic (Søgaard et al., 2018; Kementchedjhieva et al., 2018). On the other hand, simply dropping the orthogonality constraints leads to overfitting, and is thus not effective in practice. The solution we propose is to start with existing state-of-the-art alignment models (Artetxe et al., 2017; Conneau et al., 2018), and to apply a further transformation to the resulting initial alignment. For each word w with translation w0 , this additional transformation aims to map the vector representations of both w and w0 onto their average, thereby creating a cross-lingual vector space which intuitively corresponds to the average of the Cross-lingual word embedd"
D18-1027,D16-1041,1,0.891202,"Missing"
D18-1027,E14-1049,0,0.598831,"atics Universidade de Vigo, Spain Cardiff University, UK yerai.doval@uvigo.es camachocolladosj@cardiff.ac.uk espinosa-ankel@cardiff.ac.uk schockaerts1@cardiff.ac.uk Abstract There exist different approaches for obtaining these cross-lingual embeddings. One of the most successful methodological directions, which constitutes the main focus of this paper, attempts to learn bilingual embeddings via a two-step process: first, word embeddings are trained on monolingual corpora and then the resulting monolingual spaces are aligned by taking advantage of bilingual dictionaries (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xing et al., 2015). These alignments are generally modeled as linear transformations, which are constrained such that the structure of the initial monolingual spaces is left unchanged. This can be achieved by imposing an orthogonality constraint on the linear transformation (Xing et al., 2015; Artetxe et al., 2016). Our hypothesis in this paper is that such approaches can be further improved, as they rely on the assumption that the internal structure of the two monolingual spaces is identical. In reality, however, this structure is influenced by languagespecific phenomena, e.g., the fact tha"
D18-1027,E17-1072,0,0.024922,"orpora (Artetxe et al., 2018b; Lample et al., 2018). Moreover, the fact that such approaches only need monolingual embeddings, instead of parallel or comparable corpora, makes them easily adaptable to different domains (e.g., social media or web corpora). Related Work Bilingual word embeddings have been extensively studied in the literature in recent years. Their nature varies with respect to the supervision signals used for training (Upadhyay et al., 2016; Ruder et al., 2018). Some common signals to learn bilingual embeddings come from parallel (Hermann and Blunsom, 2014; Luong et al., 2015; Levy et al., 2017) or comparable corpora (Vuli´c and Moens, 2015a; Søgaard et al., 2015; Vuli´c and Moens, 2016), or lexical resources such as WordNet, ConceptNet or BabelNet (Speer et al., 2017; Mrksic et al., 2017; Goikoetxea et al., 2018). However, these sources of supervision may be scarce, limited to certain domains or may not be directly available for certain language pairs. Another branch of research exploits pre-trained monolingual embeddings with weak signals such as bilingual lexicons for learning bilingual embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Ammar et al., 2016; Artetxe et al.,"
D18-1027,W15-1521,0,0.0427378,"quire any parallel corpora (Artetxe et al., 2018b; Lample et al., 2018). Moreover, the fact that such approaches only need monolingual embeddings, instead of parallel or comparable corpora, makes them easily adaptable to different domains (e.g., social media or web corpora). Related Work Bilingual word embeddings have been extensively studied in the literature in recent years. Their nature varies with respect to the supervision signals used for training (Upadhyay et al., 2016; Ruder et al., 2018). Some common signals to learn bilingual embeddings come from parallel (Hermann and Blunsom, 2014; Luong et al., 2015; Levy et al., 2017) or comparable corpora (Vuli´c and Moens, 2015a; Søgaard et al., 2015; Vuli´c and Moens, 2016), or lexical resources such as WordNet, ConceptNet or BabelNet (Speer et al., 2017; Mrksic et al., 2017; Goikoetxea et al., 2018). However, these sources of supervision may be scarce, limited to certain domains or may not be directly available for certain language pairs. Another branch of research exploits pre-trained monolingual embeddings with weak signals such as bilingual lexicons for learning bilingual embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Ammar et al., 20"
D18-1027,P05-1014,0,0.00949331,"ated to the same word in Spanish (i.e., tel´efono and pel´ıcula in the first two cases) or are already very close in the Spanish space. More generally, we found that word pairs which move together the most tend to be semantically very similar and belong to the same domain, e.g., car-bicycle, opera-cinema, or snow-ice. 4.2.3 Cross-lingual hypernym discovery Modeling hypernymy is a crucial task in NLP, with direct applications in diverse areas such as semantic search (Hoffart et al., 2014; Roller and Erk, 2016), question answering (Prager et al., 2008; Yahya et al., 2013) or textual entailment (Geffet and Dagan, 2005). Hypernyms, in addition, are the backbone of lexical ontologies (Yu et al., 2015), which are in turn useful for organizing, navigating and retrieving online content (Bordea et al., 2016). Thus, we propose to evaluate the contribution of cross-lingual embeddings towards the task of hypernym discovery, i.e., given an input word (e.g., cat), retrieve or discover its most likely (set of) valid hypernyms (e.g., animal, mammal, feline, and so on). Intuitively, by leveraging a bilingual vector space condensing the semantics of two languages, one of them being English, the need for large amounts of t"
D18-1027,S13-1005,0,0.0196932,"rst and second language, respectively. For pairs (w, w0 ) ∈ D, we can simply compute the corresponding average vector ~v +~v µ ~ w,w0 = w 2 w0 . Then, using the pairs in D as training data, we learn a linear mapping X such that X~vw ≈ µ ~ w,w0 for all (w, w0 ) ∈ D. This mapping X can then be used to predict the averages for words outside the given dictionary. To find the mapping X, we solve the following least squares linear regression problem: X E= kX w ~ −µ ~ w,w0 k2 (1) 4.1 Corpora. In our experiments we make use of web-extracted corpora. For English we use the 3B-word UMBC WebBase Corpus (Han et al., 2013), while we chose the Spanish Billion Words Corpus (Cardellino, 2016) for Spanish. For Italian and German, we use the itWaC and sdeWaC corpora from the WaCky project (Baroni et al., 2009), containing 2 and 0.8 billion words, respectively.2 Lastly, for Finnish, we use the Common Crawl monolingual corpus from the Machine Translation of News Shared Task 20163 , composed of 2.8B words. All corpora are tokenized and lowercased. Monolingual embeddings. The monolingual word embeddings are trained with the Skipgram model from FastText (Bojanowski et al., 2017) on the corpora described above. The dimens"
D18-1027,N16-1083,0,0.0162264,"ings are one of the most widely used resources in NLP, as they have proven to be of enormous importance for modeling linguistic phenomena in both supervised and unsupervised settings. In particular, the representation of words in cross-lingual vector spaces (henceforth, crosslingual word embeddings) is quickly gaining in popularity. One of the main reasons is that they play a crucial role in transferring knowledge from one language to another, specifically in downstream tasks such as information retrieval (Vuli´c and Moens, 2015b), entity linking (Tsai and Roth, 2016) and text classification (Mogadala and Rettinger, 2016), while at the same time providing improvements in multilingual NLP problems such as machine translation (Zou et al., 2013). 294 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 294–304 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics gual embedding spaces into a new shared space. Artetxe et al. (2016) proposed a similar linear mapping to Mikolov et al. (2013b), generalizing it and providing theoretical justifications which also served to reinterpret the methods of Faruqui and Dyer (2014) and Xing et"
D18-1027,C92-2082,0,0.311284,"4 64.1 65.3 78.8 80.5 Table 3: Cross-lingual word similarity results. Pearson (r) and Spearman (ρ) correlation. and hypernym spaces, which is afterwards used to predict the most likely (set of) hypernyms, given an unseen hyponym. Training and evaluation data come from the SemEval 2018 Shared Task on Hypernym Discovery (Camacho-Collados et al., 2018). Note that current state-of-the-art systems aimed at modeling hypernymy (Shwartz et al., 2016; Bernier-Colborne and Barriere, 2018) combine large amounts of annotated data along with language-specific rules and cue phrases such as Hearst Patterns (Hearst, 1992), both of which are generally scarcely (if at all) available for languages other than English. Therefore, we report experiments with training data only from English (11,779 hyponym-hypernym pairs), and “enriched” models informed with relatively few training pairs (500, 1k and 2k) from the target languages. Evaluation is conducted with the same metrics as in the original SemEval task, i.e., Mean Reciprocal Rank (MRR), Mean Average Precision (MAP) and Precision at 5 (P@5). These measures explain a model’s behavior from complementary prisms, namely how often at least one valid hypernym was highly"
D18-1027,Q17-1022,0,0.0625024,"Missing"
D18-1027,P14-1006,0,0.0575018,"ion systems which do not require any parallel corpora (Artetxe et al., 2018b; Lample et al., 2018). Moreover, the fact that such approaches only need monolingual embeddings, instead of parallel or comparable corpora, makes them easily adaptable to different domains (e.g., social media or web corpora). Related Work Bilingual word embeddings have been extensively studied in the literature in recent years. Their nature varies with respect to the supervision signals used for training (Upadhyay et al., 2016; Ruder et al., 2018). Some common signals to learn bilingual embeddings come from parallel (Hermann and Blunsom, 2014; Luong et al., 2015; Levy et al., 2017) or comparable corpora (Vuli´c and Moens, 2015a; Søgaard et al., 2015; Vuli´c and Moens, 2016), or lexical resources such as WordNet, ConceptNet or BabelNet (Speer et al., 2017; Mrksic et al., 2017; Goikoetxea et al., 2018). However, these sources of supervision may be scarce, limited to certain domains or may not be directly available for certain language pairs. Another branch of research exploits pre-trained monolingual embeddings with weak signals such as bilingual lexicons for learning bilingual embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 20"
D18-1027,D14-1162,0,0.0859929,"bedding space. translation. Instead, we train a regression model to predict this average word vector from the vector representation of the given word only, i.e., without using the vector representation of its translation. 3 Methodology Our approach for improving cross-lingual embeddings consists of three main steps, where the first two steps are the same as in existing methods. In particular, given two monolingual corpora, a word vector space is first learned independently for each language. This can be achieved with common word embedding models, e.g., Word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017). Second, a linear alignment strategy is used to map the monolingual embeddings to a common bilingual vector space (Section 3.1). Third, a final transformation is applied on the aligned embeddings so the word vectors from both languages are refined and further integrated with each other (Section 3.2). This third step is the main contribution of our paper. 3.1 3.2 Meeting in the middle After the initial alignment of the monolingual word embeddings, our proposed method leverages an additional linear model to refine the resulting bilingual word embeddings. Th"
D18-1027,P16-1157,0,0.0341811,"aches can be found in Section 3.1. These models have in turn paved the way for the development of machine translation systems which do not require any parallel corpora (Artetxe et al., 2018b; Lample et al., 2018). Moreover, the fact that such approaches only need monolingual embeddings, instead of parallel or comparable corpora, makes them easily adaptable to different domains (e.g., social media or web corpora). Related Work Bilingual word embeddings have been extensively studied in the literature in recent years. Their nature varies with respect to the supervision signals used for training (Upadhyay et al., 2016; Ruder et al., 2018). Some common signals to learn bilingual embeddings come from parallel (Hermann and Blunsom, 2014; Luong et al., 2015; Levy et al., 2017) or comparable corpora (Vuli´c and Moens, 2015a; Søgaard et al., 2015; Vuli´c and Moens, 2016), or lexical resources such as WordNet, ConceptNet or BabelNet (Speer et al., 2017; Mrksic et al., 2017; Goikoetxea et al., 2018). However, these sources of supervision may be scarce, limited to certain domains or may not be directly available for certain language pairs. Another branch of research exploits pre-trained monolingual embeddings with"
D18-1027,N18-1056,0,0.0143412,"evaluate the contribution of cross-lingual embeddings towards the task of hypernym discovery, i.e., given an input word (e.g., cat), retrieve or discover its most likely (set of) valid hypernyms (e.g., animal, mammal, feline, and so on). Intuitively, by leveraging a bilingual vector space condensing the semantics of two languages, one of them being English, the need for large amounts of training data in the target language may be reduced. Results The results listed in Table 4 indicate several trends.12 First and foremost, in terms of 12 Note that this task is harder than hypernymy detection (Upadhyay et al., 2018). Hypernymy detection is framed as a binary classification task, while in hypernym discovery hypernyms have to be retrieved from the whole vocabulary. Experimental setting We follow EspinosaAnke et al. (2016) and learn a (cross-lingual) linear transformation matrix between the hyponym 300 Train data - EN EN + 500 EN + 1k EN + 2k Model BestUns VecMap VecMapµ MUSE MUSEµ VecMap VecMapµ MUSE MUSEµ VecMap VecMapµ MUSE MUSEµ VecMap VecMapµ MUSE MUSEµ Spanish Italian MAP MRR P@5 MAP MRR P@5 2.4 6.4 6.1 5.9 6.2 7.3 7.0 6.4 6.9 7.9 7.8 7.2 7.8 8.0 8.2 7.2 8.3 5.5 16.5 15.4 14.1 14.8 18.2 17.6 15.9 16.9"
D18-1027,D16-1234,0,0.0142208,"transformation: cellphone-telephone, moviefilm, book-manuscript or rhythm-cadence, which are either translated to the same word in Spanish (i.e., tel´efono and pel´ıcula in the first two cases) or are already very close in the Spanish space. More generally, we found that word pairs which move together the most tend to be semantically very similar and belong to the same domain, e.g., car-bicycle, opera-cinema, or snow-ice. 4.2.3 Cross-lingual hypernym discovery Modeling hypernymy is a crucial task in NLP, with direct applications in diverse areas such as semantic search (Hoffart et al., 2014; Roller and Erk, 2016), question answering (Prager et al., 2008; Yahya et al., 2013) or textual entailment (Geffet and Dagan, 2005). Hypernyms, in addition, are the backbone of lexical ontologies (Yu et al., 2015), which are in turn useful for organizing, navigating and retrieving online content (Bordea et al., 2016). Thus, we propose to evaluate the contribution of cross-lingual embeddings towards the task of hypernym discovery, i.e., given an input word (e.g., cat), retrieve or discover its most likely (set of) valid hypernyms (e.g., animal, mammal, feline, and so on). Intuitively, by leveraging a bilingual vecto"
D18-1027,P15-2118,0,0.0651219,"Missing"
D18-1027,P16-1226,0,0.0149568,".9 62.7 70.5 70.1 61.2 62.7 English-German SemEval WordSim RG-65 r ρ r ρ r ρ 71.6 71.3 64.1 65.9 78.1 78.8 72.0 71.5 64.2 65.4 78.6 79.7 70.4 70.1 63.5 65.1 78.4 79.5 71.9 71.4 64.1 65.3 78.8 80.5 Table 3: Cross-lingual word similarity results. Pearson (r) and Spearman (ρ) correlation. and hypernym spaces, which is afterwards used to predict the most likely (set of) hypernyms, given an unseen hyponym. Training and evaluation data come from the SemEval 2018 Shared Task on Hypernym Discovery (Camacho-Collados et al., 2018). Note that current state-of-the-art systems aimed at modeling hypernymy (Shwartz et al., 2016; Bernier-Colborne and Barriere, 2018) combine large amounts of annotated data along with language-specific rules and cue phrases such as Hearst Patterns (Hearst, 1992), both of which are generally scarcely (if at all) available for languages other than English. Therefore, we report experiments with training data only from English (11,779 hyponym-hypernym pairs), and “enriched” models informed with relatively few training pairs (500, 1k and 2k) from the target languages. Evaluation is conducted with the same metrics as in the original SemEval task, i.e., Mean Reciprocal Rank (MRR), Mean Averag"
D18-1027,N15-1104,0,0.387469,"Missing"
D18-1027,E17-1007,0,0.0121026,"in a model’s behavior from complementary prisms, namely how often at least one valid hypernym was highly ranked (MRR), and in cases where there is more than one correct hypernym, to what extent they were all correctly retrieved (MAP and P@5). Finally, as in the previous experiments, we report comparative results between our proposed models and the two competing baselines (VecMap and MUSE). As an additional informative baseline, we include the highest scoring unsupervised system at the SemEval task for both Spanish and Italian (BestUns), which is based on the distributional models described in Shwartz et al. (2017). help understand these monolingual gains. For example, a recurring pattern is that words in English which are translated to the same word, or to semantically close words, in the target language end up closer together after our transformation. For example, in the case of EnglishSpanish the following pairs were among the pairs whose similarity increased the most by applying our transformation: cellphone-telephone, moviefilm, book-manuscript or rhythm-cadence, which are either translated to the same word in Spanish (i.e., tel´efono and pel´ıcula in the first two cases) or are already very close"
D18-1027,P15-1165,0,0.113493,"Missing"
D18-1027,P16-1128,0,0.0714907,"Missing"
D18-1027,P18-1072,0,0.16607,"Missing"
D18-1027,D13-1141,0,0.0675294,"na in both supervised and unsupervised settings. In particular, the representation of words in cross-lingual vector spaces (henceforth, crosslingual word embeddings) is quickly gaining in popularity. One of the main reasons is that they play a crucial role in transferring knowledge from one language to another, specifically in downstream tasks such as information retrieval (Vuli´c and Moens, 2015b), entity linking (Tsai and Roth, 2016) and text classification (Mogadala and Rettinger, 2016), while at the same time providing improvements in multilingual NLP problems such as machine translation (Zou et al., 2013). 294 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 294–304 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics gual embedding spaces into a new shared space. Artetxe et al. (2016) proposed a similar linear mapping to Mikolov et al. (2013b), generalizing it and providing theoretical justifications which also served to reinterpret the methods of Faruqui and Dyer (2014) and Xing et al. (2015). Smith et al. (2017) further showed how orthogonality was required to improve the consistency of bilingual mapp"
D18-1027,N16-1072,0,0.0192926,"l evaluation tasks. 1 Introduction Word embeddings are one of the most widely used resources in NLP, as they have proven to be of enormous importance for modeling linguistic phenomena in both supervised and unsupervised settings. In particular, the representation of words in cross-lingual vector spaces (henceforth, crosslingual word embeddings) is quickly gaining in popularity. One of the main reasons is that they play a crucial role in transferring knowledge from one language to another, specifically in downstream tasks such as information retrieval (Vuli´c and Moens, 2015b), entity linking (Tsai and Roth, 2016) and text classification (Mogadala and Rettinger, 2016), while at the same time providing improvements in multilingual NLP problems such as machine translation (Zou et al., 2013). 294 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 294–304 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics gual embedding spaces into a new shared space. Artetxe et al. (2016) proposed a similar linear mapping to Mikolov et al. (2013b), generalizing it and providing theoretical justifications which also served to reinter"
D18-1508,E17-2017,1,0.714713,"ended meaning of a social media message. In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online. It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). The problem of emoji prediction, albeit recent, has already seen important developments. For example, Barbieri et al. (2017) describe an LSTM model which outperforms a logistic regression baseline based on word vector averaging, and even human judgement in some scenarios. The above contributions, in addition to emoji similarity datasets (Barbieri et al., 2016; Wijeratne et al., 2017) or emoji sentiment lexicons (Novak et al., 2015; Wijeratne et al., 2016; Kimura and Katsurai, 2017; Rodrigues et al., 2018), have paved the way for better understanding the semantics of emojis. However, our understanding of what exactly the neural models for emoji prediction are capturing is currently very limited. What is a model prio"
D18-1508,S18-2011,1,0.881576,"Missing"
D18-1508,S18-1003,1,0.85637,"tention modifications to the “vanilla” 2-BiLSTM model. αj,l hj j=1 stacked Bi-LSTMs (2-BiLSTMs) without attention; and (3) 2 stacked Bi-LSTMs with standard attention (2-BiLSTMsa ) (Felbo et al., 2017). Finally, we denote as 2-BiLSTMsl our proposed label-wise attentive Bi-LSTM architecture. βl = wf,l sl + bf,l eβl pl = PL r=1 e 3 βr Evaluation This section describes the main experiment w.r.t the performance of our proposed attention mechanism, in comparison with existing emoji prediction systems. We use the data made available in the context of the SemEval 2018 Shared Task on Emoji Prediction (Barbieri et al., 2018). Given a tweet, the task consists of predicting an associated emoji from a predefined set of 20 emoji labels. We evaluate our model on the English split of the official task dataset. We also show results from additional experiments in which the label space ranged from 20 to 200 emojis. These extended experiments are performed on a corpus of around 100M tweets geolocalized in the United States and posted between October 2015 and May 2018. Models. In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1) FastText (Joulin et al"
D18-1508,L16-1626,1,0.748553,"ieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online. It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). The problem of emoji prediction, albeit recent, has already seen important developments. For example, Barbieri et al. (2017) describe an LSTM model which outperforms a logistic regression baseline based on word vector averaging, and even human judgement in some scenarios. The above contributions, in addition to emoji similarity datasets (Barbieri et al., 2016; Wijeratne et al., 2017) or emoji sentiment lexicons (Novak et al., 2015; Wijeratne et al., 2016; Kimura and Katsurai, 2017; Rodrigues et al., 2018), have paved the way for better understanding the semantics of emojis. However, our understanding of what exactly the neural models for emoji prediction are capturing is currently very limited. What is a model prioritizing when associating a message with, for example, positive ( ), negative ( ) or patriotic ( ) intents? A natural way of assessing this would be to implement an attention mechanism over the hidden states of LSTM layers. Attentive arc"
D18-1508,S18-1004,0,0.0966617,"ent features are used for predictions, a topic of increasing interest in NLP (Linzen et al., 2016; Palangi et al., 2017). As we experimented with sets of emoji labels of different sizes, our proposed label-wise attention architecture proved especially well-suited for emojis which were infrequent in the training data, making the system less biased towards the most frequent. We see this as a first step to improve the robustness of recurrent neural networks in datasets with unbalanced distributions, as they were shown not to perform better than well-tuned SVMs on the emoji predicion task (C ¸ o¨ ltekin and Rama, 2018). As for future work, we plan to apply our labelwise attention mechanism to understand other interesting linguistic properties of human-generated text in social media, and other multi-class or multilabel classification problems. Finally, code to reproduce our experiments and additional examples of label-wise attention weights from input tweets can be downloaded at https://fvancesco.github. io/label_wise_attention/. Acknowledgments F. Barbieri and H. Saggion acknowledge support from the TUNER project (TIN2015-65308-C5-5R, MINECO/FEDER, UE). Luis Espinosa-Anke, Jose Camacho-Collados and Steven S"
D18-1508,D17-1169,0,0.511515,"y ( ). Emoji modeling and prediction is, therefore, an important problem towards the end goal of properly capturing the intended meaning of a social media message. In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online. It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). The problem of emoji prediction, albeit recent, has already seen important developments. For example, Barbieri et al. (2017) describe an LSTM model which outperforms a logistic regression baseline based on word vector averaging, and even human judgement in some scenarios. The above contributions, in addition to emoji similarity datasets (Barbieri et al., 2016; Wijeratne et al., 2017) or emoji sentiment lexicons (Novak et al., 2015; Wijeratne et al., 2016; Kimura and Katsurai, 2017; Rodrigues et al., 2018), have paved the way for better understanding the semantics of emojis. However, our unde"
D18-1508,E17-2068,0,0.289792,"ake decisions over one single emoji at a time, but without the computational burden and risk of overfitting associated with learning separate LSTMbased classifiers for each emoji. Our contribution in this paper is twofold. First, we use the proposed label-wise mechanism to analyze the behavior of neural emoji classifiers, exploiting the attention weights to uncover and interpret emoji usages. Second, we experimentally compare the effect of the label-wise mechanism on the performance of an emoji classifier. We observed a performance improvement over competitive baselines such as FastText (FT) (Joulin et al., 2017) and Deepmoji (Felbo et al., 2017), which is most noticeable in the case of infrequent emojis. This suggests that an attentive mechanism can be leveraged to make neural architectures more sensitive to instances of underrepresented classes. 2 Methodology Our base architecture is the Deepmoji model (Felbo et al., 2017), which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention"
D18-1508,Q16-1037,0,0.0308213,"experience ! ( ) second day snowboarding ever and i decided to try night boarding ... what an experience ! ( ) second day snowboarding ever and i decided to try night boarding ... what an experience ! ( ) Figure 4: Attention weights α and αl of single and label-wise attentive models. Gold: 5 Conclusion In this paper we have presented a neural architecture for emoji prediction based on a label-wise attention mechanism, which, in addition to improving performance, provides a degree of interpretability about how different features are used for predictions, a topic of increasing interest in NLP (Linzen et al., 2016; Palangi et al., 2017). As we experimented with sets of emoji labels of different sizes, our proposed label-wise attention architecture proved especially well-suited for emojis which were infrequent in the training data, making the system less biased towards the most frequent. We see this as a first step to improve the robustness of recurrent neural networks in datasets with unbalanced distributions, as they were shown not to perform better than well-tuned SVMs on the emoji predicion task (C ¸ o¨ ltekin and Rama, 2018). As for future work, we plan to apply our labelwise attention mechanism to"
D18-1508,D15-1166,0,0.0356094,"are capturing is currently very limited. What is a model prioritizing when associating a message with, for example, positive ( ), negative ( ) or patriotic ( ) intents? A natural way of assessing this would be to implement an attention mechanism over the hidden states of LSTM layers. Attentive architectures in NLP, in fact, have recently received substantial interest, mostly for sequenceto-sequence models (which are useful for machine translation, summarization or language modeling), and a myriad of modifications have been proposed, including additive (Bahdanau et al., 2015), multiplicative (Luong et al., 2015) or self (Lin et al., 2017) attention mechanisms. However, standard attention mechanisms only tell us which text fragments are considered impor4766 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4766–4771 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Standard attention BiLSTM Labelwise attention BiLSTM LookUp LookUp BiLSTMs BiLSTMs h1, ..., hN h1, ..., hN Attention (α) s Linear |s|→ L β1, ..., βL Attention (α1) s1 Linear |s |→ 1 β1 Attention (α2) s2 Linear |s |→ 1 β2 Attention (αi) si Linea"
D18-1508,N16-1174,0,0.0658504,"ase of infrequent emojis. This suggests that an attentive mechanism can be leveraged to make neural architectures more sensitive to instances of underrepresented classes. 2 Methodology Our base architecture is the Deepmoji model (Felbo et al., 2017), which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention mechanisms allow the model to focus on specific words of the input (Yang et al., 2016), instead of having to memorize all the important features in a fixed-length vector. The main architectural difference with respect to the typical attention is illustrated in Figure 1. In Felbo et al. (2017), attention is computed as follows: zi = wa hi + ba e zi αi = PN zj j=1 e s= N X αj hj j=1 Here hi ∈ Rd is the hidden representation of the LSTM corresponding to the ith word, with N the total number of words in the sentence. The weight vector wa ∈ Rd and bias term ba ∈ R map this hidden representation to a value that reflects the importance of this state for the considered classification p"
E17-1010,E09-1005,0,0.0315951,"account definitions from related words (Banerjee and Pedersen, 2003), or by calculating the distributional similarity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2"
E17-1010,S10-1013,0,0.05303,"Missing"
E17-1010,S01-1001,0,0.929515,"Missing"
E17-1010,L16-1239,0,0.0353117,"Missing"
E17-1010,eisele-chen-2010-multiun,0,0.0122763,"604 802,443 30,441,386 #Annotations 2,282 1,850 455 1,644 1,022 226,036 911,134 #Sense types 1,335 1,167 375 827 659 33,362 3,730 #Word types 1,093 977 330 751 512 22,436 1,149 Ambiguity 5.4 6.8 8.5 4.9 5.5 6.8 8.9 Table 1: Statistics of the WSD datasets used in the evaluation framework (after standardization). • OMSTI (Taghipour and Ng, 2015a). OMSTI (One Million Sense-Tagged Instances) is a large corpus annotated with senses from the WordNet 3.0 inventory. It was automatically constructed by using an alignmentbased WSD approach (Chan and Ng, 2005) on a large English-Chinese parallel corpus (Eisele and Chen, 2010, MultiUN corpus). OMSTI5 has already shown its potential as a training corpus by improving the performance of supervised systems which add it to existing training data (Taghipour and Ng, 2015a; Iacobacci et al., 2016). • SemEval-07 task 17 (Pradhan et al., 2007). This is the smallest among the five datasets, containing 455 sense annotations for nouns and verbs only. It was originally annotated using WordNet 2.1 sense inventory. • SemEval-13 task 12 (Navigli et al., 2013). This dataset includes thirteen documents from various domains. In this case the original sense inventory was WordNet 3.0,"
E17-1010,C14-1151,0,0.660668,"these approaches rely on the structure or content of manually-curated knowledge resources for disambiguation. One of the first approaches of this kind was Lesk (1986), which in its original version consisted of calculating the overlap between the context of the target word and its definitions as given by the sense inventory. Based on the same principle, various works have adapted the original algorithm by also taking into account definitions from related words (Banerjee and Pedersen, 2003), or by calculating the distributional similarity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based"
E17-1010,P10-1156,0,0.0221383,"m related words (Banerjee and Pedersen, 2003), or by calculating the distributional similarity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its colloca"
E17-1010,P16-1085,1,0.413258,"ng systems. Even 99 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 99–110, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics (Taghipour and Ng, 2015a; Raganato et al., 2016; Camacho-Collados et al., 2016a). In this work we compare supervised systems and study the role of their underlying senseannotated training corpus. Since semi-supervised models have been shown to outperform fully supervised systems in some settings (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Iacobacci et al., 2016; Yuan et al., 2016), we evaluate and compare models using both manually-curated and automatically-constructed sense-annotated corpora for training. into a unified format, (2) semi-automatically converting annotations from any dataset to WordNet 3.0, and (3) preprocessing the datasets by consistently using the same pipeline. Second, we use this evaluation framework to perform a fair quantitative and qualitative empirical comparison of the main techniques proposed in the WSD literature, including the latest advances based on neural networks. 2 State of the Art The task of Word Sense Disambiguat"
E17-1010,P15-1072,1,0.718796,"Missing"
E17-1010,W16-5307,0,0.340237,"Missing"
E17-1010,L16-1269,1,0.877179,"Missing"
E17-1010,W02-1006,0,0.0338101,"urces (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trained on unlabeled corpora have also been explored (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). These features are generally taken as input to train a linear classifier (Zhong and Ng, 2010; Shen et al., 2013). In addition to these conventional approaches, the latest developments in neural language models have motivated some researchers to include them in their WSD architectures (K˚ageb¨ack and Salomonsson, 2016; Melamud et al., 2016; Yuan et al., 2016). Supervised m"
E17-1010,D14-1110,0,0.140902,"Missing"
E17-1010,P14-5010,0,0.00371456,"SD datasets In this section we explain our pipeline for transforming any given evaluation dataset or senseannotated corpus into a preprocessed unified for100 Figure 1: Pipeline for standardizing any given WSD dataset. mat. In our pipeline we do not make any distinction between evaluation datasets and senseannotated training corpora, as the pipeline can be applied equally to both types. For simplicity we will refer to both evaluation datasets and training corpora as WSD datasets. Figure 1 summarizes our pipeline to standardize a WSD dataset. The process consists of four steps: CoreNLP toolkit (Manning et al., 2014) for Part-of-Speech (PoS) tagging3 and lemmatization. This step is performed in order to ensure that all systems use the same preprocessed data. 4. Finally, we developed a script to check that the final dataset conforms to the aforementioned guidelines. In this final verification we also ensured that the sense annotations match the lemma and the PoS tag provided by Stanford CoreNLP by automatically fixing all divergences. 1. Most WSD datasets in the literature use a similar XML format, but they have some divergences on how to encode the information. For instance, the SemEval-15 dataset (Moro a"
E17-1010,S15-1007,0,0.0847564,"Missing"
E17-1010,K16-1006,0,0.13943,"rroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trained on unlabeled corpora have also been explored (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). These features are generally taken as input to train a linear classifier (Zhong and Ng, 2010; Shen et al., 2013). In addition to these conventional approaches, the latest developments in neural language models have motivated some researchers to include them in their WSD architectures (K˚ageb¨ack and Salomonsson, 2016; Melamud et al., 2016; Yuan et al., 2016). Supervised models have traditionally been able to outperform knowledge-based systems (Navigli, 2009). However, obtaining sense-annotated corpora is highly expensive, and in many cases such corpora are not available for specific domains. This is the reason why some of these supervised methods have started to rely on unlabeled corpora as well. These approaches, which are often classified as semi-supervised, are targeted at overcoming the knowledge acquisition bottleneck of conventional supervised models (Pilehvar and Navigli, 2014). In fact, there is a line of research spec"
E17-1010,H94-1046,0,0.147967,"erable increase over the number of sense annotations of SemCor. However, SemCor is much more balanced in terms of unique senses covered (3,730 covered by OMSTI in contrast to over 33K covered by SemCor). Additionally, while OMSTI was constructed automatically, SemCor was manually built and, hence, its quality is expected to be higher. Finally, we calculated the ambiguity level of each dataset, computed as the total number of canSense-annotated training corpora We now describe the two WordNet senseannotated corpora used for training the supervised systems in our evaluation framework: • SemCor (Miller et al., 1994). SemCor4 is a manually sense-annotated corpus divided into 352 documents for a total of 226,040 sense annotations. It was originally tagged with senses from the WordNet 1.4 sense inventory. SemCor is, to our knowledge, the largest corpus manually annotated with WordNet senses, and is the main corpus used in the literature to train supervised WSD systems (Agirre et al., 2010b; Zhong and Ng, 2010). 5 In this paper we refer to the portion of sense-annotated data from the MultiUN corpus as OMSTI. Note that OMSTI was released along with SemCor. 6 Statistics included in Table 1: number of documents"
E17-1010,L16-1268,0,0.0631683,"Missing"
E17-1010,C12-1109,0,0.00980971,"oaches of this kind was Lesk (1986), which in its original version consisted of calculating the overlap between the context of the target word and its definitions as given by the sense inventory. Based on the same principle, various works have adapted the original algorithm by also taking into account definitions from related words (Banerjee and Pedersen, 2003), or by calculating the distributional similarity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given"
E17-1010,S15-2049,1,0.750288,"2014) for Part-of-Speech (PoS) tagging3 and lemmatization. This step is performed in order to ensure that all systems use the same preprocessed data. 4. Finally, we developed a script to check that the final dataset conforms to the aforementioned guidelines. In this final verification we also ensured that the sense annotations match the lemma and the PoS tag provided by Stanford CoreNLP by automatically fixing all divergences. 1. Most WSD datasets in the literature use a similar XML format, but they have some divergences on how to encode the information. For instance, the SemEval-15 dataset (Moro and Navigli, 2015) was developed for both WSD and Entity Linking and its format was especially designed for this latter task. Therefore, we decided to convert all datasets to a unified format. As unified format we use the XML scheme used for the SemEval-13 allwords WSD task (Navigli et al., 2013), where preprocessing information of a given corpus is also encoded. 4 Data In this section we summarize the WSD datasets used in the evaluation framework. To all these datasets we apply the standardization pipeline described in Section 3. First, we enumerate all the datasets used for the evaluation (Section 4.1). Secon"
E17-1010,P15-1173,0,0.0429977,"Missing"
E17-1010,Q14-1019,1,0.943174,"distributional similarity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trai"
E17-1010,S13-1003,0,0.129919,"esentation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trained on unlabeled corpora have also been explored (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). These features are generally taken as input to train a linear classifier (Zhong and Ng, 2010; Shen et al., 2013). In addition to these conventional approaches, the latest developments in neural language models have motivated some researchers to include them in their WSD architectures (K˚ageb¨ack and Salomonsson, 2016; Melamud et al., 2016; Yuan et al., 2016). Supervised models have traditionally been able to outperform knowledge-based systems (Navigli, 2009). However, obtaining sense-annotated corpora is highly expensive, and in many cases such corpora are not available for specific domains. This is the reason why some of these supervised methods have started to rely on unlabeled corpora as well. These"
E17-1010,W04-0811,0,0.878524,"Missing"
E17-1010,S07-1006,1,0.38032,"Missing"
E17-1010,K15-1037,0,0.625004,"wnstream NLP applications (de Lacalle and Agirre, 2015). In general the field does not have a clear path, partially owing to the fact that identifying real improvements over existing approaches becomes a hard task with current evaluation benchmarks. This is mainly due to the lack of a unified framework, which prevents direct and fair comparison among systems. Even 99 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 99–110, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics (Taghipour and Ng, 2015a; Raganato et al., 2016; Camacho-Collados et al., 2016a). In this work we compare supervised systems and study the role of their underlying senseannotated training corpus. Since semi-supervised models have been shown to outperform fully supervised systems in some settings (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Iacobacci et al., 2016; Yuan et al., 2016), we evaluate and compare models using both manually-curated and automatically-constructed sense-annotated corpora for training. into a unified format, (2) semi-automatically converting annotations from any dataset to WordNet 3.0,"
E17-1010,N15-1035,0,0.677673,"wnstream NLP applications (de Lacalle and Agirre, 2015). In general the field does not have a clear path, partially owing to the fact that identifying real improvements over existing approaches becomes a hard task with current evaluation benchmarks. This is mainly due to the lack of a unified framework, which prevents direct and fair comparison among systems. Even 99 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 99–110, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics (Taghipour and Ng, 2015a; Raganato et al., 2016; Camacho-Collados et al., 2016a). In this work we compare supervised systems and study the role of their underlying senseannotated training corpus. Since semi-supervised models have been shown to outperform fully supervised systems in some settings (Taghipour and Ng, 2015b; Bas¸kaya and Jurgens, 2016; Iacobacci et al., 2016; Yuan et al., 2016), we evaluate and compare models using both manually-curated and automatically-constructed sense-annotated corpora for training. into a unified format, (2) semi-automatically converting annotations from any dataset to WordNet 3.0,"
E17-1010,S13-2040,1,0.749905,"ation we also ensured that the sense annotations match the lemma and the PoS tag provided by Stanford CoreNLP by automatically fixing all divergences. 1. Most WSD datasets in the literature use a similar XML format, but they have some divergences on how to encode the information. For instance, the SemEval-15 dataset (Moro and Navigli, 2015) was developed for both WSD and Entity Linking and its format was especially designed for this latter task. Therefore, we decided to convert all datasets to a unified format. As unified format we use the XML scheme used for the SemEval-13 allwords WSD task (Navigli et al., 2013), where preprocessing information of a given corpus is also encoded. 4 Data In this section we summarize the WSD datasets used in the evaluation framework. To all these datasets we apply the standardization pipeline described in Section 3. First, we enumerate all the datasets used for the evaluation (Section 4.1). Second, we describe the sense-annotated corpora used for training (Section 4.2). Finally, we show some relevant statistics extracted from these resources (Section 4.3). 2. Once the dataset is converted to a unified format, we map the sense annotations from its original WordNet versio"
E17-1010,petrov-etal-2012-universal,0,0.0190664,"Missing"
E17-1010,P15-1058,0,0.0758502,"larity between definitions and the context of the target word (Basile et al., 2014; Chen et al., 2014). Distributional similarity has also been exploited in different settings in various works (Miller et al., 2012; CamachoCollados et al., 2015; Camacho-Collados et al., 2016b). In addition to these approaches based on distributional similarity, an important branch of knowledge-based systems found their techniques on the structural properties of semantic graphs from lexical resources (Agirre and Soroa, 2009; Guo and Diab, 2010; Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014; Weissenborn et al., 2015; Tripodi and Pelillo, 2016). Generally, these graph-based WSD systems first create a graph representation of the input text and then exploit different graph-based algorithms over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trained on unlabeled corpora h"
E17-1010,J14-4005,1,0.844858,"architectures (K˚ageb¨ack and Salomonsson, 2016; Melamud et al., 2016; Yuan et al., 2016). Supervised models have traditionally been able to outperform knowledge-based systems (Navigli, 2009). However, obtaining sense-annotated corpora is highly expensive, and in many cases such corpora are not available for specific domains. This is the reason why some of these supervised methods have started to rely on unlabeled corpora as well. These approaches, which are often classified as semi-supervised, are targeted at overcoming the knowledge acquisition bottleneck of conventional supervised models (Pilehvar and Navigli, 2014). In fact, there is a line of research specifically aimed at automatically obtaining large amounts of high-quality sense-annotated corpora 3 Standardization of WSD datasets In this section we explain our pipeline for transforming any given evaluation dataset or senseannotated corpus into a preprocessed unified for100 Figure 1: Pipeline for standardizing any given WSD dataset. mat. In our pipeline we do not make any distinction between evaluation datasets and senseannotated training corpora, as the pipeline can be applied equally to both types. For simplicity we will refer to both evaluation da"
E17-1010,P10-4014,0,0.240495,"over the given representation (e.g., PageRank) to perform WSD. Supervised WSD Supervised models train different features extracted from manually sense-annotated corpora. These features have been mostly based on the information provided by the surroundings words of the target word (Keok and Ng, 2002; Navigli, 2009) and its collocations. Recently, more complex features based on word embeddings trained on unlabeled corpora have also been explored (Taghipour and Ng, 2015b; Rothe and Sch¨utze, 2015; Iacobacci et al., 2016). These features are generally taken as input to train a linear classifier (Zhong and Ng, 2010; Shen et al., 2013). In addition to these conventional approaches, the latest developments in neural language models have motivated some researchers to include them in their WSD architectures (K˚ageb¨ack and Salomonsson, 2016; Melamud et al., 2016; Yuan et al., 2016). Supervised models have traditionally been able to outperform knowledge-based systems (Navigli, 2009). However, obtaining sense-annotated corpora is highly expensive, and in many cases such corpora are not available for specific domains. This is the reason why some of these supervised methods have started to rely on unlabeled cor"
E17-1010,S07-1016,0,\N,Missing
E17-2036,D16-1057,0,0.0270607,"Missing"
E17-2036,P15-1010,1,0.829944,"utional) or not filtering training data by domains. hypernymy information is not encoded equally in different regions of distributional vector spaces, as it is stored differently depending on the domain. The hypernym discovery task consists of, given a term as input, finding its most appropriate hypernym. In this evaluation we followed the approach of Espinosa-Anke et al. (2016, TaxoEmbed), who provides a framework to train a domainwise transformation matrix (Mikolov et al., 2013) between the vector spaces of terms and hypernyms. As in the original work, we used the senselevel vector space of Iacobacci et al. (2015) and training data from Wikidata.18 We used the domain annotations of BabelDomains for clustering the training data by domain, and compared it with the domains obtained through the distributional step, as used in Espinosa-Anke et al. (2016). We additionally included a baseline which did not filter the training data by domain. The training data19 was composed of 20K term-hypernym pairs for the domain-filtered systems and 200K for the baseline, while the test data was composed of 250 randomly-extracted terms with their corresponding hypernyms in Wikidata. Table 4 shows the results of TaxoEmbed i"
E17-2036,W04-2214,0,0.648735,"a featured articles page. The BabelNet dataset is composed of 200 synsets randomly extracted from BabelNet 3.0 which were manually annotated with domains. As comparison systems we included a baseline based on Wikipedia (Wikipedia-idf). This baseline first constructs a tf-idf -weighted bag-ofword vector representation of Wikipedia pages and, similarly to our distributional approach, calculates its similarity with the concatenation of all Wikipedia pages associated with a domain in the Wikipedia featured articles page.15 We additionally compared with WN-Domains-3.2 (Magnini and Cavagli`a, 2000; Bentivogli et al., 2004), which is the latest released version of WordNet Domains16 . However, this approach involves manual curation, both in the selection of seeds and correction of errors. In order to enable a fair comparison, we report the results of a system based on its main automatic component. This baseline takes annotated synsets as input and propagates them through the WordNet taxonomy (WN-Taxonomy Prop.). Likewise, we report the results of the same baseline by propagating through the BabelNet taxonomy (BN-Taxonomy Prop.). These two systems were evaluated by 10-fold cross validation on the 4.2 Extrinsic Eva"
E17-2036,P07-1034,0,0.125907,"ean Union inter-institutional terminology database. The domain labels of IATE are based on the Eurovoc thesaurus2 and were introduced manually. The fact that each of these approaches involves manual curation/intervention limits their extension to other resources, and therefore to downstream applications. Introduction Since the early days of Natural Language Processing (NLP) and Machine Learning, generalizing a given algorithm or technique has been extremely challenging. One of the main factors that has led to this issue in NLP has been the wide variety of domains for which data are available (Jiang and Zhai, 2007). Algorithms trained on the business domain are not to be expected to work well in biology, for example. Moreover, even if we manage to obtain a balanced training set across domains, our algorithm may not be as effective on some specific domain as if it had been trained on that same target domain. This issue has become even more challenging and significant with the rise of supervised learning techniques. These techniques are fed with large amounts of data and ought to be able generalize to various target domains. Several studies have proposed regularization frameworks for domain adaptation in"
E17-2036,D16-1095,0,0.112482,"Missing"
E17-2036,N15-1059,1,0.846306,"euristic is based on the BabelNet hypernymy structure, which is an integration of various taxonomies: WikiData, WordNet and MultiWiBi (Flati et al., 2016). The main intuition is that, in general, synsets connected by a hypernymy relation tend to share the same domain 4 https://en.wikipedia.org/wiki/ Wikipedia:Featured_articles 5 Biography domains are not considered. 6 For simplicity we refer to each domain with its first word (e.g., Geography to refer to Geography and Places). 7 http://lcl.uniroma1.it/nasari/ 8 Weighted Overlap has been proved to suit interpretable vectors better than cosine (Camacho-Collados et al., 2015). 9 This value was set through observation to increase precision but without drastically decreasing recall. 224 (Magnini and Cavagli`a, 2000).10 This taxonomybased heuristic is intended to both increase coverage and refine the quality of synsets annotated by the distributional approach. First, if all the hypernyms (at least two) of a given synset share the same top domain, this synset is annotated (or reannotated) with that domain. Second, if the top domain of an annotated synset is different from at least two of its hypernyms, this domain tag is removed. Distributional Taxonomy Labels Propaga"
E17-2036,magnini-cavaglia-2000-integrating,0,0.631538,"Missing"
E17-2036,P07-1033,0,0.105479,"Missing"
E17-2036,D16-1041,1,0.863988,"ropagating through the BabelNet taxonomy (BN-Taxonomy Prop.). These two systems were evaluated by 10-fold cross validation on the 4.2 Extrinsic Evaluation One of the main applications of including domain information in sense inventories is to be able to cluster textual data by domain. Supervised systems may be particularly sensitive to this issue (Daum´e III, 2007), and therefore training data should be clustered accordingly. In particular, two recent studies found that clustering training data was essential for distributional hypernym discovery systems to perform accurately (Fu et al., 2014; Espinosa-Anke et al., 2016). They discovered that 15 For the annotation of WordNet we used the direct Wikipedia-WordNet mapping from BabelNet. 16 http://wndomains.fbk.eu/ 17 Defined as right granted by law or contract (especially a right to benefits). 226 BabelDomains Distributional Non-filtered Art 0.30 0.18 0.00 Bio 0.87 0.41 0.68 Edu 0.39 0.30 0.00 Geo 0.43 0.26 0.10 Hea 0.12 0.10 0.05 Med 0.71 0.46 0.25 Mus 0.42 0.43 0.11 Phy 0.20 0.08 0.00 Tra 0.63 0.56 0.34 War 0.13 0.11 0.00 Table 4: MRR (Mean Reciprocal Rank) performance of TaxoEmbed in the hypernym discovery task by filtering (BabelDomains and Distributional) o"
E17-2036,D12-1129,1,0.896578,"Missing"
E17-2036,P13-1132,1,0.8725,"each domain is first associated with a given vector. Then, the Wikipedia pages from the featured articles page are leveraged as follows. First, all Wikipedia pages associated with a given domain are concatenated into a single text. Second, a lexical vector is constructed for each text as in Camacho-Collados et al. (2016), by applying lexical specificity over the bag-of-word representation of the text. Finally, given a BabelNet synset s, the similarity between its respective NASARI lexical vector and the lexical vector of each domain is calculated using the Weighted Overlap comparison measure (Pilehvar et al., 2013).8 This enables us to obtain, for each BabelNet synset, scores for each domain label denoting their importance. For notational brevity, we will refer to the domain whose similarity score is highest across all domains as its top domain. For instance, the top domain of the BabelNet synset corresponding to rifle is Warfare, while its second domain is Engineering. In order to increase precision, initially we only tag those BabelNet synsets whose maximum score is higher than 0.35.9 Our goal is to enrich lexical resources with domain information. To this end, we rely on BabelNet 3.0, which merges bo"
E17-2036,P14-1113,0,0.217305,"ame baseline by propagating through the BabelNet taxonomy (BN-Taxonomy Prop.). These two systems were evaluated by 10-fold cross validation on the 4.2 Extrinsic Evaluation One of the main applications of including domain information in sense inventories is to be able to cluster textual data by domain. Supervised systems may be particularly sensitive to this issue (Daum´e III, 2007), and therefore training data should be clustered accordingly. In particular, two recent studies found that clustering training data was essential for distributional hypernym discovery systems to perform accurately (Fu et al., 2014; Espinosa-Anke et al., 2016). They discovered that 15 For the annotation of WordNet we used the direct Wikipedia-WordNet mapping from BabelNet. 16 http://wndomains.fbk.eu/ 17 Defined as right granted by law or contract (especially a right to benefits). 226 BabelDomains Distributional Non-filtered Art 0.30 0.18 0.00 Bio 0.87 0.41 0.68 Edu 0.39 0.30 0.00 Geo 0.43 0.26 0.10 Hea 0.12 0.10 0.05 Med 0.71 0.46 0.25 Mus 0.42 0.43 0.11 Phy 0.20 0.08 0.00 Tra 0.63 0.56 0.34 War 0.13 0.11 0.00 Table 4: MRR (Mean Reciprocal Rank) performance of TaxoEmbed in the hypernym discovery task by filtering (Babel"
F14-1032,W03-1802,0,0.114525,"Missing"
F14-1032,W02-1006,0,0.0659778,"Missing"
K17-1012,N16-1163,0,0.0669838,"edge-Enhanced Training Massimiliano Mancini*, Jose Camacho-Collados*, Ignacio Iacobacci and Roberto Navigli Department of Computer Science Sapienza University of Rome mancini@dis.uniroma1.it {collados,iacobacci,navigli}@di.uniroma1.it Abstract Previous works have addressed this limitation by automatically inducing word senses from monolingual corpora (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Di Marco and Navigli, 2013; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Vu and Parker, 2016; Qiu et al., 2016), or bilingual parallel data (Guo et al., 2014; Ettinger et al., 2016; ˇ Suster et al., 2016). However, these approaches learn solely on the basis of statistics extracted from text corpora and do not exploit knowledge from semantic networks. Additionally, their induced senses are neither readily interpretable (Panchenko et al., 2017) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conf"
K17-1012,N09-1003,0,0.141055,"Missing"
K17-1012,K16-1026,0,0.0187522,"d Sch¨utze (2015) aimed at building a shared space of word and sense embeddings based on two steps: a first training step of only word embeddings and a second training step to produce sense and synset embeddings. These two approaches require multiple steps of training and make use of a relatively small resource like WordNet, which limits their coverage and applicability. Camacho-Collados et al. (2016) increased the coverage of these WordNetbased approaches by exploiting the complementary knowledge of WordNet and Wikipedia along with pre-trained word embeddings. Finally, Wang et al. (2014) and Fang et al. (2016) proposed a model to align vector spaces of words and entities from knowledge bases. However, these approaches are restricted to nominal instances only (i.e. Wikipedia pages or entities). In contrast, we propose a model which learns both words and sense embeddings from a single joint training phase, producing a common vector 3 Connecting words and senses in context In order to jointly produce embeddings for words and senses, SW2V needs as input a corpus where words are connected to senses1 in each given context. One option for obtaining such connections could be to take a sense-annotated corpu"
K17-1012,N15-1184,0,0.0592797,"Missing"
K17-1012,P16-1143,0,0.0260088,"Missing"
K17-1012,P16-1191,0,0.176883,"scores depending on the specific nature of the pair16 . Recent works have managed to obtain significant improvements by tweaking usual word embedding approaches into providing low similarity scores for antonym pairs (Pham et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016; Mrksic et al., 2017), but this is outside the scope of this paper. 6.2 Sense Clustering Current lexical resources tend to suffer from the high granularity of their sense inventories (Palmer et al., 2007). In fact, a meaningful clustering of their senses may lead to improvements on downstream tasks (Hovy et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In this section we evaluate our synset representations on the Wikipedia sense clustering task. For a fair comparison with respect to the BabelNet-based com15 Two annotators decided the degree of antonymy between word pairs: clear antonyms, weak antonyms or neither. 16 For instance, the pairs sunset-sunrise and day-night are given, respectively, 1.88 and 2.47 gold scores in the 0-10 scale, while our model gives them a higher similarity score. In fact, both pairs appear as coordinate synsets in WordNet. 14 https://github.com/mfaruqui/ retrofitting 106 SW2V SensEmbed NAS"
K17-1012,D14-1067,0,0.0108602,"tively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to stateof-the-art word- and sense-based models. 1 Introduction Recently, approaches based on neural networks which embed words into low-dimensional vector spaces from text corpora (i.e. word embeddings) have become increasingly popular (Mikolov et al., 2013; Pennington et al., 2014). Word embeddings have proved to be beneficial in many Natural Language Processing tasks, such as Machine Translation (Zou et al., 2013), syntactic parsing (Weiss et al., 2015), and Question Answering (Bordes et al., 2014), to name a few. Despite their success in capturing semantic properties of words, these representations are generally hampered by an important limitation: the inability to discriminate among different meanings of the same word. Authors marked with an asterisk (*) contributed equally. 100 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 100–111, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics 2 Related work space of words and senses as an emerging feature. Embedding words from large corpora into a lo"
K17-1012,J06-1003,0,0.0428959,"tity linking system based on BabelNet. We compare to both the default Babelfy system which • Benchmark. Word similarity has been one of the most popular benchmarks for in-vitro evaluation of vector space models (Pennington et al., 2014; Levy et al., 2015). For the analysis we use two word similarity datasets: the similarity portion (Agirre et al., 2009, WS-Sim) of the WordSim-353 dataset (Finkelstein et al., 2002) and RG-65 (Rubenstein and Goodenough, 1965). In order to compute the similarity of two words using our sense embeddings, we apply the standard closest senses strategy (Resnik, 1995; Budanitsky and Hirst, 2006; Camacho-Collados 7 In this analysis we used the word similarity task for optimizing the sense embeddings, without caring about the performance of word embeddings or their interconnectivity. Therefore, this configuration may not be optimal for word embeddings and may be further tuned on specific applications. More information about different configurations in the documentation of the source code. 8 http://babelfy.org 5 http://ebiquity.umbc. edu/blogger/2013/05/01/ umbc-webbase-corpus-of-3b-english-words/ 6 http://babelnet.org 104 Input Words Senses Both Words WS-Sim RG-65 r ρ r ρ 0.49 0.48 0."
K17-1012,C14-1048,0,0.0262793,"er via Joint Knowledge-Enhanced Training Massimiliano Mancini*, Jose Camacho-Collados*, Ignacio Iacobacci and Roberto Navigli Department of Computer Science Sapienza University of Rome mancini@dis.uniroma1.it {collados,iacobacci,navigli}@di.uniroma1.it Abstract Previous works have addressed this limitation by automatically inducing word senses from monolingual corpora (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Di Marco and Navigli, 2013; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Vu and Parker, 2016; Qiu et al., 2016), or bilingual parallel data (Guo et al., 2014; Ettinger et al., 2016; ˇ Suster et al., 2016). However, these approaches learn solely on the basis of statistics extracted from text corpora and do not exploit knowledge from semantic networks. Additionally, their induced senses are neither readily interpretable (Panchenko et al., 2017) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without s"
K17-1012,P15-1072,1,0.709399,"Missing"
K17-1012,S13-1005,0,0.0204774,"ith this input setting. For instance, in the case of including both words and senses in the input layer, the co-occurrence information learned by the network would be duplicated for both words and senses. • Training model and hyperparameters. For evaluation purposes, we use the CBOW model of word2vec with standard hyperparameters: the dimensionality of the vectors is set to 300 and the window size to 8, and hierarchical softmax is used for normalization. These hyperparameter values are set across all experiments. • Corpus and semantic network. We use a 300M-words corpus from the UMBC project (Han et al., 2013), which contains English paragraphs extracted from the web.5 As semantic network we use BabelNet 3.06 , a large multilingual semantic network with over 350 million semantic connections, integrating resources such as Wikipedia and WordNet. We chose BabelNet owing to its wide coverage of named entities and lexicographic knowledge. 5.2 Disambiguation / Shallow word-sense connectivity algorithm In this section we evaluate the impact of our shallow word-sense connectivity algorithm (Section 3) by testing our model directly taking a predisambiguated text as input. In this case the network exploits t"
K17-1012,J15-4004,0,0.0127603,"vity algorithm achieves the best overall results. We believe that these results are due to the semantic connectivity ensured by our algorithm and to the possibility of associating words with more than one sense, which seems beneficial for training, making it more robust to possible disambiguation errors and to the sense granularity issue (Erk et al., 2013). The results are especially significant considering that our algorithm took a tenth of the time needed by Babelfy to process the corpus. 6 6.1 Word Similarity In this section we evaluate our sense representations on the standard SimLex-999 (Hill et al., 2015) and MEN (Bruni et al., 2014) word similarity datasets13 . SimLex and MEN contain 999 and 3000 word pairs, respectively, which constitute, to our knowledge, the two largest similar9 http://pan.baidu.com/s/1eQcPK8i We used the AutoExtend code (http://cistern. cis.lmu.de/˜sascha/AutoExtend/) to obtain sense vectors using W2V embeddings trained on UMBC (GoogleNews corpus used in their pre-trained models is not publicly available). We also tried the code to include BabelNet as lexical resource, but it was not easily scalable (BabelNet is two orders of magnitude larger than WordNet). 11 http://lcl."
K17-1012,D14-1110,0,0.350098,"Missing"
K17-1012,R13-1022,0,0.0519165,"nnotations (Bennett et al., 2016). Given an input word w, we compute the cosine similarity between w and all its candidate senses, picking the sense leading to the highest similarity: Table 4: Accuracy and F-Measure percentages of different systems on the SemEval Wikipedia sense clustering dataset. parison systems that use the Wikipedia corpus for training, in this experiment we report the results of our model trained on the Wikipedia corpus and using BabelNet as lexical resource only. For the evaluation we consider the two Wikipedia sense clustering datasets (500-pair and SemEval) created by Dandala et al. (2013). In these datasets sense clustering is viewed as a binary classification task in which, given a pair of Wikipedia pages, the system has to decide whether to cluster them into a single instance or not. To this end, we use our synset embeddings and cluster Wikipedia pages17 together if their similarity exceeds a threshold γ. In order to set the optimal value of γ, we follow Dandala et al. (2013) and use the first 500-pairs sense clustering dataset for tuning. We set the threshold γ to 0.35, which is the value leading to the highest F-Measure among all values from 0 to 1 with a 0.05 step size on"
K17-1012,J13-3008,1,0.913407,"Missing"
K17-1012,P12-1092,0,0.14184,"Missing"
K17-1012,J13-3003,0,0.0355605,"ed). We will refer to this latter version as Babelfy* and report the best configuration of each strategy according to our analysis. Table 2 shows the results of our model using the three different strategies on RG-65 and WSSim. Our shallow word-sense connectivity algorithm achieves the best overall results. We believe that these results are due to the semantic connectivity ensured by our algorithm and to the possibility of associating words with more than one sense, which seems beneficial for training, making it more robust to possible disambiguation errors and to the sense granularity issue (Erk et al., 2013). The results are especially significant considering that our algorithm took a tenth of the time needed by Babelfy to process the corpus. 6 6.1 Word Similarity In this section we evaluate our sense representations on the standard SimLex-999 (Hill et al., 2015) and MEN (Bruni et al., 2014) word similarity datasets13 . SimLex and MEN contain 999 and 3000 word pairs, respectively, which constitute, to our knowledge, the two largest similar9 http://pan.baidu.com/s/1eQcPK8i We used the AutoExtend code (http://cistern. cis.lmu.de/˜sascha/AutoExtend/) to obtain sense vectors using W2V embeddings trai"
K17-1012,P15-1010,1,0.913194,"pages 100–111, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics 2 Related work space of words and senses as an emerging feature. Embedding words from large corpora into a lowdimensional vector space has been a popular task since the appearance of the probabilistic feedforward neural network language model (Bengio et al., 2003) and later developments such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, little research has focused on exploiting lexical resources to overcome the inherent ambiguity of word embeddings. Iacobacci et al. (2015) overcame this limitation by applying an off-the-shelf disambiguation system (i.e. Babelfy (Moro et al., 2014)) to a corpus and then using word2vec to learn sense embeddings over the pre-disambiguated text. However, in their approach words are replaced by their intended senses, consequently producing as output sense representations only. The representation of words and senses in the same vector space proves essential for applying these knowledgebased sense embeddings in downstream applications, particularly for their integration into neural architectures (Pilehvar et al., 2017). In the literat"
K17-1012,S13-2040,1,0.374752,"Missing"
K17-1012,P16-1085,1,0.0662351,"ally annotating large amounts of data is extremely expensive and therefore impractical in normal settings. Obtaining sense-annotated data from current off-the-shelf disambiguation and entity linking systems is possible, but generally suffers from two major problems. First, supervised systems are hampered by the very same problem of needing large amounts of sense-annotated data. Second, the relatively slow speed of current disambiguation systems, such as graph-based approaches (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014), or word-expert supervised systems (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016), could become an obstacle when applied to large corpora. This is the reason why we propose a simple yet effective unsupervised shallow word-sense connectivity algorithm, which can be applied to virtually any given semantic network and is linear on the corpus size. The main idea of the algorithm is to exploit the connections of a semantic network by associating words with the senses that are most connected within the sentence, according to the underlying network. Shallow word-sense connectivity algorithm. Formally, a corpus and a semantic network are taken as input and a"
K17-1012,N15-1070,0,0.0173292,"7) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conflation issue. In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016; Camacho-Collados et al., 2016). Instead, we propose SW2V (Senses and Words to Vectors), a neural model that exploits knowledge from both text corpora and semantic networks in order to simultaneously learn embeddings for both words and senses. Moreover, our model provides three additional key features: (1) both word and sense embeddings are represented in the same vector space, (2) it is flexible, as it can be applied to different predictive models, and (3) it is scalable for very large semantic networks and text corpora. Word embeddings a"
K17-1012,D14-1113,0,0.154295,"Missing"
K17-1012,N09-2059,0,0.07871,"Missing"
K17-1012,P16-2074,0,0.0208263,"s, which are over-represented in this dataset: 38 word pairs hold a clear antonymy relation (e.g. encourage-discourage or long-short), while 41 additional pairs hold some degree of antonymy (e.g. new-ancient or man-woman).15 In contrast to the consistently low gold similarity scores given to antonym pairs, our system varies its similarity scores depending on the specific nature of the pair16 . Recent works have managed to obtain significant improvements by tweaking usual word embedding approaches into providing low similarity scores for antonym pairs (Pham et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016; Mrksic et al., 2017), but this is outside the scope of this paper. 6.2 Sense Clustering Current lexical resources tend to suffer from the high granularity of their sense inventories (Palmer et al., 2007). In fact, a meaningful clustering of their senses may lead to improvements on downstream tasks (Hovy et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In this section we evaluate our synset representations on the Wikipedia sense clustering task. For a fair comparison with respect to the BabelNet-based com15 Two annotators decided the degree of antonymy between word pairs: cle"
K17-1012,N15-1164,0,0.02838,"ble (Panchenko et al., 2017) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conflation issue. In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016; Camacho-Collados et al., 2016). Instead, we propose SW2V (Senses and Words to Vectors), a neural model that exploits knowledge from both text corpora and semantic networks in order to simultaneously learn embeddings for both words and senses. Moreover, our model provides three additional key features: (1) both word and sense embeddings are represented in the same vector space, (2) it is flexible, as it can be applied to different predictive models, and (3) it is scalable for very large semantic networks and text corpo"
K17-1012,Q15-1016,0,0.0328712,"e the impact of our shallow word-sense connectivity algorithm (Section 3) by testing our model directly taking a predisambiguated text as input. In this case the network exploits the connections between each word and its disambiguated sense in context. For this comparison we used Babelfy8 (Moro et al., 2014), a state-of-the-art graph-based disambiguation and entity linking system based on BabelNet. We compare to both the default Babelfy system which • Benchmark. Word similarity has been one of the most popular benchmarks for in-vitro evaluation of vector space models (Pennington et al., 2014; Levy et al., 2015). For the analysis we use two word similarity datasets: the similarity portion (Agirre et al., 2009, WS-Sim) of the WordSim-353 dataset (Finkelstein et al., 2002) and RG-65 (Rubenstein and Goodenough, 1965). In order to compute the similarity of two words using our sense embeddings, we apply the standard closest senses strategy (Resnik, 1995; Budanitsky and Hirst, 2006; Camacho-Collados 7 In this analysis we used the word similarity task for optimizing the sense embeddings, without caring about the performance of word embeddings or their interconnectivity. Therefore, this configuration may not"
K17-1012,D15-1200,0,0.062831,"Missing"
K17-1012,K16-1006,0,0.0149123,"ounts of data is extremely expensive and therefore impractical in normal settings. Obtaining sense-annotated data from current off-the-shelf disambiguation and entity linking systems is possible, but generally suffers from two major problems. First, supervised systems are hampered by the very same problem of needing large amounts of sense-annotated data. Second, the relatively slow speed of current disambiguation systems, such as graph-based approaches (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014), or word-expert supervised systems (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016), could become an obstacle when applied to large corpora. This is the reason why we propose a simple yet effective unsupervised shallow word-sense connectivity algorithm, which can be applied to virtually any given semantic network and is linear on the corpus size. The main idea of the algorithm is to exploit the connections of a semantic network by associating words with the senses that are most connected within the sentence, according to the underlying network. Shallow word-sense connectivity algorithm. Formally, a corpus and a semantic network are taken as input and a set of connected words"
K17-1012,E17-1009,0,0.012973,"addressed this limitation by automatically inducing word senses from monolingual corpora (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Di Marco and Navigli, 2013; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Vu and Parker, 2016; Qiu et al., 2016), or bilingual parallel data (Guo et al., 2014; Ettinger et al., 2016; ˇ Suster et al., 2016). However, these approaches learn solely on the basis of statistics extracted from text corpora and do not exploit knowledge from semantic networks. Additionally, their induced senses are neither readily interpretable (Panchenko et al., 2017) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conflation issue. In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings (Chen et al., 2014; Johansson and Pina, 2015; Ja"
K17-1012,D14-1162,0,0.120226,"ense embeddings jointly. Our model exploits large corpora and knowledge from semantic networks in order to produce a unified vector space of word and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to stateof-the-art word- and sense-based models. 1 Introduction Recently, approaches based on neural networks which embed words into low-dimensional vector spaces from text corpora (i.e. word embeddings) have become increasingly popular (Mikolov et al., 2013; Pennington et al., 2014). Word embeddings have proved to be beneficial in many Natural Language Processing tasks, such as Machine Translation (Zou et al., 2013), syntactic parsing (Weiss et al., 2015), and Question Answering (Bordes et al., 2014), to name a few. Despite their success in capturing semantic properties of words, these representations are generally hampered by an important limitation: the inability to discriminate among different meanings of the same word. Authors marked with an asterisk (*) contributed equally. 100 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017"
K17-1012,P15-2004,0,0.024573,"n SimLex-999 were produced on antonym pairs, which are over-represented in this dataset: 38 word pairs hold a clear antonymy relation (e.g. encourage-discourage or long-short), while 41 additional pairs hold some degree of antonymy (e.g. new-ancient or man-woman).15 In contrast to the consistently low gold similarity scores given to antonym pairs, our system varies its similarity scores depending on the specific nature of the pair16 . Recent works have managed to obtain significant improvements by tweaking usual word embedding approaches into providing low similarity scores for antonym pairs (Pham et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016; Mrksic et al., 2017), but this is outside the scope of this paper. 6.2 Sense Clustering Current lexical resources tend to suffer from the high granularity of their sense inventories (Palmer et al., 2007). In fact, a meaningful clustering of their senses may lead to improvements on downstream tasks (Hovy et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In this section we evaluate our synset representations on the Wikipedia sense clustering task. For a fair comparison with respect to the BabelNet-based com15 Two annotators decided th"
K17-1012,P17-1170,1,0.520897,"rd embeddings. Iacobacci et al. (2015) overcame this limitation by applying an off-the-shelf disambiguation system (i.e. Babelfy (Moro et al., 2014)) to a corpus and then using word2vec to learn sense embeddings over the pre-disambiguated text. However, in their approach words are replaced by their intended senses, consequently producing as output sense representations only. The representation of words and senses in the same vector space proves essential for applying these knowledgebased sense embeddings in downstream applications, particularly for their integration into neural architectures (Pilehvar et al., 2017). In the literature, various different methods have attempted to overcome this limitation. Chen et al. (2014) proposed a model for obtaining both word and sense representations based on a first training step of conventional word embeddings, a second disambiguation step based on sense definitions, and a final training phase which uses the disambiguated text as input. Likewise, Rothe and Sch¨utze (2015) aimed at building a shared space of word and sense embeddings based on two steps: a first training step of only word embeddings and a second training step to produce sense and synset embeddings."
K17-1012,H93-1061,0,0.176924,"Missing"
K17-1012,D16-1174,0,0.18825,"ich limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conflation issue. In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016; Camacho-Collados et al., 2016). Instead, we propose SW2V (Senses and Words to Vectors), a neural model that exploits knowledge from both text corpora and semantic networks in order to simultaneously learn embeddings for both words and senses. Moreover, our model provides three additional key features: (1) both word and sense embeddings are represented in the same vector space, (2) it is flexible, as it can be applied to different predictive models, and (3) it is scalable for very large semantic networks and text corpora. Word embeddings are widely used in Natural Language Processing, mainly"
K17-1012,Q14-1019,1,0.959855,"lated work space of words and senses as an emerging feature. Embedding words from large corpora into a lowdimensional vector space has been a popular task since the appearance of the probabilistic feedforward neural network language model (Bengio et al., 2003) and later developments such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, little research has focused on exploiting lexical resources to overcome the inherent ambiguity of word embeddings. Iacobacci et al. (2015) overcame this limitation by applying an off-the-shelf disambiguation system (i.e. Babelfy (Moro et al., 2014)) to a corpus and then using word2vec to learn sense embeddings over the pre-disambiguated text. However, in their approach words are replaced by their intended senses, consequently producing as output sense representations only. The representation of words and senses in the same vector space proves essential for applying these knowledgebased sense embeddings in downstream applications, particularly for their integration into neural architectures (Pilehvar et al., 2017). In the literature, various different methods have attempted to overcome this limitation. Chen et al. (2014) proposed a model"
K17-1012,Q17-1022,0,0.00690929,"Missing"
K17-1012,D16-1018,0,0.062108,"Missing"
K17-1012,P10-4014,0,0.0871862,"input. However, manually annotating large amounts of data is extremely expensive and therefore impractical in normal settings. Obtaining sense-annotated data from current off-the-shelf disambiguation and entity linking systems is possible, but generally suffers from two major problems. First, supervised systems are hampered by the very same problem of needing large amounts of sense-annotated data. Second, the relatively slow speed of current disambiguation systems, such as graph-based approaches (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014), or word-expert supervised systems (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016), could become an obstacle when applied to large corpora. This is the reason why we propose a simple yet effective unsupervised shallow word-sense connectivity algorithm, which can be applied to virtually any given semantic network and is linear on the corpus size. The main idea of the algorithm is to exploit the connections of a semantic network by associating words with the senses that are most connected within the sentence, according to the underlying network. Shallow word-sense connectivity algorithm. Formally, a corpus and a semantic network"
K17-1012,E17-1010,1,0.0499564,"Missing"
K17-1012,D13-1141,0,0.040185,"rd and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to stateof-the-art word- and sense-based models. 1 Introduction Recently, approaches based on neural networks which embed words into low-dimensional vector spaces from text corpora (i.e. word embeddings) have become increasingly popular (Mikolov et al., 2013; Pennington et al., 2014). Word embeddings have proved to be beneficial in many Natural Language Processing tasks, such as Machine Translation (Zou et al., 2013), syntactic parsing (Weiss et al., 2015), and Question Answering (Bordes et al., 2014), to name a few. Despite their success in capturing semantic properties of words, these representations are generally hampered by an important limitation: the inability to discriminate among different meanings of the same word. Authors marked with an asterisk (*) contributed equally. 100 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 100–111, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics 2 Related work space of"
K17-1012,N10-1013,0,0.034831,"Missing"
K17-1012,P15-1173,0,0.0603118,"Missing"
K17-1012,J98-1004,0,0.301178,"Missing"
K17-1012,K15-1026,0,0.0122149,"roduced on antonym pairs, which are over-represented in this dataset: 38 word pairs hold a clear antonymy relation (e.g. encourage-discourage or long-short), while 41 additional pairs hold some degree of antonymy (e.g. new-ancient or man-woman).15 In contrast to the consistently low gold similarity scores given to antonym pairs, our system varies its similarity scores depending on the specific nature of the pair16 . Recent works have managed to obtain significant improvements by tweaking usual word embedding approaches into providing low similarity scores for antonym pairs (Pham et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016; Mrksic et al., 2017), but this is outside the scope of this paper. 6.2 Sense Clustering Current lexical resources tend to suffer from the high granularity of their sense inventories (Palmer et al., 2007). In fact, a meaningful clustering of their senses may lead to improvements on downstream tasks (Hovy et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In this section we evaluate our synset representations on the Wikipedia sense clustering task. For a fair comparison with respect to the BabelNet-based com15 Two annotators decided the degree of antonymy be"
K17-1012,S17-2008,0,0.0253672,"Missing"
K17-1012,N16-1160,0,0.0412252,"Missing"
K17-1012,C14-1016,0,0.0936118,"Missing"
K17-1012,N16-1151,0,0.042999,"Missing"
K17-1012,D14-1167,0,0.0271697,"put. Likewise, Rothe and Sch¨utze (2015) aimed at building a shared space of word and sense embeddings based on two steps: a first training step of only word embeddings and a second training step to produce sense and synset embeddings. These two approaches require multiple steps of training and make use of a relatively small resource like WordNet, which limits their coverage and applicability. Camacho-Collados et al. (2016) increased the coverage of these WordNetbased approaches by exploiting the complementary knowledge of WordNet and Wikipedia along with pre-trained word embeddings. Finally, Wang et al. (2014) and Fang et al. (2016) proposed a model to align vector spaces of words and entities from knowledge bases. However, these approaches are restricted to nominal instances only (i.e. Wikipedia pages or entities). In contrast, we propose a model which learns both words and sense embeddings from a single joint training phase, producing a common vector 3 Connecting words and senses in context In order to jointly produce embeddings for words and senses, SW2V needs as input a corpus where words are connected to senses1 in each given context. One option for obtaining such connections could be to take"
K17-1012,P15-1032,0,0.0169518,"he main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to stateof-the-art word- and sense-based models. 1 Introduction Recently, approaches based on neural networks which embed words into low-dimensional vector spaces from text corpora (i.e. word embeddings) have become increasingly popular (Mikolov et al., 2013; Pennington et al., 2014). Word embeddings have proved to be beneficial in many Natural Language Processing tasks, such as Machine Translation (Zou et al., 2013), syntactic parsing (Weiss et al., 2015), and Question Answering (Bordes et al., 2014), to name a few. Despite their success in capturing semantic properties of words, these representations are generally hampered by an important limitation: the inability to discriminate among different meanings of the same word. Authors marked with an asterisk (*) contributed equally. 100 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 100–111, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics 2 Related work space of words and senses as an emerging feature"
K17-1012,P14-2089,0,0.0151258,"14; Tian et al., 2014; Li and Jurafsky, 2015; Vu and Parker, 2016; Qiu et al., 2016), or bilingual parallel data (Guo et al., 2014; Ettinger et al., 2016; ˇ Suster et al., 2016). However, these approaches learn solely on the basis of statistics extracted from text corpora and do not exploit knowledge from semantic networks. Additionally, their induced senses are neither readily interpretable (Panchenko et al., 2017) nor easily mappable to lexical resources, which limits their application. Recent approaches have utilized semantic networks to inject knowledge into existing word representations (Yu and Dredze, 2014; Faruqui et al., 2015; Goikoetxea et al., 2015; Speer and LowryDuda, 2017; Mrksic et al., 2017), but without solving the meaning conflation issue. In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Pilehvar and Collier, 2016; Camacho-Collados et al., 2016). Instead, we propose SW2V (Senses and Words to Vectors), a neural model that exploits knowledge"
K17-1012,S07-1016,0,\N,Missing
L16-1269,E09-1005,0,0.603306,"ed corpus into their pipeline. Keywords: Word Sense Disambiguation, Entity Linking, textual definitions, definitional knowledge, multilingual corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikida"
L16-1269,P13-1052,1,0.897374,"Missing"
L16-1269,N15-1059,1,0.834875,"Linking, textual definitions, definitional knowledge, multilingual corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a"
L16-1269,P15-1072,1,0.875453,"Linking, textual definitions, definitional knowledge, multilingual corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a"
L16-1269,D14-1110,0,0.0630207,"mbiguation, Entity Linking, textual definitions, definitional knowledge, multilingual corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: i"
L16-1269,P15-2003,0,0.0581894,"Missing"
L16-1269,R13-1022,0,0.123122,"ularly suitable for enriching a semantic network. The rest of the default NASARI lexical pipeline for obtaining semantic representations (lexical specificity applied to the contextual information) remains unchanged. By integrating the high-precision disambiguated glosses into the NASARI pipeline, we obtain a new set of vector representations for BabelNet synsets, increasing its initial coverage (4.4M synsets covered by the default NASARI compared to 4.6M synsets covered by NASARI enriched with our disambiguated glosses). Experimental setup. We used the two sense clustering datasets created by Dandala et al. (2013). The task in these datasets consists of, given a pair of Wikipedia articles, to decide whether they should be merged into a single cluster or not. The first dataset (500-pair henceforth) contains 500 pairs of Wikipedia articles, while the second dataset (SemEval) consists of 925 pairs coming from a set of highly ambiguous words taken from disambiguation tasks of SemEval workshops. We follow the original setting of (Camacho-Collados et al., 2015a) and only cluster a pair of Wikipedia articles if their similarity, calculated by using the square-rooted Weighted Overlap comparison measure (Pilehv"
L16-1269,Q15-1038,1,0.850776,"om dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a Wikipedia article is generally regarded as the definition of its subject1 . In any case, an accurate semantic analysis of a definition corpus is made difficult by the short and con"
L16-1269,fernandez-ordonez-etal-2012-unsupervised,0,0.0218343,"eline. Keywords: Word Sense Disambiguation, Entity Linking, textual definitions, definitional knowledge, multilingual corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include"
L16-1269,P14-1089,1,0.854594,"on In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a Wikipedia article is generally regarded as the definition of its subject1 ."
L16-1269,Q16-1002,0,0.0219232,"Missing"
L16-1269,W04-0804,0,0.113499,"Missing"
L16-1269,H93-1061,0,0.880223,"Missing"
L16-1269,Q14-1019,1,0.891396,"uld have to deal with the added difficulty of selecting context-appropriate synsets from an extremely large sense inventory. In fact, WordNet 3.0 comprises 117 659 synsets and a definition for each synset, while BabelNet 3.0 covers 13 801 844 synsets with a total of 40 328 194 definitions. Instead, in this paper we propose an automatic disambiguation approach which leverages multilinguality and cross-resource information along with a state-of-the-art 3 http://www.hlt.utdallas.edu/˜xwn/ http://wordnet.princeton.edu/glosstag. shtml 4 multilingual Word Sense Disambiguation/Entity Linking system (Moro et al., 2014) and a vector-based semantic representation of concepts and entities (Camacho-Collados et al., 2015a). By exploiting these features, we are able to produce a large-scale high-quality corpus of glosses, automatically disambiguated with BabelNet synsets5 . 3. Methodology The gist of our approach lies in the combination of different languages and resources for high-quality disambiguation. In fact, since many definitions are short and concise, the lack of meaningful context would negatively affect the performance of a Word Sense Disambiguation/Entity Linking system targeted at individual definitio"
L16-1269,P10-1134,1,0.865078,"Missing"
L16-1269,P13-1132,1,0.851297,"Missing"
L16-1269,P98-2180,0,0.257523,"e, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a Wikipedia article is generally regarded as the definition of its subject1 . In any case, an accurate semantic analysis of a definition corpus is made diff"
L16-1269,N03-1033,0,0.0203355,"the combination of different languages and resources for high-quality disambiguation. In fact, since many definitions are short and concise, the lack of meaningful context would negatively affect the performance of a Word Sense Disambiguation/Entity Linking system targeted at individual definitions. To improve the data quality before the disambiguation step, we tokenize and Part-of-Speech (PoS) tag the definitions for a subset of languages: Tokenization. We use the tokenization system available from the polyglot project6 for 165 languages. Part-of-Speech tagging. We train the Stanford tagger (Toutanova et al., 2003), for 30 languages using the available data from the Universal Dependencies project7 (Nivre, 2015). Our disambiguation strategy is based on two steps: (1) all definitions are gathered together, grouped by definiendum and disambiguated using a multilingual disambiguation system (Section 3.1.); (2) the disambiguation output is then refined using semantic similarity (Section 3.2.). 3.1. Context-rich Disambiguation As an example, consider the following definition of castling in chess as provided by WordNet: Interchanging the positions of the king and a rook. (1) The context in (1) is limited and i"
L16-1269,J13-3007,1,0.846113,"l corpus 1. Introduction In addition to lexicography, where their use is of paramount importance, textual definitions drawn from dictionaries or encyclopedias have been widely used in various Natural Language Processing tasks and applications. Some of the areas where the use of definitional knowledge has proved to be key in achieving state-of-the-art results are Word Sense Disambiguation (Lesk, 1986; Banerjee and Pedersen, 2002; Navigli and Velardi, 2005; Agirre and Soroa, 2009; Fernandez-Ordonez et al., 2012; Chen et al., 2014; Camacho-Collados et al., 2015b), Taxonomy and Ontology Learning (Velardi et al., 2013; Flati et al., 2014; Espinosa-Anke et al., 2016), Information Extraction (Richardson et al., 1998; Delli Bovi et al., 2015), Plagiarism Detection (Franco-Salvador et al., 2016), and Question Answering (Hill et al., 2015). In fact, textual definitions (or glosses) are today widely to be found in resources of various kinds, from lexicons and dictionaries, such as WordNet (Miller et al., 1990) or Wiktionary, to encyclopedias and knowledge bases, such as Wikidata and OmegaWiki. These include Wikipedia itself: indeed, the first sentence of a Wikipedia article is generally regarded as the definitio"
L16-1269,C98-2175,0,\N,Missing
N15-1059,agirre-de-lacalle-2004-publicly,0,0.185389,"Missing"
N15-1059,N09-1003,0,0.248006,"Missing"
N15-1059,P13-4021,0,0.0165032,"Missing"
N15-1059,P14-1023,0,0.056742,"W (Pilehvar et al., 2013) are WordNet-based approaches that leverage the structural information of WordNet for the computation of semantic similarity. Most similar to our work are Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007, ESA), which represents a word in a high-dimensional space of Wikipedia articles, and Salient Semantic Analysis (Hassan and Mihalcea, 2011, SSA), which leverages the linking of concepts within Wikipedia articles for generating semantic profiles of words. Word2Vec (Mikolov et al., 2013) and PMI-SVD are the best predictive and cooccurrence models obtained by Baroni et al. (2014) on a 2.8 billion-token corpus that also includes the English Wikipedia.4 Word2Vec is based on neural network context prediction models (Mikolov et al., 2013), whereas PMI-SVD is a traditional cooccurrence based vector wherein weights are calculated by means of Pointwise Mutual Information (PMI) and the vector’s dimension is reduced to 500 by singular value decomposition (SVD). We use the DKProSimilarity (B¨ar et al., 2013) implementation of Lin and ESA in order to evaluate these measures on the WS-Sim dataset. 4.1.3 Results Table 1 shows the Pearson correlation of the different similarity mea"
N15-1059,F14-1032,1,0.865084,"Missing"
N15-1059,J06-1003,0,0.530067,"ty reduction and an effective weighting scheme, our representation attains state-of-the-art performance on multiple datasets in two standard benchmarks: word similarity and sense clustering. We are releasing our vector representations at http://lcl.uniroma1.it/nasari/. 1 Introduction Obtaining accurate semantic representations of individual word senses or concepts is vital for several applications in Natural Language Processing (NLP) such as, for example, Word Sense Disambiguation (Navigli, 2009; Navigli, 2012), Entity Linking (Bunescu and Pas¸ca, 2006; Rao et al., 2013), semantic similarity (Budanitsky and Hirst, 2006), Information Extraction (Banko et al., 2007), and resource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by"
N15-1059,E06-1002,0,0.170695,"Missing"
N15-1059,R13-1022,0,0.205557,"kolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 2007; Dandala et al., 2013; Gabrilovich and Markovitch, 2007; Strube and Ponzetto, 2006), which are, respectively, the most widely-used lexicographic and encyclopedic resources in lexical semantics (Hovy et al., 2013). This restriction to a single resource brings about two main limitations: (1) the sense modeling does not benefit from the complementary knowledge of different resources, and (2) the obtained representations are resource-specific and cannot be used across settings. In this paper we put forward a novel concept representation technique, called NASARI, which exploits the knowledge available in both types of"
N15-1059,D10-1113,0,0.017686,"LP) such as, for example, Word Sense Disambiguation (Navigli, 2009; Navigli, 2012), Entity Linking (Bunescu and Pas¸ca, 2006; Rao et al., 2013), semantic similarity (Budanitsky and Hirst, 2006), Information Extraction (Banko et al., 2007), and resource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained fr"
N15-1059,D08-1094,0,0.101817,"Missing"
N15-1059,P05-1045,0,0.00430959,"having, on average, 2.6 words. 571 3.2.2 Concept extraction If the two input words w1 and w2 are not found in the same synonym set in S, we proceed by obtaining their sets of senses Cw1 and Cw2 , respectively. Depending on the type of wi , we use two different resources for obtaining Cwi : the WordNet sense inventory and Wikipedia. WordNet words. When the word wi is defined in the WordNet sense inventory and is not a named entity (line 6 in Algorithm 1), we set Cwi as all the WordNet synsets that contain wi , i.e., Cwi = {synset s ∈ WordNet : wi ∈ s}. We use Stanford Named Entity Recognizer (Finkel et al., 2005) in our experiments. WordNet OOV and named entities. For named entities and words that do not exist in WordNet’s vocabulary (OOV) we construct the set Cwi by exploiting Wikipedia’s piped links (line 10 in Algorithm 1). To this end, we take as elements of Cwi the Wikipedia pages of the hyperlinks which have wi as their surface form, i.e., piped-links (wi ). If |Cwi |&gt; 5, we prune Cwi to its top-5 pages in terms of their number of ingoing links. Our choice of Wikipedia as a source for named entities is due to its higher coverage in comparison to WordNet. 4 Experiments We evaluated NASARI on two"
N15-1059,E14-1044,1,0.624945,"ea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Pilehvar et al., 2013), or Wikipedia (Gabrilovich and Markovitch, 2007; Mihalcea, 2007). None of these techniques, however, combine knowledge from multiple types of resource, making their representations resourcespecific and also prone to sparsity. In contrast, our method is based on the complementary knowledge of two different resources and their interlinking, leading to richer semantic representations that are also applicable across resources. Most similar to our combination of complementary knowledge is the work of Franco-Salvador et al. (2014) for crosslingual document retrieval. 575 Concept similarity. Concept similarity techniques are mainly limited to the knowledge that their underlying lexical resources provide. For instance, methods designed for measuring semantic similarity of WordNet synsets (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) usually leverage lexicographic or structural information in this lexical resource. Similarly, Wikipedia-based approaches (Hassan and Mihalcea, 2011; Strube and Ponzetto, 2006; Milne and Witten, 2008) do not usually benefit from the expert-based lexico-semant"
N15-1059,J15-4004,0,0.102299,"Missing"
N15-1059,P12-1092,0,0.261884,"ept representation. Distributional semantic models are usually the first choice for representing textual items such as words or sentences (Turney and Pantel, 2010). These models have attracted considerable research interest, resulting in various co-occurrence based representations (Salton et al., 1975; Evert, 2005; Pado and Lapata, 2007; Erk and Pad´o, 2008) or predictive models (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; Baroni et al., 2014). Although there have been approaches proposed in the literature for learning sense-specific embeddings (Weston et al., 2013; Huang et al., 2012; Neelakantan et al., 2014), their coverage is limited only to those senses that are covered in the underlying corpus. Moreover, the obtained sense representations are usually not linked to any sense inventory, and therefore such linking has to be carried out, either manually, or with the help of sense-annotated data. Hence, unless they are provided with large amounts of sense-annotated data, these techniques cannot furnish an effective representation of word senses in an existing standard sense inventory. Consequently, most sense modeling techniques have based their representation on the know"
N15-1059,N07-1025,0,0.142037,"et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 2007; Dandala et al., 2013; Gabrilovich and Markovitch, 2007; Strube and Ponzetto, 2006), which are, respectively, the most widely-used lexicographic and encyclopedic resources in lexical semantics (Hovy et al., 2013). This restriction to a single resource brings about two main limitations: (1) the sense modeling does not benefit from the complementary knowledge of different resources, and (2) the obtained representations are resource-specific and cannot be used across settings. In this paper we put forward a novel concept representation technique, called NASARI, which exploits the knowledge avail"
N15-1059,D14-1113,0,0.0504506,"Distributional semantic models are usually the first choice for representing textual items such as words or sentences (Turney and Pantel, 2010). These models have attracted considerable research interest, resulting in various co-occurrence based representations (Salton et al., 1975; Evert, 2005; Pado and Lapata, 2007; Erk and Pad´o, 2008) or predictive models (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; Baroni et al., 2014). Although there have been approaches proposed in the literature for learning sense-specific embeddings (Weston et al., 2013; Huang et al., 2012; Neelakantan et al., 2014), their coverage is limited only to those senses that are covered in the underlying corpus. Moreover, the obtained sense representations are usually not linked to any sense inventory, and therefore such linking has to be carried out, either manually, or with the help of sense-annotated data. Hence, unless they are provided with large amounts of sense-annotated data, these techniques cannot furnish an effective representation of word senses in an existing standard sense inventory. Consequently, most sense modeling techniques have based their representation on the knowledge derived from resource"
N15-1059,J07-2002,0,0.141103,"Missing"
N15-1059,P14-1044,1,0.829179,"ple datasets in two standard benchmarks: word similarity and sense clustering. We are releasing our vector representations at http://lcl.uniroma1.it/nasari/. 1 Introduction Obtaining accurate semantic representations of individual word senses or concepts is vital for several applications in Natural Language Processing (NLP) such as, for example, Word Sense Disambiguation (Navigli, 2009; Navigli, 2012), Entity Linking (Bunescu and Pas¸ca, 2006; Rao et al., 2013), semantic similarity (Budanitsky and Hirst, 2006), Information Extraction (Banko et al., 2007), and resource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, u"
N15-1059,P13-1132,1,0.923416,"y using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 2007; Dandala et al., 2013; Gabrilovich and Markovitch, 2007; Strube and Ponzetto, 2006), which are, respectively, the most widely-used lexicographic and encyclopedic resources in lexical semantics (Hovy et al., 2013). This restriction to a single resource brings about two main limitations: (1) the sense modeling does not benefit from the complementary knowledge of different resources, and (2) the obtained representations are resource-specific and cannot be used across settings. In this paper we put forward a novel concept representation techniqu"
N15-1059,P10-1040,0,0.341448,"esource linking and integration (Pilehvar and Navigli, 2014). One prominent semantic representation approach is the distributional semantic model, which represents lexical items as vectors in a semantic space. The weights in these vectors were traditionally computed on the basis of co-occurrence statistics (Salton et al., 1975; Turney and Pantel, 2010; Dinu and Lapata, 2010; Lappin and Fox, 2014), whereas for the more recent generation of distributional models weight computation is viewed as a context prediction problem, often to be solved by using neural networks (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013). Unfortunately, unless they are provided with large amounts of sense-annotated data these corpus-based techniques cannot capture polysemy in their representations, as they conflate different meanings of a word into a single vector. Therefore, most sense modeling techniques tend to base their computation on the knowledge obtained from various lexical resources. However, these techniques mainly utilize the knowledge derived from either WordNet (Banerjee and Pedersen, 2002; Budanitsky and Hirst, 2006; Pilehvar et al., 2013) or Wikipedia (Medelyan et al., 2009; Mihalcea, 20"
N15-1059,D13-1136,0,0.0100905,"ntic similarity. Concept representation. Distributional semantic models are usually the first choice for representing textual items such as words or sentences (Turney and Pantel, 2010). These models have attracted considerable research interest, resulting in various co-occurrence based representations (Salton et al., 1975; Evert, 2005; Pado and Lapata, 2007; Erk and Pad´o, 2008) or predictive models (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; Baroni et al., 2014). Although there have been approaches proposed in the literature for learning sense-specific embeddings (Weston et al., 2013; Huang et al., 2012; Neelakantan et al., 2014), their coverage is limited only to those senses that are covered in the underlying corpus. Moreover, the obtained sense representations are usually not linked to any sense inventory, and therefore such linking has to be carried out, either manually, or with the help of sense-annotated data. Hence, unless they are provided with large amounts of sense-annotated data, these techniques cannot furnish an effective representation of word senses in an existing standard sense inventory. Consequently, most sense modeling techniques have based their repres"
N18-6004,J14-1003,0,0.064392,"Missing"
N18-6004,P13-2095,0,0.0248108,"Missing"
N18-6004,E17-2036,1,0.850307,"lson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and taxonomy learning. Insights from recent SemEval tasks (Bordea et al. 2015, 2016) and related efforts on the automatic extraction of hypernymy relations from text corpora (Velardi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as other works attempting to improve the automatic procedures to align lexical resources (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014). ● Ontology enrichment. Enriching lexical ontologies with novel"
N18-6004,D14-1110,0,0.0233225,"WordNet or DBpedia. We will explain some of the current challenges in Word Sense Disambiguation and Entity Linking, as key tasks in natural language understanding which also enable a direct integration of knowledge from lexical resources. We will explain some knowledge-based and supervised methods for these tasks which play a decisive role in connecting lexical resources and text data (Zhong and Ng, 2010; Agirre et al. 2014; Moro et al.. 2014; Ling et al. 2015; Raganato et al. 2017). Moreover, we will present the field of knowledge-based representations, in particular word sense embeddings (Chen et al. 2014; Rothe and Schuetze, 2015; Camacho-Collados et al. 2016; Pilehvar and Collier, 2016; Mancini et al. 2017), as flexible techniques which act as a bridge between lexical resources and 18 applications. Finally, we will briefly present some recent work on the integration of this encoded knowledge from lexical resources into neural architectures for improving downstream NLP applications (Flekova and Gurevych, 2016; Pilehvar et al. 2017). 2. Outline ➢ Introduction and Motivation (15 mins) Adding explicit knowledge into AI/NLP systems is currently an important challenge due to the gains that can be"
N18-6004,D15-1084,0,0.0128731,"la and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and taxonomy learning. Insights from recent SemEval tasks (Bordea et al. 2015, 2016) and related efforts on the automatic extraction of hypernymy relations from text corpora (Velardi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resourc"
N18-6004,R15-1025,0,0.0176738,"and relations, phraseological units, or clustering techniques for senses and topics, as well as the integration of resources of different nature. The following topics are going to be covered in detail: ● Terminology extraction. Measures for terminology extraction, the simple conventional tf-idf (Sparck Jones, 1972), lexical specificity (Lafon, 1980), and more recent approaches exploiting linguistic knowledge (Hulth 2003; Vivaldi and Rodríguez, 2010). ● Definition extraction. Techniques for extracting definitional text snippets from corpora (Navigli and Velardi, 2010; Boella and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and t"
N18-6004,D16-1041,1,0.860585,"designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and taxonomy learning. Insights from recent SemEval tasks (Bordea et al. 2015, 2016) and related efforts on the automatic extraction of hypernymy relations from text corpora (Velardi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as"
N18-6004,C16-1323,1,0.854218,"designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and taxonomy learning. Insights from recent SemEval tasks (Bordea et al. 2015, 2016) and related efforts on the automatic extraction of hypernymy relations from text corpora (Velardi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as"
N18-6004,D11-1142,0,0.0139666,"nitional text snippets from corpora (Navigli and Velardi, 2010; Boella and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify (Delli Bovi et al., 2015). ● Hypernym discovery and taxonomy learning. Insights from recent SemEval tasks (Bordea et al. 2015, 2016) and related efforts on the automatic extraction of hypernymy relations from text corpora (Velardi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignme"
N18-6004,E12-1059,0,0.0325611,"rdi et al. 2013; Alfarone and Davis 2015; Flati et al. 2016; Shwartz et al. 2016; Espinosa-Anke et al. 2016a; Gupta et al. 2016). ● Topic clustering techniques. Relevant techniques for filtering general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as other works attempting to improve the automatic procedures to align lexical resources (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014). ● Ontology enrichment. Enriching lexical ontologies with novel synsets or with additional relations (Jurgens and Pilehvar, 2015; 2016; Espinosa-Anke et al., 2016b). In addition to these automatic efforts for easing the task of constructing and enriching lexical resources, we will present NLP tasks in which lexical resources have shown an important contribution. E"
N18-6004,P16-1191,0,0.0154848,", 2010; Agirre et al. 2014; Moro et al.. 2014; Ling et al. 2015; Raganato et al. 2017). Moreover, we will present the field of knowledge-based representations, in particular word sense embeddings (Chen et al. 2014; Rothe and Schuetze, 2015; Camacho-Collados et al. 2016; Pilehvar and Collier, 2016; Mancini et al. 2017), as flexible techniques which act as a bridge between lexical resources and 18 applications. Finally, we will briefly present some recent work on the integration of this encoded knowledge from lexical resources into neural architectures for improving downstream NLP applications (Flekova and Gurevych, 2016; Pilehvar et al. 2017). 2. Outline ➢ Introduction and Motivation (15 mins) Adding explicit knowledge into AI/NLP systems is currently an important challenge due to the gains that can be obtained in many downstream applications. At the same time, these resources can be further enriched and better exploited by making use of NLP techniques. In this context, the main motivation of this tutorial is to show how Natural Language Processing and Lexical Resources have interacted so far, and a view towards potential scenarios in the near future. The tutorial is then divided in two main blocks. First, w"
N18-6004,W03-1028,0,0.0995272,"lexical resources and NLP. Additionally, we will summarize existing attempts in this direction, such as modeling linguistic phenomena like terminology, definitions and glosses, examples and relations, phraseological units, or clustering techniques for senses and topics, as well as the integration of resources of different nature. The following topics are going to be covered in detail: ● Terminology extraction. Measures for terminology extraction, the simple conventional tf-idf (Sparck Jones, 1972), lexical specificity (Lafon, 1980), and more recent approaches exploiting linguistic knowledge (Hulth 2003; Vivaldi and Rodríguez, 2010). ● Definition extraction. Techniques for extracting definitional text snippets from corpora (Navigli and Velardi, 2010; Boella and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approa"
N18-6004,N15-1169,1,0.850096,"ignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as other works attempting to improve the automatic procedures to align lexical resources (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014). ● Ontology enrichment. Enriching lexical ontologies with novel synsets or with additional relations (Jurgens and Pilehvar, 2015; 2016; Espinosa-Anke et al., 2016b). In addition to these automatic efforts for easing the task of constructing and enriching lexical resources, we will present NLP tasks in which lexical resources have shown an important contribution. Effectively leveraging linguistically expressible cues with their associated knowledge remains a difficult task. Knowledge may be extracted from (roughly) three types of resources (Hovy et al., 2013): unstructured, e.g. text corpora; semistructured, such as encyclopedic collaborative repositories like Wikipedia and Wiktionary, or structured, which include lexic"
N18-6004,S16-1169,1,0.907779,"Missing"
N18-6004,Q13-1013,0,0.0234379,"g general domain resources via topic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as other works attempting to improve the automatic procedures to align lexical resources (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014). ● Ontology enrichment. Enriching lexical ontologies with novel synsets or with additional relations (Jurgens and Pilehvar, 2015; 2016; Espinosa-Anke et al., 2016b). In addition to these automatic efforts for easing the task of constructing and enriching lexical resources, we will present NLP tasks in which lexical resources have shown an important contribution. Effectively leveraging linguistically expressible cues with their associated knowledge remains a difficult task. Knowledge may be extracted from (roughly) three types of resources (Hovy et al., 2013): u"
N18-6004,P10-1134,0,0.0128014,"like terminology, definitions and glosses, examples and relations, phraseological units, or clustering techniques for senses and topics, as well as the integration of resources of different nature. The following topics are going to be covered in detail: ● Terminology extraction. Measures for terminology extraction, the simple conventional tf-idf (Sparck Jones, 1972), lexical specificity (Lafon, 1980), and more recent approaches exploiting linguistic knowledge (Hulth 2003; Vivaldi and Rodríguez, 2010). ● Definition extraction. Techniques for extracting definitional text snippets from corpora (Navigli and Velardi, 2010; Boella and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic relations from text: NELL (Carlson et al., 2010), ReVerb (Fader et al. 2011), PATTY (Nakashole et al., 2011), KB-Unify ("
N18-6004,D16-1174,1,0.853113,"Sense Disambiguation and Entity Linking, as key tasks in natural language understanding which also enable a direct integration of knowledge from lexical resources. We will explain some knowledge-based and supervised methods for these tasks which play a decisive role in connecting lexical resources and text data (Zhong and Ng, 2010; Agirre et al. 2014; Moro et al.. 2014; Ling et al. 2015; Raganato et al. 2017). Moreover, we will present the field of knowledge-based representations, in particular word sense embeddings (Chen et al. 2014; Rothe and Schuetze, 2015; Camacho-Collados et al. 2016; Pilehvar and Collier, 2016; Mancini et al. 2017), as flexible techniques which act as a bridge between lexical resources and 18 applications. Finally, we will briefly present some recent work on the integration of this encoded knowledge from lexical resources into neural architectures for improving downstream NLP applications (Flekova and Gurevych, 2016; Pilehvar et al. 2017). 2. Outline ➢ Introduction and Motivation (15 mins) Adding explicit knowledge into AI/NLP systems is currently an important challenge due to the gains that can be obtained in many downstream applications. At the same time, these resources can be f"
N18-6004,P17-1170,1,0.839468,"Moro et al.. 2014; Ling et al. 2015; Raganato et al. 2017). Moreover, we will present the field of knowledge-based representations, in particular word sense embeddings (Chen et al. 2014; Rothe and Schuetze, 2015; Camacho-Collados et al. 2016; Pilehvar and Collier, 2016; Mancini et al. 2017), as flexible techniques which act as a bridge between lexical resources and 18 applications. Finally, we will briefly present some recent work on the integration of this encoded knowledge from lexical resources into neural architectures for improving downstream NLP applications (Flekova and Gurevych, 2016; Pilehvar et al. 2017). 2. Outline ➢ Introduction and Motivation (15 mins) Adding explicit knowledge into AI/NLP systems is currently an important challenge due to the gains that can be obtained in many downstream applications. At the same time, these resources can be further enriched and better exploited by making use of NLP techniques. In this context, the main motivation of this tutorial is to show how Natural Language Processing and Lexical Resources have interacted so far, and a view towards potential scenarios in the near future. The tutorial is then divided in two main blocks. First, we delve into NLP for C"
N18-6004,P14-1044,1,0.834755,"opic grouping (Roget’s, 1911; Navigli and Velardi, 2004, Camacho-Collados and Navigli, 2017). ● Alignment of lexical resources: Alignment of heterogeneous lexical resources contributing to the creation of large resources containing different sources of knowledge. We will present approaches for the construction of such resources, such as Yago (Suchanek et al. 2007), UBY (Gurevych et al. 2012), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al. 2017), as well as other works attempting to improve the automatic procedures to align lexical resources (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014). ● Ontology enrichment. Enriching lexical ontologies with novel synsets or with additional relations (Jurgens and Pilehvar, 2015; 2016; Espinosa-Anke et al., 2016b). In addition to these automatic efforts for easing the task of constructing and enriching lexical resources, we will present NLP tasks in which lexical resources have shown an important contribution. Effectively leveraging linguistically expressible cues with their associated knowledge remains a difficult task. Knowledge may be extracted from (roughly) three types of resources (Hovy et al., 2013): unstructured, e.g. text corpora;"
N18-6004,P15-1173,0,0.0571924,"Missing"
N18-6004,vivaldi-rodriguez-2010-finding,0,0.0115787,"ources and NLP. Additionally, we will summarize existing attempts in this direction, such as modeling linguistic phenomena like terminology, definitions and glosses, examples and relations, phraseological units, or clustering techniques for senses and topics, as well as the integration of resources of different nature. The following topics are going to be covered in detail: ● Terminology extraction. Measures for terminology extraction, the simple conventional tf-idf (Sparck Jones, 1972), lexical specificity (Lafon, 1980), and more recent approaches exploiting linguistic knowledge (Hulth 2003; Vivaldi and Rodríguez, 2010). ● Definition extraction. Techniques for extracting definitional text snippets from corpora (Navigli and Velardi, 2010; Boella and DiCaro, 2013; Espinosa-Anke et al. 2015; Li et al. 2016). 17 Proceedings of NAACL-HLT 2018: Tutorial Abstracts, pages 17–23 c New Orleans, Louisiana, June 1, 2018. 2018 Association for Computational Linguistics ● Automatic extraction of examples. Description of example extraction techniques and designs on this direction, e.g., the GDEX criteria and their implementation (Kilgariff et al., 2008). ● Information extraction. Recent approaches for extracting semantic r"
N18-6004,P10-4014,0,0.0279959,"s (Hovy et al., 2013): unstructured, e.g. text corpora; semistructured, such as encyclopedic collaborative repositories like Wikipedia and Wiktionary, or structured, which include lexicographic resources like WordNet or DBpedia. We will explain some of the current challenges in Word Sense Disambiguation and Entity Linking, as key tasks in natural language understanding which also enable a direct integration of knowledge from lexical resources. We will explain some knowledge-based and supervised methods for these tasks which play a decisive role in connecting lexical resources and text data (Zhong and Ng, 2010; Agirre et al. 2014; Moro et al.. 2014; Ling et al. 2015; Raganato et al. 2017). Moreover, we will present the field of knowledge-based representations, in particular word sense embeddings (Chen et al. 2014; Rothe and Schuetze, 2015; Camacho-Collados et al. 2016; Pilehvar and Collier, 2016; Mancini et al. 2017), as flexible techniques which act as a bridge between lexical resources and 18 applications. Finally, we will briefly present some recent work on the integration of this encoded knowledge from lexical resources into neural architectures for improving downstream NLP applications (Flekov"
N18-6004,D12-1104,0,\N,Missing
N18-6004,J04-2002,0,\N,Missing
N18-6004,S15-2151,0,\N,Missing
N18-6004,P15-1010,1,\N,Missing
N18-6004,N13-1092,0,\N,Missing
N18-6004,N15-1164,0,\N,Missing
N18-6004,S16-1168,0,\N,Missing
N18-6004,K16-1006,0,\N,Missing
N18-6004,P16-1085,1,\N,Missing
N18-6004,C16-1217,0,\N,Missing
N18-6004,E17-1010,1,\N,Missing
N18-6004,D17-1120,0,\N,Missing
N18-6004,P16-1226,0,\N,Missing
N19-1128,N19-1423,0,0.160101,"Missing"
N19-1128,D18-1200,0,0.0163851,"as a human-level performance upperbound, is 0.52. Moreover, most of the instances in SCWS have context pairs with different target words.14 This makes it possible to test context-independent models, which only considers word pairs in isolation, on the dataset. Importantly, such a context-independent model can easily surpass the human-level performance upperbound. For instance, we computed the performance of the Google News Word2vec pre-trained word embeddings (Mikolov et al., 2013b) on the dataset to be 0.65 (ρ), which is significantly higher than the optimistic IRA for the dataset. In fact, Dubossarsky et al. (2018) showed how the reported high performance of multi-prototype techniques in this dataset was not due to an accurate sense representation, but rather to a subsampling effect, which had not been controlled for in similarity datasets. In contrast, a context-insensitive word embedding model would perform no better than a random baseline on our dataset. Related work 5 The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to standard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and SimLex (Hill et al., 2015)"
N19-1128,P16-1191,0,0.0428877,"an word senses), a target word (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In the next section we show that the pruning resulted in a significant boost in the clarity of the dataset. 2.2 Quality check To verify the quality and the difficulty of the dataset and to estimate the human-level performance upperbound, we randomly sampled four sets of 100 instances from the test set, with an overlap of 50 instances between two of the annotators. Each set was assigned to an annotator who was asked to label each instance based on whether they thought the two occurrences of the word referred to the same meaning or not.6 The annotators were not provided"
N19-1128,J15-4004,0,0.0755782,"rsky et al. (2018) showed how the reported high performance of multi-prototype techniques in this dataset was not due to an accurate sense representation, but rather to a subsampling effect, which had not been controlled for in similarity datasets. In contrast, a context-insensitive word embedding model would perform no better than a random baseline on our dataset. Related work 5 The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to standard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and SimLex (Hill et al., 2015), in which the task is to automatically estimate the semantic similarity of word pairs. Ideally, the estimated similarity scores should have high correlation with those given by human annotators. However, there is a fundamental difference between SCWS and other word similarity datasets: each word in SCWS is associated with a context which triggers a specific meaning of the word. The unique property of the dataset makes it a suitable benchmark for multi-prototype and contextualized word embeddings. However, in the following, we highlight some of the limitations of the dataset which hinder its s"
N19-1128,P12-1092,0,0.578031,"resented in isolation; hence, they are not suitable for verifying the dynamic nature of word semantics) or carry out impact analysis in downstream NLP applications (usually, by taking word embeddings as baseline). Despite providing a suitable means of verifying the effectiveness of the embeddings, the downstream evaluation cannot replace generic evaluations as it is difficult to isolate the impact of embeddings from many other factors involved, including the algorithmic configuration and parameter setting of the system. To our knowledge, the Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) is the only existing benchmark that specifically focuses on the dynamic nature of word semantics.2 In Section 4 we will explain the limitations of this dataset for the evaluation of recent work in the literature. In this paper we propose WiC, a novel dataset that provides a high-quality benchmark for the evaluation of context-sensitive word embeddings. WiC provides multiple interesting characteristics: (1) it is suitable for evaluating a wide range of techniques, including contextualized word and sense representation and word sense disambiguation; (2) it is framed as a binary classification d"
N19-1128,P06-1014,0,0.103333,"r initial training dataset. Semi-automatic check. Even though very few in number, all resources (even exprt-based ones) contain errors such as incorrect part-of-speech tags or ill-formed examples. Moreover, the extraction of examples and the mappings across resources were not always accurate. In order to have as few resource-specific and mapping errors as possible, all training, development and test sets were semi-automatically post-processed, either with small fixes whenever possible or by removing problematic instances otherwise. 2.1.2 Pruning WordNet is known to be a fine-grained resource (Navigli, 2006). Often, different senses of the same word are hardly distinguishable from one another even for humans. For example, more than 40 senses are listed for the verb run, with many of them corresponding to similar concepts, e.g., “move fast”, “travel rapidly”, and “run with the ball”. In order to avoid this high-granularity, we performed an automatic pruning of the resource, removing instances with subtle sense distinctions. Sense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense distinction (Snow et al., 2007; Pilehvar et"
N19-1128,D14-1113,0,0.0631549,"1 Introduction One of the main limitations of mainstream word embeddings lies in their static nature, i.e., a word is associated with the same embedding, independently from the context in which it appears. Therefore, these embeddings are unable to reflect the dynamic nature of ambiguous words1 , in that they can correspond to different (potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word. Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluation. Most works in this doma"
N19-1128,W16-1620,0,0.074152,"main limitations of mainstream word embeddings lies in their static nature, i.e., a word is associated with the same embedding, independently from the context in which it appears. Therefore, these embeddings are unable to reflect the dynamic nature of ambiguous words1 , in that they can correspond to different (potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word. Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluation. Most works in this domain either perform evalua"
N19-1128,N18-1202,0,0.484341,"(potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word. Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluation. Most works in this domain either perform evaluations on word similarity datasets (in which words are presented in isolation; hence, they are not suitable for verifying the dynamic nature of word semantics) or carry out impact analysis in downstream NLP applications (usually, by taking word embeddings as baseline). Despite providing a suitable means of ver"
N19-1128,P17-1170,1,0.883717,"rd (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In the next section we show that the pruning resulted in a significant boost in the clarity of the dataset. 2.2 Quality check To verify the quality and the difficulty of the dataset and to estimate the human-level performance upperbound, we randomly sampled four sets of 100 instances from the test set, with an overlap of 50 instances between two of the annotators. Each set was assigned to an annotator who was asked to label each instance based on whether they thought the two occurrences of the word referred to the same meaning or not.6 The annotators were not provided with knowledge from any"
N19-1128,D16-1174,1,0.933732,"Missing"
N19-1128,K17-1012,1,0.948555,"ferent senses of the same word are hardly distinguishable from one another even for humans. For example, more than 40 senses are listed for the verb run, with many of them corresponding to similar concepts, e.g., “move fast”, “travel rapidly”, and “run with the ball”. In order to avoid this high-granularity, we performed an automatic pruning of the resource, removing instances with subtle sense distinctions. Sense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense distinction (Snow et al., 2007; Pilehvar et al., 2013; Mancini et al., 2017). We adopted a simple strategy and 4 Given that WordNet provides examples for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet sense inventory has been"
N19-1128,P13-1132,1,0.771028,"igli, 2006). Often, different senses of the same word are hardly distinguishable from one another even for humans. For example, more than 40 senses are listed for the verb run, with many of them corresponding to similar concepts, e.g., “move fast”, “travel rapidly”, and “run with the ball”. In order to avoid this high-granularity, we performed an automatic pruning of the resource, removing instances with subtle sense distinctions. Sense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense distinction (Snow et al., 2007; Pilehvar et al., 2013; Mancini et al., 2017). We adopted a simple strategy and 4 Given that WordNet provides examples for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet s"
N19-1128,N10-1013,0,0.0928172,"://pilehvar.github.io/wic/. 1 Introduction One of the main limitations of mainstream word embeddings lies in their static nature, i.e., a word is associated with the same embedding, independently from the context in which it appears. Therefore, these embeddings are unable to reflect the dynamic nature of ambiguous words1 , in that they can correspond to different (potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word. Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluatio"
N19-1128,K16-1006,0,0.0996134,"rrespond to different (potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multiprototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and contextualized 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word. Despite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluation. Most works in this domain either perform evaluations on word similarity datasets (in which words are presented in isolation; hence, they are not suitable for verifying the dynamic nature of word semantics) or carry out impact analysis in downstream NLP applications (usually, by taking word embeddings as baseline). Despite providing a"
N19-1128,P11-1097,0,0.0396217,"Missing"
N19-1128,P13-2125,0,0.0404721,"for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In the next section we show that the pruning resulted in a significant boost in the clarity of the dataset. 2.2 Quality check To verify the quality and the difficulty of the dataset and to estimate the human-level performance upperbound, we randomly sampled four sets of 100 instances from the test set, with an overlap of 50 instances between two of the annotators. Each set was assigned to an annotator who was asked to label each instance based on whether they thought the two occurrences of the word referred to the same meaning or not.6 The"
N19-1128,D07-1107,0,0.0152477,"ained resource (Navigli, 2006). Often, different senses of the same word are hardly distinguishable from one another even for humans. For example, more than 40 senses are listed for the verb run, with many of them corresponding to similar concepts, e.g., “move fast”, “travel rapidly”, and “run with the ball”. In order to avoid this high-granularity, we performed an automatic pruning of the resource, removing instances with subtle sense distinctions. Sense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense distinction (Snow et al., 2007; Pilehvar et al., 2013; Mancini et al., 2017). We adopted a simple strategy and 4 Given that WordNet provides examples for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset. 1268 removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files5 . There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coar"
N19-1128,S17-1004,0,0.0213614,"this dataset for the evaluation of recent work in the literature. In this paper we propose WiC, a novel dataset that provides a high-quality benchmark for the evaluation of context-sensitive word embeddings. WiC provides multiple interesting characteristics: (1) it is suitable for evaluating a wide range of techniques, including contextualized word and sense representation and word sense disambiguation; (2) it is framed as a binary classification dataset, in which, unlike SCWS, identical words are paired with each other (in different con2 With a similar goal in mind but focused on hypernymy, Vyas and Carpuat (2017) developed a benchmark to assess the capability of automatic systems to detect hypernymy relations in context. 1267 Proceedings of NAACL-HLT 2019, pages 1267–1273 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics F There’s a lot of trash on the bed of the river — I keep a glass of water next to my bed when I sleep F Justify the margins — The end justifies the means T Air pollution — Open a window and let in some air T The expanded window will give us time to catch the thieves — You have a two-hour window of clear weather to finish working on the la"
N19-1128,J16-2003,0,\N,Missing
P15-1072,E09-1005,0,0.756023,"not consider those BabelNet synsets that are not associated with Wikipedia pages. WordNet sense inventory. Similarly, when restricted to the WordNet inventory, we discard those BabelNet synsets that do not contain a WordNet synset. In this setting, we also leverage relations from WordNet’s semantic network and its disambiguated glosses3 in order to obtain a richer set of Wikipedia articles in the sub-corpus construction. The enrichment of the semantic network with the disambiguated glosses has been shown to be beneficial in various graph-based disambiguation tasks (Navigli and Velardi, 2005; Agirre and Soroa, 2009; Pilehvar et al., 2013). 4 Experiments We assess the reliability of M UFFIN in two standard evaluation benchmarks: semantic similarity (Section 4.1) and Word Sense Disambiguation (Section 4.2). 4.1 Datasets Semantic Similarity As our semantic similarity experiment we opted for word similarity, which is one of the most popular evaluation frameworks in lexical semantics. Given a pair of words, the task in word similarity is to automatically judge their semantic similarity and, ideally, this judgement should be close to that given by humans. 4.1.2 Comparison systems Monolingual. We benchmark our"
P15-1072,N09-1003,0,0.0629266,"Missing"
P15-1072,P14-1023,0,0.440805,"ine-interpretable form, is a fundamental problem in Natural Language Processing (NLP). The Vector Space Model (VSM) is a prominent approach for semantic representation, with widespread popularity in numerous NLP applications. The prevailing methods for the computation of a vector space representation are based on distributional semantics (Harris, 1954). However, these approaches, whether in their conventional co-occurrence based form (Salton et al., 1975; Turney and Pantel, 2010; Landauer and Dooley, 2002), or in their newer predictive branch (Collobert and Weston, 2008; Mikolov et al., 2013; Baroni et al., 2014), suffer from a major drawback: they are unable to model individual word senses or concepts, as they conflate 1. Multilingual: it enables sense representation in dozens of languages; 2. Unified: it represents a linguistic item, irrespective of its language, in a unified seman741 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 741–751, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 1: Our procedure for constructing a multilingual vec"
P15-1072,F14-1032,1,0.762091,"on multiple datasets and settings in both frameworks, which confirms the reliability and flexibility of our representations. 2 2.2 Methodology Vector construction: lexical specificity Lexical specificity (Lafon, 1980) is a statistical measure based on the hypergeometric distribution. Due to its efficiency in extracting a set of highly relevant words from a sub-corpus, the measure has recently gained popularity in different NLP applications, such as textual data analysis (Lebart et al., 1998), term extraction (Drouin, 2003), and domain-based term disambiguation (Camacho-Collados et al., 2014; Billami et al., 2014). We leverage lexical specificity to compute the weights in our vectors. In our earlier work (Camacho-Collados et al., 2015), we conducted different experiments which demonstrated the improvement that lexical specificity can provide over the popular term frequency-inverse document frequency weighting scheme (Jones, 1972, tf-idf ). Lexical specificity computes the vector weights for an item, i.e., a word or a set of words, by comparing and contrasting its contextual information with a reference corpus. In our setting, we take the whole Wikipedia as our reference corpus RC (we use the October 20"
P15-1072,E09-1013,0,0.0487404,"al Semantic Representation of Concepts Jos´e Camacho-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract different meanings of a word into a single vectorial representation. This hinders the functionality of this group of vector space models in tasks such as Word Sense Disambiguation (WSD) that require the representation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches h"
P15-1072,D09-1124,0,0.0465708,"the RG-65 dataset (Rubenstein and Goodenough, 1965) as our monolingual word similarity dataset. The dataset comprises 65 English word pairs which have been manually annotated by several annotators according to their similarity on a scale of 0 to 4. We also perform evaluations on the French (Joubarne and Inkpen, 2011) and German (Gurevych, 2005) adaptations of this dataset. c∈Cw 7: return cˆ Thanks to the use of BabelNet, our approach is applicable to arbitrary languages. For the task of WSD, we focus on two major sense inventories integrated in BabelNet: Wikipedia and WordNet. Cross-lingual. Hassan and Mihalcea (2009) developed two sets of cross-lingual datasets based on the English MC-30 (Miller and Charles, 1991) and WordSim-353 (Finkelstein et al., 2002) datasets, for four different languages: English, German, Romanian, and Arabic. However, the construction procedure they adopted, consisting of translating the pairs to other languages while preserving the original similarity scores, has led to inconsistencies in the datasets. For instance, the Spanish dataset contains the identical pair mediodiamediodia with a similarity score of 3.42 (in the scale [0,4]). Additionally, the datasets contain several orth"
P15-1072,N15-1059,1,0.795724,"esentation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual"
P15-1072,D14-1110,0,0.232776,"Missing"
P15-1072,P12-1092,0,0.153897,"-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract different meanings of a word into a single vectorial representation. This hinders the functionality of this group of vector space models in tasks such as Word Sense Disambiguation (WSD) that require the representation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model indi"
P15-1072,D07-1061,0,0.0271844,"e of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Ou"
P15-1072,N15-1184,0,0.0879558,"Missing"
P15-1072,I05-1067,0,0.360848,"4.1.1 Input: a target word w and a document d (context of w) Output: cˆ, the intended sense of w 1: for each concept c ∈ Cw 2: scorec ← 0 3: for each lemma l ∈ d 4: if l ∈ lexc then −1 5: scorec ← scorec + rank(l, lexc ) 6: cˆ ← arg max scorec Monolingual. We picked the RG-65 dataset (Rubenstein and Goodenough, 1965) as our monolingual word similarity dataset. The dataset comprises 65 English word pairs which have been manually annotated by several annotators according to their similarity on a scale of 0 to 4. We also perform evaluations on the French (Joubarne and Inkpen, 2011) and German (Gurevych, 2005) adaptations of this dataset. c∈Cw 7: return cˆ Thanks to the use of BabelNet, our approach is applicable to arbitrary languages. For the task of WSD, we focus on two major sense inventories integrated in BabelNet: Wikipedia and WordNet. Cross-lingual. Hassan and Mihalcea (2009) developed two sets of cross-lingual datasets based on the English MC-30 (Miller and Charles, 1991) and WordSim-353 (Finkelstein et al., 2002) datasets, for four different languages: English, German, Romanian, and Arabic. However, the construction procedure they adopted, consisting of translating the pairs to other lang"
P15-1072,S13-2042,0,0.0480006,"Missing"
P15-1072,S10-1003,0,0.0437631,"vided with suitable amounts of sense-annotated data, their applicability is limited to those words and languages for which such data is available, practically limiting them to a small subset of words mainly in the English language. Knowledge-based approaches (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2009) significantly improve the coverage of supervised systems. However, similarly to their supervised counterparts, knowledge-based techniques are usually limited to the English language. Recent years have seen a growing interest in cross-lingual and multilingual WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality is usually offered by methods that exploit the structural information of large-scale multilingual lexical resources such as Wikipedia (Guti´errez et al., 2013; Manion and Sainudiin, 2013; Hovy et al., 2013). Babelfy (Moro et al., 2014) is an approach with state-ofthe-art performance that relies on random walks Related work We briefly review the recent literature on the two NLP tasks to which we applied our representations, i.e., Word Sense Disambiguation and semantic similarity. WSD. There are two main categories of WSD techniqu"
P15-1072,S13-2029,0,0.0397807,"ts of sense-annotated data, their applicability is limited to those words and languages for which such data is available, practically limiting them to a small subset of words mainly in the English language. Knowledge-based approaches (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2009) significantly improve the coverage of supervised systems. However, similarly to their supervised counterparts, knowledge-based techniques are usually limited to the English language. Recent years have seen a growing interest in cross-lingual and multilingual WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality is usually offered by methods that exploit the structural information of large-scale multilingual lexical resources such as Wikipedia (Guti´errez et al., 2013; Manion and Sainudiin, 2013; Hovy et al., 2013). Babelfy (Moro et al., 2014) is an approach with state-ofthe-art performance that relies on random walks Related work We briefly review the recent literature on the two NLP tasks to which we applied our representations, i.e., Word Sense Disambiguation and semantic similarity. WSD. There are two main categories of WSD techniques: knowledge-based and s"
P15-1072,S13-2043,0,0.0202365,"pproaches (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2009) significantly improve the coverage of supervised systems. However, similarly to their supervised counterparts, knowledge-based techniques are usually limited to the English language. Recent years have seen a growing interest in cross-lingual and multilingual WSD (Lefever and Hoste, 2010; Lefever and Hoste, 2013; Navigli et al., 2013). Multilinguality is usually offered by methods that exploit the structural information of large-scale multilingual lexical resources such as Wikipedia (Guti´errez et al., 2013; Manion and Sainudiin, 2013; Hovy et al., 2013). Babelfy (Moro et al., 2014) is an approach with state-ofthe-art performance that relies on random walks Related work We briefly review the recent literature on the two NLP tasks to which we applied our representations, i.e., Word Sense Disambiguation and semantic similarity. WSD. There are two main categories of WSD techniques: knowledge-based and supervised 748 word senses. Thanks to its effective combination of distributional statistics and structured knowledge, the approach can compute efficient representations of arbitrary word senses, with high coverage and irrespect"
P15-1072,D14-1162,0,0.0877904,"lados et al., 2015). However, these techniques are either limited in the languages to which they can be applied, or in their applicability to tasks other than semantic similarity (Navigli and Ponzetto, 2012b). Corpus-based techniques are more flexible, enabling the training of models on corpora other than English. However, these approaches, either in their conventional co-occurrence based form (Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997; Turney and Pantel, 2010; Bullinaria and Levy, 2012), or the more recent predictive models (Mikolov et al., 2013; Collobert and Weston, 2008; Pennington et al., 2014), are restricted in two ways: (1) they cannot be used to compare word senses; and (2) they cannot be directly applied to cross-lingual semantic similarity. Though the first problem has been solved by multi-prototype models (Huang et al., 2012), or by the sense-specific representations obtained as a result of exploiting WordNet glosses (Chen et al., 2014), the second problem remains unaddressed. In contrast, our approach models word senses and concepts effectively, while providing a unified representation for different languages that enables cross-lingual semantic similarity. 6 Acknowledgments"
P15-1072,P14-1044,1,0.783795,"and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Our approach provides multiple advantages in comparison to the previous VSM techniques: Semantic representa"
P15-1072,P13-1132,1,0.862073,"andard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Our approach provides multiple advantages in comparison to the previous VSM te"
P15-1072,H93-1061,0,0.184777,"(SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2009, UKB w2w) and the BabelNet semantic network (Moro et al., 2014, Babelfy). We follow Babelfy and also exploit the WordNet’s sense frequency information from the SemCor senseannotated corpus (Miller et al., 1993). However, instead of simply backing off to the most frequent sense, we propose a more meaningful exploitation of this information. To this end, we compute the relevance of a specific sense as the average of its normalized sense frequency and its corresponding Word Sense Disambiguation Wikipedia In this setting, we selected the SemEval 2013 allwords WSD task (Navigli et al., 2013) as our evaluation benchmark. The task provides datasets for five different languages: Italian, English, French, Spanish and German. There are on average 1123 words to disambiguate in each language’s dataset. As compa"
P15-1072,Q14-1019,1,0.944398,"and All-Words task (Pradhan et al., 2007). The all-words datasets of the two tasks contain 1644 instances (SemEval-2013) and 162 noun instances (SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2009, UKB w2w) and the BabelNet semantic network (Moro et al., 2014, Babelfy). We follow Babelfy and also exploit the WordNet’s sense frequency information from the SemCor senseannotated corpus (Miller et al., 1993). However, instead of simply backing off to the most frequent sense, we propose a more meaningful exploitation of this information. To this end, we compute the relevance of a specific sense as the average of its normalized sense frequency and its corresponding Word Sense Disambiguation Wikipedia In this setting, we selected the SemEval 2013 allwords WSD task (Navigli et al., 2013) as our evaluation benchmark. The task provides datasets for five dif"
P15-1072,W09-3204,0,0.0161049,"Missing"
P15-1072,J91-1002,0,0.380594,"However, the approach is limited to the WSD and Entity Linking tasks. In contrast, our approach is global as it can be used in different NLP tasks, including WSD. Semantic similarity. Semantic similarity of word pairs is usually computed either on the basis of the structural properties of lexical databases and thesauri, or by comparing vectorial representations of words learned from massive text corpora. Structural approaches usually measure the similarity on the basis of the distance information on semantic networks, such as WordNet (Budanitsky and Hirst, 2006), or thesauri, such as Roget’s (Morris and Hirst, 1991; Jarmasz and Szpakowicz, 2003). The semantic network of WordNet has also been used in more sophisticated techniques such as those based on random graph walks (Ramage et al., 2009; Pilehvar et al., 2013), or coupled with the complementary knowledge from Wikipedia (Camacho-Collados et al., 2015). However, these techniques are either limited in the languages to which they can be applied, or in their applicability to tasks other than semantic similarity (Navigli and Ponzetto, 2012b). Corpus-based techniques are more flexible, enabling the training of models on corpora other than English. However,"
P15-1072,N10-1013,0,0.149374,"on of Concepts Jos´e Camacho-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract different meanings of a word into a single vectorial representation. This hinders the functionality of this group of vector space models in tasks such as Word Sense Disambiguation (WSD) that require the representation of individual word senses. There have been several efforts to adapt and apply distributional approaches to the representation of word senses (Pantel and Lin, 2002; Brody and Lapata, 2009; Reisinger and Mooney, 2010; Huang et al., 2012). However, none of these techniques provides representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully u"
P15-1072,W09-3206,0,0.0188389,"that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data. Chen et al. (2014) addressed this issue and obtained vectors for individual word senses by leveraging WordNet glosses. NASARI (Camacho-Collados et al., 2015) is another approach that obtains accurate sense-specific representations by combining the complementary knowledge from WordNet and Wikipedia. Graph-based approaches have also been successfully utilized to model individual words (Hughes and Ramage, 2007; Agirre et al., 2009; Yeh et al., 2009), or concepts (Pilehvar et al., 2013; Pilehvar and Navigli, 2014), drawing on the structural properties of semantic networks. The applicability of all these techniques, however, is usually either constrained to a single language (usually English), or to a specific task. We put forward M UFFIN (Multilingual, UniFied and Flexible INterpretation), a novel method that exploits both structural knowledge derived from semantic networks and distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Our approach provides multiple advantages"
P15-1072,S13-2040,1,0.922763,"oreover, M UFFINpivot attains the best results among the pivot systems on all datasets, confirming the reliability of our system in the monolingual setting. We note that since the cross-lingual datasets were built by translating the word pairs in the original English RG-65 dataset, the pivot-based comparison systems proved to be highly competitive, outperforming the CL-MSR2.0 system by a considerable margin. 4.2 4.2.1 4.2.2 WordNet As regards the WordNet disambiguation task, we take as our benchmark the two recent SemEval English all-words WSD tasks: the SemEval-2013 task on Multilingual WSD (Navigli et al., 2013) and the SemEval-2007 English Lexical Sample, SRL and All-Words task (Pradhan et al., 2007). The all-words datasets of the two tasks contain 1644 instances (SemEval-2013) and 162 noun instances (SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2"
P15-1072,P10-4014,0,0.479385,"e take as our benchmark the two recent SemEval English all-words WSD tasks: the SemEval-2013 task on Multilingual WSD (Navigli et al., 2013) and the SemEval-2007 English Lexical Sample, SRL and All-Words task (Pradhan et al., 2007). The all-words datasets of the two tasks contain 1644 instances (SemEval-2013) and 162 noun instances (SemEval-2007), respectively. As comparison system, we report the performance of the best configuration of the topperforming system in the SemEval-2013 task, i.e., UMCC-DLSI (Guti´errez et al., 2013). We also show results for the state-of-the-art supervised system (Zhong and Ng, 2010, IMS), as well as for two graph-based approaches that are based on random walks on the WordNet graph (Agirre and Soroa, 2009, UKB w2w) and the BabelNet semantic network (Moro et al., 2014, Babelfy). We follow Babelfy and also exploit the WordNet’s sense frequency information from the SemCor senseannotated corpus (Miller et al., 1993). However, instead of simply backing off to the most frequent sense, we propose a more meaningful exploitation of this information. To this end, we compute the relevance of a specific sense as the average of its normalized sense frequency and its corresponding Wor"
P15-1072,S07-1016,0,\N,Missing
P15-1072,W09-2413,0,\N,Missing
P15-1072,J06-1003,0,\N,Missing
P15-2001,agirre-de-lacalle-2004-publicly,0,0.0794886,"Missing"
P15-2001,J15-4004,0,0.0289188,"Missing"
P15-2001,P14-1023,0,0.0149352,"he automatic procedure on different languages. Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-anno"
P15-2001,N15-1059,1,0.712727,"nd Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and"
P15-2001,S14-2003,1,0.67505,"ization of the approach which would be capable of automatically constructing reliable cross-lingual similarity datasets for any pair of languages. Scoring the dataset Twelve native Spanish speakers were asked to evaluate the similarity for the Spanish translations. In order to obtain a more global distribution of judges, we included judges both both Spain and Latin America. As far as the Farsi dataset was concerned, twelve Farsi native speakers scored the newly translated pairs. The guidelines provided to the annotators were based on the recent SemEval task on Cross-Level Semantic Similarity (Jurgens et al., 2014), which provides clear indications in order to distinguish similarity and relatedness. The annotators were allowed to give scores from 0 to 4, with a step size of 0.5. Table 1 shows example pairs with their corresponding scores from the English and the newly created Spanish and Farsi versions of the RG65 dataset. As we can see from the table, the scores across languages are not necessarily identical, with small, in a few cases significant, differences between the corresponding scores. This is due to the fact that associated senses with words do not hold one-to-one correspondence across differe"
P15-2001,P15-1072,1,0.362867,"nd Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and"
P15-2001,E14-1044,1,0.813372,"e English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-l"
P15-2001,N07-1025,0,0.0174777,"Missing"
P15-2001,I05-1067,0,0.376725,"´e Camacho-Collados, Mohammad Taher Pilehvar and Roberto Navigli Department of Computer Science Sapienza University of Rome {collados,pilehvar,navigli}@di.uniroma1.it Abstract in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. W"
P15-2001,P11-1076,0,0.0297043,"Kennedy and Hirst (2012) for the automatic construction of cross-lingual datasets from aligned monolingual datasets. Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had The paper is structured as follows. We first briefly review some of the major monolingual and cross-lingual word similarity datasets in Section 2. We then discuss the details of our procedure for the construction of the Spanish and Farsi word similarity datasets in Section 3. Section 4 provides the details of our algorithm for the automatic construction o"
P15-2001,D09-1124,0,0.216669,"@di.uniroma1.it Abstract in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an"
P15-2001,P14-1044,1,0.385004,"igned monolingual datasets. Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had The paper is structured as follows. We first briefly review some of the major monolingual and cross-lingual word similarity datasets in Section 2. We then discuss the details of our procedure for the construction of the Spanish and Farsi word similarity datasets in Section 3. Section 4 provides the details of our algorithm for the automatic construction of the cross-lingual datasets. We report the results of the evaluation performed on the generated"
P15-2001,P13-1132,1,0.716354,"vide an evaluation of the automatic procedure on different languages. Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al.,"
P16-5004,D15-1200,0,\N,Missing
P16-5004,D08-1048,0,\N,Missing
P16-5004,P15-1173,0,\N,Missing
P16-5004,J06-1003,0,\N,Missing
P16-5004,S14-2003,0,\N,Missing
P16-5004,N13-1130,0,\N,Missing
P16-5004,D14-1113,0,\N,Missing
P16-5004,P12-1092,0,\N,Missing
P16-5004,P15-1010,1,\N,Missing
P16-5004,P14-1044,0,\N,Missing
P16-5004,P15-2003,0,\N,Missing
P16-5004,P15-2001,1,\N,Missing
P16-5004,N15-1070,0,\N,Missing
P16-5004,N15-1164,0,\N,Missing
P16-5004,N16-1160,0,\N,Missing
P16-5004,D07-1107,0,\N,Missing
P16-5004,S16-1169,0,\N,Missing
P16-5004,W16-2508,1,\N,Missing
P16-5004,N16-1151,0,\N,Missing
P17-1170,R13-1022,0,0.0177951,"oth tasks, irrespective of the underlying sense inventory, i.e. WordNet or Wikipedia, which corroborates previous findings (Hovy et al., 2013; Flekova and Gurevych, 2016). This suggests that text classification does not require fine-grained semantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been prop"
P17-1170,baccianella-etal-2010-sentiwordnet,0,0.0519029,"Missing"
P17-1170,J15-2004,0,0.0193871,"ce of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie revi"
P17-1170,D15-1041,0,0.0141171,"s of their input. The word level functionality can affect the performance of these systems in two ways: (1) it can hamper their efficiency in handling words that are not encountered frequently during training, such as multiwords, inflections and derivations, and (2) it can restrict their semantic understanding to the level of words, with all their ambiguities, and thereby prevent accurate capture of the intended meanings. The first issue has recently been alleviated by techniques that aim to boost the generalisation power of NLP systems by resorting to sub-word or character-level information (Ballesteros et al., 2015; Kim et al., 2016). The second limitation, however, has not yet been studied sufficiently. A reasonable way to handle word ambiguity, and hence to tackle the second issue, is to semantify the input text: transform it from its surface-level semantics to the deeper level of word senses, i.e. their intended meanings. We take a step in this direction by designing a pipeline that enables seamless integration of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) a"
P17-1170,D16-1041,1,0.0551113,"a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential decay function (Equation 3) to compute new sense embeddings in the same semantic space. In order to represent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012). For the WordNet synsets which are mapped to Wikipedia pages in BabelNet, we average the corresponding Wikipediabased and WordNet-based sense embed"
P17-1170,W13-5003,0,0.0210547,"se distinctions can be beneficial to both tasks, irrespective of the underlying sense inventory, i.e. WordNet or Wikipedia, which corroborates previous findings (Hovy et al., 2013; Flekova and Gurevych, 2016). This suggests that text classification does not require fine-grained semantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representati"
P17-1170,N16-1163,0,0.0312474,"were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. C"
P17-1170,D14-1067,0,0.00713165,"onfigurations of the embedding layer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach exploits the semantic netw"
P17-1170,L16-1269,1,0.895177,"bedding of its corresponding word. Owing to its reliance on WordNet’s semantic network, DeConf is limited to generating only those word senses that are covered by this lexical resource. We propose to use Wikipedia in order to expand the vocabulary of the computed word senses. Wikipedia provides a high coverage of named entities and domain-specific terms in many languages, while at the same time also benefiting from a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential"
P17-1170,E17-2036,1,0.840387,"Missing"
P17-1170,D14-1110,0,0.0816486,"Missing"
P17-1170,D13-1184,0,0.0200958,"biguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses i"
P17-1170,W16-2501,0,0.00621644,"ved over the word-based model. This can be attributed to the quality of the representations, as the model utilizing them was unable to benefit from the advantage offered by sense distinctions. Our results suggest that research in sense representation should put special emphasis on real-world evaluations on benchmarks for downstream applications, rather than on artificial tasks such as word similarity. In fact, research has previously shown that word similarity might not constitute a reliable proxy to measure the performance of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the"
P17-1170,P16-1191,0,0.660744,"r many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. In our experiments we use these supersenses in order to reduce granularity of our WordNet and Wikipedia senses. To generate supersense embeddings, we simply average the embeddings of senses in the corresponding cluster. 5 Evaluation We evaluated our model on two classification tasks: topic categorization (Section 5.2) and polarity detection (Section 5.3). In the following section we present the common experimental setup. 5.1 Experimental set"
P17-1170,C14-1048,0,0.0139087,"al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmar"
P17-1170,P82-1020,0,0.792916,"Missing"
P17-1170,Q15-1023,0,0.0247776,"(ˆ s)} 11: return Disambiguation output Sˆ towards resolving ambiguities, but it brings about other advantages mentioned in the previous section. The aim is to provide the system with an input of reduced ambiguity which can facilitate its decision making. To this end, we developed a simple graph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input d"
P17-1170,P12-1092,0,0.0165482,"unami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has o"
P17-1170,P11-1015,0,0.0188444,"rk for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose values were below 0.4 as negative and above 0.6 as positive. This resulted in a binary polarity dataset of 119,783 phrases. Unlike the previous four datasets, this dataset does not contain an even"
P17-1170,P15-1010,1,0.869344,"he shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense embedding” pipeline to che"
P17-1170,N15-1070,0,0.0148647,"have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense em"
P17-1170,N15-1164,0,0.0284146,"he word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) propo"
P17-1170,N15-1011,0,0.00802726,"nses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 Proceedings of the 55th An"
P17-1170,P14-5010,0,0.00165544,"for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in fact, a small sub-graph of the input semantic network, N . Our algorithm then selects the best candidates iteratively. In each iteration, the 1 As defined in the underlying sense inventory, up to trigrams. We used Stanford CoreNLP (Manning et al., 2014) for tokenization, Part-of-Speech (PoS) tagging and lemmatization. 1858 Figure 2: Simplified graph-based representation of a sample sentence. Figure 3: Text classification model architecture. candidate sense that has the highest graph degree maxDeg is chosen as the winning sense: maxDeg = max |{(s, s0 ) ∈ E : s0 ∈ S}| s∈S (1) After each iteration, when a candidate sense sˆ is selected, all the possible candidate senses of the corresponding word (i.e. getLex(ˆ s)) are removed from E (line 10 in the algorithm). Figure 2 shows a simplified version of the graph for a sample sentence. The algorithm"
P17-1170,K16-1006,0,0.012425,"inking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in"
P17-1170,P14-1062,0,0.00336837,"nables seamless integration of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based mod"
P17-1170,D14-1181,0,0.123887,"of word senses into downstream NLP applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 P"
P17-1170,D15-1200,0,0.0215655,"14; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the problem of integrating sense representations into downstream NLP applications has remained understudied, despite the potential benefits it can have. Li and Jurafsky (2015) proposed a “multi-sense embedding” pipeline to check the benefit that can be gained by replacing word embeddings with sense embeddings in multiple tasks. With the help of two simple disambiguation algorithms, unsupervised sense embeddings were integrated into various downstream applications, with varying degrees of success. Given the interdependency of sense representation and disambiguation in this model, it is very difficult to introduce alternative algorithms into its pipeline, either to benefit from the state of the art, or to carry out an evaluation. Instead, our pipeline provides the ad"
P17-1170,P16-1096,1,0.782735,"can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges among different senses in S. The graph is, in fact, a small sub-graph of the i"
P17-1170,P06-1014,1,0.24475,"sent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012). For the WordNet synsets which are mapped to Wikipedia pages in BabelNet, we average the corresponding Wikipediabased and WordNet-based sense embeddings. 4.2 Pre-trained Supersense Embeddings It has been argued that WordNet sense distinctions are too fine-grained for many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. I"
P17-1170,D14-1113,0,0.0183534,"et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative"
P17-1170,P04-1035,0,0.00916293,"for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases"
P17-1170,P05-1015,0,0.141168,"sults in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification which determines if the sentiment of a given sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose val"
P17-1170,D14-1162,0,0.119517,"mantic distinctions. In this work we used a simple technique based on WordNet’s lexicographer files for coarsening senses in this sense inventory as well as in Wikipedia. We leave the exploration of this promising area as well as the evaluation of other granularity reduction techniques for WordNet (Snow et al., 2007; Bhagwani et al., 2013) and Wikipedia (Dandala et al., 2013) sense inventories to future work. 6 Related Work The past few years have witnessed a growing research interest in semantic representation, mainly as a consequence of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al."
P17-1170,D16-1174,1,0.850136,"Missing"
P17-1170,J14-4005,1,0.680885,"isambiguation output Sˆ towards resolving ambiguities, but it brings about other advantages mentioned in the previous section. The aim is to provide the system with an input of reduced ambiguity which can facilitate its decision making. To this end, we developed a simple graph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retriev"
P17-1170,D16-1018,0,0.0905951,"ifferent NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or artificial tasks, such as word similarity. Consequently, the pr"
P17-1170,E17-1010,1,0.0268406,"3.6 83.2 IMDB 87.7 87.4 PL05 77.3 76.6 PL04 67.9 67.4 Stanford 91.8 91.3 Wikipedia 83.1 88.0 75.9† 67.1 91.0 WordNet Wikipedia 84.4 83.1 88.0 88.4∗ 75.9 75.8 66.2 69.3∗ 91.4† 91.0 85.5 88.3 80.2 72.5 93.1 83.4 88.3 79.2 69.7† 92.6 Wikipedia 83.8 87.0† 79.2 73.1 92.3 WordNet 85.2 88.8 79.5 73.8 92.7† Wikipedia 84.2 87.9 78.3† 72.6 92.2 Word Pre-trained Sense Supersense WordNet Table 4: Accuracy performance on five polarity detection datasets. Given that polarity datasets are balanced17 , we do not report F1 which would have been identical to accuracy. texts is a known issue (Moro et al., 2014; Raganato et al., 2017), the tackling of which remains an area of exploration. spective of the classification task. We attribute this to two main factors: 1. Sparsity: Splitting a word into multiple word senses can have the negative side effect that the corresponding training data for that word is distributed among multiple independent senses. This reduces the training instances per word sense, which might affect the classifier’s performance, particularly when senses are semantically related (in comparison to fine-grained senses, supersenses address this issue to some extent). 2. Disambiguation quality: As also ment"
P17-1170,N10-1013,0,0.011305,"nce of the word embedding tsunami 1864 (Mikolov et al., 2013; Pennington et al., 2014). Soon after their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research"
P17-1170,P15-1173,0,0.0610169,"Missing"
P17-1170,P11-1097,0,0.160583,"Missing"
P17-1170,N15-1099,0,0.0142975,"these systems is still a barrier to the depth of their natural language understanding. Our proposal is particularly tailored towards addressing this issue. Multiword expressions (MWE). MWE are lexical units made up of two or more words which are idiosyncratic in nature (Sag et al., 2002), e.g, Lewis Hamilton, Nico Rosberg and Formula 1. Most existing word-based models ignore the interdependency between MWE’s subunits and treat them as individual units. Handling MWE has been a long-standing problem in NLP and has recently received a considerable amount of interest (Tsvetkov and Wintner, 2014; Salehi et al., 2015). Our pipeline facilitates this goal. Co-reference. Co-reference resolution of concepts and entities is not explicitly tackled by our approach. However, thanks to the fact that words that refer to the same meaning in context, e.g., Formula 1-F1 or German Grand Prix-German GPHockenheim, are all disambiguated to the same concept, the co-reference issue is also partly addressed by our pipeline. 3 Disambiguation Algorithm Our proposal relies on a seamless integration of word senses in word-based systems. The goal is to semantify the text prior to its being fed into the system by transforming its i"
P17-1170,J98-1004,0,0.448429,"Missing"
P17-1170,P13-2125,0,0.0526578,"re too fine-grained for many NLP applications (Hovy et al., 2013). The issue can be tackled by grouping together similar senses of the same word, either using automatic clustering techniques (Navigli, 2006; Agirre and Lopez, 2003; Snow et al., 2007) or with the help of WordNet’s lexicographer 3 We downloaded the salient words for Wikipedia pages (NASARI English lexical vectors, version 3.0) from http://lcl. uniroma1.it/nasari/ 4 We used the Java API from http://babelnet.org 1860 files5 . Various applications have been shown to improve upon moving from senses to supersenses (R¨ud et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016). In WordNet’s lexicographer files there are a total of 44 sense clusters, referred to as supersenses, for categories such as event, animal, and quantity. In our experiments we use these supersenses in order to reduce granularity of our WordNet and Wikipedia senses. To generate supersense embeddings, we simply average the embeddings of senses in the corresponding cluster. 5 Evaluation We evaluated our model on two classification tasks: topic categorization (Section 5.2) and polarity detection (Section 5.3). In the following section we present the common experimenta"
P17-1170,P13-1045,0,0.0139818,"sentence or document is negative or positive. 5.3.1 Datasets For the polarity detection task we used five standard evaluation datasets. Table 1 summarizes statistics. PL04 (Pang and Lee, 2004) is a polarity detection dataset composed of full movie reviews. PL0518 (Pang and Lee, 2005), instead, is composed of short snippets from movie reviews. RTC contains critic reviews from Rotten Tomatoes19 , divided into 436,000 training and 2,000 test instances. IMDB (Maas et al., 2011) includes 50,000 movie reviews, split evenly between training and test. Finally, we used the Stanford Sentiment dataset (Socher et al., 2013), which associates each review with a value that denotes its sentiment. To be consistent with the binary classification of the other datasets, we removed the neutral phrases according to the dataset’s scale (between 0.4 and 0.6) and considered the reviews whose values were below 0.4 as negative and above 0.6 as positive. This resulted in a binary polarity dataset of 119,783 phrases. Unlike the previous four datasets, this dataset does not contain an even distribution of positive and negative labels. 5.3.2 Results Table 4 lists accuracy performance of our classification model and all its varian"
P17-1170,N16-1160,0,0.0557682,"Missing"
P17-1170,D15-1167,0,0.00767,"applications, while benefiting from knowledge extracted from semantic networks. To this end, we propose a quick graph-based Word Sense Disambiguation (WSD) algorithm which allows high confidence disambiguation of words without much computation overload on the system. We evaluate the pipeline in two downstream NLP applications: polarity detection and topic categorization. Specifically, we use a classification model based on Convolutional Neural Networks which has been shown to be very effective in various text classification tasks (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016). We show that a simple disambiguation of input can lead to performance improvement of a state-of-the-art text classification system on multiple datasets, particularly for long inputs and when the granularity of the sense inventory is reduced. Our pipeline is quite flexible and modular, as it permits the integration of different WSD and sense representation techniques. 2 Motivation With the help of an example news article from the BBC, shown in Figure 1, we highlight some of the potential deficiencies of word-based models. 1857 Proceedings of the 55th Annual Meeting of the"
P17-1170,C14-1016,0,0.0219186,"fter their introduction, word embeddings were integrated into different NLP applications, thanks to the migration of the field to deep learning and the fact that most deep learning models view words as dense vectors. The waves of the word embedding tsunami have also lapped on the shores of sense representation. Several techniques have been proposed that either extend word embedding models to cluster contexts and induce senses, usually referred to as unsupervised sense representations (Sch¨utze, 1998; Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; ˇ Guo et al., 2014; Tian et al., 2014; Suster et al., 2016; Ettinger et al., 2016; Qiu et al., 2016) or exploit external sense inventories and lexical resources for generating sense representations for individual meanings of words (Chen et al., 2014; Johansson and Pina, 2015; Jauhar et al., 2015; Iacobacci et al., 2015; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2016b; Mancini et al., 2016; Pilehvar and Collier, 2016). However, the integration of sense representations into deep learning models has not been so straightforward, and research in this field has often opted for alternative evaluation benchmarks such as WSD, or"
P17-1170,J17-1002,0,0.0225265,"s. Wikipedia provides a high coverage of named entities and domain-specific terms in many languages, while at the same time also benefiting from a continuous update by collaborators. Moreover, it can easily be viewed as a sense inventory where individual articles are word senses arranged through hyperlinks and redirections. Camacho-Collados et al. (2016b) proposed NASARI3 , a technique to compute the most salient words for each Wikipedia page. These salient words were computed by exploiting the structure and content of Wikipedia and proved effective in tasks such as Word Sense Disambiguation (Tripodi and Pelillo, 2017; Camacho-Collados et al., 2016a), knowledge-base construction (Lieto et al., 2016), domain-adapted hypernym discovery (Espinosa-Anke et al., 2016; CamachoCollados and Navigli, 2017) or object recognition (Young et al., 2016). We view these lists as biasing words for individual Wikipedia pages, and then leverage the exponential decay function (Equation 3) to compute new sense embeddings in the same semantic space. In order to represent both WordNet and Wikipedia sense representations in the same space, we rely on the WordNetWikipedia mapping provided by BabelNet4 (Navigli and Ponzetto, 2012)."
P17-1170,D15-1243,0,0.0130565,"no improvement is observed over the word-based model. This can be attributed to the quality of the representations, as the model utilizing them was unable to benefit from the advantage offered by sense distinctions. Our results suggest that research in sense representation should put special emphasis on real-world evaluations on benchmarks for downstream applications, rather than on artificial tasks such as word similarity. In fact, research has previously shown that word similarity might not constitute a reliable proxy to measure the performance of word embeddings in downstream applications (Tsvetkov et al., 2015; Chiu et al., 2016). Among the three datasets, Ohsumed proves to be the most challenging one, mainly for its larger number of classes (i.e. 23) and its domain-specific nature (i.e. medicine). Interestingly, unlike for the other two datasets, the introduction of pre-trained word embeddings to the system results in reduced performance on Ohsumed. This suggests that general domain embeddings might not be beneficial 1862 5.3 Polarity Detection Polarity detection is the most popular evaluation framework for sentiment analysis (Dong et al., 2015). The task is essentially a binary classification whi"
P17-1170,J14-2007,0,0.0411995,"word-level functionality of these systems is still a barrier to the depth of their natural language understanding. Our proposal is particularly tailored towards addressing this issue. Multiword expressions (MWE). MWE are lexical units made up of two or more words which are idiosyncratic in nature (Sag et al., 2002), e.g, Lewis Hamilton, Nico Rosberg and Formula 1. Most existing word-based models ignore the interdependency between MWE’s subunits and treat them as individual units. Handling MWE has been a long-standing problem in NLP and has recently received a considerable amount of interest (Tsvetkov and Wintner, 2014; Salehi et al., 2015). Our pipeline facilitates this goal. Co-reference. Co-reference resolution of concepts and entities is not explicitly tackled by our approach. However, thanks to the fact that words that refer to the same meaning in context, e.g., Formula 1-F1 or German Grand Prix-German GPHockenheim, are all disambiguated to the same concept, the co-reference issue is also partly addressed by our pipeline. 3 Disambiguation Algorithm Our proposal relies on a seamless integration of word senses in word-based systems. The goal is to semantify the text prior to its being fed into the system"
P17-1170,P15-1032,0,0.018956,"ayer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach exploits the semantic network of WordNet (Miller, 1995),"
P17-1170,E17-1109,0,0.0294874,"Missing"
P17-1170,P10-4014,0,0.0202993,"ph-based joint disambiguation and entity linking algorithm which can take any arbitrary semantic network as input. The gist of our disambiguation technique lies in its speed and scalability. Conventional knowledge-based disambiguation systems (Hoffart et al., 2012; Agirre et al., 2014; Moro et al., 2014; Ling et al., 2015; Pilehvar and Navigli, 2014) often rely on computationally expensive graph algorithms, which limits their application to on-the-fly processing of large number of text documents, as is the case in our experiments. Moreover, unlike supervised WSD and entity linking techniques (Zhong and Ng, 2010; Cheng and Roth, 2013; Melamud et al., 2016; Limsopatham and Collier, 2016), our algorithm relies only on semantic networks and does not require any senseannotated data, which is limited to English and almost non-existent for other languages. Algorithm 1 shows our procedure for disambiguating an input document T . First, we retrieve from our semantic network the list of candidate senses1 for each content word, as well as semantic relationships among them. As a result, we obtain a graph representation (S, E) of the input text, where S is the set of candidate senses and E is the set of edges am"
P17-1170,D13-1141,0,0.0158915,"riments with two configurations of the embedding layer: (1) Random, initialized randomly and updated during training, and (2) Pre-trained, initialized by pre-trained representations and updated during training. In the following section we describe the pre-trained word and sense representation used for the initialization of the second configuration. 4.1 Pre-trained Word and Sense Embeddings One of the main advantages of neural models is that they usually represent the input words as dense vectors. This can significantly boost a system’s generalisation power and results in improved performance (Zou et al., 2013; Bordes et al., 2014; Kim, 2014; Weiss et al., 2015, interalia). This feature also enables us to directly plug in pre-trained sense representations and check them in a downstream application. In our experiments we generate a set of sense embeddings by extending DeConf, a recent technique with state-of-the-art performance on multiple semantic similarity benchmarks (Pilehvar and Collier, 2016). We leave the evaluation of other representations to future work. DeConf gets a pre-trained set of word embeddings and computes sense embeddings in the same semantic space. To this end, the approach explo"
P17-1170,D07-1107,0,\N,Missing
P17-1170,K17-1012,1,\N,Missing
P17-2094,S13-2032,0,0.0227148,"however, the rapid development of NLP pipelines for languages other than English has been opening up the possibilities for the automatic generation of multilingual sense-annotated data. Nevertheless, the few approaches that have been proposed so far are either focused on treating each individual language in isolation (Otegi et al., 2016), or limited to short and concise definitional text (Camacho-Collados et al., 2016a). On the other hand, the use of parallel text to perform WSD (Ng et al., 2003; Lefever et al., 2011; Yao et al., 2012; Bonansinga and Bond, 2016) or even Word Sense Induction (Apidianaki, 2013) has been widely explored in the literature, and has demonstrated its effectiveness in producing high-quality sense-annotated data (Chan and Ng, 2005). This strategy, however, requires word alignments for each language pair to be taken into account, with alignment errors that might propagate and hamper subsequent stages unless human supervision is employed to correct erroneous annotations (Taghipour and Ng, 2015a). Moreover, cross-language disambiguation using parallel text requires a language-independent annotation framework that goes beyond monolingual WordNet-like sense inventories (Lefever"
P17-2094,P15-1010,1,0.854457,"ctically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks. In this paper we focus on Europarl (Koehn, 2005)1 , one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems. Extracted from the proceedings of the European Parliament, the latest release of the Europarl corpus compris"
P17-2094,D16-1250,0,0.0741,"Missing"
P17-2094,P16-1085,1,0.780596,"endent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for Word Sense Disambiguation. 1 Introduction One of the long-standing challenges in Natural Language Processing (NLP) lies in automatically identifying the meaning of words in context. Various lines of research have been geared towards achieving this goal, most notably Word Sense Disambiguation (Navigli, 2009, WSD) and Entity Linking (Rao et al., 2013, EL). In both tasks, supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016) tend to obtain the best performances over standard benchmarks but, from a practical standpoint, they lose ground to knowledge-based approaches (Agirre et al., 2014; Moro et al., 2014b; Weissenborn et al., 2015), which scale better in terms of scope and number of languages. In fact, the development of supervised disambiguation systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada,"
P17-2094,2016.gwc-1.8,0,0.0310773,"gest corpus of its kind. 2 Moro et al., 2014b). Nowadays, however, the rapid development of NLP pipelines for languages other than English has been opening up the possibilities for the automatic generation of multilingual sense-annotated data. Nevertheless, the few approaches that have been proposed so far are either focused on treating each individual language in isolation (Otegi et al., 2016), or limited to short and concise definitional text (Camacho-Collados et al., 2016a). On the other hand, the use of parallel text to perform WSD (Ng et al., 2003; Lefever et al., 2011; Yao et al., 2012; Bonansinga and Bond, 2016) or even Word Sense Induction (Apidianaki, 2013) has been widely explored in the literature, and has demonstrated its effectiveness in producing high-quality sense-annotated data (Chan and Ng, 2005). This strategy, however, requires word alignments for each language pair to be taken into account, with alignment errors that might propagate and hamper subsequent stages unless human supervision is employed to correct erroneous annotations (Taghipour and Ng, 2015a). Moreover, cross-language disambiguation using parallel text requires a language-independent annotation framework that goes beyond mon"
P17-2094,W16-5307,0,0.0699806,"Missing"
P17-2094,2005.mtsummit-papers.11,0,0.113113,"een shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks. In this paper we focus on Europarl (Koehn, 2005)1 , one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems. Extracted from the proceedings of the European Parliament, the latest release of the Europarl corpus comprises parallel text for 21 European languages, with more than 743 million tokens overall. Apart from its prominent role in MT as a training set, the Europarl corpus has been used Parallel corpora are widely used in a variety of Natural Language Processing tasks, from Machine Translation to cross-lingual Word Sense Disambiguation, where parallel sentenc"
P17-2094,L16-1269,1,0.802688,"ual sense inventory of BabelNet, and covering all the 21 languages of the Europarl corpus. As such E URO S ENSE constitutes, to our knowledge, the largest corpus of its kind. 2 Moro et al., 2014b). Nowadays, however, the rapid development of NLP pipelines for languages other than English has been opening up the possibilities for the automatic generation of multilingual sense-annotated data. Nevertheless, the few approaches that have been proposed so far are either focused on treating each individual language in isolation (Otegi et al., 2016), or limited to short and concise definitional text (Camacho-Collados et al., 2016a). On the other hand, the use of parallel text to perform WSD (Ng et al., 2003; Lefever et al., 2011; Yao et al., 2012; Bonansinga and Bond, 2016) or even Word Sense Induction (Apidianaki, 2013) has been widely explored in the literature, and has demonstrated its effectiveness in producing high-quality sense-annotated data (Chan and Ng, 2005). This strategy, however, requires word alignments for each language pair to be taken into account, with alignment errors that might propagate and hamper subsequent stages unless human supervision is employed to correct erroneous annotations (Taghipour an"
P17-2094,S13-2029,0,0.104183,"Missing"
P17-2094,P11-2055,0,0.0381769,"Missing"
P17-2094,K16-1006,0,0.0768567,"s from a languageindependent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for Word Sense Disambiguation. 1 Introduction One of the long-standing challenges in Natural Language Processing (NLP) lies in automatically identifying the meaning of words in context. Various lines of research have been geared towards achieving this goal, most notably Word Sense Disambiguation (Navigli, 2009, WSD) and Entity Linking (Rao et al., 2013, EL). In both tasks, supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016) tend to obtain the best performances over standard benchmarks but, from a practical standpoint, they lose ground to knowledge-based approaches (Agirre et al., 2014; Moro et al., 2014b; Weissenborn et al., 2015), which scale better in terms of scope and number of languages. In fact, the development of supervised disambiguation systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–6"
P17-2094,D15-1131,0,0.0223863,"n systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2094 for cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2016), and widely exploited to develop cross-lingual word embeddings (Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Vyas and Carpuat, 2016; Vuli´c and Korhonen, 2016; Artetxe et al., 2016) as well as multi-sense embedˇ dings (Ettinger et al., 2016; Suster et al., 2016). In this paper, our aim is to augment Europarl with sense-level information for multiple languages, thereby constructing a large-scale senseannotated multilingual corpus which has the potential to boost both WSD and MT research. We follow an approach that has already proved effective in a definitional setting (CamachoCollados et al., 2016a): unlike previous crosslingual approaches, we do not rely on word alignments against a pivot language,"
P17-2094,H93-1061,0,0.940054,"ec. 67.5 Cov. 100 E URO S ENSE (full) E URO S ENSE (refined) 80.3 81.5 67.9 71.8 100 63.5 84.6 89.3 76.7 82.5 100 62.9 100 75.0 FR 100 53.8 ES Table 2: Precision (Prec.) and coverage (Cov.) of E URO S ENSE, manually evaluated on a random sample in 4 languages. Precision is averaged between the two judges, and coverage is computed assuming each content word in the sense inventory to be a valid disambiguation target. notations for English as a training set for a supervised all-words WSD system, It Makes Sense (Zhong and Ng, 2010, IMS). Following Taghipour and Ng (2015a), we started with SemCor (Miller et al., 1993) as initial training dataset, and then performed a subsampling of E URO S ENSE up to 500 additional training examples per word sense. We then trained IMS on this augmented training set and tested on the two most recent standard benchmarks for all-words WSD: the SemEval2013 task 12 (Navigli et al., 2013) and the SemEval-2015 task 13 (Moro and Navigli, 2015) test sets. As baselines we considered IMS trained on SemCor only and OMSTI, the sense-annotated dataset constructed by Taghipour and Ng (2015a) which also includes SemCor. Finally, we report the results of UKB, a knowledge-based system (Agir"
P17-2094,N16-1163,0,0.0217942,"ing of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2094 for cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2016), and widely exploited to develop cross-lingual word embeddings (Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Vyas and Carpuat, 2016; Vuli´c and Korhonen, 2016; Artetxe et al., 2016) as well as multi-sense embedˇ dings (Ettinger et al., 2016; Suster et al., 2016). In this paper, our aim is to augment Europarl with sense-level information for multiple languages, thereby constructing a large-scale senseannotated multilingual corpus which has the potential to boost both WSD and MT research. We follow an approach that has already proved effective in a definitional setting (CamachoCollados et al., 2016a): unlike previous crosslingual approaches, we do not rely on word alignments against a pivot language, but instead leverage all languages at the same time in a joint disambiguation procedure that is subsequently refined using distribut"
P17-2094,moro-etal-2014-annotating,1,0.912363,".it Abstract liable sense-annotated corpora, which are indispensable in order to provide solid training and testing grounds (Pilehvar and Navigli, 2014). However, hand-labeled sense annotations are notoriously difficult to obtain on a large scale, and manually curated corpora (Miller et al., 1993; Passonneau et al., 2012) have a limited size. Given that scaling the manual annotation process becomes practically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist i"
P17-2094,P16-1191,0,0.0741367,"both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks. In this paper we focus on Europarl (Koehn, 2005)1 , one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems. Extracted from the proceedings of the European Parliament, the latest release of the Europarl corpus comprises parallel text for 21 Europ"
P17-2094,Q14-1019,1,0.937008,".it Abstract liable sense-annotated corpora, which are indispensable in order to provide solid training and testing grounds (Pilehvar and Navigli, 2014). However, hand-labeled sense annotations are notoriously difficult to obtain on a large scale, and manually curated corpora (Miller et al., 1993; Passonneau et al., 2012) have a limited size. Given that scaling the manual annotation process becomes practically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist i"
P17-2094,C16-1256,0,0.0209036,"senborn et al., 2015), which scale better in terms of scope and number of languages. In fact, the development of supervised disambiguation systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2094 for cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2016), and widely exploited to develop cross-lingual word embeddings (Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Vyas and Carpuat, 2016; Vuli´c and Korhonen, 2016; Artetxe et al., 2016) as well as multi-sense embedˇ dings (Ettinger et al., 2016; Suster et al., 2016). In this paper, our aim is to augment Europarl with sense-level information for multiple languages, thereby constructing a large-scale senseannotated multilingual corpus which has the potential to boost both WSD and MT research. We follow an approach that has already proved effective in a definitional setting"
P17-2094,N16-1160,0,0.0567553,"Missing"
P17-2094,K15-1037,0,0.42081,"sense-annotated corpora, which are indispensable in order to provide solid training and testing grounds (Pilehvar and Navigli, 2014). However, hand-labeled sense annotations are notoriously difficult to obtain on a large scale, and manually curated corpora (Miller et al., 1993; Passonneau et al., 2012) have a limited size. Given that scaling the manual annotation process becomes practically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedeman"
P17-2094,P03-1058,0,0.0943459,"such E URO S ENSE constitutes, to our knowledge, the largest corpus of its kind. 2 Moro et al., 2014b). Nowadays, however, the rapid development of NLP pipelines for languages other than English has been opening up the possibilities for the automatic generation of multilingual sense-annotated data. Nevertheless, the few approaches that have been proposed so far are either focused on treating each individual language in isolation (Otegi et al., 2016), or limited to short and concise definitional text (Camacho-Collados et al., 2016a). On the other hand, the use of parallel text to perform WSD (Ng et al., 2003; Lefever et al., 2011; Yao et al., 2012; Bonansinga and Bond, 2016) or even Word Sense Induction (Apidianaki, 2013) has been widely explored in the literature, and has demonstrated its effectiveness in producing high-quality sense-annotated data (Chan and Ng, 2005). This strategy, however, requires word alignments for each language pair to be taken into account, with alignment errors that might propagate and hamper subsequent stages unless human supervision is employed to correct erroneous annotations (Taghipour and Ng, 2015a). Moreover, cross-language disambiguation using parallel text requi"
P17-2094,tiedemann-2012-parallel,0,0.0448825,"Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks. In this paper we focus on Europarl (Koehn, 2005)1 , one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems. Extracted from the proceedings of the European Parliament, the latest release of the Europarl corpus comprises parallel text for 21 European languages, with more than 743 million tokens overall. Apart from its prominent role in MT as a training set, the Europarl corpus has been used Parallel corpora are widely used in a variety o"
P17-2094,P16-1024,0,0.0396579,"Missing"
P17-2094,N16-1142,0,0.0252696,"lly on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2094 for cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2016), and widely exploited to develop cross-lingual word embeddings (Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Vyas and Carpuat, 2016; Vuli´c and Korhonen, 2016; Artetxe et al., 2016) as well as multi-sense embedˇ dings (Ettinger et al., 2016; Suster et al., 2016). In this paper, our aim is to augment Europarl with sense-level information for multiple languages, thereby constructing a large-scale senseannotated multilingual corpus which has the potential to boost both WSD and MT research. We follow an approach that has already proved effective in a definitional setting (CamachoCollados et al., 2016a): unlike previous crosslingual approaches, we do not rely on word alignments against a pivot language, but instead leverage al"
P17-2094,passonneau-etal-2012-masc,0,0.0553585,"uage-independent sense annotations for a wide variety of concepts and named entities, which can be seamlessly mapped to individual semantic resources (e.g WordNet, Wikipedia, DBpedia) via BabelNet’s inter-resource mappings. Related Work Extending sense annotations to multiple languages is a demanding endeavor, especially when manual intervention is required. Despite the fact that sense-annotated corpora for a number of languages have been around for more than a decade (Petrolito and Bond, 2014), they either include few samples per word sense, or only cover a restricted set of ambiguous words (Passonneau et al., 2012); as a result, multilingual WSD was until recently almost exclusively tackled using knowledge-based approaches (Agirre et al., 2014; 2 3 Building E URO S ENSE Following Camacho-Collados et al. (2016a), our fully automatic disambiguation pipeline for constructing E URO S ENSE couples a graph-based multilingual joint WSD/EL system, Babelfy (Moro et al., 2014b)3 , and a language-independent vector representation of concepts and entities, NASARI (Camacho-Collados et al., 2016b).4 It comprises two stages: multilingual disambigua3 4 http://babelnet.org 595 http://babelfy.org http://lcl.uniroma1.it/n"
P17-2094,P15-1058,0,0.0318192,"Natural Language Processing (NLP) lies in automatically identifying the meaning of words in context. Various lines of research have been geared towards achieving this goal, most notably Word Sense Disambiguation (Navigli, 2009, WSD) and Entity Linking (Rao et al., 2013, EL). In both tasks, supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016) tend to obtain the best performances over standard benchmarks but, from a practical standpoint, they lose ground to knowledge-based approaches (Agirre et al., 2014; Moro et al., 2014b; Weissenborn et al., 2015), which scale better in terms of scope and number of languages. In fact, the development of supervised disambiguation systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2094 for cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2"
P17-2094,W14-0132,0,0.0559787,"riched context for a joint multilingual disambiguation. Using BabelNet, a unified multilingual sense inventory, we obtain language-independent sense annotations for a wide variety of concepts and named entities, which can be seamlessly mapped to individual semantic resources (e.g WordNet, Wikipedia, DBpedia) via BabelNet’s inter-resource mappings. Related Work Extending sense annotations to multiple languages is a demanding endeavor, especially when manual intervention is required. Despite the fact that sense-annotated corpora for a number of languages have been around for more than a decade (Petrolito and Bond, 2014), they either include few samples per word sense, or only cover a restricted set of ambiguous words (Passonneau et al., 2012); as a result, multilingual WSD was until recently almost exclusively tackled using knowledge-based approaches (Agirre et al., 2014; 2 3 Building E URO S ENSE Following Camacho-Collados et al. (2016a), our fully automatic disambiguation pipeline for constructing E URO S ENSE couples a graph-based multilingual joint WSD/EL system, Babelfy (Moro et al., 2014b)3 , and a language-independent vector representation of concepts and entities, NASARI (Camacho-Collados et al., 201"
P17-2094,N12-1078,0,0.0564615,"Missing"
P17-2094,J14-4005,1,0.914964,"Missing"
P17-2094,P10-4014,0,0.811237,"concepts and entities from a languageindependent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for Word Sense Disambiguation. 1 Introduction One of the long-standing challenges in Natural Language Processing (NLP) lies in automatically identifying the meaning of words in context. Various lines of research have been geared towards achieving this goal, most notably Word Sense Disambiguation (Navigli, 2009, WSD) and Entity Linking (Rao et al., 2013, EL). In both tasks, supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; K˚ageb¨ack and Salomonsson, 2016) tend to obtain the best performances over standard benchmarks but, from a practical standpoint, they lose ground to knowledge-based approaches (Agirre et al., 2014; Moro et al., 2014b; Weissenborn et al., 2015), which scale better in terms of scope and number of languages. In fact, the development of supervised disambiguation systems depends crucially on the availability of re1 http://opus.lingfil.uu.se/Europarl. php 594 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Shor"
P17-2094,E17-1010,1,0.798174,"12) have a limited size. Given that scaling the manual annotation process becomes practically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016). Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016). A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks. In this paper we focus on Europarl (Koehn, 2005)1 , one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems. Extracted from the proc"
P17-2094,S13-2040,1,\N,Missing
P19-1318,P14-1023,0,0.0380215,"e-enhanced embeddings of Retrofitting and Attract-Repel in the BLESS dataset. For DiffVec, let us recall that both these approaches have the unfair advantage of having had WordNet as source knowledge base, used both to construct the test set and to enhance the word embeddings. In general, the improvement of RWE over standard word embeddings suggests that our vectors capture relations in a way that is compatible to standard word vectors (which will be further discussed in Section 6.2). 5.2 Lexical Feature Modelling Standard word embedding models tend to capture semantic similarity rather well (Baroni et al., 2014; Levy et al., 2015a). However, even though other kinds of lexical properties may also be encoded (Gupta et al., 2015), they are not explicitly modeled. Based on the hypothesis that relational word embeddings should allow us to model such properties in a more consistent and transparent fashion, we select the well-known McRae Feature Norms benchmark (McRae et al., 2005) as testbed. This dataset10 is composed of 541 words (or concepts), each of them associated with one or more features. For example, ‘a bear is an animal’, or ‘a bowl is round’. As for the specifics of our evaluation, given that s"
P19-1318,W11-2501,0,0.0356427,"e use of WordNet (Fellbaum, 1998) as the input knowledge base, while for Attract-Repel we use the default configuration with all constraints from PPDB (Pavlick et al., 2015), WordNet and BabelNet (Navigli and Ponzetto, 2012). All comparison systems are 300-dimensional and trained on the same Wikipedia corpus. 5.1 Relation Classification Given a pre-defined set of relation types and a pair of words, the relation classification task consists in selecting the relation type that best describes the relationship between the two words. As test sets we used DiffVec (Vylomova et al., 2016) and BLESS8 (Baroni and Lenci, 2011). The DiffVec dataset includes 12,458 word pairs, covering fifteen relation types including hypernymy, causepurpose or verb-noun derivations. On the other hand, BLESS includes semantic relations such as hypernymy, meronymy, and co-hyponymy.9 BLESS includes a train-test partition, with 13,258 and 6,629 word pairs, respectively. This task is treated as a multi-class classification problem As a baseline model (Diff), we consider the usual representation of word pairs in terms of their vector differences (Fu et al., 2014; Roller et al., 7 We used the pre-trained model of its official repository. h"
P19-1318,Q17-1010,0,0.133474,"Missing"
P19-1318,C18-1138,1,0.848851,"t cinema”, which is how ConceptNet (Speer et al., 2017), for example, encodes this relationship1 . In fact, regardless of how a word embedding is learned, if its primary aim is to capture similarity, there are inherent limitations on the kinds of relations they can capture. For instance, such word embeddings can only encode similarity preserving relations (i.e. similar entities have to be related to similar entities) and it is often difficult to encode that w is in a particular relationship while preventing the inference that words with similar vectors to w are also in this relationship; e.g. Bouraoui et al. (2018) found that both (Berlin,Germany) and (Moscow,Germany) were predicted to be instances of the capital-of relation due to the similarity of the word vectors for Berlin and Moscow. Furthermore, while the ability to capture word analogies (e.g. king-man+woman≈queen) emerged as a successful illustration of how word embeddings can encode some types of relational information (Mikolov et al., 2013b), the generalization of this interesting property has proven to be less successful than initially anticipated (Levy et al., 2014; 1 http://conceptnet.io/c/en/popcorn 3286 Proceedings of the 57th Annual Meet"
P19-1318,C18-1225,1,0.877546,"Missing"
P19-1318,R15-1026,0,0.0282743,"s infrequent (and thus typically less informative) words. Learning Relation Vectors. In this paper, we will rely on word vector averaging for learning relation vectors, which has the advantage of being much faster than other existing approaches, and thus allows us to consider a higher number of word pairs (or a larger corpus) within a fixed 3 A co-occurrence in which there are k words in between 1 w and v then receives a weight of k+1 . 3288 time budget. Word vector averaging has moreover proven surprisingly effective for learning relation vectors (Weston et al., 2013; Hashimoto et al., 2015; Fan et al., 2015; Espinosa Anke and Schockaert, 2018), as well as in related tasks such as sentence embedding (Wieting et al., 2016). Specifically, to construct the relation vector rwv capturing the relationship between the words w and v we proceed as follows. First, we compute a bag of words representation {(w1 , f1 ), ..., (wn , fn )}, where fi is the number of times the word wi occurs in between the words w and v in any given sentence in the corpus. The relation vector rwv is then essentially computed as a weighted average: ! n X rwv = norm fi · wi (2) i=1 where we write wi for the vector representation of"
P19-1318,N15-1184,0,0.480627,"For example, some authors have proposed models which combine the loss function of a word embedding model, to ensure that word vectors are predictive of their context words, with the loss function of a knowledge graph embedding model, to encourage the word vectors to additionally be predictive of a given set of relational facts (Xu et al., 2014; Celikyilmaz et al., 2015; Chen et al., 2016). Other authors have used knowledge bases in a more restricted way, by taking the fact that two words are linked to each other in a given knowledge graph as evidence that their word vectors should be similar (Faruqui et al., 2015; Speer et al., 2017). Finally, there has also been work that uses lexicons to learn word embeddings which are specialized towards certain types of lexical knowledge, such as hypernymy (Nguyen et al., 2017; Vulic and Mrksic, 2018), antonymy (Liu et al., 2015; Ono et al., 2015) or a combination of various linguistic constraints (Mrkˇsi´c et al., 2017). Our method differs in two important ways from these existing approaches. First, rather than relying on an external knowledge base, or other forms of supervision, as in e.g. (Chen et al., 2016), our method is completely unsupervised, as our only i"
P19-1318,P14-1113,0,0.031746,"rds. As test sets we used DiffVec (Vylomova et al., 2016) and BLESS8 (Baroni and Lenci, 2011). The DiffVec dataset includes 12,458 word pairs, covering fifteen relation types including hypernymy, causepurpose or verb-noun derivations. On the other hand, BLESS includes semantic relations such as hypernymy, meronymy, and co-hyponymy.9 BLESS includes a train-test partition, with 13,258 and 6,629 word pairs, respectively. This task is treated as a multi-class classification problem As a baseline model (Diff), we consider the usual representation of word pairs in terms of their vector differences (Fu et al., 2014; Roller et al., 7 We used the pre-trained model of its official repository. http://clic.cimec.unitn.it/distsem 9 Note that both datasets exhibit overlap in a number of relations as some instances from DiffVec were taken from BLESS. 3290 8 Encoding Pair2Vec FastText Retrofitting† Attract-Repel† Pair2Vec FastText (This paper) (Joshi et al., 2019) (Bojanowski et al., 2017) (Faruqui et al., 2015) (Mrkˇsi´c et al., 2017) (Joshi et al., 2019) (Bojanowski et al., 2017) Acc. 85.3 85.0 84.2 86.1* 86.0* 84.8 84.3 DiffVec F1 Prec. 64.2 65.1 64.0 65.0 61.4 62.6 64.6* 66.6* 64.6* 66.0* 64.1 65.7 61.3 62.4"
P19-1318,D15-1002,0,0.0457005,"Missing"
P19-1318,K15-1027,0,0.0118251,"being less biased towards infrequent (and thus typically less informative) words. Learning Relation Vectors. In this paper, we will rely on word vector averaging for learning relation vectors, which has the advantage of being much faster than other existing approaches, and thus allows us to consider a higher number of word pairs (or a larger corpus) within a fixed 3 A co-occurrence in which there are k words in between 1 w and v then receives a weight of k+1 . 3288 time budget. Word vector averaging has moreover proven surprisingly effective for learning relation vectors (Weston et al., 2013; Hashimoto et al., 2015; Fan et al., 2015; Espinosa Anke and Schockaert, 2018), as well as in related tasks such as sentence embedding (Wieting et al., 2016). Specifically, to construct the relation vector rwv capturing the relationship between the words w and v we proceed as follows. First, we compute a bag of words representation {(w1 , f1 ), ..., (wn , fn )}, where fi is the number of times the word wi occurs in between the words w and v in any given sentence in the corpus. The relation vector rwv is then essentially computed as a weighted average: ! n X rwv = norm fi · wi (2) i=1 where we write wi for the vector"
P19-1318,P18-1003,1,0.85038,"of the Association for Computational Linguistics, pages 3286–3296 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Linzen, 2016; Rogers et al., 2017). This suggests that relational information has to be encoded separately from standard similaritycentric word embeddings. One appealing strategy is to represent relational information by learning, for each pair of related words, a vector that encodes how the words are related. This strategy was first adopted by Turney (2005), and has recently been revisited by a number of authors (Washio and Kato, 2018a; Jameel et al., 2018; Espinosa Anke and Schockaert, 2018; Washio and Kato, 2018b; Joshi et al., 2019). However, in many applications, word vectors are easier to deal with than vector representations of word pairs. The research question we consider in this paper is whether it is possible to learn word vectors that capture relational information. Our aim is for such relational word vectors to be complementary to standard word vectors. To make relational information available to NLP models, it then suffices to use a standard architecture and replace normal word vectors by concatenations of standard and relational wo"
P19-1318,N19-1362,0,0.115569,"ly, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Linzen, 2016; Rogers et al., 2017). This suggests that relational information has to be encoded separately from standard similaritycentric word embeddings. One appealing strategy is to represent relational information by learning, for each pair of related words, a vector that encodes how the words are related. This strategy was first adopted by Turney (2005), and has recently been revisited by a number of authors (Washio and Kato, 2018a; Jameel et al., 2018; Espinosa Anke and Schockaert, 2018; Washio and Kato, 2018b; Joshi et al., 2019). However, in many applications, word vectors are easier to deal with than vector representations of word pairs. The research question we consider in this paper is whether it is possible to learn word vectors that capture relational information. Our aim is for such relational word vectors to be complementary to standard word vectors. To make relational information available to NLP models, it then suffices to use a standard architecture and replace normal word vectors by concatenations of standard and relational word vectors. In particular, we show that such relational word vectors can be learn"
P19-1318,Q15-1016,0,0.432399,"ings, our aim is to learn vector representations that are complementary to standard word embeddings. 3 Selecting Related Word Pairs. Starting from a vocabulary V containing the words of interest (e.g. all sufficiently frequent words), as a first step we need to choose a set R ⊆ V × V of potentially related words. For each of the word pairs in R we will then learn a relation vector, as explained below. To select this set R, we only consider word pairs that co-occur in the same sentence in a given reference corpus. For all such word pairs, we then compute their strength of relatedness following Levy et al. (2015a) by using a smoothed version of pointwise mutual information (PMI), where we use 0.5 as exponent factor. In particular, for each word w ∈ V, the set R contains all sufficiently frequently co-occurring pairs (w, v) for which v is within the top-100 most closely related words to w, according to the following score:   nwv · s∗∗ PMI0.5 (u, v) = log (1) nw∗ · sv∗ Model Description We aim to learn representations that are complementary to standard word vectors and are specialized towards relational knowledge. To differentiate them from standard word vectors, they will be referred to as relationa"
P19-1318,W14-1618,0,0.126612,"that words with similar vectors to w are also in this relationship; e.g. Bouraoui et al. (2018) found that both (Berlin,Germany) and (Moscow,Germany) were predicted to be instances of the capital-of relation due to the similarity of the word vectors for Berlin and Moscow. Furthermore, while the ability to capture word analogies (e.g. king-man+woman≈queen) emerged as a successful illustration of how word embeddings can encode some types of relational information (Mikolov et al., 2013b), the generalization of this interesting property has proven to be less successful than initially anticipated (Levy et al., 2014; 1 http://conceptnet.io/c/en/popcorn 3286 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3286–3296 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Linzen, 2016; Rogers et al., 2017). This suggests that relational information has to be encoded separately from standard similaritycentric word embeddings. One appealing strategy is to represent relational information by learning, for each pair of related words, a vector that encodes how the words are related. This strategy was first adopted by Turney (2005)"
P19-1318,N15-1098,0,0.358673,"ings, our aim is to learn vector representations that are complementary to standard word embeddings. 3 Selecting Related Word Pairs. Starting from a vocabulary V containing the words of interest (e.g. all sufficiently frequent words), as a first step we need to choose a set R ⊆ V × V of potentially related words. For each of the word pairs in R we will then learn a relation vector, as explained below. To select this set R, we only consider word pairs that co-occur in the same sentence in a given reference corpus. For all such word pairs, we then compute their strength of relatedness following Levy et al. (2015a) by using a smoothed version of pointwise mutual information (PMI), where we use 0.5 as exponent factor. In particular, for each word w ∈ V, the set R contains all sufficiently frequently co-occurring pairs (w, v) for which v is within the top-100 most closely related words to w, according to the following score:   nwv · s∗∗ PMI0.5 (u, v) = log (1) nw∗ · sv∗ Model Description We aim to learn representations that are complementary to standard word vectors and are specialized towards relational knowledge. To differentiate them from standard word vectors, they will be referred to as relationa"
P19-1318,W16-2503,0,0.0147992,"lin and Moscow. Furthermore, while the ability to capture word analogies (e.g. king-man+woman≈queen) emerged as a successful illustration of how word embeddings can encode some types of relational information (Mikolov et al., 2013b), the generalization of this interesting property has proven to be less successful than initially anticipated (Levy et al., 2014; 1 http://conceptnet.io/c/en/popcorn 3286 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3286–3296 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Linzen, 2016; Rogers et al., 2017). This suggests that relational information has to be encoded separately from standard similaritycentric word embeddings. One appealing strategy is to represent relational information by learning, for each pair of related words, a vector that encodes how the words are related. This strategy was first adopted by Turney (2005), and has recently been revisited by a number of authors (Washio and Kato, 2018a; Jameel et al., 2018; Espinosa Anke and Schockaert, 2018; Washio and Kato, 2018b; Joshi et al., 2019). However, in many applications, word vectors are easier to deal with"
P19-1318,P15-1145,0,0.0225567,"s to additionally be predictive of a given set of relational facts (Xu et al., 2014; Celikyilmaz et al., 2015; Chen et al., 2016). Other authors have used knowledge bases in a more restricted way, by taking the fact that two words are linked to each other in a given knowledge graph as evidence that their word vectors should be similar (Faruqui et al., 2015; Speer et al., 2017). Finally, there has also been work that uses lexicons to learn word embeddings which are specialized towards certain types of lexical knowledge, such as hypernymy (Nguyen et al., 2017; Vulic and Mrksic, 2018), antonymy (Liu et al., 2015; Ono et al., 2015) or a combination of various linguistic constraints (Mrkˇsi´c et al., 2017). Our method differs in two important ways from these existing approaches. First, rather than relying on an external knowledge base, or other forms of supervision, as in e.g. (Chen et al., 2016), our method is completely unsupervised, as our only input consists of a text corpus. Second, whereas existing work has focused on methods for improving word embeddings, our aim is to learn vector representations that are complementary to standard word embeddings. 3 Selecting Related Word Pairs. Starting from a"
P19-1318,N13-1090,0,0.0705916,"lated to similar entities) and it is often difficult to encode that w is in a particular relationship while preventing the inference that words with similar vectors to w are also in this relationship; e.g. Bouraoui et al. (2018) found that both (Berlin,Germany) and (Moscow,Germany) were predicted to be instances of the capital-of relation due to the similarity of the word vectors for Berlin and Moscow. Furthermore, while the ability to capture word analogies (e.g. king-man+woman≈queen) emerged as a successful illustration of how word embeddings can encode some types of relational information (Mikolov et al., 2013b), the generalization of this interesting property has proven to be less successful than initially anticipated (Levy et al., 2014; 1 http://conceptnet.io/c/en/popcorn 3286 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3286–3296 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Linzen, 2016; Rogers et al., 2017). This suggests that relational information has to be encoded separately from standard similaritycentric word embeddings. One appealing strategy is to represent relational information by learning,"
P19-1318,Q17-1022,0,0.110163,"Missing"
P19-1318,N15-1100,0,0.0247899,"be predictive of a given set of relational facts (Xu et al., 2014; Celikyilmaz et al., 2015; Chen et al., 2016). Other authors have used knowledge bases in a more restricted way, by taking the fact that two words are linked to each other in a given knowledge graph as evidence that their word vectors should be similar (Faruqui et al., 2015; Speer et al., 2017). Finally, there has also been work that uses lexicons to learn word embeddings which are specialized towards certain types of lexical knowledge, such as hypernymy (Nguyen et al., 2017; Vulic and Mrksic, 2018), antonymy (Liu et al., 2015; Ono et al., 2015) or a combination of various linguistic constraints (Mrkˇsi´c et al., 2017). Our method differs in two important ways from these existing approaches. First, rather than relying on an external knowledge base, or other forms of supervision, as in e.g. (Chen et al., 2016), our method is completely unsupervised, as our only input consists of a text corpus. Second, whereas existing work has focused on methods for improving word embeddings, our aim is to learn vector representations that are complementary to standard word embeddings. 3 Selecting Related Word Pairs. Starting from a vocabulary V conta"
P19-1318,P15-2070,0,0.042101,"Missing"
P19-1318,D14-1162,0,0.100122,"en set of word pairs (a,b), based on sentences in which these word pairs co-occur. For instance, Turney (2005) introduced a method called Latent Relational Analysis (LRA), which relies on first identifying a set of sufficiently frequent lexical patterns and then constructs a matrix which encodes for each considered word pair (a,b) how frequently each pattern P appears in between a and b in sentences that contain both words. Relation vectors are then obtained using singular value decomposition. More recently, Jameel et al. (2018) proposed an approach inspired by the GloVe word embedding model (Pennington et al., 2014) to learn relation vectors based on cooccurrence statistics between the target word pair (a, b) and other words. Along similar lines, Espinosa Anke and Schockaert (2018) learn relation vectors based on the distribution of words occurring in sentences that contain a and b by averaging the word vectors of these co-occurring words. Then, a conditional autoencoder is used to obtain lower-dimensional relation vectors. Taking a slightly different approach, Washio and Kato (2018a) train a neural network to predict dependency paths from a given word pair. Their approach uses standard word vectors as i"
P19-1318,S17-1017,0,0.0337245,"Missing"
P19-1318,C14-1097,0,0.19627,"Missing"
P19-1318,P15-2119,0,0.0207999,"s may also be encoded (Gupta et al., 2015), they are not explicitly modeled. Based on the hypothesis that relational word embeddings should allow us to model such properties in a more consistent and transparent fashion, we select the well-known McRae Feature Norms benchmark (McRae et al., 2005) as testbed. This dataset10 is composed of 541 words (or concepts), each of them associated with one or more features. For example, ‘a bear is an animal’, or ‘a bowl is round’. As for the specifics of our evaluation, given that some features are only associated with a few words, we follow the setting of Rubinstein et al. (2015) and consider the eight features with the largest number of associated words. We carry out this evaluation by treating the task as a multi-class classification problem, where the labels are the word features. As in the previous task, we use a linear SVM classifier and perform 3-fold cross-validation. For each input word, the 10 Downloaded from https://sites.google.com/ site/kenmcraelab/norms-data 3291 Model RWE Pair2Vec Retrofitting† Attract-Repel† FastText Overall 55.2 55.0 50.6 50.4 metal 73.6 71.9 72.3 73.2 is small 46.7 49.2 44.0 44.4 54.6 72.7 48.4 McRae Feature Norms is large animal is e"
P19-1318,P16-1226,0,0.0344953,"Missing"
P19-1318,D15-1243,0,0.0252069,".8 QVEC 55.4 52.7 56.8* 55.9* 54.6 Table 2: Results on the McRae feature norms dataset (Macro F-Score) and QVEC (correlation score). Models marked with † use external resources. The results with * indicate that WordNet was used for both the development of the model and the construction of the dataset. word embedding of the corresponding feature is fed to the classifier concatenated with its baseline FastText embedding. Given that the McRae Feature Norms benchmark is focused on nouns, we complement this experiment with a specific evaluation on verbs. To this end, we use the verb set of QVEC11 (Tsvetkov et al., 2015), a dataset specifically aimed at measuring the degree to which word vectors capture semantic properties which has shown to strongly correlate with performance in downstream tasks such as text categorization and sentiment analysis. QVEC was proposed as an intrinsic evaluation benchmark for estimating the quality of word vectors, and in particular whether (and how much) they predict lexical properties, such as words belonging to one of the fifteen verb supersenses contained in WordNet (Miller, 1995). As is customary in the literature, we compute Pearson correlation with respect to these predefi"
P19-1318,S18-2020,0,0.0139997,"e vectors ei + ej and ei · ej (referred to as the Mult+Avg setting; our method is referred to as RWE). We use a similar representation for the other methods, simply replacing the relational word vectors by the corresponding vectors (but keeping the FastText vector difference). We also consider a variant in which the FastText vector difference is concatenated with wi + wj and wi · wj , which offers a more direct comparison with the other methods. This goes in line with recent works that have shown how adding complementary features on top of the vector differences, e.g. multiplicative features (Vu and Shwartz, 2018), help improve the performance. Finally, for completeness, we also include variants where the average ei + ej is replaced by the concatenation ei ⊕ ej (referred to as Mult+Conc), which is the encoding considered in Joshi et al. (2019). For these experiments we train a linear SVM classifier directly on the word pair encoding, performing a 10-fold cross-validation in the case of DiffVec, and using the train-test splits of BLESS. Results Table 1 shows the results of our relational word vectors, the standard FastText embeddings and other baselines on the two relation classification datasets (i.e."
P19-1318,J17-4004,0,0.0277307,"Missing"
P19-1318,N18-1103,0,0.050042,"model, to encourage the word vectors to additionally be predictive of a given set of relational facts (Xu et al., 2014; Celikyilmaz et al., 2015; Chen et al., 2016). Other authors have used knowledge bases in a more restricted way, by taking the fact that two words are linked to each other in a given knowledge graph as evidence that their word vectors should be similar (Faruqui et al., 2015; Speer et al., 2017). Finally, there has also been work that uses lexicons to learn word embeddings which are specialized towards certain types of lexical knowledge, such as hypernymy (Nguyen et al., 2017; Vulic and Mrksic, 2018), antonymy (Liu et al., 2015; Ono et al., 2015) or a combination of various linguistic constraints (Mrkˇsi´c et al., 2017). Our method differs in two important ways from these existing approaches. First, rather than relying on an external knowledge base, or other forms of supervision, as in e.g. (Chen et al., 2016), our method is completely unsupervised, as our only input consists of a text corpus. Second, whereas existing work has focused on methods for improving word embeddings, our aim is to learn vector representations that are complementary to standard word embeddings. 3 Selecting Related"
P19-1318,P16-1158,0,0.0134476,"constraint. For Retrofitting we make use of WordNet (Fellbaum, 1998) as the input knowledge base, while for Attract-Repel we use the default configuration with all constraints from PPDB (Pavlick et al., 2015), WordNet and BabelNet (Navigli and Ponzetto, 2012). All comparison systems are 300-dimensional and trained on the same Wikipedia corpus. 5.1 Relation Classification Given a pre-defined set of relation types and a pair of words, the relation classification task consists in selecting the relation type that best describes the relationship between the two words. As test sets we used DiffVec (Vylomova et al., 2016) and BLESS8 (Baroni and Lenci, 2011). The DiffVec dataset includes 12,458 word pairs, covering fifteen relation types including hypernymy, causepurpose or verb-noun derivations. On the other hand, BLESS includes semantic relations such as hypernymy, meronymy, and co-hyponymy.9 BLESS includes a train-test partition, with 13,258 and 6,629 word pairs, respectively. This task is treated as a multi-class classification problem As a baseline model (Diff), we consider the usual representation of word pairs in terms of their vector differences (Fu et al., 2014; Roller et al., 7 We used the pre-trained"
P19-1318,N18-1102,0,0.107872,"the 57th Annual Meeting of the Association for Computational Linguistics, pages 3286–3296 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Linzen, 2016; Rogers et al., 2017). This suggests that relational information has to be encoded separately from standard similaritycentric word embeddings. One appealing strategy is to represent relational information by learning, for each pair of related words, a vector that encodes how the words are related. This strategy was first adopted by Turney (2005), and has recently been revisited by a number of authors (Washio and Kato, 2018a; Jameel et al., 2018; Espinosa Anke and Schockaert, 2018; Washio and Kato, 2018b; Joshi et al., 2019). However, in many applications, word vectors are easier to deal with than vector representations of word pairs. The research question we consider in this paper is whether it is possible to learn word vectors that capture relational information. Our aim is for such relational word vectors to be complementary to standard word vectors. To make relational information available to NLP models, it then suffices to use a standard architecture and replace normal word vectors by concatenations of stan"
P19-1318,D18-1058,0,0.152715,"the 57th Annual Meeting of the Association for Computational Linguistics, pages 3286–3296 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Linzen, 2016; Rogers et al., 2017). This suggests that relational information has to be encoded separately from standard similaritycentric word embeddings. One appealing strategy is to represent relational information by learning, for each pair of related words, a vector that encodes how the words are related. This strategy was first adopted by Turney (2005), and has recently been revisited by a number of authors (Washio and Kato, 2018a; Jameel et al., 2018; Espinosa Anke and Schockaert, 2018; Washio and Kato, 2018b; Joshi et al., 2019). However, in many applications, word vectors are easier to deal with than vector representations of word pairs. The research question we consider in this paper is whether it is possible to learn word vectors that capture relational information. Our aim is for such relational word vectors to be complementary to standard word vectors. To make relational information available to NLP models, it then suffices to use a standard architecture and replace normal word vectors by concatenations of stan"
P19-1318,C14-1212,0,0.0285739,"88.3 88.1 89.0 88.8 88.8 88.6 90.6 90.8 Rec. 92.6 89.7 90.2 88.6 89.3 89.1 90.4 FastText (Bojanowski et al., 2017) 81.9 57.3 57.8 88.5 85.4 85.4 Model RWE Mult+Avg Mult+Conc Diff (only) Reference 59.3 85.7 Table 1: Accuracy and macro-averaged F-Measure, precision and recall on BLESS and DiffVec. Models marked with † use external resources. The results with * indicate that WordNet was used for both the development of the model and the construction of the dataset. All models concatenate their encoded representations with the baseline vector difference of standard FastText word embeddings. 2014; Weeds et al., 2014), using FastText word embeddings. Since our goal is to show the complementarity of relational word embeddings with standard word vectors, for our method we concatenate the difference wj − wi with the vectors ei + ej and ei · ej (referred to as the Mult+Avg setting; our method is referred to as RWE). We use a similar representation for the other methods, simply replacing the relational word vectors by the corresponding vectors (but keeping the FastText vector difference). We also consider a variant in which the FastText vector difference is concatenated with wi + wj and wi · wj , which offers a"
P19-1318,D13-1136,0,0.0224323,"has the advantage of being less biased towards infrequent (and thus typically less informative) words. Learning Relation Vectors. In this paper, we will rely on word vector averaging for learning relation vectors, which has the advantage of being much faster than other existing approaches, and thus allows us to consider a higher number of word pairs (or a larger corpus) within a fixed 3 A co-occurrence in which there are k words in between 1 w and v then receives a weight of k+1 . 3288 time budget. Word vector averaging has moreover proven surprisingly effective for learning relation vectors (Weston et al., 2013; Hashimoto et al., 2015; Fan et al., 2015; Espinosa Anke and Schockaert, 2018), as well as in related tasks such as sentence embedding (Wieting et al., 2016). Specifically, to construct the relation vector rwv capturing the relationship between the words w and v we proceed as follows. First, we compute a bag of words representation {(w1 , f1 ), ..., (wn , fn )}, where fi is the number of times the word wi occurs in between the words w and v in any given sentence in the corpus. The relation vector rwv is then essentially computed as a weighted average: ! n X rwv = norm fi · wi (2) i=1 where we"
S17-2002,N09-1003,0,0.413176,"Missing"
S17-2002,W13-3520,0,0.0139707,"eline system we included the results of the concept and entity embeddings of NASARI (Camacho-Collados et al., 2016). These embeddings were obtained by exploiting knowledge from Wikipedia and WordNet coupled with general domain corpus-based Word2Vec embeddings (Mikolov et al., 2013). We performed the evaluation with the 300-dimensional English embedded vectors (version 3.0)9 and used them for all languages. For the comparison within and • Subtask 1. The common corpus for subtask 1 was the Wikipedia corpus of the target language. Specifically, systems made use of the Wikipedia dumps released by Al-Rfou et al. (2013).6 • Subtask 2. The common corpus for subtask 2 was the Europarl parallel corpus7 . This corpus is available for all languages except 6 https://sites.google.com/site/rmyeid/ projects/polyglot 7 http://opus.lingfil.uu.se/Europarl. php 8 http://opus.lingfil.uu.se/ OpenSubtitles2016.php 9 http://lcl.uniroma1.it/nasari/ 20 System English r Luminoso run2 0.78 Luminoso run1 0.78 0.78 QLUT run1∗ hhu run1∗ 0.71 HCCL run1∗ 0.68 NASARI (baseline) 0.68 hhu run2∗ 0.66 QLUT run2∗ 0.67 RUFINO run1∗ 0.65 0.60 Citius run2 l2f run2 (a.d.) 0.64 l2f run1 (a.d.) 0.64 Citius run1∗ 0.57 MERALI run1∗ 0.59 Amateur ru"
S17-2002,J06-1003,0,0.0958441,"0 0.60 0.48 0.53 0.44 0.44 0.57 0.61 0.50 0.31 0.40 0.57 0.61 0.05 -0.06 - ρ Final 0.75 0.75 0.72 0.60 0.57 0.64 0.63 0.62 0.41 0.62 -0.06 - 0.74 0.74 0.70 0.60 0.55 0.52 0.51 0.62 0.41 0.62 0.00 - Table 6: Pearson (r), Spearman (ρ) and official (Final) results of participating systems on the five monolingual word similarity datasets (subtask 1). across languages NASARI relies on the lexicalizations provided by BabelNet (Navigli and Ponzetto, 2012) for the concepts and entities in each language. Then, the final score was computed through the conventional closest senses strategy (Resnik, 1995; Budanitsky and Hirst, 2006), using cosine similarity as the comparison measure. 3.2 Results We present the results of subtask 1 in Section 3.2.1 and subtask 2 in Section 3.2.2. 3.2.1 System Score Official Rank Luminoso run2 Luminoso run1 HCCL run1∗ NASARI (baseline) RUFINO run1∗ SEW run2 (a.d.) SEW run1 RUFINO run2∗ hjpwhuer run1 0.743 0.740 0.658 0.598 0.555 0.552 0.506 0.369 0.018 1 2 3 4 5 6 7 Table 7: Global results of participating systems on subtask 1 (multilingual word similarity). Subtask 1 Table 6 lists the results on all monolingual datasets.10 The systems which made use of the shared Wikipedia corpus are mark"
S17-2002,S17-2034,0,0.0204818,"inds of semantic representation techniques and semantic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obta"
S17-2002,W16-2508,1,0.276959,"reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked with * contributed equally. 15 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 15–26, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics the dataset suffers from other issues. First, given that SimLex-999 has been annotated by turkers, and not by human experts, the similarity scores assigned to individual word pairs have a high variance, resulting in relatively low IAA (Camacho-Collados and Navigli, 2016). In fact, the reported IAA for this dataset is 0.67 in terms of average pairwise correlation, which is considerably lower than conventional expert-based datasets whose IAA are generally above 0.80 (Rubenstein and Goodenough, 1965; Camacho-Collados et al., 2015). Second, similarly to many of the above-mentioned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of system"
S17-2002,D16-1235,0,0.0758158,"Missing"
S17-2002,E17-2036,1,0.804232,"Missing"
S17-2002,P15-2001,1,0.838039,"s 15–26, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics the dataset suffers from other issues. First, given that SimLex-999 has been annotated by turkers, and not by human experts, the similarity scores assigned to individual word pairs have a high variance, resulting in relatively low IAA (Camacho-Collados and Navigli, 2016). In fact, the reported IAA for this dataset is 0.67 in terms of average pairwise correlation, which is considerably lower than conventional expert-based datasets whose IAA are generally above 0.80 (Rubenstein and Goodenough, 1965; Camacho-Collados et al., 2015). Second, similarly to many of the above-mentioned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approach"
S17-2002,I05-1067,0,0.020408,"t contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarity datasets suffer from various issues: 1. The similarity scale used for the annotation of WordSim-353 and MEN (Bruni et al., 2014) does not distinguish between similarity and relatedness, and hence conflates these two. As a result, the datasets contain pairs that are judged to be highly similar even if they are not of similar type or nature. F"
S17-2002,2015.mtsummit-papers.27,0,0.0742157,"Missing"
S17-2002,D09-1124,0,0.0561085,"nce Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming Food and drink Games and video games Geography and places Geology and geophysics Health and medicine Heraldry, honors, and vexillology History been increasingly studied (Xiao and Guo, 2014; Franco-Salvador et al., 2016). However, there have been very few reliable datasets for evaluating cross-lingual systems. Similarly to the case of multilingual datasets, these cross-lingual datasets have been constructed on the basis of conventional English word similarity datasets: MC-30 and WordSim-353 (Hassan and Mihalcea, 2009), and RG-65 (Camacho-Collados et al., 2015). As a result, they inherit the issues affecting their parent datasets mentioned in the previous subsection: while MC-30 and RG-65 are composed of only 30 and 65 pairs, WordSim-353 conflates similarity and relatedness in different languages. Moreover, the datasets of Hassan and Mihalcea (2009) were not re-scored after having been translated to the other languages, thus ignoring possible semantic shifts across languages and producing unreliable scores for many translated word pairs. For this subtask we provided ten high quality cross-lingual datasets,"
S17-2002,S17-2041,0,0.0352303,"Missing"
S17-2002,S17-2033,0,0.0191197,"arks for evaluation. All kinds of semantic representation techniques and semantic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have"
S17-2002,N15-1184,0,0.0720481,"Missing"
S17-2002,J15-4004,0,0.047794,". Hence, these benchmarks do not allow reliable conclusions to be drawn, since performance improvements have to be large to be statistically significant (Batchkarov et al., 2016). 1.2 Subtask 2: Cross-lingual Semantic Similarity Over the past few years multilingual embeddings that represent lexical items from multiple languages in a unified semantic space have garnered considerable research attention (Zou et al., 2013; de Melo, 2015; Vuli´c and Moens, 2016; Ammar et al., 2016; Upadhyay et al., 2016), while at the same time cross-lingual applications have also 4. The recent SimLex-999 dataset (Hill et al., 2015) improves both the size and consistency issues of the conventional datasets by providing word similarity scores for 999 word pairs on a consistent scale that focuses on similarity only (and not relatedness). However, 16 Animals Art, architecture and archaeology Biology Business, economics, and finance Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming Food and drink Games and video games Geography and places Geology and geophysics Health and medicine Heraldry, honors, and vexillology History been increasingly studied (Xiao and Guo, 2014; Franco-"
S17-2002,S17-2032,0,0.0235773,"ic similarity systems were encouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair com"
S17-2002,S17-2037,0,0.0372334,"Missing"
S17-2002,C12-1109,0,0.034733,"24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages ha"
S17-2002,S14-2003,1,0.88311,"Missing"
S17-2002,P11-1076,0,0.00934146,"lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages has generally proved difficult to evaluate. A reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked wit"
S17-2002,P16-2074,0,0.0185427,"ex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarity datasets suffer from"
S17-2002,S17-2035,0,0.0441697,"Missing"
S17-2002,D14-1162,0,0.118611,"c Word Similarity Jose Camacho-Collados*1 , Mohammad Taher Pilehvar*2 , Nigel Collier2 and Roberto Navigli1 1 2 Department of Computer Science, Sapienza University of Rome Department of Theoretical and Applied Linguistics, University of Cambridge 1 {collados,navigli}@di.uniroma1.it 2 {mp792,nhc30}@cam.ac.uk Abstract word representation, a research field that has recently received massive research attention mainly as a result of the advancements in the use of neural networks for learning dense low-dimensional semantic representations, often referred to as word embeddings (Mikolov et al., 2013; Pennington et al., 2014). Almost any application in NLP that deals with semantics can benefit from efficient semantic representation of words (Turney and Pantel, 2010). However, research in semantic representation has in the main focused on the English language only. This is partly due to the limited availability of word similarity benchmarks in languages other than English. Given the central role of similarity datasets in lexical semantics, and given the importance of moving beyond the barriers of the English language and developing languageindependent and multilingual techniques, we felt that this was an appropriat"
S17-2002,S17-2036,0,0.0222596,"ncouraged to participate. In the end we received a wide variety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3."
S17-2002,P15-2004,0,0.014103,"oned datasets, SimLex-999 does not contain named entities (e.g., Microsoft), or multiword expressions (e.g., black hole). In fact, the dataset includes only words that are defined in WordNet's vocabulary (Miller et al., 1990), and therefore lacks the ability to test the reliability of systems for WordNet out-of-vocabulary words. Third, the dataset contains a large number of antonymy pairs. Indeed, several recent works have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy (Schwartz et al., 2015; Pham et al., 2015; Nguyen et al., 2016). representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005; Joubarne and Inkpen, 2011; Granada et al., 2014; CamachoCollados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarit"
S17-2002,S17-2038,0,0.0206795,"iety of participants: proposing distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3.1.4 Baseline As the baseline system we included the results"
S17-2002,S17-2039,0,0.044791,"Missing"
S17-2002,P14-1044,1,0.822702,"task website: http://alt.qcri. org/semeval2017/task2/ . 1 Introduction Measuring the extent to which two words are semantically similar is one of the most popular research fields in lexical semantics, with a wide range of Natural Language Processing (NLP) applications. Examples include Word Sense Disambiguation (Miller et al., 2012), Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Ontology Alignment (Pilehvar and Navigli, 2014). Moreover, word similarity is generally accepted as the most direct in-vitro evaluation framework for 1.1 Subtask 1: Multilingual Semantic Similarity While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages has generally proved difficult to evaluate. A reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic Authors marked with * contributed equally. 15 Proceedings of the 11th International Workshop on Semantic Evaluations ("
S17-2002,S17-2040,0,0.0437279,"Missing"
S17-2002,S17-2042,0,0.0464227,"Missing"
S17-2002,D13-1141,0,0.039439,"such as RG-65, MC30 (Miller and Charles, 1991), and WS-Sim (Agirre et al., 2009) (the similarity portion of WordSim-353) are relatively small, containing 65, 30, and 200 word pairs, respectively. Hence, these benchmarks do not allow reliable conclusions to be drawn, since performance improvements have to be large to be statistically significant (Batchkarov et al., 2016). 1.2 Subtask 2: Cross-lingual Semantic Similarity Over the past few years multilingual embeddings that represent lexical items from multiple languages in a unified semantic space have garnered considerable research attention (Zou et al., 2013; de Melo, 2015; Vuli´c and Moens, 2016; Ammar et al., 2016; Upadhyay et al., 2016), while at the same time cross-lingual applications have also 4. The recent SimLex-999 dataset (Hill et al., 2015) improves both the size and consistency issues of the conventional datasets by providing word similarity scores for 999 word pairs on a consistent scale that focuses on similarity only (and not relatedness). However, 16 Animals Art, architecture and archaeology Biology Business, economics, and finance Chemistry and mineralogy Computing Culture and society Education Engineering and technology Farming"
S17-2002,K15-1026,0,0.0981438,"erit the issues affecting their parent datasets mentioned in the previous subsection: while MC-30 and RG-65 are composed of only 30 and 65 pairs, WordSim-353 conflates similarity and relatedness in different languages. Moreover, the datasets of Hassan and Mihalcea (2009) were not re-scored after having been translated to the other languages, thus ignoring possible semantic shifts across languages and producing unreliable scores for many translated word pairs. For this subtask we provided ten high quality cross-lingual datasets, constructed according to the procedure of Camacho-Collados et al. (2015), in a semi-automatic manner exploiting the monolingual datasets of subtask 1. These datasets constitute a reliable evaluation framework across five languages. 2 Table 1: The set of thirty-four domains. wide range of domains (Section 2.1.1), (2) through translation of these pairs, we obtained word pairs for the other four languages (Section 2.1.2) and, (3) all word pairs of each dataset were manually scored by multiple annotators (Section 2.1.3). 2.1.1 English dataset creation Seed set selection. The dataset creation started with the selection of 500 English words. One of the main objectives o"
S17-2002,S17-2008,0,0.123538,"distributional semantic models learnt directly from raw corpora, using syntactic features, exploiting knowledge from lexical resources, and hybrid approaches combining corpus-based and knowledge-based clues. Due to lack of space we cannot describe all the systems in detail, but we recommend the reader to refer to the system description papers for more information about the individual systems: HCCL (He et al., 2017), Citius (Gamallo, 2017), jmp8 (Melka and Bernard, 2017), l2f (Fialho et al., 2017), QLUT (Meng et al., 2017), RUFINO (Jimenez et al., 2017), MERALI (Mensa et al., 2017), Luminoso (Speer and Lowry-Duda, 2017), hhu (QasemiZadeh and Kallmeyer, 2017), Mahtab (Ranjbar et al., 2017), SEW (Delli Bovi and Raganato, 2017) and Wild Devs (Rotari et al., 2017), and OoO. 3.1.2 Shared training corpus We encouraged the participants to use a shared text corpus for the training of their systems. The use of the shared corpus was intended to mitigate the influence that the underlying training corpus might have upon the quality of obtained representations, laying a common ground for a fair comparison of the systems. 3.1.4 Baseline As the baseline system we included the results of the concept and entity embeddings of"
S17-2002,D14-1034,0,\N,Missing
S17-2002,P16-1157,0,\N,Missing
S17-2002,W16-2502,0,\N,Missing
S18-1003,N18-2107,1,0.891609,", the goal is to alleviate the lack of an associated grammar. This context, which makes it difficult to encode a clear and univocous single meaning for each emoji, has given rise to work considering emojis as function words or even affective markers (Na’aman et al., 2017), potentially affecting the overall semantics of longer utterances like sentences (Monti et al., 2016; Donato and Paggio, 2017). The polysemy of emoji has been explored userwise (Miller et al., 2017), location-wise, specifically in countries (Barbieri et al., 2016b) and cities (Barbieri et al., 2016a), gender-wise, time-wise (Barbieri et al., 2018b; Chen et al., 2017), and even device-wise, due to the fact that emojis may have different pictorial characteristics (and therefore, different interpretations), depending on the device (e.g., Iphone, Android, Samsung, etc.) or app (Whatsapp, Twitter, Facebook, and so forth)3 (Tigwell and Flatla, 2016; Miller et al., 2016). An aspect related with emoji semantic modeling in which awareness is increasing dramatically is the inherent bias existing in these representations. For example, Barbieri and CamachoCollados (2018) show that emoji modifiers can affect the semantics of emojis (they looked sp"
S18-1003,E17-2017,1,0.505123,"iently rich that oversimplifying them to sentiment carriers or boosters would be to neglect the semantic richness of these ideograms, which in addition to mood ( ) include in their vocabulary references to food ( ), sports ( ), scenery ( ), etc2 . In general, however, effectively predicting the emoji associated with a piece of content may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emoji-enriched social media content, suggestion of emojis when writing text messages or sharing pictures online. Given that emojis may also mislead humans (Barbieri et al., 2017; Miller et al., 2017), the automated prediction of emojis may help to achieve better language understanding. As a consequence, by modeling the semantics of emojis, we can improve highly-subjective tasks like sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). In this context, Barbieri et al. (2017) introduced the task of emoji prediction in Twitter by training several models based on bidirectional Long ShortTerm Memory networks (LSTMs) (Graves, 2012), and showing they can outperform humans in solvThis paper describes the results of the first shared task on Multil"
S18-1003,S18-2011,1,0.899882,"Missing"
S18-1003,L16-1626,1,0.837949,"elements of this shared task (Section 2 and 3). Then, we cover the dataset compilation, curation and release process (Section 4). In Section 5 we detail the evaluation metrics and describe the overall results obtained by participating systems. Finally, we wrap this task description paper up with the main conclusions drawn from the organization of this challenge, as well as outlining potential avenues for future work, in Section 6. 2 Today, modeling emoji semantics via vector representations is a well defined avenue of work. Contributions in this respect include models trained on Twitter data (Barbieri et al., 2016c), Twitter data together with the official unicode description (Eisner et al., 2016), or using text from a popular keyboard app Ai et al. (2017). In the latter contribution it is argued that emojis used in an affective context are more likely to become popular, and in general, the most important factor for an emoji to become popular is to have a clear meaning. In fact, the area of emoji vector evaluation has also experienced a significant growth as of recent. For instance, Wijeratne et al. (2017a) propose a platform for exploring emoji semantics. Further studies on evaluating emoji semantics"
S18-1003,S18-1075,0,0.0205523,"do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble ap"
S18-1003,S18-1062,0,0.0222283,"arch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine"
S18-1003,D17-1169,0,0.303851,"the emoji associated with a piece of content may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emoji-enriched social media content, suggestion of emojis when writing text messages or sharing pictures online. Given that emojis may also mislead humans (Barbieri et al., 2017; Miller et al., 2017), the automated prediction of emojis may help to achieve better language understanding. As a consequence, by modeling the semantics of emojis, we can improve highly-subjective tasks like sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). In this context, Barbieri et al. (2017) introduced the task of emoji prediction in Twitter by training several models based on bidirectional Long ShortTerm Memory networks (LSTMs) (Graves, 2012), and showing they can outperform humans in solvThis paper describes the results of the first shared task on Multilingual Emoji Prediction, organized as part of SemEval 2018. Given the text of a tweet, the task consists of predicting the most likely emoji to be used along such tweet. Two subtasks were proposed, one for English and one for Spanish, and participants were allowed to submit a system run t"
S18-1003,S18-1079,0,0.0244507,", but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competiti"
S18-1003,S18-1067,0,0.0177432,"rse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperfor"
S18-1003,S18-1072,0,0.0393464,"Missing"
S18-1003,S18-1081,0,0.0367034,"Missing"
S18-1003,S18-1004,0,0.0800282,"Missing"
S18-1003,S18-1078,0,0.0516896,"Missing"
S18-1003,S18-1070,0,0.037717,"Missing"
S18-1003,W17-5216,0,0.0205954,"d Work Modeling the semantics of emojis, and their applications thereof, is a relatively novel research problem with direct applications in any social media task. By explicitly modeling emojis as selfcontaining semantic units, the goal is to alleviate the lack of an associated grammar. This context, which makes it difficult to encode a clear and univocous single meaning for each emoji, has given rise to work considering emojis as function words or even affective markers (Na’aman et al., 2017), potentially affecting the overall semantics of longer utterances like sentences (Monti et al., 2016; Donato and Paggio, 2017). The polysemy of emoji has been explored userwise (Miller et al., 2017), location-wise, specifically in countries (Barbieri et al., 2016b) and cities (Barbieri et al., 2016a), gender-wise, time-wise (Barbieri et al., 2018b; Chen et al., 2017), and even device-wise, due to the fact that emojis may have different pictorial characteristics (and therefore, different interpretations), depending on the device (e.g., Iphone, Android, Samsung, etc.) or app (Whatsapp, Twitter, Facebook, and so forth)3 (Tigwell and Flatla, 2016; Miller et al., 2016). An aspect related with emoji semantic modeling in wh"
S18-1003,S18-1077,0,0.021307,"018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve Bayes, Lo27 gistic Regression, Random Forests, etc.). Infrequent classes are oversampled using the SMOTE algorithm. As for features, they use both unigrams and bigrams. English Emo F1 87.8 37.8 47.1 26.9 55.5 16.2 22.6 36.2 24 22.2 40 64.7 63.7 17.1 13 29.2 14.3 73.6 38.4 9 particularities of each individual language should be taken into consideration for best performance. The most precise systems were EmoNLP and T¨ubingen-Oslo, whereas the highest Recall was obtained by NTUA-SLP a"
S18-1003,E17-2068,0,0.0346041,"is was a single label classification problem, the classic precision (Prec.), recall (Recall), fscore (F1) and accuracy (Acc.) were used as official evaluation metrics. Note that because of the skewed distribution of the label set we opted for macro average over all labels. 5.2 • EmoNLP (Liu, 2018). This system is based on a Gradient Boosting Regression Tree Approach combined with a Bi-LSTM on character and word ngrams. It is complemented with several lexicons as well as learning sentiment specific word embeddings. Baseline The baseline system for this task was a classifier based on FastText6 (Joulin et al., 2017). Given a set of N documents, the loss that the model attempts to minimize is the negative log-likelihood over the labels (in our case, the emojis): loss = − • UMDuluth-CS8761 (Beaulieu and Asamoah Owusu, 2018) This supervised system combines an SVM with a bag-of-words approach for extracting salient features. This is one of the most competitive systems with the highest precision in English and the third best result in Spanish. n=1 1 X en log(softmax (BAxn )) N N where en is the emoji included in the n-th Twitter post, represented as hot vector, and used as label. Hyperparameters were set as d"
S18-1003,W16-6208,0,0.0910628,"Missing"
S18-1003,S18-1064,0,0.0208509,"default7 . 5.3 • Hatching Chick (Coster et al., 2018). This system builds an SVM classifier (with gradient descent optimization) on words and character ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but"
S18-1003,S18-1059,0,0.0252405,"on, and pretrained word2vec vectors. They used external resources for associating each tweet with information on emotions, concreteness, familiarity, and others. They only participated in the English subtask but they classified second (according to the F1 score) with the highest recall. Evaluation Metrics As this was a single label classification problem, the classic precision (Prec.), recall (Recall), fscore (F1) and accuracy (Acc.) were used as official evaluation metrics. Note that because of the skewed distribution of the label set we opted for macro average over all labels. 5.2 • EmoNLP (Liu, 2018). This system is based on a Gradient Boosting Regression Tree Approach combined with a Bi-LSTM on character and word ngrams. It is complemented with several lexicons as well as learning sentiment specific word embeddings. Baseline The baseline system for this task was a classifier based on FastText6 (Joulin et al., 2017). Given a set of N documents, the loss that the model attempts to minimize is the negative log-likelihood over the labels (in our case, the emojis): loss = − • UMDuluth-CS8761 (Beaulieu and Asamoah Owusu, 2018) This supervised system combines an SVM with a bag-of-words approach"
S18-1003,S18-1068,0,0.0230149,"with gradient descent optimization) on words and character ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This sys"
S18-1003,S18-1066,0,0.053806,"Missing"
S18-1003,S18-1073,0,0.0252344,"below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve Bayes, Lo27 gistic Regression, Random Forests, etc.). Infrequent classes are oversampled usi"
S18-1003,S18-1060,0,0.0230182,"is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve Bayes, Lo27 gistic Regression, Random Forests, etc.). Infreque"
S18-1003,P17-3022,0,0.0972214,"Missing"
S18-1003,S18-1063,0,0.0211944,"om/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve"
S18-1003,S18-1065,0,0.0231871,"timization) on words and character ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM clas"
S18-1003,S18-1076,0,0.0529383,"Missing"
S18-1003,S18-1071,0,0.0142065,"cter ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of f"
S18-1115,S13-1005,0,0.16065,"Missing"
S18-1115,S18-1116,0,0.0792436,"Missing"
S18-1115,S18-1149,0,0.0363408,"Missing"
S18-1115,E17-2013,0,0.185703,"Missing"
S18-1115,C92-2082,0,0.323886,"systems for any individual subtask. Along with a specific source corpus and vocabulary, each subtask features its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification"
S18-1115,S15-2151,1,0.928124,"i♥ Luis Espinosa-Anke♣ Sergio Oramas♦ Tommaso Pasini♥ Enrico Santus♥ Vered Shwartz♠ Roberto Navigli♥ Horacio Saggion♦ ♣ School of Computer Science and Informatics, Cardiff University, United Kingdom ♥ Computer Science Department, Sapienza University of Rome, Italy ♦ Pompeu Fabra University, Barcelona, Spain ♥ MIT, United States ♠ Bar-Ilan University, Ramat Gan, Israel ♣ {camachocolladosj,espinosa-ankel}@cardiff.ac.uk, ♥ {dellibovi,pasini,navigli}@di.uniroma1.it, ♦ {name.surname}@upf.edu, ♥ esantus@mit.edu, ♠ vered1986@gmail.com Abstract web retrieval, website navigation or records management (Bordea et al., 2015). This paper describes the SemEval 2018 Shared Task on Hypernym Discovery. We put forward this task as a complementary benchmark for modeling hypernymy, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input. Instead, our reformulated task is defined as follows: given an input term, retrieve (or discover) its suitable hypernyms from a target corpus. We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were al"
S18-1115,S16-1168,0,0.263061,"s either a concept or a 4 As an example, the term apple could either refer to a fruit (if labeled as concept) or to a company (if labeled as named entity). 5 http://ebiquity.umbc. edu/blogger/2013/05/01/ umbc-webbase-corpus-of-3b-english-words/ 6 http://dbpubs.stanford.edu:8091/ testbed/doc2/WebBase/ ˜ 3 In fact, WordNet encodes hypernym and instance as two separate semantic relations. Instances are always leaf (terminal) nodes in their hierarchies. 714 sources of information with respect to the corpora used in previous tasks, such as Wikipedia in the SemEval 2016 task on taxonomy extraction (Bordea et al., 2016). In fact, the encyclopedic nature of Wikipedia has been exploited in a wide variety of works (Ponzetto and Strube, 2007; Flati et al., 2016; Gupta et al., 2016), and differs substantially from the web-based corpus we put forward here. As source corpus for the Italian subtask (1B) we instead used the 1.3-billion-word itWac corpus7 (Baroni et al., 2009), extracted from different sources of the web within the .it domain. Finally, as source corpus for the Spanish subtask (1C) we considered the 1.8-billion-word Spanish corpus8 (Cardellino, 2016), which also contains heterogeneous documents from di"
S18-1115,S18-1150,0,0.0250373,"Missing"
S18-1115,E17-2036,1,0.898611,"Missing"
S18-1115,N15-1098,0,0.0369984,"ttps://competitions. codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within do"
S18-1115,D16-1041,1,0.888795,"Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications (Camacho-Collados, 2017). In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains. The ma"
S18-1115,S18-1151,0,0.0726421,"14 We used the open-source code available at https:// bitbucket.org/luisespinosa/taxoembed 15 https://github.com/vered1986/ UnsupervisedHypernymy 16 Following the conclusions from Shwartz et al. (2017), we set the hyper-parameters to: SLQS: median, PLMI, N = 100 and APSyn: N = 500. 13 Although only P@5 is displayed in the tables due to lack of space, the other thresholds were used in the official evaluation as well. 717 5.2 Participant Systems in general they were outperformed by supervised systems, in some cases their performance came close, especially for concepts. For instance, the ADAPT (Maldonado and Klubika, 2018) system, which is based on a simple similarity measure applied to word embeddings, achieved a very decent 8.13 MAP percentage performance on the medical dataset, using neither supervision nor external resources. Supervised systems produced a larger gap for entities, probably due, as mentioned above, to the lower diversity of possible hypernyms. Table 3 shows a summary of all participant systems, displaying their main features with respect to supervison and external resources used, if any. 5.3 Results A summary of the results is provided in tables 3 to 7, respectively describing results for Eng"
S18-1115,P14-1113,0,0.377885,"t al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathe"
S18-1115,P16-2081,1,0.844897,"Missing"
S18-1115,P10-1134,1,0.838694,"y individual subtask. Along with a specific source corpus and vocabulary, each subtask features its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni"
S18-1115,D16-1234,0,0.0290792,"rrent research in hypernymy modeling with this novel discovery setting. Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition. It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (Shwartz et al., 2016): indeed, successfully identifying this lexical relation substantially improves Question Answering applications (Prager et al., 2008; Yahya et al., 2013), Textual Entailment and Semantic Search systems (Hoffart et al., 2014; Roller et al., 2014; Roller and Erk, 2016). In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (Yu et al., 2015), which are in turn useful resources for downstream tasks such as 712 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 712–724 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 1A: English 1B: Italian 1C: Spanish Term sorrow Nina Simone guacamole 2A: Medical pulmonary embolism 2B: Music Green Day Hypernym(s) sadness, unhappiness musicista, pianista, persona salsa para mojar, salsa, alimento"
S18-1115,D17-1022,0,0.0580762,"Missing"
S18-1115,C14-1097,0,0.0705885,"Missing"
S18-1115,S18-1146,0,0.019286,"Missing"
S18-1115,E17-2064,0,0.0114909,"stributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathered from different and heterogeneous sources. A system operating in this setting r"
S18-1115,L16-1722,1,0.929363,". codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld ap"
S18-1115,L16-1528,1,0.817501,"t appeared too vague or general, as well as terms with mis-attributed domains. Domain-specific corpora. As source corpus for the medical domain (subtask 2A) we provided a combination of texts drawn from the MEDLINE9 (Medical Literature Analysis and Retrieval System) repository, which contains academic documents such as scientific publications and paper abstracts. This corpus contains 130 million words. As regards the music domain (subtask 2B), instead, the source corpus we compiled is a concatenation of several music-specific corpora, i.e. music biographies from Last.fm contained in ELMD 2.0 (Oramas et al., 2016), articles from the music branch of Wikipedia, and a corpus of album customer reviews from Amazon (Oramas et al., 2017). The resulting corpus reaches 100 million words in total. 4.1.2 Term Collection Vocabulary Creation With the aim of simplifying the task for participants by providing a unified hypernym search space, we built a series of vocabulary files including all the possible hypernyms on each dataset. Each vocabulary was constructed by considering all the words occurring at least N times across the source corpus of the corresponding subtask. We set N to five and three in the general-pur"
S18-1115,E14-4008,1,0.929727,"i Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose te"
S18-1115,P17-1192,0,0.0129572,"es its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al."
S18-1115,P16-1226,1,0.918452,"017). In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains. The main goal of this task is that of complementing current research in hypernymy modeling with this novel discovery setting. Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition. It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (Shwartz et al., 2016): indeed, successfully identifying this lexical relation substantially improves Question Answering applications (Prager et al., 2008; Yahya et al., 2013), Textual Entailment and Semantic Search systems (Hoffart et al., 2014; Roller et al., 2014; Roller and Erk, 2016). In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (Yu et al., 2015), which are in turn useful resources for downstream tasks such as 712 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 712–724 New Orleans, Louisiana, June 5–6, 201"
S18-1115,E17-1007,1,0.68931,"ions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications (Camacho-Coll"
S18-1115,S18-1148,0,0.0202622,"Missing"
S18-1115,J17-4004,0,0.0330605,"Missing"
S18-1115,S17-1004,0,0.0128295,"ish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were allowed to compete in any or all of the subtasks. Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks. Data, results and further information about the task can be found at https://competitions. codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2"
S18-1115,D17-1123,0,0.0255651,"Missing"
S18-1115,C14-1212,0,0.0535272,"a et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathered from different a"
S18-1115,P10-2021,0,0.026139,"aining set, separately for each subtask and measure. where Q is a sample of experiment runs, AP(·) refers to average precision, i.e. an average of the correctness of each individual obtained hypernym from the search space. Mean Reciprocal Rank (MRR). MRR rewards the position of the first correct result in a ranked list of outcomes, and is defined as: |Q| 1 X 1 MRR = |Q| ranki i=1 where ranki refers to the rank position of the first relevant outcome for the ith run. While its main field of application is Information Retrieval, it has also been used in NLP tasks such as collocation recognition (Wu et al., 2010; Rodr´ıguezFern´andez et al., 2016). In addition to the above, we also provide results according to P@k, i.e. the number of correctly retrieved hypernyms at different cut-off thresholds, specifically k ∈ {1, 3, 5, 15}.13 5.1 Baselines We compared the participating systems with both supervised and unsupervised baselines for each subtask, inspired by recent work on hypernym detection and discovery. In this section we briefly describe each of them. 5.1.1 Unsupervised Baselines Supervised Baselines We first used a na¨ıve most frequent hypernym (MFH) baseline, which simply returns, for each input"
S18-1115,S18-1147,0,0.0304331,"Missing"
S18-2011,K17-1012,1,0.885233,"erically measure some biases with respect to gender and race. Recently, emojis have introduced modifiers as part of their encoding. With these modifiers the same emoji can be used with different features: as male or female, or with different skin colors, for example. We approach the problem from two methodological perspectives. First, we analyze the use of emojis and their modifiers from a numerical point of view, counting their occurrences in a corpus. This already gives us important hints of how these emojis are used. Then, we leverage the SW2V (Senses and Words to Vectors) embedding model (Mancini et al., 2017) to train a joint vector space in which emojis and their modifiers are encoded together, enabling us to analyze their semantic interpretation. While there have been approaches attempting to model emojis with distributional semantics (Aoki and Uchida, 2011; Barbieri et al., 2016; Eisner et al., 2016; Ljubeˇsic and Fiˇser, 2016; Wijeratne et al., 2017), to the best of our knowledge this is the first work that semantically analyzes modifiers as well. In fact, even though the information provided by modifiers can be extremely useful (for the modeling of emojis in particular, and of messages in soc"
S18-2011,E17-2017,1,0.794038,"o analyze their semantic interpretation. While there have been approaches attempting to model emojis with distributional semantics (Aoki and Uchida, 2011; Barbieri et al., 2016; Eisner et al., 2016; Ljubeˇsic and Fiˇser, 2016; Wijeratne et al., 2017), to the best of our knowledge this is the first work that semantically analyzes modifiers as well. In fact, even though the information provided by modifiers can be extremely useful (for the modeling of emojis in particular, and of messages in social media in general), this has been neglected by previous approaches modeling and predicting emojis (Barbieri et al., 2017; Felbo et al., 2017). Following our two complementary methodological perspectives, we reached similar conclusions: many stereotypes related to gender and race are also present in this new form of communication. In this paper we analyze the use of emojis in social media with respect to gender and skin tone. By gathering a dataset of over twenty two million tweets from United States some findings are clearly highlighted after performing a simple frequency-based analysis. Moreover, we carry out a semantic analysis on the usage of emojis and their modifiers (e.g. gender and skin tone) by embeddin"
S18-2011,W16-6208,0,0.10885,"Missing"
S18-2011,D17-1323,0,0.223797,"ends to be semantically close to emojis related to business or technology, whereas their female counterparts appear closer to emojis about love or makeup. 1 Introduction Gender and race stereotypes are still present in many places of our lives. These stereotype-based biases are directly reflected on the data that can be gathered from different sources such as visual or textual contents. In fact, it has been shown how these biases can lead to problematic behaviours such as an increase in discrimination (Podesta et al., 2014). These biases have already been studied in diverse text data sources (Zhao et al., 2017), and have been proved to propagate to supervised and unsupervised techniques learning from them, including word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2018) and end-user applications like online ads (Sweeney, 2013). In this paper we study the biases produced in a newer form of communication in social media 101 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 101–106 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics ple, a hand-based emoji (e.g. ) can have different skin colors: light, medi"
S18-2011,D17-1169,0,0.0780735,"c interpretation. While there have been approaches attempting to model emojis with distributional semantics (Aoki and Uchida, 2011; Barbieri et al., 2016; Eisner et al., 2016; Ljubeˇsic and Fiˇser, 2016; Wijeratne et al., 2017), to the best of our knowledge this is the first work that semantically analyzes modifiers as well. In fact, even though the information provided by modifiers can be extremely useful (for the modeling of emojis in particular, and of messages in social media in general), this has been neglected by previous approaches modeling and predicting emojis (Barbieri et al., 2017; Felbo et al., 2017). Following our two complementary methodological perspectives, we reached similar conclusions: many stereotypes related to gender and race are also present in this new form of communication. In this paper we analyze the use of emojis in social media with respect to gender and skin tone. By gathering a dataset of over twenty two million tweets from United States some findings are clearly highlighted after performing a simple frequency-based analysis. Moreover, we carry out a semantic analysis on the usage of emojis and their modifiers (e.g. gender and skin tone) by embedding all words, emojis a"
S18-2011,N18-2003,0,0.0246464,"eotypes are still present in many places of our lives. These stereotype-based biases are directly reflected on the data that can be gathered from different sources such as visual or textual contents. In fact, it has been shown how these biases can lead to problematic behaviours such as an increase in discrimination (Podesta et al., 2014). These biases have already been studied in diverse text data sources (Zhao et al., 2017), and have been proved to propagate to supervised and unsupervised techniques learning from them, including word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2018) and end-user applications like online ads (Sweeney, 2013). In this paper we study the biases produced in a newer form of communication in social media 101 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 101–106 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics ple, a hand-based emoji (e.g. ) can have different skin colors: light, medium-light, medium, medium-dark, or dark. This information has been recently added in the official encoding of emojis1 . At the same time, some emojis like a person rising a hand could be disp"
S19-2091,S19-2007,0,0.0672033,"Missing"
S19-2091,Q17-1010,0,0.0151867,"their frequency in a tweet. • Sentiment analysis: Hate speech and sentiment analysis are closely related, and we can assume that negative sentiment usually pertains to a hate speech message (Schmidt and 3 This was achieved by leveraging the VarianceThreshold tool from scikit-learn (Pedregosa et al., 2011): https://scikit-learn.org/stable/modules/ feature_selection.html 2 We use an extra standard feature to these three groups, the length of the tweet in words. 509 3 Evaluation As far as word embeddings are concerned, we made use of Spanish and English 100-dimensional FastText word embeddings (Bojanowski et al., 2017) trained on two large Twitter corpus from Spain and United States, respectively (Barbieri et al., 2016). In this section we describe the experimental setup (Section 3.1) of our system along with the results obtained (Section 3.2), including a brief analysis of errors detected in the evaluation phase (Section 3.3). 3.1 Parameter tuning. We experimented with several kernels and parameter configurations to train the Support Vector Machines, including polynomial and linear kernels. Since our system is trained with a large amount of features, it is hard to find an optimal parameter configuration fo"
S19-2091,W17-1101,0,0.11508,"Missing"
W16-2508,P14-1023,0,0.0173613,"Missing"
W16-2508,W16-2502,0,0.145044,"uation of these word vector representations (Baroni et al., 2014; Levy et al., 2015). Given a gold standard of human-assigned scores, the usual evaluation procedure consists of calculating the correlation between these human similarity scores and scores calculated by the system. While word similarity has been shown to be an interesting task for measuring the semantic coherence of a vector space model, it suffers from various problems. First, the human inter-annotator agreement of standard datasets has been shown to be relatively too low for it to be considered a reliable evaluation benchmark (Batchkarov et al., 2016). In fact, many systems have already surpassed the human inter-annotator agreement upper bound in most of the standard word similarity datasets (Hill et al., 2015). Another drawback of the word similarity evaluation benchmark is its simplicity, as words are simply viewed as points in the vector space. Other interesting properties of vector space models are not directly addressed in the task. We present a new framework for an intrinsic evaluation of word vector representations based on the outlier detection task. This task is intended to test the capability of vector space models to create sema"
W16-2508,P15-1144,0,0.0075124,"is based on a standard vocabulary question of language exams (Richards, 1976). Given a group of words, the goal is to identify the word that does not belong in the group. This question is intended to test the student’s vocabulary understanding and knowledge of the world. For example, book would be an outlier for the set of words apple, banana, lemon, book, orange, as it is not a fruit like the others. A similar task has already been explored as an ad-hoc evaluation of the interpretability of topic models (Chang et al., 2009) and word vector dimensions (Murphy et al., 2012; Fyshe et al., 2015; Faruqui et al., 2015). In order to deal with the outlier detection task, vector space models should be able to create semantic clusters (i.e. fruits in the example) compact enough to detect all possible outliers. A formalization of the task and its evaluation is presented in Section 2.1 and some potential applications are discussed in Section 2.2. 2.1 P OP P = Accuracy = X X × 100 OD(W ) × 100 |D| W ∈D (2) (3) The compactness score of a word may be expensive to calculate if the number of elements in the cluster is large. In fact, the complexity of calculating OP and OD measures given a cluster and an outlier is (n"
W16-2508,P14-1113,0,0.00941061,"tion (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Word Sense Disambiguation (Patwardhan et al., 2003), to name a few. Furthermore, there are other NLP applications directly connected with the semantic clustering proposed in the outlier detection task. Ontology Learning is probably the most straightforward application, as a meaningful cluster of items is expected to share a common hypernym, a property that has already been exploited in recent studies using embeddings (Fu et al., 2014; Espinosa-Anke et al., 2016). In fact, building ontologies is a time-consuming task and generally relies on automatic or semi-automatic steps (Velardi et al., 2013; Alfarone and Davis, 2015). Ontologies are one of the basic components of the Semantic Web (Berners-Lee et al., 2000) and have already proved their importance in downstream applications like Question Answering (Mann, 2002), Formally, given a set of words W = {w1 , w2 , . . . , wn , wn+1 }, the task consists of identifying the word (outlier) that does not belong to the same group as the remaining words. For notational simplicity, we"
W16-2508,D14-1067,0,0.0312982,"otball teams FC Barcelona Bayern Munich Real Madrid AC Milan Juventus Atletico Madrid Chelsea Borussia Dortmund Miami Dolphins McLaren Los Angeles Lakers Bundesliga football goal couch fridge Solar System planets Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune Sun Moon Triton Comet Halley eclipse astronaut lunch window Months January March May July September November February June Wednesday winter date year astrology birthday ball paper Table 1: First four clusters (including outliers) of the 8-8-8 outlier detection dataset. 3 which in the main rely on large structured knowledge bases (Bordes et al., 2014). In this paper we do not perform any quantitative evaluation to measure the correlation between the performance of word vectors on the outlier detection task and downstream applications. We argue that the conclusions drawn by recent works (Tsvetkov et al., 2015; Chiu et al., 2016) as a result of measuring the correlation between standard intrinsic evaluation benchmarks (e.g. word similarity datasets) and downstream task performances are hampered by a serious methodological issue: in both cases, the sample set of word vectors used for measuring the correlation is not representative enough, whi"
W16-2508,N15-1004,0,0.013363,"tection henceforth, is based on a standard vocabulary question of language exams (Richards, 1976). Given a group of words, the goal is to identify the word that does not belong in the group. This question is intended to test the student’s vocabulary understanding and knowledge of the world. For example, book would be an outlier for the set of words apple, banana, lemon, book, orange, as it is not a fruit like the others. A similar task has already been explored as an ad-hoc evaluation of the interpretability of topic models (Chang et al., 2009) and word vector dimensions (Murphy et al., 2012; Fyshe et al., 2015; Faruqui et al., 2015). In order to deal with the outlier detection task, vector space models should be able to create semantic clusters (i.e. fruits in the example) compact enough to detect all possible outliers. A formalization of the task and its evaluation is presented in Section 2.1 and some potential applications are discussed in Section 2.2. 2.1 P OP P = Accuracy = X X × 100 OD(W ) × 100 |D| W ∈D (2) (3) The compactness score of a word may be expensive to calculate if the number of elements in the cluster is large. In fact, the complexity of calculating OP and OD measures given a clust"
W16-2508,N15-1059,1,0.794675,"ftmax for CBOW and negative sampling for Skip-Gram and GloVe. 46 Model CBOW Skip-Gram GloVe Corpus UMBC Wikipedia UMBC Wikipedia Google News UMBC Wikipedia OPP 93.8 95.3 92.6 93.8 94.7 81.6 91.8 Acc. 73.4 73.4 64.1 70.3 70.3 40.6 56.3 to miss a representation for a given lexicalization if that lexicalization is not found enough times in the corpus6 . In order to overcome these ambiguity and synonymy issues, it might be interesting for future work to leverage vector representations constructed from large lexical resources such, as FreeBase (Bordes et al., 2011; Bordes et al., 2014), Wikipedia (Camacho-Collados et al., 2015a), or BabelNet (Iacobacci et al., 2015; Camacho-Collados et al., 2015b). Table 3: Outlier Position Percentage (OPP) and Accuracy (Acc.) of different word embedding models on the 8-8-8 outlier detection dataset. 4 Conclusion In this paper we presented the outlier detection task and a framework for an intrinsic evaluation of word vector space models. The task is intended to test interesting semantic properties of vector space models not fully addressed to date. As shown in our pilot study, state-of-the-art word embeddings perform reasonably well in the task but are still far from human performa"
W16-2508,S13-1005,0,0.0919158,"Missing"
W16-2508,P15-1072,1,0.703748,"ftmax for CBOW and negative sampling for Skip-Gram and GloVe. 46 Model CBOW Skip-Gram GloVe Corpus UMBC Wikipedia UMBC Wikipedia Google News UMBC Wikipedia OPP 93.8 95.3 92.6 93.8 94.7 81.6 91.8 Acc. 73.4 73.4 64.1 70.3 70.3 40.6 56.3 to miss a representation for a given lexicalization if that lexicalization is not found enough times in the corpus6 . In order to overcome these ambiguity and synonymy issues, it might be interesting for future work to leverage vector representations constructed from large lexical resources such, as FreeBase (Bordes et al., 2011; Bordes et al., 2014), Wikipedia (Camacho-Collados et al., 2015a), or BabelNet (Iacobacci et al., 2015; Camacho-Collados et al., 2015b). Table 3: Outlier Position Percentage (OPP) and Accuracy (Acc.) of different word embedding models on the 8-8-8 outlier detection dataset. 4 Conclusion In this paper we presented the outlier detection task and a framework for an intrinsic evaluation of word vector space models. The task is intended to test interesting semantic properties of vector space models not fully addressed to date. As shown in our pilot study, state-of-the-art word embeddings perform reasonably well in the task but are still far from human performa"
W16-2508,J15-4004,0,0.0358605,"ists of calculating the correlation between these human similarity scores and scores calculated by the system. While word similarity has been shown to be an interesting task for measuring the semantic coherence of a vector space model, it suffers from various problems. First, the human inter-annotator agreement of standard datasets has been shown to be relatively too low for it to be considered a reliable evaluation benchmark (Batchkarov et al., 2016). In fact, many systems have already surpassed the human inter-annotator agreement upper bound in most of the standard word similarity datasets (Hill et al., 2015). Another drawback of the word similarity evaluation benchmark is its simplicity, as words are simply viewed as points in the vector space. Other interesting properties of vector space models are not directly addressed in the task. We present a new framework for an intrinsic evaluation of word vector representations based on the outlier detection task. This task is intended to test the capability of vector space models to create semantic clusters in the space. We carried out a pilot study building a gold standard dataset and the results revealed two important features: human performance on the"
W16-2508,W16-2501,0,0.0792341,"Comet Halley eclipse astronaut lunch window Months January March May July September November February June Wednesday winter date year astrology birthday ball paper Table 1: First four clusters (including outliers) of the 8-8-8 outlier detection dataset. 3 which in the main rely on large structured knowledge bases (Bordes et al., 2014). In this paper we do not perform any quantitative evaluation to measure the correlation between the performance of word vectors on the outlier detection task and downstream applications. We argue that the conclusions drawn by recent works (Tsvetkov et al., 2015; Chiu et al., 2016) as a result of measuring the correlation between standard intrinsic evaluation benchmarks (e.g. word similarity datasets) and downstream task performances are hampered by a serious methodological issue: in both cases, the sample set of word vectors used for measuring the correlation is not representative enough, which is essential for this type of statistical study (Patton, 2005). All sample vectors came from corpus-based models1 trained on the same corpus and all perform well on the considered intrinsic tasks, which constitute a highly homogeneous and not representative sample set. Moreover,"
W16-2508,P12-1092,0,0.090314,"ina Peru Venezuela Chile Ecuador Bolivia Bogot´a Rio de Janeiro New York Madrid town government bottle telephone Table 2: Last four clusters (including outliers) from the 8-8-8 outlier detection dataset. human-assigned scores with a relatively low interannotator agreement. For example, the interannotator agreements in the standard WordSim353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) word similarity datasets were, respectively, 0.61 and 0.67 according to average pair-wise Spearman correlation. In fact, both upper-bound values have already been surpassed by automatic models (Huang et al., 2012; Wieting et al., 2015). expertise. The time spent for the actual creation of a cluster (including outliers) was in all cases less than ten minutes. 3.1 Human performance We assessed the human performance of eight annotators in the task via accuracy. To this end, each annotator was given eight different groups of words, one for each of the topics of the 8-88 dataset. Each group of words was made up of the set of eight words comprising the cluster, plus one additional outlier. All the words were shuffled and given to the annotator without any additional information (e.g. annotators did not know"
W16-2508,W02-0908,0,0.0349216,"ability of vector space models to create semantic clusters in the space. We carried out a pilot study building a gold standard dataset and the results revealed two important features: human performance on the task is extremely high compared to the standard word similarity task, and stateof-the-art word embedding models, whose current shortcomings were highlighted as part of the evaluation, still have considerable room for improvement. 1 Introduction Vector Space Models have been successfully used on many NLP tasks (Turney and Pantel, 2010) such as automatic thesaurus generation (Crouch, 1988; Curran and Moens, 2002), word similarity (Deerwester et al., 1990; Turney et al., 2003; Radinsky et al., 2011) and clustering (Pantel and Lin, 2002), query expansion (Xu and Croft, 1996), information extraction (Laender et al., 2002), semantic role labeling (Erk, 2007; Pennacchiotti et al., 2008), spelling correction (Jones and Martin, 1997), and Word Sense Disambiguation (Navigli, 2012). These models are in the main based on the distributional hypothesis of Harris (1954) claiming that words that occur in the same contexts tend to have similar meanings. Recently, more complex models based on neural networks going be"
W16-2508,P15-1010,1,0.0430078,"Gram and GloVe. 46 Model CBOW Skip-Gram GloVe Corpus UMBC Wikipedia UMBC Wikipedia Google News UMBC Wikipedia OPP 93.8 95.3 92.6 93.8 94.7 81.6 91.8 Acc. 73.4 73.4 64.1 70.3 70.3 40.6 56.3 to miss a representation for a given lexicalization if that lexicalization is not found enough times in the corpus6 . In order to overcome these ambiguity and synonymy issues, it might be interesting for future work to leverage vector representations constructed from large lexical resources such, as FreeBase (Bordes et al., 2011; Bordes et al., 2014), Wikipedia (Camacho-Collados et al., 2015a), or BabelNet (Iacobacci et al., 2015; Camacho-Collados et al., 2015b). Table 3: Outlier Position Percentage (OPP) and Accuracy (Acc.) of different word embedding models on the 8-8-8 outlier detection dataset. 4 Conclusion In this paper we presented the outlier detection task and a framework for an intrinsic evaluation of word vector space models. The task is intended to test interesting semantic properties of vector space models not fully addressed to date. As shown in our pilot study, state-of-the-art word embeddings perform reasonably well in the task but are still far from human performance. As opposed to the word similarity"
W16-2508,A97-1025,0,0.0475522,", whose current shortcomings were highlighted as part of the evaluation, still have considerable room for improvement. 1 Introduction Vector Space Models have been successfully used on many NLP tasks (Turney and Pantel, 2010) such as automatic thesaurus generation (Crouch, 1988; Curran and Moens, 2002), word similarity (Deerwester et al., 1990; Turney et al., 2003; Radinsky et al., 2011) and clustering (Pantel and Lin, 2002), query expansion (Xu and Croft, 1996), information extraction (Laender et al., 2002), semantic role labeling (Erk, 2007; Pennacchiotti et al., 2008), spelling correction (Jones and Martin, 1997), and Word Sense Disambiguation (Navigli, 2012). These models are in the main based on the distributional hypothesis of Harris (1954) claiming that words that occur in the same contexts tend to have similar meanings. Recently, more complex models based on neural networks going beyond simple co-occurrence statistics have been developed (Mikolov et al., 2013; Pennington et al., 2014) and have proved beneficial on key NLP applications such as syntactic parsing (Weiss et al., 2015), Machine Translation (Zou et al., 2013), and As an alternative we propose the outlier detection task, which tests the"
W16-2508,D14-1162,0,0.106051,", 2011) and clustering (Pantel and Lin, 2002), query expansion (Xu and Croft, 1996), information extraction (Laender et al., 2002), semantic role labeling (Erk, 2007; Pennacchiotti et al., 2008), spelling correction (Jones and Martin, 1997), and Word Sense Disambiguation (Navigli, 2012). These models are in the main based on the distributional hypothesis of Harris (1954) claiming that words that occur in the same contexts tend to have similar meanings. Recently, more complex models based on neural networks going beyond simple co-occurrence statistics have been developed (Mikolov et al., 2013; Pennington et al., 2014) and have proved beneficial on key NLP applications such as syntactic parsing (Weiss et al., 2015), Machine Translation (Zou et al., 2013), and As an alternative we propose the outlier detection task, which tests the capability of vector space models to create semantic clusters (i.e. clusters of semantically similar items). As is the case with word similarity, this task aims at evaluating the semantic coherence of vector space models, but providing two main advantages: (1) it provides a clear gold standard, thanks to the high human performance on the task, and (2) it tests an interesting langu"
W16-2508,Q15-1016,0,0.0456006,"Missing"
W16-2508,W02-1111,0,0.0265731,"arning is probably the most straightforward application, as a meaningful cluster of items is expected to share a common hypernym, a property that has already been exploited in recent studies using embeddings (Fu et al., 2014; Espinosa-Anke et al., 2016). In fact, building ontologies is a time-consuming task and generally relies on automatic or semi-automatic steps (Velardi et al., 2013; Alfarone and Davis, 2015). Ontologies are one of the basic components of the Semantic Web (Berners-Lee et al., 2000) and have already proved their importance in downstream applications like Question Answering (Mann, 2002), Formally, given a set of words W = {w1 , w2 , . . . , wn , wn+1 }, the task consists of identifying the word (outlier) that does not belong to the same group as the remaining words. For notational simplicity, we will assume that w1 , ... , wn belong to the same cluster and wn+1 is the outlier. In what follows we explain a procedure for detecting outliers based on semantic similarity. We define the compactness score c(w) of a word w ∈ W as the compactness of the cluster W  {w}, calculated by averaging all pair-wise semantic similarities of the words in W  {w}: 1 k |D| P Formalization c(w) ="
W16-2508,D15-1243,0,0.0190147,"eptune Sun Moon Triton Comet Halley eclipse astronaut lunch window Months January March May July September November February June Wednesday winter date year astrology birthday ball paper Table 1: First four clusters (including outliers) of the 8-8-8 outlier detection dataset. 3 which in the main rely on large structured knowledge bases (Bordes et al., 2014). In this paper we do not perform any quantitative evaluation to measure the correlation between the performance of word vectors on the outlier detection task and downstream applications. We argue that the conclusions drawn by recent works (Tsvetkov et al., 2015; Chiu et al., 2016) as a result of measuring the correlation between standard intrinsic evaluation benchmarks (e.g. word similarity datasets) and downstream task performances are hampered by a serious methodological issue: in both cases, the sample set of word vectors used for measuring the correlation is not representative enough, which is essential for this type of statistical study (Patton, 2005). All sample vectors came from corpus-based models1 trained on the same corpus and all perform well on the considered intrinsic tasks, which constitute a highly homogeneous and not representative s"
W16-2508,P11-1076,0,0.038418,"e proof are included in Appendix A. 2.2 Potential applications In this work we focus on the intrinsic semantic properties of vector space models which can be inferred from the outlier detection task. In addition, since it is a task based partially on semantic similarity, high-performing models in the outlier detection task are expected to contribute to applications in which semantic similarity has already shown its potential: Information Retrieval (Hliaoutakis et al., 2006), Machine Translation (Lavie and Denkowski, 2009), Lexical Substitution (McCarthy and Navigli, 2009), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), and Word Sense Disambiguation (Patwardhan et al., 2003), to name a few. Furthermore, there are other NLP applications directly connected with the semantic clustering proposed in the outlier detection task. Ontology Learning is probably the most straightforward application, as a meaningful cluster of items is expected to share a common hypernym, a property that has already been exploited in recent studies using embeddings (Fu et al., 2014; Espinosa-Anke et al., 2016). In fact, building ontologies is a time-consuming task and generally relies on a"
W16-2508,J13-3007,1,0.11014,"Hirst, 2012), and Word Sense Disambiguation (Patwardhan et al., 2003), to name a few. Furthermore, there are other NLP applications directly connected with the semantic clustering proposed in the outlier detection task. Ontology Learning is probably the most straightforward application, as a meaningful cluster of items is expected to share a common hypernym, a property that has already been exploited in recent studies using embeddings (Fu et al., 2014; Espinosa-Anke et al., 2016). In fact, building ontologies is a time-consuming task and generally relies on automatic or semi-automatic steps (Velardi et al., 2013; Alfarone and Davis, 2015). Ontologies are one of the basic components of the Semantic Web (Berners-Lee et al., 2000) and have already proved their importance in downstream applications like Question Answering (Mann, 2002), Formally, given a set of words W = {w1 , w2 , . . . , wn , wn+1 }, the task consists of identifying the word (outlier) that does not belong to the same group as the remaining words. For notational simplicity, we will assume that w1 , ... , wn belong to the same cluster and wn+1 is the outlier. In what follows we explain a procedure for detecting outliers based on semantic"
W16-2508,C12-1118,0,0.00950283,"rred to as outlier detection henceforth, is based on a standard vocabulary question of language exams (Richards, 1976). Given a group of words, the goal is to identify the word that does not belong in the group. This question is intended to test the student’s vocabulary understanding and knowledge of the world. For example, book would be an outlier for the set of words apple, banana, lemon, book, orange, as it is not a fruit like the others. A similar task has already been explored as an ad-hoc evaluation of the interpretability of topic models (Chang et al., 2009) and word vector dimensions (Murphy et al., 2012; Fyshe et al., 2015; Faruqui et al., 2015). In order to deal with the outlier detection task, vector space models should be able to create semantic clusters (i.e. fruits in the example) compact enough to detect all possible outliers. A formalization of the task and its evaluation is presented in Section 2.1 and some potential applications are discussed in Section 2.2. 2.1 P OP P = Accuracy = X X × 100 OD(W ) × 100 |D| W ∈D (2) (3) The compactness score of a word may be expensive to calculate if the number of elements in the cluster is large. In fact, the complexity of calculating OP and OD me"
W16-2508,P15-1032,0,0.0170111,"tion (Laender et al., 2002), semantic role labeling (Erk, 2007; Pennacchiotti et al., 2008), spelling correction (Jones and Martin, 1997), and Word Sense Disambiguation (Navigli, 2012). These models are in the main based on the distributional hypothesis of Harris (1954) claiming that words that occur in the same contexts tend to have similar meanings. Recently, more complex models based on neural networks going beyond simple co-occurrence statistics have been developed (Mikolov et al., 2013; Pennington et al., 2014) and have proved beneficial on key NLP applications such as syntactic parsing (Weiss et al., 2015), Machine Translation (Zou et al., 2013), and As an alternative we propose the outlier detection task, which tests the capability of vector space models to create semantic clusters (i.e. clusters of semantically similar items). As is the case with word similarity, this task aims at evaluating the semantic coherence of vector space models, but providing two main advantages: (1) it provides a clear gold standard, thanks to the high human performance on the task, and (2) it tests an interesting language understanding property of vector space models not fully addressed to date, and this is their a"
W16-2508,Q15-1025,0,0.00584692,"hile Ecuador Bolivia Bogot´a Rio de Janeiro New York Madrid town government bottle telephone Table 2: Last four clusters (including outliers) from the 8-8-8 outlier detection dataset. human-assigned scores with a relatively low interannotator agreement. For example, the interannotator agreements in the standard WordSim353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) word similarity datasets were, respectively, 0.61 and 0.67 according to average pair-wise Spearman correlation. In fact, both upper-bound values have already been surpassed by automatic models (Huang et al., 2012; Wieting et al., 2015). expertise. The time spent for the actual creation of a cluster (including outliers) was in all cases less than ten minutes. 3.1 Human performance We assessed the human performance of eight annotators in the task via accuracy. To this end, each annotator was given eight different groups of words, one for each of the topics of the 8-88 dataset. Each group of words was made up of the set of eight words comprising the cluster, plus one additional outlier. All the words were shuffled and given to the annotator without any additional information (e.g. annotators did not know the topic of the clust"
W16-2508,D13-1141,0,0.0337486,"labeling (Erk, 2007; Pennacchiotti et al., 2008), spelling correction (Jones and Martin, 1997), and Word Sense Disambiguation (Navigli, 2012). These models are in the main based on the distributional hypothesis of Harris (1954) claiming that words that occur in the same contexts tend to have similar meanings. Recently, more complex models based on neural networks going beyond simple co-occurrence statistics have been developed (Mikolov et al., 2013; Pennington et al., 2014) and have proved beneficial on key NLP applications such as syntactic parsing (Weiss et al., 2015), Machine Translation (Zou et al., 2013), and As an alternative we propose the outlier detection task, which tests the capability of vector space models to create semantic clusters (i.e. clusters of semantically similar items). As is the case with word similarity, this task aims at evaluating the semantic coherence of vector space models, but providing two main advantages: (1) it provides a clear gold standard, thanks to the high human performance on the task, and (2) it tests an interesting language understanding property of vector space models not fully addressed to date, and this is their ability to create semantic clusters in th"
W16-2508,D08-1048,0,\N,Missing
W16-2508,P07-1028,0,\N,Missing
W16-4019,P14-2131,0,0.0128197,"based on word embeddings after translation (MT+W2V). This improvement over the system based on word embeddings may be due to two main factors. First, since the translation is carried out automatically, it may be prompt to errors. Second, even though word embeddings have already shown its potential in obtaining accurate semantic representations of lexical items, they may not be so accurate to model larger semantic units such as documents. In fact, word embeddings are in the main used in tasks which make use of the local context of words, e.g., dependency syntactic parsing (Weiss et al., 2015; Bansal et al., 2014), rather than in tasks requiring the global semantic representations of documents or paragraphs. The results are especially meaningful considering that our system does not require a prior translation step between languages. In fact, obtaining and integrating reliable translation models for all pairs of languages is generally a heavily impractical task (Jones and Irvine, 2013). This is definitely an encouraging 10 https://www.bing.com/translator https://code.google.com/archive/p/word2vec/ 12 Accuracy is computed as the number of times a system retrieves the same chapter in the output language d"
W16-4019,P13-1133,0,0.0217859,"details: http://creativecommons.org/licenses/by/4.0/ 140 Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities (LT4DH), pages 140–147, Osaka, Japan, December 11-17 2016. Figure 1: Sample output disambiguation. 2 Methodology In this section we explain our pipeline for the semantic processing of multilingual corpora. For the semantic processing we rely on BabelNet (Navigli and Ponzetto, 2012), a large multilingual encyclopedic dictionary and semantic network. BabelNet1 integrates various resources such as WordNet (Miller, 1995), Open Multilingual WordNet (Bond and Foster, 2013), Wikipedia, OmegaWiki, Wiktionary and Wikidata, among others. All the aforementioned resources are merged into a very large lexical resource in which equivalent concepts and entities are aggregated from the different resources in a unique instance, called BabelNet synset. Each synset contains all the synonyms and definitions harvested from the respective resources in a range of different languages. In fact, BabelNet includes 271 languages and has already shown its potential in various multilingual and cross-lingual Natural Language Processing applications (Moro et al., 2014; Camacho-Collados"
W16-4019,brugman-etal-2008-common,0,0.0639514,"Missing"
W16-4019,P15-1072,1,0.826334,"and Foster, 2013), Wikipedia, OmegaWiki, Wiktionary and Wikidata, among others. All the aforementioned resources are merged into a very large lexical resource in which equivalent concepts and entities are aggregated from the different resources in a unique instance, called BabelNet synset. Each synset contains all the synonyms and definitions harvested from the respective resources in a range of different languages. In fact, BabelNet includes 271 languages and has already shown its potential in various multilingual and cross-lingual Natural Language Processing applications (Moro et al., 2014; Camacho-Collados et al., 2015; Camacho-Collados et al., 2016b). We propose to use this knowledge base to semantically index large collections of multilingual texts. Our methodology is divided in two main steps: (1) corpus preprocessing including disambiguation and entity linking (Section 2.1) and (2) semantic indexing (Section 2.2). 2.1 Disambiguation and Entity Linking The goal of this step is to associate each content word2 with a unique unambiguous identifier (i.e., a BabelNet synset). First, texts are preprocessed (tokenized, Part-Of-Speech tagged and lemmatized) using Stanford CoreNLP (Manning et al., 2014) and TreeT"
W16-4019,L16-1269,1,0.930336,"megaWiki, Wiktionary and Wikidata, among others. All the aforementioned resources are merged into a very large lexical resource in which equivalent concepts and entities are aggregated from the different resources in a unique instance, called BabelNet synset. Each synset contains all the synonyms and definitions harvested from the respective resources in a range of different languages. In fact, BabelNet includes 271 languages and has already shown its potential in various multilingual and cross-lingual Natural Language Processing applications (Moro et al., 2014; Camacho-Collados et al., 2015; Camacho-Collados et al., 2016b). We propose to use this knowledge base to semantically index large collections of multilingual texts. Our methodology is divided in two main steps: (1) corpus preprocessing including disambiguation and entity linking (Section 2.1) and (2) semantic indexing (Section 2.2). 2.1 Disambiguation and Entity Linking The goal of this step is to associate each content word2 with a unique unambiguous identifier (i.e., a BabelNet synset). First, texts are preprocessed (tokenized, Part-Of-Speech tagged and lemmatized) using Stanford CoreNLP (Manning et al., 2014) and TreeTagger (Schmid, 1994) on the lan"
W16-4019,D14-1110,0,0.0233271,"s the four languages considered in the evaluation. The first baseline system (MT+Jacc.) calculates the similarity between the content words of the output texts after translation by using the Jaccard index. The second baseline (MT+W2V) leverages word embeddings to calculate the similarity between the translated texts. The similarity measure consists of the cosine similarity between the average vector of the content word embeddings of both respective translated texts. This approach based on the centroid vector is often used in the literature to obtain representations of sentences and documents (Chen et al., 2014; Yu et al., 2014). As word embeddings we use the pre-trained Word2Vec (Mikolov et al., 2013) vectors trained on the Google News corpus11 . 4.2.2 Results and Discussion Table 2 shows the accuracy12 results of all comparison systems in the cross-lingual text retrieval task using the Bible as gold standard comparable corpus for four different languages: English, Spanish, French, and Russian. Given the current state of MT systems, the high results obtained by the translationbased system are not surprising. However, our simple system based on inherently imperfect disambiguation achieves comparable"
W16-4019,W13-2715,0,0.0479468,"Missing"
W16-4019,W14-0607,0,0.0278837,"age. Second, occurrences of the same concept/event/entity are often referred to via different lexicalizations (e.g. Louis XIV, Louis the Great and Sun King), which are not captured by keyword-based text retrieval techniques. Finally, these approaches are bound to remain monolingual by nature, limiting their applicability to multilingual corpora, which is growing in interest over the years (Johansson, 2007). There have been recent approaches to automatically link cultural heritage items from text corpora to knowledge bases (Brugman et al., 2008; Fernando and Stevenson, 2012; Hall et al., 2012; Efremova et al., 2014; Poelitz and Bartz, 2014) but without going beyond the monolingual level. In fact, to date most approaches towards the accessibility of cultural heritage content in multiple languages have focused on the generation of natural language content through knowledge bases or via the Semantic Web (Davies, 2009; Dann´ells et al., 2013). Instead, we propose a knowledge-based pipeline for automatically processing multilingual corpora which overcomes all previously mentioned limitations by going beyond standard statistical techniques and keyword-based queries. Our approach is based on the disambiguation"
W16-4019,W12-1014,0,0.0205806,"do not handle the inherent ambiguity within language. Second, occurrences of the same concept/event/entity are often referred to via different lexicalizations (e.g. Louis XIV, Louis the Great and Sun King), which are not captured by keyword-based text retrieval techniques. Finally, these approaches are bound to remain monolingual by nature, limiting their applicability to multilingual corpora, which is growing in interest over the years (Johansson, 2007). There have been recent approaches to automatically link cultural heritage items from text corpora to knowledge bases (Brugman et al., 2008; Fernando and Stevenson, 2012; Hall et al., 2012; Efremova et al., 2014; Poelitz and Bartz, 2014) but without going beyond the monolingual level. In fact, to date most approaches towards the accessibility of cultural heritage content in multiple languages have focused on the generation of natural language content through knowledge bases or via the Semantic Web (Davies, 2009; Dann´ells et al., 2013). Instead, we propose a knowledge-based pipeline for automatically processing multilingual corpora which overcomes all previously mentioned limitations by going beyond standard statistical techniques and keyword-based queries. O"
W16-4019,Q16-1004,0,0.0628735,"Missing"
W16-4019,P14-5010,0,0.00366267,"; Camacho-Collados et al., 2015; Camacho-Collados et al., 2016b). We propose to use this knowledge base to semantically index large collections of multilingual texts. Our methodology is divided in two main steps: (1) corpus preprocessing including disambiguation and entity linking (Section 2.1) and (2) semantic indexing (Section 2.2). 2.1 Disambiguation and Entity Linking The goal of this step is to associate each content word2 with a unique unambiguous identifier (i.e., a BabelNet synset). First, texts are preprocessed (tokenized, Part-Of-Speech tagged and lemmatized) using Stanford CoreNLP (Manning et al., 2014) and TreeTagger (Schmid, 1994) on the languages for which these tools are available. For the remaining languages we rely on the multilingual preprocessing tools integrated in Babelfy. Since the disambiguation is targeted to historical texts, we include a list of stopwords belonging to the archaic form of a given language for the languages for which this list is available. For example, for English we used a list3 including archaic expressions such as thou or ye. These stopwords are therefore not taken into account in the disambiguation process. Then, preprocessed texts4 are disambiguated using"
W16-4019,Q14-1019,1,0.869607,"gual WordNet (Bond and Foster, 2013), Wikipedia, OmegaWiki, Wiktionary and Wikidata, among others. All the aforementioned resources are merged into a very large lexical resource in which equivalent concepts and entities are aggregated from the different resources in a unique instance, called BabelNet synset. Each synset contains all the synonyms and definitions harvested from the respective resources in a range of different languages. In fact, BabelNet includes 271 languages and has already shown its potential in various multilingual and cross-lingual Natural Language Processing applications (Moro et al., 2014; Camacho-Collados et al., 2015; Camacho-Collados et al., 2016b). We propose to use this knowledge base to semantically index large collections of multilingual texts. Our methodology is divided in two main steps: (1) corpus preprocessing including disambiguation and entity linking (Section 2.1) and (2) semantic indexing (Section 2.2). 2.1 Disambiguation and Entity Linking The goal of this step is to associate each content word2 with a unique unambiguous identifier (i.e., a BabelNet synset). First, texts are preprocessed (tokenized, Part-Of-Speech tagged and lemmatized) using Stanford CoreNLP ("
W16-4019,S13-2040,0,0.0734536,"Missing"
W16-4019,W14-0606,0,0.0306956,"s of the same concept/event/entity are often referred to via different lexicalizations (e.g. Louis XIV, Louis the Great and Sun King), which are not captured by keyword-based text retrieval techniques. Finally, these approaches are bound to remain monolingual by nature, limiting their applicability to multilingual corpora, which is growing in interest over the years (Johansson, 2007). There have been recent approaches to automatically link cultural heritage items from text corpora to knowledge bases (Brugman et al., 2008; Fernando and Stevenson, 2012; Hall et al., 2012; Efremova et al., 2014; Poelitz and Bartz, 2014) but without going beyond the monolingual level. In fact, to date most approaches towards the accessibility of cultural heritage content in multiple languages have focused on the generation of natural language content through knowledge bases or via the Semantic Web (Davies, 2009; Dann´ells et al., 2013). Instead, we propose a knowledge-based pipeline for automatically processing multilingual corpora which overcomes all previously mentioned limitations by going beyond standard statistical techniques and keyword-based queries. Our approach is based on the disambiguation of text corpora through a"
W16-4019,P15-1032,0,0.0159748,"esults of the system based on word embeddings after translation (MT+W2V). This improvement over the system based on word embeddings may be due to two main factors. First, since the translation is carried out automatically, it may be prompt to errors. Second, even though word embeddings have already shown its potential in obtaining accurate semantic representations of lexical items, they may not be so accurate to model larger semantic units such as documents. In fact, word embeddings are in the main used in tasks which make use of the local context of words, e.g., dependency syntactic parsing (Weiss et al., 2015; Bansal et al., 2014), rather than in tasks requiring the global semantic representations of documents or paragraphs. The results are especially meaningful considering that our system does not require a prior translation step between languages. In fact, obtaining and integrating reliable translation models for all pairs of languages is generally a heavily impractical task (Jones and Irvine, 2013). This is definitely an encouraging 10 https://www.bing.com/translator https://code.google.com/archive/p/word2vec/ 12 Accuracy is computed as the number of times a system retrieves the same chapter in"
W16-4019,W13-2713,0,\N,Missing
W18-5406,N15-1011,0,0.0200302,"t al., 2016; Kuznetsov and Gurevych, 2018), the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied.2 In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN). CNNs have proven to be effective in a wide range of NLP applications, including text classification tasks such as topic categorization (Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016; Conneau et al., 2017) and polarity detection Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on sta"
W18-5406,P14-1062,0,0.027683,"boxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 40–46 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Due to its simplicity, lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages (Pennington et al., 2014; Faruqui et al., 2015). Despite its desirable property of reducing sparsity and vocabulary size, lowercasing may negatively impact system’s performance by increasing ambiguity. For instance, the Apple company in our example and the apple fruit would be considered as identical entities. (Kalchbrenner et al., 2014; Kim, 2014; Dos Santos and Gatti, 2014; Yin et al., 2017), which are the tasks considered in this work. The goal of our evaluation study is to find answers to the following two questions: 1. Are neural network architectures (in particular CNNs) affected by seemingly small preprocessing decisions in the input text? 2. Does the preprocessing of the embeddings’ underlying training corpus have an impact on the final performance of a state-of-the-art neural network text classifier? 2.2 The process of lemmatizing consists of replacing a given token with its corresponding lemma: According to our exp"
W18-5406,E17-1104,0,0.0210703,"extrinsic performance when integrated into a neural network architecture has remained understudied.2 In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN). CNNs have proven to be effective in a wide range of NLP applications, including text classification tasks such as topic categorization (Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016; Conneau et al., 2017) and polarity detection Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analys"
W18-5406,D14-1181,0,0.22698,"lehvar, 2018). However, while some studies have focused on intrinsically analyzing the role of lemmatization in their underlying training corpus (Ebert et al., 2016; Kuznetsov and Gurevych, 2018), the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied.2 In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN). CNNs have proven to be effective in a wide range of NLP applications, including text classification tasks such as topic categorization (Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016; Conneau et al., 2017) and polarity detection Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (parti"
W18-5406,J15-2004,0,0.013275,"these multiwords, or directly include multiwords along with single words in their pretrained embedding spaces (Mikolov et al., 2013b). Lowercasing This is the simplest preprocessing technique which consists of lowercasing each single token of the input text: apple is asking its manufacturers to move macbook air production to the united states . 41 We considered two tasks for our experiments: topic categorization, i.e. assigning a topic to a given document from a pre-defined set of topics, and polarity detection, i.e. detecting if the sentiment of a given piece of text is positive or negative (Dong et al., 2015). Two different settings were studied: (1) word embedding’s training corpus and the evaluation dataset were preprocessed in a similar manner (Section 3.2); and (2) the two were preprocessed differently (Section 3.3). In what follows we describe the common experimental setting as well as the datasets and preprocessing used for the evaluation. 3.1 Dataset Type TOPIC Evaluation BBC 20News Reuters Ohsumed POLARITY 3 Labels # of docs RTC IMDB PL05 PL04 Stanford Eval. News News News Medical 5 6 8 23 2,225 18,846 9,178 23,166 10-cross Train-test 10-cross Train-test Snippets Reviews Snippets Reviews P"
W18-5406,C18-1020,0,0.0125999,"languages such as Chinese, Japanese and Korean. As opposed to our work, their analysis was focused on UTF-8 bytes, characters, words, romanized characters and romanized words as encoding levels, rather than the preprocessing techniques analyzed in this paper. Additionally, word embeddings have been shown to play an important role in boosting the generalization capabilities of neural systems (Goldberg, 2016; Camacho-Collados and Pilehvar, 2018). However, while some studies have focused on intrinsically analyzing the role of lemmatization in their underlying training corpus (Ebert et al., 2016; Kuznetsov and Gurevych, 2018), the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied.2 In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN). CNNs have proven to be effective in a wide range of NLP applications, including text classification tasks such as topic categorization (Johnson and Zhang, 2015; Tang et al., 2015"
W18-5406,C14-1008,0,0.0184049,"Networks for NLP, pages 40–46 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Due to its simplicity, lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages (Pennington et al., 2014; Faruqui et al., 2015). Despite its desirable property of reducing sparsity and vocabulary size, lowercasing may negatively impact system’s performance by increasing ambiguity. For instance, the Apple company in our example and the apple fruit would be considered as identical entities. (Kalchbrenner et al., 2014; Kim, 2014; Dos Santos and Gatti, 2014; Yin et al., 2017), which are the tasks considered in this work. The goal of our evaluation study is to find answers to the following two questions: 1. Are neural network architectures (in particular CNNs) affected by seemingly small preprocessing decisions in the input text? 2. Does the preprocessing of the embeddings’ underlying training corpus have an impact on the final performance of a state-of-the-art neural network text classifier? 2.2 The process of lemmatizing consists of replacing a given token with its corresponding lemma: According to our experiments in topic categorization and po"
W18-5406,D16-1071,0,0.0488515,"Missing"
W18-5406,N15-1184,0,0.0665178,"Missing"
W18-5406,P16-1191,0,0.0131783,"sh.1 The first module in an NLP pipeline is a tokenizer which transforms texts to sequences of words. However, in practise, other preprocessing techniques can be (and are) further used together with tokenization. These include lemmatization, lowercasing and multiword grouping, among others. Although these preprocessing decisions have 1 Note that although word-based models are mainstream in NLP in general and text classification in particular, recent work has also considered other linguistic units, such as characters (Kim et al., 2016; Xiao and Cho, 2016) or word senses (Li and Jurafsky, 2015; Flekova and Gurevych, 2016; Pilehvar et al., 2017). These techniques require a different kind of preprocessing and, while they have been shown effective in various settings, in this work we only focus on the mainstream word-based models. 2 Not only the preprocessing of the corpus may play an important role but also its nature, domain, etc. Levy et al. (2015) also showed how small hyperparameter variations may have an impact on the performance of word embeddings. However, these considerations remain out of the scope of this paper. 40 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural N"
W18-5406,Q15-1016,0,0.0506403,"that although word-based models are mainstream in NLP in general and text classification in particular, recent work has also considered other linguistic units, such as characters (Kim et al., 2016; Xiao and Cho, 2016) or word senses (Li and Jurafsky, 2015; Flekova and Gurevych, 2016; Pilehvar et al., 2017). These techniques require a different kind of preprocessing and, while they have been shown effective in various settings, in this work we only focus on the mainstream word-based models. 2 Not only the preprocessing of the corpus may play an important role but also its nature, domain, etc. Levy et al. (2015) also showed how small hyperparameter variations may have an impact on the performance of word embeddings. However, these considerations remain out of the scope of this paper. 40 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 40–46 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Due to its simplicity, lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages (Pennington et al., 2014; Faruqui et al., 2015). Despite its desirable property of reducin"
W18-5406,D15-1200,0,0.0264423,"guages, including English.1 The first module in an NLP pipeline is a tokenizer which transforms texts to sequences of words. However, in practise, other preprocessing techniques can be (and are) further used together with tokenization. These include lemmatization, lowercasing and multiword grouping, among others. Although these preprocessing decisions have 1 Note that although word-based models are mainstream in NLP in general and text classification in particular, recent work has also considered other linguistic units, such as characters (Kim et al., 2016; Xiao and Cho, 2016) or word senses (Li and Jurafsky, 2015; Flekova and Gurevych, 2016; Pilehvar et al., 2017). These techniques require a different kind of preprocessing and, while they have been shown effective in various settings, in this work we only focus on the mainstream word-based models. 2 Not only the preprocessing of the corpus may play an important role but also its nature, domain, etc. Levy et al. (2015) also showed how small hyperparameter variations may have an impact on the performance of word embeddings. However, these considerations remain out of the scope of this paper. 40 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzi"
W18-5406,P11-1015,0,0.0752179,"common experimental setting as well as the datasets and preprocessing used for the evaluation. 3.1 Dataset Type TOPIC Evaluation BBC 20News Reuters Ohsumed POLARITY 3 Labels # of docs RTC IMDB PL05 PL04 Stanford Eval. News News News Medical 5 6 8 23 2,225 18,846 9,178 23,166 10-cross Train-test 10-cross Train-test Snippets Reviews Snippets Reviews Phrases 2 2 2 2 2 438,000 50,000 10,662 2,000 119,783 Train-test Train-test 10-cross 10-cross 10-cross Table 1: Evaluation datasets for topic categorization and polarity detection. PL04 (Pang and Lee, 2004), PL058 (Pang and Lee, 2005), RTC9 , IMDB (Maas et al., 2011) and the Stanford sentiment dataset10 (Socher et al., 2013, SF) were considered for polarity detection. Statistics of the versions of the datasets used are displayed in Table 1.11 For both tasks the evaluation was carried out either by 10-fold cross-validation or using the train-test splits of the datasets, in case of availability. Experimental setup We tried with two classification models. The first one is a standard CNN model similar to that of Kim (2014), using ReLU (Nair and Hinton, 2010) as non-linear activation function. In the second model, we add a recurrent layer (specifically an LSTM"
W18-5406,S13-1005,0,0.0156076,"directly to the fully connected softmax layer.3 The inclusion of this LSTM layer has been shown to be able to effectively replace multiple layers of convolution and be beneficial particularly for large inputs (Xiao and Cho, 2016). These models were used for both topic categorization and polarity detection tasks, with slight hyperparameter variations given their different natures (mainly in their text size) which were fixed across all datasets. The embedding layer was initialized using 300-dimensional CBOW Word2vec embeddings (Mikolov et al., 2013a) trained on the 3B-word UMBC WebBase corpus (Han et al., 2013) with standard hyperparameters4 . Preprocessing. Four different techniques (see Section 2) were used to preprocess the datasets as well as the corpus which was used to train word embeddings (i.e. UMBC). For tokenization and lemmatization we relied on Stanford CoreNLP (Manning et al., 2014). As for multiwords, we used the phrases from the pre-trained Google News Word2vec vectors, which were obtained using a simple statistical approach (Mikolov et al., 2013b).12 3.2 Experiment 1: Preprocessing effect Table 2 shows the accuracy13 of the classification models using our four preprocessing technique"
W18-5406,P14-5010,0,0.00325792,"nd polarity detection tasks, with slight hyperparameter variations given their different natures (mainly in their text size) which were fixed across all datasets. The embedding layer was initialized using 300-dimensional CBOW Word2vec embeddings (Mikolov et al., 2013a) trained on the 3B-word UMBC WebBase corpus (Han et al., 2013) with standard hyperparameters4 . Preprocessing. Four different techniques (see Section 2) were used to preprocess the datasets as well as the corpus which was used to train word embeddings (i.e. UMBC). For tokenization and lemmatization we relied on Stanford CoreNLP (Manning et al., 2014). As for multiwords, we used the phrases from the pre-trained Google News Word2vec vectors, which were obtained using a simple statistical approach (Mikolov et al., 2013b).12 3.2 Experiment 1: Preprocessing effect Table 2 shows the accuracy13 of the classification models using our four preprocessing techniques. We observe a certain variability of results depending on the preprocessing techniques used (averEvaluation datasets. For the topic categorization task we used the BBC news dataset5 (Greene and Cunningham, 2006), 20News (Lang, 1995), Reuters6 (Lewis et al., 2004) and Ohsumed7 . 8 Both PL"
W18-5406,W04-3253,0,0.25722,"gorization and polarity detection, these decisions are important in certain cases. Moreover, we shed some light on the motivations of each preprocessing decision and provide some hints on how to normalize the input corpus to better suit each setting. The accompanying materials of this submission can be downloaded at the following repository: https://github.com/pedrada88/ preproc-textclassification. 2 Apple be ask its manufacturer to move MacBook Air production to the United States . Lemmatization has been traditionally a standard preprocessing technique for linear text classification systems (Mullen and Collier, 2004; Toman et al., 2006; Hassan et al., 2007). However, it is rarely used as a preprocessing stage in neuralbased systems. The main idea behind lemmatization is to reduce sparsity, as different inflected forms of the same lemma may occur infrequently (or not at all) during training. However, this may come at the cost of neglecting important syntactic nuances. Text Preprocessing Given an input text, words are gathered as input units of classification models through tokenization. We refer to the corpus which is only tokenized as vanilla. For example, given the sentence “Apple is asking its manufact"
W18-5406,P14-3006,0,0.0606468,"Missing"
W18-5406,P04-1035,0,0.00895054,"sed differently (Section 3.3). In what follows we describe the common experimental setting as well as the datasets and preprocessing used for the evaluation. 3.1 Dataset Type TOPIC Evaluation BBC 20News Reuters Ohsumed POLARITY 3 Labels # of docs RTC IMDB PL05 PL04 Stanford Eval. News News News Medical 5 6 8 23 2,225 18,846 9,178 23,166 10-cross Train-test 10-cross Train-test Snippets Reviews Snippets Reviews Phrases 2 2 2 2 2 438,000 50,000 10,662 2,000 119,783 Train-test Train-test 10-cross 10-cross 10-cross Table 1: Evaluation datasets for topic categorization and polarity detection. PL04 (Pang and Lee, 2004), PL058 (Pang and Lee, 2005), RTC9 , IMDB (Maas et al., 2011) and the Stanford sentiment dataset10 (Socher et al., 2013, SF) were considered for polarity detection. Statistics of the versions of the datasets used are displayed in Table 1.11 For both tasks the evaluation was carried out either by 10-fold cross-validation or using the train-test splits of the datasets, in case of availability. Experimental setup We tried with two classification models. The first one is a standard CNN model similar to that of Kim (2014), using ReLU (Nair and Hinton, 2010) as non-linear activation function. In the"
W18-5406,P05-1015,0,0.129875,"). In what follows we describe the common experimental setting as well as the datasets and preprocessing used for the evaluation. 3.1 Dataset Type TOPIC Evaluation BBC 20News Reuters Ohsumed POLARITY 3 Labels # of docs RTC IMDB PL05 PL04 Stanford Eval. News News News Medical 5 6 8 23 2,225 18,846 9,178 23,166 10-cross Train-test 10-cross Train-test Snippets Reviews Snippets Reviews Phrases 2 2 2 2 2 438,000 50,000 10,662 2,000 119,783 Train-test Train-test 10-cross 10-cross 10-cross Table 1: Evaluation datasets for topic categorization and polarity detection. PL04 (Pang and Lee, 2004), PL058 (Pang and Lee, 2005), RTC9 , IMDB (Maas et al., 2011) and the Stanford sentiment dataset10 (Socher et al., 2013, SF) were considered for polarity detection. Statistics of the versions of the datasets used are displayed in Table 1.11 For both tasks the evaluation was carried out either by 10-fold cross-validation or using the train-test splits of the datasets, in case of availability. Experimental setup We tried with two classification models. The first one is a standard CNN model similar to that of Kim (2014), using ReLU (Nair and Hinton, 2010) as non-linear activation function. In the second model, we add a recu"
W18-5406,D14-1162,0,0.079463,"he corpus may play an important role but also its nature, domain, etc. Levy et al. (2015) also showed how small hyperparameter variations may have an impact on the performance of word embeddings. However, these considerations remain out of the scope of this paper. 40 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 40–46 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Due to its simplicity, lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages (Pennington et al., 2014; Faruqui et al., 2015). Despite its desirable property of reducing sparsity and vocabulary size, lowercasing may negatively impact system’s performance by increasing ambiguity. For instance, the Apple company in our example and the apple fruit would be considered as identical entities. (Kalchbrenner et al., 2014; Kim, 2014; Dos Santos and Gatti, 2014; Yin et al., 2017), which are the tasks considered in this work. The goal of our evaluation study is to find answers to the following two questions: 1. Are neural network architectures (in particular CNNs) affected by seemingly small preprocessin"
W18-5406,P17-1170,1,0.823591,"NLP pipeline is a tokenizer which transforms texts to sequences of words. However, in practise, other preprocessing techniques can be (and are) further used together with tokenization. These include lemmatization, lowercasing and multiword grouping, among others. Although these preprocessing decisions have 1 Note that although word-based models are mainstream in NLP in general and text classification in particular, recent work has also considered other linguistic units, such as characters (Kim et al., 2016; Xiao and Cho, 2016) or word senses (Li and Jurafsky, 2015; Flekova and Gurevych, 2016; Pilehvar et al., 2017). These techniques require a different kind of preprocessing and, while they have been shown effective in various settings, in this work we only focus on the mainstream word-based models. 2 Not only the preprocessing of the corpus may play an important role but also its nature, domain, etc. Levy et al. (2015) also showed how small hyperparameter variations may have an impact on the performance of word embeddings. However, these considerations remain out of the scope of this paper. 40 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 4"
W18-5406,E17-2081,0,0.0216112,"Missing"
W18-5406,P13-1045,0,0.00897806,"preprocessing used for the evaluation. 3.1 Dataset Type TOPIC Evaluation BBC 20News Reuters Ohsumed POLARITY 3 Labels # of docs RTC IMDB PL05 PL04 Stanford Eval. News News News Medical 5 6 8 23 2,225 18,846 9,178 23,166 10-cross Train-test 10-cross Train-test Snippets Reviews Snippets Reviews Phrases 2 2 2 2 2 438,000 50,000 10,662 2,000 119,783 Train-test Train-test 10-cross 10-cross 10-cross Table 1: Evaluation datasets for topic categorization and polarity detection. PL04 (Pang and Lee, 2004), PL058 (Pang and Lee, 2005), RTC9 , IMDB (Maas et al., 2011) and the Stanford sentiment dataset10 (Socher et al., 2013, SF) were considered for polarity detection. Statistics of the versions of the datasets used are displayed in Table 1.11 For both tasks the evaluation was carried out either by 10-fold cross-validation or using the train-test splits of the datasets, in case of availability. Experimental setup We tried with two classification models. The first one is a standard CNN model similar to that of Kim (2014), using ReLU (Nair and Hinton, 2010) as non-linear activation function. In the second model, we add a recurrent layer (specifically an LSTM (Hochreiter and Schmidhuber, 1997)) before passing the po"
W18-5406,D15-1167,0,0.0275623,"d Gurevych, 2018), the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied.2 In this paper we focus on the role of preprocessing the input text, particularly in how it is split into individual (meaning-bearing) tokens and how it affects the performance of standard neural text classification models based on Convolutional Neural Networks (LeCun et al., 2010; Kim, 2014, CNN). CNNs have proven to be effective in a wide range of NLP applications, including text classification tasks such as topic categorization (Johnson and Zhang, 2015; Tang et al., 2015; Xiao and Cho, 2016; Conneau et al., 2017) and polarity detection Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks fr"
