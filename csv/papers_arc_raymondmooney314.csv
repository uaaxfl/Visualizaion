2021.findings-acl.53,{T}ell{M}e{W}hy: A Dataset for Answering Why-Questions in Narratives,2021,-1,-1,3,0,7627,yash lal,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2020.acl-main.168,Learning to Update Natural Language Comments Based on Code Changes,2020,39,0,5,0,22674,sheena panthaplackel,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies. We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications. We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment. We compare our approach against multiple baselines using both automatic metrics and human evaluation. Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits."
2020.aacl-main.49,Systematic Generalization on g{SCAN} with Language Conditioned Embedding,2020,-1,-1,3,0,23238,tong gao,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Systematic Generalization refers to a learning algorithm{'}s ability to extrapolate learned behavior to unseen situations that are distinct but semantically similar to its training data. As shown in recent work, state-of-the-art deep learning models fail dramatically even on tasks for which they are designed when the test set is systematically different from the training data. We hypothesize that explicitly modeling the relations between objects in their contexts while learning their representations will help achieve systematic generalization. Therefore, we propose a novel method that learns objects{'} contextualized embeddings with dynamic message passing conditioned on the input natural language and end-to-end trainable with other downstream deep learning modules. To our knowledge, this model is the first one that significantly outperforms the provided baseline and reaches state-of-the-art performance on grounded SCAN (gSCAN), a grounded natural language navigation dataset designed to require systematic generalization in its test splits."
W19-4807,Do Human Rationales Improve Machine Explanations?,2019,24,1,3,0,24003,julia strout,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"Work on {``}learning with rationales{''} shows that humans providing explanations to a machine learning system can improve the system{'}s predictive accuracy. However, this work has not been connected to work in {``}explainable AI{''} which concerns machines explaining their reasoning to humans. In this work, we show that learning with rationales can also improve the quality of the machine{'}s explanations as evaluated by human judges. Specifically, we present experiments showing that, for CNN-based text classification, explanations generated using {``}supervised attention{''} are judged superior to explanations generated using normal unsupervised attention."
W19-4812,Faithful Multimodal Explanation for Visual Question Answering,2019,0,4,2,0,21269,jialin wu,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"AI systems{'} ability to explain their reasoning is critical to their utility and trustworthiness. Deep neural networks have enabled significant progress on many challenging problems such as visual question answering (VQA). However, most of them are opaque black boxes with limited explanatory capability. This paper presents a novel approach to developing a high-performing VQA system that can elucidate its answers with integrated textual and visual explanations that faithfully reflect important aspects of its underlying reasoning while capturing the style of comprehensible human explanations. Extensive experimental evaluation demonstrates the advantages of this approach compared to competing methods using both automated metrics and human evaluation."
P19-1348,Generating Question Relevant Captions to Aid Visual Question Answering,2019,0,3,3,0,21269,jialin wu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,Visual question answering (VQA) and image captioning require a shared body of general knowledge connecting language and vision. We present a novel approach to better VQA performance that exploits this connection by jointly generating captions that are targeted to help answer a specific visual question. The model is trained using an existing caption dataset by automatically determining question-relevant captions using an online gradient-based method. Experimental results on the VQA v2 challenge demonstrates that our approach obtains state-of-the-art VQA performance (e.g. 68.4{\%} in the Test-standard set using a single model) by simultaneously generating question-relevant captions.
N18-1201,Stacking with Auxiliary Features for Visual Question Answering,2018,0,7,2,1,3344,nazneen rajani,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Visual Question Answering (VQA) is a well-known and challenging task that requires systems to jointly reason about natural language and vision. Deep learning models in various forms have been the standard for solving VQA. However, some of these VQA models are better at certain types of image-question pairs than other models. Ensembling VQA models intelligently to leverage their diverse expertise is, therefore, advantageous. Stacking With Auxiliary Features (SWAF) is an intelligent ensembling technique which learns to combine the results of multiple models using features of the current problem as context. We propose four categories of auxiliary features for ensembling for VQA. Three out of the four categories of features can be inferred from an image-question pair and do not require querying the component models. The fourth category of auxiliary features uses model-specific explanations. In this paper, we describe how we use these various categories of auxiliary features to improve performance for VQA. Using SWAF to effectively ensemble three recent systems, we obtain a new state-of-the-art. Our work also highlights the advantages of explainable AI models."
D18-1165,Learning a Policy for Opportunistic Active Learning,2018,0,3,3,1,1450,aishwarya padmakumar,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Active learning identifies data points to label that are expected to be the most useful in improving a supervised model. Opportunistic active learning incorporates active learning into interactive tasks that constrain possible queries during interactions. Prior work has shown that opportunistic active learning can be used to improve grounding of natural language descriptions in an interactive object retrieval task. In this work, we use reinforcement learning for such an object retrieval task, to learn a policy that effectively trades off task completion with model improvement that would benefit future tasks."
W17-2803,Guiding Interaction Behaviors for Multi-modal Grounded Language Learning,2017,15,9,3,1,19594,jesse thomason,Proceedings of the First Workshop on Language Grounding for Robotics,0,"Multi-modal grounded language learning connects language predicates to physical properties of objects in the world. Sensing with multiple modalities, such as audio, haptics, and visual colors and shapes while performing interaction behaviors like lifting, dropping, and looking on objects enables a robot to ground non-visual predicates like {``}empty{''} as well as visual predicates like {``}red{''}. Previous work has established that grounding in multi-modal space improves performance on object retrieval from human descriptions. In this work, we gather behavior annotations from humans and demonstrate that these improve language grounding performance by allowing a system to focus on relevant behaviors for words like {``}white{''} or {``}half-full{''} that can be understood by looking or lifting, respectively. We also explore adding modality annotations (whether to focus on audio or haptics when performing a behavior), which improves performance, and sharing information between linguistically related predicates (if {``}green{''} is a color, {``}white{''} is a color), which improves grounding recall but at the cost of precision."
I17-2021,Improving Black-box Speech Recognition using Semantic Parsing,2017,16,1,3,0,816,rodolfo corona,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Speech is a natural channel for human-computer interaction in robotics and consumer applications. Natural language understanding pipelines that start with speech can have trouble recovering from speech recognition errors. Black-box automatic speech recognition (ASR) systems, built for general purpose use, are unable to take advantage of in-domain language models that could otherwise ameliorate these errors. In this work, we present a method for re-ranking black-box ASR hypotheses using an in-domain language model and semantic parser trained for a particular task. Our re-ranking method significantly improves both transcription accuracy and semantic understanding over a state-of-the-art ASR{'}s vanilla output."
I17-2030,Dialog for Language to Code,2017,14,7,2,0,32863,shobhit chaurasia,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Generating computer code from natural language descriptions has been a long-standing problem. Prior work in this domain has restricted itself to generating code in one shot from a single description. To overcome this limitation, we propose a system that can engage users in a dialog to clarify their intent until it has all the information to produce correct code. To evaluate the efficacy of dialog in code generation, we focus on synthesizing conditional statements in the form of IFTTT recipes."
I17-1059,Leveraging Discourse Information Effectively for Authorship Attribution,2017,20,0,3,0,3638,elisa ferracane,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"We explore techniques to maximize the effectiveness of discourse information in the task of authorship attribution. We present a novel method to embed discourse features in a Convolutional Neural Network text classifier, which achieves a state-of-the-art result by a significant margin. We empirically investigate several featurization methods to understand the conditions under which discourse features contribute non-trivial performance gains, and analyze discourse embeddings."
E17-1052,Integrated Learning of Dialog Strategies and Semantic Parsing,2017,13,12,3,1,1450,aishwarya padmakumar,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Natural language understanding and dialog management are two integral components of interactive dialog systems. Previous research has used machine learning techniques to individually optimize these components, with different forms of direct and indirect supervision. We present an approach to integrate the learning of both a dialog strategy using reinforcement learning, and a semantic parser for robust natural language understanding, using only natural dialog interaction for supervision. Experimental results on a simulated task of robot instruction demonstrate that joint learning of both components improves dialog performance over learning either of these components alone."
W16-6003,Statistical Script Learning with Recurrent Neural Networks,2016,97,13,2,1,3456,karl pichotta,Proceedings of the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods,0,"Statistical Scripts are probabilistic models of sequences of events. For example, a script model might encode the information that the event xe2x80x9cSmith met with the Presidentxe2x80x9d should strongly predict the event xe2x80x9cSmith spoke to the President.xe2x80x9d We present a number of results improving the state of the art of learning statistical scripts for inferring implicit events. First, we demonstrate that incorporating multiple arguments into events, yielding a more complex event representation than is used in previous work, helps to improve a co-occurrence-based script systemxe2x80x99s predictive power. Second, we improve on these results with a Recurrent Neural Network script sequence model which uses a Long Short-Term Memory component. We evaluate in two ways: first, we evaluate systemsxe2x80x99 ability to infer held-out events from documents (the xe2x80x9cNarrative Clozexe2x80x9d evaluation); second, we evaluate novel event inferences by collecting human judgments. We propose a number of further extensions to this work. First, we propose a number of new probabilistic script models leveraging recent advances in Neural Network training. These include recurrent sequence models with different hidden unit structure and Convolutional Neural Network models. Second, we propose integrating more lexical and linguistic information into events. Third, we propose incorporating discourse relations between spans of text into event co-occurrence models, either as output by an off-the-shelf discourse parser or learned automatically. Finally, we propose investigating the interface between models of event co-occurrence and coreference resolution, in particular by integrating script information into general coreference systems."
P16-1027,Using Sentence-Level {LSTM} Language Models for Script Inference,2016,32,13,2,1,3456,karl pichotta,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on structured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents."
J16-4007,Representing Meaning with a Combination of Logical and Distributional Models,2016,87,25,5,0,34477,beltagy,Computational Linguistics,0,"NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not capture sentence structure in the same detail as logic-based approaches. It has therefore been argued that the two are complementary.n n We adopt a hybrid approach that combines logical and distributional semantics using probabilistic logic, specifically Markov Logic Networks. In this article, we focus on the three components of a practical system:1 1 Logical representation focuses on representing the input problems in probabilistic logic; 2 knowledge base construction creates weighted inference rules by integrating distributional information with other sources; and 3 probabilistic inference involves solving the resulting MLN inference problems efficiently. To evaluate our approach, we use the task of textual entailment, which can utilize the strengths of both logic-based and distributional representations. In particular we focus on the SICK data set, where we achieve state-of-the-art results. We also release a lexical entailment data set of 10,213 rules extracted from the SICK data set, which is a valuable resource for evaluating lexical entailment systems.2"
D16-1201,Combining Supervised and Unsupervised Enembles for Knowledge Base Population,2016,8,7,2,1,3344,nazneen rajani,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1204,Improving {LSTM}-based Video Description with Linguistic Knowledge Mined from Text,2016,35,41,3,1,35603,subhashini venugopalan,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality."
P15-1018,Stacked Ensembles of Information Extractors for Knowledge-Base Population,2015,22,15,4,0,37479,vidhoon viswanathan,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We present results on using stacking to ensemble multiple systems for the Knowledge Base Population English Slot Filling (KBP-ESF) task. In addition to using the output and confidence of each system as input to the stacked classifier, we also use features capturing how well the systems agree about the provenance of the information they extract. We demonstrate that our stacking approach outperforms the best system from the 2014 KBPESF competition as well as alternative ensembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines. Additionally, we demonstrate that including provenance information further increases the performance of stacking."
P15-1085,Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes,2015,24,80,2,0,4460,chris quirk,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Using natural language to write programs is a touchstone problem for computational linguistics. We present an approach that learns to map natural-language descriptions of simple xe2x80x9cif-thenxe2x80x9d rules to executable code. By training and testing on a large corpus of naturally-occurring programs (called xe2x80x9crecipesxe2x80x9d) and their natural language descriptions, we demonstrate the ability to effectively map language to code. We compare a number of semantic parsing approaches on the highly noisy training data collected from ordinary users, and find that loosely synchronous systems perform best."
N15-1173,Translating Videos to Natural Language Using Deep Recurrent Neural Networks,2015,44,478,5,1,35603,subhashini venugopalan,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M images with category labels and 100,000 images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation."
W14-2402,Semantic Parsing using Distributional Semantics and Probabilistic Logic,2014,22,15,3,1,37162,islam beltagy,Proceedings of the {ACL} 2014 Workshop on Semantic Parsing,0,"We propose a new approach to semantic parsing that is not constrained by a fixed formal ontology and purely logical inference. Instead, we use distributional semantics to generate only the relevant part of an on-the-fly ontology. Sentences and the on-the-fly ontology are represented in probabilistic logic. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) and Probabilistic Soft Logic (PSL). This semantic parsing approach is evaluated on two tasks, Textual Entitlement (RTE) and Textual Similarity (STS), both accomplished using inference in probabilistic logic. Experiments show the potential of the approach."
S14-2141,{UT}exas: Natural Language Semantics using Distributional Semantics and Probabilistic Logic,2014,25,12,5,1,37162,islam beltagy,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We represent natural language semantics by combining logical and distributional information in probabilistic logic. We use Markov Logic Networks (MLN) for the RTE task, and Probabilistic Soft Logic (PSL) for the STS task. The system is evaluated on the SICK dataset. Our best system achieves 73% accuracy on the RTE task, and a Pearsonxe2x80x99s correlation of 0.71 on the STS task."
P14-1114,Probabilistic Soft Logic for Semantic Textual Similarity,2014,28,43,3,1,37162,islam beltagy,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Probabilistic Soft Logic (PSL) is a recently developed framework for probabilistic logic. We use PSL to combine logical and distributional representations of natural-language meaning, where distributional information is represented in the form of weighted inference rules. We apply this framework to the task of Semantic Textual Similarity (STS) (i.e. judging the semantic similarity of naturallanguage sentences), and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks (MLNs) and a purely distributional approach."
E14-1024,Statistical Script Learning with Multi-Argument Events,2014,33,49,2,1,3456,karl pichotta,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Scripts represent knowledge of stereotypical event sequences that can aid text understanding. Initial statistical methods have been developed to learn probabilistic scripts from raw text corpora; however, they utilize a very impoverished representation of events, consisting of a verb and one dependent argument. We present a script learning approach that employs events with multiple arguments. Unlike previous work, we model the interactions between multiple entities in a script. Experiments on a large corpus using the task of inferring held-out events (the xe2x80x9cnarrative cloze evaluationxe2x80x9d) demonstrate that modeling multi-argument events improves predictive accuracy."
C14-1115,Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild,2014,28,138,5,1,19594,jesse thomason,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper integrates techniques in natural language processing and computer vision to improve recognition and description of entities and activities in real-world videos. We propose a strategy for generating textual descriptions of videos by using a factor graph to combine visual detections with language statistics. We use state-of-the-art visual recognition systems to obtain confidences on entities, activities, and scenes present in the video. Our factor graph model combines these detection confidences with probabilistic knowledge mined from text corpora to estimate the most likely subject, verb, object, and place. Results on YouTube videos show that our approach improves both the joint detection of these latent, diverse sentence components and the detection of some individual components when compared to using the vision system alone, as well as over a previous n-gram language-modeling approach. The joint detection allows us to automatically generate more accurate, richer sentential descriptions of videos with a wide array of possible content."
W13-1302,Generating Natural-Language Video Descriptions Using Text-Mined Knowledge,2013,34,133,3,0,41091,niveda krishnamoorthy,Proceedings of the Workshop on Vision and Natural Language Processing,0,"We present a holistic data-driven technique that generates natural-language descriptions for videos. We combine the output of state-of-the-art object and activity detectors with real-world knowledge to select the most probable subject-verb-object triplet for describing a video. We show that this knowledge, automatically mined from web-scale text corpora, enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification. Unlike previous methods, our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus. We evaluate our technique against a baseline that does not use text-mined knowledge and show that humans prefer our descriptions 61% of the time."
S13-1002,{M}ontague Meets {M}arkov: Deep Semantics with Probabilistic Logical Form,2013,42,57,6,1,37162,islam beltagy,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"We combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks (MLNs). We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives. We also show that distributional phrase similarity, used as textual inference rules created on the fly, improves its performance."
P13-1022,Adapting Discriminative Reranking to Grounded Language Learning,2013,27,26,2,1,41472,joohyun kim,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We adapt discriminative reranking to improve the performance of grounded language acquisition, specifically the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded setting. Instead, we show how the weak supervision of response feedback (e.g. successful task completion) can be used as an alternative, experimentally demonstrating that its performance is comparable to training on gold-standard parse trees."
D13-1190,Detecting Promotional Content in {W}ikipedia,2013,22,2,3,0,13892,shruti bhosale,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents an approach for detecting promotional content in Wikipedia. By incorporating stylometric features, including features based on n-gram and PCFG language models, we demonstrate improved accuracy at identifying promotional articles, compared to using only lexical information and metafeatures."
P12-1037,Learning to {``}Read Between the Lines{''} using {B}ayesian Logic Programs,2012,27,12,2,1,42685,sindhu raghavan,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Most information extraction (IE) systems identify facts that are explicitly stated in text. However, in natural language, some facts are implicit, and identifying them requires reading between the lines. Human readers naturally use common sense knowledge to infer such implicit information from the explicitly stated facts. We propose an approach that uses Bayesian Logic Programs (BLPs), a statistical relational model combining firstorder logic and Bayesian networks, to infer additional implicit information from extracted facts. It involves learning uncertain common-sense knowledge (in the form of probabilistic first-order rules) from natural language text by mining a large corpus of automatically extracted facts. These rules are then used to derive additional facts from extracted information using BLP inference. Experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach."
E12-1061,Learning Language from Perceptual Context,2012,73,4,1,1,7628,raymond mooney,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Current systems that learn to process natural language require laboriously constructed human-annotated training data. Ideally, a computer would be able to acquire language like a child by being exposed to linguistic input in the context of a relevant but ambiguous perceptual environment. As a step in this direction, we present a system that learns to sportscast simulated robot soccer games by example. The training data consists of textual human commentaries on Robocup simulation games. A set of possible alternative meanings for each comment is automatically constructed from game event traces. Our previously developed systems for learning to parse and generate natural language (KRISP and WASP) were augmented to learn from this data and then commentate novel games. The system is evaluated based on its ability to parse sentences into correct meanings and generate accurate descriptions of game events. Human evaluation was also conducted on the overall quality of the generated sportscasts and compared to human-generated commentaries."
D12-1040,Unsupervised {PCFG} Induction for Grounded Language Learning with Highly Ambiguous Supervision,2012,25,40,2,1,41472,joohyun kim,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Grounded language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts. Borschinger et al. (2011) introduced an approach to grounded language learning based on unsupervised PCFG induction. Their approach works well when each sentence potentially refers to one of a small set of possible meanings, such as in the sportscasting task. However, it does not scale to problems with a large set of potential meanings for each sentence, such as the navigation instruction following task studied by Chen and Mooney (2011). This paper presents an enhancement of the PCFG approach that scales to such problems with highly-ambiguous supervision. Experimental results on the navigation task demonstrates the effectiveness of our approach."
W11-0107,Implementing Weighted Abduction in {M}arkov {L}ogic,2011,-1,-1,5,0,44451,james blythe,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,None
W11-0112,Integrating Logical Representations with Probabilistic Information using {M}arkov {L}ogic,2011,17,48,3,0,8780,dan garrette,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"First-order logic provides a powerful and flexible mechanism for representing natural language semantics. However, it is an open question of how best to integrate it with uncertain, probabilistic knowledge, for example regarding word meaning. This paper describes the first steps of an approach to recasting first-order semantics into the probabilistic models that are part of Statistical Relational AI. Specifically, we show how Discourse Representation Structures can be combined with distributional models for word meaning inside a Markov Logic Network and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic information on word meaning in context."
D11-1130,Cross-Cutting Models of Lexical Semantics,2011,24,6,2,1,44686,joseph reisinger,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Context-dependent word similarity can be measured over multiple cross-cutting dimensions. For example, lung and breath are similar thematically, while authoritative and superficial occur in similar syntactic contexts, but share little semantic similarity. Both of these notions of similarity play a role in determining word meaning, and hence lexical semantic models must take them both into account. Towards this end, we develop a novel model, Multi-View Mixture (MVM), that represents words as multiple overlapping clusterings. MVM finds multiple data partitions based on different subsets of features, subject to the marginal constraint that feature subsets are distributed according to Latent Dirichlet Allocation. Intuitively, this constraint favors feature partitions that have coherent topical semantics. Furthermore, MVM uses soft feature assignment, hence the contribution of each data point to each clustering view is variable, isolating the impact of data only to views where they assign the most features. Through a series of experiments, we demonstrate the utility of MVM as an inductive bias for capturing relations between words that are intuitive to humans, outperforming related models such as Latent Dirichlet Allocation."
W10-2924,Joint Entity and Relation Extraction Using Card-Pyramid Parsing,2010,23,50,2,0.952381,37230,rohit kate,Proceedings of the Fourteenth Conference on Computational Natural Language Learning,0,"Both entity and relation extraction can benefit from being performed jointly, allowing each task to correct the errors of the other. We present a new method for joint entity and relation extraction using a graph we call a card-pyramid. This graph compactly encodes all possible entities and relations in a sentence, reducing the task of their joint extraction to jointly labeling its nodes. We give an efficient labeling algorithm that is analogous to parsing using dynamic programming. Experimental results show improved results for our joint extraction method compared to a pipelined approach."
P10-2008,Authorship Attribution Using Probabilistic Context-Free Grammars,2010,17,63,3,1,42685,sindhu raghavan,Proceedings of the {ACL} 2010 Conference Short Papers,0,"In this paper, we present a novel approach for authorship attribution, the task of identifying the author of a document, using probabilistic context-free grammars. Our approach involves building a probabilistic context-free grammar for each author and using this grammar as a language model for classification. We evaluate the performance of our method on a wide range of datasets to demonstrate its efficacy."
N10-1013,Multi-Prototype Vector-Space Models of Word Meaning,2010,32,227,2,1,44686,joseph reisinger,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Current vector-space models of lexical semantics create a single prototype vector to represent the meaning of a word. However, due to lexical ambiguity, encoding word meaning with a single vector is problematic. This paper presents a method that uses clustering to produce multiple sense-specific vectors for each word. This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy. Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models."
D10-1114,A Mixture Model with Sharing for Lexical Semantics,2010,36,39,2,1,44686,joseph reisinger,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We introduce tiered clustering, a mixture model capable of accounting for varying degrees of shared (context-independent) feature structure, and demonstrate its applicability to inferring distributed representations of word meaning. Common tasks in lexical semantics such as word relatedness or selectional preference can benefit from modeling such structure: Polysemous word usage is often governed by some common background metaphoric usage (e.g. the senses of line or run), and likewise modeling the selectional preference of verbs relies on identifying commonalities shared by their typical arguments. Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded. We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental."
C10-2062,Generative Alignment and Semantic Parsing for Learning from Ambiguous Supervision,2010,14,50,2,1,41472,joohyun kim,Coling 2010: Posters,0,"We present a probabilistic generative model for learning semantic parsers from ambiguous supervision. Our approach learns from natural language sentences paired with world states consisting of multiple potential logical meaning representations. It disambiguates the meaning of each sentence while simultaneously learning a semantic parser that maps sentences into logical form. Compared to a previous generative model for semantic alignment, it also supports full semantic parsing. Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators."
C10-1062,Learning to Predict Readability using Diverse Linguistic Features,2010,25,63,6,0.952381,37230,rohit kate,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,In this paper we consider the problem of building a system to predict readability of natural-language documents. Our system is trained using diverse features based on syntax and language models which are generally indicative of readability. The experimental results on a dataset of documents from a mix of genres show that the predictions of the learned system are more accurate than the predictions of naive human judges when compared against the predictions of linguistically-trained expert human judges. The experiments also compare the performances of different learning algorithms and different types of feature sets when used for predicting readability.
P09-1069,Learning a Compositional Semantic Parser using an Existing Syntactic Parser,2009,22,47,2,1,47219,ruifang ge,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"We present a new approach to learning a semantic parser (a system that maps natural language sentences into logical form). Unlike previous methods, it exploits an existing syntactic parser to produce disambiguated parse trees that drive the compositional semantic interpretation. The resulting system produces improved results on standard corpora on natural language interfaces for database querying and simulated robot control."
P07-1121,Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus,2007,18,247,2,1,45635,yuk wong,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with xefxbfxbdoperators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the bestperforming system so far in a database query domain."
P07-1073,Learning to Extract Relations from the Web using Minimal Supervision,2007,16,232,2,1,26259,razvan bunescu,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We present a new approach to relation extraction that requires only a handful of training examples. Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web. We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents."
N07-2021,Semi-Supervised Learning for Semantic Parsing using Support Vector Machines,2007,15,25,2,0.952381,37230,rohit kate,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"We present a method for utilizing unan-notated sentences to improve a semantic parser which maps natural language (NL) sentences into their formal meaning representations (MRs). Given NL sentences annotated with their MRs, the initial supervised semantic parser learns the mapping by training Support Vector Machine (SVM) classifiers for every production in the MR grammar. Our new method applies the learned semantic parser to the unannotated sentences and collects unla-beled examples which are then used to retrain the classifiers using a variant of transductive SVMs. Experimental results show the improvements obtained over the purely supervised parser, particularly when the annotated training set is small."
N07-1022,Generation by Inverting a Semantic Parser that Uses Statistical Machine Translation,2007,31,89,2,1,45635,yuk wong,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"This paper explores the use of statistical machine translation (SMT) methods for tactical natural language generation. We present results on using phrase-based SMT for learning to map meaning representations to natural language. Improved results are obtained by inverting a semantic parser that uses SMT methods to map sentences into meaning representations. Finally, we show that hybridizing these two approaches results in still more accurate generation systems. Automatic and human evaluation of generated sentences are presented across two domains and four languages."
W06-3307,Integrating Co-occurrence Statistics with Information Extraction for Robust Retrieval of Protein Interactions from {M}edline,2006,21,38,2,1,26259,razvan bunescu,Proceedings of the {HLT}-{NAACL} {B}io{NLP} Workshop on Linking Natural Language and Biology,0,"The task of mining relations from collections of documents is usually approached in two different ways. One type of systems do relation extraction from individual sentences, followed by an aggregation of the results over the entire collection. Other systems follow an entirely different approach, in which co-occurrence counts are used to determine whether the mentioning together of two entities is due to more than simple chance. We show that increased extraction performance can be obtained by combining the two approaches into an integrated relation extraction model."
P06-2034,Discriminative Reranking for Semantic Parsing,2006,20,47,2,1,47219,ruifang ge,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Semantic parsing is the task of mapping natural language sentences to complete formal meaning representations. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. In this paper, we investigate discriminative reranking upon a baseline semantic parser, SCISSOR, where the composition of meaning representations is guided by syntax. We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics. We report experimental results on two real applications, an interpreter for coaching instructions in robotic soccer and a natural-language database interface. The results show that reranking can improve the performance on the coaching interpreter, but not on the database interface."
P06-1115,Using String-Kernels for Learning Semantic Parsers,2006,17,196,2,0.952381,37230,rohit kate,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,We present a new approach for mapping natural language sentences to their formal meaning representations using string-kernel-based classifiers. Our system learns these classifiers for every production in the formal language grammar. Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers. Our experiments on two real-world data sets show that this approach compares favorably to other existing systems and is particularly robust to noise.
N06-1056,Learning for Semantic Parsing with Statistical Machine Translation,2006,24,206,2,1,45635,yuk wong,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning representations. The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order."
W05-1307,Using Biomedical Literature Mining to Consolidate the Set of Known Human Protein-Protein Interactions,2005,26,9,3,0,49623,arun ramani,"Proceedings of the {ACL}-{ISMB} Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics",0,"This paper presents the results of a large-scale effort to construct a comprehensive database of known human protein interactions by combining and linking known interactions from existing databases and then adding to them by automatically mining additional interactions from 750,000 Medline abstracts. The end result is a network of 31,609 interactions amongst 7,748 proteins. The text mining system first identifies protein names in the text using a trained Conditional Random Field (CRF) and then identifies interactions through a filtered co-citation analysis. We also report two new strategies for mining interactions, either by finding explicit statements of interactions in the text using learned pattern-based rules or a Support-Vector Machine using a string kernel. Using information in existing ontologies, the automatically extracted data is shown to be of equivalent accuracy to manually curated data sets."
W05-0602,A Statistical Semantic Parser that Integrates Syntax and Semantics,2005,15,146,2,1,47219,ruifang ge,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"We introduce a learning semantic parser, Scissor, that maps natural-language sentences to a detailed, formal, meaning-representation language. It first uses an integrated statistical parser to produce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We present experimental results demonstrating that Scissor produces more accurate semantic representations than several previous approaches."
H05-1091,A Shortest Path Dependency Kernel for Relation Extraction,2005,13,665,2,1,26259,razvan bunescu,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph. Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels."
P04-1056,Collective Information Extraction with Relational {M}arkov Networks,2004,17,134,2,1,26259,razvan bunescu,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"Most information extraction (IE) systems treat separate potential extractions as independent. However, in many cases, considering influences between different potential extractions could improve overall accuracy. Statistical methods based on undirected graphical models, such as conditional random fields (CRFs), have been shown to be an effective approach to learning accurate IE systems. We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions. This allows for collective information extraction that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach."
W00-1317,Automated Construction of Database Interfaces: Intergrating Statistical and Relational Learning for Semantic Parsing,2000,13,44,2,0,54210,lappoon tang,2000 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,The development of natural language interfaces (NLI's) for databases has been a challenging problem in natural language processing (NLP) since the 1970's. The need for NLI's has become more pronounced due to the widespread access to complex databases now available through the Internet. A challenging problem for empirical NLP is the automated acquisition of NLI's from training examples. We present a method for integrating statistical and relational learning techniques for this task which exploits the strength of both approaches. Experimental results from three different domains suggest that such an approach is more robust than a previous purely logic-based approach.
W98-1107,Semantic Lexicon Acquisition for Learning Natural Language Interfaces,1998,23,11,2,0,38564,cynthia thompson,Sixth Workshop on Very Large Corpora,0,"A long-standing goal for the field of artificial intelligence is to enable computer understanding of human languages. A core requirement in reaching this goal is the ability to transform individual sentences into a form better suited for computer manipulation. This ability, called semantic parsing, requires several knowledge sources, such as a grammar, lexicon, and parsing mechanism. n Building natural language parsing systems by hand is a tedious, error-prone undertaking. We build on previous research in automating the construction of such systems using machine learning techniques. The result is a combined system that learns semantic lexicons and semantic parsers from one common set of training examples. The input required is a corpus of sentence/representation pairs, where the representations are in the output format desired. A new system, WOLFIE, learns semantic lexicons to be used as background knowledge by a previously developed parser acquisition system, CHILL. The combined system is tested on a real world domain of answering database queries. We also compare this combination to a combination of CHILL with a previously developed lexicon learner, demonstrating superior performance with our system. In addition, we show the ability of the system to learn to process natural languages other than English. Finally, we test the system on an alternate sentence representation, and on a set of large, artificial corpora with varying levels of ambiguity and synonymy. n One difficulty in using machine learning methods for building natural language interfaces is building the required annotated corpus. Therefore, we also address this issue by using active learning to reduce the number of training examples required by both WOLFIE and CHILL. Experimental results show that the number of examples needed to reach a given level of performance can be significantly reduced with this method."
W97-1002,Relational Learning of Pattern-Match Rules for Information Extraction,1997,13,30,2,0,55523,mary califf,{C}o{NLL}97: Computational Natural Language Learning,0,None
P97-1062,Learning Parse and Translation Decisions from Examples with Rich Context,1997,53,55,2,0,28971,ulf hermjakob,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal. Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a deterministic shift-reduce parser in the form of a decision structure. It relies heavily on context, as encoded in features which describe the morphological, syntactic, semantic and other aspects of a given parse state."
W96-0208,Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning,1996,26,201,1,1,7628,raymond mooney,Conference on Empirical Methods in Natural Language Processing,0,None
