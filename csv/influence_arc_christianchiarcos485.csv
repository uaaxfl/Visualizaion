2016.jeptalnrecital-invite.1,L16-1386,1,0.68302,"Missing"
2016.jeptalnrecital-invite.1,L16-1234,1,0.800005,"Missing"
2020.coling-main.308,W11-2123,0,0.0181983,"olingual it seems plausible to employ methods of phrasebased machine translation (Lample et al., 2018). For the English monolingual data, we used the Europarl data set (Koehn, 2005), and first created a bilingual dictionary leveraging the independent monolingual data sets by aligning a monolingual word embedding space in an unsupervised way as described by Conneau et al. (2017). Using this bilingual dictionary we populated the phrase tables for Sumerian to English and English to Sumerian. Then, we trained n-gram language models for the Sumerian and English domain using the methods outlined in Heafield (2011). In a later step, we improved these translation models using iterative back-translation (He et al., 2016). 4.5 Transfer Learning Supervised machine translation relies on massive amounts of data, hence typically performs poorly on low resource languages. The idea of transfer learning (Zoph et al., 2016) is to train a machine translation model in a high-resource language setting, e.g., from French to English as a parent model and then initializing the training constraints using the parent model and apply it to the child model. In our experimentation, we first trained a French to English model o"
2020.coling-main.308,W11-0315,0,0.0808709,"Missing"
2020.coling-main.308,P17-4012,0,0.0235896,"a machine translation model in a high-resource language setting, e.g., from French to English as a parent model and then initializing the training constraints using the parent model and apply it to the child model. In our experimentation, we first trained a French to English model on the Europarl Corpus using transformers, then trained our child model from Sumerian to English. The training procedure for the French–English model is identical to the one outlined in Section 4.3. 5 Results & Evaluation All supervised models and experiments described in this paper were implemented using OpenNMT3 (Klein et al., 2017). For the phrase-based and transfer learning techniques, we used FairSeq (Ott et al., 2019). All translation models described in the previous section were trained, tuned, and evaluated on the same standardized training, development and test splits, respectively. First, we calculated BLEU scores (Papineni et al., 2002) for Sumerian translations against the gold data using various settings. The best results obtained are shown in the second column of Table 1. Moreover, in a qualitative evaluation, two experts in Sumerian rated 50 randomly chosen translations from each model, using the following s"
2020.coling-main.308,2005.mtsummit-papers.11,0,0.0874722,"e position-wise feed forward networks with residual connections are employed between every two sub-layers. Finally, the input to the decoder phase is the output embedding and the positional encoding using a similar grouping of stacks of multi-head self-attention layers. The decoder generates one word at a time greedily in a left-to-right fashion. 4.4 Phrase-Based Machine Translation As a large portion of our raw data set is monolingual it seems plausible to employ methods of phrasebased machine translation (Lample et al., 2018). For the English monolingual data, we used the Europarl data set (Koehn, 2005), and first created a bilingual dictionary leveraging the independent monolingual data sets by aligning a monolingual word embedding space in an unsupervised way as described by Conneau et al. (2017). Using this bilingual dictionary we populated the phrase tables for Sumerian to English and English to Sumerian. Then, we trained n-gram language models for the Sumerian and English domain using the methods outlined in Heafield (2011). In a later step, we improved these translation models using iterative back-translation (He et al., 2016). 4.5 Transfer Learning Supervised machine translation relie"
2020.coling-main.308,N15-1167,0,0.20282,"opotamian languages, including economists, historians, or linguists, as well as researchers working on ancient languages, for whom the manual translation of these texts is hardly possible. 2 Related Work Aside from earlier work of the authors Pag´e-Perron et al. (2017), we are not aware of any attempt to apply machine translation to cuneiform languages. However, the field does have a tradition with dictionarybased glossing of transliterated text. Similar to technologies commonly used in language documentation and linguistic typology (Robinson et al., 2007), the ORACC Lemmatizer (Robson, 2018; Liu et al., 2015) can provide word-by-word glosses along with a morphological analysis, albeit without contextual disambiguation, and without producing coherent text. 3455 3 Data & Preprocessing We work with the Ur-III corpus provided by the Cuneiform Digital Library Initiative1 as part of the project Machine Translation and Automated Analysis of Cuneiform Languages (MTAAC, 2017-2020). The Cuneiform Digital Library, founded in 1998, represents the central hub for digital philological data in Assyriology, and provides records for more than 340,000 cuneiform objects, out of which 120,000 come with transcriptions"
2020.coling-main.308,D15-1166,0,0.0185733,"and |VE |=3,146 for Sumerian and English, respectively. The mean length of Sumerian and English phrases is rather short with 2.8 and 4.4 tokens, respectively. 4 Training MT Systems for Sumerian Previous research pointed out that machine translation models suffer from issues related to polysemy and multiple word senses (Calvo et al., 2019; Huang et al., 2011). To tackle these, we experimented with embeddings which we trained on our own small domain of English translations, as well as different pretrained word embeddings. Different attention designs such as global and local attention networks (Luong et al., 2015) and multi-head attention networks (Hans and Milton, 2016) were also subject for experimentation in order to test the efficiency on different sequence lengths. Overall, we experimented with several neural machine translation models, incl. phrase-based MT and transfer learning and implemented: a Base Translator with custom in-domain trained embeddings, an Extended Translator using pretrained embeddings, and a Transformer Translator (Vaswani et al., 2017). We believe that the latter is beneficial regarding the out-of-vocabulary and polysemy issues described above, which is an inherent problem in"
2020.coling-main.308,N19-4009,0,0.022644,"h as a parent model and then initializing the training constraints using the parent model and apply it to the child model. In our experimentation, we first trained a French to English model on the Europarl Corpus using transformers, then trained our child model from Sumerian to English. The training procedure for the French–English model is identical to the one outlined in Section 4.3. 5 Results & Evaluation All supervised models and experiments described in this paper were implemented using OpenNMT3 (Klein et al., 2017). For the phrase-based and transfer learning techniques, we used FairSeq (Ott et al., 2019). All translation models described in the previous section were trained, tuned, and evaluated on the same standardized training, development and test splits, respectively. First, we calculated BLEU scores (Papineni et al., 2002) for Sumerian translations against the gold data using various settings. The best results obtained are shown in the second column of Table 1. Moreover, in a qualitative evaluation, two experts in Sumerian rated 50 randomly chosen translations from each model, using the following scored ranking schema: good [3], helpful [2], incorrect [1] with exact definitions given in"
2020.coling-main.308,W17-2202,1,0.904975,"Missing"
2020.coling-main.308,P02-1040,0,0.109997,"sformers, then trained our child model from Sumerian to English. The training procedure for the French–English model is identical to the one outlined in Section 4.3. 5 Results & Evaluation All supervised models and experiments described in this paper were implemented using OpenNMT3 (Klein et al., 2017). For the phrase-based and transfer learning techniques, we used FairSeq (Ott et al., 2019). All translation models described in the previous section were trained, tuned, and evaluated on the same standardized training, development and test splits, respectively. First, we calculated BLEU scores (Papineni et al., 2002) for Sumerian translations against the gold data using various settings. The best results obtained are shown in the second column of Table 1. Moreover, in a qualitative evaluation, two experts in Sumerian rated 50 randomly chosen translations from each model, using the following scored ranking schema: good [3], helpful [2], incorrect [1] with exact definitions given in the supplementary material. All average ratings are shown in the last column of Table 1. A few important observations can be made: 3 http://opennmt.net/ 3457 (1) The Base Translator is outperformed by the Model Architecture BLEU"
2020.coling-main.308,D14-1162,0,0.0842374,"Missing"
2020.coling-main.308,D16-1163,0,0.0607369,"rd embedding space in an unsupervised way as described by Conneau et al. (2017). Using this bilingual dictionary we populated the phrase tables for Sumerian to English and English to Sumerian. Then, we trained n-gram language models for the Sumerian and English domain using the methods outlined in Heafield (2011). In a later step, we improved these translation models using iterative back-translation (He et al., 2016). 4.5 Transfer Learning Supervised machine translation relies on massive amounts of data, hence typically performs poorly on low resource languages. The idea of transfer learning (Zoph et al., 2016) is to train a machine translation model in a high-resource language setting, e.g., from French to English as a parent model and then initializing the training constraints using the parent model and apply it to the child model. In our experimentation, we first trained a French to English model on the Europarl Corpus using transformers, then trained our child model from Sumerian to English. The training procedure for the French–English model is identical to the one outlined in Section 4.3. 5 Results & Evaluation All supervised models and experiments described in this paper were implemented usin"
2020.globalex-1.1,2020.lrec-1.695,1,0.81461,"Missing"
2020.globalex-1.16,D14-1162,0,0.0821421,"Missing"
2020.globalex-1.16,J17-3004,0,0.0484474,"Missing"
2020.globalex-1.16,P13-1133,0,0.0154553,"tion of the dictionaries provided by the task organizers. Whereas we only use the languages and language pairs provided in these dictionaries, it would be possible to add more language pairs to be processed by our approach, as long as they are available in the TIAD-TSV format. We provide such data for more than 1,500 language pairs as part of the ACoLi dictionary graph (Chiarcos et al., 2020),3 but this has not been considered in this experiment. As for concept inventories, we use WordNet data, and we expect it to come as TSV data in accordance to the Open Multilingual WordNet specifications (Bond and Foster, 2013, OMW),4 i.e., a three-column table containing synset ID in the first column, the string ‘lemma’ (or other relation identifiers) in the second column, and the word form in the third column. As for the word form, we differ from the OMW format by requiring that it is a Turtle string with a language tag, e.g., ""able""@en instead if able in the English OMW WordNet. For OMW data, we provide a script that adds quotes and BCP47 language tags. We also provide a converter that produces OMW TSV from the RDF edition of Princeton WordNet 3.1. A key advantage of OMW data is that it provides crosslinguistica"
2020.globalex-1.16,2020.lrec-1.696,1,0.766351,"Missing"
2020.iwltp-1.15,W16-3503,1,0.91547,"PI Manager Workflow Manager Preprocessing Provisioning of Datasets and Content Language Identification Storage Knowledge Graph File Storage Duplicate Detection Document Structure Analysis Semantic Analysis Content Generation Summarization Named Entity Recognition and Linking Paraphrasing Temporal Expression Analysis Machine Translation Relation Extraction Semantic Storytelling Event Detection Security Figure 5: Technical architecture of the QURATOR platform velops a curation technology platform, which is also being populated with services, simplifying and accelerating the curation of content (Bourgonje et al., 2016a; Rehm et al., 2019a; Schneider and Rehm, 2018a; Schneider and Rehm, 2018b). The project develops, evaluates and integrates services for preprocessing, analyzing and generating content, spanning use cases from the sectors of culture, media, health and industry. To process and transform incoming data, text or multimedia streams into device-adapted, publishable content, various groups of components, services and technologies are applied. These include adapters to data, content and knowledge sources, as well as infrastructural tools and AI methods for the acquisition, analysis and generation of"
2020.iwltp-1.15,2020.lrec-1.696,1,0.735511,"nt annotation schemes can refer to. (2) Multiple OLiA Annotation Models formalize annotation schemes and tagsets. Fig. 8 illustrates this with an annotation model developed as part of the Korean NLP2RDF stack (Hahm et al., 2012). (3) For every annotation model, a linking model defines subclass-relationships between concepts in the annotation model and the reference model. Linking models are interpretations of annotation model concepts and properties in terms of the reference model. (4) Similarly, other community-maintained vocabularies are linked with OLiA, e. g., the CLARIN Concept Registry (Chiarcos et al., 2020). OLiA was developed as part of an infrastructure for the sustainable maintenance of linguistic resources (Wörner et al., 2006; Schmidt et al., 2006; Rehm et al., 2008b; Witt et al., 2009; Rehm et al., 2009). Its field of application included the formalization of annotation schemes and concept-based querying over heterogeneously annotated corpora (Rehm et al., 2008a). As several institutions and resources from various disciplines were involved, no holistic annotation standard could be enforced onto the contributors. 101 3.4. Figure 8: Modular OLiA ontologies 3.2. Level 1: Simple Cross-Platform"
2020.iwltp-1.15,W12-5201,0,0.0160466,"tral hub for linguistic annotation terminology in the web of data. OLiA was designed for mediating between various terminology repositories on the one hand and annotated resources (i. e., their annotation schemes), on the other. Four different types of ontologies are distinguished (Fig. 8): (1) The OLiA Reference Model is an OWL ontology that specifies the common terminology that different annotation schemes can refer to. (2) Multiple OLiA Annotation Models formalize annotation schemes and tagsets. Fig. 8 illustrates this with an annotation model developed as part of the Korean NLP2RDF stack (Hahm et al., 2012). (3) For every annotation model, a linking model defines subclass-relationships between concepts in the annotation model and the reference model. Linking models are interpretations of annotation model concepts and properties in terms of the reference model. (4) Similarly, other community-maintained vocabularies are linked with OLiA, e. g., the CLARIN Concept Registry (Chiarcos et al., 2020). OLiA was developed as part of an infrastructure for the sustainable maintenance of linguistic resources (Wörner et al., 2006; Schmidt et al., 2006; Rehm et al., 2008b; Witt et al., 2009; Rehm et al., 2009"
2020.iwltp-1.15,2020.lrec-1.420,1,0.820145,"Missing"
2020.iwltp-1.15,W17-4212,1,0.852914,". g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data Documents Figure 6: The Lynx technology platform The platform’s microservice architecture is a variant of the service-oriented architecture (SOA), in which an application is structured as a collection of loosely coupled services. It uses Docker containers ho"
2020.iwltp-1.15,2020.iwltp-1.12,1,0.914049,"ments, yet others provide a user interface. The Document Manager provides the storage and annotation of documents with an emphasis on keeping them synchronized, providing read and write access, as well as updates of documents and annotations. It can be queried in terms of annotations and documents, through REST APIs. The interface includes a set of create, read, update, and delete APIs to manage collections, documents and annotations. The orchestration and execution of services involved in more complex tasks is addressed by a Workflow Manager. It defines combinations of services as workflows (Moreno-Schneider et al., 2020b; Bourgonje et al., 2016a; Schneider and Rehm, 2018a; Schneider and Rehm, 2018b). Workflows are described using BPMN and executed using Camunda.8 Interoperability is addressed at the following levels: Since the QURATOR platform is a closed ecosystem, the platform can be thought of as an experimental toolbox with services customised by the partners for their own use cases. As the platform is used only by the QURATOR partners, it does not contain a catalogue or any kind or structured metadata. However, two of the ten QURATOR projects have a focus on service composition and workflows with protot"
2020.iwltp-1.15,2020.lrec-1.284,1,0.886537,"Missing"
2020.iwltp-1.15,piperidis-2012-meta,1,0.875093,"t least upon a certain (obligatory) subset (Labropoulou et al., 2020; McCrae et al., 2015). Such a more detailed, semantics-driven approach enables more efficient and more user-friendly search results from multiple platforms that can be visually aggregated and also easily ranked. The actual search can be performed through publicly available APIs but returned objects would be semantically richer. Alternatively, the metadata records of external repositories can be harvested using standard protocols such as OAI-PMH, which allow the construction of a master index out of decentralised inventories (Piperidis, 2012). A known issue that needs to be addressed using such an approach involves the detection of duplicate resources. Figure 9: A cross-platform workflow example A similar approach was implemented in the project OpenMinTeD (OMTD) (Labropoulou et al., 2018) using the Galaxy workflow management system.10 Three types of LT components are supported: (1) components packaged in Docker images that follow the OMTD specifications; (2) components wrapped with UIMA or GATE, available in a Maven repository; (3) Text and Data Mining web services that run outside the OMTD platform and that follow the OMTD specif"
2020.iwltp-1.15,L18-1519,1,0.396376,"ed in AI4EU Experiments. 2.2. European Language Grid (ELG) Multilingualism and cross-lingual communication in Europe can only be enabled through Language Technologies (LTs) (Rehm et al., 2016). The European LT landscape is fragmented (Vasiljevs et al., 2019), holding back its impact. Another crucial issue is that many languages are underresourced and, thus, in danger of digital extinction (Rehm and Uszkoreit, 2012; Kornai, 2013; Rehm et al., 2014). There is an enormous need for an European LT platform as a unifying umbrella (Rehm and Uszkoreit, 2013; Rehm et al., 2016; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018; Rehm et al., 2020c). The project European Language Grid (2019-2021) attempts to establish the primary platform and marketplace for the European LT community, both industry and research (Rehm et al., 2020a). This scalable cloud platform will provide access to hundreds of LTs for all European languages, including running services as well as data sets. ELG will enable the European LT community to upload their technologies and data sets, to deploy them, and to connect with other resources. ELG caters for commercial and non-commercial LTs (i. e., LTs with a high Technol"
2020.iwltp-1.15,rehm-etal-2008-ontology,1,0.781144,"Missing"
2020.iwltp-1.15,W17-2707,1,0.837412,"road groups: (1) Preprocessing encompasses services for obtaining and processing information from different content sources so that they can be used in the platform and integrated into other services (Schneider et al., 2018), e. g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data Documents Figure 6:"
2020.iwltp-1.15,2020.lrec-1.413,1,0.802365,"Missing"
2020.iwltp-1.15,2016.tc-1.14,1,0.737947,"n be divided into three broad groups: (1) Preprocessing encompasses services for obtaining and processing information from different content sources so that they can be used in the platform and integrated into other services (Schneider et al., 2018), e. g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data"
2020.iwltp-1.2,L16-1707,1,0.790295,"e applications tailored to the needs of linguists, lexicographers, researchers in NLP and knowledge engineering. Promising approaches in this direction do exist: Existing tools can be complemented with an RDF layer to facilitate their interoperability. Likewise, LLOD-native applications are possible, e.g., to use RDFa (RDF in attributes) (Herman et al., 2015) to complement an XML workflow with SPARQL-based semantic search by means of web services (Sabine Tittel and Chiarcos, 2018), to provide aggregation, enrichment and search routines for language resource metadata (McCrae and Cimiano, 2015; Chiarcos et al., 2016), to use RDF as a formalism for annotation integration and data management (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasion"
2020.iwltp-1.2,L18-1717,1,0.852139,"e their interoperability. Likewise, LLOD-native applications are possible, e.g., to use RDFa (RDF in attributes) (Herman et al., 2015) to complement an XML workflow with SPARQL-based semantic search by means of web services (Sabine Tittel and Chiarcos, 2018), to provide aggregation, enrichment and search routines for language resource metadata (McCrae and Cimiano, 2015; Chiarcos et al., 2016), to use RDF as a formalism for annotation integration and data management (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasional users of this technology. A notable exception in this regard is LexO (Bellandi et al., 2017), which is a graphical tool for the collaborative editing of lexical and ontological resources natively building on the OntoLex voc"
2020.iwltp-1.2,2020.lrec-1.395,1,0.811257,"Missing"
2020.iwltp-1.2,W17-7010,0,0.0379115,"t (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasional users of this technology. A notable exception in this regard is LexO (Bellandi et al., 2017), which is a graphical tool for the collaborative editing of lexical and ontological resources natively building on the OntoLex vocabulary and RDF, designed to conduct lexicographical work in a philological context (i.e., creating the Dictionnaire des Termes Médico-botaniques de l’Ancien Occitan). Other projects whose objective is to provide LLOD-based tools for specific areas of application have been recently approved, so that progress in this direction is to be expected within the next years. Ten years after the formation of the OWLG, the situation of linked data in language technology and l"
2020.iwltp-1.2,I08-1051,0,0.0564092,"e engineering. Promising approaches in this direction do exist: Existing tools can be complemented with an RDF layer to facilitate their interoperability. Likewise, LLOD-native applications are possible, e.g., to use RDFa (RDF in attributes) (Herman et al., 2015) to complement an XML workflow with SPARQL-based semantic search by means of web services (Sabine Tittel and Chiarcos, 2018), to provide aggregation, enrichment and search routines for language resource metadata (McCrae and Cimiano, 2015; Chiarcos et al., 2016), to use RDF as a formalism for annotation integration and data management (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasional users of this technology. A notable exception in this regard is LexO (Bellandi et al., 2017), w"
2020.iwltp-1.2,W07-1501,0,0.0696464,"ersion should be provided. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for common problems, e.g., the development of a database that is capable of support flexible, graphbased data structures as necessary for multi-layer corpora (Ide and Suderman, 2007). Linked (Open) Data for Language Resources Publishing Linked Data allows resources to be globally and uniquely identified such that they can be retrieved through standard Web protocols. Moreover, resources can be easily linked to one another in a uniform fashion and thus become structurally interoperable. (Chiarcos et al., 2013) identified the five main benefits of Linked Data for Linguistics and NLP: Conceptual Interoperability Semantic Web technologies allow to provide, to maintain and to share centralized, but freely accessible terminology repositories. Reference to such terminology reposi"
2020.iwltp-1.2,wright-2004-global,0,0.123506,"ly referenced from any other resource on the Web through URIs. Similar to hyperlinks in the HTML web, the web of data created by these links allows navigation along these connections, and thereby to freely integrate information from different resources in the cloud. Dynamic Import When linguistic resources are interlinked by references to resolvable URIs instead of system-defined IDs (or static copies of parts from another resource), we always provide access to the most recent version of a resource. For communitymaintained terminology repositories like the ISO TC37/SC4 Data Category Registry (Wright, 2004; Windhouwer and Wright, 2012), for example, new categories, definitions or examples can be introduced occasionally, and this information is available immediately to anyone whose resources refer to ISOcat URIs. In order to preserve link consistency among Linguistic Linked Open Data resources, however, it is strongly advised to apply a proper versioning system such that backward-compatibility can be preserved: Adding concepts or examples is unproblematic, but when concepts are deleted, renamed or redefined, a new version should be provided. Ecosystem RDF as a data exchange framework is maintain"
2020.lrec-1.401,2016.gwc-1.9,0,0.0672748,"Missing"
2020.lrec-1.401,francopoulo-etal-2006-lexical,0,0.27776,"forms is of utmost importance, as this enables technical solutions to provide services across language barriers, in particular for speakers of languages that are underresourced with respect to NLP tools and language resources. A challenge for the vast amount of lexical information available in digital form, is, however, that it comes in various forms. Several widely-used standards for the representation of lexical information in interoperable form do exist for a long time, e.g., in the context of the Text Encoding Initiative (Burnard and Sperberg-McQueen, 2012), the Lexical Markup Framework (Francopoulo et al., 2006) or tool-specific formats such as the StarDict format,1 but they are limited with respect to their interoperability with each other,2 and – more importantly – they are focusing on the standardization of dictionaries as independent, machine-readable entities, whereas many technical applications require an additional focus on the capability of integrating information across different lexical-conceptional resources. Both problems have been driving the more recent development of the OntoLex-Lemon vocabulary (McCrae et al., 2017) into the most important data model for the publication of lexical res"
2020.lrec-1.401,kamholz-etal-2014-panlex,0,0.0265952,"context of the WordNet Collaborative Interlingual Index (Bond et al., 2016), where the model is being used to provide a single interlingual identifier for every concept in every language. The ACoLi Dictionary Graph contributes to the growing amount of lexical-conceptual resources that provide OntoLex-Lemon compliant lexical data. It is created by aggregating over several sources with heterogeneous data models and differences in depth of representation. Our contribution is to provide a two-level normalization of this data: dictionaries: Apertium (Tyers et al., 2010)5 , FreeDict6 , and PanLex (Kamholz et al., 2014)7 as well as their export to TIAD TSV. In addition to the dictionaries mentioned above, we also provide a TIAD TSV export for DBnary (S´erasset, 2015), a lexical database that comes natively in OntoLexLemon, resp., RDF. Note that these resources are of very different character which entail different modelling strategies in OntoLexLemon: • Bi-dictionaries such as the Apertium dictionaries provide translation information together with grammatical information or pronunciation data about both source and target language. (a) RDF: A coherent OntoLex-Lemon representation that preserves content and st"
2020.lrec-1.695,P18-1073,0,0.016647,"ersion of 5 terminological resources in TBX to RDF. 5.3. Linking Finally, the project is developing (semi-)automated linking mechanisms. This concerns both the conceptual level of language descriptions as also the lexical data. We are working both in a mono- and in a cross-lingual set up. Since this work is still in progress, at this time we can only report on preliminary approaches. In the context of cross-lingual concept matching, we are updating an already existent ontology matching tool, CIDERCL (Gracia and Asooja, 2013) with contemporary techniques based on cross-lingual word embeddings (Artetxe et al., 2018). Regarding linking cross-lingual lexical data, the project has laid the groundwork for research in the topic of ”translation inference across dictionaries” by organising the TIAD’19 shared task (Gracia et al., 2019), in which a benchmark and evaluation framework were provided to allow for systematic comparisons between systems. Such systems were able to infer indirect translations between language pairs that were initially disconnected in the Apertium RDF graph (Gracia et al., 2018), showing promising results but also the need of further research. Ontology lexicalisation aims at developing te"
2020.lrec-1.695,2019.gwc-1.34,1,0.826147,"Missing"
2020.lrec-1.695,E09-2008,0,0.0376885,"ons for morphology (Klimek et al., 2019) and the representation of frequency, attestation and corpus information (Chiarcos and Ionov, 2019). The morphology module is specifically important for the cross-linguistic applicability of OntoLex-Lemon, as it aims to support languages with a lot of stem internal alternations: By using regular expressions to represent morphology generation rules, it provides implementation-independent means to generate inflected forms from lemma information, that can be subsequently incorporated in conventional morphology frameworks such as XFST (Ranta, 1998) or FOMA (Hulden, 2009). Specifications for phonological processes and mor3 See https://www.w3.org/2016/05/ontolex/ See (McCrae et al., 2012) and (Cimiano et al., 2016). 5 SKOS stands for “Simple Knowledge Organization System”. SKOS provides “a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary” (https://www. w3.org/TR/skos-primer/). 6 See http://www.elex.is/ for more detail. 4 Figure 2: The core Modules of OntoLex-Lemon. Graphic taken from https://www.w3.or"
2020.lrec-1.695,L16-1386,1,0.78537,"Missing"
2020.lrec-1.695,W98-1308,0,0.29918,"d emerging specifications for morphology (Klimek et al., 2019) and the representation of frequency, attestation and corpus information (Chiarcos and Ionov, 2019). The morphology module is specifically important for the cross-linguistic applicability of OntoLex-Lemon, as it aims to support languages with a lot of stem internal alternations: By using regular expressions to represent morphology generation rules, it provides implementation-independent means to generate inflected forms from lemma information, that can be subsequently incorporated in conventional morphology frameworks such as XFST (Ranta, 1998) or FOMA (Hulden, 2009). Specifications for phonological processes and mor3 See https://www.w3.org/2016/05/ontolex/ See (McCrae et al., 2012) and (Cimiano et al., 2016). 5 SKOS stands for “Simple Knowledge Organization System”. SKOS provides “a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary” (https://www. w3.org/TR/skos-primer/). 6 See http://www.elex.is/ for more detail. 4 Figure 2: The core Modules of OntoLex-Lemon. Graphic taken"
2020.lrec-1.696,chiarcos-etal-2012-open,1,0.860467,"c Annotation (Chiarcos and Sukhareva, 2015, OLiA). Historically, OLiA originated as an OWL formalization of the EAGLES recommendations (Leech and Wilson, 1996), extended by a linking with GOLD and the morphosyntactic and syntactic profiles of ISOCat as well as definitions and introduced by various annotation schemes it was applied to. OLiA owes its continued relevance to its application beyond its original use case: It has been conceived as a meta-vocabulary for tagset documentation and cross-resource corpus querying, but with the emergence of the Linguistic Linked Open Data cloud since 2010 (Chiarcos et al., 2012a),3 it evolved to become the primary vocab2 In fact, an ontological formalization, or at least the use of Semantic Web technology had been a design concept in early days of the ISO Data Category Registry (Ide and Romary, 2004), but not adopted for the effective implementation of ISOCat. 3 http://linguistic-lod.org 5668 ulary to formalize linguistic annotations for Semantic Web applications and language resources in the web of data. A fundamental insight of the 2000s was that the development and usability of widely used, shared annotation terminology for linguistic annotations must be based on"
2020.lrec-1.696,chiarcos-2014-towards,1,0.79548,"linking model defines rdfs:subClassOfrelationships between concepts in the annotation model and the reference model. Linking models are interpretations of annotation model concepts and properties in terms of the reference model. (4) Similarly, other communitymaintained vocabularies are linked with OLiA, e. g., ISOCat and GOLD. The OLiA ontologies cover different grammatical phenomena, including inflectional morphology, word classes, phrase and edge labels of different syntax annotations, as well as extensions for coreference, discourse relations, discourse structure and information structure (Chiarcos, 2014). Annotations for lexical semantics are only covered to the extent that they are found in syntactic and morphosyntactic annotation schemes. 3.2. Universal Part of Speech Tags (UP) In order to facilitate annotation projection on the basis of parallel text, Petrov et al. (2012) introduced a highly reductionist, but ‘universal’ (i.e., cross-linguistically applicable, not universal in the sense of linguistic theory) representation of morphosyntactic annotation, based on a mapping of various annotation schemes to a minimal set of about a dozen part-of-speech tags. We integrate his information in OL"
2020.lrec-1.696,W08-1301,0,0.271702,"Missing"
2020.lrec-1.696,R19-1027,0,0.0184768,"ry of formal data categories for linguistic features of lexical entries and related information in dictionaries, wordnets and multilingual ontologies. So far, both ontologies have not been put into relation, although OLiA has been applied for encoding features of lexical resources, as well (Eckle-Kohler et al., 2015). Creating an interlinking between LexInfo and OLiA, and, via OLiA, with UniMorph and UD thus comes with the prospect of enormous synergies between lexical resources and natural language processing as well as for the enrichment of lexical resources and morphological resources, cf. Declerck and Racioppa (2019). Similar to OLiA, LexInfo is partially based on ISOCat, but it differs from the OLiA Reference Model in 16 http://purl.org/olia/ud-v1 purl.org/olia/ud-v2 18 http://purl.org/olia/unimorph/ 17 5672 that it provides individuals rather than classes. Accordingly, it is not possible to establish formal equivalence relations (owl:equivalentClass), to assert identity (owl:sameAs) or to use the conventional OLiA linking properties (rdfs:subClassOf). Instead, LexInfo terms can only be defined as instances of OLiA concepts, so that LexInfo is linked with OLiA in the style of an OLiA annotation model,19"
2020.lrec-1.696,francopoulo-etal-2006-lexical,0,0.065764,"ntrinsic ties with the popular OntoLex-Lemon vocabulary (Cimiano et al., 2016). Originally, it was designed as an ontology for “associat[ing] linguistic information with respect to any level of linguistic description and expressivity to elements in an ontology” (Cimiano et al., 2011). In this function, it predates Ontolex-Lemon, but with LexInfo v.2.0, it was re-designed to serve as a terminology backend of OntoLex-Lemon with the goal of making Ontolex-lemon itself agnostic of any linguistic category system. The LexInfo ontology developed out of an RDF edition of the Lexical Markup Framework (Francopoulo et al., 2006, LMF), i.e., a major source of ISOCat concepts, so that LexInfo is largely compatible with ISOCat. LexInfo provides an axiomatized set of linguistic categories, covering areas such as part of speech, tense, number, animacy, degree, mood, register, etc. These categories are largely derived from ISOCat, but LexInfo provides a stronger axiomatization and a coherent global organization. LexInfo v.2.0 is the reference vocabulary for linguistic categories and features in lexical-conceptual resources in the web of data. Since December 2019, version 3.0 is in preparation,8 with the goal to increase i"
2020.lrec-1.696,W12-5201,0,0.325651,"relations about their concepts (and properties). The OLiA ontologies are available from http://purl. org/olia under a Creative Commons Attribution license (CC-BY), and they are developed as an open source project using GitHub.12 Four different types of ontologies are distinguished (Fig. 1): (1) The OLiA Reference Model is an OWL ontology that specifies the common terminology that different annotation schemes can refer to. (2) Multiple OLiA Annotation Models formalize annotation schemes and tagsets. Fig. 1 illustrates this with an annotation model developed as part of the Korean NLP2RDF stack (Hahm et al., 2012). (3) For every 8 11 12 5670 https://github.com/ld4lt/metashare https://github.com/acoli-repo/olia Figure 1: Modular OLiA ontologies annotation model, a linking model defines rdfs:subClassOfrelationships between concepts in the annotation model and the reference model. Linking models are interpretations of annotation model concepts and properties in terms of the reference model. (4) Similarly, other communitymaintained vocabularies are linked with OLiA, e. g., ISOCat and GOLD. The OLiA ontologies cover different grammatical phenomena, including inflectional morphology, word classes, phrase and"
2020.lrec-1.696,ide-romary-2004-registry,0,0.0655987,"tic profiles of ISOCat as well as definitions and introduced by various annotation schemes it was applied to. OLiA owes its continued relevance to its application beyond its original use case: It has been conceived as a meta-vocabulary for tagset documentation and cross-resource corpus querying, but with the emergence of the Linguistic Linked Open Data cloud since 2010 (Chiarcos et al., 2012a),3 it evolved to become the primary vocab2 In fact, an ontological formalization, or at least the use of Semantic Web technology had been a design concept in early days of the ISO Data Category Registry (Ide and Romary, 2004), but not adopted for the effective implementation of ISOCat. 3 http://linguistic-lod.org 5668 ulary to formalize linguistic annotations for Semantic Web applications and language resources in the web of data. A fundamental insight of the 2000s was that the development and usability of widely used, shared annotation terminology for linguistic annotations must be based on web technologies, and in particular, resolvable URIs. The technical standard in this regard still remains to be ISOCat, which provides persistent URIs, even though these are redirected to a static dump now rather than the unde"
2020.lrec-1.696,W18-6011,0,0.022689,"nd that much language-specific information still refers to UD v.1. We provide the UD v.2 vocabularies and their linking with OLiA,17 and the build scripts via the OLiA GitHub repository. The Universal Morphology (UniMorph) project is a recent community effort aiming to complement the Universal Dependencies and their focus on syntax with coverage of inflectional morphology. UniMorph provides inflection tables for 110 languages using a TSV format, with lemma, form, and morphological features. Compatibility with UD is a requirement of the UniMorph community that has been partially achieved only (McCarthy et al., 2018), but by reference to a common reference vocabulary the relation between both vocabularies can be expressed easily. To this end, we created e a machine-readable representation of the UniMorph vocabulary in OWL and its linking with OLiA, closely following the approach taken for UD,18 with build scripts included in the OLiA GitHub repository. 3.5. LexInfo We focus on LexInfo v. 2.0, as this is closely coupled with the highly popular OntoLex-Lemon vocabulary and the reference vocabulary for lexical data categories in the Linguistic Linked Open Data cloud community. Unlike UP, UD or UniMorph, LexI"
2020.lrec-1.696,petrov-etal-2012-universal,0,0.0289546,"ocabularies are linked with OLiA, e. g., ISOCat and GOLD. The OLiA ontologies cover different grammatical phenomena, including inflectional morphology, word classes, phrase and edge labels of different syntax annotations, as well as extensions for coreference, discourse relations, discourse structure and information structure (Chiarcos, 2014). Annotations for lexical semantics are only covered to the extent that they are found in syntactic and morphosyntactic annotation schemes. 3.2. Universal Part of Speech Tags (UP) In order to facilitate annotation projection on the basis of parallel text, Petrov et al. (2012) introduced a highly reductionist, but ‘universal’ (i.e., cross-linguistically applicable, not universal in the sense of linguistic theory) representation of morphosyntactic annotation, based on a mapping of various annotation schemes to a minimal set of about a dozen part-of-speech tags. We integrate his information in OLiA by directly interpreting UP concepts as OLiA concepts and converting the mapping files from their original TSV format to OLiA annotation and linking models, using the following interpretation: UP tag VERB NOUN PRON ADJ ADV ADP CONJ DET NUM PRT X . UP definition verbs (all"
2020.lrec-1.696,zeman-2008-reusable,0,0.279665,"models for annotation schemes previously covered by OLiA. Note that UP tags are provided without definitions, there is thus no way to evaluate in a language-independent manner whether this mapping is correct. However, OLiA design encapsulates annotation models, so that OLiA users are free to replace an existing linking model with their own, e.g., by copying the linking model provided and adjusting it according to their specifications. 3.3. UD v.1 The Universal Dependencies comprise ‘universal’ part of speech annotations (originating in UP), morphosyntactic features (optional, originating from Zeman 2008, and, indirectly, from EAGLES) and syntactic dependencies (originating from the Stanford dependencies, De Marneffe and Manning 2008). UD data and documentation are available from https://universaldependencies.org/ and have been subject to a long (and on-going) process of refinement that led to two iterations of the UD vocabulary (UD v.1 and UD v.2). In relation to OLiA, which aims to cover all types of linguistic annotation, we see the UD community as working on a much more restricted problem, and on a limited (albeit impressive) scale of linguistic phenomena. We thus integrate UD specificati"
2020.lrec-1.885,L18-1717,1,0.892509,"Missing"
2020.lrec-1.885,chiarcos-2012-generic,1,0.906243,"a structures, for example, for parsing topological fields in Middle High German (Chiarcos et al., 2018), or for the creation of syntactic-semantic annotations in the context of Role and Reference Grammar (Chiarcos and F¨ath, 2019). However, as its vocabulary is grounded in CoNLL-TSV, CoNLL-RDF does not specify datatypes for trees and directed graphs not grounded in individual words, nor does the CoNLL-RDF package parse or serialize such annotations. In this paper we introduce the extensions we made to the CoNLL-RDF library to natively support tree-like annotations. Using the POWLA vocabulary (Chiarcos, 2012c), we model an annotation graph independent of the sentence and word structure imposed by CoNLL. In this way, we can even represent generic linguistic annotations that go beyond token-level annotation e.g. for usage in spoken discourse. We will first revisit the original concept and functionality of the CoNLL-RDF libraries. Second, this paper will describe a number of one-word-per-line TSV dialects with extensions for tree structures. We show that CoNLL-RDF was only partially able to represent them, and incapable of processing them effectively. We then explain how we complement the CoNLL-RDF"
2020.lrec-1.885,W06-2709,0,0.0366472,"Missing"
2020.lrec-1.885,N06-2015,0,0.135527,"language specific speech tag, FEATS contains morphosyntactic features encoded as key-value pairs with a pipe (‘|’) as delimiter. The HEAD column points to the ID of the parent in the dependency tree, with 0 representing the root. Finally, the EDGE column carries the label of the dependency relation that holds between a word and its head, resp., the (virtual root of the) sentence.5 CoNLL-U only represents one specific CoNLL dialect, but is supported by most multilingual dependency parsers – as these are usually trained on UD data. Fig. 1 shows a slightly simplified example from the OntoNotes (Hovy et al., 2006) wsj-0655 file, with annotations subsequently provided by the authors. Other dialects omit, reorder or complement these columns with additional content. All of them share the general advantages of this format: They enjoy widespread integration in existing NLP technology, they are easy to read and can be transformed and read without a lot of overhead. 2.2. From TSV to Annotation Graphs Being easily processable, interpretable and extensible, tabular data structures in TSV formats are popular and effective for word-level annotations. They are also capable of representing other annotations, e.g.,"
2020.lrec-1.885,ide-etal-2008-masc,0,0.0418692,"The POWLA Vocabulary POWLA (Chiarcos, 2012c) is an OWL2DL vocabulary for systematizing linguistic annotations. Compared to other formalisms, POWLA does not make any assumptions on what the atomic structures look like. POWLA is an OWL2/DL serialization of PAULA (Dipper and G¨otze, 2005; Dipper and G¨otze, 2006)11 an early implementation of LAF (Ide and Romary, 2004) and serialized in a standoff XML format (Stede et al., 2006; Zeldes et al., 2009). POWLA has been applied to modeling multi-layer corpora, including case studies on the Manually Annotated Sub-Corpus of the American National Corpus (Ide et al., 2008, MASC), syntactic and coreference annotated corpora (Chiarcos, 2012a), high-precision information extraction (de Araujo et al., 2017), annotation engineering (Chiarcos and F¨ath, 2019), and syntactic parsing (Chiarcos et al., 2018). POWLA aims to formalize linguistic annotations by building on existing standards with respect to their anchoring in the original document. PAULA XML uses XLink/XPointer references for this purpose. POWLA’s design goal is to complement existing NIF, Web Annotations or applicationspecific RDF renderings of linguistic annotations with in11 Every PAULA data structure"
2020.lrec-1.885,J93-2004,0,0.0691238,"RDF works seamlessly for word-level annotations, dependency syntax and semantic role annotations. However, one-word-per-line TSV formats provide rudimentary support for annotations on the level of text (markup) and syntax (phrase structures) only, and CoNLL-RDF inherits this limitation. We present two suggested extensions of one-word-per-line TSV formats as the basis of the extension of CoNLL-RDF with data structures for trees: the bracketing notation in accordance with the Penn Treebank and the XML markup used by SketchEngine or CorpusWorkbench. 3.1. PTB Bracketing Notation The Penn Treebank(Marcus et al., 1993, PTB) featured a formalism for phrase structure grammar that uses opening and closing brackets to indicate beginning and end of a phrase. They enclose first the phrase annotation followed by the primary data. For representing such data, the CoNLL-2005 format added another column with parse information split into word-level pieces, and the primary data (= content of the WORD column) replaced by the placeholder ‘*’. For each unit of primary data, the bracketing and annotations containing the *-replacement are presented in a separate column. In CoNLL-RDF, this information is simply preserved as"
2020.lrec-1.891,L18-1090,1,0.807979,"ed from algorithmic, rule-based strategies towards more data-driven approaches. This includes not only statistical methods but also neural networks and deep learning. However, all these methods heavily rely on the amount of available language resources and their data structures. The EU-funded Prˆet-`a-LLOD project1 aims at tackling the crucial challenges of discovering, linking and transforming language resources and making them available as interoperable Linked Data (LD) using well-established RDF-based2 formats like OntoLex-Lemon (Cimiano et al., 2016), CoNLL-RDF3 (Chiarcos and F¨ath, 2017; Chiarcos and Schenk, 2018) or the NLP Interchance Format (Hellmann et al., 2012, NIF). While resource discovery, maintenance and licensing is covered by other parts of the project, in this paper we focus on the Flexible and Integrated Transformation and Annotation eNgineering (Fintan) platform which provides a workflow-driven, modular approach to graphtransformation while still enabling the usage of wellestablished resource-specific converters. 2. 2.1. Challenges in Linguistic Linked Data Heterogeneity of Linguistic Resources Since language resources on the web are highly heterogeneous and tailored towards specific use"
2020.lrec-1.891,2020.lrec-1.401,1,0.80912,"Missing"
2020.lrec-1.891,N15-1119,0,0.0558901,"Missing"
2020.lrec-1.891,P15-2111,0,0.0275903,"s://www.w3.org/ns/lemon/ontolex#&gt; . @prefix olia: <http://purl.org/olia/unimorph.owl#&gt; . @prefix : <https://github.com/unimorph/sqi/&gt; . :akrepit a ontolex:LexicalEntry ; ontolex:canonicalForm [ ontolex:writtenRep &quot;akrepit&quot; ] ; ontolex:lexicalForm :s1_1296 , :s1_1298 , :s1_1297 . :s1_1296 In order to test the perfomance of the stream based graph transformation, the pipeline was executed using three distinct variants: In order to test the capabilities of our stream-based graph transformation we conducted a small case study which builds on an earlier effort to transform the Universal Morphology (Sylak-Glassman et al., 2015, UniMorph) to OntoLex-Lemon, publicly available as part of the LLODifier16 toolset. UniMorph describes inflected words of a given language together with their lexical meaning (as lemma) and a set of morphological features from the UniMorph annotation schema. The data in TSV format, together with licence information is available on the project’s github page17 . The following example shows an excerpt of the inflectional forms of the Albanian lemma akrep: akrep akrep akrep akrep akrep akrepin N;ACC;SG;DEF akrepi N;NOM;SG;DEF akrepit N;ABL;SG;DEF akrepit N;DAT;SG;DEF akrepit N;GEN;SG;DEF Because"
2020.lrec-1.891,P15-1128,0,0.0965623,"Missing"
2020.lrec-1.891,L18-1383,0,0.0279437,". However, the Filter node is attached to all variables addressed within. This makes it easy to see which parts of the graph are affected by it. We currently support SPARQL queries with SELECT and CONSTRUCT, as well as updates with INSERT and 15 A short, subjective overview is given on the W3C websites: https://www.w3.org/2018/09/rdf-data-viz/ 7216 DELETE. Figure 4 shows the visualization for the SPARQL update for transforming part of speech tags to LexInfo as described in section 3.4.2.. 3.6. Workflow Management For the workflow management we build on the user interface developed for Teanga (Ziad et al., 2018) which is also part of the Prˆet-`a-LLOD project. By rendering the transformer modules described in the Fintan ontology as nodes with their respective constraints on input and output data, the workflow manager will enable users to visually create complex transformation pipelines and assess their compatibility. Figure 5 shows an exemplary pipeline which was used for the case study described in section 4. 4. Case Study: Universal Morphology @prefix ontolex: <https://www.w3.org/ns/lemon/ontolex#&gt; . @prefix olia: <http://purl.org/olia/unimorph.owl#&gt; . @prefix : <https://github.com/unimorph/sqi/&gt; ."
2021.semdeep-1.3,C18-1139,0,0.0237652,"efines k. For the GloVe example, a lemma (lexical entry) embedding can be represented as follows: : frak a ontolex : LexicalEntry ; ontolex : canonicalForm / o n t o l e x : w r i t t e n R e p ” f r a k ”@en ; f r a c : embedding [ a frac : FixedSizeVector ; r d f : value ”0.015246 . . . ” ; dct : source <h t t p s : / / c a t a l o g . l d c . . . . > ; d c t : e x t e n t 5 0 ˆ ˆ ˆ xsd : i n t ; d c t : d e s c r i p t i o n ” GloVe v . 1 . 1 , . . . ” @en . ] . 4.2 Contextualized Embeddings Above, we mentioned contextualized embeddings, and more recent methods such as ELMo [16], Flair NLP [1], or BERT [7] have been shown to be remarkably effective at many NLP problems. In the context of lexical semantics, contextual embeddings can prove beneficial for inducing or distinguishing word senses, and in extension of the classical Lesk algorithm, for example, a lexical sense can be described by means of the contextualized word embeddings for the examples associated with that particular lexical sense, and words for which word sense disambiguation is to be performed can then just be compared with these. These examples then serve a similar function as attestations in a dictionary, and indee"
2021.semdeep-1.3,2016.gwc-1.9,0,0.060729,"conventions in the field, such mistakes will very likely go unnoticed, thus leading to highly unexpected results in applications developed on this basis. Our suggestion here is to use resolvable URIs as concept identifiers, and if they provide machine-readable lexical data, lexical information about concept embeddings can be more easily verified and (this is another application) integrated with predictions from distributional semantics. Indeed, the WordNet community has adopted OntoLex-Lemon as an RDF-based representation schema and developed the Collaborative Interlingual Index (ILI or CILI) [3] to establish sense mappings across a large number of WordNets. Reference to ILI URIs would allow to retrieve the lexical information behind a particular concept embedding, as the WordNet can be queried for the lexemes this concept (synset) is associated with. A versioning mismatch can then be easily spotted by comparing the cosine distance between the word embeddings of these lexemes and the embedding of the concept presumably derived from them. 3 See ili https://github.com/globalwordnet/ 2.2 Organizing Contextualized Embeddings A related challenge is the organization of contextualized embedd"
2021.semdeep-1.3,2020.globalex-1.1,1,0.879106,"eddings generated with different algorithms that can be freely used in various NLP applications. With this paper, we present the current state of an effort to connect these embeddings with lexical knowledge graphs. This effort is a part of an extension of a widely used community standard for representing, linking and publishing lexical resources on the web, OntoLex-Lemon1 . Our work aims to complement the emerging OntoLex module for representing Frequency, Attestation and Corpus Information (FrAC) which is currently being developed by the W3C Community Group “Ontology-Lexica”, as presented in [4]. There we addressed only frequency and attestations, whereas core aspects of corpus-based information such as embeddings were 1 https://www.w3.org/2016/05/ontolex/ identified as a topic for future developments. Here, we describe possible use-cases for the latter and present our current model for this. 2 Sharing Embeddings on the Web Although word embeddings are often calculated on the fly, the community recognizes the importance of pre-trained embeddings as these are readily available (it saves time), and cover large quantities of text (their replication would be energy- and timeintense). Fin"
2021.semdeep-1.3,N19-1423,0,0.0230632,"the GloVe example, a lemma (lexical entry) embedding can be represented as follows: : frak a ontolex : LexicalEntry ; ontolex : canonicalForm / o n t o l e x : w r i t t e n R e p ” f r a k ”@en ; f r a c : embedding [ a frac : FixedSizeVector ; r d f : value ”0.015246 . . . ” ; dct : source <h t t p s : / / c a t a l o g . l d c . . . . > ; d c t : e x t e n t 5 0 ˆ ˆ ˆ xsd : i n t ; d c t : d e s c r i p t i o n ” GloVe v . 1 . 1 , . . . ” @en . ] . 4.2 Contextualized Embeddings Above, we mentioned contextualized embeddings, and more recent methods such as ELMo [16], Flair NLP [1], or BERT [7] have been shown to be remarkably effective at many NLP problems. In the context of lexical semantics, contextual embeddings can prove beneficial for inducing or distinguishing word senses, and in extension of the classical Lesk algorithm, for example, a lexical sense can be described by means of the contextualized word embeddings for the examples associated with that particular lexical sense, and words for which word sense disambiguation is to be performed can then just be compared with these. These examples then serve a similar function as attestations in a dictionary, and indeed, the link h"
2021.semdeep-1.3,D14-1162,0,0.10149,"calculated from plain strings, but from normalized strings, e.g., lemmatized text. For such data, we model every individual lemma as an ontolex:LexicalEntry. Moreover, as argued in Sec. 2, embeddings are equally relevant for lexical senses and lexical concepts; the embedding property that associates a lexical entity with an embedding is thus applicable to every Observable. 4.1 Word Embeddings Pre-trained word embeddings are often distributed as text files consisting of the label (token) and a sequence of whitespace-separated numbers. E.g. the entry for the word frak from the GloVe embeddings [15]: frak 0.015246 -0.30472 0.68107 ... Since our focus on publishing and sharing embeddings, we propose to provide the value of an embedding as a literal rdf:value. If necessary, more elaborate representations, e.g., using rdf:List, may subsequently be generated from these literals. A natural and effort-less modelling choice is to represent embedding values as string literals with whitespace-separated numbers. For decoding and verification, such a representation benefits from metadata about the length of the vector. For a fixed-size vector, this should be provided by dc:extent. An alternative is"
2021.semdeep-1.3,N18-1202,0,0.0502005,"erty dc:extent defines k. For the GloVe example, a lemma (lexical entry) embedding can be represented as follows: : frak a ontolex : LexicalEntry ; ontolex : canonicalForm / o n t o l e x : w r i t t e n R e p ” f r a k ”@en ; f r a c : embedding [ a frac : FixedSizeVector ; r d f : value ”0.015246 . . . ” ; dct : source <h t t p s : / / c a t a l o g . l d c . . . . > ; d c t : e x t e n t 5 0 ˆ ˆ ˆ xsd : i n t ; d c t : d e s c r i p t i o n ” GloVe v . 1 . 1 , . . . ” @en . ] . 4.2 Contextualized Embeddings Above, we mentioned contextualized embeddings, and more recent methods such as ELMo [16], Flair NLP [1], or BERT [7] have been shown to be remarkably effective at many NLP problems. In the context of lexical semantics, contextual embeddings can prove beneficial for inducing or distinguishing word senses, and in extension of the classical Lesk algorithm, for example, a lexical sense can be described by means of the contextualized word embeddings for the examples associated with that particular lexical sense, and words for which word sense disambiguation is to be performed can then just be compared with these. These examples then serve a similar function as attestations in a dictio"
2021.semdeep-1.3,J17-3004,0,0.0142652,"ut rather documented in human-readable form or given implicitly as part of file names.2 2.1 Concept Embeddings It is to be noted, however, that our focus is not so much on word embeddings, since lexical information in this context is apparently trivial – plain 2 See the ISO-639-1 code ‘en’ in FastText/MUSE files such as https://dl.fbaipublicfiles.com/arrival/ vectors/wiki.multi.en.vec. tokens without any lexical information do not seem to require a structured approach to lexical semantics. This changes drastically for embeddings of more abstract lexical entities, e.g., word senses or concepts [17], that need to be synchronized between the embedding store and the lexical knowledge graph by which they are defined. WordNet [14] synset identifiers are a notorious example for the instability of concepts between different versions: Synset 00019837-a means ‘incapable of being put up with’ in WordNet 1.71, but ‘established by authority’ in version 2.1. In WordNet 3.0, the first synset has the ID 02435671-a, the second 00179035-s.3 The precompiled synset embeddings provided with AutoExtend [17] illustrate the consequences: The synset IDs seem to refer to WordNet 2.1 (wn-2.1-00001742-n), but use"
buyko-etal-2008-ontology,rehm-etal-2008-ontology,1,\N,Missing
buyko-etal-2008-ontology,J93-2004,0,\N,Missing
buyko-etal-2008-ontology,W07-1505,1,\N,Missing
buyko-etal-2008-ontology,N06-2015,0,\N,Missing
buyko-etal-2008-ontology,ide-romary-2004-registry,0,\N,Missing
buyko-etal-2008-ontology,P07-2043,0,\N,Missing
buyko-etal-2008-ontology,W04-0604,0,\N,Missing
buyko-etal-2008-ontology,de-cea-etal-2004-ontotags,0,\N,Missing
buyko-etal-2008-ontology,I08-1051,0,\N,Missing
buyko-etal-2008-ontology,W94-0103,0,\N,Missing
chiarcos-2012-generic,J93-2004,0,\N,Missing
chiarcos-2012-generic,W10-1820,0,\N,Missing
chiarcos-2012-generic,ide-etal-2008-masc,0,\N,Missing
chiarcos-2012-generic,W00-1434,0,\N,Missing
chiarcos-2012-generic,W07-1501,0,\N,Missing
chiarcos-2012-generic,C04-1061,0,\N,Missing
chiarcos-2012-generic,C04-1074,0,\N,Missing
chiarcos-2012-generic,W09-3005,1,\N,Missing
chiarcos-2012-generic,C00-2157,0,\N,Missing
chiarcos-2012-generic,P10-1068,1,\N,Missing
chiarcos-2012-generic,P98-1013,0,\N,Missing
chiarcos-2012-generic,C98-1013,0,\N,Missing
chiarcos-2012-generic,W09-3021,0,\N,Missing
chiarcos-2012-generic,ide-romary-2006-representing,0,\N,Missing
chiarcos-2012-generic,sasaki-etal-2004-concept,0,\N,Missing
chiarcos-2012-generic,gronroos-miettinen-2004-infrastructure,0,\N,Missing
chiarcos-2012-generic,I08-1051,0,\N,Missing
chiarcos-2012-generic,P10-2013,0,\N,Missing
chiarcos-2012-generic,chiarcos-etal-2012-open,1,\N,Missing
chiarcos-2012-ontologies,pareja-lora-de-cea-2010-ontology,0,\N,Missing
chiarcos-2012-ontologies,ide-etal-2008-masc,0,\N,Missing
chiarcos-2012-ontologies,W04-0213,0,\N,Missing
chiarcos-2012-ontologies,C08-1098,0,\N,Missing
chiarcos-2012-ontologies,P10-1068,1,\N,Missing
chiarcos-2012-ontologies,J01-2002,0,\N,Missing
chiarcos-2012-ontologies,P98-1029,0,\N,Missing
chiarcos-2012-ontologies,C98-1029,0,\N,Missing
chiarcos-2012-ontologies,I08-1051,0,\N,Missing
chiarcos-2012-ontologies,W11-0402,1,\N,Missing
chiarcos-2012-ontologies,buyko-etal-2008-ontology,1,\N,Missing
chiarcos-2012-ontologies,chiarcos-etal-2012-open,1,\N,Missing
chiarcos-2014-towards,tonelli-etal-2010-annotation,0,\N,Missing
chiarcos-2014-towards,pareja-lora-de-cea-2010-ontology,0,\N,Missing
chiarcos-2014-towards,poesio-artstein-2008-anaphoric,0,\N,Missing
chiarcos-2014-towards,riester-etal-2010-recursive,0,\N,Missing
chiarcos-2014-towards,W09-3029,0,\N,Missing
chiarcos-2014-towards,W04-0213,0,\N,Missing
chiarcos-2014-towards,W01-1605,0,\N,Missing
chiarcos-2014-towards,W05-0307,0,\N,Missing
chiarcos-2014-towards,W07-1525,1,\N,Missing
chiarcos-2014-towards,N06-2015,0,\N,Missing
chiarcos-2014-towards,P98-1044,0,\N,Missing
chiarcos-2014-towards,C98-1044,0,\N,Missing
chiarcos-2014-towards,P10-1068,1,\N,Missing
chiarcos-2014-towards,J03-4002,0,\N,Missing
chiarcos-2014-towards,ide-romary-2004-registry,0,\N,Missing
chiarcos-2014-towards,P95-1018,0,\N,Missing
chiarcos-2014-towards,prasad-etal-2008-penn,0,\N,Missing
chiarcos-2014-towards,W10-1817,0,\N,Missing
chiarcos-2014-towards,P12-1008,0,\N,Missing
chiarcos-2014-towards,I11-1170,0,\N,Missing
chiarcos-2014-towards,W11-2819,0,\N,Missing
chiarcos-2014-towards,W04-2327,0,\N,Missing
chiarcos-2014-towards,paggio-2006-annotating,0,\N,Missing
chiarcos-etal-2012-open,rehm-etal-2008-ontology,1,\N,Missing
chiarcos-etal-2012-open,kemps-snijders-etal-2008-isocat,0,\N,Missing
chiarcos-etal-2012-open,W11-0122,1,\N,Missing
chiarcos-etal-2012-open,ide-etal-2008-masc,0,\N,Missing
chiarcos-etal-2012-open,J08-3010,0,\N,Missing
chiarcos-etal-2012-open,W07-1501,0,\N,Missing
chiarcos-etal-2012-open,C04-1074,0,\N,Missing
chiarcos-etal-2012-open,P98-1013,0,\N,Missing
chiarcos-etal-2012-open,C98-1013,0,\N,Missing
chiarcos-etal-2012-open,W09-3021,0,\N,Missing
chiarcos-etal-2012-open,2005.mtsummit-papers.11,0,\N,Missing
chiarcos-etal-2012-open,chiarcos-2012-ontologies,1,\N,Missing
chiarcos-etal-2012-open,I11-1099,1,\N,Missing
chiarcos-etal-2012-open,vossen-etal-2008-integrating,0,\N,Missing
K15-2006,P13-2013,0,0.420271,"tives which increases performance close to a human baseline. 3.1 Argument Identification The SDP pipeline processes the documents sentence by sentence. Due to the strict time constraints of the Shared Task, we have set up a rulebased detector for both Arg1 and Arg2 spans as follows: Implicit relation classification In the early attempt by Marcu and Echihabi (2002), implicit relation classification was grounded on synthetic training data (relation patterns with explicit cue phrases removed) and a Naive Bayes model trained on word-pair features. Aggregation over such word-pairs was described by Biran and McKeown (2013), while Park and Cardie (2012) optimized feature sets through feature selection, preprocessing and special binning techniques. Out of these, implicit relation classification remains the most problematic subtask, and attracted • Extract an explicit Arg1–Arg2 pair, where Arg2 is a complete sentence starting with an explicit connective.4 The previous sentence serves as Arg1. 3 http://www.cs.brandeis.edu/˜clp/ conll15st/dataset.html 4 An exhaustive list of explicit cue words was obtained from the training section of the PDTB, ranging from unigrams to 7-grams. 43 • Refining step 1, we extract sente"
K15-2006,N07-1054,0,0.016957,"experiments, others do not. But if a frequency filter is applied, the specific value for the threshold is usually not motivated. We see a possible explanation for the negative impact of cutoffs in the extremely sparse feature space: Many word-pair features which are present in the training section of the PDTB are not found in the development set and vice versa, and with frequency cutoffs applied, sparsity even grows further. Closely related to our observation are earlier findings that using even a small stop word list has adverse effects on performance, which seems implausible at first sight (Blair-Goldensohn et al., 2007). Biran and McKeown (2013) address this issue in closer detail by replacing the sparse lexical word-pair features by more dense, aggregated score features. Based on their experiments, the authors argue that the most powerful features are mainly function words. Yet, their lack of semantic content whatsoever still calls for an explanation why they are useful in distinguishing the different types of implicit relations—except through overfitting the data. As a side experiment, we performed 10-fold cross validation on the PDTB, and again trained implicit relations by varying the cutoff. The results"
K15-2006,P02-1047,0,0.309417,"h and sentence boundaries. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syntactic features to disambiguate explicit connectives which increases performance close to a human baseline. 3.1 Argument Identification The SDP pipeline processes the documents sentence by sentence. Due to the strict time constraints of the Shared Task, we have set up a rulebased detector for both Arg1 and Arg2 spans as follows: Implicit relation classification In the early attempt by Marcu and Echihabi (2002), implicit relation classification was grounded on synthetic training data (relation patterns with explicit cue phrases removed) and a Naive Bayes model trained on word-pair features. Aggregation over such word-pairs was described by Biran and McKeown (2013), while Park and Cardie (2012) optimized feature sets through feature selection, preprocessing and special binning techniques. Out of these, implicit relation classification remains the most problematic subtask, and attracted • Extract an explicit Arg1–Arg2 pair, where Arg2 is a complete sentence starting with an explicit connective.4 The p"
K15-2006,meyers-etal-2004-annotating,0,0.051671,"a needs the sugar Arg2: it will be in sooner or later to buy it. • or to ground argument definitions in psycholinguistically more plausible models of propositions, cf. Lascarides and Asher (1993) or Kintsch (1998), resp.—their more operationalizable approximation in terms of, say, frame semantics as previously annotated for the PDTB data in the context of PropBank At least on a general basis, both argument spans are correctly identified by our system. The only 14 sense Document ID: wsj 2265, Relation ID: 36896. 46 We elaborate on this in our final subsection. and NomBank (Palmer et al., 2005; Meyers et al., 2004). 4.3.3 Abstracting from Surface-Level Features Our experiments for implicit relation classification have shown that is is beneficial to abstract from surface-level (token) features for two reasons: The latter idea may be challenging, as it involves efficient handling of multi-layer annotations for different major annotation projects, yet, experiments in this direction have successfully been conducted (Pustejovsky et al., 2005). This integrative direction of research has been the original focus of our system. 4.3.2 (i) word embeddings seem to express a more general, semantic representation of"
K15-2006,P09-1075,0,0.196398,"ing contextual and dependency information from the argument pairs and show that syntactic phrase-structure features help in level-2 relation type classifications. Moreover, Zhou et al. (2010) use a language model to “predict” explicit connectives from implicit relations. Our approach is most similar to the one by Rutherford and Xue (2014), who successfully integrate distributional representations to substitute word-pair features. Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. 3 Approach Our SDP system participates in the closed track of the Shared Task.3 Its components are illustrated in Figure 1. Input is tokenized text in the provided JSON format including"
K15-2006,J05-1004,0,0.0591107,"ate, she added, “India needs the sugar Arg2: it will be in sooner or later to buy it. • or to ground argument definitions in psycholinguistically more plausible models of propositions, cf. Lascarides and Asher (1993) or Kintsch (1998), resp.—their more operationalizable approximation in terms of, say, frame semantics as previously annotated for the PDTB data in the context of PropBank At least on a general basis, both argument spans are correctly identified by our system. The only 14 sense Document ID: wsj 2265, Relation ID: 36896. 46 We elaborate on this in our final subsection. and NomBank (Palmer et al., 2005; Meyers et al., 2004). 4.3.3 Abstracting from Surface-Level Features Our experiments for implicit relation classification have shown that is is beneficial to abstract from surface-level (token) features for two reasons: The latter idea may be challenging, as it involves efficient handling of multi-layer annotations for different major annotation projects, yet, experiments in this direction have successfully been conducted (Pustejovsky et al., 2005). This integrative direction of research has been the original focus of our system. 4.3.2 (i) word embeddings seem to express a more general, seman"
K15-2006,P12-1007,0,0.0945741,"information from the argument pairs and show that syntactic phrase-structure features help in level-2 relation type classifications. Moreover, Zhou et al. (2010) use a language model to “predict” explicit connectives from implicit relations. Our approach is most similar to the one by Rutherford and Xue (2014), who successfully integrate distributional representations to substitute word-pair features. Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. 3 Approach Our SDP system participates in the closed track of the Shared Task.3 Its components are illustrated in Figure 1. Input is tokenized text in the provided JSON format including meta information about"
K15-2006,W12-1622,0,0.266204,"se a language model to “predict” explicit connectives from implicit relations. Our approach is most similar to the one by Rutherford and Xue (2014), who successfully integrate distributional representations to substitute word-pair features. Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. 3 Approach Our SDP system participates in the closed track of the Shared Task.3 Its components are illustrated in Figure 1. Input is tokenized text in the provided JSON format including meta information about parts-of-speech and sentence boundaries. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue"
K15-2006,W12-1614,0,0.138398,"e close to a human baseline. 3.1 Argument Identification The SDP pipeline processes the documents sentence by sentence. Due to the strict time constraints of the Shared Task, we have set up a rulebased detector for both Arg1 and Arg2 spans as follows: Implicit relation classification In the early attempt by Marcu and Echihabi (2002), implicit relation classification was grounded on synthetic training data (relation patterns with explicit cue phrases removed) and a Naive Bayes model trained on word-pair features. Aggregation over such word-pairs was described by Biran and McKeown (2013), while Park and Cardie (2012) optimized feature sets through feature selection, preprocessing and special binning techniques. Out of these, implicit relation classification remains the most problematic subtask, and attracted • Extract an explicit Arg1–Arg2 pair, where Arg2 is a complete sentence starting with an explicit connective.4 The previous sentence serves as Arg1. 3 http://www.cs.brandeis.edu/˜clp/ conll15st/dataset.html 4 An exhaustive list of explicit cue words was obtained from the training section of the PDTB, ranging from unigrams to 7-grams. 43 • Refining step 1, we extract sentenceinternal explicit Arg1–Arg2"
K15-2006,J02-1002,0,0.0462617,"uch related to the span identification problem sketched above is the detection of discontinuous argument spans and cases in which our system adds a subordinate clause to the argument, which is not present in the gold data. We believe that—in line with the annotation guidelines of the PDTB— these are relevant factors to consider when implementing a SDP, but that it should not affect the overall evaluation in such a strict and rigid manner. We would therefore encourage future evaluations to • either employ additional metrics permitting partial matches, e.g., using sliding-window metrics such as Pevzner and Hearst (2002), Gold: Arg1: At any rate India needs the sugar Arg2: it will be in sooner or later to buy it Our System Output: Arg1: At any rate, she added, “India needs the sugar Arg2: it will be in sooner or later to buy it. • or to ground argument definitions in psycholinguistically more plausible models of propositions, cf. Lascarides and Asher (1993) or Kintsch (1998), resp.—their more operationalizable approximation in terms of, say, frame semantics as previously annotated for the PDTB data in the context of PropBank At least on a general basis, both argument spans are correctly identified by our syst"
K15-2006,D14-1008,0,0.0480823,"utherford and Xue (2014), who successfully integrate distributional representations to substitute word-pair features. Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. 3 Approach Our SDP system participates in the closed track of the Shared Task.3 Its components are illustrated in Figure 1. Input is tokenized text in the provided JSON format including meta information about parts-of-speech and sentence boundaries. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syntactic features to disambiguate explicit connectives whi"
K15-2006,P09-2004,0,0.0484452,"word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackle argument span detection on the constituent-level with features for subtrees and special constraints. 3 Approach Our SDP system participates in the closed track of the Shared Task.3 Its components are illustrated in Figure 1. Input is tokenized text in the provided JSON format including meta information about parts-of-speech and sentence boundaries. Explicit relation classification Classifying the senses of explicit relations is rather straightforward, given the cue phrase. Pitler and Nenkova (2009) introduce a refinement using syntactic features to disambiguate explicit connectives which increases performance close to a human baseline. 3.1 Argument Identification The SDP pipeline processes the documents sentence by sentence. Due to the strict time constraints of the Shared Task, we have set up a rulebased detector for both Arg1 and Arg2 spans as follows: Implicit relation classification In the early attempt by Marcu and Echihabi (2002), implicit relation classification was grounded on synthetic training data (relation patterns with explicit cue phrases removed) and a Naive Bayes model t"
K15-2006,P09-1077,0,0.0969123,"he surface-based definition of argument spans and their evaluation as string ranges, but with respect to sense disambiguation (in particular, in terms of precision), it is competitive with other systems in the task. Inspired by the diversity of different approaches to handle the more challenging—and more interesting—non-explicit relations, our description focuses on inferring implicit senses and benefits from abstracting from traditional surface-based features in favor of distributional representations of the argument spans. 2 Figure 1: Our three-component SDP pipeline. considerable interest: Pitler et al. (2009) present an extensive evaluation of mostly linguistically motivated features for implicit sense labeling in a 4-way classification experiment. Useful indicators, among others, are verb information, polarity labels and the first and last three words of an argument. Lin et al. (2009) refine their work by introducing contextual and dependency information from the argument pairs and show that syntactic phrase-structure features help in level-2 relation type classifications. Moreover, Zhou et al. (2010) use a language model to “predict” explicit connectives from implicit relations. Our approach is"
K15-2006,prasad-etal-2008-penn,0,0.548577,"Missing"
K15-2006,E14-1068,0,0.179032,"ation of mostly linguistically motivated features for implicit sense labeling in a 4-way classification experiment. Useful indicators, among others, are verb information, polarity labels and the first and last three words of an argument. Lin et al. (2009) refine their work by introducing contextual and dependency information from the argument pairs and show that syntactic phrase-structure features help in level-2 relation type classifications. Moreover, Zhou et al. (2010) use a language model to “predict” explicit connectives from implicit relations. Our approach is most similar to the one by Rutherford and Xue (2014), who successfully integrate distributional representations to substitute word-pair features. Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g., Ghosh et al. (2012) on the word and intersentential level, using a CRF-based approach including local and global features. Kong et al. (2014) tackl"
K15-2006,P10-1040,0,0.0216079,"Missing"
K15-2006,C10-2172,0,0.279185,"ns of the argument spans. 2 Figure 1: Our three-component SDP pipeline. considerable interest: Pitler et al. (2009) present an extensive evaluation of mostly linguistically motivated features for implicit sense labeling in a 4-way classification experiment. Useful indicators, among others, are verb information, polarity labels and the first and last three words of an argument. Lin et al. (2009) refine their work by introducing contextual and dependency information from the argument pairs and show that syntactic phrase-structure features help in level-2 relation type classifications. Moreover, Zhou et al. (2010) use a language model to “predict” explicit connectives from implicit relations. Our approach is most similar to the one by Rutherford and Xue (2014), who successfully integrate distributional representations to substitute word-pair features. Related Work At the moment, few full-fledged end-to-end discourse parsers exist, but they use different theories of discourse, e.g., PDTB (Lin et al., 2010), or RST (duVerle and Prendinger, 2009; Feng and Hirst, 2012). Most of the literature on automated discourse analysis has focused on specialized subtasks: Argument identification is approached by, e.g."
K15-2006,D09-1036,0,\N,Missing
K15-2006,K15-2001,0,\N,Missing
K16-2005,D14-1008,0,0.0223012,"onal neural networks to model implicit relations within the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to"
K16-2005,P13-2013,0,0.0137898,"milarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., P"
K16-2005,D14-1220,0,0.0130708,"for sense classification. Distributed representations for both arguments were obtained by vector concatenation of embeddings. An earlier attempt in a similar direction of representation learning (Bengio et al., 2013) has been made by Ji and Eisenstein (2014). The authors demonstrated successfully how to discriminatively learn a latent, low-dimensional feature representation for RST-style discourse parsing, which has the benefit of capturing the underlying meaning of elementary discourse units without suffering from data sparsity of the originally high dimensional input data. Closely related, Li et al. (2014) introduced a recursive neural network for discourse parsing which jointly models distributed representations for sentences based on words and syntactic information. The approach is motivated by Socher et al. (2013) and models the discourse unit’s root embedding to represent the whole discourse unit which is being obtained from its parts by an iterative process. Their system is made up of a binary structure classifier and a multi-class relation classifier and achieves similar performance compared to Ji and Eisenstein (2014). Very recently, Liu et al. (2016) and Zhang et al. (2015) have success"
K16-2005,W15-4612,0,0.0118349,"ng has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., PDTB or RST; and ii) different (sub)modules involve custom settings, feature- and tool-specific parameters, (esp. for the most challenging task of implicit sense labeling). Furthermore, iii) most previous works are not directly comparable in terms of overall accuracies as their underl"
K16-2005,D09-1036,0,0.248149,"Missing"
K16-2005,K15-2006,1,0.739845,"e implicit relations by our neural net-based architecture described in Section 3 given only the tokens and their dependencies in both argument spans. Finally, we merge all combined explicit and re-classified implicit relations into the final set for evaluation. 4.2 3. We heuristically post-process the CRF-labeled argument tokens in order to assign connectors to same-sentence or separate-sentence Arg1 and Arg2 spans. 4. The so-obtained explicit argument pairs are sense labeled by a (linear-kernel) SVM classifier12 with the connector word as the only feature, following the minimalist setting in Chiarcos and Schenk (2015). 5. As implicit relations we consider all intersentential relations which are not already part of an explicit relation. Same-sentence relations are ignored altogether. 4.4 For the provided argument pairs, we label explicit relations (i.e. those containing a non-empty connector) by the SVM classifier which has been trained using only a single feature – the connector token. For all other relations, we again employ our neural network-based strategy described in Section 3. The overall architecture is exactly the same as for the English subtask; only the (hyper)parameters have been updated in acco"
K16-2005,P09-1075,0,0.021725,"ized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., PDTB or RST; and ii) different (sub)modules involve custom settings, feature- and tool-specific parameters, (esp. for the most challenging task of implicit sense labeling). Furthermore, iii) most previous works are not directly comparable in terms of overall accuracies as their underlying evaluation data suffers f"
K16-2005,P12-1007,0,0.190241,"ment identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., PDTB or RST; and ii) different (sub)modules involve custom settings, feature- and tool-specific parameters, (esp. for the most challenging task of implicit sense labeling). Furthermore, iii) most previous works are not directly comparable in terms of overall accuracies as their underlying evaluation data suffers from inconsistent label"
K16-2005,P02-1047,0,0.134441,"work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual comp"
K16-2005,W12-1622,1,0.854026,"ly applied convolutional neural networks to model implicit relations within the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enor"
K16-2005,I11-1170,0,0.323049,"selves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., PDTB or RST; and ii) different (sub)modules involve custom settings, feature- and tool-specific parameters, (esp. for the most challenging task of implicit sense labeling). Furthermore, iii) most previous works are not directly comparable in terms of overall accuracies as their underlying evaluation data suffers from inconsistent label sizes among studies (e.g., full sense inventory vs. simplified 1- or 2-level classes, cf. Huang and Chen (2011)). 42 approach substitutes the traditional sparse and hand-crafted features from the literature to account for a minimalist, but at the same time, general (latent) representation of the discourse units. In the next sections, we elaborate on our novel neural network-based approach for implicit sense labeling and how it is fit into the overall system architecture of the parser. 3 proving their predictive power in the sense classification task. Specifically, the pre-trained vectors of size 300 were updated by the skip-gram method (Mikolov et al., 2013)5 in multiple passes over the Newswire texts"
K16-2005,P08-1028,0,0.0608724,"over the Newswire texts with decreasing learning rate. This procedure is supposed to improve the quality of the embeddings and also their coverage. Our new word vector model provides general vector representations for each token in the two argument spans6 , which forms the basis for producing compositional vectors to represent the two spans. Compositional vectors that introduce a fixed-length representation of a variable-length span of tokens are practical features for feedforward neural networks. Thus, we may combine the token vectors of each span by simply averaging vectors, or – following Mitchell and Lapata (2008) – by calculating an aggregated argument vector v~0 : A Neural Sense Labeler for Implicit and Entity Relations We construct a neural network-based module for the classification of senses for both implicit and entity (EntRel) relations.3 As a very general and highly data-driven approach to modeling discourse relations, our classifier incorporates only word embeddings and basic syntactic dependency information. Also, in order to keep the setup easily adaptable to new data and other languages, we avoid the use of very specific and costly hand-crafted features (such as sentiment polarities, word-p"
K16-2005,P14-1002,0,0.015102,"ost prior works on implicit SDP, but rather relies extensively on features learned from data. 2 2.1 Deep Learning Approaches to SDP In last year’s shared task, first implementations on deep learning have seen a surge of interest: Wang et al. (2015) and Okita et al. (2015) proposed a recurrent neural network for argument identification and a paragraph vector model for sense classification. Distributed representations for both arguments were obtained by vector concatenation of embeddings. An earlier attempt in a similar direction of representation learning (Bengio et al., 2013) has been made by Ji and Eisenstein (2014). The authors demonstrated successfully how to discriminatively learn a latent, low-dimensional feature representation for RST-style discourse parsing, which has the benefit of capturing the underlying meaning of elementary discourse units without suffering from data sparsity of the originally high dimensional input data. Closely related, Li et al. (2014) introduced a recursive neural network for discourse parsing which jointly models distributed representations for sentences based on words and syntactic information. The approach is motivated by Socher et al. (2013) and models the discourse un"
K16-2005,K15-2011,0,0.0167264,"hitecture is modular, highly generic and mostly language-independent, by leveraging the full power of pre-trained word embeddings for the SDP sense classification task. Our parser performs well on both English and Chinese data and is highly competitive with the state-of-the-art, though does not require manual feature engineering as employed in most prior works on implicit SDP, but rather relies extensively on features learned from data. 2 2.1 Deep Learning Approaches to SDP In last year’s shared task, first implementations on deep learning have seen a surge of interest: Wang et al. (2015) and Okita et al. (2015) proposed a recurrent neural network for argument identification and a paragraph vector model for sense classification. Distributed representations for both arguments were obtained by vector concatenation of embeddings. An earlier attempt in a similar direction of representation learning (Bengio et al., 2013) has been made by Ji and Eisenstein (2014). The authors demonstrated successfully how to discriminatively learn a latent, low-dimensional feature representation for RST-style discourse parsing, which has the benefit of capturing the underlying meaning of elementary discourse units without"
K16-2005,W12-1614,0,0.127357,"d techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theor"
K16-2005,K15-2002,0,0.115549,"is completed by alternative lexicalization (AltLex, discourse marker rephrased), entity relation (EntRel, i.e., anaphoric coherence), resp. the absence of any relation (NoRel). 41 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 41–49, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Fortunately, with the first edition of the shared task on SDP, Xue et al. (2015) had established a unified framework and had made an independent evaluation possible. The best performing participating systems – most notably those by Wang and Lan (2015) and Stepanov et al. (2015) – have reimplemented the well-established techniques, for example the one by Lin et al. (2014). between any given argument pair in the PDTB. Our Contribution: We participate in the CoNLL 2016 Shared Task on SDP (Xue et al., 2016; Potthast et al., 2014) and propose a novel, neural network-based approach for implicit sense labeling. Its system architecture is modular, highly generic and mostly language-independent, by leveraging the full power of pre-trained word embeddings for the SDP sense classification task. Our parser performs well on both English and Chinese dat"
K16-2005,P09-2004,0,0.0374985,"in the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves,"
K16-2005,K15-2014,0,0.0209491,"abeling. Its system architecture is modular, highly generic and mostly language-independent, by leveraging the full power of pre-trained word embeddings for the SDP sense classification task. Our parser performs well on both English and Chinese data and is highly competitive with the state-of-the-art, though does not require manual feature engineering as employed in most prior works on implicit SDP, but rather relies extensively on features learned from data. 2 2.1 Deep Learning Approaches to SDP In last year’s shared task, first implementations on deep learning have seen a surge of interest: Wang et al. (2015) and Okita et al. (2015) proposed a recurrent neural network for argument identification and a paragraph vector model for sense classification. Distributed representations for both arguments were obtained by vector concatenation of embeddings. An earlier attempt in a similar direction of representation learning (Bengio et al., 2013) has been made by Ji and Eisenstein (2014). The authors demonstrated successfully how to discriminatively learn a latent, low-dimensional feature representation for RST-style discourse parsing, which has the benefit of capturing the underlying meaning of elementary"
K16-2005,P09-1077,0,0.159442,"also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other com"
K16-2005,P15-3003,0,0.0115197,"ntactic information. The approach is motivated by Socher et al. (2013) and models the discourse unit’s root embedding to represent the whole discourse unit which is being obtained from its parts by an iterative process. Their system is made up of a binary structure classifier and a multi-class relation classifier and achieves similar performance compared to Ji and Eisenstein (2014). Very recently, Liu et al. (2016) and Zhang et al. (2015) have successfully applied convolutional neural networks to model implicit relations within the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2"
K16-2005,prasad-etal-2008-penn,0,0.162386,"Missing"
K16-2005,E14-1068,0,0.0618245,"ins a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., PDTB or RST; and ii) differe"
K16-2005,D15-1266,0,0.279615,"losely related, Li et al. (2014) introduced a recursive neural network for discourse parsing which jointly models distributed representations for sentences based on words and syntactic information. The approach is motivated by Socher et al. (2013) and models the discourse unit’s root embedding to represent the whole discourse unit which is being obtained from its parts by an iterative process. Their system is made up of a binary structure classifier and a multi-class relation classifier and achieves similar performance compared to Ji and Eisenstein (2014). Very recently, Liu et al. (2016) and Zhang et al. (2015) have successfully applied convolutional neural networks to model implicit relations within the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument"
K16-2005,D13-1170,0,0.0030223,"2013) has been made by Ji and Eisenstein (2014). The authors demonstrated successfully how to discriminatively learn a latent, low-dimensional feature representation for RST-style discourse parsing, which has the benefit of capturing the underlying meaning of elementary discourse units without suffering from data sparsity of the originally high dimensional input data. Closely related, Li et al. (2014) introduced a recursive neural network for discourse parsing which jointly models distributed representations for sentences based on words and syntactic information. The approach is motivated by Socher et al. (2013) and models the discourse unit’s root embedding to represent the whole discourse unit which is being obtained from its parts by an iterative process. Their system is made up of a binary structure classifier and a multi-class relation classifier and achieves similar performance compared to Ji and Eisenstein (2014). Very recently, Liu et al. (2016) and Zhang et al. (2015) have successfully applied convolutional neural networks to model implicit relations within the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network"
K16-2005,C10-2172,0,0.0209829,"neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems"
K16-2005,K15-2001,0,\N,Missing
K16-2005,P12-1008,0,\N,Missing
K16-2005,K15-2003,1,\N,Missing
L16-1234,P15-2044,0,0.0461925,"Missing"
L16-1234,W11-0402,1,0.85616,"al means to express imprecise mappings to the meta-tagset, e.g., by defining a languagespecific category as falling in the join (disjunction) of two language-indepenent meta-tags. Since even approaches relying on manual annotation refinement did not produce consistent results yet, any attempt to automatically integrate annotations from different source annotations using a standardized meta-tagset will eventually lead to information loss and potential inconsistencies in the definition for the respective tags – as previously observed by Hughes et al. (1995) for English EAGLES specifications and Chiarcos and Erjavec (2011) for MULTEXTEast. Ontology-based Annotation Integration • As a representation formalism, OWL ontologies provide means to express and to constrain imprecise mappings using properties expressing different degrees of confidence and class operators like join (t), intersection (u) and negation (¬) operators. In previous work on ensemble combination (Chiarcos, 2010) and annotation integration (Sukhareva and Chiarcos, 2015) decomposed string-based annotations into individual features (attribute-value pairs, represented as RDF triples) using the OLiA architecture (Chiarcos, 2008):3 Annotation schemes"
L16-1234,W14-0604,1,0.809564,"ingle-corpus trained neural networks) and yields state-of-the-art performance if compared with string-based POS taggers. Here, we adopt this approach to integrate multilingual source annotations in application to a single corpus of a historical language variety. We show that by exploiting OLiA, • there is no need to limit the annotations to coarsegrained universal POS tags, • neural networks in combination with pre-trained word embeddings allows us to achieve results comparable to Agi´c et al. (2015), but with higher granularity, and finally, translations, retellings or excerpts of the Bible (Chiarcos et al., 2014). Sukhareva and Chiarcos (2014) used such data to train fragmental dependency parsers on multilingual annotation projections to Old English, Middle Icelandic, and Early Modern High German. Later, Agi´c et al. (2015) used annotation projections from multiple Bibles aggregated by majority voting to train POS taggers for several target languages, including modern English, German, Danish and Icelandic. For our MLG experiments, we used a 15th c. Gospel of John digitized by the Middle Low German Reference Corpus.4 With only 19000 tokens (2442 verses) in total, this dataset is extremely sparse. As th"
L16-1234,N13-1132,0,0.0175904,"(2011) original proposal for UT. In order to put our results in relation to UT/UD-based projection results reported by Agi´c et al. (2015), we also provide UT-based accuracy scores in Tab. 1. UT annotation were obtained from the PTB, STTS and Alpino annotations converted to the universal tags using the mappings provided by Petrov.6 The existing STTS mapping was adapted for MLG gold annotations. Table 1 reports UT results for the test set, with direct mapping of single-source annotations to UT, UT disambiguation using majority vote on three source annotations and UT disambiguation using MACE (Hovy et al., 2013),7 a more elaborated expectation maximisation method originally developed for estimating the reliability of crowdsourced annotations. The accuracy of the tags predicted by MACE is slightly better than the accuracy of the tags predicted by majority vote. Nevertheless, the combination of the three predicted tags did not result in any improvement over the best monolingual tagger. The German source annotations have considerably higher accuracy than English and Dutch. This was rather predictable because of the similarities of MLG gold 6 https://github.com/slavpetrov/ universal-pos-tags 7 http://www"
L16-1234,N07-1047,0,0.0607452,"Missing"
L16-1234,J93-2004,0,0.0563569,"the target languages, wp the pialign-predicted target word, wm the m2m-predicted target word, p(·|w) the GIZA++ translation probability, and λ(x, y) relative Levenshtein distance. The following disambiguation heuristics apply: if wp = wm , return wp , else if p(wp |w) ≥ p(wm |w) and p(wp |w) > 0, return wp , else if p(wm |w) > 0, return wm , else 0 set wm and wp0 to those target language words with minimal Levenshtein distance from wm and wp 0 if λ(wp0 , wp ) ≥ λ(wm , wm ), return wp0 , 0 else return wm The normalized text is then annotated by the Stanford Tagger trained on the Penn Treebank (Marcus et al., 1993, PTB), the German NEGRA corpus (Skut et al., 1997, STTS) and the Dutch Alpino corpus (Bouma et al., 2001), respectively. These annotations are transferred to the original MLG text, these represent the sole annotation available for the training data and the basis for the experiments described below. 4. Experiment 1: Annotation Integration In order to compare OLiA-based annotation integration with mapping-based annotation integration, we compare the results of the direct mapping from individual tagsets to OLiA triples with a mapping mediated by Petrov et al.’s (2011) original proposal for UT. I"
L16-1234,P12-2059,0,0.0181377,"iginal Low German to modern German, English and Dutch; the normalised train set is then annotated with the Stanford tagger (Toutanova et al., 2003) trained on modern language corpora, and these annotations are then transferred to the original MLG text. Our task is to normalise (hyperlemmatize) every word in the test and training sets, but any word-based MT system available will lack sufficient coverage on the sparse amount of MLG training data. Accordingly, we employ characterbased machine translation (CBMT), known to be particularly useful when applied to related languages as in our setting (Nakov and Tiedemann, 2012). We combine two state-of-the-art character alignment tools (pialign, Neubig et al., 2012; m2m-aligner, Jiampojamarn et al., 2007) and one word alignment tool (GIZA++, Och and Ney, 2003). For training the different modules, 1. the parallel data is word-aligned using the GIZA++ IBM-2 model to English, German and Dutch, respectively, • triple decomposition allows us to untangle robustly predicted features and less robustly predicted features, so that we can bootstrap a prototypical annotation scheme specifically designed for Middle Low German. 3. 3.1. 2. we extract 1:1 alignments by resolving mu"
L16-1234,P12-1018,0,0.0547485,"d with the Stanford tagger (Toutanova et al., 2003) trained on modern language corpora, and these annotations are then transferred to the original MLG text. Our task is to normalise (hyperlemmatize) every word in the test and training sets, but any word-based MT system available will lack sufficient coverage on the sparse amount of MLG training data. Accordingly, we employ characterbased machine translation (CBMT), known to be particularly useful when applied to related languages as in our setting (Nakov and Tiedemann, 2012). We combine two state-of-the-art character alignment tools (pialign, Neubig et al., 2012; m2m-aligner, Jiampojamarn et al., 2007) and one word alignment tool (GIZA++, Och and Ney, 2003). For training the different modules, 1. the parallel data is word-aligned using the GIZA++ IBM-2 model to English, German and Dutch, respectively, • triple decomposition allows us to untangle robustly predicted features and less robustly predicted features, so that we can bootstrap a prototypical annotation scheme specifically designed for Middle Low German. 3. 3.1. 2. we extract 1:1 alignments by resolving multi-words alignment using lexical translation probabilities, Prerequisites 3. we limit th"
L16-1234,J03-1002,0,0.00668652,"annotations are then transferred to the original MLG text. Our task is to normalise (hyperlemmatize) every word in the test and training sets, but any word-based MT system available will lack sufficient coverage on the sparse amount of MLG training data. Accordingly, we employ characterbased machine translation (CBMT), known to be particularly useful when applied to related languages as in our setting (Nakov and Tiedemann, 2012). We combine two state-of-the-art character alignment tools (pialign, Neubig et al., 2012; m2m-aligner, Jiampojamarn et al., 2007) and one word alignment tool (GIZA++, Och and Ney, 2003). For training the different modules, 1. the parallel data is word-aligned using the GIZA++ IBM-2 model to English, German and Dutch, respectively, • triple decomposition allows us to untangle robustly predicted features and less robustly predicted features, so that we can bootstrap a prototypical annotation scheme specifically designed for Middle Low German. 3. 3.1. 2. we extract 1:1 alignments by resolving multi-words alignment using lexical translation probabilities, Prerequisites 3. we limit the raw word list to word pairs hsrc, tgti with relative Levenshtein distance5 λ(src, tgt) ≤ 0.4 (t"
L16-1234,pareja-lora-de-cea-2010-ontology,0,0.0741897,"Missing"
L16-1234,petrov-etal-2012-universal,0,0.0526967,"from inconsistencies between different annotation schemes employed for modern major languages like German, Dutch and English. Two primary approaches have been proposed to address this problem, 1471 based on post-hoc mappings and ontology-based tagset decomposition, respectively. 2.1. Mapping-based Annotation Integration The traditional approach to this problem is to map different existing schemes to a uniform meta tagset, as exemplified in EAGLES (Leech and Wilson, 1996), MULTEXT-East (Erjavec and Ide, 1998), and, more recently, in Petrov et al.’s original proposal for a Universal POS tagset (Petrov et al., 2012). It should be noted, though, that a post-hoc mapping of existing schemes is highly problematic as it aims to leverage independent terminological and analytical traditions developed by different communities for individual languages. This has been acknowledged by current proposals for a universal tagset as part of the ‘Universal Dependencies’ (Nivre and others, 2015, UD)2 which actively enforce reannotating existing corpora to facilitate conformance to language-independent specifications. But even Universal Tagset (further abbreviated UT) suffers from languagespecific differences which – at the"
L16-1234,W14-0605,0,0.0130669,"nual annotation is extremely expensive, as it requires rare and specialized linguistic expertise. For other languages, where manually annotated resources are available, these are generally sparse, orthographically inconsistent and use different annotation schemes that hinder the direct application of the state-of-the-art statistical NLP tools (e.g., Old English as part of PROIEL and PCHE). In order to solve the problem to adapt statistical NLP tools for contemporary languages to historical and/or lowresource varieties, both annotation projection (Tiedemann, 2014) and normalisation approaches (Pettersson et al., 2014). Yet, as far as Middle Low German is concerned, we present the first experiment in automatically producing morphosyntactic annotations for this particular language. Yet, albeit no part-of-speech (POS) tagset is currently agreed upon, information from related languages can be employed to approximate a part-of-speech (POS) tagset: Low German has a common origin with English, German and Dutch, and it evolved in close language contact with Dutch, German and Scandinavian languages, so that it shares many phonological, morphological and morphosyntactic traits with them. Accordingly, we employ the p"
L16-1234,rognvaldsson-etal-2012-icelandic,0,0.109255,"Missing"
L16-1234,A97-1014,0,0.0703639,"t word, wm the m2m-predicted target word, p(·|w) the GIZA++ translation probability, and λ(x, y) relative Levenshtein distance. The following disambiguation heuristics apply: if wp = wm , return wp , else if p(wp |w) ≥ p(wm |w) and p(wp |w) > 0, return wp , else if p(wm |w) > 0, return wm , else 0 set wm and wp0 to those target language words with minimal Levenshtein distance from wm and wp 0 if λ(wp0 , wp ) ≥ λ(wm , wm ), return wp0 , 0 else return wm The normalized text is then annotated by the Stanford Tagger trained on the Penn Treebank (Marcus et al., 1993, PTB), the German NEGRA corpus (Skut et al., 1997, STTS) and the Dutch Alpino corpus (Bouma et al., 2001), respectively. These annotations are transferred to the original MLG text, these represent the sole annotation available for the training data and the basis for the experiments described below. 4. Experiment 1: Annotation Integration In order to compare OLiA-based annotation integration with mapping-based annotation integration, we compare the results of the direct mapping from individual tagsets to OLiA triples with a mapping mediated by Petrov et al.’s (2011) original proposal for UT. In order to put our results in relation to UT/UD-ba"
L16-1234,W14-5302,1,0.423896,"ral networks) and yields state-of-the-art performance if compared with string-based POS taggers. Here, we adopt this approach to integrate multilingual source annotations in application to a single corpus of a historical language variety. We show that by exploiting OLiA, • there is no need to limit the annotations to coarsegrained universal POS tags, • neural networks in combination with pre-trained word embeddings allows us to achieve results comparable to Agi´c et al. (2015), but with higher granularity, and finally, translations, retellings or excerpts of the Bible (Chiarcos et al., 2014). Sukhareva and Chiarcos (2014) used such data to train fragmental dependency parsers on multilingual annotation projections to Old English, Middle Icelandic, and Early Modern High German. Later, Agi´c et al. (2015) used annotation projections from multiple Bibles aggregated by majority voting to train POS taggers for several target languages, including modern English, German, Danish and Icelandic. For our MLG experiments, we used a 15th c. Gospel of John digitized by the Middle Low German Reference Corpus.4 With only 19000 tokens (2442 verses) in total, this dataset is extremely sparse. As the corpus did not have any POS a"
L16-1234,W15-5505,1,0.831519,"this approach is bound to lose morphological and morphosyntactic information, making the resulting resource less valuable for studies in history, linguistics or philology. It is thus not desirable in a Digital Humanities context (as the primary locus of most research on historical language varieties). For the case of historical varieties of Germanic languages, this is even more problematic, as they tend to be morphologically richer than modern Germanic languages. 2.2. Ontology-based approaches, as described by Chiarcos (2010), Pareja-Lora and Aguado de Cea (2010), Hellmann et al. (2013), and Sukhareva and Chiarcos (2015) represent a promising, though understudied, alternative to mappingbased annotation integration, as they allow to circumvent the problems of mapping-based annotation integration: • Without enforcing a fixed (and minimal) set of possible tags onto different languages, a broad set of terms can be organized in a hierarchical fashion, permitting varying degrees of granularity. • Unlike (meta-)tagsets, this hierarchical organization is not a tree, but it employs a directed graph of subClassOf (v) relations; it does not impose implicit disjointness constraints which need to be resolved in laborinten"
L16-1234,C14-1175,0,0.012785,"texts for medieval (Middle) Low German, its manual annotation is extremely expensive, as it requires rare and specialized linguistic expertise. For other languages, where manually annotated resources are available, these are generally sparse, orthographically inconsistent and use different annotation schemes that hinder the direct application of the state-of-the-art statistical NLP tools (e.g., Old English as part of PROIEL and PCHE). In order to solve the problem to adapt statistical NLP tools for contemporary languages to historical and/or lowresource varieties, both annotation projection (Tiedemann, 2014) and normalisation approaches (Pettersson et al., 2014). Yet, as far as Middle Low German is concerned, we present the first experiment in automatically producing morphosyntactic annotations for this particular language. Yet, albeit no part-of-speech (POS) tagset is currently agreed upon, information from related languages can be employed to approximate a part-of-speech (POS) tagset: Low German has a common origin with English, German and Dutch, and it evolved in close language contact with Dutch, German and Scandinavian languages, so that it shares many phonological, morphological and morphos"
L16-1234,N03-1033,0,0.0621094,"d tagset STTS (Schiller et al., 1999), further referred to as ‘MLG gold’. The MLG gold annotations of both annotators were represented as OLiA triples. For training, the remaining 18000 tokens were employed. 3.2. As we consider the amount of MLG training data barely sufficient for direct annotation projection, we opted for a combined multilingual projection-adaptation approach, instead. We employ machine translation to normalize (or, more precisely, hyperlemmatize) the original Low German to modern German, English and Dutch; the normalised train set is then annotated with the Stanford tagger (Toutanova et al., 2003) trained on modern language corpora, and these annotations are then transferred to the original MLG text. Our task is to normalise (hyperlemmatize) every word in the test and training sets, but any word-based MT system available will lack sufficient coverage on the sparse amount of MLG training data. Accordingly, we employ characterbased machine translation (CBMT), known to be particularly useful when applied to related languages as in our setting (Nakov and Tiedemann, 2012). We combine two state-of-the-art character alignment tools (pialign, Neubig et al., 2012; m2m-aligner, Jiampojamarn et a"
L16-1234,zeman-2008-reusable,0,0.0242805,"severe problems arise with approaches ignorant against grammatical features involved in POS annotation. As such, Petrov et al.’s (2011) original proposal for a universal POS tagset was massively underspecified: It involved reductions from hundreds of tags (e.g., 294 tags for the Sinica/CoNLL07 treebank) to a total of of only 12 – later extended to 18. Although this approach is still considered state of the art (Agi´c et al., 2015), it is extremely reductionist. It might have an application in technical applications, but without including additional specifications for morphosyntactic features (Zeman, 2008), this approach is bound to lose morphological and morphosyntactic information, making the resulting resource less valuable for studies in history, linguistics or philology. It is thus not desirable in a Digital Humanities context (as the primary locus of most research on historical language varieties). For the case of historical varieties of Germanic languages, this is even more problematic, as they tend to be morphologically richer than modern Germanic languages. 2.2. Ontology-based approaches, as described by Chiarcos (2010), Pareja-Lora and Aguado de Cea (2010), Hellmann et al. (2013), and"
L16-1386,2016.gwc-1.9,1,0.821562,"gy, since they are used as domain specific ontologies. Additionally, other supporting ontologies have been added, such as GeoNames for the named entities; PROTON as an upper ontology; SKOS as a mapper between ontologies and terminological lexicons; Dublin Core as a metadata ontology. Also, for the purposes of search, Web Interface Querying EUCases Linking Platform was designed. For its Web Interface, the EUCases Linking Platform relies on a customized version of the GraphDB Workbench27 , developed by Ontotext AD. 5.2. Wordnet Interlingual Index (ILI) A recent development (Vossen et al., 2016; Bond et al., 2016) has been the adoption of LLOD technology by the wordnet community, with a new plan that uses LLOD as the basic mechanism for the creation of links between wordnets in different languages. This Collaborative InterLingual Index enables wordnets to share and link their resources for concepts lexicalized in any of the group’s languages. This was supported directly by a workshop at the 2016 Global WordNet Conference and will lead to the adoption of LLOD technology by a new community. In addition, the open multilingual wordnet (Bond et al., 2014) provides all open wordnets for download using the le"
L16-1386,calzolari-etal-2012-lre,0,0.0712075,"Missing"
L16-1386,ehrmann-etal-2014-representing,1,0.804428,"lable by attempting to download it and discarding all resources that are no longer available. We have attempted to notify the authors of resources that no longer meet the criteria for inclusion in the cloud. However, our experience has been that this did not motivate many authors to update their resources. 2.5. Vocabularies The Linguistic Linked Open Data Cloud has grown significantly in the last few years and most notably, unlike the non-linguistic LOD Cloud, is not centered around one nucleus but instead has used many different vocabularies and datasets to link to. Among these are BabelNet (Ehrmann et al., 2014), LexInfo (Cimiano et al., 2011), and Lexvo (de Melo, 2015). In addition, a number of new vocabularies have emerged including the OntoLex model,13 the NLP Interchange format NIF (Hellmann et al., 2013), the WordNet Interlingual Index (Sect. 5.2.), and the FrameBase schema (Rouces et al., 2015a) (Sect. 5.3.). These vocabularies have increased the power of linked data to represent the complete spectrum of language resources and show that new resources can be created that use the power of linked data to link across different types of languages resources, such as terminologies and dictionaries (Si"
L16-1386,federmann-etal-2012-meta,0,0.0601026,"Missing"
L16-1386,W15-4205,1,0.920405,"not necessarily created for this purpose, e.g., large collections of texts such as news articles, terminological or encyclopedic and general-purpose knowledge bases such as DBpedia (Bizer et al., 2009), or metadata collections. 2.2. Infrastructure and Metadata The OWLG provides guidelines to data publishers on how to include their resources in the LLOD cloud.6 The cloud diagram is currently generated from metadata maintained at DataHub7 and hence contains only resources described in DataHub. An alternative metadata repository specialized for linguistic resources is under development: Linghub (McCrae et al., 2015a).8 It aims to provide a search engine and index for linguistic resources and attempts to harmonize metadata from a number of different sources, including Metashare (Federmann et al., 2012), CLARIN VLO (Van Uytvanck et al., 2012), DataHub and LRE Map (Calzolari et al., 2012). It will soon replace DataHub in the generation of the cloud diagram. LingHub, 4 http://lod2.eu/ http://qtleap.eu/ 6 http://wiki.okfn.org/Working_Groups/ Linguistics/How_to_contribute 7 http://datahub.io 8 http://linghub.org 5 Datasets Links 28 53 103 126 128 41 78 167 203 209 February 2012 September 2013 November 2014 Ma"
L16-1386,W15-4201,0,0.02663,"Missing"
L16-1386,W15-4207,1,0.813122,"4), LexInfo (Cimiano et al., 2011), and Lexvo (de Melo, 2015). In addition, a number of new vocabularies have emerged including the OntoLex model,13 the NLP Interchange format NIF (Hellmann et al., 2013), the WordNet Interlingual Index (Sect. 5.2.), and the FrameBase schema (Rouces et al., 2015a) (Sect. 5.3.). These vocabularies have increased the power of linked data to represent the complete spectrum of language resources and show that new resources can be created that use the power of linked data to link across different types of languages resources, such as terminologies and dictionaries (Siemoneit et al., 2015) and corpora and dictionaries (McGovern et al., 2015). 3. OWLG members have been very active in promoting the development and adoption of linguistic linked data, which had an effect not only in the growth of the LLOD cloud but in the development of representation models, guidelines, and best practices. These activities have been developed in the context of a number of W3C groups and projects, as it is detailed in the rest of this section. 13 12 http://lodvader.aksw.org/ Other Community Group Efforts http://cimiano.github.io/ontolex/ specification.html 2437 3.1. OntoLex 8. LLOD aware services 1"
L16-1386,van-uytvanck-etal-2012-semantic,0,0.0695217,"Missing"
L16-1642,A00-2032,0,0.392411,"types of word segmentation algorithms: rule-based segmentation rules derived from grammar dictionary-based segmentation by lookup in a (statically enhanced) dictionary statistical/machine learning data-driven segmentation as learnt from segmented corpora As shown in several SIGHAN BakeOffs in the last decade (Sproat and Emerson, 2003), in Chinese machine learning and dictionary-based approaches like MaxMatch (Chen and Liu, 1992) produce reasonable results while rule-based methods are commonly used as a Baseline (Palmer and Burger, 1997). In Japanese, however, rule-based algorithms like Tango (Ando and Lee, 2000) proved to be more successful. This is partially due to the morphological richness of Japanese as compared to Chinese. As a point of orientation for subsequent studies on cuneiform, we evaluate selected approaches from these classes in their performance on Akkadian. Neither the Akkadian language nor cuneiform as a writing system have been addressed in this respect before. Along with other cuneiform languages, Akkadian has a considerable research history in NLP. For the greatest part, existing approaches are concerned with rule-based morphological analyzers, e.g., Kataja and Koskenniemi (1988),"
L16-1642,W98-1010,0,0.795529,"proved to be more successful. This is partially due to the morphological richness of Japanese as compared to Chinese. As a point of orientation for subsequent studies on cuneiform, we evaluate selected approaches from these classes in their performance on Akkadian. Neither the Akkadian language nor cuneiform as a writing system have been addressed in this respect before. Along with other cuneiform languages, Akkadian has a considerable research history in NLP. For the greatest part, existing approaches are concerned with rule-based morphological analyzers, e.g., Kataja and Koskenniemi (1988), Barthlemy (1998), Macks (2002), Barthlemy (2009), Khait (accepted) for Akkadian, or Valentin Tablan Wim Peters (2006) for Sumerian. As for data-driven morphological tools, the state of the art in the field is represented by the Lemmatizer of the Open Richly Annotated Cuneiform Corpus (ORACC),1 which supports manual morphological annotation for Akkadian, Sumerian and (to a limited degree) Hittite with a lookup-functionality in the annotated corpus. Such example-based approaches can be extended to automatically transfer morphological rules through phonological equivalences, as demonstrated by Snyder et al. (201"
L16-1642,W09-0802,0,0.0181553,"is is partially due to the morphological richness of Japanese as compared to Chinese. As a point of orientation for subsequent studies on cuneiform, we evaluate selected approaches from these classes in their performance on Akkadian. Neither the Akkadian language nor cuneiform as a writing system have been addressed in this respect before. Along with other cuneiform languages, Akkadian has a considerable research history in NLP. For the greatest part, existing approaches are concerned with rule-based morphological analyzers, e.g., Kataja and Koskenniemi (1988), Barthlemy (1998), Macks (2002), Barthlemy (2009), Khait (accepted) for Akkadian, or Valentin Tablan Wim Peters (2006) for Sumerian. As for data-driven morphological tools, the state of the art in the field is represented by the Lemmatizer of the Open Richly Annotated Cuneiform Corpus (ORACC),1 which supports manual morphological annotation for Akkadian, Sumerian and (to a limited degree) Hittite with a lookup-functionality in the annotated corpus. Such example-based approaches can be extended to automatically transfer morphological rules through phonological equivalences, as demonstrated by Snyder et al. (2010) for the projection of Hebrew"
L16-1642,C92-1019,0,0.661994,"n cuneiform OCR and Akkadian NLP. • CVC syllables (e.g., dan ) can be as a pair of CV-VC characters ( da-an ) or with a single CVC character 4067 2. State Of the Art We distinguish three types of word segmentation algorithms: rule-based segmentation rules derived from grammar dictionary-based segmentation by lookup in a (statically enhanced) dictionary statistical/machine learning data-driven segmentation as learnt from segmented corpora As shown in several SIGHAN BakeOffs in the last decade (Sproat and Emerson, 2003), in Chinese machine learning and dictionary-based approaches like MaxMatch (Chen and Liu, 1992) produce reasonable results while rule-based methods are commonly used as a Baseline (Palmer and Burger, 1997). In Japanese, however, rule-based algorithms like Tango (Ando and Lee, 2000) proved to be more successful. This is partially due to the morphological richness of Japanese as compared to Chinese. As a point of orientation for subsequent studies on cuneiform, we evaluate selected approaches from these classes in their performance on Akkadian. Neither the Akkadian language nor cuneiform as a writing system have been addressed in this respect before. Along with other cuneiform languages,"
L16-1642,C08-1047,0,0.015172,"lity in the annotated corpus. Such example-based approaches can be extended to automatically transfer morphological rules through phonological equivalences, as demonstrated by Snyder et al. (2010) for the projection of Hebrew morphology and lexicon to Ugaritic, another Semitic cuneiform language. As for higher levels of linguistic analysis, we are not aware of any tools for syntactic or semantic annotation for Akkadian, however, the latter has been considered for administrative texts from the Sumerian period, whose highly conventionalized structure can be exploited for concept classification (Jaworski, 2008). Aside from linguistic analysis, another aspect of cuneiform languages that recently aroused interest are approaches focusing on the material side of cuneiform writing, i.e., scanning and digitizing clay tablets (Subodh et al., 2003; Cohen et al., 2004), reconstructing tablets and tales by automatically combining their fragments (Collins et al., accepted; Tyndall, 2012), and recently, initial steps towards cuneiform OCR have been undertaken (Mara et al., 2010). As this line of research is flourishing mostly in the field of computer graphics, the obvious gap between both lines of research lies"
L16-1642,C88-1064,0,0.839312,"ike Tango (Ando and Lee, 2000) proved to be more successful. This is partially due to the morphological richness of Japanese as compared to Chinese. As a point of orientation for subsequent studies on cuneiform, we evaluate selected approaches from these classes in their performance on Akkadian. Neither the Akkadian language nor cuneiform as a writing system have been addressed in this respect before. Along with other cuneiform languages, Akkadian has a considerable research history in NLP. For the greatest part, existing approaches are concerned with rule-based morphological analyzers, e.g., Kataja and Koskenniemi (1988), Barthlemy (1998), Macks (2002), Barthlemy (2009), Khait (accepted) for Akkadian, or Valentin Tablan Wim Peters (2006) for Sumerian. As for data-driven morphological tools, the state of the art in the field is represented by the Lemmatizer of the Open Richly Annotated Cuneiform Corpus (ORACC),1 which supports manual morphological annotation for Akkadian, Sumerian and (to a limited degree) Hittite with a lookup-functionality in the annotated corpus. Such example-based approaches can be extended to automatically transfer morphological rules through phonological equivalences, as demonstrated by"
L16-1642,I05-3025,0,0.0927113,"Missing"
L16-1642,W02-0501,0,0.765557,"successful. This is partially due to the morphological richness of Japanese as compared to Chinese. As a point of orientation for subsequent studies on cuneiform, we evaluate selected approaches from these classes in their performance on Akkadian. Neither the Akkadian language nor cuneiform as a writing system have been addressed in this respect before. Along with other cuneiform languages, Akkadian has a considerable research history in NLP. For the greatest part, existing approaches are concerned with rule-based morphological analyzers, e.g., Kataja and Koskenniemi (1988), Barthlemy (1998), Macks (2002), Barthlemy (2009), Khait (accepted) for Akkadian, or Valentin Tablan Wim Peters (2006) for Sumerian. As for data-driven morphological tools, the state of the art in the field is represented by the Lemmatizer of the Open Richly Annotated Cuneiform Corpus (ORACC),1 which supports manual morphological annotation for Akkadian, Sumerian and (to a limited degree) Hittite with a lookup-functionality in the annotated corpus. Such example-based approaches can be extended to automatically transfer morphological rules through phonological equivalences, as demonstrated by Snyder et al. (2010) for the pro"
L16-1642,H94-1054,0,0.568908,"Missing"
L16-1642,J02-1002,0,0.0672096,"Missing"
L16-1642,N12-1038,0,0.0545577,"Missing"
L16-1642,P10-1107,0,0.0328225,", Barthlemy (1998), Macks (2002), Barthlemy (2009), Khait (accepted) for Akkadian, or Valentin Tablan Wim Peters (2006) for Sumerian. As for data-driven morphological tools, the state of the art in the field is represented by the Lemmatizer of the Open Richly Annotated Cuneiform Corpus (ORACC),1 which supports manual morphological annotation for Akkadian, Sumerian and (to a limited degree) Hittite with a lookup-functionality in the annotated corpus. Such example-based approaches can be extended to automatically transfer morphological rules through phonological equivalences, as demonstrated by Snyder et al. (2010) for the projection of Hebrew morphology and lexicon to Ugaritic, another Semitic cuneiform language. As for higher levels of linguistic analysis, we are not aware of any tools for syntactic or semantic annotation for Akkadian, however, the latter has been considered for administrative texts from the Sumerian period, whose highly conventionalized structure can be exploited for concept classification (Jaworski, 2008). Aside from linguistic analysis, another aspect of cuneiform languages that recently aroused interest are approaches focusing on the material side of cuneiform writing, i.e., scann"
L16-1642,I08-4025,0,0.0635808,"Missing"
L16-1642,W03-1719,0,0.058271,"nt experiments on cuneiform word segmentation and will be of utmost importance to future experiments on cuneiform OCR and Akkadian NLP. • CVC syllables (e.g., dan ) can be as a pair of CV-VC characters ( da-an ) or with a single CVC character 4067 2. State Of the Art We distinguish three types of word segmentation algorithms: rule-based segmentation rules derived from grammar dictionary-based segmentation by lookup in a (statically enhanced) dictionary statistical/machine learning data-driven segmentation as learnt from segmented corpora As shown in several SIGHAN BakeOffs in the last decade (Sproat and Emerson, 2003), in Chinese machine learning and dictionary-based approaches like MaxMatch (Chen and Liu, 1992) produce reasonable results while rule-based methods are commonly used as a Baseline (Palmer and Burger, 1997). In Japanese, however, rule-based algorithms like Tango (Ando and Lee, 2000) proved to be more successful. This is partially due to the morphological richness of Japanese as compared to Chinese. As a point of orientation for subsequent studies on cuneiform, we evaluate selected approaches from these classes in their performance on Akkadian. Neither the Akkadian language nor cuneiform as a w"
L16-1642,P12-2048,0,0.0200644,"or syntactic or semantic annotation for Akkadian, however, the latter has been considered for administrative texts from the Sumerian period, whose highly conventionalized structure can be exploited for concept classification (Jaworski, 2008). Aside from linguistic analysis, another aspect of cuneiform languages that recently aroused interest are approaches focusing on the material side of cuneiform writing, i.e., scanning and digitizing clay tablets (Subodh et al., 2003; Cohen et al., 2004), reconstructing tablets and tales by automatically combining their fragments (Collins et al., accepted; Tyndall, 2012), and recently, initial steps towards cuneiform OCR have been undertaken (Mara et al., 2010). As this line of research is flourishing mostly in the field of computer graphics, the obvious gap between both lines of research lies in the absence of any studies concerned with the transition from the (identified) sign and its linguistic interpretation, a challenging task, as mentioned before. With our paper, we describe the first experiments in this direction, with a specific focus on segmenting character sequences into words as a core component for future approaches on transliteration. 3. Experime"
L16-1642,tablan-etal-2006-creating,0,0.544512,"Missing"
L16-1707,W15-4205,0,0.0311613,"DF store. We currently evaluate different data base solutions. Candidate RDF stores include Jena TDB31 , Blazegraph32 , Allegrograph33 , Openlink Virtuoso34 and RDF-HDT35 . With performance results becoming available we will publish our experiences in upcoming publications. Implementation Considerations We employ Linghub (McCrae and Cimiano, 2015) as a starting point for retrieving LLOD data as it provides a uniform way to access LLOD resources. The Linghub portal stores metadata about roughly 250,000 linguistic resources. Metadata is modelled using DCAT, Dublin Core and META-SHARE standards (McCrae et al., 2015). The portal allows for browsing its online catalogue, and also supports SPARQL queries on the site as well as a service. As such linghub metadata is also available as an RDF dump which we will exploit instead of using its online SPARQL service. The main reason being that the service is limited because it supports only a small fragment of the SPARQL standard (YuzuQL). Furthermore, querying over a network would lead to increased query times and overloading of the linghub service. In order to identify resources relevant for the Lin|gu|is|tik portal, the metadata can be queried for up to 400 prop"
L18-1090,chiarcos-2012-generic,1,0.903718,"g., :s1.2 conll:A1 :s1.1 . In consequence, we obtain an isomorphic representation of the original CoNLL data structure in RDF which is semantically shallow,6 but can be effectively queried, manipulated and serialized back into CoNLL using off-the-shelf RDF technology. In particular, this includes a rich infrastructure of databases, webservices, APIs, models for resource publication and linking (Chiarcos et al., 2013). Even though it lacks formal semantics (by design), the CoNLL RDF model can also serve as a basis to transform CoNLL data into semantically well-defined formalisms such as POWLA (Chiarcos, 2012) or full-fledged NIF (Hellmann et al., 2013). For the en-bloc conversion of CoNLL data to CoNLL-RDF, we provide the JAVA class CoNLL2RDF. Fig. 3 illustrates CoNLL-RDF sample data for the first sentence of the German UD development set, together with its rendering in CoNLL and other derived representations. 4. Advanced Graph Operations A key advantage of an RDF representation is that a W3Cstandardized query language for the flexible querying and 4 The UD sent id is currently not used, because it only appears in a comment. However, future support for UD-specifics is possible. 5 http://persistenc"
L18-1090,N06-2015,0,0.225925,"Missing"
L18-1387,C08-1047,0,0.0328433,"y classification, for which we build on earlier efforts towards semi-automated entity annotation (Liu et al., 2015, SNER).19 Ur III administrative data are comparably easy in terms of morphosyntax, since morphology is often not expressed in writing (though the information may be inferred from the context). However, this also means that morphology is somewhat uninformative, so that effective querying and searching of these data requires structural analysis. We thus retrieve relational information in addition to morphosyntax as found in ETCSRI. This extends earlier work on (semantic) parsing by Jaworski (2008a) in that we do not rely on domain-specific rules, rather we employ state-of-the-art machine learning techniques. At the moment, we are evaluating the suitability of the Universal Dependency (UD)20 schema and UD-based annotation projection for these kind of data, possibly to be augmented with an additional layer of semantics (Peterson et al., 2014): since administrative texts are not exclusively composed of grammatical sentences but also often comprise lists, semantic role labeling (SRL) annotation is considered crucial for this genre. The SRL inventory will be based on Hayes (2000) and Jawor"
L18-1387,N15-1167,0,0.0707006,"Missing"
L18-1387,W17-2202,1,0.897349,"Missing"
L18-1387,peterson-etal-2014-focusing,0,0.0192266,"ology is somewhat uninformative, so that effective querying and searching of these data requires structural analysis. We thus retrieve relational information in addition to morphosyntax as found in ETCSRI. This extends earlier work on (semantic) parsing by Jaworski (2008a) in that we do not rely on domain-specific rules, rather we employ state-of-the-art machine learning techniques. At the moment, we are evaluating the suitability of the Universal Dependency (UD)20 schema and UD-based annotation projection for these kind of data, possibly to be augmented with an additional layer of semantics (Peterson et al., 2014): since administrative texts are not exclusively composed of grammatical sentences but also often comprise lists, semantic role labeling (SRL) annotation is considered crucial for this genre. The SRL inventory will be based on Hayes (2000) and Jaworski (2008b), yet grounded in the English PropBank. Since further details of the annotation process will be presented elsewhere, we focus here on infrastructural measures. 3. 3.1. Towards Linked Data Corpus Representation The (Canonical-)ASCII Transliteration Format ((C)ATF) is a text encoding format developed by CDLI and the Electronic Pennsylvania"
L18-1417,J95-2003,0,0.749005,"g between existing morpheme-based annotations (which 2635 refer to grammatical roles) and cases of the corresponding arguments (whose identification depends on identifying the tense feature) – which may create a barrier for creating UniMorph resources.13 4.2. A ranking, again nominative &gt; accusative &gt; other In extension of the ranking-based modeling of multiple case marking, it is possible to generalize over both the casebased and the grammatical-role-based encoding of arguments as well as over compound and regular features for arguments in different languages. At least since Dowty (1991) and Grosz et al. (1995), the importance of aligned hierarchies of grammatical and semantic roles has been recognized in different communicative functions, and established as such in both linguistics and natural language processing: According to Dowty, a ranking of semantic roles (AGENT &gt; PATIENT &gt; ...) is underlying the assignment of grammatical roles; according to Grosz et al., a ranking of grammatical (or semantic) roles is taken to reflect and to establish discourse salience (Subject &gt; Object &gt; ...) which is closely tied with pronominalization. By extension of this approach, highly salient discourse referents can"
L18-1417,L16-1498,0,0.013118,", with columns for the word form, the lemma and morphological features; it is thus roughly comparable to the CoNLL format as previously used for, e.g., syntactically annotated corpora of Classical Armenian (Haug and Jøhndal, 2008).5 The primary data structure of UniMorph is an unordered set of semicolon-separated, unqualified features. Figure 1 shows an example of a conventional gloss of the Megrelian word keˇserxvaduk ‘I will meet you’ together with its UniMorph representation. UniMorph resources are rarely original resources, but rather extracted from existing material,6 such as Wiktionary (Kirov et al., 2016, first-generation UniMorph inventories) and other dictionaries, bootstrapped from morpheme inventories or corpora (as described here), or generated by rulebased morphologies. However, this conversion-based approach means that the segmentation and annotation principles of the underlying resource tend to be preserved. In general, UniMorph follows a word-based approach to morphology where inflected forms are organized in paradigms, but their internal structure left unanalyzed. In language documentation, however, a morpheme-based approach prevails, i.e., words are segmented into morphemes which a"
L18-1417,L16-1386,1,0.89517,"Missing"
L18-1417,P15-2111,0,0.168025,"Armenian, we discuss challenges that the complex morphology of these and related languages poses to the current design of UniMorph, and suggest possibilities to improve the applicability of UniMorph for languages of the Caucasus region in particular and for low resource languages in general. We also criticize the UniMorph TSV format for its limited expressiveness, and suggest to complement the existing UniMorph workflow with support for additional source formats on grounds of Linked Open Data technology. Keywords: morphology, Caucasus, UniMorph 1. Background The Universal Morphology project (Sylak-Glassman et al., 2015b, UniMorph)1 is a recent community effort aiming to complement the Universal Dependencies (Nivre and others, 2015, UD),2 which focus on syntax, with coverage of morphology. We describe the development of UniMorph resources for languages of the Caucasus region, known for its rich and diverse arrays of languages and language families, and often posing challenges to European-centered views established in traditional linguistics. In particular, we focus on Nakh-Daghestanian (North-East Caucasian) and Kartvelian (South Caucasian) languages, as well as on Classical Armenian, and discuss challenges"
L18-1717,W14-0612,0,0.0496074,"Missing"
L18-1717,I08-1051,0,0.552612,"Missing"
L18-1717,chiarcos-2012-generic,1,0.891797,"9.26 conll:LEMMA ‘‘¨ er’’. In consequence, we obtain an isomorphic representation of the original CoNLL data structure in RDF which is semantically shallow,8 but can be effectively queried, manipulated and serialized back into CoNLL using off-the-shelf RDF technology. In particular, this includes a rich infrastructure of databases, webservices, APIs, models for resource publication and linking (Chiarcos et al., 2013). Even though it lacks formal semantics (by design), the CoNLL-RDF model can also serve as a basis to transform CoNLL data into semantically well-defined formalisms such as POWLA (Chiarcos, 2012) or NIF (Hellmann et al., 2013). CoNLL-RDF comes with a Java API that allows to parse CoNLL data into CoNLL-RDF, to apply and to iterate SPARQL update transformations on this data, and to serialize conll: graphs in a lossless fashion as TSV (e.g., CoNLL-U or CoNLL-X), a human-readable dependency view or as a compact RDF/TTL representation that uses one word per line, ;-separated annotations and attribute-value pairs for different annotations. The latter serialization is also referred to as canonical CoNLL-RDF. 6 https://github.com/acoli-repo/conll-rdf http://persistence.uni-leipzig.org/ nlp2rd"
L18-1717,W97-0802,0,0.702529,"Missing"
L18-1717,henrich-hinrichs-2010-gernedit,0,0.0308784,"Wohnung). Therefore, we consulted K¨obler (2014) as a source of secondary evidence. Hyperlemmatization modules can be run multiple times, with different word lists and in different configurations (e.g., for fuzzy search), each adding a column with hyperlemma candidates. 3.3. Enriching with animacy Animacy is considered to be a major factor of word order, with the assumption that animate referents tend to precede inanimate referents (Jacobs, 1988) , and it can be relatively easily derived from lexical resources such as the Princeton WordNet (Fellbaum, 1998), resp., GermaNet (Hamp et al., 1997; Henrich and Hinrichs, 2010). As WordNet is hierarchically organized, we retrieved the top-level synsets from WordNet 3.1 and (where appropriate) classified them for their animacy. We employ three primary classes: Human, Animate (non-human), Inanimate. Human includes persons, but also groups and organizations; Animate includes animals, plants and bacteria, but no plant or animal products; Inanimate includes substances and objects, but also abstractions. For non-classified synsets, we increased the search depth and iterated the procedure. For verification, we ran animacy annotation against the entire WordNet and inspected"
L18-1717,hinrichs-zastrow-2012-automatic,0,0.0150988,"he same clause Figure 8: Advanced SPARQL: Identifying the last NX in the preceding partial parse (b) Distribution of direct and indirect objects in MHG verses (a) Distribution of direct and indirect objects in MHG prose Figure 9: Diachronic quantitative analysis of the word order of direct and indirect objects in MHG Figure 10: Resulting POWLA RDF graph 6. Summary and conclusion We describe a pipeline for the syntactic annotation and the semantic enrichment of Middle High German. To our best knowledge, NLP for Middle High German consists of early prototypes towards morphosyntactic annotation (Hinrichs and Zastrow, 2012; Schulz and Kuhn, 2016). For more abstract levels, however, we are not aware of any attempts to conduct automated syntactic or semantic annotation on Middle High German. Our approach builds on two core formalisms, the CoNLL format (resp., a specific dialect), and RDF. In general, pipeline modules communicate via CoNLL, resp. a TSV format, however, this seamlessly integrates with the SPARQL-based extraction of semantic features from WordNet 3.1 (i.e., a SPARQL SELECT query which produces TSV data) and with the SPARQL-based syntactic annotation (building on CoNLL-RDF). The resulting POWLA RDF d"
L18-1717,de-marneffe-etal-2014-universal,0,0.0370534,"un multiple times for different hyperlemma columns. If a TSV file for another feature is provided, this annotator can also be used for other kinds of lexical annotation. CL VF LB MF si zeigete si zeigete dem k¨ uninge den mandel NP NP dem k¨uninge den mandel (CL(VF *) (LB *) (MF(NP * *) (NP * *))) Figure 3: MHG sample parse (ReM, M403-G1, simplified) in tree view and conventional CoNLL 3.4. Annotation with CoNLL-RDF CoNLL is an established exchange format in NLP, and enjoys high popularity as a representation formalism for dependency syntax, e.g., in the context of the Universal Dependencies (Marneffe et al., 2014) . For representing topological fields as part of a syntactic analysis, however, it is necessary to establish nonterminal nodes that span multiple words, and that are combined to form clauses and sentences. The conventional representation of CFG parses introduced with CoNLL 2005, however, requires to represent nonterminal nodes implicitly by pairs of matching brackets in different words: The word si in Fig. 3 thus carries the annotations of the prefield (VF) node as well as those of 4528 its dominating clause (CL) node, whose right bracket only closes with the word mandel. Processing phrasal a"
L18-1717,W10-1820,0,0.0349102,", a SPARQL SELECT query which produces TSV data) and with the SPARQL-based syntactic annotation (building on CoNLL-RDF). The resulting POWLA RDF data structure can be conveniently queried using SPARQL SELECT. With respect to syntactic parsing we provide – to our best knowledge – the first publicly available implementation of a parser which solely relies on off-the-shelf Semantic Web technology. Related research includes the application of RDF and OWL for corpus representation and querying (Burchardt et al., 2008; Chiarcos, 2012) as well as a backend formalism for manual dependency annotation (Mazziotta, 2010). The only experiment on automated natural language parsing we are aware of (Wilcock, 2007), differs greatly by design from our implementation. Unfortunately, this implementation never left an experimental stage (p.c. G. Wilcock, Sep 2015). This experiment heavily relied on OWL/DL reasoning, resp., the use of rule languages building on top of OWL (Wilcock, 2006), and was thus relatively resource-intense. In comparison, our approach is designed to perform shallow, fast and transparent graph transformations using a formalism (CoNLL-RDF) that allows its integration in existing NLP pipelines. Its"
L18-1717,L16-1684,0,0.0139974,"anced SPARQL: Identifying the last NX in the preceding partial parse (b) Distribution of direct and indirect objects in MHG verses (a) Distribution of direct and indirect objects in MHG prose Figure 9: Diachronic quantitative analysis of the word order of direct and indirect objects in MHG Figure 10: Resulting POWLA RDF graph 6. Summary and conclusion We describe a pipeline for the syntactic annotation and the semantic enrichment of Middle High German. To our best knowledge, NLP for Middle High German consists of early prototypes towards morphosyntactic annotation (Hinrichs and Zastrow, 2012; Schulz and Kuhn, 2016). For more abstract levels, however, we are not aware of any attempts to conduct automated syntactic or semantic annotation on Middle High German. Our approach builds on two core formalisms, the CoNLL format (resp., a specific dialect), and RDF. In general, pipeline modules communicate via CoNLL, resp. a TSV format, however, this seamlessly integrates with the SPARQL-based extraction of semantic features from WordNet 3.1 (i.e., a SPARQL SELECT query which produces TSV data) and with the SPARQL-based syntactic annotation (building on CoNLL-RDF). The resulting POWLA RDF data structure can be con"
L18-1717,P07-2043,0,0.0307589,"on (building on CoNLL-RDF). The resulting POWLA RDF data structure can be conveniently queried using SPARQL SELECT. With respect to syntactic parsing we provide – to our best knowledge – the first publicly available implementation of a parser which solely relies on off-the-shelf Semantic Web technology. Related research includes the application of RDF and OWL for corpus representation and querying (Burchardt et al., 2008; Chiarcos, 2012) as well as a backend formalism for manual dependency annotation (Mazziotta, 2010). The only experiment on automated natural language parsing we are aware of (Wilcock, 2007), differs greatly by design from our implementation. Unfortunately, this implementation never left an experimental stage (p.c. G. Wilcock, Sep 2015). This experiment heavily relied on OWL/DL reasoning, resp., the use of rule languages building on top of OWL (Wilcock, 2006), and was thus relatively resource-intense. In comparison, our approach is designed to perform shallow, fast and transparent graph transformations using a formalism (CoNLL-RDF) that allows its integration in existing NLP pipelines. Its modular structure allows simple and comfortable integration of additional rules implemented"
L18-1721,L16-1707,1,0.379149,"Missing"
N16-1173,P98-1013,0,0.463204,"or languages) for which no training annotations are available. Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) has become a well-established and highly important NLP component which directly benefits various downstream applications, such as text summarization (Trandab˘a¸t, 2011), recognizing textual entailment (Sammons et al., 2012) or QA systems (Shen and Lapata, 2007; Moreda et al., 2011). Its goal is to detect verbal or nominal predicates, together with their associated arguments and semantic roles, either by PropBank/Nombank (Palmer et al., 2005; Meyers et al., 2004) or FrameNet (Baker et al., 1998) analysis. In its traditional form, however, SRL is restricted to the local syntactic context of the predicate as in the following example from Ruppenhofer et al. (2010): In a FrameNet-style analysis of the sentence, the predicate place evokes the P LACING frame, with two frame elements (roles) overtly expressed (T HEME and T IME) but with one role – G OAL – beyond the embedded relative clause and thus beyond the scope of the SRL parser. Such implicit roles, or null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005) are much harder to detect automatically, as they require to broaden the"
N16-1173,W09-1206,0,0.0677521,"Missing"
N16-1173,S10-1059,0,0.0584804,"rrounding discourse, commonly also to preceding (or following) sentences. State-of-the-art approaches to implicit SRL (iSRL) are supervised and need a groundwork of hand-annotated training data – which is costly, extremely sparse, limited to only a handful of predicates, and requires careful feature engineering (Gerber and Chai, 2012; Silberer and Frank, 2012; Li et al., 2015). A first attempt has been made to combine the scarce resources available (Feizabadi and Pad´o, 2015), but given the great diversity of predicatespecific roles and enormous complexity of the task, the main issues remain (Chen et al., 2010). A promising exploratory effort recently made by Gorinski et al. (2013) aims to overcome the annotation bottleneck by using distributional methods to infer evidence for elements filling null instantiated roles. The authors do not rely on gold annotations but instead learn distributional properties of fillers induced from a large corpus. [G OAL /NI In the centre of this room] there was an upright beam, [T HEME which] had been placed [T IME at some period] as a support for Our Contribution: We propose an extension of the distributional idea for unsupervised iSRL to loosen the need for annotated"
N16-1173,J14-1002,0,0.055323,"heir respective (manual) syntax annotation, but could easily be extracted using automated phrase-structure parsers. The candidate vectors for arbitrary length n-grams are derived in the same way (by means of Equation 1). 2.3 Training Resources & Tools In accordance with domain-specific evaluation data, we chose to learn protofillers on two distinct corpora: The Corpus of Late Modern English Texts, CLMET (Smet, 2005) (≈35M tokens, 18th–20th century novels) and a subset of the English Gigaword corpus (Graff and Cieri, 2003) (≈500M tokens of Newswire texts). We label the first one with SEMAFOR1 (Das et al., 2014), a FrameNet-style semantic parser. We employ MATE2 (Bj¨orkelund et al., 2009) to obtain a PropBank/NomBank analysis for each sentence in Gigaword. # explicit roles # predicate instances # roles per predicate # predicates per sentence CLMET Gigaword 21.9M 9.5M 2.3 7.6 264.0M 122.5M 2.2 4.2 Table 1: Statistics on the number of explicit fillers used for training protofillers. Table 1 highlights general statistics on the number of predicates collected from both corpora. Two observations are worth noting: While on average the number of explicitly realized roles/frame elements per predicate/frame i"
N16-1173,S15-1005,0,0.141562,"Missing"
N16-1173,P10-1160,0,0.617975,".0 0.2 x Figure 1: Clustered projection of the ten nominal predicates from Gerber and Chai (2010) in protofiller space. method also on this data set: Table 3 reports the classification scores for implicit argument resolution compared to the state-of-the-art (Laparra and Rigau, 2013). We restrict the search for implicit arguments to certain predicate-specific parts-of-speech, since some syntactic constituents (e.g., SBAR) never occur as implicit arguments. For choosing the final implicit arguments for each individual predicate instance, we follow the same deterministic strategy as described in Gerber and Chai (2010), which informally states that, if a certain role is not overtly expressed (within a chain of mentions of the same predicate in previous sentences), it is an implicit candidate. POS lists and cosine similarity thresholds which trigger an actual prediction have been optimized on the development set. The context window for candidate NIs is optimal for the current and previous two sentences in our setting, which explains why the the number of candidate constituents is approximately twice as large for the NomBank predicates (cf. Table 2). Our best-performing protofillers are again obtained by Coll"
N16-1173,J12-4003,0,0.357817,"but with one role – G OAL – beyond the embedded relative clause and thus beyond the scope of the SRL parser. Such implicit roles, or null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005) are much harder to detect automatically, as they require to broaden the analysis to the surrounding discourse, commonly also to preceding (or following) sentences. State-of-the-art approaches to implicit SRL (iSRL) are supervised and need a groundwork of hand-annotated training data – which is costly, extremely sparse, limited to only a handful of predicates, and requires careful feature engineering (Gerber and Chai, 2012; Silberer and Frank, 2012; Li et al., 2015). A first attempt has been made to combine the scarce resources available (Feizabadi and Pad´o, 2015), but given the great diversity of predicatespecific roles and enormous complexity of the task, the main issues remain (Chen et al., 2010). A promising exploratory effort recently made by Gorinski et al. (2013) aims to overcome the annotation bottleneck by using distributional methods to infer evidence for elements filling null instantiated roles. The authors do not rely on gold annotations but instead learn distributional properties of fillers induce"
N16-1173,J02-3001,0,0.46407,"notations for supervised implicit semantic role labeling are extremely sparse and costly. As a lightweight alternative, this paper describes an approach based on unsupervised parsing which can do without iSRL-specific training data: We induce prototypical roles from large amounts of explicit SRL annotations paired with their distributed word representations. An evaluation shows competitive performance with supervised methods on the SemEval 2010 data, and our method can easily be applied to predicates (or languages) for which no training annotations are available. Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) has become a well-established and highly important NLP component which directly benefits various downstream applications, such as text summarization (Trandab˘a¸t, 2011), recognizing textual entailment (Sammons et al., 2012) or QA systems (Shen and Lapata, 2007; Moreda et al., 2011). Its goal is to detect verbal or nominal predicates, together with their associated arguments and semantic roles, either by PropBank/Nombank (Palmer et al., 2005; Meyers et al., 2004) or FrameNet (Baker et al., 1998) analysis. In its traditional form, however, SRL is restricted to the local syntactic context of the"
N16-1173,W13-0111,0,0.641232,"nces. State-of-the-art approaches to implicit SRL (iSRL) are supervised and need a groundwork of hand-annotated training data – which is costly, extremely sparse, limited to only a handful of predicates, and requires careful feature engineering (Gerber and Chai, 2012; Silberer and Frank, 2012; Li et al., 2015). A first attempt has been made to combine the scarce resources available (Feizabadi and Pad´o, 2015), but given the great diversity of predicatespecific roles and enormous complexity of the task, the main issues remain (Chen et al., 2010). A promising exploratory effort recently made by Gorinski et al. (2013) aims to overcome the annotation bottleneck by using distributional methods to infer evidence for elements filling null instantiated roles. The authors do not rely on gold annotations but instead learn distributional properties of fillers induced from a large corpus. [G OAL /NI In the centre of this room] there was an upright beam, [T HEME which] had been placed [T IME at some period] as a support for Our Contribution: We propose an extension of the distributional idea for unsupervised iSRL to loosen the need for annotated training data. Specifically, we 1 Introduction 1473 Proceedings of NAAC"
N16-1173,P14-1136,0,0.0123095,"ution: We propose an extension of the distributional idea for unsupervised iSRL to loosen the need for annotated training data. Specifically, we 1 Introduction 1473 Proceedings of NAACL-HLT 2016, pages 1473–1479, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics propose to induce predicate and role-specific prototypical fillers from large amounts of SRL annotated texts in order to resolve null instantiations as (semantically and syntactically) similar elements found in the context. Parts of our approach have been successfully applied in traditional SRL (Hermann et al., 2014), but not yet to implicit roles. Our work differs from Gorinski et al. (2013) in that we extend discrete context vectors to SRL-guided embeddings and experiment with a variety of different configurations. We intend not to set a new benchmark beating the current state-of-the-art for supervised iSRL, but rather provide a simple and alternative strategy which does not rely on manually annotated gold data. Still, we demonstrate that our method is highly competitive with supervised methods on one out of two standard evaluation sets and that it can easily be extended to other predicates for which no"
N16-1173,P13-1116,0,0.384221,"Missing"
N16-1173,P14-2050,0,0.022828,"meNet lexicon and its more fine-grained modeling of lexical units, as 1 2 http://www.cs.cmu.edu/˜ark/SEMAFOR/ https://code.google.com/p/mate-tools/ opposed to PropBank. Also note that FrameNet currently specifies 9.7 frame elements per lexical frame3 which – despite the fact that this number also comprises non-core arguments – is much larger than what can explicitly be labeled by the SRL systems. Regarding the distributional component, we experimented with a variety of distributed word representations: We chose out of the box vectors; Collobert et al. (2011), dependency-based word embeddings (Levy and Goldberg, 2014) and the pre-trained Google News vectors from word2vec4 (Mikolov et al., 2013). Using the same tool, we also trained custom embeddings (bag-of-words and skip-gram) with 50 dimensions on our two corpora. 3 Evaluation In order to assess the usefulness of our approach, a quantitative evaluation has been conducted on two iSRL test sets which have become a de facto standard in this domain: a collection of fiction novels from the SemEval 2010 Shared Task with manual annotations of null instantiations (Ruppenhofer et al., 2010), and Gerber and Chai (2010)’s augmented NomBank data set. Table 2 shows s"
N16-1173,P15-1122,0,0.0157164,"relative clause and thus beyond the scope of the SRL parser. Such implicit roles, or null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005) are much harder to detect automatically, as they require to broaden the analysis to the surrounding discourse, commonly also to preceding (or following) sentences. State-of-the-art approaches to implicit SRL (iSRL) are supervised and need a groundwork of hand-annotated training data – which is costly, extremely sparse, limited to only a handful of predicates, and requires careful feature engineering (Gerber and Chai, 2012; Silberer and Frank, 2012; Li et al., 2015). A first attempt has been made to combine the scarce resources available (Feizabadi and Pad´o, 2015), but given the great diversity of predicatespecific roles and enormous complexity of the task, the main issues remain (Chen et al., 2010). A promising exploratory effort recently made by Gorinski et al. (2013) aims to overcome the annotation bottleneck by using distributional methods to infer evidence for elements filling null instantiated roles. The authors do not rely on gold annotations but instead learn distributional properties of fillers induced from a large corpus. [G OAL /NI In the cen"
N16-1173,W04-2705,0,0.425586,"easily be applied to predicates (or languages) for which no training annotations are available. Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) has become a well-established and highly important NLP component which directly benefits various downstream applications, such as text summarization (Trandab˘a¸t, 2011), recognizing textual entailment (Sammons et al., 2012) or QA systems (Shen and Lapata, 2007; Moreda et al., 2011). Its goal is to detect verbal or nominal predicates, together with their associated arguments and semantic roles, either by PropBank/Nombank (Palmer et al., 2005; Meyers et al., 2004) or FrameNet (Baker et al., 1998) analysis. In its traditional form, however, SRL is restricted to the local syntactic context of the predicate as in the following example from Ruppenhofer et al. (2010): In a FrameNet-style analysis of the sentence, the predicate place evokes the P LACING frame, with two frame elements (roles) overtly expressed (T HEME and T IME) but with one role – G OAL – beyond the embedded relative clause and thus beyond the scope of the SRL parser. Such implicit roles, or null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005) are much harder to detect automatically"
N16-1173,J05-1004,0,0.157112,"a, and our method can easily be applied to predicates (or languages) for which no training annotations are available. Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) has become a well-established and highly important NLP component which directly benefits various downstream applications, such as text summarization (Trandab˘a¸t, 2011), recognizing textual entailment (Sammons et al., 2012) or QA systems (Shen and Lapata, 2007; Moreda et al., 2011). Its goal is to detect verbal or nominal predicates, together with their associated arguments and semantic roles, either by PropBank/Nombank (Palmer et al., 2005; Meyers et al., 2004) or FrameNet (Baker et al., 1998) analysis. In its traditional form, however, SRL is restricted to the local syntactic context of the predicate as in the following example from Ruppenhofer et al. (2010): In a FrameNet-style analysis of the sentence, the predicate place evokes the P LACING frame, with two frame elements (roles) overtly expressed (T HEME and T IME) but with one role – G OAL – beyond the embedded relative clause and thus beyond the scope of the SRL parser. Such implicit roles, or null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005) are much harder t"
N16-1173,S10-1008,0,0.427516,"portant NLP component which directly benefits various downstream applications, such as text summarization (Trandab˘a¸t, 2011), recognizing textual entailment (Sammons et al., 2012) or QA systems (Shen and Lapata, 2007; Moreda et al., 2011). Its goal is to detect verbal or nominal predicates, together with their associated arguments and semantic roles, either by PropBank/Nombank (Palmer et al., 2005; Meyers et al., 2004) or FrameNet (Baker et al., 1998) analysis. In its traditional form, however, SRL is restricted to the local syntactic context of the predicate as in the following example from Ruppenhofer et al. (2010): In a FrameNet-style analysis of the sentence, the predicate place evokes the P LACING frame, with two frame elements (roles) overtly expressed (T HEME and T IME) but with one role – G OAL – beyond the embedded relative clause and thus beyond the scope of the SRL parser. Such implicit roles, or null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005) are much harder to detect automatically, as they require to broaden the analysis to the surrounding discourse, commonly also to preceding (or following) sentences. State-of-the-art approaches to implicit SRL (iSRL) are supervised and need a"
N16-1173,D07-1002,0,0.127709,"large amounts of explicit SRL annotations paired with their distributed word representations. An evaluation shows competitive performance with supervised methods on the SemEval 2010 data, and our method can easily be applied to predicates (or languages) for which no training annotations are available. Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) has become a well-established and highly important NLP component which directly benefits various downstream applications, such as text summarization (Trandab˘a¸t, 2011), recognizing textual entailment (Sammons et al., 2012) or QA systems (Shen and Lapata, 2007; Moreda et al., 2011). Its goal is to detect verbal or nominal predicates, together with their associated arguments and semantic roles, either by PropBank/Nombank (Palmer et al., 2005; Meyers et al., 2004) or FrameNet (Baker et al., 1998) analysis. In its traditional form, however, SRL is restricted to the local syntactic context of the predicate as in the following example from Ruppenhofer et al. (2010): In a FrameNet-style analysis of the sentence, the predicate place evokes the P LACING frame, with two frame elements (roles) overtly expressed (T HEME and T IME) but with one role – G OAL –"
N16-1173,S12-1001,0,0.425219,"OAL – beyond the embedded relative clause and thus beyond the scope of the SRL parser. Such implicit roles, or null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005) are much harder to detect automatically, as they require to broaden the analysis to the surrounding discourse, commonly also to preceding (or following) sentences. State-of-the-art approaches to implicit SRL (iSRL) are supervised and need a groundwork of hand-annotated training data – which is costly, extremely sparse, limited to only a handful of predicates, and requires careful feature engineering (Gerber and Chai, 2012; Silberer and Frank, 2012; Li et al., 2015). A first attempt has been made to combine the scarce resources available (Feizabadi and Pad´o, 2015), but given the great diversity of predicatespecific roles and enormous complexity of the task, the main issues remain (Chen et al., 2010). A promising exploratory effort recently made by Gorinski et al. (2013) aims to overcome the annotation bottleneck by using distributional methods to infer evidence for elements filling null instantiated roles. The authors do not rely on gold annotations but instead learn distributional properties of fillers induced from a large corpus. [G"
N16-1173,W14-5302,1,0.844877,"from using small-scale inventories of semantic roles. It should be noted though, that our approach is not restricted to any particular SRL tagset, but can be equally applied to other role inventories with similar degrees of consistence and size. Beyond SRL annotations in a strict sense, this might even extend to syntactic dependency annotations that are occasionally taken as a substitute for semantic roles proper. In particular, we see potential in combining our experiments with on-going efforts to cross-lingual projection, adaptation and harmonization of syntax annotations along the lines of Sukhareva and Chiarcos (2014, 2016) and related approaches based on frameworks such as the Universal Dependencies (Nivre, 2015, UD).7 If successful, an adaptation using grammatical relations rather than semantic roles represents a promising possibility to create iSRL annotation and iSRL annotation tools for other languages, as Universal Dependencies are becoming increasingly available for major and low-resourced languages and can be projected to others. The protofillers involved in this study are available at: http://acoli.cs. uni-frankfurt.de/resources. 7 http://universaldependencies.github.io Acknowledgments The author"
N16-1173,L16-1234,1,0.884094,"Missing"
N16-1173,W11-2822,0,0.0301183,"Missing"
N16-1173,C98-1013,0,\N,Missing
P10-1068,P98-1029,0,0.150429,"the closer ontology-based integration of grammatical and semantic information using OntoTag and several NLP tools for Spanish. Aguado de Cea et al. (2008) evaluate the benefits of this approach for the Spanish particle se, and conclude for this example that the combination of multiple tools yields more detailed and more accurate linguistic analyses of particularly problematic, polysemous function words. A similar increase in accuracy has also been repeatedly reported for ensemble combination approaches, that are, however, limited to tools that produce annotations according to the same tagset (Brill and Wu, 1998; Halteren et al., 2001). These observations provide further support for our conclusion that the ontology-based integration of morphosyntactic analyses enhances both the robustness and the level of detail of morphosyntactic and morphological analyses. Our approach extends the philosophy of ensemble combination approaches to NLP tools that do not only employ different strategies and philosophies, but also different annotation schemes. (iv) Application of the algorithm for the ontological processing of node labels and edge labels in syntax annotations. (v) Integration with other ontological know"
P10-1068,de-cea-etal-2008-tagging,0,0.0620976,"Missing"
P10-1068,I08-1051,0,0.176891,"Missing"
P10-1068,buyko-etal-2008-ontology,1,0.701653,"Missing"
P10-1068,W10-1825,1,0.831143,"morphological analyses. Our approach extends the philosophy of ensemble combination approaches to NLP tools that do not only employ different strategies and philosophies, but also different annotation schemes. (iv) Application of the algorithm for the ontological processing of node labels and edge labels in syntax annotations. (v) Integration with other ontological knowledge sources in order to improve the recall of morphosyntactic and morphological analyses (e.g., for disambiguating grammatical case). Extensions (iii) and (iv) are currently pursued in an ongoing research effort described by Chiarcos et al. (2010). Like morphosyntactic and morphological features, node and edge labels of syntactic trees are ontologically represented in several Annotation Models, the OLiA Reference Model, and External Reference Models, the merging algorithm as described above can thus be applied for syntax, as well. Syntactic annotations, however, involve the additional challenge to align different structures before node and edge labels can be addressed, an issue not further discussed here for reasons of space limitations. Alternative strategies to merge grammatical analyses may include alternative voting strategies as d"
P10-1068,borin-2000-something,0,0.197504,"e Abstract (parts of speech, pos) and morphology in German: In comparison to English, German shows a rich and polysemous morphology, and a considerable number of NLP tools are available, making it a promising candidate for such an experiment. Previous research indicates that the integration of multiple part of speech taggers leads to more accurate analyses. So far, however, this line of research focused on tools that were trained on the same corpus (Brill and Wu, 1998; Halteren et al., 2001), or that specialize to different subsets of the same tagset (Zavrel and Daelemans, 2000; Tufis¸, 2000; Borin, 2000). An even more substantial increase in accuracy and detail can be expected if tools are combined that make use of different annotation schemes. For this task, ontologies of linguistic annotations are employed to assess the linguistic information conveyed in a particular annotation and to integrate the resulting ontological descriptions in a consistent and tool-independent way. The merged set of ontological descriptions is then evaluated with reference to morphosyntactic and morphological annotations of three corpora of German newspaper articles, the NEGRA corpus (Skut et al., 1998), the TIGER"
P10-1068,brants-hansen-2002-developments,0,0.153313,"ions of properties. Instead, it assumes that such constraints are inherited by means of v relationships from an External Reference Model. Different External Reference Models may take different positions on the issue – as languages do3 –, so that this aspect is left underspecified in the Reference Model. Figs. 2 and 4 show excerpts of category and feature hierarchies in the Reference Model. With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and J¨arvinen, 1997, pos, morph). Further Annotation Models for pos and morph cover five different annotation schemes for English (Marcus et al., 1994; Sampson, 1995; Mandel, 2006; Kim et al., 2003, Connexor), two annotation schemes for Russian (Meyer, 2003; Sharoff et al., 2008), an annotation scheme designed for typological research and currently applied to approx. 30 different languages (Dipper et al., 2007), an annotation scheme for Old High German (Petrova et al."
P10-1068,N06-2015,0,0.0422805,"tive pronouns in the Morphisto scheme) will be missing in the resulting string representation. For complex annotations, where ontological descriptions correspond to different substrings, an additional ‘tag grammar’ may be necessary to determine the appropriate ordering of substrings according to the annotation scheme (e.g., in the Connexor analysis). 8 Preposition-determiner compounds like German am ‘on the’, for example, are both prepositions and determiners. 666 resources such as WordNet (Gangemi et al., 2003), FrameNet (Scheffczyk et al., 2006), the linking of corpora with such ontologies (Hovy et al., 2006), the modelling of entire corpora in OWL/DL (Burchardt et al., 2008), and the extension of existing ontologies with ontological representations of selected linguistic features (Buitelaar et al., 2006; Davis et al., 2008). Aguado de Cea et al. (2004) sketched an architecture for the closer ontology-based integration of grammatical and semantic information using OntoTag and several NLP tools for Spanish. Aguado de Cea et al. (2008) evaluate the benefits of this approach for the Spanish particle se, and conclude for this example that the combination of multiple tools yields more detailed and more"
P10-1068,davis-etal-2008-linguistically,0,0.0279157,"be necessary to determine the appropriate ordering of substrings according to the annotation scheme (e.g., in the Connexor analysis). 8 Preposition-determiner compounds like German am ‘on the’, for example, are both prepositions and determiners. 666 resources such as WordNet (Gangemi et al., 2003), FrameNet (Scheffczyk et al., 2006), the linking of corpora with such ontologies (Hovy et al., 2006), the modelling of entire corpora in OWL/DL (Burchardt et al., 2008), and the extension of existing ontologies with ontological representations of selected linguistic features (Buitelaar et al., 2006; Davis et al., 2008). Aguado de Cea et al. (2004) sketched an architecture for the closer ontology-based integration of grammatical and semantic information using OntoTag and several NLP tools for Spanish. Aguado de Cea et al. (2008) evaluate the benefits of this approach for the Spanish particle se, and conclude for this example that the combination of multiple tools yields more detailed and more accurate linguistic analyses of particularly problematic, polysemous function words. A similar increase in accuracy has also been repeatedly reported for ensemble combination approaches, that are, however, limited to to"
P10-1068,ide-romary-2004-registry,0,0.328714,"This paper evaluates the potential benefits of such an approach with respect to morphosyntax 2 Ontologies and annotations Various repositories of linguistic annotation terminology have been developed in the last decades, ranging from early texts on annotation standards (Bakker et al., 1993; Leech and Wilson, 1996) over relational data base models (Bickel and Nichols, 2000; Bickel and Nichols, 2002) to more recent formalizations in OWL/RDF (or with OWL/RDF export), e.g., the General Ontology of Linguistic Description (Farrar and Langendoen, 2003, GOLD), the ISO TC37/SC4 Data Category Registry (Ide and Romary, 2004; Kemps659 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 659–670, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Snijders et al., 2009, DCR), the OntoTag ontology (Aguado de Cea et al., 2002), or the Typological Database System ontology (Saulwick et al., 2005, TDS). Despite their common level of representation, however, these efforts have not yet converged into a unified and generally accepted ontology of linguistic annotation terminology, but rather, different resources are maintained by different communities"
P10-1068,kermes-evert-2002-yac,0,0.0216912,"eficits, e.g., with respect to annotation granularity. Also, the improved recall can be explained by a compensation of overfitting, or deficits that are inherent to 6 Extensions and Related Research Natural extensions of the approach described in this paper include: (i) Experiments with formally defined consistency conditions (e.g., with respect to restrictions on the domain of properties). (ii) Context-sensitive disambiguation of morphological features (e.g., by combination with a chunker and adjustment of confidence scores for morphological features over all tokens in the current chunk, cf. Kermes and Evert, 2002). (iii) Replacement of majority vote by more elaborate strategies to merge grammatical analyses. 9 The mapping from ontological descriptions to tags of a particular scheme is possible, but neither trivial nor necessarily lossless: Information of ontological descriptions that cannot be expressed in the annotation scheme under consideration (e.g., the distinction between attributive and substitutive pronouns in the Morphisto scheme) will be missing in the resulting string representation. For complex annotations, where ontological descriptions correspond to different substrings, an additional ‘ta"
P10-1068,P03-1054,0,0.00363427,"rd Tagger (Toutanova et al., 2003), (5) PRO.Dem.Attr.-3.Acc.Sg.Fem (RFTagger) (6) rdf:type(olia:PronounOrDeterminer) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) olia:hasCase(some olia:Accusative) rdf:type(olia:DemonstrativeDeterminer) rdf:type(olia:Determiner) For every description obtained from these (and further) analyses, an integrated and consistent generalization can be established as described in the following section. 3 (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and J¨arvinen, 1997). Processing linguistic annotations These tools annotate parts of speech, and those in (i), (iii) and (v) also provide morphological features. All components ran in parallel threads on the same machine, with the exception of Morphisto that was addressed as a web service. The set of matching Annotation Model individuals for every annotation and the respective set of Reference Model descriptions are determined by means of 3.1 Evaluation setup Fig. 6 sketches the architecture o"
P10-1068,W09-3016,0,0.0247884,"Missing"
P10-1068,P98-1081,0,0.13689,"Missing"
P10-1068,J01-2002,0,0.307415,"Missing"
P10-1068,N07-1051,0,0.0105647,"ttr.-3.Acc.Sg.Fem (RFTagger) (6) rdf:type(olia:PronounOrDeterminer) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) olia:hasCase(some olia:Accusative) rdf:type(olia:DemonstrativeDeterminer) rdf:type(olia:Determiner) For every description obtained from these (and further) analyses, an integrated and consistent generalization can be established as described in the following section. 3 (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and J¨arvinen, 1997). Processing linguistic annotations These tools annotate parts of speech, and those in (i), (iii) and (v) also provide morphological features. All components ran in parallel threads on the same machine, with the exception of Morphisto that was addressed as a web service. The set of matching Annotation Model individuals for every annotation and the respective set of Reference Model descriptions are determined by means of 3.1 Evaluation setup Fig. 6 sketches the architecture of the evaluation environment set up for this stu"
P10-1068,W04-0213,0,0.583564,"e expected if tools are combined that make use of different annotation schemes. For this task, ontologies of linguistic annotations are employed to assess the linguistic information conveyed in a particular annotation and to integrate the resulting ontological descriptions in a consistent and tool-independent way. The merged set of ontological descriptions is then evaluated with reference to morphosyntactic and morphological annotations of three corpora of German newspaper articles, the NEGRA corpus (Skut et al., 1998), the TIGER corpus (Brants et al., 2002) and the Potsdam Commentary Corpus (Stede, 2004, PCC). This paper describes a series of experiments to test the hypothesis that the parallel application of multiple NLP tools and the integration of their results improves the correctness and robustness of the resulting analysis. It is shown how annotations created by seven NLP tools are mapped onto toolindependent descriptions that are defined with reference to an ontology of linguistic annotations, and how a majority vote and ontological consistency constraints can be used to integrate multiple alternative analyses of the same token in a consistent way. For morphosyntactic (parts of speech"
P10-1068,A97-1011,0,0.0383489,"Missing"
P10-1068,N03-1033,0,0.0214722,"of the following NLP tools: (4) rdf:type(olia:PronounOrDeterminer) rdf:type(olia:Pronoun) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) rdf:type(olia:DemonstrativePronoun) olia:hasCase(some olia:Nominative) (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6). (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (5) PRO.Dem.Attr.-3.Acc.Sg.Fem (RFTagger) (6) rdf:type(olia:PronounOrDeterminer) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) olia:hasCase(some olia:Accusative) rdf:type(olia:DemonstrativeDeterminer) rdf:type(olia:Determiner) For every description obtained from these (and further) analyses, an integrated and consistent generalization can be established as described in the following section. 3 (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the B"
P10-1068,tufis-2000-using,0,0.0598232,"Missing"
P10-1068,wagner-zeisler-2004-syntactically,0,0.0163568,"pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and J¨arvinen, 1997, pos, morph). Further Annotation Models for pos and morph cover five different annotation schemes for English (Marcus et al., 1994; Sampson, 1995; Mandel, 2006; Kim et al., 2003, Connexor), two annotation schemes for Russian (Meyer, 2003; Sharoff et al., 2008), an annotation scheme designed for typological research and currently applied to approx. 30 different languages (Dipper et al., 2007), an annotation scheme for Old High German (Petrova et al., 2009), and an annotation scheme for Tibetan (Wagner and Zeisler, 2004). 3 Based on primary experience with Western European languages, for example, one might assume that a hasGender property applies to nouns, adjectives, pronouns and determiners only. Yet, this is language-specific restriction: Russian finite verbs, for example, show gender congruency in past tense. 661 or the DCR). As an example, consider the attributive demonstrative pronoun diese in (1). (1) Diese this der the nicht not neue new Markt market Sonnabend Saturday der of.the in in unterstreichen underline Erkenntnis insight konnte could M¨oglichkeiten possibilities am on.the Treuenbrietzen Treuen"
P10-1068,C08-1098,0,0.138202,"tionships from an External Reference Model. Different External Reference Models may take different positions on the issue – as languages do3 –, so that this aspect is left underspecified in the Reference Model. Figs. 2 and 4 show excerpts of category and feature hierarchies in the Reference Model. With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and J¨arvinen, 1997, pos, morph). Further Annotation Models for pos and morph cover five different annotation schemes for English (Marcus et al., 1994; Sampson, 1995; Mandel, 2006; Kim et al., 2003, Connexor), two annotation schemes for Russian (Meyer, 2003; Sharoff et al., 2008), an annotation scheme designed for typological research and currently applied to approx. 30 different languages (Dipper et al., 2007), an annotation scheme for Old High German (Petrova et al., 2009), and an annotation scheme for Tibetan (Wagner and Zeisler, 2004). 3 Based on primary"
P10-1068,zavrel-daelemans-2000-bootstrapping,0,0.17114,"of Potsdam, Germany chiarcos@uni-potsdam.de Abstract (parts of speech, pos) and morphology in German: In comparison to English, German shows a rich and polysemous morphology, and a considerable number of NLP tools are available, making it a promising candidate for such an experiment. Previous research indicates that the integration of multiple part of speech taggers leads to more accurate analyses. So far, however, this line of research focused on tools that were trained on the same corpus (Brill and Wu, 1998; Halteren et al., 2001), or that specialize to different subsets of the same tagset (Zavrel and Daelemans, 2000; Tufis¸, 2000; Borin, 2000). An even more substantial increase in accuracy and detail can be expected if tools are combined that make use of different annotation schemes. For this task, ontologies of linguistic annotations are employed to assess the linguistic information conveyed in a particular annotation and to integrate the resulting ontological descriptions in a consistent and tool-independent way. The merged set of ontological descriptions is then evaluated with reference to morphosyntactic and morphological annotations of three corpora of German newspaper articles, the NEGRA corpus (Sk"
P10-1068,sharoff-etal-2008-designing,0,0.0201599,"orphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and J¨arvinen, 1997, pos, morph). Further Annotation Models for pos and morph cover five different annotation schemes for English (Marcus et al., 1994; Sampson, 1995; Mandel, 2006; Kim et al., 2003, Connexor), two annotation schemes for Russian (Meyer, 2003; Sharoff et al., 2008), an annotation scheme designed for typological research and currently applied to approx. 30 different languages (Dipper et al., 2007), an annotation scheme for Old High German (Petrova et al., 2009), and an annotation scheme for Tibetan (Wagner and Zeisler, 2004). 3 Based on primary experience with Western European languages, for example, one might assume that a hasGender property applies to nouns, adjectives, pronouns and determiners only. Yet, this is language-specific restriction: Russian finite verbs, for example, show gender congruency in past tense. 661 or the DCR). As an example, consi"
P10-1068,sankaran-etal-2008-common,0,\N,Missing
P10-1068,dzeroski-etal-2000-morphosyntactic,0,\N,Missing
P10-1068,J93-2004,0,\N,Missing
P10-1068,erjavec-2010-multext,0,\N,Missing
P10-1068,D07-1046,0,\N,Missing
P10-1068,W07-1709,0,\N,Missing
P10-1068,P02-1056,0,\N,Missing
P10-1068,C98-1078,0,\N,Missing
P10-1068,C98-1029,0,\N,Missing
P10-1068,wright-2004-global,0,\N,Missing
P12-2042,P09-1068,0,0.0367834,"z and Girju (2010) exploit distributional information about pairs of utterances. Unlike approach described here, they are not restricted to adjacent utterances, and do not rely on explicit and recurrent relation words. Their approach can thus be applied to comparably small data sets. However, they are restricted to a specific type of relations whereas here the entire bandwidth of discourse relations that are explicitly realized in a language are covered. Prospectively, both approaches could be combined to compensate their respective weaknesses. Similar observations can be made with respect to Chambers and Jurafsky (2009) and Kasch and Oates (2010), who also study a single discourse relation (narration), and are thus more limited in scope than the approach described here. However, as their approach extends beyond pairs of events to complex event chains, it seems that both approaches provide complementary types of information and their results could also be combined in a fruitful way to achieve a more detailed assessment of discourse relations. The goal of this paper was to evaluate the methdological validity of the approach. It thus represents the basis for further experiments, e.g., with respect to the enrich"
P12-2042,W10-0905,0,0.0136669,"butional information about pairs of utterances. Unlike approach described here, they are not restricted to adjacent utterances, and do not rely on explicit and recurrent relation words. Their approach can thus be applied to comparably small data sets. However, they are restricted to a specific type of relations whereas here the entire bandwidth of discourse relations that are explicitly realized in a language are covered. Prospectively, both approaches could be combined to compensate their respective weaknesses. Similar observations can be made with respect to Chambers and Jurafsky (2009) and Kasch and Oates (2010), who also study a single discourse relation (narration), and are thus more limited in scope than the approach described here. However, as their approach extends beyond pairs of events to complex event chains, it seems that both approaches provide complementary types of information and their results could also be combined in a fruitful way to achieve a more detailed assessment of discourse relations. The goal of this paper was to evaluate the methdological validity of the approach. It thus represents the basis for further experiments, e.g., with respect to the enrichment the BKB with informati"
P12-2042,nivre-etal-2006-maltparser,0,0.0912343,"Missing"
P12-2042,prasad-etal-2008-penn,0,0.41566,"global structure of texts and the relations linking its different parts (Vallduv´ı 1992; Gernsbacher et al. 2004). To capture these aspects of meaning in NLP, it is necessary to develop operationalizable theories, and, within a supervised approach, large amounts of annotated training data. To facilitate manual annotation, weakly supervised or unsupervised techniques can be applied as preprocessing step for semimanual annotation, and this is part of the motivation of the approach described here. Assuming that relation words are associated with specific discourse relations (Knott and Dale 1994; Prasad et al. 2008), the distribution of relation words found between two (types of) events can yield insights into the range of discourse relations possible at this occasion and their respective likeliness. For this purpose, this paper proposes a background knowledge base (BKB) that hosts pairs of events (here heuristically represented by verbs) along with distributional profiles for relation words. The primary data structure of the BKB is a triple where one event (type) is connected with a particular relation word to another event (type). Triples are further augmented with a frequency score (expressing the lik"
P12-2042,J03-4002,0,0.11563,"Missing"
P12-2042,C10-2172,0,0.0428322,"e relations, with encouraging preliminary results: Large collections of parsed text are used to assess distributional profiles of relation words that indicate discourse relations that are possible between specific types of events; on this basis, a background knowledge base (BKB) was created that can be used to predict an appropriate discourse marker to connect two utterances with no overt relation word. This information can be used, for example, to facilitate the semiautomated annotation of discourse relations, by pointing out the ‘default’ relation word for a given pair of events. Similarly, Zhou et al. (2010) used a language model to predict discourse markers for implicitly realized discourse relations. As opposed to this shallow, n-gram-based approach, here, the internal structure of utterances is exploited: based on semantic considerations, syntactic patterns have been devised that extract triples of event pairs and relation words. The resulting BKB provides a distributional approximation of the discourse relations that can hold between two specific event types. Both approaches exploit complementary sources of knowledge, and may be combined with each other to achieve a more precise prediction of"
P17-2040,D13-1158,0,0.0736936,"Missing"
P17-2040,W15-0117,0,0.100154,"s single arguments. A data point (a1 , a2 , y), with ai being the token sequence of argument i, is expanded into {(a1 , a2 , y), (a1 , a2 , y), (a1 , y), (a2 , y)}. We duplicate bi-argument samples (a1 , a2 , y) (in training and development data only) to balance their frequencies against single-argument samples. Two lines of motivation support the inclusion of single argument training examples, grounded in linguistics and machine learning, respectively. First, it has been shown that single arguments in isolation can evoke a strong expectation towards a certain implicit discourse relation, cf. Asr and Demberg (2015) and, in particular, Rohde and Horton (2010) in their psycholinguistic study on implicit causality verbs. Second, the procedure may encourage the model to learn better representations of individual argument spans in support of modeling of arguments in composition, cf. LeCun et al. (2015). Due to these aspects, we believe this data augmentation technique to be effective in reinforcing the overall robustness of our model. Implementational Details: We train the model using fixed-length sequences of 256 tokens with zero padding at the beginning of shorter sequences and truncate longer ones. Each L"
P17-2040,P16-1163,0,0.171078,"ks, they discussed some principles and specific questions in depth, and reached some understandings Arg2: 双 方 一 致 认 为 会 谈 具 有 积 极 成 果 Both sides agree that the talks have positive results Motivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (Pitler et al., 2009; Feng and Hirst, 2012; Huang and Chen, 2011). Only recently, resource-lean architectures have been proposed. These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016). So far, unfortunately, these models have been evaluated only on four top-level senses—sometimes even with inconsistent evaluation setups.2 Furthermore, most systems have initially been designed for the English PDTB and involve complex, taskWe introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also vi"
P17-2040,I11-1170,0,0.0216433,"mplicit relations is by far more challenging because the argument pairs lack the marker as an important feature. Consider, for instance, the following example from the CDTB as implicit C ONJUNCTION: Arg1: 会谈就一些原则和具体问题进行了 深入讨论，达成了一些谅解 In the talks, they discussed some principles and specific questions in depth, and reached some understandings Arg2: 双 方 一 致 认 为 会 谈 具 有 积 极 成 果 Both sides agree that the talks have positive results Motivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (Pitler et al., 2009; Feng and Hirst, 2012; Huang and Chen, 2011). Only recently, resource-lean architectures have been proposed. These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016). So far, unfortunately, these models have been evaluated only on four top-level senses—sometimes even with inconsistent evaluation setups.2 Furthermore, most systems have initially been designed for the English PDTB and involve complex, taskWe introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argume"
P17-2040,N16-1037,0,0.030694,"成了一些谅解 In the talks, they discussed some principles and specific questions in depth, and reached some understandings Arg2: 双 方 一 致 认 为 会 谈 具 有 积 极 成 果 Both sides agree that the talks have positive results Motivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (Pitler et al., 2009; Feng and Hirst, 2012; Huang and Chen, 2011). Only recently, resource-lean architectures have been proposed. These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016). So far, unfortunately, these models have been evaluated only on four top-level senses—sometimes even with inconsistent evaluation setups.2 Furthermore, most systems have initially been designed for the English PDTB and involve complex, taskWe introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse"
P17-2040,P12-1007,0,0.0119676,"e classification for implicit relations is by far more challenging because the argument pairs lack the marker as an important feature. Consider, for instance, the following example from the CDTB as implicit C ONJUNCTION: Arg1: 会谈就一些原则和具体问题进行了 深入讨论，达成了一些谅解 In the talks, they discussed some principles and specific questions in depth, and reached some understandings Arg2: 双 方 一 致 认 为 会 谈 具 有 积 极 成 果 Both sides agree that the talks have positive results Motivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (Pitler et al., 2009; Feng and Hirst, 2012; Huang and Chen, 2011). Only recently, resource-lean architectures have been proposed. These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016). So far, unfortunately, these models have been evaluated only on four top-level senses—sometimes even with inconsistent evaluation setups.2 Furthermore, most systems have initially been designed for the English PDTB and involve complex, taskWe introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstra"
P17-2040,C08-1043,0,0.021616,"t parts of an input sequence. 1 Introduction True text understanding is one of the key goals in Natural Language Processing and requires capabilities beyond the lexical semantics of individual words or phrases. Natural language descriptions are typically driven by an inter-sentential coherent structure, exhibiting specific discourse properties, which in turn contribute significantly to the global meaning of a text. Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al., 2013). Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004). The annotation schemata of the Penn Discourse Treebank (Prasad et al., 2008, PDTB) and the Chinese Discourse Treebank (Zhou and Xue, 2012, CDTB), for instance, define ∗ 1 The set of relation types and senses is completed by alternative lexicalizations (A LT L EX/discourse marker rephrased), and entity relatio"
P17-2040,D16-1130,0,0.152147,"Missing"
P17-2040,N16-1098,0,0.0286306,"e transition between the argument boundary, establishing a connection between the semantically related terms understandings–agree. Most E NT R ELs show an opposite trend: here second arguments exhibit larger intensities than Arg1, as most entity relations follow the characteristic writing style of newspapers by adding additional information by reference to the same entity. 4 ily adapted to similar relation recognition tasks. In future work, we intend to extend our approach to different languages and domains, e.g., to the recent data sets on narrative story understanding or question answering (Mostafazadeh et al., 2016; Feng et al., 2015). We believe that recurrent modeling of implicit discourse information can be a driving force in successfully handling such complex semantic processing tasks.7 Acknowledgments The authors would like to thank Ayah Zirikly, Philip Schulz and Wei Ding for their very helpful suggestions on an early draft version of the paper, and also thank the anonymous reviewers for their valuable feedback and insightful comments. We are grateful to Farrokh Mehryary for technical support with the attention layer implementation. Computational resources were provided by CSC – IT Centre for Scie"
P17-2040,N13-1100,0,0.022299,"troduction True text understanding is one of the key goals in Natural Language Processing and requires capabilities beyond the lexical semantics of individual words or phrases. Natural language descriptions are typically driven by an inter-sentential coherent structure, exhibiting specific discourse properties, which in turn contribute significantly to the global meaning of a text. Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al., 2013). Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004). The annotation schemata of the Penn Discourse Treebank (Prasad et al., 2008, PDTB) and the Chinese Discourse Treebank (Zhou and Xue, 2012, CDTB), for instance, define ∗ 1 The set of relation types and senses is completed by alternative lexicalizations (A LT L EX/discourse marker rephrased), and entity relations (E NT R EL/anaphoric coherence). 2 E.g., four bi"
P17-2040,P09-1077,0,0.681413,"Missing"
P17-2040,K16-2004,0,0.221089,"Missing"
P17-2040,prasad-etal-2008-penn,0,0.385785,"rn contribute significantly to the global meaning of a text. Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al., 2013). Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004). The annotation schemata of the Penn Discourse Treebank (Prasad et al., 2008, PDTB) and the Chinese Discourse Treebank (Zhou and Xue, 2012, CDTB), for instance, define ∗ 1 The set of relation types and senses is completed by alternative lexicalizations (A LT L EX/discourse marker rephrased), and entity relations (E NT R EL/anaphoric coherence). 2 E.g., four binary classifiers vs. four-way classification. Both first authors contributed equally to this work. 256 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 256–262 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics"
P17-2040,K16-2006,0,0.0591949,"Missing"
P17-2040,K16-2010,0,0.123616,"Missing"
P17-2040,D15-1266,0,0.0787647,"就一些原则和具体问题进行了 深入讨论，达成了一些谅解 In the talks, they discussed some principles and specific questions in depth, and reached some understandings Arg2: 双 方 一 致 认 为 会 谈 具 有 积 极 成 果 Both sides agree that the talks have positive results Motivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (Pitler et al., 2009; Feng and Hirst, 2012; Huang and Chen, 2011). Only recently, resource-lean architectures have been proposed. These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016). So far, unfortunately, these models have been evaluated only on four top-level senses—sometimes even with inconsistent evaluation setups.2 Furthermore, most systems have initially been designed for the English PDTB and involve complex, taskWe introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the C"
P17-2040,K16-2007,0,0.0726405,"Missing"
P17-2040,P16-2034,0,0.219991,"ures have been claimed by Rutherford et al. (2016) to generally outperform any thoroughly-tuned recurrent architecture. ay er Sense labels pu tl ut O r ⊕ A&apos;1 h&apos;2 h&apos;&apos;2 A&apos;2 A&apos;&apos;1 A&apos;&apos;2 e1 e2 t1 t2 h&apos;k .. . h&apos;&apos;1 hk .. . h2 ⊕ ⊕ h&apos;&apos;k A&apos;k A&apos;&apos;k .. . Re c la urre ye n rs t b la edd ay ye in er r g Em tl pu In h1 ek .. . A tte la ntio ye n r ·α h&apos;1 tk (<ARG1&gt;, 会 谈, ..., </ARG1&gt;, <ARG2&gt;, 双 方, ..., </ARG2&gt;) Token input sequence Our Contribution: In this work, we release the first attention-based recurrent neural sense classifier, specifically developed for Chinese implicit discourse relations. Inspired by Zhou et al. (2016), our system is a practical adaptation of the recent advances in relation modeling extended by a novel sampling scheme. Contrary to previous assertions by Rutherford et al. (2016), our model demonstrates superior performance over traditional bag-of-words approaches with feedfoward networks by treating discourse arguments as a joint sequence. We evaluate our method within an independent framework and show that it performs very well beyond standard class-level predictions, achieving stateof-the-art accuracy on the CDTB test set. We illustrate how our model’s attention mechanism provides means to"
P17-2040,P12-1008,0,0.0518367,"tomatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al., 2013). Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004). The annotation schemata of the Penn Discourse Treebank (Prasad et al., 2008, PDTB) and the Chinese Discourse Treebank (Zhou and Xue, 2012, CDTB), for instance, define ∗ 1 The set of relation types and senses is completed by alternative lexicalizations (A LT L EX/discourse marker rephrased), and entity relations (E NT R EL/anaphoric coherence). 2 E.g., four binary classifiers vs. four-way classification. Both first authors contributed equally to this work. 256 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 256–262 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2040 specific architectures ("
P17-2040,K16-2005,1,0.906501,"Missing"
R15-1074,P98-1013,0,0.0610973,"not in the text are regarded as null instantiations. Such pattern-based methods perform satisfactorily, yet there are drawbacks: (1) They are inflexible and absolute according to their type, in that they assume that all candidate NIs are equally likely to be missing, which is unrealistic given the variety of different linguistic contexts in which predicates co-occur with their semantic roles. (2) They are expensive in that they require handcrafted, idiosyncratic rules (Ruppenhofer et al., 2011) and rich background knowledge in the form of language-specific lexical resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) or NomBank (Meyers et al., 2004). Dictionaries providing information about each predicate and status of the individual roles (e.g., whether they can serve as implicit elements or not) are costly, and for most other languages not available to the same extent as for English. (3) Most earlier studies heuristically restrict implicit arguments to core roles only,2 but this is problematic as it ignores the fact that implicit noncore roles also provide valid and valuable information. Our approach remains agnostic regarding the role inventory, and can address both core"
R15-1074,W13-2322,0,0.0286497,"s, we employ a generic role set which is based on PropBank/NomBank rather than FrameNet: The PropBank format comprises a relatively small role inventory which is better suited to obtain statistical generalizations than the great variety of highly specific FrameNet roles. While FrameNet roles seem to be more fine-grained, their greater number arises mostly from predicate-specific semantic roles, whose specific semantics can be recovered from PropBank annotations by pairing semantic roles with the predicate. Yet another motivation of our work is related to the recent development of AMR parsing (Banarescu et al., 2013, Abstract Meaning Representation) which aims at modeling the semantic representation of a sentence while abstracting from syntactic idiosyncrasies. This particular appraoch makes extensive use of the PropBank-style framesets, as well, and would greatly benefit from the integration of information on implicit roles. Our contribution We propose a novel, generic approach to infer information about implicit roles which does not rely on the availability of manually annotated gold data. Our focus is exclusively on NI role identification, i.e., per-predicate detection of the missing implicit semantic"
R15-1074,P13-1116,0,0.517714,"Missing"
R15-1074,W09-1206,0,0.234601,"Missing"
R15-1074,W04-2705,0,0.342208,"Missing"
R15-1074,S10-1059,0,0.368813,"php by various researchers in the community for direct or indirect evaluation of their results. The NIs in the data set are further subdivided into two categories: Definite NIs (DNIs) are locally unexpressed arguments which can be resolved to elements in the proceeding or following discourse; Indefinite NIs (INIs) are elements for which no antecedent can be identified in the surrounding context.4 Also, the evaluation data comes in two flavors: a base format which is compliant with the FrameNet paradigm and a CoNLL-based PropBank format. Previous research has exclusively focused on the former. Chen et al. (2010) present an extension of an existing FrameNet-style parser (SEMAFOR) to handle implicit elements in text. The identification of NIs is guided by the assumption that, whenever the traditional SRL parser returns the default label involved in a non-saturated analysis for a sentence, an implicit role has to be found in the context instead. Additional FrameNet-specific heuristics are employed in which, e.g., the presence of one particular role in a frame makes the identification of another implicit role redundant.5 Tonelli and Delmonte (2010, VENSES++) present a deep semantic approach to NI resolut"
R15-1074,J05-1004,0,0.072749,"´o (2015) for an attempt to enlarge the number of annotation instances by combination of scarce resources. As a result, most state-of-theart iSRL systems cannot be trained in a supervised setting and thus integrate custom, rule-based components to detect NIs. (We elaborate on related work in Section 2.) To this end, a predicate’s overt roles are matched against a predefined predicate[A0 Twenty-two month old] with history of recurrent right middle lobe infiltrate. Increased [A0 ∅] cough, [A0 ∅] tachypnea, and [A0 ∅] work of breathing. 1 For details on the PropBank labels used in our study, see Palmer et al. (2005). 570 Proceedings of Recent Advances in Natural Language Processing, pages 570–578, Hissar, Bulgaria, Sep 7–9 2015. specific template. Informally, all roles found in the template but not in the text are regarded as null instantiations. Such pattern-based methods perform satisfactorily, yet there are drawbacks: (1) They are inflexible and absolute according to their type, in that they assume that all candidate NIs are equally likely to be missing, which is unrealistic given the variety of different linguistic contexts in which predicates co-occur with their semantic roles. (2) They are expensiv"
R15-1074,P12-2042,1,0.689693,"Missing"
R15-1074,S13-1043,0,0.323634,"Missing"
R15-1074,S15-1005,0,0.563878,"Missing"
R15-1074,J12-4003,0,0.0659552,"he action), etc.1 The output of SRL systems have proven to offer a good approximation to a deeper semantic modeling of natural language. However, given its inherent complexity, recent efforts for improvement have tried to extend traditional SRL from the sentence-internal context to the surrounding discourse. As an illustration, consider the following biomedical example from Ruppenhofer et al. (2010). Current issues in iSRL Corpus data with manually annotated implicit roles is extremely sparse and hard to obtain, and annotation efforts have emerged only recently; cf. Ruppenhofer et al. (2010), Gerber and Chai (2012), and also Feizabadi and Pad´o (2015) for an attempt to enlarge the number of annotation instances by combination of scarce resources. As a result, most state-of-theart iSRL systems cannot be trained in a supervised setting and thus integrate custom, rule-based components to detect NIs. (We elaborate on related work in Section 2.) To this end, a predicate’s overt roles are matched against a predefined predicate[A0 Twenty-two month old] with history of recurrent right middle lobe infiltrate. Increased [A0 ∅] cough, [A0 ∅] tachypnea, and [A0 ∅] work of breathing. 1 For details on the PropBank la"
R15-1074,W09-2417,0,0.58463,"Missing"
R15-1074,J02-3001,0,0.0765839,"n models generalize over large amounts of explicit annotations only, in order to acquire information about implicit roles. We establish a generic background knowledge base of probablistic predicate-role co-occurrences in an unsupervised manner, and estimate thresholds which trigger the prediction of a missing role. Our approach outperforms the stateof-the-art in terms of recognition rate and offers a more flexible alternative to rulebased solutions which rely on costly, language and domain-specific lexica. 1 Introduction In its classical form, an automated semantic role labeling (SRL) system (Gildea and Jurafsky, 2002) detects events (verbal or nominal predicates), together with their associated participants within the local context. Semantic roles are assigned to syntactic elements, such as A0 for the agent of an event, A1 for the patient (i.e. the entity which undergoes the action), etc.1 The output of SRL systems have proven to offer a good approximation to a deeper semantic modeling of natural language. However, given its inherent complexity, recent efforts for improvement have tried to extend traditional SRL from the sentence-internal context to the surrounding discourse. As an illustration, consider t"
R15-1074,R11-1046,0,0.377445,"Processing, pages 570–578, Hissar, Bulgaria, Sep 7–9 2015. specific template. Informally, all roles found in the template but not in the text are regarded as null instantiations. Such pattern-based methods perform satisfactorily, yet there are drawbacks: (1) They are inflexible and absolute according to their type, in that they assume that all candidate NIs are equally likely to be missing, which is unrealistic given the variety of different linguistic contexts in which predicates co-occur with their semantic roles. (2) They are expensive in that they require handcrafted, idiosyncratic rules (Ruppenhofer et al., 2011) and rich background knowledge in the form of language-specific lexical resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) or NomBank (Meyers et al., 2004). Dictionaries providing information about each predicate and status of the individual roles (e.g., whether they can serve as implicit elements or not) are costly, and for most other languages not available to the same extent as for English. (3) Most earlier studies heuristically restrict implicit arguments to core roles only,2 but this is problematic as it ignores the fact that implicit noncore roles also provi"
R15-1074,S12-1001,0,0.323881,"system-specific output is mapped to FrameNet valency patterns. For the detection of NIs, they assume that these are always core arguments, i.e., non-omissible roles in the interaction with a specific predicate. It is unclear how different predicate senses are handled by their approach. Moreover, not all types of NIs can be detected, resulting in a low overall recall of identified NIs, also having drawbacks for nouns. Again using FrameNet-specific modeling assumptions, their work has been significantly refined in Tonelli and Delmonte (2011). Despite their good performance in the overall task, Silberer and Frank (2012, S&F) give a rather vague explanation regarding NI identification in text. Using a FrameNet API, the authors restrict their analysis only to the core roles by excluding “conceptually redundant” roles without further elaboration. Laparra and Rigau (2013) propose a deterministic algorithm to detect NIs on grounds of discourse coherence: It predicts an NI for a predicate if the corresponding role has been explicitly realized for the same predicate in the preceding discourse but is currently unfilled. Their approach is promising but ignorant of INIs. Earlier, Laparra and Rigau (2012, L&R) introdu"
R15-1074,S10-1065,0,0.830513,"Missing"
R15-1074,W11-0908,0,0.270356,"Delmonte (2010, VENSES++) present a deep semantic approach to NI resolution whose system-specific output is mapped to FrameNet valency patterns. For the detection of NIs, they assume that these are always core arguments, i.e., non-omissible roles in the interaction with a specific predicate. It is unclear how different predicate senses are handled by their approach. Moreover, not all types of NIs can be detected, resulting in a low overall recall of identified NIs, also having drawbacks for nouns. Again using FrameNet-specific modeling assumptions, their work has been significantly refined in Tonelli and Delmonte (2011). Despite their good performance in the overall task, Silberer and Frank (2012, S&F) give a rather vague explanation regarding NI identification in text. Using a FrameNet API, the authors restrict their analysis only to the core roles by excluding “conceptually redundant” roles without further elaboration. Laparra and Rigau (2013) propose a deterministic algorithm to detect NIs on grounds of discourse coherence: It predicts an NI for a predicate if the corresponding role has been explicitly realized for the same predicate in the preceding discourse but is currently unfilled. Their approach is"
R15-1074,W10-0903,0,\N,Missing
R15-1074,S10-1008,0,\N,Missing
R15-1074,C98-1013,0,\N,Missing
R15-1074,D13-1095,0,\N,Missing
rehm-etal-2008-metadata,rehm-etal-2008-ontology,1,\N,Missing
rehm-etal-2008-metadata,W03-0804,0,\N,Missing
rehm-etal-2008-metadata,rehm-etal-2008-towards,1,\N,Missing
rehm-etal-2008-ontology,ide-etal-2000-xces,0,\N,Missing
rehm-etal-2008-ontology,telljohann-etal-2004-tuba,0,\N,Missing
rehm-etal-2008-ontology,J93-2004,0,\N,Missing
rehm-etal-2008-ontology,rehm-etal-2008-metadata,1,\N,Missing
W07-1525,W99-0213,0,0.0818923,"Missing"
W07-1525,W98-1119,0,0.0469521,"rred to the shorter ones by annotation. This defines preferences for coding decisions by ambiguity (see 4.1). In the remainder of this section, annotation principles employed in the PoCoS scheme are shortly presented and discussed as to their relationship to these four requirements. 157 3.2 Markable identification Cross-linguistically consistent markable identification strategies are a necessary pre-condition for a comparative evaluation of anaphor annotation and anaphor resolution across different languages. It has been controversial, however, how to set markable boundaries. So, for example, Ge et al. (1998) and, MUC (Hirschman, 1997) propose a minimal string constraint motivated by evaluation considerations. This procedure leads to systematic violations of the CONSTITUENCY and COMPLETENESS principles, though, cf. the potential markables Denver and bankruptcy in ex. (1) (1) The [Denver]?-based concern, which emerged from bancruptcy ... its new, post[bancruptcy]? law structure ...&quot; (WSJ, 1328) We explicitly propose a maximum size principle as an alternative to the minimum string constraint (see Principle 1 below). So, a markable consists of the head, usually a noun or a pronoun, and of all modifie"
W07-1525,holler-etal-2004-exploiting,0,0.0288029,"th information status and co-reference is in preparation. A corpus of Russian is currently under construction, which PoCoS is being applied to (cf. Krasavina et al. 2007). 7 Discussion The majority of earlier coreference annotation experiences were dealing with English, including the standard-like MUC-scheme (Hirschman, 1997). MATE was an attempt to extend annotation to other languages than English (Poesio, 2004). For German, several annotation schemes appeared and were applied to annotation of corpora recently: for 162 newspaper texts, such as the TüBa-D/Z (Naumann, 2006) and for hypertexts, Holler et al. (2004). As for Slavic languages, the Prague Dependency Treebank has been recently enriched by coreference annotation, see Kučová and Hajičová (2004) . For Russian, though, we are aware of no similar experiences so far. The current approach is an advance on the existing work as it attempts at providing language-independent and systematic annotation principles, including a language-neutral repertoire of relations and a language-neutral apparatus for identification of markables. This makes the resulting annotation scheme extendable and applicable across languages. The Core Scheme is comparable to MUC b"
W07-1525,W04-2327,0,0.284063,"ions or schemes, but has not been formulated explicitly by now. The number of existing schemes released just in the last few years is overwhelming and is out of the 1 The research by Olga Krasavina was supported by Russian Foundation for the Humanities, grant 05-04-04240а. scope here. The MUC is still generally accepted as the most standard-like annotation scheme (Hirschman, 1997). Given its simplicity is its most important advantage, it has been criticized for its limited coverage and its contra-intuitive understanding of coreference. One of the most wellknown later approaches is MATE/GNOME (Poesio, 2004). As the author fairly notices, “there can be no such thing as a general-purpose anaphoric annotation instructions”, due to the complexity of phenomena associated with the term of anaphora. So, its essential idea is combining a “generalpurporse markup scheme” (MATE) with application-specific scheme instantiations (GNOME). In PoCoS, we adapted and elaborated this idea, by suggesting the Core and Extended Schemes. The PoCoS, the Potsdam Coreference Scheme, both adapts selected features of existing schemes and implements a set of innovative features. We distinguish between the Core and Extended S"
W07-1525,popescu-belis-etal-2004-online,0,0.0634697,"Missing"
W07-1525,W97-1308,0,0.084665,"Missing"
W07-1525,W04-0213,0,0.27068,"Missing"
W07-1525,W01-1605,0,\N,Missing
W07-1525,C69-7001,0,\N,Missing
W07-1525,C69-6902,0,\N,Missing
W09-0703,brants-plaehn-2000-interactive,0,0.0135585,"means that queries and query results referred to in, e.g., a scientific paper, can be reproduced and quoted by means of (complex) links (see following example). conflictingly overlapping or discontinuous. The types of annotations handled by ANNIS include, among others, flat, layer-based annotations (e.g., for glossing) and hierarchical trees (e.g., syntax). Source data. As an architecture designed to facilitate diverse and integrative research on IS, ANNIS can import formats from a broad variety of tools from NLP and manual annotation, the latter including EXMARaLDA (Schmidt, 2004), annotate (Brants and Plaehn, 2000), Synpathy (www.lat-mpi.eu/tools/synpathy/), MMAX2 (Müller and Strube, 2006), RSTTool (O'Donnell, 2000), PALinkA (Orasan, 2003), Toolbox (Busemann & Busemann, 2008) etc. These tools allow researchers to annotate data for syntax, semantics, morphology, prosody, phonetics, referentiality, lexis and much more, as their research questions require. All annotated data are merged together via a general interchange format PAULA (Dipper 2005, Dipper & Götze 2005), a highly expressive standoff XML format that specifically allows further annotation levels to be added at a later time without disrupting th"
W09-0703,W00-1434,0,0.0971164,"Missing"
W09-0703,W03-2120,0,0.0647559,"Missing"
W09-0703,W09-2604,0,\N,Missing
W09-3005,A00-1031,0,0.0794563,"earch interface. Integrative representation. All annotations that are consistent with the merged tokenization should refer to the merged tokenization. This is necessary in order to query across multiple annotations orig35 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35–43, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP inating from different annotation layers or tools. Unsupervised merging. The integration of conflicting tokenizations should not require manual interference. 1.2 (4) doesn’t a. [does][n’t] (Marcus et al., 1993, PTB) b. [doesn][’][t] (Brants, 2000, TnT) Tokenization Tokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997). However, different research questions or applications induce different conceptions of the term ‘word’. For a shallow morphosyntactic analysis (part of speech tagging), a ‘simple’ tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in (1). A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each. (1) a. department store b. Herzog-von der Heide1 c. Red Cross/Red Cre"
W09-3005,W06-2709,0,0.0505607,"Missing"
W09-3005,C00-2157,0,0.690933,"Missing"
W09-3005,J93-2004,0,0.0357686,"lating queries for words, e.g. in a corpus search interface. Integrative representation. All annotations that are consistent with the merged tokenization should refer to the merged tokenization. This is necessary in order to query across multiple annotations orig35 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35–43, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP inating from different annotation layers or tools. Unsupervised merging. The integration of conflicting tokenizations should not require manual interference. 1.2 (4) doesn’t a. [does][n’t] (Marcus et al., 1993, PTB) b. [doesn][’][t] (Brants, 2000, TnT) Tokenization Tokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997). However, different research questions or applications induce different conceptions of the term ‘word’. For a shallow morphosyntactic analysis (part of speech tagging), a ‘simple’ tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in (1). A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each. (1) a. department store b. Herzo"
W09-3005,J97-4004,0,0.0394198,"to the merged tokenization. This is necessary in order to query across multiple annotations orig35 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35–43, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP inating from different annotation layers or tools. Unsupervised merging. The integration of conflicting tokenizations should not require manual interference. 1.2 (4) doesn’t a. [does][n’t] (Marcus et al., 1993, PTB) b. [doesn][’][t] (Brants, 2000, TnT) Tokenization Tokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997). However, different research questions or applications induce different conceptions of the term ‘word’. For a shallow morphosyntactic analysis (part of speech tagging), a ‘simple’ tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in (1). A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each. (1) a. department store b. Herzog-von der Heide1 c. Red Cross/Red Crescent movement Similarly, examples (2a) and (2b) can be argued to be treated as one token for (morpho)syntactic analyse"
W09-3005,kingsbury-palmer-2002-treebank,0,\N,Missing
W09-3005,W04-2705,0,\N,Missing
W09-3005,poesio-artstein-2008-anaphoric,0,\N,Missing
W09-3005,M95-1005,0,\N,Missing
W09-3005,W07-1501,0,\N,Missing
W09-3005,W01-1605,0,\N,Missing
W09-3005,W03-1309,0,\N,Missing
W09-3005,N06-2015,0,\N,Missing
W09-3005,J95-4004,0,\N,Missing
W09-3005,J05-2005,0,\N,Missing
W09-3005,prasad-etal-2008-penn,0,\N,Missing
W09-3005,P02-1022,0,\N,Missing
W10-1825,E06-2001,0,0.0174022,"alization. Using the example of PP attachment, we show how parsing can benefit from the use of such a resource. 1 Introduction In this paper, we describe the workflow and the infrastructure to create and explore a corpus that contains multiple parses of German sentences. A corpus of alternative parses created by different tools allows us to study structural differences between the parses in a systematic way. The resource described in this paper is a collection of German sentences with -ung nominalizations extracted from the SD E WAC corpus (Faaß et al., 2010), based on the D E WAC web corpus (Baroni and Kilgarriff, 2006). These sentences are employed for the study of lexical ambiguities in German -ung nominalizations (Eberle et al., 2009); e.g., German Absperrung, derived from absperren ‘to block’, can denote an event (‘blocking’), a state (‘blockade’) or an object (‘barrier’). Sortal disambiguation, however, is highly contextdependent, and reliable and detailed analyses of the linguistic context are crucial for a sortal disambiguation of these nominalizations. More reliable and detailed linguistic analyses can be achieved, for example, by combining the information produced by different parsers: On the basis"
W10-1825,borin-2000-something,0,0.584171,"Missing"
W10-1825,W09-3005,1,0.847957,"urposes (representation and querying of richly annotated corpora), its generic character allowed us to apply it with more than satisfactory results to a new scenario. Subsequent research may further exploit the potential of the ANNIS/PAULA infrastructure and the development of application-specific extensions. In particular, it is possible to register in ANNIS a problem-specific visualization for parallel parses that applies in place of the generic tree/DAG view for the namespaces bitpar and b3. Another extension pertains to the handling of conflicting tokenizations: The algorithm described by Chiarcos et al. (2009) is sufficiently generic to be applied to any PAULA project, but it may be extended to account for B3-specific deletions (Sect. 2.2). Further, ANNIS supports an annotation enrichment cycle: Matches are exported as WEKA tables, statistical, symbolic or neural classifiers can be trained on or applied to this data, and the modified match table can be reintegrated with the original corpus. This allows, for example, to learn an automatic mapping between B3 and BitPar annotations. tion between entire parses, cf. Crysmann et al. (2002), we employ a full merging between B3 parses and BitPar parses. Th"
W10-1825,faass-etal-2010-design,0,0.0135781,"rmed using ANNIS, a tool for corpus querying and visualization. Using the example of PP attachment, we show how parsing can benefit from the use of such a resource. 1 Introduction In this paper, we describe the workflow and the infrastructure to create and explore a corpus that contains multiple parses of German sentences. A corpus of alternative parses created by different tools allows us to study structural differences between the parses in a systematic way. The resource described in this paper is a collection of German sentences with -ung nominalizations extracted from the SD E WAC corpus (Faaß et al., 2010), based on the D E WAC web corpus (Baroni and Kilgarriff, 2006). These sentences are employed for the study of lexical ambiguities in German -ung nominalizations (Eberle et al., 2009); e.g., German Absperrung, derived from absperren ‘to block’, can denote an event (‘blocking’), a state (‘blockade’) or an object (‘barrier’). Sortal disambiguation, however, is highly contextdependent, and reliable and detailed analyses of the linguistic context are crucial for a sortal disambiguation of these nominalizations. More reliable and detailed linguistic analyses can be achieved, for example, by combini"
W10-1825,francom-hulden-2008-parallel,0,0.0256228,"el Parses Christian Chiarcos∗ and Kerstin Eckart∗∗ and Julia Ritz∗ ∗ ∗∗ Collaborative Research Centre 632 Collaborative Research Centre 732 “Information Structure” “Incremental Specification in Context” Universit¨at Potsdam Universit¨at Stuttgart {chiarcos|jritz}@uni-potsdam.de eckartkn@ims.uni-stuttgart.de Abstract representation, and weights for the parallel application and combination of multiple parsers. This approach has been previously applied to morphological and morphosyntactic annotations (Borin, 2000; Zavrel and Daelemans, 2000; Tufis¸, 2000), but only recently to syntax annotation (Francom and Hulden, 2008; de la Clergerie et al., 2008). Because of the complexity of syntax annotations as compared to part of speech tags, however, novel technologies have to be applied that allow us to represent, to visualize and to query multiple syntactic analyses of the same sentence. This paper describes the workflow from raw text to a searchable representation of the corpus. One of the aims of this new resource is to assess potential weaknesses in the parsers as well as their characteristic strengths. For the example of ambiguities in PP attachment, Sect. 4 shows how linguistic analyses can be improved by com"
W10-1825,W07-1501,0,0.0337184,"den, 15-16 July 2010. 2010 Association for Computational Linguistics 3 (1) Der Dax reagiert derzeit auf die the Dax reacts presently on the Meldungen aus London. messages from London ‘Presently, the Dax [German stock index, N.B.] is reacting to the news from London.’ Querying and Visualizing Alternative Parses In order to integrate multiple annotations created by different tools, we employ a generic XML format, PAULA XML (Dipper and G¨otze, 2005). PAULA XML is an XML linearization of the data model underlying the ANNIS data base.5 It is comparable to NITE XML (Carletta et al., 2005) and GrAF (Ide, 2007). PAULA XML supports diverse data structures (trees, graphs, and flat spans of tokens) and allows for conflicting hierarchies. The integrated PAULA representation of the multiple-parses corpus can be accessed using ANNIS, a web interface for querying and visualizing richly annotated corpora. Fig. 1 shows the ANNIS interface: top left is the query field; below that is the ’match count’ field (presenting the number of instances matching the query). Below this field is the list of corpora the user choses from. Matches are visualized in the right window. Tokens and token-level annotations are show"
W10-1825,C04-1024,0,0.0324599,"ing the information produced by different parsers: On the basis of qualitative and quantitative analyses, generalized rules for the improvement of the respective parsers can be developed, as well as rules for the mapping of their output to a tool-independent 2 Parsing In order to maximize both coverage and granularity of linguistic analyses, we chose parsers from different classes: A probabilistic constituent parser and a rule-based parser that produces semantically enriched dependency parses. 2.1 BitPar BitPar (Schmid, 2006) is a probabilistic context free parser using bit-vector operations (Schmid, 2004). Node categories are annotated along with grammatical functions, part-of-speech tags and morphological information in a parse tree. BitPar analyses are conformant to the TIGER annotation scheme (Brants et al., 2004), and the tool’s output format is similar to the list-based bracketing format of the Penn Treebank (Bies et al., 1995). The BitPar analysis of sentence (1) is visualized as the right-most tree in Fig. 1. 166 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 166–171, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 3 (1) Der"
W10-1825,P06-1023,0,0.0147723,"reliable and detailed linguistic analyses can be achieved, for example, by combining the information produced by different parsers: On the basis of qualitative and quantitative analyses, generalized rules for the improvement of the respective parsers can be developed, as well as rules for the mapping of their output to a tool-independent 2 Parsing In order to maximize both coverage and granularity of linguistic analyses, we chose parsers from different classes: A probabilistic constituent parser and a rule-based parser that produces semantically enriched dependency parses. 2.1 BitPar BitPar (Schmid, 2006) is a probabilistic context free parser using bit-vector operations (Schmid, 2004). Node categories are annotated along with grammatical functions, part-of-speech tags and morphological information in a parse tree. BitPar analyses are conformant to the TIGER annotation scheme (Brants et al., 2004), and the tool’s output format is similar to the list-based bracketing format of the Penn Treebank (Bies et al., 1995). The BitPar analysis of sentence (1) is visualized as the right-most tree in Fig. 1. 166 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 166–171, c Uppsala,"
W10-1825,tufis-2000-using,0,0.703629,"Missing"
W10-1825,zavrel-daelemans-2000-bootstrapping,0,0.755434,"Missing"
W10-1825,villemonte-de-la-clergerie-etal-2008-passage,0,\N,Missing
W10-1825,P02-1056,0,\N,Missing
W11-0402,I08-7013,0,0.0188419,"o enhance the consistency of linguistic annotations is to make use of crosslinguistic meta schemes or annotation standards, such as EAGLES (Leech and Wilson, 1996). The problem is that these enforce the use of the same categories across multiple languages, and this may be inappropriate for historically and geographically unrelated languages. For specific linguistic and historical regions, the application of standardization approaches has, however, been performed with great success, e.g., for Western (Leech and Wilson, 1996) and Eastern Europe (Erjavec et al., 2003) or the Indian subcontinent (Baskaran et al., 2008). 11 Proceedings of the Fifth Law Workshop (LAW V), pages 11–20, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics In this paper, we illustrate differences and commonalities of both approaches by creating an OWL/DL terminology repository from the MULTEXT-East (MTE) specifications (Erjavec et al., 2003; Erjavec, 2010), which define features for the morphosyntactic level of linguistic description, instantiate them for 16 languages and provide morphosyntactic tagsets for these languages. The specifications are a part of the MTE resources, which also include lexic"
W11-0402,brants-hansen-2002-developments,0,0.0373834,"notation mapping The classic approach to link annotations with reference concepts is to specify rules that define a direct mapping (Zeman, 2008). It is, however, not always possible to find a 1:1 mapping. One problem is conceptual overlap: A common noun may occur as a part of a proper name, e.g., German Palais ‘baroque-style palace’ in Neues Palais lit. ‘new palace’, a Prussian royal palace in Potsdam/Germany. Palais is thus both a proper noun (in its function), and a common noun (in its form). Such conceptual overlap is sometimes represented with a specialized tag, e.g., in the TIGER scheme (Brants and Hansen, 2002). ISOcat (like other terminological repositories) does currently not provide the corresponding hybrid category, so that Palais is to be linked to both properNoun/DC-1371 and commonNoun/DC1256 if the information carried by the original annotation is to be preserved. Contractions pose similar problems: English gonna combines going (PTB tag VBG, Marcus et al., 1994) and to (TO). If whitespace tokenization is applied, both tags need to be assigned to the same token. A related problem is the representation of ambiguity: The SUSANNE (Sampson, 1995) tag ICSt applies to English after both as a preposi"
W11-0402,I08-1051,0,0.190073,"Missing"
W11-0402,buyko-etal-2008-ontology,1,0.689512,"n Slavic languages is expressed differently: For Czech, reduced adjectives are marked by Formation=nominal, but for Polish by Definiteness=short-art. In the ontology, such redundancies are resolved by owl:equivalentClass statements, marked by ≡ in Fig. 3. 5 Summary and Discussion We have described the semi-automatic creation of an ontological model of the MTE morphosyntactic specifications for 16 different languages. Such a model may be fruitfully applied in various ways, e.g., within an NLP pipeline that uses ontological specifications of annotations rather than their string representations (Buyko et al., 2008; Hellmann, 2010). The ontological modeling may serve also as a first step towards an ontology-based documentation of the annotations within a corpus query system (Rehm et al., 2007; Chiarcos et al., 2008), or even the ontological modeling of entire corpora (Burchardt et al., 2008; Hellmann et al., 2010) and lexicons (Martin et al., 2009). As an interesting side-effect of the OWL conversion of the entire body of MTE resources, they could be easily integrated with existing lexical-semantic resources as Linked Data, e.g., OWL/RDF versions of WordNet (Gangemi et al., 2003), which are currently be"
W11-0402,W03-2904,1,0.875673,"Missing"
W11-0402,erjavec-2010-multext,1,0.801985,"guistic and historical regions, the application of standardization approaches has, however, been performed with great success, e.g., for Western (Leech and Wilson, 1996) and Eastern Europe (Erjavec et al., 2003) or the Indian subcontinent (Baskaran et al., 2008). 11 Proceedings of the Fifth Law Workshop (LAW V), pages 11–20, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics In this paper, we illustrate differences and commonalities of both approaches by creating an OWL/DL terminology repository from the MULTEXT-East (MTE) specifications (Erjavec et al., 2003; Erjavec, 2010), which define features for the morphosyntactic level of linguistic description, instantiate them for 16 languages and provide morphosyntactic tagsets for these languages. The specifications are a part of the MTE resources, which also include lexicons and an annotated parallel corpus that use these morphosyntactic tagsets. The encoding of the MTE specifications follows the Text Encoding Initiative Guidelines, TEI P5 (TEI Consortium, 2007), and this paper concentrates on developing a semi-automatic procedure for converting them from TEI XML to OWL. While TEI is more appropriate for authoring th"
W11-0402,C94-1097,0,0.277953,"Missing"
W11-0402,zeman-2008-reusable,0,0.0903705,"As we argue below, different design decisions in the terminology repositories make it necessary to use a linking formalism that is capable of expressing both disjunctions and conjunctions of concepts. For this reason, we propose the application of OWL/DL. By representing the MTE specifications, the repositories, and the linking between them as separate OWL/DL models, we follow the architectural concept of the OLiA architecture (Chiarcos, 2008), see Sect. 5. 3.2 Annotation mapping The classic approach to link annotations with reference concepts is to specify rules that define a direct mapping (Zeman, 2008). It is, however, not always possible to find a 1:1 mapping. One problem is conceptual overlap: A common noun may occur as a part of a proper name, e.g., German Palais ‘baroque-style palace’ in Neues Palais lit. ‘new palace’, a Prussian royal palace in Potsdam/Germany. Palais is thus both a proper noun (in its function), and a common noun (in its form). Such conceptual overlap is sometimes represented with a specialized tag, e.g., in the TIGER scheme (Brants and Hansen, 2002). ISOcat (like other terminological repositories) does currently not provide the corresponding hybrid category, so that"
W11-0402,J93-2004,0,\N,Missing
W11-2805,P09-1092,0,0.159056,"xt – or, better, a salience score calculated on this basis – can help to predict contextually adequate packaging preferences. In NLG, discourse salience has been employed to generate referring expressions (McCoy and Strube, 1999), to assign grammatical roles (Stede, 1998), and word order preferences (Kruijff et al., 2001). More recently, however, salience-based approaches have been increasingly superseded by statistical approaches, that nevertheless build on earlier theories of salience, e.g., Shiramatsu et al. (2007) for referring expressions, Zarrieß et al. (2011) for voice alternation, and Cahill and Riester (2009) for word order. One of the reasons for this methodological shift may be the observation (noted, for example, by 1 Along with referring expressions and grammatical roles, word order alternation has been described in a similar way, and it is of particular importance for the motivation of twodimensional models of salience (Chiarcos, 2011b). For reasons of space, however, this paper concentrates on referring expressions and grammatical roles. 2 Discourse salience is to be distinguished from other types of salience, that are either not specific to discourse referents (e.g., salience of semantic fe"
W11-2805,P88-1023,0,0.506463,"iv´on, 2001) and Functional Generative Description (Sgall et al., 1986, FGD) share a set of common insights, in particular, the close association between referential coherence and attentional states (as manifested in the salience of discourse referents), but they focus on different aspects of referential coherence and formalize them in different ways.3 Even worse, the field is notoriously plagued by a multitude of incompatible terminologies: ‘Salience’, for example, is used as a near-synonym of ‘givenness’ (Sgall et al., 1986, p.54f.), but also as a near-synonym of ‘newness (for the hearer)’ (Davis and Hirschberg, 1988), or ‘degree of interest (of the speaker)’ (Langacker, 1997, p.22). Therefore, the operationalization of discourse salience in NLG requires a theoretical foundation and a formalization of salience and its effects on information packaging. This paper takes its point of departure from a theoretical framework of discourse salience that has been developed as a generalization over Centering, Topicality and FGD. This framework, as sketched in Sect. 2, resolves the terminological difficulties associated with the notion of salience by distinguishing two dimensions of salience, with independent effects"
W11-2805,J95-2003,0,0.778098,"ferents, the insurance agent Toni, her sister Cynthia and their apartment suffer from an earthquake, the central protagonist of the paragraph is Toni, and the text goes on elaborating her situation. (1) The apartment she shares with her sister was rattled ... (a) The apartment the agent shares with her sister ... We consider two packaging phenomena: Referring expressions (1a: definite NP vs. pronoun), and grammatical roles (1b: active vs. passive).1 These variants are meaning-equivalent in the sense of Dorr et al. (2004), but according to theories of referential coherence (Sgall et al., 1986; Grosz et al., 1995; Giv´on, 2001), they express different discourse functions, often described with reference to the notion of ‘discourse salience’.2 Accordingly, the local discourse context – or, better, a salience score calculated on this basis – can help to predict contextually adequate packaging preferences. In NLG, discourse salience has been employed to generate referring expressions (McCoy and Strube, 1999), to assign grammatical roles (Stede, 1998), and word order preferences (Kruijff et al., 2001). More recently, however, salience-based approaches have been increasingly superseded by statistical approa"
W11-2805,W07-1525,1,0.857173,"e hearer salient than referents with focal (new) antecedents. The opposite claim, formulated by Sgall et al. (1986), requires alternative oc formulations of these salience factors reffante (r) := top f oc top 2 − refante (r) and woante (r) := 2 − woante (r). 38 4 Evaluation The parameters identified above are evaluated against referring expressions and grammatical roles in two German newspaper corpora that combine syntactic and anaphoric annotations, i.e., a coreferenceannotated subcorpus of the NEGRA corpus (Skut et al., 1997; Schiehlen, 2004), and the Potsdam Commentary Corpus (Stede, 2004; Krasavina and Chiarcos, 2007, PCC). 4.1 Pronominalization and hsal metrics Hearer salience is evaluated with respect to pronominalization. As shown in Fig. 2, personal pronouns are characterized by a high degree of hearer salience (otherwise, a definite description would have been used) and a low degree of speaker salience (otherwise, a demonstrative pronoun would have been used). As speaker salience is neutralized, pronominalization provides a test case for metrics of hearer salience. For the study of hearer salience, we applied CART and C4.5 decision trees and classified hearer salience scores against the pronominal an"
W11-2805,W01-0810,0,0.0373072,"Missing"
W11-2805,J94-4002,0,0.19163,"), that are aligned with cumulated salience scores calculated from hearer salience and speaker salience (Sect. 2.4), and • principles for the mapping between packaging hierarchies and salience scores (Sect. 2.5). As opposed to related models in functionalcognitive linguistics, e.g., Mulkern (2007), our formalization is operationalizable for NLG applications: It allows to predict packaging preferences for discourse referents from numerical salience scores (Sect. 2.5). Metrics of salience applied in Natural Language Processing are dominated by research on anaphora resolution in the tradition of Lappin and Leass (1994). Such salience metrics do, however, focus on the backward-looking, hearer-oriented aspect of salience, whereas the speaker-oriented, forward-looking aspect of salience is neglected. This tradition also had a strong impact on NLG, in particular in the field of generating referring expressions (GRE). Current metrics of discourse salience in GRE are thus essentially concerned with hearer salience,4 although the relevance of speakeroriented factors has been recognized for other aspects of NLG, e.g., for German word order as being sensitive to a domain-specific ‘aboutness’ criterion (Filippova and"
W11-2805,W99-0108,0,0.234116,"pronoun), and grammatical roles (1b: active vs. passive).1 These variants are meaning-equivalent in the sense of Dorr et al. (2004), but according to theories of referential coherence (Sgall et al., 1986; Grosz et al., 1995; Giv´on, 2001), they express different discourse functions, often described with reference to the notion of ‘discourse salience’.2 Accordingly, the local discourse context – or, better, a salience score calculated on this basis – can help to predict contextually adequate packaging preferences. In NLG, discourse salience has been employed to generate referring expressions (McCoy and Strube, 1999), to assign grammatical roles (Stede, 1998), and word order preferences (Kruijff et al., 2001). More recently, however, salience-based approaches have been increasingly superseded by statistical approaches, that nevertheless build on earlier theories of salience, e.g., Shiramatsu et al. (2007) for referring expressions, Zarrieß et al. (2011) for voice alternation, and Cahill and Riester (2009) for word order. One of the reasons for this methodological shift may be the observation (noted, for example, by 1 Along with referring expressions and grammatical roles, word order alternation has been d"
W11-2805,J04-3003,0,0.0634179,"Missing"
W11-2805,C04-1074,0,0.0168662,"the claim that referents with topical (given) antecedents are more hearer salient than referents with focal (new) antecedents. The opposite claim, formulated by Sgall et al. (1986), requires alternative oc formulations of these salience factors reffante (r) := top f oc top 2 − refante (r) and woante (r) := 2 − woante (r). 38 4 Evaluation The parameters identified above are evaluated against referring expressions and grammatical roles in two German newspaper corpora that combine syntactic and anaphoric annotations, i.e., a coreferenceannotated subcorpus of the NEGRA corpus (Skut et al., 1997; Schiehlen, 2004), and the Potsdam Commentary Corpus (Stede, 2004; Krasavina and Chiarcos, 2007, PCC). 4.1 Pronominalization and hsal metrics Hearer salience is evaluated with respect to pronominalization. As shown in Fig. 2, personal pronouns are characterized by a high degree of hearer salience (otherwise, a definite description would have been used) and a low degree of speaker salience (otherwise, a demonstrative pronoun would have been used). As speaker salience is neutralized, pronominalization provides a test case for metrics of hearer salience. For the study of hearer salience, we applied CART and C4.5"
W11-2805,A97-1014,0,0.110088,"oante (r) formalize the claim that referents with topical (given) antecedents are more hearer salient than referents with focal (new) antecedents. The opposite claim, formulated by Sgall et al. (1986), requires alternative oc formulations of these salience factors reffante (r) := top f oc top 2 − refante (r) and woante (r) := 2 − woante (r). 38 4 Evaluation The parameters identified above are evaluated against referring expressions and grammatical roles in two German newspaper corpora that combine syntactic and anaphoric annotations, i.e., a coreferenceannotated subcorpus of the NEGRA corpus (Skut et al., 1997; Schiehlen, 2004), and the Potsdam Commentary Corpus (Stede, 2004; Krasavina and Chiarcos, 2007, PCC). 4.1 Pronominalization and hsal metrics Hearer salience is evaluated with respect to pronominalization. As shown in Fig. 2, personal pronouns are characterized by a high degree of hearer salience (otherwise, a definite description would have been used) and a low degree of speaker salience (otherwise, a demonstrative pronoun would have been used). As speaker salience is neutralized, pronominalization provides a test case for metrics of hearer salience. For the study of hearer salience, we appl"
W11-2805,W04-0213,0,0.0356279,"dents are more hearer salient than referents with focal (new) antecedents. The opposite claim, formulated by Sgall et al. (1986), requires alternative oc formulations of these salience factors reffante (r) := top f oc top 2 − refante (r) and woante (r) := 2 − woante (r). 38 4 Evaluation The parameters identified above are evaluated against referring expressions and grammatical roles in two German newspaper corpora that combine syntactic and anaphoric annotations, i.e., a coreferenceannotated subcorpus of the NEGRA corpus (Skut et al., 1997; Schiehlen, 2004), and the Potsdam Commentary Corpus (Stede, 2004; Krasavina and Chiarcos, 2007, PCC). 4.1 Pronominalization and hsal metrics Hearer salience is evaluated with respect to pronominalization. As shown in Fig. 2, personal pronouns are characterized by a high degree of hearer salience (otherwise, a definite description would have been used) and a low degree of speaker salience (otherwise, a demonstrative pronoun would have been used). As speaker salience is neutralized, pronominalization provides a test case for metrics of hearer salience. For the study of hearer salience, we applied CART and C4.5 decision trees and classified hearer salience sc"
W11-2805,J99-3001,0,0.100906,"Missing"
W11-2805,P11-1101,0,0.106914,"alience’.2 Accordingly, the local discourse context – or, better, a salience score calculated on this basis – can help to predict contextually adequate packaging preferences. In NLG, discourse salience has been employed to generate referring expressions (McCoy and Strube, 1999), to assign grammatical roles (Stede, 1998), and word order preferences (Kruijff et al., 2001). More recently, however, salience-based approaches have been increasingly superseded by statistical approaches, that nevertheless build on earlier theories of salience, e.g., Shiramatsu et al. (2007) for referring expressions, Zarrieß et al. (2011) for voice alternation, and Cahill and Riester (2009) for word order. One of the reasons for this methodological shift may be the observation (noted, for example, by 1 Along with referring expressions and grammatical roles, word order alternation has been described in a similar way, and it is of particular importance for the motivation of twodimensional models of salience (Chiarcos, 2011b). For reasons of space, however, this paper concentrates on referring expressions and grammatical roles. 2 Discourse salience is to be distinguished from other types of salience, that are either not specific"
W11-2805,J98-3003,0,\N,Missing
W11-2805,W01-1605,0,\N,Missing
W13-5201,chiarcos-etal-2012-open,1,0.713906,"Missing"
W13-5201,francopoulo-etal-2006-lexical,0,0.0366736,"road band-width of formalisms and resources used to analyze, process and generate natural language. With the transition to empirical, data-driven research, the primary challenge in the field is thus to store, connect and exploit the wealth of language data available in all its heterogeneity. Interoperability of language resources has hence been an important issue addressed by the community since the late 1980s (Text Encoding Initiative, 1990), but still remains a problem that is solved only partially, i.e., on the level of specific sub-types of linguistic resources, such as lexical resources (Francopoulo et al., 2006) or annotated corpora (Ide and Suderman, 2007), respectively. A closely related challenge is information integration, i.e., how information from different sources can be retrieved and combined in an efficient way. Recently, both challenges have been addressed by means of Linked Data principles (Chiarcos et al., 2013a,b), eventually leading to the formation of a Linguistic Linked Open Data (LLOD) cloud (Chiarcos et al., 2012b). The talk describes its current state of development, it presents se1. promoting the idea of open linguistic resources, 2. developing means for their representation, and"
W13-5201,W07-1501,0,0.0373837,"to analyze, process and generate natural language. With the transition to empirical, data-driven research, the primary challenge in the field is thus to store, connect and exploit the wealth of language data available in all its heterogeneity. Interoperability of language resources has hence been an important issue addressed by the community since the late 1980s (Text Encoding Initiative, 1990), but still remains a problem that is solved only partially, i.e., on the level of specific sub-types of linguistic resources, such as lexical resources (Francopoulo et al., 2006) or annotated corpora (Ide and Suderman, 2007), respectively. A closely related challenge is information integration, i.e., how information from different sources can be retrieved and combined in an efficient way. Recently, both challenges have been addressed by means of Linked Data principles (Chiarcos et al., 2013a,b), eventually leading to the formation of a Linguistic Linked Open Data (LLOD) cloud (Chiarcos et al., 2012b). The talk describes its current state of development, it presents se1. promoting the idea of open linguistic resources, 2. developing means for their representation, and 3. encouraging the exchange of ideas and resou"
W13-5501,I08-1051,0,0.0753902,"esent typologically relevant phenomena, along with examples for their illustration and annotations (glosses) and translations applied to these examples (structurally comparable to corpus data), or word lists (structurally comparable to lexical-semantic resources). RDF as a generic representation formalism is thus particularly appealing for this class of resources. Finally, for linguistic corpora (Fig. 1, corpora), the potential of the Linked Data paradigm for modeling, processing and querying of corpora is immense, and RDF conversions of semantically annotated corpora have been proposed early [3]. RDF provides a graph-based data model as required for the interoperable representation of arbitrary kinds of annotation [2, 15], and this flexibility makes it a promising candidate for a general means of representation for corpora with complex and heterogeneous annotations. RDF does not only establish interoperability between annotations within a corpus, but also between corpora and other linguistic resources [4]. In comparison to other types of linguistic resources, corpora are currently underrepresented in the LLOD cloud, but the development of schemes for corpora and/or NLP annotations re"
W13-5501,chiarcos-2012-ontologies,1,0.897695,"to establish conceptual interoperability between language resources. If resourcespecific annotations or abbreviations are expanded into references to repositories of linguistic terminology and/or metadata categories, linguistic annotations, grammatical features and metadata specifications become more easily comparable. Important repositories developed by different communities include GOLD [9] and ISOcat [20, 19], yet, only recently these terminology repositories were put in relation with each other using Linked Data principles and with linguistic resources, e.g., within the OLiA architecture [5]. Linguistic databases are a particularly heterogeneous group of linguistic resources; they contain complex and manifold types of information, e.g., feature structures that represent typologically relevant phenomena, along with examples for their illustration and annotations (glosses) and translations applied to these examples (structurally comparable to corpus data), or word lists (structurally comparable to lexical-semantic resources). RDF as a generic representation formalism is thus particularly appealing for this class of resources. Finally, for linguistic corpora (Fig. 1, corpora), the p"
W13-5501,W07-1501,0,0.0927928,"s refer to ISOcat URIs. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for common problems, e.g., the development of a database that is capable of supporting flexible, graph-based data structures as necessary for multi-layer corpora [15]. Beyond this, another advantage warrants a mention: The distributed approach of the Linked Data paradigm facilitates the distributed development of a web of resources and collaboration between researchers that provide and use this data and that employ a shared set of technologies. One consequence is the emergence of interdisciplinary efforts to create large and interconnected sets of resources in linguistics and beyond. LDL-2013 aims to provide a forum to discuss and to facilitate such on-going developments. LLOD: Building the Cloud Recent years have seen not only a number of approaches to pr"
W13-5501,wright-2004-global,0,0.0894745,"able URIs, it is possible to combine information from physically separated repositories in a single query at runtime. Information from different resources in the cloud can then be integrated freely. Dynamic Import If cross-references between linguistic resources are represented by resolvable URIs instead of system-defined ID references or static copies of parts from another resource, it is not only possible to resolve them at runtime, but also to have access to the most recent version of a resource. For community-maintained terminology repositories like the ISO TC37/SC4 Data Category Registry [20, 19, ISOcat], for example, new categories, definitions or examples can be introduced occasionally, and this information is available immediately to anyone whose resources refer to ISOcat URIs. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for c"
W14-0604,W11-4106,0,0.039943,"Missing"
W14-0604,W07-2416,0,0.264446,"PPCEME Middle PPME2 YCOE Old PROIEL (Taylor et al., 2003a; Kroch et al., 2010) (Kroch et al., 2004) (Kroch and Taylor, 2000) (Taylor et al., 2003b) (Taylor et al., 2003b) Modern STTS Early Mod. PCENHG Sect. 2.2 Old T-CODEX (Schiller et al., 1999) (Light, 2013) Modern Old Norse (Petrova et al., 2009) Alpino (Bouma et al., 2001) Menota (Haugen et al., 2008) language English High German period scheme PTB Modern Stanford deps Penn2Malt deps Early Mod. PPCEME Middle PPME2 YCOE Old PROIEL Modern Early Mod. corpus reference (Taylor et al., 2003a; Kroch et al., 2010) (De Marneffe and Manning, 2008) (Johansson and Nugues, 2007) (Kroch et al., 2004) (Kroch and Taylor, 2000) (Taylor et al., 2003b) (Taylor et al., 2003b) TIGER T¨uba-D/Z NEGRA PCENHG (Brants et al., 2004) (Telljohann et al., 2003) (Skut et al., 1997) (Light, 2013) (Bouma et al., 2001) (Leech and Wilson, 1996) Dutch Modern Alpino Mamba (Nivre et al., 2006) Swedish Modern Mamba (Nivre et al., 2006) Icelandic IcePaHC (R¨ognvaldsson et al., 2012) Icelandic IcePaHC (R¨ognvaldsson et al., 2012) Gothic PROIEL (Haug and Jøhndal, 2008) Gothic PROIEL (Haug and Jøhndal, 2008) Danish Modern EAGLES Swedish Modern (b) Syntactic annotations (a) Morphosyntactic annotat"
W14-0604,P10-1068,1,0.840919,"her languages and evaluation against another set of dependency (DS) annotations for Gothic and Old English (Tab. 1), for which related annotation schemes for Latin, Greek and Czech are available – all of these languages are characterized by rich morphology and flexible syntax. ted, and corrected, but the agreement information could remain unaffected. These annotations have also been successfully employed in ensemble combination architectures, where information from different sources (say, NLP tools) was integrated on the basis of the Reference Model and disambiguated using ontological axioms (Chiarcos, 2010; Pareja-Lora and on on related languages Aguado de Cea, 2010). In an annotation proTgt Tgt Best monoling. Best biling. Triling. lang. model ∆UAS model ∆UAS model ∆UAS jection scenario, these sources could be projecDE .41 IS +.02n.s. +ME +.05∗ +OE +.04∗∗ tions from different languages annotated accordIS .32 ME −.06∗∗∗ +DE −.03n.s. +OE −.04∗ IS −.04∗∗∗ +OE −.01n.s. +DE −.02n.s. ing to different schemes, e.g., German, English, ME .60 OE .30 ME .00n.s. +IS .00n.s. +DE .00n.s. Swedish or Latin. These experiments are currently being conducted, but Annotation Models for sevTable 5: Performance of pa"
W14-0604,P99-1065,0,0.0971723,"Missing"
W14-0604,W08-1301,0,0.0608295,"Missing"
W14-0604,rognvaldsson-etal-2012-icelandic,0,0.190836,"Missing"
W14-0604,A97-1014,0,0.643194,"Missing"
W14-0604,P12-1018,0,0.0520396,"ions developed on the data described before, the automated phraselevel alignment of quasi-parallel text, and two experiments on annotation projection on parallel text. All of these experiments are still in a relatively early stage. 4.1 relative Levenshtein similarity ld δlev (wOS , wOHG ) = 1 − |wOS |+|w OHG | where ld is the standard Levenstein distance and |wOS |and |wOHG |are the number of characters in each word. Automated phrase-level alignment of quasi-parallel text statistical character replacement probability as approximated by a character-based statistical machine translation system (Neubig et al., 2012) The needs of historical lingustics demand a more fine-grained alignment than the currently available thematical alignment of Heliand with Tatian and the gospels. We thus investigate parallel phrase detection between Heliand (OS) and Tatian (OHG), resp., Heliand and the West Saxon gospels (OE). To identify cognate phrases, we explore 6 types of similarity metrics δ(wOS , wOHG ) for every OS word wOS and its potential OHG cognate wOHG . 5. normalization δnorm (wOS , wOHG ) = 0 0 ,w δi (wOS OHG ) , with wOS being the OHG ‘normalization’ of the original wOS . Here, normalization uses a weighted L"
W14-0604,W09-1104,0,0.0204729,"as the number of parallel passages that the metrics failed to align. Eventually, it was indicated that the best results can be achieved by combining multiple metrics. A combination of either direct lexicon-based or normalization-based alignment and geometrical alignment appears to be particularly promising. Yet, systematic experiments to automatically explore this feature space are still being prepared and depend on the availability of a gold alignment for selected verses. 4.2 On these projections, a fragment-aware parser was trained using the English (hyper)lemmas and the original POS tags (Spreyer and Kuhn, 2009). We limited the amount of parallel data available to a training set of 437 sentences per language and a test set of 174 per language. Our hypothesis was that in this setting, (projected) training data from related languages can be used in place of training data for the language under consideration, if the amount of data is sufficient and the languages are sufficiently closely related. Furthermore, we assumed that with an increasing number of languages considered (and thus training set size), the quality of the projected annotations would continuously improve as long as the languages are suffi"
W14-0604,nivre-etal-2006-talbanken05,0,0.09505,"Missing"
W14-0604,J03-1002,0,0.00444178,". . Except for automatically parsed Bibles in modern English, German and Swedish, the texts in this collection are not annotated. Where annotations are available from other corpora (Tab. 1), however, these were aligned with our Bibles. 2.2 2.3 Thematical alignment within and across biblical texts Translations of religious texts are well-suited for language comparison as well as NLP experiments exploiting parallel data as they are not only faithfully translated, but also, they come with a verselevel alignment which can serve as a basis for statistical word-level alignment, using, e.g., GIZA++ (Och and Ney, 2003). Where such a verse-level is not explicitly given, it can be automatically identified for actual translations. However, for independent compositions such as gospel harmonies, alignment is harder to identify and can only be established at the level of sections. In addition, similar links also exist between different parts of the Bible, e.g., parallel passages in different gospels. For these, an index providing a coarse-grained thematical alignment at the level of sections was extrapolated from the literature. This index can be exploited to increase the coverage of the alignment: where no exact"
W14-0604,pareja-lora-de-cea-2010-ontology,0,0.243541,"Missing"
W14-0604,petrov-etal-2012-universal,0,0.12323,"Missing"
W14-5302,ballesteros-nivre-2012-maltoptimizer-system,0,0.0169266,"te number of archaisms, it is particularly wellsuited for automated annotation. 3 Experimental setup We study the projection of dependency syntax, as it is considered particularly suitable for free word-order languages like IS, OE and DE. The existing constituent annotations were thus converted with standard tools for PTB conversion. Figure 1 summarizes the experimental setup. For annotating EN, we created dependency versions of WSJ and Brown sections of the PTB with the LTH Converter (Johansson and Nugues, 2007). We trained Malt 1.7.2 (Nivre, 2003), optimized its features with MaltOptimizer (Ballesteros and Nivre, 2012), and parsed the EN bible using the resulting feature model. 1 http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html http://enhgcorpus.wikispaces.com 3 http://esv.org 2 13 The ME, OE, DE and IS datasets were word aligned with EN using GIZA++ (Och and Ney, 2003). 1 : n alignments were resolved to the most probable 1 : 1 mapping. During annotation projection, we assume that the aligned words represent the respective heads for the remaining n − 1 words. These dependent words are assigned the dependency relation FRAG to the word that got the highest score in the translation table. Thi"
W14-5302,W11-4106,0,0.0857416,"ated Early Modern High German (DE) and Middle Icelandic (IS) for which we possess comparable annotations, and test the following hypotheses: (H1) Adding data from related varieties compensates the sparsity of target language training data. (H2) Data from related languages compensates the lack of target language training data. (H3) The greater the diachronic proximity, the better the performance of (H1) and (H2). We test these hypotheses in the following setup: (1) Hyperlemmatization: Different historical variants are normalized to a consistent standard, e.g., represented by a modern language (Bollmann et al., 2011). We emulate hyperlemmatization by English glosses automatically obtained through SMT. (2) Projection: We create training data for a fragment-aware dependency parser (Spreyer et al., 2010) using annotation projection from modern English. (3) Combination and evaluation: Parser modules are trained on different training data sets, and evaluated against existing gold annotations. In our setting, we enforce data sparsity by using deliberately small training data sets. This is because we emulate the situation of less-documented languages that will be in the focus of subsequent experiments, namely, O"
W14-5302,W13-2302,0,0.0231028,"wonder how the annotation projection approach performs in comparison to direct applications of modern language NLP tools to normalized historical data language (Scheible et al., 2011). While it is unlikely that such an approach could scale beyond closely related varieties, successful experiments on the annotation of normalized historical language have been reported, although mostly focused on token-level annotations (POS, lemma, morphology) of language stages which syntax does not greatly deviate from modern rules (Rayson et al., 2007; Pennacchiotti and Zanzotto, 2008; Kestemont et al., 2010; Bollmann, 2013). For the annotation of more remotely related varieties with more drastic differences in word order rigidity or morphology as considered here, however, projection techniques are more promising as they have been successfully applied to unrelated languages, as well, but still benefit from diachronic proximity, cf. Meyer (2011) for the projection-based morphological analysis of Modern and Old Russian. The goal of our experiment was not to achieve state-of-the-art performance, but to show whether background material from related languages with different degrees of diachronic distance can help to c"
W14-5302,P99-1065,0,0.236764,"Missing"
W14-5302,W07-2416,0,0.0253038,"n IcePaHC side-project, it adapts the IS annotation scheme. EN For EN, we use the ESV Bible.3 Due to a moderate number of archaisms, it is particularly wellsuited for automated annotation. 3 Experimental setup We study the projection of dependency syntax, as it is considered particularly suitable for free word-order languages like IS, OE and DE. The existing constituent annotations were thus converted with standard tools for PTB conversion. Figure 1 summarizes the experimental setup. For annotating EN, we created dependency versions of WSJ and Brown sections of the PTB with the LTH Converter (Johansson and Nugues, 2007). We trained Malt 1.7.2 (Nivre, 2003), optimized its features with MaltOptimizer (Ballesteros and Nivre, 2012), and parsed the EN bible using the resulting feature model. 1 http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html http://enhgcorpus.wikispaces.com 3 http://esv.org 2 13 The ME, OE, DE and IS datasets were word aligned with EN using GIZA++ (Och and Ney, 2003). 1 : n alignments were resolved to the most probable 1 : 1 mapping. During annotation projection, we assume that the aligned words represent the respective heads for the remaining n − 1 words. These dependent words"
W14-5302,P07-2052,0,0.0175349,"incomplete projections. In our setting, fMalt used two features, POS and hyperlemmas. POS The tagsets of the historical corpora originate in PTB, but show incompatible adaptations to the native morphosyntax. Tagset extensions on grammatical case in OE, IS and DE were removed and language-specific extensions for auxiliaries and modal verbs were leveled, in favor of a common, but underspecified tagset for all four languages. As these generalized tags preserve information not found in EN, they were fed into the parser. (hyper-)lemma Lexicalization is utterly important for the dependency parsing (Kawahara and Uchimoto, 2007), but to generalize over specifics of historical language varieties, hyperlemmatization needs to be performed. Similar to Zeman and Resnik (2008), we use projected English words as hyperlemmas and feed them into the parser. Hyperlemmatization against a closely related languages is acceptable as we can expect that the syntactic properties of words are likely to be similar. The projected annotations were then evaluated against dependency annotations created analoguously to the EN annotations from manual PTB-style constituency syntax. As LTH works exclusively on PTB data, the historical corpora w"
W14-5302,P12-1018,0,0.118322,"on from other languages and evaluation against syntactic annotations according to other schemes not derived from the Penn Treebank, as currently available, for example, for Old High German, Old Norse, and Gothic. 17 2. The hyperlemmatization in our approach was achieved through alignment/SMT, and a similar lexically-oriented approach has been suggested by (Zeman and Resnik, 2008). Alternative strategies more suitable for scenarios with limited amounts of training data may include the use of orthographical normalization techniques (Bollmann et al., 2011) or substring-based machine translation (Neubig et al., 2012) and are also subject to on-going research. We assume that SMT-based hyperlemmatization introduces more noise than these strategies, so that it is harder to achieve statistically significant results. Our findings are thus likely to remain valid regardless of the hyperlemmatization strategy. This hypothesis is, however, yet to be confirmed in subsequent studies. 3. Our experiment mostly deals with data translated from (or at least informed by) the Latin Vulgate. Our data may be biased by translation strategies which evolved over time, from very literal translations (actually, glossings) of Lati"
W14-5302,W03-3017,0,0.0918275,"scheme. EN For EN, we use the ESV Bible.3 Due to a moderate number of archaisms, it is particularly wellsuited for automated annotation. 3 Experimental setup We study the projection of dependency syntax, as it is considered particularly suitable for free word-order languages like IS, OE and DE. The existing constituent annotations were thus converted with standard tools for PTB conversion. Figure 1 summarizes the experimental setup. For annotating EN, we created dependency versions of WSJ and Brown sections of the PTB with the LTH Converter (Johansson and Nugues, 2007). We trained Malt 1.7.2 (Nivre, 2003), optimized its features with MaltOptimizer (Ballesteros and Nivre, 2012), and parsed the EN bible using the resulting feature model. 1 http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html http://enhgcorpus.wikispaces.com 3 http://esv.org 2 13 The ME, OE, DE and IS datasets were word aligned with EN using GIZA++ (Och and Ney, 2003). 1 : n alignments were resolved to the most probable 1 : 1 mapping. During annotation projection, we assume that the aligned words represent the respective heads for the remaining n − 1 words. These dependent words are assigned the dependency relation"
W14-5302,J03-1002,0,0.00463235,"s converted with standard tools for PTB conversion. Figure 1 summarizes the experimental setup. For annotating EN, we created dependency versions of WSJ and Brown sections of the PTB with the LTH Converter (Johansson and Nugues, 2007). We trained Malt 1.7.2 (Nivre, 2003), optimized its features with MaltOptimizer (Ballesteros and Nivre, 2012), and parsed the EN bible using the resulting feature model. 1 http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html http://enhgcorpus.wikispaces.com 3 http://esv.org 2 13 The ME, OE, DE and IS datasets were word aligned with EN using GIZA++ (Och and Ney, 2003). 1 : n alignments were resolved to the most probable 1 : 1 mapping. During annotation projection, we assume that the aligned words represent the respective heads for the remaining n − 1 words. These dependent words are assigned the dependency relation FRAG to the word that got the highest score in the translation table. This solution solves, among others, the problem of separable verb prefixes in DE, for example, DE ruffen with prefix an would be aligned to English word call: As P (”call”|”an”) < P (”call”|”ruffen”), the syntactic information of ”call” will be projected to ”ruffen” and ”an” w"
W14-5302,rognvaldsson-etal-2012-icelandic,0,0.172538,"Missing"
W14-5302,W11-1503,0,0.0306328,"n. A representative investigation of annotation projection techniques thus requires the consideration of quasi-parallel data along with parallel data. This can be found in the great wealth of medieval religious literature, with Bible paraphrases, gospel harmonies, sermons and homilies as well as poetic and prose adaptations of biblical motives. The parallel corpus of Germanic languages thus needs to be extended accordingly. 4. One may wonder how the annotation projection approach performs in comparison to direct applications of modern language NLP tools to normalized historical data language (Scheible et al., 2011). While it is unlikely that such an approach could scale beyond closely related varieties, successful experiments on the annotation of normalized historical language have been reported, although mostly focused on token-level annotations (POS, lemma, morphology) of language stages which syntax does not greatly deviate from modern rules (Rayson et al., 2007; Pennacchiotti and Zanzotto, 2008; Kestemont et al., 2010; Bollmann, 2013). For the annotation of more remotely related varieties with more drastic differences in word order rigidity or morphology as considered here, however, projection techn"
W14-5302,spreyer-etal-2010-training,0,0.107051,"tes the sparsity of target language training data. (H2) Data from related languages compensates the lack of target language training data. (H3) The greater the diachronic proximity, the better the performance of (H1) and (H2). We test these hypotheses in the following setup: (1) Hyperlemmatization: Different historical variants are normalized to a consistent standard, e.g., represented by a modern language (Bollmann et al., 2011). We emulate hyperlemmatization by English glosses automatically obtained through SMT. (2) Projection: We create training data for a fragment-aware dependency parser (Spreyer et al., 2010) using annotation projection from modern English. (3) Combination and evaluation: Parser modules are trained on different training data sets, and evaluated against existing gold annotations. In our setting, we enforce data sparsity by using deliberately small training data sets. This is because we emulate the situation of less-documented languages that will be in the focus of subsequent experiments, namely, Old High German and Old Saxon, which are relatively poorly documented. We do hope, however, that scalable NLP solutions can be developed if we add background information from their descenda"
W14-5302,W03-3023,0,0.015942,"mmatization needs to be performed. Similar to Zeman and Resnik (2008), we use projected English words as hyperlemmas and feed them into the parser. Hyperlemmatization against a closely related languages is acceptable as we can expect that the syntactic properties of words are likely to be similar. The projected annotations were then evaluated against dependency annotations created analoguously to the EN annotations from manual PTB-style constituency syntax. As LTH works exclusively on PTB data, the historical corpora were converted with its antecessor Penn2Malt4 using user-defined head-rules (Yamada and Matsumoto, 2003). 4 Evaluation results ME OE DE IS baseline UAS .60 .31 .41 .32 +DE +IS +OE +IS ∆UAS worst model +1 +2 +.00n.s. +DE+IS n.s. -.00 +DE+IS +.02n.s. +OE+IS n.s. -.02 +DE+OE n.s. -.01 -.02n.s. +.03∗ -.02n.s. +OE +DE +ME +ME ∆UAS best model +1 +2 +.01n.s. +OE+IS n.s. +.02 +ME+DE +.04∗∗∗ +ME+IS n.s. +.00 +ME+DE n.s. +.01 +.00n.s. +.03∗ -.01n.s. ∆UAS +3 -.00n.s. +.02n.s. +.04∗∗ -.04∗∗ (a) trained on target and related language(s) ME OE DE IS baseline UAS .60 .31 .41 .32 OE DE OE OE ∆UAS worst model 1 2 -.09∗∗∗ DE-IS ∗ -.03 ME-DE -.01n.s. OE-IS ∗∗∗ -.07 DE-OE n.s -.01 -.01n.s. +.02n.s. -.02n.s. IS ME I"
W14-5302,N01-1026,0,0.157221,", (b) that a parser trained on data from two related languages (and none from the target language) can reach a performance that is statistically not significantly worse than that of a parser trained on the projections to the target language, and (c) that both conclusions holds only among the three most closely related languages under consideration, but not necessarily the fourth. The experiments motivate the compilation of a larger parallel corpus of historical Germanic varieties as a basis for subsequent studies. 1 Background and motivation We describe an experiment on annotation projection (Yarowski and Ngai, 2001) between different Germanic languages, resp., their historical varieties, with the goal to assess to what extent sparsity of parallel data can be compensated by material from varieties related to the target variety, and studying the impact of diachronic proximity onto such applications. Statistical NLP of historical language data involves general issues typical for low-resource languages (the lack of annotated corpora, data sparsity, etc.), but also very specific challenges such as lack of standardized orthography, unsystematized punctuation, and a considerable degree of morphological variatio"
W14-5302,I08-3008,0,0.166894,"show incompatible adaptations to the native morphosyntax. Tagset extensions on grammatical case in OE, IS and DE were removed and language-specific extensions for auxiliaries and modal verbs were leveled, in favor of a common, but underspecified tagset for all four languages. As these generalized tags preserve information not found in EN, they were fed into the parser. (hyper-)lemma Lexicalization is utterly important for the dependency parsing (Kawahara and Uchimoto, 2007), but to generalize over specifics of historical language varieties, hyperlemmatization needs to be performed. Similar to Zeman and Resnik (2008), we use projected English words as hyperlemmas and feed them into the parser. Hyperlemmatization against a closely related languages is acceptable as we can expect that the syntactic properties of words are likely to be similar. The projected annotations were then evaluated against dependency annotations created analoguously to the EN annotations from manual PTB-style constituency syntax. As LTH works exclusively on PTB data, the historical corpora were converted with its antecessor Penn2Malt4 using user-defined head-rules (Yamada and Matsumoto, 2003). 4 Evaluation results ME OE DE IS baselin"
W15-4626,P98-1013,0,0.0383686,"Palmer et al. (2005). 178 Proceedings of the SIGDIAL 2015 Conference, pages 178–187, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics their type, in that they assume that all candidate NIs are equally likely to be missing, which is unrealistic given the variety of different linguistic contexts in which predicates co-occur with their semantic roles. (2) They are expensive in that they require handcrafted, idiosyncratic rules (Ruppenhofer et al., 2011) and rich background knowledge in the form of language-specific lexical resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) or NomBank (Meyers et al., 2004). Dictionaries providing information about each predicate and status of the individual roles (e.g., whether they can serve as implicit elements or not) are costly, and for most other languages not available to the same extent as for English. (3) Most earlier studies heuristically restrict implicit arguments to core roles2 only (Tonelli and Delmonte, 2010; Silberer and Frank, 2012), but this is problematic as it ignores the fact that implicit non-core roles also provide valid and valuable information. Our approach remains agnostic"
W15-4626,W13-2322,0,0.013172,"s, we employ a generic role set which is based on PropBank/NomBank rather than FrameNet: The PropBank format comprises a relatively small role inventory which is better suited to obtain statistical generalizations than the great variety of highly specific FrameNet roles. While FrameNet roles seem to be more fine-grained, their greater number arises mostly from predicate-specific semantic roles, whose specific semantics can be recovered from PropBank annotations by pairing semantic roles with the predicate. Yet another motivation of our work is related to the recent development of AMR parsing (Banarescu et al., 2013, Abstract Meaning Representation) which aims at modeling the semantic representation of a sentence while abstracting from syntactic idiosyncrasies. This particular appraoch makes extensive use of the PropBank-style framesets, as well, and would greatly benefit from the integration of information on implicit roles. The paper is structured as follows: Section 2 outlines related work in which we exclusively focus on how previous research has handled the sole identification of NIs. Sect. 3 describes our approach to probabilistic NI detection; Sect. 4 presents two experiments and their evaluation;"
W15-4626,P13-1116,0,0.0304503,"Missing"
W15-4626,P14-5010,0,0.00759556,"Missing"
W15-4626,W09-1206,0,0.0546541,"Missing"
W15-4626,W04-2705,0,0.209408,"Missing"
W15-4626,S10-1059,0,0.0175337,"locally unexpressed arguments which can be resolved to elements in the proceeding or following discourse; 2 Core roles are obligatory arguments of a predicate. Informally, non-core roles are optional arguments often realized as adjuncts or modifiers. 3 179 http://semeval2.fbk.eu/semeval2.php Indefinite NIs (INIs) are elements for which no antecedent can be identified in the surrounding context.4 Also, the evaluation data comes in two flavors: a base format which is compliant with the FrameNet paradigm and a CoNLL-based PropBank format. Previous research has exclusively focused on the former. Chen et al. (2010) present an extension of an existing FrameNet-style parser (SEMAFOR) to handle implicit elements in text. The identification of NIs is guided by the assumption that, whenever the traditional SRL parser returns the default label involved in a non-saturated analysis for a sentence, an implicit role has to be found in the context instead. Additional FrameNet-specific heuristics are employed in which, e.g., the presence of one particular role in a frame makes the identification of another implicit role redundant.5 Tonelli and Delmonte (2010, VENSES++) present a deep semantic approach to NI resolut"
W15-4626,J05-1004,0,0.056846,"re inflexible and absolute according to El Salvador is now the only Latin American country which still has troops in [Iraq]. Nicaragua, Honduras and the Dominican Republic have withdrawn their troops [∅]. In the second sentence, a standard SRL parser would ideally identify withdraw as the main verbal predicate. In its thematic relation to the other words within the same sentence, all countries serve as the overtly expressed (explicit) agents, and are thus labeled as arguments A0.1 Semantically, they are the action performers, whereas 1 For details on all PropBank labels used in our study, see Palmer et al. (2005). 178 Proceedings of the SIGDIAL 2015 Conference, pages 178–187, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics their type, in that they assume that all candidate NIs are equally likely to be missing, which is unrealistic given the variety of different linguistic contexts in which predicates co-occur with their semantic roles. (2) They are expensive in that they require handcrafted, idiosyncratic rules (Ruppenhofer et al., 2011) and rich background knowledge in the form of language-specific lexical resources, such as FrameNet (Baker et al., 1998),"
W15-4626,R15-1074,1,0.144122,"Missing"
W15-4626,P12-2042,1,0.875503,"Missing"
W15-4626,S13-1043,0,0.395221,"s beyond the state-of-the-art. 1 Introduction Automated implicit semantic role labeling (iSRL) has emerged as a novel area of interest in the recent years. In contrast to traditional SRL, which aims to detect events (e.g., verbal or nominal predicates) together with their associated semantic roles (agent, theme, recipient, etc.) as overtly realized in the current sentence, iSRL extends this analysis with locally unexpressed linguistic items. Hence, iSRL requires to broaden the scope beyond isolated sentences to the surrounding discourse. As an illustration, consider the following example from Roth and Frank (2013): Current issues in iSRL Corpus data with manually annotated implicit roles is extremely sparse and hard to obtain, and annotation efforts have emerged only recently; cf. Ruppenhofer et al. (2010), Gerber and Chai (2012), and also Feizabadi and Pad´o (2015) for an attempt to enlarge the number of annotation instances by combination of scarce resources. As a result, most state-ofthe-art iSRL systems cannot be trained in a supervised setting and thus integrate custom, rule-based components to detect NIs (we elaborate on related work in Section 2). To this end, a predicate’s overt roles are match"
W15-4626,S15-1005,0,0.0215952,"Missing"
W15-4626,W09-2417,0,0.0444197,"Missing"
W15-4626,R11-1046,0,0.0144095,"guments A0.1 Semantically, they are the action performers, whereas 1 For details on all PropBank labels used in our study, see Palmer et al. (2005). 178 Proceedings of the SIGDIAL 2015 Conference, pages 178–187, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics their type, in that they assume that all candidate NIs are equally likely to be missing, which is unrealistic given the variety of different linguistic contexts in which predicates co-occur with their semantic roles. (2) They are expensive in that they require handcrafted, idiosyncratic rules (Ruppenhofer et al., 2011) and rich background knowledge in the form of language-specific lexical resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) or NomBank (Meyers et al., 2004). Dictionaries providing information about each predicate and status of the individual roles (e.g., whether they can serve as implicit elements or not) are costly, and for most other languages not available to the same extent as for English. (3) Most earlier studies heuristically restrict implicit arguments to core roles2 only (Tonelli and Delmonte, 2010; Silberer and Frank, 2012), but this is problematic as it"
W15-4626,S12-1001,0,0.0146799,"ndcrafted, idiosyncratic rules (Ruppenhofer et al., 2011) and rich background knowledge in the form of language-specific lexical resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) or NomBank (Meyers et al., 2004). Dictionaries providing information about each predicate and status of the individual roles (e.g., whether they can serve as implicit elements or not) are costly, and for most other languages not available to the same extent as for English. (3) Most earlier studies heuristically restrict implicit arguments to core roles2 only (Tonelli and Delmonte, 2010; Silberer and Frank, 2012), but this is problematic as it ignores the fact that implicit non-core roles also provide valid and valuable information. Our approach remains agnostic regarding the role inventory, and can address both core and non-core arguments. Yet, in accordance with the limited evaluation data and in line with earlier literature, we had to restrict ourselves to evaluate NI predictions for core arguments only. alize over large quantities of explicit roles to find evidence for implicit information in a mildly supervised manner. Our proposed models are largely domain independent, include a sense distinctio"
W15-4626,S10-1065,0,0.0150191,"sive in that they require handcrafted, idiosyncratic rules (Ruppenhofer et al., 2011) and rich background knowledge in the form of language-specific lexical resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) or NomBank (Meyers et al., 2004). Dictionaries providing information about each predicate and status of the individual roles (e.g., whether they can serve as implicit elements or not) are costly, and for most other languages not available to the same extent as for English. (3) Most earlier studies heuristically restrict implicit arguments to core roles2 only (Tonelli and Delmonte, 2010; Silberer and Frank, 2012), but this is problematic as it ignores the fact that implicit non-core roles also provide valid and valuable information. Our approach remains agnostic regarding the role inventory, and can address both core and non-core arguments. Yet, in accordance with the limited evaluation data and in line with earlier literature, we had to restrict ourselves to evaluate NI predictions for core arguments only. alize over large quantities of explicit roles to find evidence for implicit information in a mildly supervised manner. Our proposed models are largely domain independent,"
W15-4626,W11-0908,0,0.0343694,"Missing"
W15-4626,W10-0903,0,\N,Missing
W15-4626,S10-1008,0,\N,Missing
W15-4626,C98-1013,0,\N,Missing
W15-4626,J12-4003,0,\N,Missing
W15-5505,buyko-etal-2008-ontology,1,0.816513,"Missing"
W15-5505,W11-0402,1,0.69798,"nded in an ontology. This is a major difference as compared to radically reductionist approaches like Petrov et al. (2012) which inevitably lead to an extensive information loss, especially for highly detailed annotation schemes such as Susanne. A different kind of information loss frequently occurs with approaches based on a meta tag set as ‘interlingua’ (Leech and Wilson, 1996; Zeman, 2008): Here, a taxonomy tags is enforced from one set of languages (that the taxonomy was developed for) to another, where the pressure to stay within the pre-defined model frequently leads to ‘tag abuse’, see Chiarcos and Erjavec (2011) for the corresponding analysis of MULTEXT-East (Erjavec, 2004). But it also differs from more flexible, bottom-up-grown meta tag sets (Zeman, 2008), because without the implicit disjointness assumption of tags (categories) in classical tagsets, it is possible to preserve divergent, but compatible analyses, e.g., enduring in capable of enduring friendships is both a verb (morphologically) and an adjective (syntactically). As being lossless, OLiA ensures that the information contained in the original schemes will be preserved to a maximal extent by its conceptual representation. Figure 1: The S"
W15-5505,P10-1068,1,0.621069,"le losses in precision arising from the heterogeneity of the training data. Introduction 2 Ontologies have long been recognized as a primary device for interoperability among annotations and linguistic descriptions (Farrar and Langendoen, 2003; Ide and Romary, 2004; Saulwick et al., 2005), and they have been applied to facilitate querying (Saulwick et al., 2005; Rehm et al., 2007), interoperability among modules in NLP pipelines (Buyko et al., 2008; Hellmann, 2010), or for post-processing (i.e., merging, enriching or disambiguating) the output of NLP tools (ParejaLora and Aguado de Cea, 2010; Chiarcos, 2010a; Hellmann et al., 2013). In this paper, we describe a novel approach towards the next challenge along this trajectory, i.e., the development of NLP tools 1 Corpora For reasons of interpretability, we use English corpora for this experiment, but we consider the approach to be language-independent, and (in the longer perspective) particularly relevant to lessresourced languages with a lower degree of de facto standardization in annotated corpora than English. Historical and modern less-resourced languages are often annotated according to a great variety of annotation schemes which can not be t"
W15-5505,silveira-etal-2014-gold,0,0.036395,"Missing"
W15-5505,ide-romary-2004-registry,0,0.0461806,"to different schemes. In this regard, this paper describes a novel approach toward automatic part-of-speech (POS) annotation, and investigates the extent to which ontology-based annotations allow us to train NLP tools on corpora with divergent, but conceptually related annotations, and whether the increase in the granularity of analysis outweighs possible losses in precision arising from the heterogeneity of the training data. Introduction 2 Ontologies have long been recognized as a primary device for interoperability among annotations and linguistic descriptions (Farrar and Langendoen, 2003; Ide and Romary, 2004; Saulwick et al., 2005), and they have been applied to facilitate querying (Saulwick et al., 2005; Rehm et al., 2007), interoperability among modules in NLP pipelines (Buyko et al., 2008; Hellmann, 2010), or for post-processing (i.e., merging, enriching or disambiguating) the output of NLP tools (ParejaLora and Aguado de Cea, 2010; Chiarcos, 2010a; Hellmann et al., 2013). In this paper, we describe a novel approach towards the next challenge along this trajectory, i.e., the development of NLP tools 1 Corpora For reasons of interpretability, we use English corpora for this experiment, but we c"
W15-5505,N03-1033,0,0.103737,"rcos (2010b) who also assumed that classes along the subclasssuperclass axis are compatible with each other, whereas siblings (and their descendants) are incompatible. 5.2 6 Experimental Results Three neural networks were trained on the training sets: EWT/PTB data only, Susanne/Susa data only, and both training sets combined. Several state-of-the-art POS taggers have been trained on this data as baseline: TreeTagger (Schmid, 1999), Lapos (Tsuruoka et al., 2011) and Stanford Corpus Pruning As an alternative to structural pruning, we estimate path consistency directly out of the tags of the 27 (Toutanova et al., 2003), all trained and tested on the same (non-combined) data as the neural networks. Training these on PTB annotations was straightforward. On Susa, however, TreeTagger could not accomodate 270 unique tags and was thus skipped, and Lapos could be trained but showed very low performance on the full tagset. The Stanford tagger was successfully trained using state-of-the-art MaxEnt (left3words) models for EWT and Susanne, respectively. Like the training data for the neural network, the output of each tagger was mapped to OLiA Reference Model concepts by means of the corresponding Annotation and Linki"
W15-5505,pareja-lora-de-cea-2010-ontology,0,0.286675,"Missing"
W15-5505,W11-0328,0,0.0161223,"th of the path |p|. Concepts that are compatible with the path but have values less than 0 (= negative evidence) are skipped. Path-based pruning follows Chiarcos (2010b) who also assumed that classes along the subclasssuperclass axis are compatible with each other, whereas siblings (and their descendants) are incompatible. 5.2 6 Experimental Results Three neural networks were trained on the training sets: EWT/PTB data only, Susanne/Susa data only, and both training sets combined. Several state-of-the-art POS taggers have been trained on this data as baseline: TreeTagger (Schmid, 1999), Lapos (Tsuruoka et al., 2011) and Stanford Corpus Pruning As an alternative to structural pruning, we estimate path consistency directly out of the tags of the 27 (Toutanova et al., 2003), all trained and tested on the same (non-combined) data as the neural networks. Training these on PTB annotations was straightforward. On Susa, however, TreeTagger could not accomodate 270 unique tags and was thus skipped, and Lapos could be trained but showed very low performance on the full tagset. The Stanford tagger was successfully trained using state-of-the-art MaxEnt (left3words) models for EWT and Susanne, respectively. Like the"
W15-5505,petrov-etal-2012-universal,0,0.171804,"Missing"
W15-5505,P10-1040,0,0.0368058,"m T = Tptb ∪Tsusa . For a given word wi with PTB annotation and its concept set s ⊆ Tptb , every output node yk with k ∈ {1..|T |} is assigned as follows:   if , tk ∈ s 1, yk = 0, if, tk ∈ T  Tptb   −1, if, tk ∈ Tptb  s Configuring and Training Neural Networks We trained neural networks on EWT reviews, an equally sized subset of the Susanne corpus (Sect. 2), and on both training sets combined. The core of the algorithm is a feed-forward neural network with resilient backpropagation with the following structure: 1. 75 input neurons that correspond to three 25dimensional word embeddings (Turian et al., 2010)3 of the target word, its predecessor and its successor from its immediate context; 2. one hidden layer with the tanh activation function. The number of neurons in the hidden layer is heuristically set to the average length of input and output layers, thus, a natural geometric (pyramidal) design; 3. a layer of output neurons that represent OLiA MorphosyntacticCategorys, again with tanh normalization. The activations of these neurons represent the output vector. For, say, training on EWT, all the output values that corresponded to the concepts generated only from Susa (e.g., DefiniteArticle and"
W15-5505,zeman-2008-reusable,0,0.199573,"tag APPGf as an example, used for 2 http://purl.org/olia/, includes PTB and Susa models 24 sets to a common meta tag set or creating a mapping between the tag sets, we decompose tag sets into statements (triples) grounded in an ontology. This is a major difference as compared to radically reductionist approaches like Petrov et al. (2012) which inevitably lead to an extensive information loss, especially for highly detailed annotation schemes such as Susanne. A different kind of information loss frequently occurs with approaches based on a meta tag set as ‘interlingua’ (Leech and Wilson, 1996; Zeman, 2008): Here, a taxonomy tags is enforced from one set of languages (that the taxonomy was developed for) to another, where the pressure to stay within the pre-defined model frequently leads to ‘tag abuse’, see Chiarcos and Erjavec (2011) for the corresponding analysis of MULTEXT-East (Erjavec, 2004). But it also differs from more flexible, bottom-up-grown meta tag sets (Zeman, 2008), because without the implicit disjointness assumption of tags (categories) in classical tagsets, it is possible to preserve divergent, but compatible analyses, e.g., enduring in capable of enduring friendships is both a"
W17-0910,P08-1090,0,0.0901928,"pt learning (Schank and Abelson, 1977; Mooney and DeJong, 1985), it is a highly challenging task which is built on top of a cascade of core NLP applications, including— among others—causal/temporal relation recognition (Mirza and Tonelli, 2016), event extraction (UzZaman and Allen, 2010), (implicit) semantic role labeling (Gerber and Chai, 2012; Schenk and Chiarcos, 2016) or inter-sentential discourse parsing (Mihaylov and Frank, 2016). Recent progress has been made in the field of narrative understanding: a variety of successful approaches have been introduced, ranging from narrative chains (Chambers and Jurafsky, 2008) to script learning techniques (Regneri et al., 2010), or event schemas (Nguyen et al., 2015). What Our Contribution: In this paper, we propose a lightweight, resource-lean framework for modeling procedural knowledge in commonsense stories whose only source of information are distributed word representations. We cast the problem of modeling text coherence as a special case of discourse processing in which our model jointly learns to distinguish correct from incorrect story endings. Our approach is inspired by promising related attempts using event embeddings and neural methods for script learn"
W17-0910,J12-4003,0,0.0286542,"on Semantic applications related to Natural Language Understanding have seen a recent surge of interest within the NLP community, and story understanding can be regarded as one of the supreme disciplines in that field. Closely related to Machine Reading (Hovy, 2006) and script learning (Schank and Abelson, 1977; Mooney and DeJong, 1985), it is a highly challenging task which is built on top of a cascade of core NLP applications, including— among others—causal/temporal relation recognition (Mirza and Tonelli, 2016), event extraction (UzZaman and Allen, 2010), (implicit) semantic role labeling (Gerber and Chai, 2012; Schenk and Chiarcos, 2016) or inter-sentential discourse parsing (Mihaylov and Frank, 2016). Recent progress has been made in the field of narrative understanding: a variety of successful approaches have been introduced, ranging from narrative chains (Chambers and Jurafsky, 2008) to script learning techniques (Regneri et al., 2010), or event schemas (Nguyen et al., 2015). What Our Contribution: In this paper, we propose a lightweight, resource-lean framework for modeling procedural knowledge in commonsense stories whose only source of information are distributed word representations. We cast"
W17-0910,W07-1401,0,0.0341435,"e at the same time discriminating them from semantically irrelevant continuations. der of both events. The distinction between different implicit discourse senses are subtle nuances and are highly challenging to detect automatically; however, they are typical of the ROCStories, as almost no explicit discourse markers are present between the individual story sentences. Finally, note that our motivation for this approach is also related to the classical view of recognizing textual entailment which would treat correct and wrong endings as the entailing and contradicting hypotheses, respectively (Giampiccolo et al., 2007; Mostafazadeh et al., 2016a). 3.1 For each component in the triplet, we have experimented with a variety of different calculations in order to capture their idiosyncratic syntactic and semantic properties. We found the vector P average over their respective words # v avg = N1 N i=1 E(ti ) to perform reasonably well, where N is the total number of tokens filling either of C, Q1 or Q2, respectively, resulting in three individual vector representations. Here, we define E(·) as an embedding function which maps a token ti to its disTraining Instances For the Story Cloze Test, we model a training"
W17-0910,P14-2050,0,0.0340501,"Missing"
W17-0910,K16-2014,0,0.0136006,"ge of interest within the NLP community, and story understanding can be regarded as one of the supreme disciplines in that field. Closely related to Machine Reading (Hovy, 2006) and script learning (Schank and Abelson, 1977; Mooney and DeJong, 1985), it is a highly challenging task which is built on top of a cascade of core NLP applications, including— among others—causal/temporal relation recognition (Mirza and Tonelli, 2016), event extraction (UzZaman and Allen, 2010), (implicit) semantic role labeling (Gerber and Chai, 2012; Schenk and Chiarcos, 2016) or inter-sentential discourse parsing (Mihaylov and Frank, 2016). Recent progress has been made in the field of narrative understanding: a variety of successful approaches have been introduced, ranging from narrative chains (Chambers and Jurafsky, 2008) to script learning techniques (Regneri et al., 2010), or event schemas (Nguyen et al., 2015). What Our Contribution: In this paper, we propose a lightweight, resource-lean framework for modeling procedural knowledge in commonsense stories whose only source of information are distributed word representations. We cast the problem of modeling text coherence as a special case of discourse processing in which ou"
W17-0910,P10-1100,0,0.0764446,"85), it is a highly challenging task which is built on top of a cascade of core NLP applications, including— among others—causal/temporal relation recognition (Mirza and Tonelli, 2016), event extraction (UzZaman and Allen, 2010), (implicit) semantic role labeling (Gerber and Chai, 2012; Schenk and Chiarcos, 2016) or inter-sentential discourse parsing (Mihaylov and Frank, 2016). Recent progress has been made in the field of narrative understanding: a variety of successful approaches have been introduced, ranging from narrative chains (Chambers and Jurafsky, 2008) to script learning techniques (Regneri et al., 2010), or event schemas (Nguyen et al., 2015). What Our Contribution: In this paper, we propose a lightweight, resource-lean framework for modeling procedural knowledge in commonsense stories whose only source of information are distributed word representations. We cast the problem of modeling text coherence as a special case of discourse processing in which our model jointly learns to distinguish correct from incorrect story endings. Our approach is inspired by promising related attempts using event embeddings and neural methods for script learning (Modi and Titov, 2014; Pichotta and Mooney, 2016)"
W17-0910,K16-2007,0,0.0674389,"Missing"
W17-0910,1985.tmi-1.17,0,0.0445161,"Missing"
W17-0910,N16-1098,0,0.591795,"is paper, we propose a lightweight, resource-lean framework for modeling procedural knowledge in commonsense stories whose only source of information are distributed word representations. We cast the problem of modeling text coherence as a special case of discourse processing in which our model jointly learns to distinguish correct from incorrect story endings. Our approach is inspired by promising related attempts using event embeddings and neural methods for script learning (Modi and Titov, 2014; Pichotta and Mooney, 2016). Our system is an end-to-end implementation of the ideas sketched in Mostafazadeh et al. (2016b) of the joint paragraph and sentence level model (cf. Section 3 for details). We evaluate our approach in the Story Cloze Test, a task for predicting story continuations. Despite its simplicity, our system demonstrates superior performance on the designated data over previous approaches to script learning and—due to its language and genre-independence—it also represents a solid basis for further optimization towards other textual domains. 1 The shared task of the LSDSem 2017 workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics: http://www.coli.uni-saarland.de/˜mrot"
W17-0910,N16-1173,1,0.846598,"s related to Natural Language Understanding have seen a recent surge of interest within the NLP community, and story understanding can be regarded as one of the supreme disciplines in that field. Closely related to Machine Reading (Hovy, 2006) and script learning (Schank and Abelson, 1977; Mooney and DeJong, 1985), it is a highly challenging task which is built on top of a cascade of core NLP applications, including— among others—causal/temporal relation recognition (Mirza and Tonelli, 2016), event extraction (UzZaman and Allen, 2010), (implicit) semantic role labeling (Gerber and Chai, 2012; Schenk and Chiarcos, 2016) or inter-sentential discourse parsing (Mihaylov and Frank, 2016). Recent progress has been made in the field of narrative understanding: a variety of successful approaches have been introduced, ranging from narrative chains (Chambers and Jurafsky, 2008) to script learning techniques (Regneri et al., 2010), or event schemas (Nguyen et al., 2015). What Our Contribution: In this paper, we propose a lightweight, resource-lean framework for modeling procedural knowledge in commonsense stories whose only source of information are distributed word representations. We cast the problem of modeling tex"
W17-0910,K16-2005,1,0.887904,"Missing"
W17-0910,W16-2505,0,0.205789,"is paper, we propose a lightweight, resource-lean framework for modeling procedural knowledge in commonsense stories whose only source of information are distributed word representations. We cast the problem of modeling text coherence as a special case of discourse processing in which our model jointly learns to distinguish correct from incorrect story endings. Our approach is inspired by promising related attempts using event embeddings and neural methods for script learning (Modi and Titov, 2014; Pichotta and Mooney, 2016). Our system is an end-to-end implementation of the ideas sketched in Mostafazadeh et al. (2016b) of the joint paragraph and sentence level model (cf. Section 3 for details). We evaluate our approach in the Story Cloze Test, a task for predicting story continuations. Despite its simplicity, our system demonstrates superior performance on the designated data over previous approaches to script learning and—due to its language and genre-independence—it also represents a solid basis for further optimization towards other textual domains. 1 The shared task of the LSDSem 2017 workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics: http://www.coli.uni-saarland.de/˜mrot"
W17-0910,P15-1019,0,0.102869,"Missing"
W17-0910,K16-2004,0,0.0270402,"iz 1 Quiz 2 I asked Sarah out on a date. She said yes. I was so excited for our date together. We went to dinner and then a movie. I had a terrible time. (wrong ending) I got to kiss Sarah goodnight. (correct ending) Table 1: An example of a ROCStory consisting of a core story and two alternative continuations. 2 2.1 The Story Cloze Test 3 Our proposed model architecture for finding the right story continuation is inspired by novel works from (shallow) discourse parsing, most notably by the recent success of neural network-based frameworks in that field (Xue et al., 2016; Schenk et al., 2016; Wang and Lan, 2016). Specifically for implicit discourse relations, i.e. for those sentence pairs which, for instance, can signal a temporal, contrast or contingency relation, but which suffer from the absence of an explicit discourse marker (such as but or because), it has been shown that the interaction of properly tuned distributed representations over adjacent text spans can be particularly powerful in the relation classification task. We cast the Story Cloze test as a special case of implicit discourse relation recognition and attempt to model an underlying, latent connection between a core story and its co"
W17-0910,D14-1162,0,0.0794365,"Missing"
W17-0910,P16-1027,0,0.0350895,"ues (Regneri et al., 2010), or event schemas (Nguyen et al., 2015). What Our Contribution: In this paper, we propose a lightweight, resource-lean framework for modeling procedural knowledge in commonsense stories whose only source of information are distributed word representations. We cast the problem of modeling text coherence as a special case of discourse processing in which our model jointly learns to distinguish correct from incorrect story endings. Our approach is inspired by promising related attempts using event embeddings and neural methods for script learning (Modi and Titov, 2014; Pichotta and Mooney, 2016). Our system is an end-to-end implementation of the ideas sketched in Mostafazadeh et al. (2016b) of the joint paragraph and sentence level model (cf. Section 3 for details). We evaluate our approach in the Story Cloze Test, a task for predicting story continuations. Despite its simplicity, our system demonstrates superior performance on the designated data over previous approaches to script learning and—due to its language and genre-independence—it also represents a solid basis for further optimization towards other textual domains. 1 The shared task of the LSDSem 2017 workshop on Linking Mod"
W17-0910,prasad-etal-2008-penn,0,0.0425948,"because), it has been shown that the interaction of properly tuned distributed representations over adjacent text spans can be particularly powerful in the relation classification task. We cast the Story Cloze test as a special case of implicit discourse relation recognition and attempt to model an underlying, latent connection between a core story and its correct vs. incorrect continuation. For instance, the final example sentence in the core story in Table 1 and the two adjacent quizzes could be treated as argument pairs (Arg1 and Arg2) in the classical view of the Penn Discourse Treebank (Prasad et al., 2008), distinguishing different types of implicit discourse relations that hold between them.3 Task Description In the Story Cloze Test a participating system is presented with a four-sentence core story along with two alternative single-sentence endings, i.e. a correct and a wrong one. The system is then supposed to select the correct ending based on a semantic analysis of the individual story components. For this binary choice, outputs are evaluated on accuracy level. 2.2 Approach Data The shared task organizers provide participants with a large corpus of approx. 98k five-sentence everyday life s"
W17-0910,C16-1007,0,\N,Missing
W17-2202,D14-1082,0,0.00823737,"milar manner to POS tags, dependency labels can be projected into Sumerian texts. Annotation projections of both POS tags and dependency labels need to be manually corrected. Using an adapted scheme for Sumerian, we will annotate a gold standard composed of a 12 total of 10,000 sentences with dependencies and POS tags to train a supervised dependency parser and POS tagger. The rest of the data will be tagged and parsed automatically. The quality of the dependency parses will be estimated by labeled and unlabeled attachment score (UAS and LAS), and different parsing toolkits will be evaluated (Chen and Manning 2014, Nivre 2003, etc.). 4 Machine Translation As MT for cuneiform languages is a novel task and there is no prior research, we will have to experiment with several approaches in order to establish the one most suitable for these languages. The standard phrase-based translation will form a good baseline. Currently, there are over 1600 parallel Sumerian and English texts which are aligned sentence-wise. The baseline will be created by the Moses SMT toolkit (Koehn et al., 2007). It will be trained on these parallel texts and applied to the rest of the data to create automatic translations. Neverthel"
W17-2202,P16-1078,0,0.0140705,"g prior probabilities to produce many-tomany character alignment; it can thus efficiently capture mid-distance dependencies, as required for dealing with rich morphology and ideosyllabic writing systems without explicit word separators (e.g., Japanese). In addition to this state-of-the-art SMT system, we will also apply innovative neural techniques to the translation of Sumerian cuneiform text. Neural Machine Translation (NMT) (Bahdanau et al., 2014) has been applied to various language pairs in the past few years, with successful applications for translating structurally different languages: Eriguchi et al. (2016) applied an attention-based neural network on Japanese and English that we will take as our point of departure, as the writing system of Japanese is structurally similar to cuneiform (using both ideographic and syllabic components), and they demonstrated that their approach is capable of generalizing over smaller amounts of training data than normally required by NMT systems. Following their syntaxFigure 2: NLP pipeline for Sumerian based extension of the traditional sequence-based encoder-decoder approach, we will integrate syntactic dependency annotation. 5 Information Extraction In this pro"
W17-2202,P07-2045,0,0.00688698,"l be estimated by labeled and unlabeled attachment score (UAS and LAS), and different parsing toolkits will be evaluated (Chen and Manning 2014, Nivre 2003, etc.). 4 Machine Translation As MT for cuneiform languages is a novel task and there is no prior research, we will have to experiment with several approaches in order to establish the one most suitable for these languages. The standard phrase-based translation will form a good baseline. Currently, there are over 1600 parallel Sumerian and English texts which are aligned sentence-wise. The baseline will be created by the Moses SMT toolkit (Koehn et al., 2007). It will be trained on these parallel texts and applied to the rest of the data to create automatic translations. Nevertheless, due to the spelling variations and morphological richness of the language, data sparsity is inevitable. Thus, the baseline will be compared with a character-based MT system based on Phrasal ITG Aligner (Pialign) (Neubig et al., 2012) but tailored towards cuneiform data. Pialign uses synchronous context-free grammars and substring prior probabilities to produce many-tomany character alignment; it can thus efficiently capture mid-distance dependencies, as required for"
W17-2202,N15-1167,0,0.173843,"de a large proportion of numero-metrological elements. They are also repetitive, brief, and formulaic. As the inscribed medium comes in varied sizes and shapes, structural elements in the transliterations indicate on which surface of the artifact the text appears. Figure 1 shows an example of an ASCII transliteration and translation of a cuneiform text, accompanied with a picture of the obverse and reverse of the artifact. 5 3 Data Preprocessing 3.2 Morphological analysis Our morphological analyzer will be partly based on existing tools such as Tablan et al. (2006)’s rule-based morphology and Liu et al. (2015)’s algorithm to identify named entities. We will design a custom parser for numero-metrological content for the occasion. Since Sumerian affixes are ambiguous, we will build on previous work on the disambiguation of morphologically rich languages, such as Sak et al. (2007)’s neural methods for Turkish and Rios and Mamani (2014)’s conditional random fields used to disambiguate Quechua morphology. Morphological tags assigned following rule-based algorithms will be reranked using different machine learning (ML) approaches. The disambiguated morphology will be used for syntactic parsing, MT, and i"
W17-2202,P12-1018,0,0.0179588,"r these languages. The standard phrase-based translation will form a good baseline. Currently, there are over 1600 parallel Sumerian and English texts which are aligned sentence-wise. The baseline will be created by the Moses SMT toolkit (Koehn et al., 2007). It will be trained on these parallel texts and applied to the rest of the data to create automatic translations. Nevertheless, due to the spelling variations and morphological richness of the language, data sparsity is inevitable. Thus, the baseline will be compared with a character-based MT system based on Phrasal ITG Aligner (Pialign) (Neubig et al., 2012) but tailored towards cuneiform data. Pialign uses synchronous context-free grammars and substring prior probabilities to produce many-tomany character alignment; it can thus efficiently capture mid-distance dependencies, as required for dealing with rich morphology and ideosyllabic writing systems without explicit word separators (e.g., Japanese). In addition to this state-of-the-art SMT system, we will also apply innovative neural techniques to the translation of Sumerian cuneiform text. Neural Machine Translation (NMT) (Bahdanau et al., 2014) has been applied to various language pairs in th"
W17-2202,W03-3017,0,0.0813317,"s, dependency labels can be projected into Sumerian texts. Annotation projections of both POS tags and dependency labels need to be manually corrected. Using an adapted scheme for Sumerian, we will annotate a gold standard composed of a 12 total of 10,000 sentences with dependencies and POS tags to train a supervised dependency parser and POS tagger. The rest of the data will be tagged and parsed automatically. The quality of the dependency parses will be estimated by labeled and unlabeled attachment score (UAS and LAS), and different parsing toolkits will be evaluated (Chen and Manning 2014, Nivre 2003, etc.). 4 Machine Translation As MT for cuneiform languages is a novel task and there is no prior research, we will have to experiment with several approaches in order to establish the one most suitable for these languages. The standard phrase-based translation will form a good baseline. Currently, there are over 1600 parallel Sumerian and English texts which are aligned sentence-wise. The baseline will be created by the Moses SMT toolkit (Koehn et al., 2007). It will be trained on these parallel texts and applied to the rest of the data to create automatic translations. Nevertheless, due to"
W17-2202,J03-1002,0,0.0106273,"merianprimer/ 7 11 (1) P322539 = CUSAS 03, 0851. tablet. obverse. 1. 1(disz) kusz udu niga 1 hide, grain-fed sheep; 2. 1(disz) kusz masz2 niga 1 hide, grain-fed goat; 3. kusz udu sa2-du11 sheep hides, regular offerings, 4. ki {d}iszkur-illat-ta from Adda-illat, reverse. 1. a-na-ah-i3-li2 Anah-ili; 2. szu ba-an-ti did receive. 3. iti ezem-an-na Month: An-festival, 4. mu na-ru2-a-mah mu-ne-du3 Year: He erected the great stele for them. (a) ASCII transliteration and English translation (b) Example of a Sumerian source text Figure 1: Artifact and its digitization shelf word-alignment tool Giza++ (Och and Ney, 2003), we can produce word alignment between English and the Sumerian texts. After we automatically tag English parallel texts, the assigned POS will be projected onto the aligned Sumerian words. The general assumption behind the annotation projection based on the word alignment is that translated words are likely to have the same POS as the source words. It is quite clear that this is a very bold assumption and there are a number of exceptions. Thus, both manual and automatic POS correction will be needed. However, the distantly supervised solution is temporary as there are parallel efforts to ann"
W17-2202,W14-5305,0,0.0272824,"ranslation of a cuneiform text, accompanied with a picture of the obverse and reverse of the artifact. 5 3 Data Preprocessing 3.2 Morphological analysis Our morphological analyzer will be partly based on existing tools such as Tablan et al. (2006)’s rule-based morphology and Liu et al. (2015)’s algorithm to identify named entities. We will design a custom parser for numero-metrological content for the occasion. Since Sumerian affixes are ambiguous, we will build on previous work on the disambiguation of morphologically rich languages, such as Sak et al. (2007)’s neural methods for Turkish and Rios and Mamani (2014)’s conditional random fields used to disambiguate Quechua morphology. Morphological tags assigned following rule-based algorithms will be reranked using different machine learning (ML) approaches. The disambiguated morphology will be used for syntactic parsing, MT, and information extraction. We plan to develop a lemmatizer that will exploit a high-coverage dictionary. The available off-the-shelf lemmatizer for Sumerian7 was NLP Pipeline for Sumerian State-of-the-art statistical NLP widely uses supervised classifiers to produce automatic linguistic annotation. Although some Sumerian and Akkadi"
W17-2202,W14-5302,1,0.881404,"Missing"
W17-2202,tablan-etal-2006-creating,0,0.818623,"exts are restricted in style and topic, and include a large proportion of numero-metrological elements. They are also repetitive, brief, and formulaic. As the inscribed medium comes in varied sizes and shapes, structural elements in the transliterations indicate on which surface of the artifact the text appears. Figure 1 shows an example of an ASCII transliteration and translation of a cuneiform text, accompanied with a picture of the obverse and reverse of the artifact. 5 3 Data Preprocessing 3.2 Morphological analysis Our morphological analyzer will be partly based on existing tools such as Tablan et al. (2006)’s rule-based morphology and Liu et al. (2015)’s algorithm to identify named entities. We will design a custom parser for numero-metrological content for the occasion. Since Sumerian affixes are ambiguous, we will build on previous work on the disambiguation of morphologically rich languages, such as Sak et al. (2007)’s neural methods for Turkish and Rios and Mamani (2014)’s conditional random fields used to disambiguate Quechua morphology. Morphological tags assigned following rule-based algorithms will be reranked using different machine learning (ML) approaches. The disambiguated morphology"
W17-2202,C14-1175,0,0.0185119,"art of this software might be reused. 3.3 POS tagging An important part of the NLP pipeline is the distantly supervised POS Tagging. As the corpus is currently unannotated, a supervised approach to POS tagging would not be applicable as it demands annotated training data. The creation of such training data through manual POS annotation of the data would demand an extremely rare expertise and is a time-consuming process. Therefore, we have to turn our attention to distantly supervised methods. As we are in possession of parallel English translations of Sumerian texts, an annotation projection (Tiedemann, 2014) approach would be a most suitable distantly supervised method. English texts can be tokenized, stemmed, lemmatized, POS tagged and parsed by off-the-shelf freely available NLP tools. Using an off-the3.4 Syntactic parsing In order to facilitate MT and information extraction from our source texts, we will syntactically parse the corpus. In a similar manner to POS tags, dependency labels can be projected into Sumerian texts. Annotation projections of both POS tags and dependency labels need to be manually corrected. Using an adapted scheme for Sumerian, we will annotate a gold standard composed"
