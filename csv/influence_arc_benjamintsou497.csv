2001.mtsummit-road.9,O98-3002,0,0.0350694,", MT evaluation, Chinese-English MT, Translation systems, Region-sensitive Romanisation Introduction There are many different aspects to consider in the evaluation of machine translation (MT) systems, including intelligibility, accuracy, error analysis and so on (e.g. Arnold et al., 1994; EAGLES, 1999). However, few seem to directly address the identification and translation of personal names within texts, especially between languages of different families, typically Chinese and English. While there are studies on the identification of personal names from Chinese texts (e.g. Sun et al., 1995; Chen & Bai, 1998), this paper discusses the importance of the translation component and evaluates several existing Chinese-English MT systems for their capability in this context. The translation of personal names might seem trivial between some languages. For instance, Bill Clinton is always Bill Clinton, in English or in French, written and sometimes even pronounced the same. However, between languages from different families such as Chinese and English, the complexity is often beyond description by a few simple rules. For example, the international Kung Fu film star, , is known all over the world as Jackie"
2009.mtsummit-wpt.3,P91-1022,0,0.544113,"ntence alignment in Sec. 4., and introduce sentence filtering, including the evaluation of its impact on SMT in Sec. 5 as well as the final parallel corpus in Sec. 6, and conclude this paper. http://www.itl.nist.gov/iad/mig/tests/mt/ Revised on 2009/08/06 -17- 2 Related Work To get parallel sentences from parallel corpora, different approaches can be used for sentence alignment. The approaches can be based on a) sentence length, b) lexical information in bilingual dictionaries, c) statistical translation model, or d) the composite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly d"
2009.mtsummit-wpt.3,P93-1002,0,0.601926,"mposite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the w"
2009.mtsummit-wpt.3,P91-1023,0,0.379605,"Sec. 4., and introduce sentence filtering, including the evaluation of its impact on SMT in Sec. 5 as well as the final parallel corpus in Sec. 6, and conclude this paper. http://www.itl.nist.gov/iad/mig/tests/mt/ Revised on 2009/08/06 -17- 2 Related Work To get parallel sentences from parallel corpora, different approaches can be used for sentence alignment. The approaches can be based on a) sentence length, b) lexical information in bilingual dictionaries, c) statistical translation model, or d) the composite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment"
2009.mtsummit-wpt.3,2001.mtsummit-papers.30,0,0.717795,"Missing"
2009.mtsummit-wpt.3,P07-2045,0,0.00531596,"icate fusing strategies than simply using average or multiplication. Filter is shown to be the best among all ensemble methods, which can be explained by the good filtering effects of Len and DictN for misaligned sentences among the highly ranked sentence pairs in the sorted list of Tran. 5.3 Impact of Sentence Filtering on SMT Although the experiment shows that sentence filtering can help identify really parallel sentences, we may wonder whether the sentence filtering actually leads to better SMT performance. Therefore, we evaluated the impact of sentence filtering on SMT. The Moses toolkit (Koehn et al., 2007) was used to conduct Chinese-&gt;English SMT experiments and BLEU and NIST scores are used as the evaluation metrics. We followed the instruction of the baseline system for the shared task in the 2008 ACL workshop on SMT. 9 8 Before the ensemble of individual scores, we first need to normalize the scores into the range between 0 and 1 according to their distributions: the length-based and dictionary-based scores are already within the range; the translation score roughly follows a linear distribution. The weights for Tran, Len, DictN are 99, 30 and 16, respectively. They are got by the exhaustive"
2009.mtsummit-wpt.3,2005.mtsummit-papers.11,0,0.131317,"Missing"
2009.mtsummit-wpt.3,ma-2006-champollion,0,0.401073,"g/tests/mt/ Revised on 2009/08/06 -17- 2 Related Work To get parallel sentences from parallel corpora, different approaches can be used for sentence alignment. The approaches can be based on a) sentence length, b) lexical information in bilingual dictionaries, c) statistical translation model, or d) the composite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does"
2009.mtsummit-wpt.3,moore-2002-fast,0,0.792182,"e online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and"
2009.mtsummit-wpt.3,J03-1002,0,0.0138393,"Missing"
2009.mtsummit-wpt.3,J05-4003,0,0.0820068,"ord-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a similarity measure, and selected only the highly ranked aligned sentence alignments. Although the construction of our ChineseE"
2009.mtsummit-wpt.3,J03-3002,0,0.0858242,"words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a sim"
2009.mtsummit-wpt.3,2007.mtsummit-papers.63,0,0.166161,"c, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a similarity measure, and selected only the highly ranked aligned sentence alignments. Although the construction of our ChineseEnglish patent parallel corpus is similar to that of the Japanese-English one (Utiyama and Isahara, 2007), we have made the following modifications on the basis of our data: 1) all sections of the patents, instead of only two parts in the description section, were used to find sentence alignments; 2) for sentence filtering, we integrated three individual measures, including the dictionary-based one (Utiyama and Isahara, 2007), and the experiments showed the combination of measures can improve the performance of sentence filtering. We also did SMT experiments, showing that filtering out misaligned sentences could improve SMT performance. 3 The Chinese-English Patents Parallel We use about 7000 Chi"
2009.mtsummit-wpt.3,D08-1058,0,0.0160422,"S c and S e respectively. 3) The bidirectional translation probability score Pt (Tran): it combines the translation probability value of both directions (i.e. Chinese-&gt;English and English-&gt;Chinese), instead of using only one direction (Moore, 2002; Chen, 2003). It is computed as follows: log ( P(S e |S c ))  log ( P(S c |S e )) lc  le where P( S e |S c ) denotes the probability that a translator will produce S e in English when presented with S c in Chinese, and vice versa for P(Sc |Se ) . pt ( S c , S e )  A wide variety of ensemble methods have been used in various fields (Polikar, 2006; Wan, 2008). -20- We evaluate the following8: 1) Average (Avg): the average of the individual scores; 2) Multiplication (Mul): the product of the individual scores; 3) Linear Combination (LinC): the weighted average by associating each individual score with a weight, indicating the relative confidence in the value; 4) Filter: use Pt for sorting, but if Pd or Pt of a sentence pair is lower than a predefined threshold, that pair will be moved to the end of the sorting list. The thresholds can be empirically set based on the data. 5.2 Empirical Evaluation of Sentence Filtering To assess the performance of i"
2009.mtsummit-wpt.3,I05-1023,0,0.0259188,"(2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a similarity measure, and selected only the highly ranked aligned sentence alignments. Although the construction of our ChineseEnglish patent parallel corpus is similar to that of the Japanese-English one (Utiyama and Isahara, 2007), we have made the following modifications on the basis of our data: 1) all sec"
2009.mtsummit-wpt.3,J93-1004,0,\N,Missing
2009.mtsummit-wpt.3,J93-2003,0,\N,Missing
2009.mtsummit-wpt.3,P03-1010,0,\N,Missing
2011.mtsummit-papers.54,W06-2810,0,0.0875888,"Missing"
2011.mtsummit-papers.54,P91-1022,0,0.49542,"h 2010, from http://www.wipo.int/ 473 Harvesting Multilingual Patents Out of the above 160K bilingual patents, we managed to locate the corresponding Japanese version of about 130K of them from the Industrial Property Digital Library9, which is provided by Japan’s National Center for Industrial Property Information and Training and is the public access portal of the Japanese Patent Office (JPO). 5 Mining Parallel Sentences Comparable Patents from Different approaches have been proposed to mine parallel sentences from bilingual documents based on the following information: (1) sentence length (Brown et al. 1991; Gale and Church, 1991); (2) lexical correspondence in bilingual dictionaries (Ma, 2006); (c) statistical translation model (Chen, 1993), or the composite of more than one approach (Simard and Plamondon, 1998; Moore, 2002). Since the comparable patents are not strictly parallel, the individual alignment methods mentioned above would not be effective. Thus, we, in this study, combine these three methods to mine high-quality parallel sentences from comparable patents by following Lu et al. (2009). For the patents, we automatically split them into individual sections according to the respective"
2011.mtsummit-papers.54,2007.mtsummit-papers.9,0,0.0177603,"by the conclusion in Section 7. 2 To overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, such as Zhao and Vogel (2002), Resnik and Smith (2003), Munteanu and Marcu (2005), Wu and Fung (2005), etc. Another direction is to 2 1 Related Work Retrieved Feb., 2011 from http://www.wipo.int/pctdb/en/. The data below involving PCT patents comes from the website of WIPO. 3 Retrieved Feb, 2011 from http://en.wikipedia.org/. 4 http://ntcir.nii.ac.jp/PatentMT/ http://www.itl.nist.gov/iad/mig/tests/mt/ 472 directly mine bilingual terms from the Web, such as Cao et al. (2007), Lin et al., (2008), Jiang et al. (2009). Smith et al. (2010) investigated the viability of Wikipedia as a comparable corpus and extracted parallel sentences from it. In their experiments, they extracted more than 1 million sentences pairs for two language pairs, namely German & English and Spanish & English, as well as 140 thousand Bulgarian-English parallel sentences. There are a few papers on related work in the patent domain. Higuchi et al. (2001) used the titles and abstracts of 32,000 Japanese-English bilingual patents to extract bilingual terms. Utiyama and Isahara (2007) mined about 2"
2011.mtsummit-papers.54,P93-1002,0,0.259177,"onding Japanese version of about 130K of them from the Industrial Property Digital Library9, which is provided by Japan’s National Center for Industrial Property Information and Training and is the public access portal of the Japanese Patent Office (JPO). 5 Mining Parallel Sentences Comparable Patents from Different approaches have been proposed to mine parallel sentences from bilingual documents based on the following information: (1) sentence length (Brown et al. 1991; Gale and Church, 1991); (2) lexical correspondence in bilingual dictionaries (Ma, 2006); (c) statistical translation model (Chen, 1993), or the composite of more than one approach (Simard and Plamondon, 1998; Moore, 2002). Since the comparable patents are not strictly parallel, the individual alignment methods mentioned above would not be effective. Thus, we, in this study, combine these three methods to mine high-quality parallel sentences from comparable patents by following Lu et al. (2009). For the patents, we automatically split them into individual sections according to the respective tags inside the patents, and segmented each section into sentences according to punctuations. The sentences in each section of English pa"
2011.mtsummit-papers.54,P08-1113,0,0.043315,"Missing"
2011.mtsummit-papers.54,P09-1098,0,0.0132014,"ercome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, such as Zhao and Vogel (2002), Resnik and Smith (2003), Munteanu and Marcu (2005), Wu and Fung (2005), etc. Another direction is to 2 1 Related Work Retrieved Feb., 2011 from http://www.wipo.int/pctdb/en/. The data below involving PCT patents comes from the website of WIPO. 3 Retrieved Feb, 2011 from http://en.wikipedia.org/. 4 http://ntcir.nii.ac.jp/PatentMT/ http://www.itl.nist.gov/iad/mig/tests/mt/ 472 directly mine bilingual terms from the Web, such as Cao et al. (2007), Lin et al., (2008), Jiang et al. (2009). Smith et al. (2010) investigated the viability of Wikipedia as a comparable corpus and extracted parallel sentences from it. In their experiments, they extracted more than 1 million sentences pairs for two language pairs, namely German & English and Spanish & English, as well as 140 thousand Bulgarian-English parallel sentences. There are a few papers on related work in the patent domain. Higuchi et al. (2001) used the titles and abstracts of 32,000 Japanese-English bilingual patents to extract bilingual terms. Utiyama and Isahara (2007) mined about 2 million parallel sentences by using the"
2011.mtsummit-papers.54,Y09-2038,1,0.843131,"nese bilingual lexicon suitable for alignment purpose. The publicly available bilingual lexical resources obtained tend to provide detailed definitions and explanations for each term. Such elaborate information does not represent the translated terms in actual usage in the patent texts. As a result, sentences cannot be well aligned based on such resources. Another possible direction is to make use of the Chinese-Japanese sentence pairs in the current trilingual corpus established to compile a Chinese-Japanese bilingual lexicon via bilingual term extraction (e.g. Kupiec, 1993; Ha et al., 2008; Lu and Tsou, 2009). After the new lexicon is built, we can directly align Chinese and Japanese sentences from scratch again, anticipating more bilingual sentences to be mined and aligned. In return, the new set of increased bilingual sentence pairs can contribute to more trilingual sentence triplets through pivoting. This creates a cycle of value-adding stages: trilingual sentences after pivoting -&gt; bilingual terms -&gt; more bilingual sentences -&gt; more trilingual sentences through pivoting. This iterative approach may also be generalized to multilingual corpora involving even more languages, but the complexity an"
2011.mtsummit-papers.54,ma-2006-champollion,0,0.21412,"gual patents, we managed to locate the corresponding Japanese version of about 130K of them from the Industrial Property Digital Library9, which is provided by Japan’s National Center for Industrial Property Information and Training and is the public access portal of the Japanese Patent Office (JPO). 5 Mining Parallel Sentences Comparable Patents from Different approaches have been proposed to mine parallel sentences from bilingual documents based on the following information: (1) sentence length (Brown et al. 1991; Gale and Church, 1991); (2) lexical correspondence in bilingual dictionaries (Ma, 2006); (c) statistical translation model (Chen, 1993), or the composite of more than one approach (Simard and Plamondon, 1998; Moore, 2002). Since the comparable patents are not strictly parallel, the individual alignment methods mentioned above would not be effective. Thus, we, in this study, combine these three methods to mine high-quality parallel sentences from comparable patents by following Lu et al. (2009). For the patents, we automatically split them into individual sections according to the respective tags inside the patents, and segmented each section into sentences according to punctuati"
2011.mtsummit-papers.54,moore-2002-fast,0,0.213137,"brary9, which is provided by Japan’s National Center for Industrial Property Information and Training and is the public access portal of the Japanese Patent Office (JPO). 5 Mining Parallel Sentences Comparable Patents from Different approaches have been proposed to mine parallel sentences from bilingual documents based on the following information: (1) sentence length (Brown et al. 1991; Gale and Church, 1991); (2) lexical correspondence in bilingual dictionaries (Ma, 2006); (c) statistical translation model (Chen, 1993), or the composite of more than one approach (Simard and Plamondon, 1998; Moore, 2002). Since the comparable patents are not strictly parallel, the individual alignment methods mentioned above would not be effective. Thus, we, in this study, combine these three methods to mine high-quality parallel sentences from comparable patents by following Lu et al. (2009). For the patents, we automatically split them into individual sections according to the respective tags inside the patents, and segmented each section into sentences according to punctuations. The sentences in each section of English patents were aligned with those in the corresponding section of the corresponding Chines"
2011.mtsummit-papers.54,J05-4003,0,0.0224257,"ng Chinese, English and Japanese, especially in the patent domain. Related work is introduced in Section 2. Patents, PCT patents, multilingual patents are described in Section 3. Then, harvesting multilingual patents from the Web and mining parallel corpora from them are introduced in Sections 4 and 5, respectively. The cultivation of the trilingual parallel corpus is introduced in Section 6, followed by the conclusion in Section 7. 2 To overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, such as Zhao and Vogel (2002), Resnik and Smith (2003), Munteanu and Marcu (2005), Wu and Fung (2005), etc. Another direction is to 2 1 Related Work Retrieved Feb., 2011 from http://www.wipo.int/pctdb/en/. The data below involving PCT patents comes from the website of WIPO. 3 Retrieved Feb, 2011 from http://en.wikipedia.org/. 4 http://ntcir.nii.ac.jp/PatentMT/ http://www.itl.nist.gov/iad/mig/tests/mt/ 472 directly mine bilingual terms from the Web, such as Cao et al. (2007), Lin et al., (2008), Jiang et al. (2009). Smith et al. (2010) investigated the viability of Wikipedia as a comparable corpus and extracted parallel sentences from it. In their experiments, they extracte"
2011.mtsummit-papers.54,J03-1002,0,0.00839367,"Missing"
2011.tc-1.9,2008.iwslt-papers.1,0,0.0192943,"and Isahara, 2007; Khalilov et al., 2008). The first step of this approach is the same as the first approach: to train source-pivot and pivot-target translation models. But the second step would be quite different: the translation step of this approach would be to first translate the source sentence to the pivot 3 sentence based on the source-pivot translation model, and then translate the pivot sentence to the target sentence based on the pivot-target translation model. The third uses existing models to build a synthetic source-target corpus, from which a source-target model can be trained (Bertoldi et al., 2008). For example, we can fist build a pivot-target translation model based on the pivot-target parallel corpus. Based on the pivot-target translation model, the pivot sentences in the original source-pivot bilingual corpus can be translated into the target language. We can then obtain a source-target corpus by translating the pivot sentences in the source-pivot corpus into the target language with the pivot-target translation model, and/or obtain a target-source corpus by translating the pivot sentences in the pivot-target corpus into the source language with the source-pivot translation model. 3"
2011.tc-1.9,P07-1092,0,0.0244535,"e been proposed and shown to be promising. Second, we discuss the rapidly increasing demands in the field of patent translation and various efforts to bootstrap patent machine translation in uncommon language pairs (e.g., Japanese and Chinese) via more common language pairs (e.g., Chinese-English and English-Japanese), and the application of the pivot approach to expedite processing. 2. Pivoting Approaches for Machine Translation There are three major approaches introduced below. Suppose the three languages involved are source, pivot and target. The first is based on phrase table translation (Cohn and Lapata 2007; Wu and Wang, 2007). The approach usually first train two translation models: source-pivot and pivot-target; then it induces a new source-target phrase table by using the translation probabilities and lexical weights in source-pivot and pivot-target translation models For example, to translate between Chinese and Japanese, we can first train Chinese-English and English-Japanese translation models based on available bilingual corpora; the two models are then combined together at the phrase level to provide a new Chinese-Japanese phrase translation table with induced translated probability for"
2011.tc-1.9,P91-1023,0,0.354761,"vant bilingual texts by sophisticated filtering processes. Thus there is the need to draw from quality texts the maximum amount of relevant parallel linguistic structures, which is of critical importance to the cultivation of high quality TM’s. The amount of useful bilingual corpora varies considerably between language pairs, and consequently the amount of useful TM's varies considerably among language pairs with English as a frequent common member in the paired TM's. For example, many parallel corpora with English as one language have been built, such as the French-English Canadian Hansards (Gale and Church, 1991), the Japanese-English parallel patent corpus (Utiyama and Isahara, 2007), the Chinese-English parallel patent corpus (Lu et al., 2010), and the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation1. However, few parallel corpora exist for language pairs among other languages (e.g. French-Chinese, German-Chinese, Japanese-Arabic or Chinese-Japanese). This is especially so for some domain-specific areas, such as patents, whose use of language embraces both legal and legalistic as well as technical considerations, thus placing limits on the useful application o"
2011.tc-1.9,2011.mtsummit-papers.54,1,0.795033,"n the Japanese-to-English patent translation task, the commercial RBMT systems still show higher adequacies than the state-of-the-art SMT systems.  On the English-to-Japanese translation task, some SMT systems achieve equal or better human evaluation scores (adequacy) than the top-level commercial RBMT systems. No SMT system did this at NTCIR-7, and this is thought to be the first time that this was achieved. 4. Building Parallel Patent Corpora using English as the Pivot We have cultivated a trilingual parallel corpus by means of bilingual parallel corpora with English as the pivot (see also Lu et al., (2011)). With the 14 million Chinese-English bilingual sentences introduced in Section 3.2, and 4.2 million Japanese-English bilingual sentences we have in our center, a trilingual sentence-aligned patent corpus has been cultivated. Specifically, we align Chinese-English and English-Japanese sentence pairs by using the English sentences as the pivot, and finally obtain Chinese-English- Japanese sentence triplets. The selectivity in the whole process of this resource building is shown in Table 3. CH-EN JP-EN Number of Sentences / Sentence Pairs RAW Filter 1 Filter 2 Filter 3 56.1M(CH) 45.1M 31.5M 14."
2011.tc-1.9,W10-4110,1,0.848739,"arallel linguistic structures, which is of critical importance to the cultivation of high quality TM’s. The amount of useful bilingual corpora varies considerably between language pairs, and consequently the amount of useful TM's varies considerably among language pairs with English as a frequent common member in the paired TM's. For example, many parallel corpora with English as one language have been built, such as the French-English Canadian Hansards (Gale and Church, 1991), the Japanese-English parallel patent corpus (Utiyama and Isahara, 2007), the Chinese-English parallel patent corpus (Lu et al., 2010), and the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation1. However, few parallel corpora exist for language pairs among other languages (e.g. French-Chinese, German-Chinese, Japanese-Arabic or Chinese-Japanese). This is especially so for some domain-specific areas, such as patents, whose use of language embraces both legal and legalistic as well as technical considerations, thus placing limits on the useful application of current MT techniques to meet the needs in the commercial and other sectors. This paper introduces two major areas of development to"
2011.tc-1.9,2007.mtsummit-papers.63,0,0.284921,"is the need to draw from quality texts the maximum amount of relevant parallel linguistic structures, which is of critical importance to the cultivation of high quality TM’s. The amount of useful bilingual corpora varies considerably between language pairs, and consequently the amount of useful TM's varies considerably among language pairs with English as a frequent common member in the paired TM's. For example, many parallel corpora with English as one language have been built, such as the French-English Canadian Hansards (Gale and Church, 1991), the Japanese-English parallel patent corpus (Utiyama and Isahara, 2007), the Chinese-English parallel patent corpus (Lu et al., 2010), and the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation1. However, few parallel corpora exist for language pairs among other languages (e.g. French-Chinese, German-Chinese, Japanese-Arabic or Chinese-Japanese). This is especially so for some domain-specific areas, such as patents, whose use of language embraces both legal and legalistic as well as technical considerations, thus placing limits on the useful application of current MT techniques to meet the needs in the commercial and other sec"
2011.tc-1.9,P07-1108,0,0.0265256,"own to be promising. Second, we discuss the rapidly increasing demands in the field of patent translation and various efforts to bootstrap patent machine translation in uncommon language pairs (e.g., Japanese and Chinese) via more common language pairs (e.g., Chinese-English and English-Japanese), and the application of the pivot approach to expedite processing. 2. Pivoting Approaches for Machine Translation There are three major approaches introduced below. Suppose the three languages involved are source, pivot and target. The first is based on phrase table translation (Cohn and Lapata 2007; Wu and Wang, 2007). The approach usually first train two translation models: source-pivot and pivot-target; then it induces a new source-target phrase table by using the translation probabilities and lexical weights in source-pivot and pivot-target translation models For example, to translate between Chinese and Japanese, we can first train Chinese-English and English-Japanese translation models based on available bilingual corpora; the two models are then combined together at the phrase level to provide a new Chinese-Japanese phrase translation table with induced translated probability for each new entry. The"
2011.tc-1.9,J93-1004,0,\N,Missing
2011.tc-1.9,2008.iwslt-evaluation.17,0,\N,Missing
2020.coling-main.309,N15-3022,0,0.0172162,"as pedagogical tools for translation in the technical domain. In a user study, novice translators revised Chinese translations of English patents through bilingual concordancing. Results show that concordancing with an in-domain bilingual corpus can yield greater improvement in translation quality of technical terms than a general-domain bilingual corpus. 1 Introduction Text corpora are increasingly used in language pedagogy, with many studies showing their effectiveness in data-driven language learning (Boulton, 2017), language exercise generation (Susanti et al., 2018) and assisted writing (Chang and Chang, 2015), among many other tasks. Bilingual corpora, however, remain underused in translation pedagogy. Most research has focused on exploiting bilingual data for training machine translation (MT) systems and for developing translation memory (TM) in computer-assisted translation tools. Professional translators generally favor dictionaries and search engines over corpora (Man et al., 2020). While these tools may be sufficient when translators work in a familiar domain, more domainspecific examples are often needed to handle unfamiliar terms and collocations in specialized areas. Complementing MT and T"
2020.coling-main.309,Y12-1012,0,0.0216573,"rough Sketch Engine (Kilgarriff et al., 2008). As a general-domain corpus, OPUS2 has more limited coverage of technical terms, and their translation may not necessarily reflect the terminology adopted in patents. In-domain version The student produced a final version via parallel concordancing with in-domain examples in PatentLex (Section 4). The search interface for PatentLex supported bilingual keyword search in the KWIC format.1 5.3 Translation quality assessment We evaluated the quality of the above translation versions both automatically and manually. In automatic evaluation, we followed Li et al. (2012) in using the BLEU score (Papineni et al., 2002). Manual evaluation focused on “technical terms”, broadly defined as noun phrases that bear a scientific or technical concept. Three human judges, all native speakers of Chinese with a bachelor’s or master’s degree in linguistics or language studies, performed the evaluation. Gold data. One of the judges identified all technical terms that appeared in the ten English abstracts, and then aligned them to their counterpart in the gold Chinese translations. After review by the other two judges, the list consisted of a total of 200 technical terms, am"
2020.coling-main.309,2009.mtsummit-wpt.3,1,0.799414,"Missing"
2020.coling-main.309,P02-1040,0,0.112979,"08). As a general-domain corpus, OPUS2 has more limited coverage of technical terms, and their translation may not necessarily reflect the terminology adopted in patents. In-domain version The student produced a final version via parallel concordancing with in-domain examples in PatentLex (Section 4). The search interface for PatentLex supported bilingual keyword search in the KWIC format.1 5.3 Translation quality assessment We evaluated the quality of the above translation versions both automatically and manually. In automatic evaluation, we followed Li et al. (2012) in using the BLEU score (Papineni et al., 2002). Manual evaluation focused on “technical terms”, broadly defined as noun phrases that bear a scientific or technical concept. Three human judges, all native speakers of Chinese with a bachelor’s or master’s degree in linguistics or language studies, performed the evaluation. Gold data. One of the judges identified all technical terms that appeared in the ten English abstracts, and then aligned them to their counterpart in the gold Chinese translations. After review by the other two judges, the list consisted of a total of 200 technical terms, amounting to an average of 20 terms per abstract."
2020.coling-main.309,W19-8714,1,0.697915,", 2009), a large-scale collection of comparable Chinese-English patents which will be used in our study. 4 Data General-domain corpus OPUS2, a large collection of freely available multilingual data (Tiedemann, 2009), served as the general-domain parallel corpus in our study. It includes text from political and administrative sources, user-provided movie subtitles, as well as software localisation, news providers, translated descriptions of medical products, religious texts and multilingual wikis and other websites. In-domain corpus Composed of over 300K Chinese and English patents, PatentLex (Tsou et al., 2019) has served as the dataset for patent MT shared tasks such as those organized by NTCIR workshops. Our in-domain bilingual data consisted of over 30 million parallel bilingual sentences extracted from PatentLex. 5 5.1 Evaluation Set-up Subjects Our subjects were 31 students in a postgraduate course in Linguistics at City University of Hong Kong. All students were native speakers of Chinese who were proficient in English, having met the university’s admissions requirement for IELTS. Our study focused on English-to-Chinese translation. This translation direction avoids L2-related errors in the ta"
2020.paclic-1.35,Y96-1011,0,0.545947,"(congshi 從事 “engage”, zuo 做 “do”, guo 搞 “make”, gan 幹 “do”, nong 弄 “make”, jia 加 “add” and yu 予 “provide”). His concern was with their grammatical usage in Mainland China. With data drawn from the Sinica Corpus, Wang (2004) adopted a corpus-based approach to study differences in the usage of three light verbs (zuo 做 “do”, guo 搞 “make”, nong 弄 “make”) in Taiwan. In 2014, Lin et al. and Huang et al. initiated comparison between light verb usage 1 Light verbs are also widely found in other East and Southeast Asian languages, e.g. suru “do” in Japanese (Grimshaw & Mester 1988), ha “do” in Korean (Chae 1996), lam “do” in Vietnamese (Pham 1999), etc. variations in Mainland and Taiwan Mandarin. They used data from the Annotated Chinese Gigaword Corpus which combines data from both Mainland and Taiwan in 1990-2002 to discuss the differential distribution of five light verbs (jinxing 進行 “proceed”, congshi 從事 “engage”, zuo 做 “do”, guo 搞 “make”, jiayi 加 以 “add”). They pointed out that 進行 jinxing in Taiwan might take verb-object phrases as complements (e.g jinxing toupiao 進行投票 “proceed to voting, (lit.) castticket”), but not in Mainland. The general conclusion as reported in Jiang et al. (2016) was that"
2020.paclic-1.35,W14-5301,0,0.0398295,"Missing"
2020.paclic-1.35,Y16-3018,0,0.0734399,"a “do” in Korean (Chae 1996), lam “do” in Vietnamese (Pham 1999), etc. variations in Mainland and Taiwan Mandarin. They used data from the Annotated Chinese Gigaword Corpus which combines data from both Mainland and Taiwan in 1990-2002 to discuss the differential distribution of five light verbs (jinxing 進行 “proceed”, congshi 從事 “engage”, zuo 做 “do”, guo 搞 “make”, jiayi 加 以 “add”). They pointed out that 進行 jinxing in Taiwan might take verb-object phrases as complements (e.g jinxing toupiao 進行投票 “proceed to voting, (lit.) castticket”), but not in Mainland. The general conclusion as reported in Jiang et al. (2016) was that light verbs in Taiwan were “more transitive” (i.e. more verbal) and less grammaticalized.2 It is interesting that one light verb stands out for escaping the attention of most linguists. This is da 打 “hit”, which had surprisingly interested Ouyang Xiu (1007-1072). 3 Most modern studies have focused on da’s diachronic development (J. Zhu 2004, Su 2009, Zhuang 2014 i.a.) but seldom on its light verb usage in Modern Chinese. Wang (1985) simply called it “a marker for verbs”. Ren (2013) studied how da was interpreted specifically in different grammaticalized contexts in Grounding Theory."
2020.paclic-1.35,W14-5810,0,0.0369701,"Missing"
C02-1055,J98-1001,0,\N,Missing
C02-1055,C92-2070,0,\N,Missing
C02-1055,A97-1018,1,\N,Missing
C04-1145,P97-1023,0,0.0837828,"Missing"
C04-1145,C00-1044,0,0.139086,"Missing"
C04-1145,P02-1053,0,0.148552,"Missing"
C04-1145,W01-1626,0,0.015359,"Missing"
C08-1058,P99-1016,0,0.0941831,"Missing"
C08-1058,W02-0903,0,0.0512252,"Missing"
C08-1058,C92-2082,0,0.0612395,"Missing"
C08-1058,D07-1034,1,0.124581,"ant part of the lexical knowledge, which will be useful and critical for many NLP applications, including natural language understanding, information retrieval, and machine translation. Tsou and Kwong (2006) proposed a comprehensive Pan-Chinese lexical resource, using a large and unique synchronous Chinese corpus as an authentic source of lexical variation among various Chinese speech communities. They also studied the feasibility of taking an existing Chinese thesaurus as leverage and classifying new words from various Chinese communities with respect to the classificatory structure therein (Kwong and Tsou, 2007). They used the catego1 The transcriptions in brackets are based on Hanyu Pinyin. 457 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 457–464 Manchester, August 2008 ries at the subclass level of the Tongyici Cilin (同 義詞詞林, abbreviated as Cilin hereafter) for the task. The classification was done by comparing the similarity of a target word (i.e. the word to be classified) and individual categories of words in the thesaurus based on a feature vector of cooccurring words in a corpus. Since words in the thesaurus are mostly based on lexical item"
C08-1058,W04-2103,0,0.0329367,"Missing"
C08-1058,W97-0313,0,0.124893,"Missing"
C08-1058,W02-1028,0,0.060491,"Missing"
C08-1058,W97-0803,0,0.0327694,"Missing"
C08-1058,P03-2011,0,0.0447363,"Missing"
C08-1058,tsou-kwong-2006-toward,1,0.850149,"y used as 居 屋 to mean general housing in Mainland China, it is rarely seen in the Hong Kong context; and 下崗 (xia4gang3) is specific, if not exclusive, to Mainland China for referring to a special concept of unemployment. Existing Chinese lexical resources are often based on language use in one particular region and are therefore not comprehensive enough to capture the substantial regional variation as an important part of the lexical knowledge, which will be useful and critical for many NLP applications, including natural language understanding, information retrieval, and machine translation. Tsou and Kwong (2006) proposed a comprehensive Pan-Chinese lexical resource, using a large and unique synchronous Chinese corpus as an authentic source of lexical variation among various Chinese speech communities. They also studied the feasibility of taking an existing Chinese thesaurus as leverage and classifying new words from various Chinese communities with respect to the classificatory structure therein (Kwong and Tsou, 2007). They used the catego1 The transcriptions in brackets are based on Hanyu Pinyin. 457 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 4"
C08-1058,P98-2127,0,0.19061,"Missing"
C08-1058,C98-2122,0,\N,Missing
C08-1058,W06-0101,0,\N,Missing
C08-1143,J96-1002,0,0.044465,"Missing"
C08-1143,P94-1020,0,0.0184848,"Missing"
C08-1143,P07-1007,0,0.0408627,"Missing"
C08-1143,W02-1006,0,0.0118415,"Missing"
C08-1143,N06-1016,0,\N,Missing
C08-1143,D07-1082,1,\N,Missing
C08-1143,P04-1075,0,\N,Missing
C08-1143,P96-1006,0,\N,Missing
C08-1143,P00-1016,0,\N,Missing
C08-1143,P02-1016,0,\N,Missing
C08-1143,I08-1048,1,\N,Missing
C92-3162,O91-1007,1,0.228458,"Missing"
C98-2201,C92-1019,0,0.0122325,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for an), Chinese infonnation processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknom~ words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 roles)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent mmotated corpus etc.) is very hard due to particularity of Chinese, and time consuming Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in designi"
C98-2201,P94-1010,0,0.0369469,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for an), Chinese infonnation processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknom~ words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 roles)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent mmotated corpus etc.) is very hard due to particularity of Chinese, and time consuming Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in designi"
C98-2201,P97-1041,0,0.038462,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for an), Chinese infonnation processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknom~ words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 roles)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent mmotated corpus etc.) is very hard due to particularity of Chinese, and time consuming Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in designi"
C98-2201,A97-1018,1,0.727926,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for an), Chinese infonnation processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknom~ words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 roles)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent mmotated corpus etc.) is very hard due to particularity of Chinese, and time consuming Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in designi"
C98-2201,J96-3004,0,\N,Missing
D07-1034,P99-1016,0,0.216167,"words within a semantic hierarchy, and to group similar words together into a class. Previous work on automatic methods for 326 building semantic lexicons could be divided into two main groups. One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Thelen and Riloff, 2002). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. Caraballo (1999) selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect"
D07-1034,W02-0903,0,0.530291,"traction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet. Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features. For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cilin; but somehow the test data were taken from Cilin, and therefore could not really demonstrate the effectiveness with unknown words found in the Sinica corpus. The current work attempts to classify new words with an existing thesaural"
D07-1034,W02-0908,0,0.0178704,"matic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Thelen and Riloff, 2002). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. Caraballo (1999) selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntact"
D07-1034,C92-2082,0,0.0702386,"asure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet. Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features. For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cilin; but somehow the test data were taken from Cilin, and therefo"
D07-1034,P98-2127,0,0.176498,"esented and discussed with future directions in Section 5, followed by a conclusion in Section 6. 2 Related Work To build a semantic lexicon, one has to identify the relation between words within a semantic hierarchy, and to group similar words together into a class. Previous work on automatic methods for 326 building semantic lexicons could be divided into two main groups. One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Thelen and Riloff, 2002). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. Caraballo (1999) selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a th"
D07-1034,W04-2103,0,0.447163,"n noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet. Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features. For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cilin; but someho"
D07-1034,W97-0313,0,0.243638,"ls used and the experimental setup respectively. Results will be presented and discussed with future directions in Section 5, followed by a conclusion in Section 6. 2 Related Work To build a semantic lexicon, one has to identify the relation between words within a semantic hierarchy, and to group similar words together into a class. Previous work on automatic methods for 326 building semantic lexicons could be divided into two main groups. One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Thelen and Riloff, 2002). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. Caraballo (1999) selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2"
D07-1034,W02-1028,0,0.170847,"l setup respectively. Results will be presented and discussed with future directions in Section 5, followed by a conclusion in Section 6. 2 Related Work To build a semantic lexicon, one has to identify the relation between words within a semantic hierarchy, and to group similar words together into a class. Previous work on automatic methods for 326 building semantic lexicons could be divided into two main groups. One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Thelen and Riloff, 2002). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. Caraballo (1999) selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature cluste"
D07-1034,W97-0803,0,0.49039,"tions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet. Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features. For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cil"
D07-1034,P03-2011,0,0.29255,"e of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet. Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features. For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cilin; but somehow the test data were taken from Cilin, and therefore could not really demonstrate the effectiveness with unknown words found in the Sinica corpus. The current work attempts to classify new words with an existing thesaural classificatory structure. However, the usual practice in past studies is to test with a portion of data from the thesaurus itself and evaluate the results against the original classific"
D07-1034,tsou-kwong-2006-toward,1,0.486994,"truction. 1 Introduction Large-scale semantic lexicons are important resources for many natural language processing (NLP) tasks. For a significant world language such as Chinese, it is especially critical to capture the substantial regional variation as an important part of the lexical knowledge, which will be useful for many NLP applications, including natural language understanding, information retrieval, and machine translation. Existing Chinese lexical resources, however, are often based on language use in one particular region and thus lack the desired comprehensiveness. Toward this end, Tsou and Kwong (2006) proposed a comprehensive Pan-Chinese lexical resource, based on a large and unique synchronous Chinese corpus as an authentic source for lexical acquisition and analysis across various Chinese speech communities. To allow maximum versatility and portability, it is expected to document the core and universal substances of the language on the one hand, and also the more subtle variations found in different communities on the other. Different Chinese speech communities might share lexical items in the same form but with different meanings. For instance, the word 居屋 refers to general housing in M"
D07-1034,C98-2122,0,\N,Missing
D07-1034,W06-0101,0,\N,Missing
I05-1026,P94-1002,0,0.544454,"to divide a document into topically-coherent sections, each corresponding to a relevant subject. Linear text segmentation has been applied in document summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, th"
I05-1026,J97-1003,0,0.784031,"to divide a document into topically-coherent sections, each corresponding to a relevant subject. Linear text segmentation has been applied in document summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, th"
I05-1026,J91-1002,0,0.149373,"to divide a document into topically-coherent sections, each corresponding to a relevant subject. Linear text segmentation has been applied in document summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, th"
I05-1026,P93-1041,0,0.0605184,"to divide a document into topically-coherent sections, each corresponding to a relevant subject. Linear text segmentation has been applied in document summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, th"
I05-1026,P94-1050,0,0.0359194,"ument summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. Some efforts have"
I05-1026,W97-0304,0,0.375905,"ument summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. Some efforts have"
I05-1026,P93-1020,0,0.0555297,"to divide a document into topically-coherent sections, each corresponding to a relevant subject. Linear text segmentation has been applied in document summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, th"
I05-1026,P99-1046,0,0.0207538,"ument summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. Some efforts have"
I05-1026,A00-2004,0,0.0285431,"ument summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. Some efforts have"
I05-1026,W01-0514,0,0.0336087,"ument summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. Some efforts have"
I05-1026,P98-2244,0,0.026931,"efforts have focused on using similarity between adjacent parts of a text to solve topic boundary detection. In fact, the similarity threshold is very hard to set, and it is very difficult to identify exactly topic boundaries only according to similarity between adjacent parts of a text. Other works have focused on the similarity between all parts of a text. Reynar[7] and Choi[13] used dotplots technique to perform linear text segmentation which can be seen as a form of approximate and local optimization. Yaari[16] has used agglomerative clustering to perform hierarchical segmentation. Others[10,17,18,19] used dynamic programming to perform exact and global optimization in which some prior parameters are needed. These parameters can be obtained via uninformative prior probabilities[18], or estimated from training data[19]. In this paper, we propose a new statistical model for linear text segmentation, which uses Multiple Discriminant Analysis (MDA) method to define a global criterion function for document segmentation. Our method focuses on within-segment word similarity and between-segment word similarity. This process can achieve global optimization in addressing the two aforementioned probl"
I05-1026,P01-1064,0,0.0335318,"efforts have focused on using similarity between adjacent parts of a text to solve topic boundary detection. In fact, the similarity threshold is very hard to set, and it is very difficult to identify exactly topic boundaries only according to similarity between adjacent parts of a text. Other works have focused on the similarity between all parts of a text. Reynar[7] and Choi[13] used dotplots technique to perform linear text segmentation which can be seen as a form of approximate and local optimization. Yaari[16] has used agglomerative clustering to perform hierarchical segmentation. Others[10,17,18,19] used dynamic programming to perform exact and global optimization in which some prior parameters are needed. These parameters can be obtained via uninformative prior probabilities[18], or estimated from training data[19]. In this paper, we propose a new statistical model for linear text segmentation, which uses Multiple Discriminant Analysis (MDA) method to define a global criterion function for document segmentation. Our method focuses on within-segment word similarity and between-segment word similarity. This process can achieve global optimization in addressing the two aforementioned probl"
I05-1026,E03-1058,0,0.0954399,"efforts have focused on using similarity between adjacent parts of a text to solve topic boundary detection. In fact, the similarity threshold is very hard to set, and it is very difficult to identify exactly topic boundaries only according to similarity between adjacent parts of a text. Other works have focused on the similarity between all parts of a text. Reynar[7] and Choi[13] used dotplots technique to perform linear text segmentation which can be seen as a form of approximate and local optimization. Yaari[16] has used agglomerative clustering to perform hierarchical segmentation. Others[10,17,18,19] used dynamic programming to perform exact and global optimization in which some prior parameters are needed. These parameters can be obtained via uninformative prior probabilities[18], or estimated from training data[19]. In this paper, we propose a new statistical model for linear text segmentation, which uses Multiple Discriminant Analysis (MDA) method to define a global criterion function for document segmentation. Our method focuses on within-segment word similarity and between-segment word similarity. This process can achieve global optimization in addressing the two aforementioned probl"
I05-1026,H92-1089,0,\N,Missing
I05-1026,C98-2239,0,\N,Missing
I05-1070,W04-2413,0,0.0467895,"Missing"
I05-1070,P98-1013,0,0.00485599,"to be labelled. We will also compare the results on two training and testing datasets. In Section 2, related work will be reviewed. In Section 3, the data used in the current study will be introduced. Our proposed method will be explained in Section 4, and the experiment reported in Section 5. Results and future work will be discussed in Section 6, followed by conclusions in Section 7. 2 Related Work The definition of semantic roles falls on a continuum from abstract ones to very specific ones. Gildea and Jurafsky [6], for instance, used a set of roles defined according to the FrameNet model [2], thus corresponding to the frame elements in individual frames under a particular domain to which a given verb belongs. Lexical entries (in fact not limited to verbs, in the case of FrameNet) falling under the same frame will share the same set of roles. Gildea and Palmer [7] defined roles with respect to individual predicates in the PropBank, without explicit naming. To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems. The theoretical treatment of semantic roles is also varied in Chinese. In practice, for example, the semantic role"
I05-1070,W04-2412,0,0.0211378,"is study we only make an effort to eliminate multiple tagging of the same role to the same target verb in a sentence on either side of the target verb, but not if they appear on both sides of the target verb. This should certainly be dealt with in future experiments. The differential degradation of performance between textbook data and news data also suggests the varied importance of constituent boundaries to simple sentences and complex ones, and hence possibly their varied requirements for full parse information for the semantic labelling task. 6 Discussion According to Carreras and Màrquez [3], the state-of-the-art results for semantic role labelling systems based on shallow syntactic information is about 15 lower than those with access to gold standard parse trees, i.e., around 60. Our experimental results for the headword location condition, with no syntactic information available 812 O.Y. Kwong and B.K. Tsou at all, give an F1 score of 52.89 and 44.35 respectively for textbook data and news data. This further degradation in performance is nevertheless within expectation, but whether this is also a result of the difference between English and Chinese remains to be seen. In respon"
I05-1070,J02-3001,0,0.710058,"ces with this kind of information would in turn facilitate the development of such applications. Large-scale production of annotated resources is often labour-intensive, and thus needs automatic labelling to streamline the work. The task can essentially be perceived as a two-phase process, namely to recognise the constituents bearing some semantic relationship to the target verb in a sentence, and then to label them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky approached the task using various features such as headword, phrase type, and parse tree path [6]. Such features have remained the basic and essential features in subsequent research, irrespective of the variation in the actual learning components. In addition, parsed sentences are often required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling [7, 8]. More precisely, in semantic role labelling, parse information is rather more critical for the identification of boundaries for candidate constituents than for the extraction R. Dale et al. (Eds.): IJCNLP 200"
I05-1070,P02-1031,0,0.0847852,"entence, and then to label them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky approached the task using various features such as headword, phrase type, and parse tree path [6]. Such features have remained the basic and essential features in subsequent research, irrespective of the variation in the actual learning components. In addition, parsed sentences are often required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling [7, 8]. More precisely, in semantic role labelling, parse information is rather more critical for the identification of boundaries for candidate constituents than for the extraction R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 804 – 814, 2005. © Springer-Verlag Berlin Heidelberg 2005 Semantic Role Tagging for Chinese at the Lexical Level 805 of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. [21]). However, given the imperfection of existing automatic parsers, which are far from producing gold standard parses, many thus resort to s"
I05-1070,W03-1008,0,0.0140963,"entence, and then to label them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky approached the task using various features such as headword, phrase type, and parse tree path [6]. Such features have remained the basic and essential features in subsequent research, irrespective of the variation in the actual learning components. In addition, parsed sentences are often required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling [7, 8]. More precisely, in semantic role labelling, parse information is rather more critical for the identification of boundaries for candidate constituents than for the extraction R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 804 – 814, 2005. © Springer-Verlag Berlin Heidelberg 2005 Semantic Role Tagging for Chinese at the Lexical Level 805 of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. [21]). However, given the imperfection of existing automatic parsers, which are far from producing gold standard parses, many thus resort to s"
I05-1070,W04-2416,0,0.0495149,"Missing"
I05-1070,W04-2417,0,0.0456926,"Missing"
I05-1070,kingsbury-palmer-2002-treebank,0,0.174303,"ent boundaries are known, while parse information might be more important for complicated sentences than simple ones. Several ways to improve the headword identification results were suggested, and we also plan to explore some class-based techniques for the task, with reference to existing semantic lexicons. 1 Introduction As the development of language resources progresses from POS-tagged corpora to syntactically annotated treebanks, the inclusion of semantic information such as predicate-argument relations is becoming indispensable. The expansion of the Penn Treebank into a Proposition Bank [11] is a typical move in this direction. Lexical resources also need to be enhanced with semantic information (e.g. [5]). In fact the ability to identify semantic role relations correctly is essential to many applications such as information extraction and machine translation; and making available resources with this kind of information would in turn facilitate the development of such applications. Large-scale production of annotated resources is often labour-intensive, and thus needs automatic labelling to streamline the work. The task can essentially be perceived as a two-phase process, namely"
I05-1070,W04-0832,0,0.0515194,"Missing"
I05-1070,E03-1081,1,0.773653,"rimary school Chinese textbooks popularly used in Hong Kong were taken for reference. The two publishers were Keys Press [22] and Modern Education Research Society Ltd [23]. Texts for Primary One to Six were digitised, segmented into words, and annotated with parts-of-speech (POS). The two sets of textbooks amount to a text collection of about 165K character tokens and upon segmentation about 109K word tokens (about 15K word types). There were about 2,500 transitive verb types, with frequency ranging from 1 to 926. The complex examples were taken from a subset of the LIVAC synchronous corpus1 [13, 18]. The subcorpus consists of newspaper texts from Hong Kong, including local news, international news, financial news, sports news, and entertainment news, collected in 1997-98. The texts were segmented into words and POS-tagged, amounting to about 1.8M character tokens and upon segmentation about 1M word tokens (about 47K word types). There were about 7,400 transitive verb types, with frequency ranging from 1 to just over 6,300. 3.2 Training and Testing Data For the current study, a set of 41 transitive verbs common to the two corpora (hereafter referred to as textbook corpus and news corpus),"
I05-1070,W04-0803,0,0.0616379,"Missing"
I05-1070,W04-0841,0,0.0620925,"Missing"
I05-1070,W04-2422,0,0.0387254,"Missing"
I05-1070,W04-3212,0,0.0185973,"overall performance. Another area of interest is to look at the behaviour of near-synonymous predicates in the tagging process. Many predicates may be unseen in the training data, but while the probability estimation could be generalized from near-synonyms as suggested by a semantic lexicon, whether the similarity and subtle differences between nearsynonyms with respect to the argument structure and the corresponding syntactic realisation could be distinguished would also be worth studying. Related to this is the possibility of augmenting the feature set with semantic features. Xue and Palmer [20], for instance, looked into new features such as syntactic frame, lexicalized constituent type, etc., and found that enriching the feature set improved the labelling performance. Another direction of future work is on the location of constituent boundaries upon the identification of the headword. As mentioned earlier on, this could probably be tackled by some finite state techniques or with the help of simple chunkers. 詞林 同義詞 Semantic Role Tagging for Chinese at the Lexical Level 813 7 Conclusion The study reported in this paper has thus tackled the unknown constituent boundary condition in se"
I05-1070,W04-1116,0,0.0122922,"g and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling [7, 8]. More precisely, in semantic role labelling, parse information is rather more critical for the identification of boundaries for candidate constituents than for the extraction R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 804 – 814, 2005. © Springer-Verlag Berlin Heidelberg 2005 Semantic Role Tagging for Chinese at the Lexical Level 805 of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. [21]). However, given the imperfection of existing automatic parsers, which are far from producing gold standard parses, many thus resort to shallow syntactic information from simple chunking, though results often turn out to be less satisfactory than with full parses. This limitation is even more pertinent for the application of semantic role labelling to languages which do not have sophisticated parsing resources. In the case of Chinese, for example, there is considerable variability in its syntax-semantics interface; and when one has more nested and complex sentences such as those from news art"
I05-1070,W04-3213,0,\N,Missing
I05-1070,C04-1179,0,\N,Missing
I05-1070,C98-1013,0,\N,Missing
I05-1070,W05-0620,0,\N,Missing
O05-5009,Y00-1012,0,0.0213692,"Missing"
O05-5009,Y03-1022,1,0.858808,"Missing"
O05-5009,O00-2002,0,\N,Missing
O98-3006,J81-2001,0,\N,Missing
O98-3006,C94-1056,0,\N,Missing
O98-3006,C96-2164,0,\N,Missing
O98-3006,O91-1007,1,\N,Missing
O98-3006,P92-1030,0,\N,Missing
P11-1033,C10-1004,0,0.326788,"Missing"
P11-1033,D08-1014,0,0.325093,"which require training data annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the lang"
P11-1033,J96-1002,0,0.0285566,"Missing"
P11-1033,N06-1014,0,0.0413855,"and . However, there could be considerable noise in real-world parallel data, i.e. the sentence pairs may be noisily parallel (or even comparable) instead of fully parallel (Munteanu and Marcu, 2005). In such noisy cases, the labels (positive or negative) could be different for the two monolingual sentences in a sentence pair. Although we do not know the exact probability that a sentence pair exhibits the same label, we can approximate it using their translation probabilities, which can be computed using word alignment toolkits such as Giza++ (Och and Ney, 2003) or the Berkeley word aligner (Liang et al., 2006). The intuition here is that if the translation probability of two sentences is high, the probability that they have the same sentiment label should be high as well. Therefore, by considering the noise in parallel data, we get: (5) just the labeled data. Then, in the E-step, the classifiers, based on current values of and , compute for each labeled example and assign probabilistically-weighted class labels to each unlabeled example. Next, in the M-step, the parameters, and , are updated using both the original labeled data ( and ) and the newly labeled data . These last two steps are iterated"
P11-1033,W02-2018,0,0.023691,"t as s, and as (6) where the first term on the right-hand side is the log likelihood of the labeled data from both and the second is the log likelihood of the unlabeled parallel data , multiplied by , a constant that controls the contribution of the unlabeled data; and is a regularization constant that penalizes model complexity or large 3. feature weights. When is 0, the algorithm ignores the unlabeled data and degenerates to two In the M-step, we can optimize the regularized MaxEnt models trained on only the labeled data. joint log likelihood using any gradient-based optimization technique (Malouf, 2002). The 3.3 The EM Algorithm on MaxEnt gradient for Equation 3 based on Equation 4 is To solve the optimization problem for the model, shown in Appendix A; those for Equations 5 and 6 we need to jointly estimate the optimal parameters can be derived similarly. In our experiments, we for the two monolingual classifiers by finding: use the L-BFGS algorithm (Liu et al., 1989) and run EM until the change in regularized joint log (7) likelihood is less than 1e-5 or we reach 100 This can be done with an EM algorithm, whose iterations.3 steps are summarized in Algorithm 1. First, the MaxEnt parameters,"
P11-1033,W06-1615,0,0.0215572,"rom one language (usually English) to other languages with few sentiment resources. Mihalcea et al. (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. Banea et 321 al. (2008; 2010) instead automatically translate the English resources using automatic machine translation engines for subjectivity classification. Prettenhofer and Stein (2010) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al., 2006). Approaches that do not explicitly involve resource adaptation include Wan (2009), which uses co-training (Blum and Mitchell, 1998) with English vs. Chinese features comprising the two independent ―views‖ to exploit unlabeled Chinese data and a labeled English corpus and thereby improves Chinese sentiment classification. Another notable approach is the work of BoydGraber and Resnik (2010), which presents a generative model --- supervised multilingual latent Dirichlet allocation --- that jointly models topics that are consistent across languages, and employs them to better predict sentiment ra"
P11-1033,P07-1123,0,0.847904,"sed learning techniques, which require training data annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is av"
P11-1033,J05-4003,0,0.0121558,"ide is the likelihood of labeled data for both and ; and the second term is the likelihood of the unlabeled parallel data . If we assume that parallel sentences are perfect translations, the two sentences in each pair should have the same polarity label, which gives us: (4) where is the unobserved class label for the -th instance in the unlabeled data. This probability directly models the sentiment label agreement between and . However, there could be considerable noise in real-world parallel data, i.e. the sentence pairs may be noisily parallel (or even comparable) instead of fully parallel (Munteanu and Marcu, 2005). In such noisy cases, the labels (positive or negative) could be different for the two monolingual sentences in a sentence pair. Although we do not know the exact probability that a sentence pair exhibits the same label, we can approximate it using their translation probabilities, which can be computed using word alignment toolkits such as Giza++ (Och and Ney, 2003) or the Berkeley word aligner (Liang et al., 2006). The intuition here is that if the translation probability of two sentences is high, the probability that they have the same sentiment label should be high as well. Therefore, by c"
P11-1033,D10-1005,0,0.168458,"Missing"
P11-1033,W10-2906,0,0.0120873,"hods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1"
P11-1033,D08-1092,0,0.032352,"g, 2009). Among the popular semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistic"
P11-1033,D08-1083,1,0.0862197,"Missing"
P11-1033,P09-1121,0,0.0337052,"0), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1 have been widely used in many NLP tasks (Be"
P11-1033,J94-4003,0,0.0600034,"Missing"
P11-1033,N10-1120,0,0.0150772,"ly for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under study, and (2) investigate methods to simultaneously improve sentiment classification for both languages. Given the labeled data in each language, we propose an approach that exploits an unlabeled parallel corpu"
P11-1033,J03-1002,0,0.00233983,"ctly models the sentiment label agreement between and . However, there could be considerable noise in real-world parallel data, i.e. the sentence pairs may be noisily parallel (or even comparable) instead of fully parallel (Munteanu and Marcu, 2005). In such noisy cases, the labels (positive or negative) could be different for the two monolingual sentences in a sentence pair. Although we do not know the exact probability that a sentence pair exhibits the same label, we can approximate it using their translation probabilities, which can be computed using word alignment toolkits such as Giza++ (Och and Ney, 2003) or the Berkeley word aligner (Liang et al., 2006). The intuition here is that if the translation probability of two sentences is high, the probability that they have the same sentiment label should be high as well. Therefore, by considering the noise in parallel data, we get: (5) just the labeled data. Then, in the E-step, the classifiers, based on current values of and , compute for each labeled example and assign probabilistically-weighted class labels to each unlabeled example. Next, in the M-step, the parameters, and , are updated using both the original labeled data ( and ) and the newly"
P11-1033,W02-1011,0,0.0339296,"re bootstrapped by adding the most confident predicted examples from the unlabeled data into the training set. We run bootstrapping for 100 iterations. In each iteration, we select the most confidently predicted 50 positive and 50 negative sentences from each of the two classifiers, and take the union of the resulting 200 sentence pairs as the newly labeled training data. (Examples with conflicting labels within the pair are not included.) 5-fold cross-validation and report average accuracy (also MicroF1 in this case) and MacroF1 scores. Unigrams are used as binary features for all models, as Pang et al. (2002) showed that binary features perform better than frequency features for sentiment classification. The weights for unlabeled data and regularization, and , are set to 1 unless otherwise stated. Later, we will show that the proposed approach performs well with a wide range of parameter values.7 5.1 Method Comparison In our experiments, the methods are tested in the two data settings with the corresponding unlabeled parallel corpus as mentioned in Section 4.6 We use We first compare the proposed joint model (Joint) with the baselines in Table 2. As seen from the table, the proposed approach outpe"
P11-1033,P10-1114,0,0.355153,"he appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under study, and (2) investigate methods to si"
P11-1033,schulz-etal-2010-multilingual,0,0.0211262,"er consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under study, and (2) investigate methods to simultaneously improve sentiment classification for both languages. Given the labeled data in each language, we propose an approach that exploits an unlabeled parallel corpus with the following 3"
P11-1033,D08-1058,0,0.0612854,"a annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under st"
P11-1033,P09-1027,0,0.81898,"et al. (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. Banea et 321 al. (2008; 2010) instead automatically translate the English resources using automatic machine translation engines for subjectivity classification. Prettenhofer and Stein (2010) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al., 2006). Approaches that do not explicitly involve resource adaptation include Wan (2009), which uses co-training (Blum and Mitchell, 1998) with English vs. Chinese features comprising the two independent ―views‖ to exploit unlabeled Chinese data and a labeled English corpus and thereby improves Chinese sentiment classification. Another notable approach is the work of BoydGraber and Resnik (2010), which presents a generative model --- supervised multilingual latent Dirichlet allocation --- that jointly models topics that are consistent across languages, and employs them to better predict sentiment ratings. Unlike the methods described above, we focus on simultaneously improving th"
P11-1033,P10-1115,0,0.0157409,"gularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1 have been widely used in many NLP tasks (Berger et al., 1996; Ratnaparkhi, 1997; Smith, 2006). The models assign the conditional proba"
P11-1033,P09-1007,0,0.0101867,"r semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum en"
P11-1033,P02-1053,0,\N,Missing
P98-2206,C92-1019,0,0.0114873,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segnaentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desi"
P98-2206,P94-1010,0,0.0356739,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segnaentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desi"
P98-2206,P97-1041,0,0.0394095,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segnaentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desi"
P98-2206,A97-1018,1,0.722729,"explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[ 1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. 1265 rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segnaentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desi"
P98-2206,J96-3004,0,\N,Missing
S10-1064,J96-1002,0,0.00729315,"ly checked the polarity of clauses containing SAAs. Due to time limitation, we only checked 465 clauses. Plus the clauses extracted from 100 trial sentences, the final clause-level training data consist of 565 positive/negative clauses containing SAAs. 3 Our Approach for Disambiguating SAAs To disambiguating SAAs, we use the maximum entropy algorithm and the sentiment lexiconbased method, and also combine them together. 3.1 The Maximum Entropy-based Method Maximum entropy classification (MaxEnt) is a technique which has proven effective in a number of natural language processing applications (Berger et al., 1996). Le Zhang’s maximum entropy tool2 is used for classification. The Chinese sentences are segmented into words using a production segmentation system. Unigrams of words are used as basic features for classification. Bigrams are also tried, but does not show improvement, and thus are not described in details here. 3.2 The Lexicon-based Method For the lexicon-based method, we first classify the 14 adjectives into two classes: intensifiers 2 293 http:// homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html and suppressors. Intensifiers refer to adjectives intensifying the intensity of context, inclu"
S10-1064,P97-1023,0,0.469992,"the intensity of context) and suppressors (i.e. adjectives decreasing the intensity of context), and then use the polarity of context to get the SAAs’ contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context. 1 Introduction In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the NLP field (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Pang and Lee, 2008). It has many applications such as social media monitoring, market research, and public relations. Some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context. Such words can be called dynamic sentiment-ambiguous adjectives (SAAs). However, SAAs have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work. Wu et al., (2008) presents an approach of combining"
S10-1064,W02-1011,0,0.0614653,"ssors (i.e. adjectives decreasing the intensity of context), and then use the polarity of context to get the SAAs’ contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context. 1 Introduction In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the NLP field (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Pang and Lee, 2008). It has many applications such as social media monitoring, market research, and public relations. Some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context. Such words can be called dynamic sentiment-ambiguous adjectives (SAAs). However, SAAs have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work. Wu et al., (2008) presents an approach of combining collocation inform"
S10-1064,P02-1053,0,0.0109931,"ves decreasing the intensity of context), and then use the polarity of context to get the SAAs’ contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context. 1 Introduction In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs, and product reviews, has drawn much attention in the NLP field (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Pang and Lee, 2008). It has many applications such as social media monitoring, market research, and public relations. Some adjectives are neutral in sentiment polarity out of context, but they could show positive, neutral or negative meaning within specific context. Such words can be called dynamic sentiment-ambiguous adjectives (SAAs). However, SAAs have not been intentionally tackled in the researches of sentiment analysis, and usually have been discarded or ignored by most previous work. Wu et al., (2008) presents an approach of combining collocation information and SVM"
S10-1064,S10-1014,0,0.0427181,"010 is intended to create a benchmark dataset for disambiguating SAAs. Given only 100 trial sentences, but not provided with any official training data, participants are required to tackle this problem data by unsupervised approaches or use their own training data. The task consists of 14 SAAs, which are all high-frequency words in Mandarin Chinese. They are 大|big, 小|small, 多|many, 少 |few, 高|high, 低|low, 厚|thick, 薄|thin, 深|deep, 浅|shallow, 重|heavy, 轻|light, 巨大|huge, 重大 |grave. This task deals with Chinese SAAs, but the disambiguating techniques should be language-independent. Please refer to (Wu and Jin, 2010) for more descriptions of the task. In our participating system, the annotated Chinese data from the NTCIR opinion analysis tasks is used as training data with the help of a combined sentiment lexicon. A machine learning-based method (namely maximum entropy) and the lexicon-based method are compared in our submissions. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve 292 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 292–295, c Uppsala, Sweden,"
tsou-etal-2006-court,J95-2001,0,\N,Missing
tsou-etal-2006-court,J88-1003,0,\N,Missing
tsou-etal-2006-court,J93-2006,0,\N,Missing
tsou-etal-2006-court,W96-0108,0,\N,Missing
tsou-kwong-2006-toward,C82-2013,0,\N,Missing
tsou-kwong-2006-toward,P99-1016,0,\N,Missing
tsou-kwong-2006-toward,O05-5009,1,\N,Missing
tsou-kwong-2006-toward,Y96-1018,0,\N,Missing
U19-1021,P06-4001,0,0.060848,"ems for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); and words that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). A study on gap-fill items for learning Chinese as a foreign language compared the quality of distractors generated by a number of criteria, including spelling, word co-occurrence and semantic similarity (Jiang and Lee, 2017). Experimental results show that a semantic similarity measure, based"
U19-1021,W15-4406,0,0.015473,"and Lee, 2017). Learner error. Typical or frequent learner mistakes can be effective as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); and words that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). A study on gap-fill items for learning Chinese as a foreign language compared the quality of distractors generated by a number of criteria, including spelling, word"
U19-1021,N19-1423,0,0.131128,"distractors at different levels of plausibility. We therefore propose to investigate the correlation between the estimated ranking of distractors and human judgment on plausibility. This research direction has indeed been taken up in item generation in the natural sciences (Liang et al., 2018; Gao et al., 2019). However, to the best of our knowledge, it has not yet been attempted for gap-fill items for language learning. Language models provide a natural framework for this task by predicting how likely a word appears in a gap within the sentence. This paper is the first attempt to apply BERT (Devlin et al., 2019), a state-of-the-art model trained on the masked language modeling objective, on the task of distractor ranking. Experimental results show that BERT outperforms semantic similarity measures, in terms of both correlation to human judgment and classification accuracy of distractor plausibility. The rest of the paper is organized as follows. In Section 2, we review related research areas. In Section 3, we outline our approach for distractor generation. In Section 4, we report experimental results on ranking distractors for gap-fill items for learning Chinese as a foreign language. Finally, we con"
U19-1021,W17-5015,1,0.247518,"ically and semantically homogenous (Pho et al., 2014). To reduce the manual effort and time needed for selecting distractors, there has been much interest He as class representative for two years. (a) served (b) acted (c) brought [target word] [distractor] [distractor] Table 1: A multiple-choice gap-fill item consists of a carrier sentence with a blank, and choices for filling the blank. In this example, the choices include two distractors and the target word (correct answer). in developing algorithms for automatic distractor generation. Existing algorithms typically take a two-step approach (Jiang and Lee, 2017; Susanti et al., 2018). The first step generates distractor candidates, typically in a list ranked according to measures of semantic similarity and collocation strength. The second step removes candidates that are acceptable answers, using n-gram and collocation frequency or other criteria. Evaluations on distractor generation tend to be limited to the highest-ranked distractors, for example the top-ranked or top three candidates only (Jiang and Lee, 2017; Susanti et al., 2018). Many practical scenarios, however, require not only the most challenging distractors, but distractors across the sp"
U19-1021,W06-1416,0,0.0452303,"e as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); and words that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). A study on gap-fill items for learning Chinese as a foreign language compared the quality of distractors generated by a number of criteria, including spelling, word co-occurrence and semantic similarity (Jiang and Lee, 2017). Experimental results show th"
U19-1021,P16-1093,1,0.794448,"have been used, including bigram counts (Susanti et al., 2018) and, more generally, n-grams in a context window centered on the distractor (Liu et al., 2005); dependency relations (Sakaguchi et al., 2013); grammatical relations in a Word Sketch (Smith et al., 2010); as well as pointwise mutual information (PMI) (Jiang and Lee, 2017). Learner error. Typical or frequent learner mistakes can be effective as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); an"
U19-1021,P18-2023,0,0.0174085,"ceptable answers. Our research focus is on the first step, to which we introduce a re-ranking process with a neural language model. 3.1 Baseline Our baseline uses semantic similarity measures, which have been reported in previous research to yield the best performance in identifying plausible distractor candidates (Section 2.1). Word embeddings have been shown to be effective in measuring word similarity and relatedness in a large range of NLP tasks, including distractor generation (Jiang and Lee, 2017). We used word embeddings trained by Skipgram with negative sampling on Baidu Encyclopedia (Li et al., 2018). Specifically, we calculated cosine similarity between the word embeddings of the distractor candidate and the target word, and obtained candidates with the highest scores. 3.2 Re-ranking The appropriateness of a distractor may depend not only on the target word but also on the context of the carrier sentence. Consider the word served as the target word. In the context of food being served at a restaurant, the word brought may be a plausible distractor since it is semantically close to the target word. However, in the context of serving in a position, the word acted would be a more plausible"
U19-1021,W18-0533,0,0.740562,"t items, for example, it can be useful to generate items at various difficulty levels for comprehensive assessment. In a CALL system, it can be effective to personalize distractor difficulty according to the user’s language proficiency. It is informative, then, to evaluate distractor algorithms on their ability to generate distractors at different levels of plausibility. We therefore propose to investigate the correlation between the estimated ranking of distractors and human judgment on plausibility. This research direction has indeed been taken up in item generation in the natural sciences (Liang et al., 2018; Gao et al., 2019). However, to the best of our knowledge, it has not yet been attempted for gap-fill items for language learning. Language models provide a natural framework for this task by predicting how likely a word appears in a gap within the sentence. This paper is the first attempt to apply BERT (Devlin et al., 2019), a state-of-the-art model trained on the masked language modeling objective, on the task of distractor ranking. Experimental results show that BERT outperforms semantic similarity measures, in terms of both correlation to human judgment and classification accuracy of dist"
U19-1021,W05-0201,0,0.0760628,"wers, leaving only the distractors that are “reliable”, i.e., those that yield an incorrect sentence. Section 2.1 reviews existing approaches for the Candidate Generation task, which is the research focus of this paper. Section 2.2 then surveys related tasks in computer-assisted learning that have adopted evaluation on candidate ranking. 2.1 according to their co-occurrence frequencies with other words in the sentence. Various definitions of co-occurrence have been used, including bigram counts (Susanti et al., 2018) and, more generally, n-grams in a context window centered on the distractor (Liu et al., 2005); dependency relations (Sakaguchi et al., 2013); grammatical relations in a Word Sketch (Smith et al., 2010); as well as pointwise mutual information (PMI) (Jiang and Lee, 2017). Learner error. Typical or frequent learner mistakes can be effective as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Sema"
U19-1021,pho-etal-2014-multiple,0,0.0230463,"Missing"
U19-1021,P13-2043,0,0.0198426,"re “reliable”, i.e., those that yield an incorrect sentence. Section 2.1 reviews existing approaches for the Candidate Generation task, which is the research focus of this paper. Section 2.2 then surveys related tasks in computer-assisted learning that have adopted evaluation on candidate ranking. 2.1 according to their co-occurrence frequencies with other words in the sentence. Various definitions of co-occurrence have been used, including bigram counts (Susanti et al., 2018) and, more generally, n-grams in a context window centered on the distractor (Liu et al., 2005); dependency relations (Sakaguchi et al., 2013); grammatical relations in a Word Sketch (Smith et al., 2010); as well as pointwise mutual information (PMI) (Jiang and Lee, 2017). Learner error. Typical or frequent learner mistakes can be effective as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be seman"
U19-1021,W05-0210,0,0.683149,". Typical or frequent learner mistakes can be effective as distractors. When generating gap-fill items for prepositions, distractors based on learner errors were indeed found to be more challenging than those selected according to word co-occurrence (Lee et al., 2016). Distractor candidates have been mined from learner corpora and further selected with a discriminative model (Sakaguchi et al., 2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); and words that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). A study on gap-fill items for learning Chinese as a foreign language compared the quality of distractors generated by a number of criteria, including spelling, word co-occurrence and semantic simi"
U19-1021,W14-1817,0,0.0185028,"2013). Semantic similarity. The distractor should be semantically close to the target word. Similarity can be quantified by semantic distance in WordNet (Pino et al., 2008; Chen et al., 2015), thesauri (Sumita et al., 2005; Smith et al., 2010), ontologies (Karamanis et al., 2006), hand-crafted rules (Chen et al., 2006), and word embeddings (Jiang and Lee, 2017; Susanti et al., 2018). Other approaches have also explored synonym of synonyms (Knoop and Wilske, 2013); and words that are semantically similar to the target word in some sense, but not in the particular sense in the carrier sentence (Zesch and Melamud, 2014). A study on gap-fill items for learning Chinese as a foreign language compared the quality of distractors generated by a number of criteria, including spelling, word co-occurrence and semantic similarity (Jiang and Lee, 2017). Experimental results show that a semantic similarity measure, based on the word2vec model (Mikolov et al., 2013), yields distractors that are significantly more plausible than those generated by spelling similarity, and by word co-occurrence strength as estimated by PMI. Candidate Generation In most approaches, a distractor needs to have the same part-of-speech (POS) as"
W00-0402,J86-3001,0,0.0890399,"Missing"
W00-0402,J97-1003,0,0.0356419,"al. (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST). The theory has been exploited in a. number of computational systems (e.g. Hovy 1993). The main idea is to build a discourse tree where each node of the tree represents a RST relation. Summarization is achieved by trimming unimportant sentences on the basis of the relative saliency or rhetorical relations. On the other hand, cohesion can also provide context to aid in the resolution of ambiguity as well as in text summarization (Halliday and Hasan 1976; Morris and Hirst 1991; Hearst 1997). Mani et al. (1998) describes a method based on text coherence which models text in terms of macro-level relations between clauses or sentences to help determine the overall argumentative structure of the text. They examine the extent to which cohesion and coherence can each be used to establish saliency of textual units. The SIFAS (S,yntactic Marker based EullText Abstration System) system has been designed and implemented to use discourse markers in the automatic summarization of Chinese. Section 2 provides an introduction to discourse markers in Chinese. An overview of SIFAS is presented i"
W00-0402,J93-3003,0,0.0205959,"mplex sentences. Our concern in this project is to identify so in the discourse sense as in (2) in contrast to so used as an adverb in the sentential sense as in (1). Similar difficulties are found in Chinese, as discussed in Section 7. 3 SIFAS System Architecture From the perspective of discourse analysis, the study of discourse markers basically involves four distinct but fundamental issues: 1) the occurrence and the frequency of occurrence of discourse markers (Moser and Moore 1995), 2) determining whether a candidate linguistic item is a discourse marker (identification / disambiguation) (Hirschberg and Litman 1993; Siegel and McKeown 1994), 3) determination or selection of the discourse function of an identified discourse marker (Moser and Moore 1995), and 4) the coverage capabilities (in terms of levels of embedding) among rhetorical relations, as well as among individual discourse markers. Discussion of these problems for Chinese compound sentences can be found in Wang et al. (1994). Previous attempts to address the above problems in Chinese text have usually been based on the investigators' intuition and knowledge, or on a small number of constructed examples. In our current research, we adopt heuri"
W00-0402,J81-2001,0,0.76193,"sts that these clauses and sentences tend to cluster together into units, called discourse segments, that are related pragmatically to form a hierarchical structure. Discourse analysis goes beyond the levels of syntactic and semantic analysis, which typically treats each sentence as an isolated, independent unit. The function of discourse analysis is to divide a text into discourse segments, and to recognize and re-construct the discourse structure of the text as intended by its author. Results of discourse analysis can be used to solve many important NLP problems such as anaphoric reference (Hirst 1981), tense and aspect analysis (Hwang and Schubert 1992), intention recognition 11 model and the use of superstructural schemes is promising for abstracting text. Johnson et al. (1993) describes a text processing system that can identify anaphors so that they may be utilized to enhance sentence selection. It is based on the assumption that sentences which contain nonanaphoric noun phrases and introduce key concepts into the text are worthy of inclusion in an abstract. Ono et al. (1994), T'sou et al. (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structu"
W00-0402,P92-1030,0,0.0701937,"d to cluster together into units, called discourse segments, that are related pragmatically to form a hierarchical structure. Discourse analysis goes beyond the levels of syntactic and semantic analysis, which typically treats each sentence as an isolated, independent unit. The function of discourse analysis is to divide a text into discourse segments, and to recognize and re-construct the discourse structure of the text as intended by its author. Results of discourse analysis can be used to solve many important NLP problems such as anaphoric reference (Hirst 1981), tense and aspect analysis (Hwang and Schubert 1992), intention recognition 11 model and the use of superstructural schemes is promising for abstracting text. Johnson et al. (1993) describes a text processing system that can identify anaphors so that they may be utilized to enhance sentence selection. It is based on the assumption that sentences which contain nonanaphoric noun phrases and introduce key concepts into the text are worthy of inclusion in an abstract. Ono et al. (1994), T'sou et al. (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST). The theory has been exploited in a."
W00-0402,O91-1007,1,0.847888,"ing Discourse Markers for Chinese Textual Summarization Samuel W. K. Chan I, Tom B. Y. Lai 2, W. J. Gao 3, Benjamin K. T'sou 4 J 24Language Information Sciences Research Centre City Un!versity of Hong Kong Tat CheeAvenue, Kowloon Tong, Hong Kong SAR, China 3 North Eastern University, China Iswkchan@cs.cityu.edu.hk, {2cttomlai, 4rlbtsou} @cpccux0.cityu.edu.hk, 3wjgao@ramm.neu.edu.cn Abstract (Grosz and Sidner 1986; Litman and Allen 1990), or'can be directly applied to computational NLP applications such as text abstraction (Ono et al. 1994; T'sou et al. 1996) and text generation (McKeown 1985; Lin et al. 1991). Automatic text abstraction has received considerable attention (see Paice (1990) for a comprehensive review). While some statistical approaches have had some success in extracting one or more sentences which can serve as a summary (Brandow et al. 1995; Kupiec et al. 1995; Salton et al. 1997), summarization in general has remained an elusive task. McKeown and Radev (1995) develop a system SUMMONS to summarize full text input using templates produced by the message understanding systems, developed under ARPA human language technology. Unlike previous approaches, their system summarizes a serie"
W00-0402,W97-0713,0,0.0336631,"n be used to solve many important NLP problems such as anaphoric reference (Hirst 1981), tense and aspect analysis (Hwang and Schubert 1992), intention recognition 11 model and the use of superstructural schemes is promising for abstracting text. Johnson et al. (1993) describes a text processing system that can identify anaphors so that they may be utilized to enhance sentence selection. It is based on the assumption that sentences which contain nonanaphoric noun phrases and introduce key concepts into the text are worthy of inclusion in an abstract. Ono et al. (1994), T'sou et al. (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST). The theory has been exploited in a. number of computational systems (e.g. Hovy 1993). The main idea is to build a discourse tree where each node of the tree represents a RST relation. Summarization is achieved by trimming unimportant sentences on the basis of the relative saliency or rhetorical relations. On the other hand, cohesion can also provide context to aid in the resolution of ambiguity as well as in text summarization (Halliday and Hasan 1976; Morris and Hirst 1991; Hearst 1997). Mani et al. (1"
W00-0402,J91-1002,0,0.03938,"et al. (1994), T'sou et al. (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST). The theory has been exploited in a. number of computational systems (e.g. Hovy 1993). The main idea is to build a discourse tree where each node of the tree represents a RST relation. Summarization is achieved by trimming unimportant sentences on the basis of the relative saliency or rhetorical relations. On the other hand, cohesion can also provide context to aid in the resolution of ambiguity as well as in text summarization (Halliday and Hasan 1976; Morris and Hirst 1991; Hearst 1997). Mani et al. (1998) describes a method based on text coherence which models text in terms of macro-level relations between clauses or sentences to help determine the overall argumentative structure of the text. They examine the extent to which cohesion and coherence can each be used to establish saliency of textual units. The SIFAS (S,yntactic Marker based EullText Abstration System) system has been designed and implemented to use discourse markers in the automatic summarization of Chinese. Section 2 provides an introduction to discourse markers in Chinese. An overview of SIFAS"
W00-0402,P95-1018,0,0.0150253,"re, so in (1) can occur in a simple (exclamatory) sentence (e.g. ""John is so tall!""), but so in (2) must occur in the context of complex sentences. Our concern in this project is to identify so in the discourse sense as in (2) in contrast to so used as an adverb in the sentential sense as in (1). Similar difficulties are found in Chinese, as discussed in Section 7. 3 SIFAS System Architecture From the perspective of discourse analysis, the study of discourse markers basically involves four distinct but fundamental issues: 1) the occurrence and the frequency of occurrence of discourse markers (Moser and Moore 1995), 2) determining whether a candidate linguistic item is a discourse marker (identification / disambiguation) (Hirschberg and Litman 1993; Siegel and McKeown 1994), 3) determination or selection of the discourse function of an identified discourse marker (Moser and Moore 1995), and 4) the coverage capabilities (in terms of levels of embedding) among rhetorical relations, as well as among individual discourse markers. Discussion of these problems for Chinese compound sentences can be found in Wang et al. (1994). Previous attempts to address the above problems in Chinese text have usually been ba"
W00-0402,C94-1056,0,0.0981806,"ts author. Results of discourse analysis can be used to solve many important NLP problems such as anaphoric reference (Hirst 1981), tense and aspect analysis (Hwang and Schubert 1992), intention recognition 11 model and the use of superstructural schemes is promising for abstracting text. Johnson et al. (1993) describes a text processing system that can identify anaphors so that they may be utilized to enhance sentence selection. It is based on the assumption that sentences which contain nonanaphoric noun phrases and introduce key concepts into the text are worthy of inclusion in an abstract. Ono et al. (1994), T'sou et al. (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST). The theory has been exploited in a. number of computational systems (e.g. Hovy 1993). The main idea is to build a discourse tree where each node of the tree represents a RST relation. Summarization is achieved by trimming unimportant sentences on the basis of the relative saliency or rhetorical relations. On the other hand, cohesion can also provide context to aid in the resolution of ambiguity as well as in text summarization (Halliday and Hasan 1976; Morris and Hi"
W00-0402,C92-3162,1,0.883889,"of discourse analysis can be used to solve many important NLP problems such as anaphoric reference (Hirst 1981), tense and aspect analysis (Hwang and Schubert 1992), intention recognition 11 model and the use of superstructural schemes is promising for abstracting text. Johnson et al. (1993) describes a text processing system that can identify anaphors so that they may be utilized to enhance sentence selection. It is based on the assumption that sentences which contain nonanaphoric noun phrases and introduce key concepts into the text are worthy of inclusion in an abstract. Ono et al. (1994), T'sou et al. (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST). The theory has been exploited in a. number of computational systems (e.g. Hovy 1993). The main idea is to build a discourse tree where each node of the tree represents a RST relation. Summarization is achieved by trimming unimportant sentences on the basis of the relative saliency or rhetorical relations. On the other hand, cohesion can also provide context to aid in the resolution of ambiguity as well as in text summarization (Halliday and Hasan 1976; Morris and Hirst 1991; Hearst 1997"
W00-1206,J86-3001,0,0.10075,"Missing"
W00-1206,J81-2001,0,0.181265,"Missing"
W00-1206,P92-1030,0,0.0249938,"Missing"
W00-1206,O91-1007,1,0.780244,"Missing"
W00-1206,C94-1056,0,0.0169013,"s, rhetorical relation, automatic tagging, machine learning 1 Introduction Discourse refers to any form of language-based communication involving multiple sentences or utterances. The most important forms of discourse of interest to Natural Language Processing (NLP) are text and dialogue. The function of discourse analysis is to divide a text into discourse segments, and to recognize and re-construct the discourse structure of the text as intended by its author. Automatic text abstraction has received considerable attention (Paice 1990). Various systems have been developed (Chan et al. 2000). Ono et al. (1994), T'sou et al. (1992) 38 and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST, Mann and Thompson 1986). The theory has been exploited in a number of computational systems (e.g. Hovy 1993). The main idea is to build a discourse tree where each node of the tree represents an RST relation. Summarization is achieved by trimming lmimportant sentences on the basis of the relative saliency or rhetorical relations. The SIFAS (Syntactic Marker based Full-Text Abstraction System) system has been implemented to use discourse markers in the automatic su"
W00-1206,W00-0402,1,\N,Missing
W00-1206,C92-3162,1,\N,Missing
W02-1404,J96-1001,0,\N,Missing
W02-1404,A97-1050,0,\N,Missing
W02-1404,N01-1020,0,\N,Missing
W02-1404,C00-1015,0,\N,Missing
W02-1404,P91-1023,0,\N,Missing
W02-1404,P00-1050,0,\N,Missing
W02-1404,P94-1012,0,\N,Missing
W02-1404,O97-4004,0,\N,Missing
W02-1802,C00-1015,0,0.0276459,"on, Hong Kong csrluk@comp.polyu.edu.hk the significant differences in lexicon, syntax, semantics and styles. The discussion in the paper is based on issues arising from the extraction of bilingual legal terms from aligned Chinese-English legal corpus in the implementation of a bilingual a text retrieval system for the Judiciary of the Hong Kong Special Administrative Region (HKSAR) Government. Much attention in computational terminology has been directed to the development of algorithms for extraction from parallel texts. For example, Chinese-English (Wu and Xia 1995), Swedish-English-Polish (Borin 2000), and Chinese-Korean (Huang and Choi 2000). Despite considerable progress, bilingual terminology so generated is often not ready for immediate and practical use. Machine extraction is often the first step of terminology extraction and must be used in conjunction with rigorous and well-managed manual efforts which are critical for the production of consistent and useable multilingual terminology. However, there has been relatively little discussion on the significance of human intervention. The process is far from being straightforward because of the different purposes of alignment, the require"
W02-1802,P00-1050,0,0.0283399,"du.hk the significant differences in lexicon, syntax, semantics and styles. The discussion in the paper is based on issues arising from the extraction of bilingual legal terms from aligned Chinese-English legal corpus in the implementation of a bilingual a text retrieval system for the Judiciary of the Hong Kong Special Administrative Region (HKSAR) Government. Much attention in computational terminology has been directed to the development of algorithms for extraction from parallel texts. For example, Chinese-English (Wu and Xia 1995), Swedish-English-Polish (Borin 2000), and Chinese-Korean (Huang and Choi 2000). Despite considerable progress, bilingual terminology so generated is often not ready for immediate and practical use. Machine extraction is often the first step of terminology extraction and must be used in conjunction with rigorous and well-managed manual efforts which are critical for the production of consistent and useable multilingual terminology. However, there has been relatively little discussion on the significance of human intervention. The process is far from being straightforward because of the different purposes of alignment, the requirements of target users and the corpus type."
W05-1001,P98-1013,0,0.0218422,"parse information for indicating constituent boundaries in semantic role labelling. In Section 2, related work will be reviewed. In Section 3, the data used in the current study will be introduced. Our proposed method will be explained in Section 4, and the experiment reported in Section 5. Results and future work will be discussed in Section 6, followed by conclusions in Section 7. 2 Related Work The definition of semantic roles falls on a continuum from abstract ones to very specific ones. Gildea and Jurafsky (2002), for instance, used a set of roles defined according to the FrameNet model (Baker et al., 1998), thus corresponding to the frame elements in individual frames under a particular domain to which a given verb belongs. Lexical entries (in fact not limited to verbs, in the case of FrameNet) falling under the same frame will share the same set of roles. Gildea and Palmer (2002) defined roles with respect to individual predicates in the PropBank, without explicit naming. To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems, as in the CoNLL-2004 shared task (Carreras and Màrquez, 2004) and SENSEVAL-3 (Litkowski, 2004). The theoretical"
W05-1001,W04-2412,0,0.0540291,"d a set of roles defined according to the FrameNet model (Baker et al., 1998), thus corresponding to the frame elements in individual frames under a particular domain to which a given verb belongs. Lexical entries (in fact not limited to verbs, in the case of FrameNet) falling under the same frame will share the same set of roles. Gildea and Palmer (2002) defined roles with respect to individual predicates in the PropBank, without explicit naming. To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems, as in the CoNLL-2004 shared task (Carreras and Màrquez, 2004) and SENSEVAL-3 (Litkowski, 2004). The theoretical treatment of semantic roles is also varied in Chinese. In practice, for example, the semantic roles in the Sinica Treebank mark not only verbal arguments but also modifier-head relations (You and Chen, 2004). In our present study, we go for a set of more abstract semantic roles similar to the thematic roles for English used in VerbNet (Kipper et al., 2002). These roles are generalisable to most Chinese verbs and are not 2 dependent on particular predicates. They will be further introduced in Section 3. Approaches in automatic semantic role lab"
W05-1001,J02-3001,0,0.858357,"rrectly is essential to many applications such as information extraction and machine translation; and making available resources with this kind of information would in turn facilitate the development of such applications. Large-scale production of annotated resources is often labour intensive, and thus calls for automatic labelling to streamline the process. The task is essentially done in two phases, namely recognising the constituents bearing some semantic relationship to the target verb in a sentence, and then labelling them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky (2002) approached the task using various features such as headword, phrase type, and parse tree path. While such features have remained the basic and essential features in subsequent research, parsed sentences are nevertheless required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling (Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003). More precisely, parse information is rather more critical for the identification of boundaries of candidate constituents than for"
W05-1001,P02-1031,0,0.399679,"bearing some semantic relationship to the target verb in a sentence, and then labelling them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky (2002) approached the task using various features such as headword, phrase type, and parse tree path. While such features have remained the basic and essential features in subsequent research, parsed sentences are nevertheless required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling (Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003). More precisely, parse information is rather more critical for the identification of boundaries of candidate constituents than for the extraction of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. You and Chen, 2004). As full parses are not always accessible, many thus resort to shallow syntactic information from simple chunking, even though results often turn out to be less satisfactory than with full parses. This limitation is even more pertinent for the application of semantic role labelling to la"
W05-1001,W03-1008,0,0.0247883,"ationship to the target verb in a sentence, and then labelling them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky (2002) approached the task using various features such as headword, phrase type, and parse tree path. While such features have remained the basic and essential features in subsequent research, parsed sentences are nevertheless required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling (Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003). More precisely, parse information is rather more critical for the identification of boundaries of candidate constituents than for the extraction of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. You and Chen, 2004). As full parses are not always accessible, many thus resort to shallow syntactic information from simple chunking, even though results often turn out to be less satisfactory than with full parses. This limitation is even more pertinent for the application of semantic role labelling to languages which do not have sophi"
W05-1001,kingsbury-palmer-2002-treebank,0,0.0212537,"he training and testing data is important especially in view of the characteristic syntaxsemantics interface in Chinese. We also plan to explore some class-based techniques for the task with reference to existing semantic lexicons, and to modify the method and augment the feature set with more linguistic input. 1 Introduction As the development of language resources progresses from POS-tagged corpora to syntactically annotated treebanks, the inclusion of semantic information such as predicate-argument relations becomes indispensable. The expansion of the Penn Treebank into a Proposition Bank (Kingsbury and Palmer, 2002) is a typical move in this direction. Lexical resources also need to be enhanced with semantic information (e.g. Fellbaum et al., 2001). The ability to identify semantic role relations correctly is essential to many applications such as information extraction and machine translation; and making available resources with this kind of information would in turn facilitate the development of such applications. Large-scale production of annotated resources is often labour intensive, and thus calls for automatic labelling to streamline the process. The task is essentially done in two phases, namely r"
W05-1001,E03-1081,1,0.812695,"sets of primary school Chinese textbooks popularly used in Hong Kong were taken for reference. The two publishers were Keys Press and Modern Education Research Society Ltd. Texts for Primary One to Six were digitised, segmented into words, and annotated with parts-of-speech (POS). This results in a text collection of about 165K character tokens and upon segmentation about 109K word tokens (about 15K word types). There were about 2,500 transitive verb types, with frequency ranging from 1 to 926. The complex examples were taken from a subset of the LIVAC synchronous corpus1 (Tsou et al., 2000; Kwong and Tsou, 2003). The subcorpus consists of newspaper texts from Hong Kong, including local news, international news, financial news, sports news, and entertainment news, collected in 1997-98. The texts were segmented into words and POS-tagged, resulting in about 1.8M character tokens and upon segmentation about 1M word tokens (about 47K word types). There were about 7,400 transitive verb types, with frequency ranging from 1 to just over 6,300. 1 http://www.livac.org 3 3.2 Training and Testing Data For the current study, a set of 41 transitive verbs common to the two corpora (hereafter referred to as textbook"
W05-1001,W04-0803,0,0.0893424,"ameNet model (Baker et al., 1998), thus corresponding to the frame elements in individual frames under a particular domain to which a given verb belongs. Lexical entries (in fact not limited to verbs, in the case of FrameNet) falling under the same frame will share the same set of roles. Gildea and Palmer (2002) defined roles with respect to individual predicates in the PropBank, without explicit naming. To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems, as in the CoNLL-2004 shared task (Carreras and Màrquez, 2004) and SENSEVAL-3 (Litkowski, 2004). The theoretical treatment of semantic roles is also varied in Chinese. In practice, for example, the semantic roles in the Sinica Treebank mark not only verbal arguments but also modifier-head relations (You and Chen, 2004). In our present study, we go for a set of more abstract semantic roles similar to the thematic roles for English used in VerbNet (Kipper et al., 2002). These roles are generalisable to most Chinese verbs and are not 2 dependent on particular predicates. They will be further introduced in Section 3. Approaches in automatic semantic role labelling are mostly statistical, ty"
W05-1001,N04-1032,0,0.185424,"Missing"
W05-1001,W04-3212,0,0.0922808,"ting, which we expect to improve the overall performance. Another area of interest is to look at the behaviour of near-synonymous predicates in the tagging process. Many predicates may be unseen in the training data, but while the probability estimation could be generalized from near-synonyms as suggested by a semantic lexicon, whether the similarity and subtle differences between near-synonyms with respect to the argument structure and the corresponding syntactic realisation could be distinguished would also be worth studying. Related to this is the possibility of augmenting the feature set. Xue and Palmer (2004), for instance, looked into new features such as syntactic frame, lexicalized constituent type, etc., and found that enriching the feature set improved the labelling performance. In particular, given the importance of data homogeneity as observed from the experimental results, and the challenges posed by the characteristic nature of Chinese, we intend to improve our method and feature set with more linguistic consideration. 7 Conclusion The study reported in this paper has thus tackled semantic role labelling in Chinese in the absence of parse information, by attempting to locate the correspon"
W05-1001,W04-1116,0,0.0561841,"the basic and essential features in subsequent research, parsed sentences are nevertheless required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling (Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003). More precisely, parse information is rather more critical for the identification of boundaries of candidate constituents than for the extraction of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. You and Chen, 2004). As full parses are not always accessible, many thus resort to shallow syntactic information from simple chunking, even though results often turn out to be less satisfactory than with full parses. This limitation is even more pertinent for the application of semantic role labelling to languages which do not have sophisticated parsing resources. In the case of Chinese, for example, there is con1 Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 1–9, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics siderable variability in its syntax-semantics inter"
W05-1001,W04-3213,0,\N,Missing
W05-1001,C98-1013,0,\N,Missing
W06-0102,xia-etal-2000-developing,0,0.0450783,"t al., 1989; Riloff and Shepherd, 1999; Lin et al, 2003). Compared to the development of thesauri and lexical databases, and research into semantic networks for major languages such as English, similar work for the Chinese language is less mature. This gap was partly due to the lack of authoritative Chinese corpora as a basis for analysis, but has been gradually reduced with the recent availability of large Chinese corpora including the LIVAC synchronous corpus (Tsou and Lai, 2003) used in this work and further described below, the Sinica Corpus (Chen et al., 1996), the Chinese Penn Treebank (Xia et al., 2000), and the like. An important issue which is seldom addressed in the construction of Chinese lexical databases is the problem of versatility and portability. For a language such as Chinese which is spoken in many different communities, different linguistic norms have emerged as a result of the individualistic evolution and development of the language within a particular community and culture. Such variations are seldom adequately reflected in existing lexical resources, which often only draw reference from one particular source. For instance, Tongyici Cilin (同義詞詞林) (Mei et al., 1984) is a thesa"
W06-0102,C82-2013,0,\N,Missing
W06-0102,P99-1016,0,\N,Missing
W06-0102,huang-etal-2004-sinica,0,\N,Missing
W06-0102,Y96-1018,0,\N,Missing
W10-4110,P91-1022,0,0.515199,"Missing"
W10-4110,J93-2003,0,0.0453684,"Missing"
W10-4110,2007.mtsummit-papers.9,0,0.0661439,"Missing"
W10-4110,P93-1002,0,0.315112,"Missing"
W10-4110,J07-2003,0,0.119073,"Missing"
W10-4110,P91-1023,0,0.712647,"Chinese bilingual patents. To our knowledge, this is the largest single parallel corpus in terms of sentence pairs. Moreover, we estimate the potential for mining multilingual parallel corpora involving English, Chinese, Japanese, Korean, German, etc., which would to some extent reduce the parallel data acquisition bottleneck in multilingual information processing. 1 Introduction Multilingual data are critical resources for building many applications, such as machine translation (MT) and cross-lingual information retrieval. Many parallel corpora have been built, such as the Canadian Hansards (Gale and Church, 1991), the Europarl corpus (Koehn, 2005), the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation. However, few parallel corpora exist for many language pairs, such as Chinese-Japanese, Japanese-Korean, ChineseFrench or Japanese-German. Even for language pairs with several parallel corpora, such as Chinese-English and Arabic-English, the size of parallel corpora is still a major limitation for SMT systems to achieve higher performance. In this paper, we present a way which could, to some extent, reduce the parallel data acquisition bottleneck in multilingual lang"
W10-4110,2001.mtsummit-papers.30,0,0.151725,"ice and then file its international application also in Chinese under the PCT. Later on, it may have the patent translated into English and file it in USA patent office, which means the patent becomes bilingual. If the applicant continues to file it in Japan with Japanese, it would be trilingual. Even more, it would be quadrilingual or involve more languages when it is filed in other countries with more languages. Such multilingual patents are considered comparable (or noisy parallel) because they are not parallel in the strict sense but still closely related in terms of information conveyed (Higuchi et al., 2001; Lu et al., 2009). 4 A Large English-Chinese Parallel Corpus Mined from Bilingual Patents In this section, we introduce the English-Chinese bilingual patents harvested from the Web and the method to mine parallel sentences from them. SMT experiments on the final parallel corpus are also described. 4.1 Harvesting English-Chinese Bilingual Patents The official patent office in China is the State Intellectual Property Office (SIPO). In early 2009, by searching on its website, we found about 200K Chinese patents previously filed as PCT applications in English and crawled their bibliographical dat"
W10-4110,2005.mtsummit-papers.11,0,0.0292542,"this is the largest single parallel corpus in terms of sentence pairs. Moreover, we estimate the potential for mining multilingual parallel corpora involving English, Chinese, Japanese, Korean, German, etc., which would to some extent reduce the parallel data acquisition bottleneck in multilingual information processing. 1 Introduction Multilingual data are critical resources for building many applications, such as machine translation (MT) and cross-lingual information retrieval. Many parallel corpora have been built, such as the Canadian Hansards (Gale and Church, 1991), the Europarl corpus (Koehn, 2005), the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation. However, few parallel corpora exist for many language pairs, such as Chinese-Japanese, Japanese-Korean, ChineseFrench or Japanese-German. Even for language pairs with several parallel corpora, such as Chinese-English and Arabic-English, the size of parallel corpora is still a major limitation for SMT systems to achieve higher performance. In this paper, we present a way which could, to some extent, reduce the parallel data acquisition bottleneck in multilingual language processing. Based on multiling"
W10-4110,P07-2045,0,0.00362542,"Missing"
W10-4110,P08-1113,0,0.0425735,"Missing"
W10-4110,P09-1098,0,0.0330593,"Missing"
W10-4110,2009.mtsummit-wpt.3,1,0.729366,"international application also in Chinese under the PCT. Later on, it may have the patent translated into English and file it in USA patent office, which means the patent becomes bilingual. If the applicant continues to file it in Japan with Japanese, it would be trilingual. Even more, it would be quadrilingual or involve more languages when it is filed in other countries with more languages. Such multilingual patents are considered comparable (or noisy parallel) because they are not parallel in the strict sense but still closely related in terms of information conveyed (Higuchi et al., 2001; Lu et al., 2009). 4 A Large English-Chinese Parallel Corpus Mined from Bilingual Patents In this section, we introduce the English-Chinese bilingual patents harvested from the Web and the method to mine parallel sentences from them. SMT experiments on the final parallel corpus are also described. 4.1 Harvesting English-Chinese Bilingual Patents The official patent office in China is the State Intellectual Property Office (SIPO). In early 2009, by searching on its website, we found about 200K Chinese patents previously filed as PCT applications in English and crawled their bibliographical data, titles, abstrac"
W10-4110,ma-2006-champollion,0,0.291728,"duced in Section 4, followed by the quantity estimation of multilingual patents involving other language pairs in Section 5. We discuss the results in Section 6, and conclude in Section 7. 2 Related Work Parallel sentences could be extracted from parallel documents or comparable corpora. Different approaches have been proposed to align sentences in parallel documents consisting of the same content in different languages based on the following information: a) the sentence length in bilingual sentences (Brown et al. 1991; Gale and Church, 1991); b) lexical information in bilingual dictionaries (Ma, 2006); c) statistical translation model (Chen, 1993), or the composite of more than one approach (Simard and Plamondon, 1998; Moore, 2002). To overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, which raises further challenges since the bilingual contents are not strictly parallel. For instance, Zhao and Vogel (2002) investigated the mining of parallel sentences for Web bilingual news. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maxi"
W10-4110,moore-2002-fast,0,0.149931,"scuss the results in Section 6, and conclude in Section 7. 2 Related Work Parallel sentences could be extracted from parallel documents or comparable corpora. Different approaches have been proposed to align sentences in parallel documents consisting of the same content in different languages based on the following information: a) the sentence length in bilingual sentences (Brown et al. 1991; Gale and Church, 1991); b) lexical information in bilingual dictionaries (Ma, 2006); c) statistical translation model (Chen, 1993), or the composite of more than one approach (Simard and Plamondon, 1998; Moore, 2002). To overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, which raises further challenges since the bilingual contents are not strictly parallel. For instance, Zhao and Vogel (2002) investigated the mining of parallel sentences for Web bilingual news. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Cao et al., (2007) and Lin et al., (2008) proposed two different methods utilizing the parenthesis pattern to"
W10-4110,J05-4003,0,0.0723704,"Missing"
W10-4110,J04-4002,0,0.170472,"Missing"
W10-4110,2007.mtsummit-papers.63,0,0.11045,"sion The websites from which the Chinese and English patents were downloaded were quite slow to access, and were occasionally down during access. To avoid too much workload for the websites, the downloading speed had been limited. Some large patents would cost much time for the websites to respond and had be specifically handled. It took considerable efforts to obtain these comparable patents. In addition our English-Chinese corpus mined in this study is at least one order of magnitude larger, we give some other differences between ours and those introduced in Section 2 (Higuchi et al., 2001; Utiyama and Isahara, 2007; Lu et al, 2009) 1) Their bilingual patents were identified by the priority information in the US patents, and could not be easily extended to language pairs without English; while our method using PCT applications as the pivot could be easily extended to other language pairs as illustrated in Section 5. 2) The translation process is different: their patents were filed in USA Patent Office in English by translating from Japanese or Chinese, while our patents were first filed in English as a PCT application, and later translated into Chinese. The different translation processes may have differ"
W10-4110,J93-1004,0,\N,Missing
W15-3401,I05-3007,0,0.0888065,"Missing"
W15-3401,J86-2003,0,0.0340561,"expanded our appreciation that what constitutes lexical knowledge goes beyond synonymy, hyponymy, metonymy, meronymy, grammatical and other collocations. Furthermore, they are fundamental to a universalistic conceptual base of ontologies and knowledge representation which are often enriched by deeper and newer analysis. In this context, each language foregrounds specific features or nodes within this knowledge base by usually non-uniform means. At the same time, the arrival of the age of Big Data has attracted extensive studies on the actual and dynamic use of language as contextualized (ala. Jakobson 1960) within a given society, especially through the mass media. What are foregrounded in this medium tend to have graded cognitive saliency characterizing members of the common speech community, and such shared knowledge is usually at great variance with the thesaurus approach and show noticeable localized features. It is proposed here that the two kinds of knowledge (thesauric vs cognitive-cultural) complement each other in human cognition, and are integral to it. We draw on two large Chinese media databases Sketch (2.1 billion character tokens1 ) and LIVAC (550 million character tokens2 ) for il"
W19-8714,2007.mtsummit-papers.9,0,0.151538,"Missing"
W19-8714,J07-2003,0,0.0269377,"Missing"
W19-8714,1994.amta-1.26,0,0.469398,"Missing"
Y00-1006,J91-1002,0,0.0812685,"olesome text, distinguishing it from a non-text. Lexical cohesion, including lexical repetition as well as rhetorical continuity in conjunction with other related overt and covert linguistic markers, contributes to textual coherence by creating cohesive ties within the text. If the lexical items in a text can be related to the preceding or to following items, obviously, the text is seen more closely knit together than a text where such relationships do not exist. It has been ascertained that sentences with the greatest degree of lexical cohesion could be used to reflect the textual structure (Morris & Hirst, 1991). Moreover, it is commonly believed that the recurrence of semantically related lexical items across the sentences could be used as an aid to identifying the core sentences which are characterized by their centrality and their expressive power. At the same time, while it is important for readers to be able to trace continuities in the entities under discussion, it is equally important to locate and understand the breaks in continuity. However, little research has demonstrated the functions of lexical cohesion in text segmentation and no computational theory or objective measure has as yet emer"
Y00-1006,P90-1004,0,0.0320771,"Missing"
Y00-1006,H89-1033,0,0.023543,"gly turned to text processing beyond sentence level, partly because text analysis is manifestly necessary, and partly through implicit or explicit endorsement that negotiation of meaning in verbal transactions is achieved within the framework of text. Text has a rich structure in which sentences are grouped and related to one another in a variety of ways. A text is usually taken to be a piece of connected and meaningful sentences, rather than just a collection or sequence of sentences, connected only by contiguity. It is clear that text cannot simply be defined as language above the sentence (Winograd, 1972:65). Nor can we assume that stretches of language use which are isomorphic with sentences are the sole business of the grammarian, for these too are realizations of the text process. To understand a text, one must understand the relations between its parts and determine how the various pieces fit together. Clearly, these parts are not just the individual sentences: rather, sentences are joined together to form larger units, which in their turn may be the building blocks of yet larger units. In information science, there is much ongoing research in finding textual regularities on the basis of"
Y00-1031,1995.tmi-1.6,0,0.0434223,"Missing"
Y00-1031,O97-1013,1,0.703848,"Missing"
Y00-1031,O98-3006,1,\N,Missing
Y02-1003,A97-1018,1,0.891396,"Missing"
Y02-1003,C92-4199,0,0.0253304,"on in names across different Chinese communities constitutes a critical factor in designing Chinese personal name identification algorithm. Keywords: Chinese personal name identification, Chinese word segmentation, Chinese IT applications, Chinese linguistic differences 1 Introduction Automatic identification of Chinese personal names in unrestricted texts plays a key role in Chinese NLP tasks such as word segmentation. Personal name identification in Chinese involves many different issues that are not found in similar tasks in English. Despite extensive research in the issue in recent years (Wang et al. 1992, Song et al. 1993, Sun et al. 1995), many NLP applications still suffer from the weakness of current available performance. The paper has four sections: Section 1 illustrates some IT applications in which Chinese personal name identification plays important roles, to promote an appreciation of the significance of the research. Section 2 introduces the structure of Chinese personal names, and Section 3 discusses the relevant processing strategies. Lastly, we will highlight the significant differences of Chinese personal names between Beijing and Hong Kong in Section 4, showing the added diffic"
Y03-1022,E03-1081,1,\N,Missing
Y09-2038,P93-1002,0,0.524078,"Missing"
Y09-2038,2001.mtsummit-papers.30,0,0.0769612,"Missing"
Y09-2038,P07-2045,0,0.00565641,"n-terms and terms. To build the classifier, we first need to find the useful features for the differentiation of correct term and wrong terms. The features can be categorized as linguistic features and statistic features. Here we introduce the features used by our SVM classifier. 1) Linguistic features Chinese monolingual term candidates (CMC): a binary feature indicating whether the Chinese part of the bilingual pair is a term candidate (Chinese noun/verb phrases mentioned in Sec. 3). 2) Statistic features (the first three features are got by using Moses) Lexical weighting probability (LWP) (Koehn et al., 2007): the probability of lexical translations ϕ (c, e) . The formula is as follows: ϕ (c, e) = log(lex(e |c)) + log(lex(c |e)) where lex(e |c) and lex(c |e) are the lexical weights. CN-&gt;EN phrase translation probability (CEP) (Koehn et al., 2007): a numeric feature ranging from 0 to 1; EN-&gt;CN phrase translation probability (ECP): similar with CEP, but the translation direction is reversed; Frequency ratio (FR): ratio between lower and higher frequency of phrases in the pair: For the SVM classifier, we use LIBSVM (Chang and Lin, 2001), and 5-fold cross-validation is used. Since the data is unbalanc"
Y09-2038,ma-2006-champollion,0,0.0604222,"Missing"
Y09-2038,moore-2002-fast,0,0.0606536,"mean µ and variance σ 2 (Gale and Church, 1991). The parameters µ and σ 2 are estimated on the preliminary sentence pairs obtained in Sec. 4. 2) The dictionary-based score Pd : the score is used to compute the content similarity of the sentence pair based on a bilingual dictionary (Utiyama and Isahara, 2003). For the Chinese-Engish dictionary, we just use the one mentioned in Sec. 4.1. 3) The bidirectional translation probability score Pt (Tran): it combines the translation probability value of both directions (i.e. Chinese-&gt;English and English-&gt;Chinese), instead of using only one direction (Moore, 2002; Chen, 2003). The preliminarily aligned sentences mentioned in Sec. 4 was used as the training data and compute the word alignment probability score given by the default training process of Giza++ (Och and Ney, 2003) 3 4 5 http://champollion.sourceforge.net/ http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm http://www.keenage.com/html/e_index.html 757 To combine the three measures, we just remove some sentence pairs if use Pd or Pl is lower than the corresponding predefined threshold, and use Pt to sort other remaining sentence pairs by descending order. We randomly selected 100 samples from e"
Y09-2038,J03-1002,0,0.0036761,"content similarity of the sentence pair based on a bilingual dictionary (Utiyama and Isahara, 2003). For the Chinese-Engish dictionary, we just use the one mentioned in Sec. 4.1. 3) The bidirectional translation probability score Pt (Tran): it combines the translation probability value of both directions (i.e. Chinese-&gt;English and English-&gt;Chinese), instead of using only one direction (Moore, 2002; Chen, 2003). The preliminarily aligned sentences mentioned in Sec. 4 was used as the training data and compute the word alignment probability score given by the default training process of Giza++ (Och and Ney, 2003) 3 4 5 http://champollion.sourceforge.net/ http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm http://www.keenage.com/html/e_index.html 757 To combine the three measures, we just remove some sentence pairs if use Pd or Pl is lower than the corresponding predefined threshold, and use Pt to sort other remaining sentence pairs by descending order. We randomly selected 100 samples from each of the 12 blocks ranked at the top 240,000 sentence pairs (each block has 20,000 pairs). An annotator classified them into correct (Cor), partially correct (PaC), and incorrect (IC) just as in Sec. 3. The results"
Y09-2038,W04-2207,0,0.0386026,"Missing"
Y09-2038,N03-1033,0,0.00720329,"patents (Lu et al., 2009). Each patent has different parts, i.e. title, abstract, claim, description, etc. we align sentences in each part of comparable patents. The patents were first segmented into sentences according to punctuations, and the detailed statistics for each section are shown in Table 1. Table 1: Statistics for each section #Chinese #English Sections Sentences Sentences Title 7K 7K Abstract 29K 32K Claim 145K 201K Description 557K 840K Total 738K 1,080K English sentences were tokenized and POS-tagged by the POS-tagger developed at Stanford NLP lab (Toutanova and Manning, 2000; Toutanova et al., 2003). The tagger uses Penn Treebank POS tagset, including the tags for content words: JJ (adjective), NN (noun), VB (verb), RB (adverb). Chinese sentences were segmented into words and POS-tagged by using a Chinese lexical analyzer ICTCLAS1. The POS tag set of ICTCLAS contains 22 first-level POS tags, in which the tags for content words include n (noun), v (verb), a (adjective), b (adjectives to describe difference, e.g. 急性(acute) vs 慢性(chronic)), z (adjective to describe status, e.g. 优 良(excellent)), and d (adverb). The numbers of word tokens and types are given in Table 2. Table 2: Word statisti"
Y09-2038,P03-1010,0,0.0259128,"English counterpart S e , and lc and le respectively denote the lengths of S c and S e in terms of the number of words. Three kinds of measures for scoring aligned sentences are introduced as follows. 1) The length-based score Pl (Len): we consider the length ratio between S c and S e has a normal distribution with mean µ and variance σ 2 (Gale and Church, 1991). The parameters µ and σ 2 are estimated on the preliminary sentence pairs obtained in Sec. 4. 2) The dictionary-based score Pd : the score is used to compute the content similarity of the sentence pair based on a bilingual dictionary (Utiyama and Isahara, 2003). For the Chinese-Engish dictionary, we just use the one mentioned in Sec. 4.1. 3) The bidirectional translation probability score Pt (Tran): it combines the translation probability value of both directions (i.e. Chinese-&gt;English and English-&gt;Chinese), instead of using only one direction (Moore, 2002; Chen, 2003). The preliminarily aligned sentences mentioned in Sec. 4 was used as the training data and compute the word alignment probability score given by the default training process of Giza++ (Och and Ney, 2003) 3 4 5 http://champollion.sourceforge.net/ http://projects.ldc.upenn.edu/Chinese/L"
Y09-2038,2007.mtsummit-papers.63,0,0.435514,"Missing"
Y09-2038,1994.amta-1.26,0,0.0604643,"t the framework can well identify correct bilingual terms from comparable patents, and the SVM classifier can further improve its performance. Keywords: Bilingual Term Extraction, Comparable patents, Multi-word Term 1 Introduction Bilingual term extraction is to extract parallel technical terms from bilingual domain-specific corpora, and it is crucial for many NLP fields, such as Machine Translation, bilingual lexicography, cross-language information retrieval, and bilingual ontology extraction. Many researchers have done bilingual term extraction, such as Kupiec (1994), Daille et al. (2004), Wu and Xia (1994), Vintar(2001), Piperidis and Harlas (2006), Ha et al. (2008). Previous studies focused on mining bilingual terms from a bilingual sentence-level parallel corpus. However, obtaining a large-scale bilingual sentence-level parallel corpus is very expensive while it is easy to obtain a bilingual comparable corpus. This paper aims to mine bilingual terms from a bilingual comparable corpus in patent domain, and present a novel framework to extract bilingual terms in comparable patents. Patents, which contain a large amount of technical terms, could be one of the major sources for term extraction. I"
Y09-2038,W00-1308,0,\N,Missing
Y09-2038,J93-2003,0,\N,Missing
Y09-2038,W02-1802,1,\N,Missing
Y09-2038,C94-1084,0,\N,Missing
Y09-2038,2009.mtsummit-wpt.3,1,\N,Missing
Y12-1003,J86-2003,0,0.678284,"Missing"
Y99-1032,J94-4002,0,0.100029,"Missing"
Y99-1032,C94-2189,0,0.0455636,"Missing"
Y99-1032,P90-1004,0,0.0278141,"Missing"
Y99-1032,C94-2191,0,\N,Missing
