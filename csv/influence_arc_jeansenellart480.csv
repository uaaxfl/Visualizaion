2003.mtsummit-papers.45,2001.mtsummit-papers.56,1,0.732828,"Missing"
2003.mtsummit-papers.45,2001.mtsummit-papers.39,0,\N,Missing
2003.mtsummit-semit.6,2003.mtsummit-papers.46,1,0.804058,"Missing"
2007.mtsummit-papers.59,2003.mtsummit-papers.15,0,0.0402186,"hort span of time. Developing a translation engine of a new language pair for a rule-based system (such as SYSTRAN’s) requires much effort in terms of manpower and duration. This effort can of course be reduced by using a strong modular approach and powerful tools for the construction of linguistic resources. To the contrary, proponents of statistics-based Machine Translation claim they are able to build a new engine overnight, provided a reasonable amount of training data is available (Koehn, 2005; Och & Ney, 2000), yet further studies proved this approach is not so easy to implement either (Foster et al., 2003). During the year 2006, SYSTRAN decided to embark on an ambitious project: to develop a set of 12 new language pairs in less than a year, and to do this with a minimal team of linguists and developers. The systems were to leverage the existing code and resources by further enhancing the modularity of SYSTRAN’s code. A radically new approach of lexical resources also had to be designed in order to rapidly build and maintain 12 bilingual dictionaries under the form of a unique multisource - multitarget dictionary. And finally, we wanted these translators to interface with a statistical approach:"
2007.mtsummit-papers.59,2006.amta-panels.3,0,0.0612251,"Missing"
2007.mtsummit-papers.59,2005.mtsummit-papers.11,0,0.147968,"anslation systems is the ability to easily add a new language pair to its already existing offer in a very short span of time. Developing a translation engine of a new language pair for a rule-based system (such as SYSTRAN’s) requires much effort in terms of manpower and duration. This effort can of course be reduced by using a strong modular approach and powerful tools for the construction of linguistic resources. To the contrary, proponents of statistics-based Machine Translation claim they are able to build a new engine overnight, provided a reasonable amount of training data is available (Koehn, 2005; Och & Ney, 2000), yet further studies proved this approach is not so easy to implement either (Foster et al., 2003). During the year 2006, SYSTRAN decided to embark on an ambitious project: to develop a set of 12 new language pairs in less than a year, and to do this with a minimal team of linguists and developers. The systems were to leverage the existing code and resources by further enhancing the modularity of SYSTRAN’s code. A radically new approach of lexical resources also had to be designed in order to rapidly build and maintain 12 bilingual dictionaries under the form of a unique mul"
2007.mtsummit-papers.59,2001.mtsummit-road.6,0,0.123895,"Missing"
2007.mtsummit-papers.59,P02-1040,0,0.0850617,"Missing"
2009.mtsummit-posters.17,2008.iwslt-papers.1,0,0.0236257,"oder is able to output the phrase and word alignments. This would speed up the process of creating the adapted SMT system since we skip the timeconsuming word alignment performed by GIZA++. Source AFP APW ASB HYT NHR UMH XIN Arabic 145M 7M 175M 188M 1M 58M French 570M 200M - Table 4: Characteristics of the available monolingual Gigaword corpora (number of words). It could also be that the decoder-induced word alignments are more appropriate than those performed by GIZA++. This was partially investigated in the framework of pivot translation to produce artificially bitexts in another language (Bertoldi et al., 2008). Finally, instead of only using the 1-best translation we could also use the n-best list. LDC’s Arabic and French Gigaword corpora are described in Table 4. There is only one source that does exist in both languages: the AFP collection. It is likely that the Arabic and French texts partially cover the same facts, but they are usually not direct translations.6 In fact, we were informed that journalists at AFP have the possibility to freely change the sentences when they report on a fact based on text already available in another language. Nevertheless, it can be expected that using these texts"
2009.mtsummit-posters.17,W08-0309,0,0.0498661,"Missing"
2009.mtsummit-posters.17,P08-2040,0,0.399797,"e probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language"
2009.mtsummit-posters.17,W07-0722,0,0.0834673,"ual data. One can distinguish two types of translation model adaptation: first, adding new source words or/and new translations to the model; and second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for i"
2009.mtsummit-posters.17,W07-0717,0,0.203525,"nd second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in"
2009.mtsummit-posters.17,W08-0509,0,0.0246917,"approached by pivoting through English, but we don’t have comparable BLEU scores for this kind of approach. 3 Baseline system The baseline system is a standard phrase-based SMT system based on the the Moses SMT toolkit (Koehn et al., 2007). It uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. It constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).4 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. All the bitexts were concatenated. The parameters of Moses are tuned on the development data using the CMERT tool. 4 The source is available at http://www.cs.cmu.edu/ ˜qing/ 3.1 Tokenization There is a large body of work in the literature showing that a morphological decomposition of the Arabic words can improve the word coverage and by these means the translation quality, see for instance (Habash and Sadat"
2009.mtsummit-posters.17,N06-2013,0,0.062687,"and Vogel, 2008).4 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. All the bitexts were concatenated. The parameters of Moses are tuned on the development data using the CMERT tool. 4 The source is available at http://www.cs.cmu.edu/ ˜qing/ 3.1 Tokenization There is a large body of work in the literature showing that a morphological decomposition of the Arabic words can improve the word coverage and by these means the translation quality, see for instance (Habash and Sadat, 2006). It is clear that such a decomposition is most helpful when the translation model training data is limited, but this is less obvious for tasks where several hundreds of millions of words of bitexts are available. Most of the published work is based on the freely available tools, like the Buckwalter transliterator and the MADA and TOKAN tools for morphological analysis from Columbia University. In this work, we compare two different tokenization of the Arabic source text: a full word mode and the morphological decomposition provided by the sentence analysis module of SYSTRAN’s rulebased Arabic"
2009.mtsummit-posters.17,hasan-ney-2008-multi,0,0.0458589,"ds Vocab Words Vocab DGA T RAMES 262k 30k 400k 18k News 1.1M 67k 1.3M 41k commentary UN 149M 712k 212M 420k Table 1: Characteristics of the available bitexts The DGA also provided a test set that was created in the same way than the in-domain bitexts. Four high-quality reference translations are available. We randomly split this data into a development set for system tuning and an internal test set. The details of the development and test set are given in Table 2. We are only aware of one other large Arabic/French news translation system, the one that was developed during the T RAMES project (Hasan and Ney, 2008). In that work, results are reported on the same test set, but different bitexts for training were 1 Traduction Automatique par M´ethodes Statistiques Direction g´en´erale de l’armement 3 http://www.project-syndicate.org 2 Dev data: Sentences Words Test data: Sentences Words Arabic French 235 9931 940 (4x) 45038 231 10246 924 (4x) 47066 Table 2: Characteristics of the development and test set. used, namely the T RAMES bitexts, UN data from the period 2001 to April 2007, archives of Amnesty International and articles from Le Monde Diplomatique. The authors report a BLEU score of 41.1 on the who"
2009.mtsummit-posters.17,2005.eamt-1.19,0,0.0862915,"guage modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language (Snover et al., 2008). Cross-lingual information retrieval was applied to find texts in the target language that are related to the domain of the source texts. However, it was difficult to get the alignments between the source and target phrases and an over-generalizing IBM1-style approach was used. Another direction of research is self-enhancing of the translation model. This was first proposed by (Ueffing, 2006). The idea is to translate the test data, to filter t"
2009.mtsummit-posters.17,W07-0733,0,0.161708,"ation: first, adding new source words or/and new translations to the model; and second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applie"
2009.mtsummit-posters.17,P07-2045,0,0.00472637,"Missing"
2009.mtsummit-posters.17,2008.iwslt-papers.6,1,0.816939,"obably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing, 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008). A somehow related approach was named lightlysupervised training (Schwenk, 2008). In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. We could obtain small improvements in the BLEU score in a French/English translation system. Although this technique seems to be close to self enhancing as proposed by (Ueffing, 2006), there is a conceptual difference. We do not use the test data to adapt the translation model, but large amounts of monolingual training data in the source language and we create a complete new model that can be applied to any test data without additional mod"
2009.mtsummit-posters.17,D08-1090,0,0.118412,"This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language (Snover et al., 2008). Cross-lingual information retrieval was applied to find texts in the target language that are related to the domain of the source texts. However, it was difficult to get the alignments between the source and target phrases and an over-generalizing IBM1-style approach was used. Another direction of research is self-enhancing of the translation model. This was first proposed by (Ueffing, 2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with t"
2009.mtsummit-posters.17,2006.iwslt-papers.3,0,0.515306,"mation retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language (Snover et al., 2008). Cross-lingual information retrieval was applied to find texts in the target language that are related to the domain of the source texts. However, it was difficult to get the alignments between the source and target phrases and an over-generalizing IBM1-style approach was used. Another direction of research is self-enhancing of the translation model. This was first proposed by (Ueffing, 2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be"
2009.mtsummit-posters.17,P07-1004,0,0.353323,"ble that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing, 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008). A somehow related approach was named lightlysupervised training (Schwenk, 2008). In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. We could obtain small improvements in the BLEU score in a French/English translation system. Although this technique seems to be close to self enhancing as proposed by (Ueffing, 2006), there is a conceptual difference. We do not use the t"
2009.mtsummit-posters.17,C04-1059,0,0.0987882,"slation model adaptation: first, adding new source words or/and new translations to the model; and second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recentl"
2009.mtsummit-posters.17,W07-0718,0,\N,Missing
2009.mtsummit-posters.6,E06-1032,1,0.866806,"Missing"
2009.mtsummit-posters.6,W07-0718,1,0.820804,"translation Given a certain quality of dictionary, we now face the question of how much translation quality benefits from these dictionaries. We evaluate translation quality with an automatic metric (Papineni et al., 2002) and human judgement. Although the BLEU metric has been shown to be unreliable (CallisonBurch et al., 2006) for comparing systems of so different architecture as rule-based and statistical systems, this does not discard its use for comparing two versions of a given system. As far as human judgement is concerned, in accordance with the findings of recent evaluation campaigns (Callison-Burch et al., 2007), we choose to rely on a ranking of the overall quality of competing outputs. In addition to evaluation, we also perform a human error analysis on a random sample of a hundred sentences. This task consists of comparing the translation output when adding all the extracted rules with the baseline translation and trying to identify reasons for possible deteriorations or improvements. 6 Experiments and results We describe here experiments for both the dictionary extraction and the translation aspects. 6.1 Dictionary extraction Our basic dictionary extraction configuration follows the pipeline desc"
2009.mtsummit-posters.6,C94-1084,0,0.246147,"Missing"
2009.mtsummit-posters.6,W07-0732,1,0.79859,"ng the first to describe a pipeline for extracting dictionaries of noun compounds. Koehn (2003) gives a thorough investigation of the topic of noun phrase translation and extraction of noun phrase lexicons in particular. As far as extraction is concerned, one variation between the different approaches lies in the choice of either extracting all monolingual terms to then find alignments or align chunks of raw text (typically, extract a phrase table) which is then filtered to keep only the syntactically meaningful ones. Daille (1994) and Kumano (1994) belong to the first category, while Itagaki (2007) belongs to the second category. Another variation is the choice of confidence measures to evaluate the quality of a candidate entry. As far as application of such dictionaries is concerned, Melamed (1997) uses mutual information as the ob2 This time, the word ”phrase” is understood as a syntactic constituent. jective function maximized during the learning process. Font Llitjos et al. (2007) describes a semiautomatic procedure to extract new dictionary entries in a rule-based system. This however deals with a small number of entries and necessitates a manual review. Itagaki (2007) presents a f"
2009.mtsummit-posters.6,W08-0328,0,0.0190113,"first explain what motivates a need for bilingual phrase dictionaries. This motivation is twofold. Previous experiments indicated that most of the improvements of a statistical phrase-based layer in a combination with a rule-based system came from lexical changes, most of them phrasal expressions. Then, the distribution of bilingual phrases in terms of phrase length and the nature and availability of manually written phrasal bilingual dictionaries for a given domain is an incentive for corpus extraction. The first argument is illustrated both by a combination of different rule-based systems (Eisele et al., 2008) and a statistical post-editing layer over a rulebased output (Simard et al., 2007), along with some qualitative analysis (Dugast et al., 2007). As for the second argument, in the past few years, statistical machine translation moved from wordbased models to phrase-based1 models. In a more linguistic approach, Bannard (2006) discusses the 1 Here, the word ”phrase” simply denotes any sequence of words, not necessarily a constituent. English big park private bank left bank fig leaf fraud scandal freight traffic to let off steam 1 2 3 4 5 6 7 French grand parc banque priv´ee rive gauche feuille d"
2009.mtsummit-posters.6,P03-1057,0,0.0616347,"Missing"
2009.mtsummit-posters.6,N03-1017,1,0.0156547,"Missing"
2009.mtsummit-posters.6,C94-1009,0,0.0770438,"Missing"
2009.mtsummit-posters.6,P93-1003,0,0.0425527,"rule-based system. A first reason lies in the ability to capture local context to disambiguate the translation (as in examples 1-2 of Table 1). Then, there are phrases that cannot be translated word-for-word, such as examples 4 and 5. And finally, some strong collocations may reduce the syntactic ambiguity of the source sentence (examples 6 and 7). We however probably do not need entries such as big park in Table 1, for its translation into French is compositional, little ambiguous and the English phrase big park could be easily modified into very big park, big natural park. 1.2 Related work Kupiec (1993) is among the first to describe a pipeline for extracting dictionaries of noun compounds. Koehn (2003) gives a thorough investigation of the topic of noun phrase translation and extraction of noun phrase lexicons in particular. As far as extraction is concerned, one variation between the different approaches lies in the choice of either extracting all monolingual terms to then find alignments or align chunks of raw text (typically, extract a phrase table) which is then filtered to keep only the syntactically meaningful ones. Daille (1994) and Kumano (1994) belong to the first category, while I"
2009.mtsummit-posters.6,W07-0410,0,0.0268455,"Missing"
2009.mtsummit-posters.6,W97-0311,0,0.0366792,"lexicons in particular. As far as extraction is concerned, one variation between the different approaches lies in the choice of either extracting all monolingual terms to then find alignments or align chunks of raw text (typically, extract a phrase table) which is then filtered to keep only the syntactically meaningful ones. Daille (1994) and Kumano (1994) belong to the first category, while Itagaki (2007) belongs to the second category. Another variation is the choice of confidence measures to evaluate the quality of a candidate entry. As far as application of such dictionaries is concerned, Melamed (1997) uses mutual information as the ob2 This time, the word ”phrase” is understood as a syntactic constituent. jective function maximized during the learning process. Font Llitjos et al. (2007) describes a semiautomatic procedure to extract new dictionary entries in a rule-based system. This however deals with a small number of entries and necessitates a manual review. Itagaki (2007) presents a filtering method for candidate entries using a Gaussian Mixture Model classifier trained on human judgements of the quality of a dictionary entry, but does not provide evaluation of final translation qualit"
2009.mtsummit-posters.6,P02-1040,0,0.0914537,"added (contiguous) phrasal rules may disable original rules and/or hurt the dependency analysis. Thus, such a verbal expression as make good progress that may have been correctly translated would then be mistranslated once the phrasal entry good progress is added to the rules’ base. A noun phrase such as ”rapid and sound progress” may also get mistranslated from adding sound progress as a contiguous noun phrase, as illustrated on figure 2. Therefore the problem consists of building the optimal subset from the set of candidate entries, according to a translation evaluation metric (here, BLEU (Papineni et al., 2002)), while being constrained by the deterministic firing of these rules. As an approximate (suboptimal) response to this problem, we test each extracted entry individually, starting from the lower n-grams to the longer (source) chunks, following Algorithm 1. For each sentence pair where the entry (of source span N) fires, the translation score (sentence level BLEU) when adding this rule is compared with the baseline translation. Rules showing only a single improved for n=1 to NgramMax do map all n-gram (length of the source phrase) entries to parallel sentences translate training corpus with cur"
2009.mtsummit-posters.6,P05-1034,0,0.0767182,"Missing"
2009.mtsummit-posters.6,2003.mtsummit-papers.46,1,0.731028,"uality of a dictionary entry, but does not provide evaluation of final translation quality. The closest work from what we describe in the present paper might be the one by Imamura (2003), in which examplebased pattern rules are filtered using an automatic evaluation of the final translation output. In the work presented here, we describe two independent training steps that first extract dictionary candidates and then automatically validate them directly within the RBMT system. 2 Dictionary extraction 2.1 Manual coding of entries The SYSTRAN rule-based system provides a dictionary coding tool (Senellart et al., 2003) that allows the manual task of coding entries to be partially automated thanks to the use of monolingual dictionaries (Table 2), morphological guess rules and probabilistic context-free local grammars (Table 3). For example, the second rule illustrated in the latter table simply describes how an English noun phrase may be composed of a adjective+noun sequence. The general rule has a phrase to inherit inflection and semantic (we won’t mention this aspect here, as it is of little significance) features from the headword. The coding tool also allows the user to fine-tune it by correcting the aut"
2009.mtsummit-posters.6,N07-1064,0,0.0215162,"ng the first to describe a pipeline for extracting dictionaries of noun compounds. Koehn (2003) gives a thorough investigation of the topic of noun phrase translation and extraction of noun phrase lexicons in particular. As far as extraction is concerned, one variation between the different approaches lies in the choice of either extracting all monolingual terms to then find alignments or align chunks of raw text (typically, extract a phrase table) which is then filtered to keep only the syntactically meaningful ones. Daille (1994) and Kumano (1994) belong to the first category, while Itagaki (2007) belongs to the second category. Another variation is the choice of confidence measures to evaluate the quality of a candidate entry. As far as application of such dictionaries is concerned, Melamed (1997) uses mutual information as the ob2 This time, the word ”phrase” is understood as a syntactic constituent. jective function maximized during the learning process. Font Llitjos et al. (2007) describes a semiautomatic procedure to extract new dictionary entries in a rule-based system. This however deals with a small number of entries and necessitates a manual review. Itagaki (2007) presents a f"
2010.amta-papers.2,P05-1032,0,0.0738012,"Missing"
2010.amta-papers.2,2009.mtsummit-papers.7,1,0.809711,"Missing"
2010.amta-papers.2,D07-1104,0,0.0123509,"ring methods using a database. They report speeds of ”8 seconds to compare 419 query sentences against 1497 reference sentences”, and ”1.5 seconds per query sentence” with a larger corpus, which is a few orders of magnitude slower than our method. Suffix arrays have been applied to a related problem in machine translation, namely looking up phrases in a word-aligned parallel corpus to compute phrase translation probabilities. Work by CallisonBurch et al. (2005); Zhang and Vogel (2005); McNamee and Mayfield (2006) was extended to socalled hierarchical phrases, essentially phrases with gaps, by Lopez (2007). 4 Suffix Arrays Our method uses n-gram matches between the input segment (the pattern) and the translation memory (the corpus) to identify potential candidate corpus segments. We store the corpus in a suffix array to enable quick lookup. The data structure uses an index of starting positions of all suffixes in the corpus, which is sorted alphabetically (see Figure 2). This allows us to use binary search to find a particular suffix in the corpus. We sort the index using quick sort, which is O(n log n). Our implementation takes a few seconds even for corpora with tens of millions of words. We"
2010.amta-papers.2,2002.tmi-tutorials.1,0,0.0159293,"ltering stage. There are various ways to process the pattern, for instance splitting it into sub-patterns for which exact matching is performed, or compiling it into a finite state machine. Our method utilizes exact matches of sub-patterns. The dynamic programming techniques can be improved in many ways. To give an example, in the canonical algorithm, we do not need to compute the entire matrix but can focus on the alignment points with the lowest cost. The only description of a method addressing the approximate string matching problem in translation memories, that we are aware of, is work by Mandreoli et al. (2002), which uses simple filtering methods using a database. They report speeds of ”8 seconds to compare 419 query sentences against 1497 reference sentences”, and ”1.5 seconds per query sentence” with a larger corpus, which is a few orders of magnitude slower than our method. Suffix arrays have been applied to a related problem in machine translation, namely looking up phrases in a word-aligned parallel corpus to compute phrase translation probabilities. Work by CallisonBurch et al. (2005); Zhang and Vogel (2005); McNamee and Mayfield (2006) was extended to socalled hierarchical phrases, essential"
2010.amta-papers.2,2006.amta-papers.12,0,0.0167637,"em in translation memories, that we are aware of, is work by Mandreoli et al. (2002), which uses simple filtering methods using a database. They report speeds of ”8 seconds to compare 419 query sentences against 1497 reference sentences”, and ”1.5 seconds per query sentence” with a larger corpus, which is a few orders of magnitude slower than our method. Suffix arrays have been applied to a related problem in machine translation, namely looking up phrases in a word-aligned parallel corpus to compute phrase translation probabilities. Work by CallisonBurch et al. (2005); Zhang and Vogel (2005); McNamee and Mayfield (2006) was extended to socalled hierarchical phrases, essentially phrases with gaps, by Lopez (2007). 4 Suffix Arrays Our method uses n-gram matches between the input segment (the pattern) and the translation memory (the corpus) to identify potential candidate corpus segments. We store the corpus in a suffix array to enable quick lookup. The data structure uses an index of starting positions of all suffixes in the corpus, which is sorted alphabetically (see Figure 2). This allows us to use binary search to find a particular suffix in the corpus. We sort the index using quick sort, which is O(n log n"
2010.amta-papers.2,steinberger-etal-2006-jrc,0,0.116149,"Missing"
2010.amta-papers.2,2005.eamt-1.39,0,0.0243755,"te string matching problem in translation memories, that we are aware of, is work by Mandreoli et al. (2002), which uses simple filtering methods using a database. They report speeds of ”8 seconds to compare 419 query sentences against 1497 reference sentences”, and ”1.5 seconds per query sentence” with a larger corpus, which is a few orders of magnitude slower than our method. Suffix arrays have been applied to a related problem in machine translation, namely looking up phrases in a word-aligned parallel corpus to compute phrase translation probabilities. Work by CallisonBurch et al. (2005); Zhang and Vogel (2005); McNamee and Mayfield (2006) was extended to socalled hierarchical phrases, essentially phrases with gaps, by Lopez (2007). 4 Suffix Arrays Our method uses n-gram matches between the input segment (the pattern) and the translation memory (the corpus) to identify potential candidate corpus segments. We store the corpus in a suffix array to enable quick lookup. The data structure uses an index of starting positions of all suffixes in the corpus, which is sorted alphabetically (see Figure 2). This allows us to use binary search to find a particular suffix in the corpus. We sort the index using q"
2010.jec-1.4,J07-2003,0,0.165786,"Missing"
2010.jec-1.4,W07-0732,1,0.857108,"y automatically analyzing translated text and learning the rules. SMT has been embraced by the academic and commercial research communities as the new dominant paradigm in machine translation. Almost all recently published papers on machine translation are published on new SMT techniques. The methodology has left the research labs and become the basis of successful companies such as Language Weaver and the highly visible Google and Microsoft web translation services. Even traditional rule-based companies such as Systran have embraced statistical methods and integrated them into their systems (Dugast et al., 2007). The two technologies have not touched much in the past not only because of the different development communities (software suppliers to translation agencies vs. mostly academic research labs). Another factor is that TM and SMT have recently addressed different translation challenges. While TM have addressed the need of translation agencies to produce high-quality translations of often repetitive material, SMT has set itself the challenge of open domain translations such as news stories and is mostly satisfied with translation quality that is good enough for gisting, i.e., transmitting the me"
2010.jec-1.4,2009.iwslt-papers.4,1,0.28367,"airs: • • • • • • 28 ( the ; les ) ( the big ; les gros ) ( the big fish ; les gros poissons ) ( big ; gros ) ( big fish ; gros poissons ) ( fish ; poissons ) ( the X fish ; les X poissons ) The symbol X is called a non-terminal, since the translation rule is viewed as a synchronous contextfree grammar rule. In essence it is a place-holder for recursively nested sub-phrases. Hierarchical rules require a different decoding algorithm that is typically drawn from syntactic parsing methods, but otherwise use a very similar training, tuning, and testing pipeline as traditional phrase-based models (Hoang et al., 2009). 4.2 TM Matches as Very Large Rules How does that relate to our XML frames? Recall the XML frame that we constructed in Section 2.1: <A` l’ article&gt; 21 <, le texte du deuxi´eme alin´ea est supprim´e .&gt; Instead of replacing the source sentence The second paragraph of Article 21 is deleted . with the XML frame, we can rewrite this frame as a hierarchical phrase rule and provide it to a hierarchical decoder: ( The second paragraph of Article X is deleted . ; A` l’ article X , le texte du deuxi´eme alin´ea est supprim´e . ) In practice, hierarchical models do not use such large rules (and keep in"
2010.jec-1.4,2009.mtsummit-papers.7,1,0.85313,"all mismatched source words are inserted, all TM target words aligned to mismatched TM source words are removed, if the alignment to the target words fails, go to previous word and follow its alignment. Acquis segments English words French words Product segments English words French words Corpus 1,165,867 24,069,452 25,533,259 Test 4,107 129,261 135,224 Corpus 83,461 1,038,762 1,110,284 Test 2,000 24,643 26,248 Table 1: Statistics of the corpus used in experiments (Product) and the English–French part of the publicly available JRC-Acquis corpus1 (Acquis), for which we use the same test set as Koehn et al. (2009). See Table 1 for basic corpus statistics. The Acquis corpus is a collection of laws and regulations that apply to all member countries of the European Union. It has more repetitive content than the parallel corpora that are more commonly used in machine translation research. Still, the commercial 1 24 http://wt.jrc.it/lt/Acquis/ (Steinberger et al., 2006) Product corpus is more representative of the type of data used in TM systems. It is much smaller (around a million words), with shorter segments (average 12 words per segments). We are especially interested in the performance of the methods"
2010.jec-1.4,J10-4005,0,0.0110158,"uals, or several drafts of legislation), being able to find existing translations of segments of the source language text, alleviates the need to carry out redundant translation. In addition, finding close matches (so-called fuzzy matches), may dramatically reduce the translation workload. Various commercial vendors offer TM software and the technology is in wide use by translation agencies. Instead of building machine translation systems by manually writing translation rules, SMT sysJean Senellart Systran La Grande Arche 1, Parvis de la D´efense 92044 Paris, France senellart@systran.fr tems (Koehn, 2010) are built by fully automatically analyzing translated text and learning the rules. SMT has been embraced by the academic and commercial research communities as the new dominant paradigm in machine translation. Almost all recently published papers on machine translation are published on new SMT techniques. The methodology has left the research labs and become the basis of successful companies such as Language Weaver and the highly visible Google and Microsoft web translation services. Even traditional rule-based companies such as Systran have embraced statistical methods and integrated them in"
2010.jec-1.4,D07-1104,0,0.0188109,"cond paragraph of Article 21 is deleted . with the XML frame, we can rewrite this frame as a hierarchical phrase rule and provide it to a hierarchical decoder: ( The second paragraph of Article X is deleted . ; A` l’ article X , le texte du deuxi´eme alin´ea est supprim´e . ) In practice, hierarchical models do not use such large rules (and keep in mind, this particular rule is drawn from a relatively short sentence, thus containing few words and only one non-terminal). But this is purely due to scaling issues and concerns about the size of the rule table. The issues can be resolved. In fact, Lopez (2007) presented a method to compute very large translation rules on the fly for hierarchical models. While these rules were limited to two non-terminals, they could contain any number of words — a very similar situation to our XML frames. November 4th , 2010 Philipp Koehn and Jean Senellart Acquis Figure 5: TM matches as very large rules (VLR): Encoding TM match frames as very large hierarchical grammar rules (VLR) outperforms all previous methods. 4.3 Results We train a hierarchical phrase-based model with Moses, which has very similar performance as the phrase-based model used in the previous exp"
2010.jec-1.4,W99-0604,0,0.0182397,"Missing"
2010.jec-1.4,2009.mtsummit-papers.14,0,0.207381,"Missing"
2010.jec-1.4,P10-1063,0,0.0126719,"Missing"
2010.jec-1.4,2009.mtsummit-papers.16,0,0.0278386,"Missing"
2010.jec-1.4,W10-3806,0,0.116613,"Missing"
2010.jec-1.4,C10-2043,0,\N,Missing
2010.jec-1.4,steinberger-etal-2006-jrc,0,\N,Missing
2010.jec-1.4,P07-2045,1,\N,Missing
2011.iwslt-evaluation.15,2011.iwslt-evaluation.16,1,0.746987,"led in the Quaero program is 1 http://www.quaero.org spoken language translation (SLT). In this work, the 2011 project-internal evaluation campaign on SLT is described. The campaign focuses on the language pair German-French in both directions, and both human and automatic transcripts of the spoken text are considered as input data. The automatic transcripts were produced by the Rover combination of single-best output of the best submission from each of the three sites participating in the internal 2010 automatic speech recognition (ASR) evaluation, which is described in an accompanying paper [1]. The campaign was designed and conducted by DGA and compares the different approaches taken by the four participating partners RWTH, KIT, LIMSI and SYSTRAN. In addition to publicly available data, monolingual and bilingual corpora collected in the Quaero program were used for training and evaluating the systems. The approaches to machine translation taken by the partners differ substantially. KIT, LIMSI and RWTH apply statistical techniques to perform the task, whereas SYSTRAN uses their commercial rule-based translation engine. KIT makes use of a phrase-based decoder augmented with partof-sp"
2011.iwslt-evaluation.15,P02-1040,0,0.0815552,"s been built from the test sets of the previous years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized"
2011.iwslt-evaluation.15,2006.amta-papers.25,0,0.0225424,"evious years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized by the average length of the references."
2011.iwslt-evaluation.15,J05-4003,0,0.0172208,"asing variant and change the case as required to be able to translate it. Some of the available data contains a lot of noise. The Giga corpus, for example, includes a large amount of noise such as non-standardized HTML characters. Also, the Bookshop and Presseurop corpora contain truncated lines, which do not match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built"
2011.iwslt-evaluation.15,2007.tmi-papers.21,0,0.0128424,"ot match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words"
2011.iwslt-evaluation.15,W09-0435,1,0.688116,"Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using"
2011.iwslt-evaluation.15,P07-2045,0,0.00836467,"generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We ad"
2011.iwslt-evaluation.15,W11-2145,1,0.808022,"oolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingua"
2011.iwslt-evaluation.15,W11-2124,1,0.82441,"g the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grain"
2011.iwslt-evaluation.15,W05-0836,1,0.88503,"ng model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering"
2011.iwslt-evaluation.15,C08-1098,0,0.0239782,"kit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering rules and lattice phrase extraction. Using the POS-based language model led to a big improvement. 3.2. LIMSI LIMSI’s participation in Quaero 2011 evaluation campaign was focused on the translation of German from and into French. The adaptation of our text translation system to speech inputs is mostly performed in preprocessing, aimed at removing dysflu"
2011.iwslt-evaluation.15,N04-4026,0,0.0164881,"slation system based on bilingual n-grams. N-code overview N-code’s translation model implements a stochastic finite-state transducer (FST) trained using an n-gram model (source,target) pairs. The training requires source-side sentence reorderings to match the target word order, also performed by a stochastic FST reordering model, which uses POS information to generalize reordering patterns beyond lexical regularities. Complementary to the translation model, ten more features are used in a linear scoring function: a target-language model; four lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-"
2011.iwslt-evaluation.15,P03-1021,0,0.0157097,"ur lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagg"
2011.iwslt-evaluation.15,P10-1052,1,0.744516,"the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the r"
2011.iwslt-evaluation.15,D09-1022,1,0.784658,"al machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The German and French data submitted by SYSTRAN were obtained by the SYSTRAN baseline engine, being traditionally classified as a rule-based system. However, over the decades, its devel"
2011.iwslt-evaluation.15,2010.iwslt-papers.6,0,0.0142765,"a.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6"
2011.iwslt-evaluation.15,C00-2162,1,0.678983,"sed tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6-gram were insignificant). Using the neural language model led to (small but consistent) improvements in all tasks. With the help of system combination, we combined the hypoth"
2011.iwslt-evaluation.15,2008.iwslt-papers.8,1,0.818685,"the pipeline was unchanged as compared to text translations. For the Quaero 2011 evaluation RWTH utilized state-ofthe-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA [24] was employed to train word alignments, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 1"
2011.iwslt-evaluation.15,E06-1005,1,0.810679,"ents, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The Ger"
2011.iwslt-evaluation.15,J03-1002,1,\N,Missing
2011.iwslt-evaluation.15,W11-2135,1,\N,Missing
2011.mtsummit-papers.17,W09-0401,0,0.0262197,"(Specia, 2011) or the comparison with human translation (Plitt and Masselot, 2010). Other approaches take into account “user activity data” covering keystrokes (Barrett et al., 2001) or eye movement detection (Doherty et al., 2010). Implicitly, estimating PE effort is the driver for establishing better quality evaluation metrics. For instance HTER (Snover et al., 2006) calculating translation edit rate towards targeted reference translation provides a reproducible metric, well correlated with human judgment on translation quality and close by deﬁnition to “translation post-editing”. In WMT09, Callison-Burch et al. (2009) introduced a new task: editing to evaluate translation where the edited translation is not used as a reference nor the reviewer asked to perform the least number of edits, but to make the translation ﬂuent without access to reference translation. The edited translation is then evaluated in a second phase of the evaluation task. However, the result of this task is not conclusive due to the variability between posteditors, and no strong correlation is observed with sentence quality judgment. With METEOR, Lavie and Agarwal (2007) introduced the possibility of evaluating quality based on intuitiv"
2011.mtsummit-papers.17,W07-0732,1,0.921397,"and is therefore naturally suited for HTER evaluation, however in our approach, translation edit rate based on “mechanical edits” count is just an intermediate analysis to expose “logical edits” taking into account part of speech, lemmatization, and constituent structure of the sentences. 1.3 Can we reduce the effort? Beyond analysis, the general problem is how PE effort can be reduced. Multiple approaches can be quoted for that purpose: Guzm´an (2007) describes a set-up where MT output passes through a set of PE rules designed to smooth out translation output for a highly customized system. Dugast et al. (2007) and Simard et al. (2007) describe a set-up where an SMT system is trained on a bilingual corpus constituted with both MT output and human reference, and show how the sys165 tem learn how to “correct the translation output”. Schwenk et al. (2009) reproduce this with a Statistical Post-Editing (SPE) system trained on very large corpus making the initial translation as a mere preprocessing. In both cases, the SMT system beneﬁts from higher similarity between pretranslated text and reference compared to source and reference; however, if the ﬁnal quality is higher, the system does not learn post-e"
2011.mtsummit-papers.17,2005.eamt-1.13,0,0.0661961,"Missing"
2011.mtsummit-papers.17,P07-2045,0,0.00327859,"y four different professional translators, who were French native speakers (Plitt and Masselot, 2010). The post-editors were provided with simple PE guidelines to produce publishable quality at the lowest effort, avoiding changes due to stylistic or personal preferences. The post-editors are presented once sentence at a time, in the same order in which they appear in the original source document, without any further functionality supporting the PE activity (e.g. no terminology lookup). Some of the PE tasks used MT outputs generated with a Moses engine, an SMT system trained on in-domain data (Koehn et al., 2007), others with the SYSTRAN system. Note that the post-editors were not informed which MT system was used. Although our aim was not to compare RBMT versus SMT, it was interesting to note that our approach applies equally on both system outputs. 4.1 Human Baseline A subset of 100 sentences (the baseline) was tagged manually using XML format as shown in ﬁgure 2. The aim is to compare our automatic results to this reference analysis. Table 1 describes the human analysis of 100 sentences: in these sentences each PEA has been classiﬁed according to the previous typology. The left part corresponds to"
2011.mtsummit-papers.17,W07-0734,0,0.0303739,"nd close by deﬁnition to “translation post-editing”. In WMT09, Callison-Burch et al. (2009) introduced a new task: editing to evaluate translation where the edited translation is not used as a reference nor the reviewer asked to perform the least number of edits, but to make the translation ﬂuent without access to reference translation. The edited translation is then evaluated in a second phase of the evaluation task. However, the result of this task is not conclusive due to the variability between posteditors, and no strong correlation is observed with sentence quality judgment. With METEOR, Lavie and Agarwal (2007) introduced the possibility of evaluating quality based on intuitive “human assimilation”: matches on lemmatized forms, and synonymy seek to address deﬁciencies of simpler word-based metrics. In our context, post-editors are professional translators with very strict guidelines to perform “light” PE (which is possible on technical documentation for already highly customized translation). This creates natural “human targeted reference” and is therefore naturally suited for HTER evaluation, however in our approach, translation edit rate based on “mechanical edits” count is just an intermediate an"
2011.mtsummit-papers.17,N10-1062,0,0.0495022,"Missing"
2011.mtsummit-papers.17,W09-0423,1,0.841583,", and constituent structure of the sentences. 1.3 Can we reduce the effort? Beyond analysis, the general problem is how PE effort can be reduced. Multiple approaches can be quoted for that purpose: Guzm´an (2007) describes a set-up where MT output passes through a set of PE rules designed to smooth out translation output for a highly customized system. Dugast et al. (2007) and Simard et al. (2007) describe a set-up where an SMT system is trained on a bilingual corpus constituted with both MT output and human reference, and show how the sys165 tem learn how to “correct the translation output”. Schwenk et al. (2009) reproduce this with a Statistical Post-Editing (SPE) system trained on very large corpus making the initial translation as a mere preprocessing. In both cases, the SMT system beneﬁts from higher similarity between pretranslated text and reference compared to source and reference; however, if the ﬁnal quality is higher, the system does not learn post-editing. Through the introduction of PEA, our study shows that a large part of the PE effort can be classiﬁed and automatically learn. The rest of this paper is organized as follows: in section 2, we introduce and deﬁne the notion of PEA. We also"
2011.mtsummit-papers.17,W07-0728,0,0.0223805,"ly suited for HTER evaluation, however in our approach, translation edit rate based on “mechanical edits” count is just an intermediate analysis to expose “logical edits” taking into account part of speech, lemmatization, and constituent structure of the sentences. 1.3 Can we reduce the effort? Beyond analysis, the general problem is how PE effort can be reduced. Multiple approaches can be quoted for that purpose: Guzm´an (2007) describes a set-up where MT output passes through a set of PE rules designed to smooth out translation output for a highly customized system. Dugast et al. (2007) and Simard et al. (2007) describe a set-up where an SMT system is trained on a bilingual corpus constituted with both MT output and human reference, and show how the sys165 tem learn how to “correct the translation output”. Schwenk et al. (2009) reproduce this with a Statistical Post-Editing (SPE) system trained on very large corpus making the initial translation as a mere preprocessing. In both cases, the SMT system beneﬁts from higher similarity between pretranslated text and reference compared to source and reference; however, if the ﬁnal quality is higher, the system does not learn post-editing. Through the intro"
2011.mtsummit-papers.17,2006.amta-papers.25,0,0.52501,"measure post-editing effort? The measure of PE effort is important from a business perspective since it sets up the productivity of post-editors and subsequently the potential for additional cost-saving. The most criteria are the measure of PE time (Specia, 2011) or the comparison with human translation (Plitt and Masselot, 2010). Other approaches take into account “user activity data” covering keystrokes (Barrett et al., 2001) or eye movement detection (Doherty et al., 2010). Implicitly, estimating PE effort is the driver for establishing better quality evaluation metrics. For instance HTER (Snover et al., 2006) calculating translation edit rate towards targeted reference translation provides a reproducible metric, well correlated with human judgment on translation quality and close by deﬁnition to “translation post-editing”. In WMT09, Callison-Burch et al. (2009) introduced a new task: editing to evaluate translation where the edited translation is not used as a reference nor the reviewer asked to perform the least number of edits, but to make the translation ﬂuent without access to reference translation. The edited translation is then evaluated in a second phase of the evaluation task. However, the"
2011.mtsummit-papers.17,2011.eamt-1.12,0,0.0968263,"ant factor in reducing post-editing time” (Martinez, 2003). However describing the errors does not provide us with a methodology for ﬁxing them and always leads to system-dependent remediation approaches. In our approach, we are less interested in understanding the errors than deﬁning the correct action to obtain a good translation. 1.2 How can we measure post-editing effort? The measure of PE effort is important from a business perspective since it sets up the productivity of post-editors and subsequently the potential for additional cost-saving. The most criteria are the measure of PE time (Specia, 2011) or the comparison with human translation (Plitt and Masselot, 2010). Other approaches take into account “user activity data” covering keystrokes (Barrett et al., 2001) or eye movement detection (Doherty et al., 2010). Implicitly, estimating PE effort is the driver for establishing better quality evaluation metrics. For instance HTER (Snover et al., 2006) calculating translation edit rate towards targeted reference translation provides a reproducible metric, well correlated with human judgment on translation quality and close by deﬁnition to “translation post-editing”. In WMT09, Callison-Burch"
2011.mtsummit-papers.17,vilar-etal-2006-error,0,0.234162,"Missing"
2012.iwslt-papers.12,W12-3123,0,0.0744865,"nsequently, they need to be regularly re-trained in order to be updated, which is usually computationally demanding. The goal of incremental adaptation is then twofold: to adapt the system on the ﬂy when new resources are available without re-training the entire system. Post-Editing (PE) the output of SMT systems is widely used, amongst others, by professional translators of localization services which need for example to translate technical data in speciﬁc domains into several languages. However, the use of PE is restricted by some aspects that must be taken into consideration. As resumed by [1], the time spent by the post-editor is a commonly used measure of the PE effort, which should not to be, in case of poor translation quality, more important than translation from scratch. Even if this temporal aspect can be see as the most important, PE effort can be evaluated using automatic metrics based on the edit Jean Senellart† †Systran SA 5, rue Feydeau 75002 Paris, France lastname@systran.fr distance. These metrics commonly use the number of required edits of the MT system output to reach a reference translation. From then, the combination of PE and incremental adaptation can be seen a"
2012.iwslt-papers.12,2011.mtsummit-papers.17,1,0.734204,"ion from scratch. Even if this temporal aspect can be see as the most important, PE effort can be evaluated using automatic metrics based on the edit Jean Senellart† †Systran SA 5, rue Feydeau 75002 Paris, France lastname@systran.fr distance. These metrics commonly use the number of required edits of the MT system output to reach a reference translation. From then, the combination of PE and incremental adaptation can be seen as a way to reduce the task effort by allowing a MT system to gradually learn from its own errors. Especially considering the repetitive nature of the task highlighted by [2]. However, incremental adaptation is still a tricky task: how to adapt the system correctly? Adaptation should not degrade system performance and accuracy. Some approaches are possible and we will try to see the impact of several of them in the second part of this article. First of all, we present a new experimental approach for incremental adaptation of a MT system using PE analysis. Starting from a generic baseline, we have gradually adapted our system by translating an in-domain corpora which was split beforehand. Each part of the corpora was translated using the translation model adapted a"
2012.iwslt-papers.12,2010.amta-papers.21,0,0.722949,"of them in the second part of this article. First of all, we present a new experimental approach for incremental adaptation of a MT system using PE analysis. Starting from a generic baseline, we have gradually adapted our system by translating an in-domain corpora which was split beforehand. Each part of the corpora was translated using the translation model adapted at the previous step, i.e. updated with new extracted phrases. These phrases are the result of a word-to-word alignment combination we present afterward. 1.1. Similar work The most similar approach in the literature is proposed in [3] who present an incremental re-training algorithm to simulate a post-editing situation. It is proposed to extract new phrases from approximate alignments which were obtained by a modiﬁed version of Giza-pp [4]. An initial alignment with one-to-one links between the same sentence positions is created and then iteratively updated as long as improvements are observed. In practice, a greedy search algorithm is used to ﬁnd the locally optimal word alignment. All source positions carrying only one link are tried, and the single link change which produces the highest probability increase according to"
2012.iwslt-papers.12,J03-1002,0,0.0039137,"adapted our system by translating an in-domain corpora which was split beforehand. Each part of the corpora was translated using the translation model adapted at the previous step, i.e. updated with new extracted phrases. These phrases are the result of a word-to-word alignment combination we present afterward. 1.1. Similar work The most similar approach in the literature is proposed in [3] who present an incremental re-training algorithm to simulate a post-editing situation. It is proposed to extract new phrases from approximate alignments which were obtained by a modiﬁed version of Giza-pp [4]. An initial alignment with one-to-one links between the same sentence positions is created and then iteratively updated as long as improvements are observed. In practice, a greedy search algorithm is used to ﬁnd the locally optimal word alignment. All source positions carrying only one link are tried, and the single link change which produces the highest probability increase according to the Giza-pp model 4 is kept. The resulting alignment is improved with two simple post-processing steps. First, each unknown word in source side is aligned with the ﬁrst non-aligned unknown word on the target"
2012.iwslt-papers.12,N10-1062,0,0.0334028,"aligned using edit distance algorithm of TER; 3. Source-reference alignment: the alignment links are deduced from combination of alignments of both step 1 and 2. Phrase pairs are then extracted, scored and added to translation model which is ﬁnally re-trained. ment algorithm which is partially based on the edit-distance algorithm. As argued in [3], “to be practical, incremental retraining must be performed in less than one second”. For comparison, our entire alignment process takes few hundredths of second for 1500 sentences, in comparison to several seconds per sentences as reported in [3]. [5] present stream based incremental adaptation using an on-line version of the EM algorithm. This approach designed for large amounts of incoming data is not really adapted for the post-editing context. Like [3], we propose an incremental adaptation workﬂow that is more oriented to real time processing. As part of our experiments, we have compared our approach with the use of the freely available tool named IncGiza-pp,1 an incremental version of Giza-pp. It is precisely intended to inject new data into an SMT system without having to restart the entire word alignment procedure. To our knowledge,"
2012.iwslt-papers.12,P07-2045,0,0.0108708,"December 6th-7th, 2012 Figure 2: Example of a source-to-reference alignment using using the automatic translation as pivot. The alignment links between the source sentence and the translation are generated by the MT system. Those between the translation and its postedited version (i.e. the reference) are calculated by TER. Finally, the source-to-reference alignment links are deduced by an alignment combination based on both alignment sets computed before. 2.1.1. Translation: source to translation alignment The SMT system used to translate the source sentences is based on the Moses SMT toolkit [6]. Moses can provide the word-to-word alignments between the source sentence and the translation hypothesis. This aligning information represents the ﬁrst part of our alignment combination. This automatic translation is “compared” with the reference translation using an edit distance algorithm. 2.1.2. Analysis: edit distance alignment In this paper, we use the Translation Error Rate (TER) algorithm as proposed in [7]. TER is an extension of the Word Error Rate (WER) which is more suitable for machine translation since it can take into account word reorderings. TER uses the following edit types:"
2012.iwslt-papers.12,2006.amta-papers.25,0,0.0471864,"on both alignment sets computed before. 2.1.1. Translation: source to translation alignment The SMT system used to translate the source sentences is based on the Moses SMT toolkit [6]. Moses can provide the word-to-word alignments between the source sentence and the translation hypothesis. This aligning information represents the ﬁrst part of our alignment combination. This automatic translation is “compared” with the reference translation using an edit distance algorithm. 2.1.2. Analysis: edit distance alignment In this paper, we use the Translation Error Rate (TER) algorithm as proposed in [7]. TER is an extension of the Word Error Rate (WER) which is more suitable for machine translation since it can take into account word reorderings. TER uses the following edit types: insertion, deletion, substitution and shift. The TER is computed between the output of our SMT system and the corresponding reference translation, and the word-to-word alignments are inferred. We only keep the aligned and substituted edit types in order to extract what we consider as the most interesting phrase pairs. Indeed, we argue that what is aligned correspond to what our system knows, while what is substitut"
2012.iwslt-papers.12,W09-0441,0,0.0335301,"ath has shift then foreach shift do updateWordPosition(tgt, shift); end end foreach edit-type of edit-path do if edit-type is ‘align’ or ‘substitution’ then alignment(tgt-word, ref-word) = 1; end end foreach ref-word of ref do foreach tgt-word aligned to ref-word do if isAligned?(src-word, tgt-word) then alignment(src-word, ref-word) = 1; end end end Algorithm 1: Source-to-reference alignment algorithm at word level. Using both source-to-translation alignments and translation-to-reference edit-path, the source-toreference alignments path are build. Our approach can be extended to use TER-Plus [8], an extension of TER using paraphrases, stemming and synonyms in order to obtain better word-to-word alignments. 3. Experimental evaluation 2.1.3. Adaptation: source to reference alignment Considering the SMT translation hypothesis as a “pivot” for aligning both source and its reference sentence, we have designed the word-to-word alignment algorithm shown by Algorithm 1. It combines source-to-translation and translationto-reference alignments, and then deduces the source-toreference alignment path. From this path, the translation model is ﬁnally updated using the standard training phrase extr"
2012.iwslt-papers.12,lambert-etal-2012-automatic,1,0.833456,"rl and News Commentary with 50 million and 3 million words, respectively. They were used to train our SMT baseline systems. The third corpus, named “absINFO”, contains 500 thousand words randomly selected from abstracts of scientiﬁc papers in the domain of Computer Science. Information on the sub-domains is also available (networks, AI, data base, theoretical CS, . . .), but was not used in this study. The corpus if freely available to support research in domain adaptation and was already used by the 2012 JHU summer workshop on this topic. A detailed description of this corpus can be found in [9]. This in-domain corpus was split into three sub-corpora: • absINFO.corr.train is composed of 350k words and is used to simulate the user post-editing or corrective training. • absINFO.dev is a set of 75k words and used for development. • absINFO.test another set of 75k words used as a test corpus to monitor the performance of our adaptation workﬂow. Moreover, in order to better simulate a sequential postediting process, the absINFO.corr.train corpus was split into 10 sub-sets (about 1.5k sentences with 35k words each). This corresponds quite well to the update of an MT system after a post-cor"
2012.iwslt-papers.12,P03-1021,0,\N,Missing
2017.jeptalnrecital-court.27,P02-1023,0,0.18376,"Missing"
2017.jeptalnrecital-court.27,E12-1016,0,0.0293633,"Missing"
2017.jeptalnrecital-court.27,D13-1176,0,0.103714,"Missing"
2017.jeptalnrecital-court.27,P17-4012,1,0.83894,"Missing"
2017.jeptalnrecital-court.27,W11-2132,1,0.795463,"Missing"
2017.jeptalnrecital-court.27,D07-1036,0,0.0972007,"Missing"
2017.jeptalnrecital-court.27,2015.iwslt-evaluation.11,0,0.0979969,"Missing"
2017.jeptalnrecital-court.27,D15-1166,0,0.0657391,"Missing"
2017.jeptalnrecital-court.27,P10-2041,0,0.108924,"Missing"
2017.jeptalnrecital-court.27,P03-1021,0,0.0842472,"Missing"
2017.jeptalnrecital-court.27,P02-1040,0,0.0994922,"Missing"
2017.jeptalnrecital-court.27,N16-1005,0,0.0424935,"Missing"
2017.jeptalnrecital-court.27,P16-1009,0,0.0608435,"Missing"
2017.jeptalnrecital-court.27,W10-1759,0,0.0550544,"Missing"
2017.jeptalnrecital-court.27,I11-1148,0,0.0263053,"Missing"
2017.jeptalnrecital-court.27,tiedemann-2012-parallel,0,0.0169973,"Missing"
2017.jeptalnrecital-court.27,2014.amta-researchers.15,1,0.864363,"Missing"
2017.jeptalnrecital-court.27,C16-1170,0,0.0372666,"Missing"
2020.acl-main.144,P19-1294,0,0.0463637,"y to guide translation of a given sentence. Similar to our work, (Farajian et al., 2017; Li et al., 2018) retrieve similar sentences from the training data to dynamically adapt individual input sentences. To compute similarity, the first work uses n-gram matches, the second includes dense vector representations. In (Xu et al., 2019) the same approach is followed but authors consider for adaptation a bunch of semantically related input sentences to reduce adaptation time. Our approach combines source and target words within a same sentence - the same type of approach has also been proposed by (Dinu et al., 2019) for introduction of terminology translation. Last, we can also compare the extra-tokens appended in augmented sentences as “side constraints” activating different translation paths on the same spirit than the work done by (Sennrich et al., 2016a; Kobus et al., 2017) for controlling translation. 6 Conclusions and Further Work This paper explores augmentation methods for boosting Neural Machine Translation performance by using similar translations. Based on “neural fuzzy repair” technique, we introduce tighter integration of fuzzy matches informing neural network of source and target and propos"
2020.acl-main.144,N13-1073,0,0.0682232,"tence s, max(q) returns the longest n-gram in the set q and ∣r∣ is the length of the n-gram r. For N gram matching retrieval we also use our in-house open-sourced toolkit. ⎧ ⎫ t ∈ tk ∶ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ∃s ∈ S ∣ (s, t) ∈ A T =⎨ ⎬ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ∧ ∀s ∉ S ∣ (s, t) ∉ A ⎩ ⎭ where A is the set of word alignments between words in sk and tk and S is the LCS (Longest Common Subsequence) set of words in sk and s. The LCS is computed as a by-product of the edit distance (Paterson and Danˇc´ık, 1994). S is found as a sub-product of computing fuzzy or n-gram matches. Word alignments are per6 formed by fast align (Dyer et al., 2013). Figure 1 illustrates the alignments and LCS words between input sentences and their corresponding fuzzy (top) and N -gram (bottom) matches. dure le vol ? »» »» N M (si , sj ) = »»»»max({S(si ) ∩ S(sj )})»»»» »» »» discuss an algorithm capable of identifying the set of target words T ∈ tk that are related to words of the input sentence s. Thus, we define the set T as: dure un 3 N -gram Matching We define the N -gram matching score N M (si , sj ) between si and sj : Figure 1: English-French TM entries with corresponding word alignments (right) and LCS of words with the input sentence (left). M"
2020.acl-main.144,W17-4713,0,0.146587,"ting with more general notions of similar sentences and techniques to inject fuzzy matches. The use of similar sentences to improve translation models has been explored at scale in (Schwenk et al., 2019), where the authors use multilingual sentence embeddings to retrieve pairs of similar sentences and train models uniquely with such sentences. In (Niehues et al., 2016), input sentences are augmented with pre-translations performed by a phrase-based MT system. In our approach, similar sentence translations are provided dynamically to guide translation of a given sentence. Similar to our work, (Farajian et al., 2017; Li et al., 2018) retrieve similar sentences from the training data to dynamically adapt individual input sentences. To compute similarity, the first work uses n-gram matches, the second includes dense vector representations. In (Xu et al., 2019) the same approach is followed but authors consider for adaptation a bunch of semantically related input sentences to reduce adaptation time. Our approach combines source and target words within a same sentence - the same type of approach has also been proposed by (Dinu et al., 2019) for introduction of terminology translation. Last, we can also compa"
2020.acl-main.144,E14-1022,0,0.164442,"a sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation. 1 Introduction For decades, the localization industry has been proposing Fuzzy Matching technology in CAT tools allowing the human translator to visualize one or several fuzzy matches from translation memory when translating a sentence leading to higher productivity and consistency (Yamada, 2011). Hence, even though the concept of fuzzy match scores is not standardized and differs between CAT tools (Bloodgood and Strauss, 2014), translators generally accept discounted translation 1 rate for sentences with ”high” fuzzy matches . With improving machine translation technology 1 https://signsandsymptomsoftranslation. com/2015/03/06/fuzzy-matches/. and training of models on translation memories, machine translated output has been progressively introduced as a substitute for fuzzy matches when no sufficiently “good” fuzzy match is found and proved to also increase translator productivity given appropriate post-editing environment (Plitt and Masselot, 2010). These two technologies are entirely different in their finality -"
2020.acl-main.144,P19-1175,0,0.256078,"Missing"
2020.acl-main.144,C18-1111,0,0.0465017,"s like use of fuzzy matches in SMT decoding (Koehn and Senellart, 2010; Wang et al., 2013), adaptive machine translation (Zaretskaya et al., 2015) or “fuzzy match repairing” (Ortega et al., 2016). With Neural Machine Translation (NMT), the integration of Fuzzy Matching is less obvious since NMT does not keep nor build a database of aligned sequences and does not explicitly use n-gram language models for decoding. The only obvious and important use of translation memory is to use them to train an NMT model from scratch or to adapt a generic translation model to a specific domain (fine-tuning) (Chu and Wang, 2018). While some works propose architecture changes (Zhang et al., 2018) or decoding constraints (Gu et al., 2018); a recent work (Bult´e and Tezcan, 2019; Bult´e et al., 2018) has proposed a simple and elegant framework where, like for human translation, translation of fuzzy matches are presented simultaneously with source sentence and the network learns to use this additional information. Even though this method has showed huge gains in quality, it also opens 1580 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1580–1590 c July 5 - 10, 2020. 2020 As"
2020.acl-main.144,P17-4012,1,0.820079,"RC); Localisation files (GNOME, KDE4 and Ubuntu) and Manual texts (PHP). Detailed statistics about these are provided in Appendix A. We randomly split the corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the Open9 NMT tokenizer (conservative mode). We train a 32K joint byte-pair encoding (BPE) (Sennrich et al., 2016b) and use a joint vocabulary for both source and target. Our NMT model follows the state-of-the-art Transformer base architecture (Vaswani et al., 10 2017) implemented in the OpenNMT-tf toolkit (Klein et al., 2017). Further configuration details are given in Appendix B. 3.2 TM Retrieval We perform fuzzy matching, ignoring exact matches, and keep the single best match if F M (si , sj ) ≥ 0.6 with no approximation. Similarly, the largest N -gram match is used for each test sentence with a threshold N M (si , sj ) ≥ 5. A similarity threshold EM (si , sj ) ≥ 0.8 is also employed when retrieving similar sentences using distributed representations. The EM model is trained on the source training data with default fasttext params on 200 dimension, and 20 epochs. Algorithm FM NM EM Indexing (s) 546 546 181+342 R"
2020.acl-main.144,kobus-etal-2017-domain,1,0.876661,"e second includes dense vector representations. In (Xu et al., 2019) the same approach is followed but authors consider for adaptation a bunch of semantically related input sentences to reduce adaptation time. Our approach combines source and target words within a same sentence - the same type of approach has also been proposed by (Dinu et al., 2019) for introduction of terminology translation. Last, we can also compare the extra-tokens appended in augmented sentences as “side constraints” activating different translation paths on the same spirit than the work done by (Sennrich et al., 2016a; Kobus et al., 2017) for controlling translation. 6 Conclusions and Further Work This paper explores augmentation methods for boosting Neural Machine Translation performance by using similar translations. Based on “neural fuzzy repair” technique, we introduce tighter integration of fuzzy matches informing neural network of source and target and propose extension to similar translations retrieved 1587 from their distributed representations. We show that the different types of similar translations and model fine-tuning provide complementary information to the neural model outperforming consistently and significantl"
2020.acl-main.144,2010.jec-1.4,1,0.861121,"Masselot, 2010). These two technologies are entirely different in their finality - indeed, for a given source sentence, fuzzy matching is just a database retrieval and scoring technique always returning a pair of source and target segments, while machine translation is actually building an original translation. However, with Statistical Machine Translation, the two technologies are sharing the same simple idea about managing and retrieving optimal combination of longest translated n-grams and this property led to the development of several techniques like use of fuzzy matches in SMT decoding (Koehn and Senellart, 2010; Wang et al., 2013), adaptive machine translation (Zaretskaya et al., 2015) or “fuzzy match repairing” (Ortega et al., 2016). With Neural Machine Translation (NMT), the integration of Fuzzy Matching is less obvious since NMT does not keep nor build a database of aligned sequences and does not explicitly use n-gram language models for decoding. The only obvious and important use of translation memory is to use them to train an NMT model from scratch or to adapt a generic translation model to a specific domain (fine-tuning) (Chu and Wang, 2018). While some works propose architecture changes (Zh"
2020.acl-main.144,L18-1146,0,0.311686,"notions of similar sentences and techniques to inject fuzzy matches. The use of similar sentences to improve translation models has been explored at scale in (Schwenk et al., 2019), where the authors use multilingual sentence embeddings to retrieve pairs of similar sentences and train models uniquely with such sentences. In (Niehues et al., 2016), input sentences are augmented with pre-translations performed by a phrase-based MT system. In our approach, similar sentence translations are provided dynamically to guide translation of a given sentence. Similar to our work, (Farajian et al., 2017; Li et al., 2018) retrieve similar sentences from the training data to dynamically adapt individual input sentences. To compute similarity, the first work uses n-gram matches, the second includes dense vector representations. In (Xu et al., 2019) the same approach is followed but authors consider for adaptation a bunch of semantically related input sentences to reduce adaptation time. Our approach combines source and target words within a same sentence - the same type of approach has also been proposed by (Dinu et al., 2019) for introduction of terminology translation. Last, we can also compare the extra-token"
2020.acl-main.144,2015.iwslt-evaluation.11,0,0.0451284,"ble 3 (2 block), the best com+ + bination of matches is achieved by ⊕(FM ,EM ) further boosting the performance of previous con+ + figurations. It is only surpassed by ⊖(FM ,EM ) in two test sets by a slight margin. Fine Tuning Results so far evaluate the ability of NMT models to integrate similar sentences. However, we have run our comparisons over a “generic” model built from a heterogeneous training data set while it is well known that these models do not achieve best performance on homogeneous test sets. Thus, we now assess the capability of our augmentation methods to enhance fine-tuned (Luong and Manning, 2015) models, a well known technique that is commonly used in domain adaptation scenarios obtaining state-of-the-art results. Table 3 illustrates the results of the model configurations previously described after fine-tuning the models towards each test set domain. Thus, building 7 fine-tuned models for each configuration. Note that similar sentences (matches) are retrieved from the same in-domain data sets used for fine tuning. As 1586 ⊕(FM ,EM ) How long does a cold last ? ∥ Combien de temps dure le vol ? ∥ Combien de temps dure un vaccin ? S S S S S S SR T T T T R R TE E E E E E E E + + + + Figu"
2020.acl-main.144,C16-1172,0,0.11237,"Missing"
2020.acl-main.144,2016.amta-researchers.3,0,0.0802957,"Missing"
2020.acl-main.144,N18-1049,0,0.0467461,"de temps dure le vol?]. Even though the TM entry may be of great help when translating the input sentence, it receives a low 5 score (1 − 12 = 0.583) because of the multiple insertion/deletion operations needed. We thus introduce a second lexicalised similarity measure that focuses on finding the longest of n-gram overlap between sentences. 1581 2 https://github.com/systran/FuzzyMatch Distributed Representations The current research on sentence similarity measures has made tremendous advances thanks to distributed word representations computed by neural nets. In this 4 work, we use sent2vec (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations of sentences. The authors claim that the algorithm performs state-of-the-art sentence representations on multiple benchmark tasks in particular for unsupervised similarity evaluation. We define the similarity score EM (si , sj ) between sentences si and sj via cosine similarity of their distributed representations hi and hj : ? last cold a does long How hi ⋅ hj ∣∣hi ∣∣ × ∣∣hj ∣∣ where ∣∣h∣∣ denotes the magnitude of vector h. To implement fast retrieval bet"
2020.acl-main.144,N16-1005,0,0.159905,"subtitles (TED); Parallel sentences extracted from Wikipedia (Wiki); Documentation from the European Central Bank (ECB); Documents from the European Medicines Agency (EMEA); Legislative texts of the European Union (JRC); Localisation files (GNOME, KDE4 and Ubuntu) and Manual texts (PHP). Detailed statistics about these are provided in Appendix A. We randomly split the corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the Open9 NMT tokenizer (conservative mode). We train a 32K joint byte-pair encoding (BPE) (Sennrich et al., 2016b) and use a joint vocabulary for both source and target. Our NMT model follows the state-of-the-art Transformer base architecture (Vaswani et al., 10 2017) implemented in the OpenNMT-tf toolkit (Klein et al., 2017). Further configuration details are given in Appendix B. 3.2 TM Retrieval We perform fuzzy matching, ignoring exact matches, and keep the single best match if F M (si , sj ) ≥ 0.6 with no approximation. Similarly, the largest N -gram match is used for each test sentence with a threshold N M (si , sj ) ≥ 5. A similarity threshold EM (si , sj ) ≥ 0.8 is also employed when retrieving s"
2020.acl-main.144,P13-1002,0,0.356951,"technologies are entirely different in their finality - indeed, for a given source sentence, fuzzy matching is just a database retrieval and scoring technique always returning a pair of source and target segments, while machine translation is actually building an original translation. However, with Statistical Machine Translation, the two technologies are sharing the same simple idea about managing and retrieving optimal combination of longest translated n-grams and this property led to the development of several techniques like use of fuzzy matches in SMT decoding (Koehn and Senellart, 2010; Wang et al., 2013), adaptive machine translation (Zaretskaya et al., 2015) or “fuzzy match repairing” (Ortega et al., 2016). With Neural Machine Translation (NMT), the integration of Fuzzy Matching is less obvious since NMT does not keep nor build a database of aligned sequences and does not explicitly use n-gram language models for decoding. The only obvious and important use of translation memory is to use them to train an NMT model from scratch or to adapt a generic translation model to a specific domain (fine-tuning) (Chu and Wang, 2018). While some works propose architecture changes (Zhang et al., 2018) or"
2020.acl-main.144,N18-1120,0,0.217504,"10; Wang et al., 2013), adaptive machine translation (Zaretskaya et al., 2015) or “fuzzy match repairing” (Ortega et al., 2016). With Neural Machine Translation (NMT), the integration of Fuzzy Matching is less obvious since NMT does not keep nor build a database of aligned sequences and does not explicitly use n-gram language models for decoding. The only obvious and important use of translation memory is to use them to train an NMT model from scratch or to adapt a generic translation model to a specific domain (fine-tuning) (Chu and Wang, 2018). While some works propose architecture changes (Zhang et al., 2018) or decoding constraints (Gu et al., 2018); a recent work (Bult´e and Tezcan, 2019; Bult´e et al., 2018) has proposed a simple and elegant framework where, like for human translation, translation of fuzzy matches are presented simultaneously with source sentence and the network learns to use this additional information. Even though this method has showed huge gains in quality, it also opens 1580 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1580–1590 c July 5 - 10, 2020. 2020 Association for Computational Linguistics many questions. In this work"
2020.acl-main.144,P16-1162,0,0.494194,"subtitles (TED); Parallel sentences extracted from Wikipedia (Wiki); Documentation from the European Central Bank (ECB); Documents from the European Medicines Agency (EMEA); Legislative texts of the European Union (JRC); Localisation files (GNOME, KDE4 and Ubuntu) and Manual texts (PHP). Detailed statistics about these are provided in Appendix A. We randomly split the corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the Open9 NMT tokenizer (conservative mode). We train a 32K joint byte-pair encoding (BPE) (Sennrich et al., 2016b) and use a joint vocabulary for both source and target. Our NMT model follows the state-of-the-art Transformer base architecture (Vaswani et al., 10 2017) implemented in the OpenNMT-tf toolkit (Klein et al., 2017). Further configuration details are given in Appendix B. 3.2 TM Retrieval We perform fuzzy matching, ignoring exact matches, and keep the single best match if F M (si , sj ) ≥ 0.6 with no approximation. Similarly, the largest N -gram match is used for each test sentence with a threshold N M (si , sj ) ≥ 5. A similarity threshold EM (si , sj ) ≥ 0.8 is also employed when retrieving s"
2020.acl-main.144,tiedemann-2012-parallel,0,0.0586845,"the duration of flu symptoms ? How long does a cold last ? ∥ Quelle est la dur´ee de la grippe ? S S S S S S SE E E E E E E E E ∗ FM As a variant of FM , we now mark target words which are not related to the input sentence in an attempt to help the network identify those target Figure 2: Input sentence augmented with different TM # ∗ + matches: FM (Bult´e and Tezcan, 2019), FM , FM and + EM . 7 The original paper uses ‘@@@’ as break token. We made sure that ∥ was not part of the vocabulary. 1583 3 Experimental Framework 3.1 Corpora and Evaluation 8 We used the following corpora in this work (Tiedemann, 2012): Proceedings of the European Parliament (EPPS); News Commentaries (NEWS); TED talk subtitles (TED); Parallel sentences extracted from Wikipedia (Wiki); Documentation from the European Central Bank (ECB); Documents from the European Medicines Agency (EMEA); Legislative texts of the European Union (JRC); Localisation files (GNOME, KDE4 and Ubuntu) and Manual texts (PHP). Detailed statistics about these are provided in Appendix A. We randomly split the corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the Open9"
2020.acl-main.144,W15-4920,0,0.0436122,"ber of overlaps between the sentences taken into account. The latter counts on the generalisation F M (si , sj ) = 1 − ED(si , sj ) max(∣si ∣, ∣sj ∣) where ED(si , sj ) is the Edit Distance between si and sj , and ∣s∣ is the length of s. Many variants have been proposed to compute the edit distance, generally performed on normalized sentences (ignoring for instance case, number, punctuation, space or inline tags differences that are typically handled at a later stage). Also, IDF and stemming techniques are used to give more weight on significant words or less weight on morphological variants (Vanallemeersch and Vandeghinste, 2015; Bloodgood and Strauss, 2014). Since we did not find an efficient TM fuzzy match library, we implemented an efficient and parameterizable algorithm in C++ based on suffixarray (Manber and Myers, 1993) that we open2 sourced . Fuzzy matching offers a great performance under large overlapping conditions. However, in some cases, sentences with large overlaps may receive low F M scores. Consider for instance the input: [How long does the flight arriving in Paris from Barcelona last?] and the TM entry of our previous example: [How long does the flight last?] ↝ [Combien de temps dure le vol?]. Even"
2020.amta-research.9,W19-5301,0,0.060609,"Missing"
2020.amta-research.9,P18-1008,0,0.0244055,"(Vaswani et al., 2017), and convolutional (Gehring et al., 2017). The project also includes components for other tasks such as encoders for non-text inputs, decoders for generative languages models, and copy attention mechanisms (See et al., 2017) for summarization. Modular design. We focus on modularity to allow ideas from one paper to be reused in another context. OpenNMT-tf pushes this mindset to the extreme by requiring users to configure their model via Python code. This enables a high level of modelling freedom to support custom architectures such as hybrid sequence-to-sequence models (Chen et al., 2018), multi-source Transformer models (Libovick´y et al., 2018), and nested input features. 3 https://forum.opennmt.net/ 4 https://slator.com/features/how-ubiqus-deploys-neural-machine-translationin-language-operations/ 5 https://news.developer.nvidia.com/tensorrt6-breaks-bert-record Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 104 OpenNMT-tf OpenNMT-py CTranslate2 + int16 + int8 + vmap Model size 367MB 542MB 374MB 197MB 110MB 121MB CPU 217.6 179.1 389.4 413.6 508.3 646.2 GTX1080 1659.2 1510.0 30"
2020.amta-research.9,P16-5005,0,0.0245093,"doption Cited in over 700 scientific publications as of May 2020, OpenNMT has also been directly used in numerous research papers. The system is employed both to conduct new experiments and as a baseline for sequence-to-sequence approaches. OpenNMT was used for other tasks related to neural machine translation such as summarization (Gehrmann et al., 2018), data-totext (Wiseman et al., 2017), image-to-text (Deng et al., 2017), automatic speech recognition (Ericson, 2019) and semantic parsing (van Noord and Bos, 2017). OpenNMT also proved to be widespread in industry. Companies such as SYSTRAN (Crego et al., 2016), Booking.com (Levin et al., 2017), or Ubiqus4 are known to deploy OpenNMT models in production. We note that a number of industrial entities published scientific papers showing their internal experiments using the framework such as SwissPost (Girletti et al., 2019) and BNP Paribas (Mghabbar and Ratnamogan, 2020), while NVIDIA used OpenNMT as a benchmark for the release of TensorRT 65 . 3 Key features 3.1 Model architectures catalog While OpenNMT initially focused on sequence-to-sequence models applied to translation, it has been extended to support additional architectures and model component"
2020.amta-research.9,D18-1443,0,0.0194927,"ocus on research at this time. After more than 3 years of active development, OpenNMT projects have been starred by over 7,400 users. A community forum3 is also home of 970 users and more than 9,800 posts about NMT research and how to use OpenNMT effectively. 2.2 Adoption Cited in over 700 scientific publications as of May 2020, OpenNMT has also been directly used in numerous research papers. The system is employed both to conduct new experiments and as a baseline for sequence-to-sequence approaches. OpenNMT was used for other tasks related to neural machine translation such as summarization (Gehrmann et al., 2018), data-totext (Wiseman et al., 2017), image-to-text (Deng et al., 2017), automatic speech recognition (Ericson, 2019) and semantic parsing (van Noord and Bos, 2017). OpenNMT also proved to be widespread in industry. Companies such as SYSTRAN (Crego et al., 2016), Booking.com (Levin et al., 2017), or Ubiqus4 are known to deploy OpenNMT models in production. We note that a number of industrial entities published scientific papers showing their internal experiments using the framework such as SwissPost (Girletti et al., 2019) and BNP Paribas (Mghabbar and Ratnamogan, 2020), while NVIDIA used Open"
2020.amta-research.9,E17-3017,0,0.0185848,"n, train, and deploy neural machine translation models. The OpenNMT initiative consists of several projects to assist researchers and developers in their NMT journey, from data preparation to inference acceleration. It supports a wide range of model architectures and training procedures for neural machine translation as well as related tasks such as natural language generation and language modeling. The open source community around neural machine translation is very active and includes several other projects that have similar goals and capabilities such as Fairseq (Ott et al., 2019), Sockeye (Hieber et al., 2017), or Marian (Junczys-Dowmunt et al., 2018). In the ongoing development of OpenNMT, we aim to build upon the strengths of those systems, while providing unique features and technology support. In this paper, we briefly give an overview of the OpenNMT project and its history. We then present the key features that make OpenNMT particularly suited for research and production. Finally, we share some benchmarks for comparison and current work directions. 1 https://opennmt.net Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1:"
2020.amta-research.9,P18-4020,0,0.0147628,"ne translation models. The OpenNMT initiative consists of several projects to assist researchers and developers in their NMT journey, from data preparation to inference acceleration. It supports a wide range of model architectures and training procedures for neural machine translation as well as related tasks such as natural language generation and language modeling. The open source community around neural machine translation is very active and includes several other projects that have similar goals and capabilities such as Fairseq (Ott et al., 2019), Sockeye (Hieber et al., 2017), or Marian (Junczys-Dowmunt et al., 2018). In the ongoing development of OpenNMT, we aim to build upon the strengths of those systems, while providing unique features and technology support. In this paper, we briefly give an overview of the OpenNMT project and its history. We then present the key features that make OpenNMT particularly suited for research and production. Finally, we share some benchmarks for comparison and current work directions. 1 https://opennmt.net Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 102 2 Project overv"
2020.amta-research.9,P17-4012,1,0.833628,"ue command line interface for training, translating, and serving models. All projects are released on GitHub2 under the MIT license. 2 https://github.com/OpenNMT Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 103 2.1 History OpenNMT was first released in late 2016 as a Torch7 implementation. This version was the result of a collaboration between Harvard NLP and SYSTRAN and was based on seq2seq-attn, an open-source project developed by Harvard student Yoon Kim. The original demonstration paper (Klein et al., 2017) was awarded “Best Demonstration Paper Runner-Up” at ACL 2017. After the release of PyTorch (Paszke et al., 2019), the Facebook A.I. Research team shared a complete rewrite of the project that later became OpenNMT-py and initiated the sunsetting of the Torch7 version of OpenNMT. The ecosystem was then extended with OpenNMT-tf which prioritized production, while OpenNMT-py had a focus on research at this time. After more than 3 years of active development, OpenNMT projects have been starred by over 7,400 users. A community forum3 is also home of 970 users and more than 9,800 posts about NMT res"
2020.amta-research.9,D18-2012,0,0.0238289,"n efficiency and includes features that we found useful for training high-quality translation models. Configurable reversibility. The tokenization can be made reversible by marking either joints or spaces. These markers can be attached to the input tokens or injected as separate tokens. Advanced text segmentation. Several flags can finely control where to segment: on digits, on alphabet change, on case change, etc. Additionally, we introduced special control characters to prevent segmentation in delimited sequences. Subword training and encoding. The project can train and apply SentencePiece (Kudo and Richardson, 2018) and BPE (Sennrich et al., 2016) models, while making them compatible with all features listed above. 3.5 Model serving OpenNMT also provides components to serve translation models. OpenNMT-py includes a REST server that can manage multiple models and unload those that are not used. The server integrates CTranslate2 for efficient execution and Tokenizer for on-the-fly tokenization. OpenNMT-tf models are compatible with TensorFlow Serving which is a scalable solution to serve machine learning models. OpenNMT-tf models are also compatible with the OpenNMT-py REST server via the CTranslate2 integ"
2020.amta-research.9,W18-6326,0,0.0631885,"Missing"
2020.amta-research.9,D15-1166,0,0.0753425,"wing their internal experiments using the framework such as SwissPost (Girletti et al., 2019) and BNP Paribas (Mghabbar and Ratnamogan, 2020), while NVIDIA used OpenNMT as a benchmark for the release of TensorRT 65 . 3 Key features 3.1 Model architectures catalog While OpenNMT initially focused on sequence-to-sequence models applied to translation, it has been extended to support additional architectures and model components (see Table 1). Multiple architectures. The toolkits implement the most used architectures for neural machine translation: recurrent with attention (Bahdanau et al., 2015; Luong et al., 2015), selfattentional (Vaswani et al., 2017), and convolutional (Gehring et al., 2017). The project also includes components for other tasks such as encoders for non-text inputs, decoders for generative languages models, and copy attention mechanisms (See et al., 2017) for summarization. Modular design. We focus on modularity to allow ideas from one paper to be reused in another context. OpenNMT-tf pushes this mindset to the extreme by requiring users to configure their model via Python code. This enables a high level of modelling freedom to support custom architectures such as hybrid sequence-to-"
2020.amta-research.9,N19-4009,0,0.010726,"opensource toolkit to design, train, and deploy neural machine translation models. The OpenNMT initiative consists of several projects to assist researchers and developers in their NMT journey, from data preparation to inference acceleration. It supports a wide range of model architectures and training procedures for neural machine translation as well as related tasks such as natural language generation and language modeling. The open source community around neural machine translation is very active and includes several other projects that have similar goals and capabilities such as Fairseq (Ott et al., 2019), Sockeye (Hieber et al., 2017), or Marian (Junczys-Dowmunt et al., 2018). In the ongoing development of OpenNMT, we aim to build upon the strengths of those systems, while providing unique features and technology support. In this paper, we briefly give an overview of the OpenNMT project and its history. We then present the key features that make OpenNMT particularly suited for research and production. Finally, we share some benchmarks for comparison and current work directions. 1 https://opennmt.net Proceedings of the 14th Conference of the Association for Machine Translation in the Americas"
2020.amta-research.9,P17-1099,0,0.042592,"le OpenNMT initially focused on sequence-to-sequence models applied to translation, it has been extended to support additional architectures and model components (see Table 1). Multiple architectures. The toolkits implement the most used architectures for neural machine translation: recurrent with attention (Bahdanau et al., 2015; Luong et al., 2015), selfattentional (Vaswani et al., 2017), and convolutional (Gehring et al., 2017). The project also includes components for other tasks such as encoders for non-text inputs, decoders for generative languages models, and copy attention mechanisms (See et al., 2017) for summarization. Modular design. We focus on modularity to allow ideas from one paper to be reused in another context. OpenNMT-tf pushes this mindset to the extreme by requiring users to configure their model via Python code. This enables a high level of modelling freedom to support custom architectures such as hybrid sequence-to-sequence models (Chen et al., 2018), multi-source Transformer models (Libovick´y et al., 2018), and nested input features. 3 https://forum.opennmt.net/ 4 https://slator.com/features/how-ubiqus-deploys-neural-machine-translationin-language-operations/ 5 https://news"
2020.amta-research.9,P16-1162,0,0.129248,"hat we found useful for training high-quality translation models. Configurable reversibility. The tokenization can be made reversible by marking either joints or spaces. These markers can be attached to the input tokens or injected as separate tokens. Advanced text segmentation. Several flags can finely control where to segment: on digits, on alphabet change, on case change, etc. Additionally, we introduced special control characters to prevent segmentation in delimited sequences. Subword training and encoding. The project can train and apply SentencePiece (Kudo and Richardson, 2018) and BPE (Sennrich et al., 2016) models, while making them compatible with all features listed above. 3.5 Model serving OpenNMT also provides components to serve translation models. OpenNMT-py includes a REST server that can manage multiple models and unload those that are not used. The server integrates CTranslate2 for efficient execution and Tokenizer for on-the-fly tokenization. OpenNMT-tf models are compatible with TensorFlow Serving which is a scalable solution to serve machine learning models. OpenNMT-tf models are also compatible with the OpenNMT-py REST server via the CTranslate2 integration. 4 Benchmarks OpenNMT, as"
2020.amta-research.9,D17-1239,0,0.0198425,"more than 3 years of active development, OpenNMT projects have been starred by over 7,400 users. A community forum3 is also home of 970 users and more than 9,800 posts about NMT research and how to use OpenNMT effectively. 2.2 Adoption Cited in over 700 scientific publications as of May 2020, OpenNMT has also been directly used in numerous research papers. The system is employed both to conduct new experiments and as a baseline for sequence-to-sequence approaches. OpenNMT was used for other tasks related to neural machine translation such as summarization (Gehrmann et al., 2018), data-totext (Wiseman et al., 2017), image-to-text (Deng et al., 2017), automatic speech recognition (Ericson, 2019) and semantic parsing (van Noord and Bos, 2017). OpenNMT also proved to be widespread in industry. Companies such as SYSTRAN (Crego et al., 2016), Booking.com (Levin et al., 2017), or Ubiqus4 are known to deploy OpenNMT models in production. We note that a number of industrial entities published scientific papers showing their internal experiments using the framework such as SwissPost (Girletti et al., 2019) and BNP Paribas (Mghabbar and Ratnamogan, 2020), while NVIDIA used OpenNMT as a benchmark for the release o"
2020.coling-main.348,P19-1175,0,0.0318136,"Missing"
2020.coling-main.348,W17-4716,0,0.0867319,"Vilar, 2018; Susanto et al., 2020) attempt to reduce the computational problem caused by using multiple beams in the inference, a well known weakness of this approach. Similar to the previous approach, constrained decoding does not consider target context when inserting translation terms, as it sets the target form and then produces a target context that fits this constraint. However, in a more realistic scenario, a source term may have multiple translation term inflections among which the MT engine should on-the-fly select the best one depending on the source and target context. Previously, Chatterjee et al. (2017) proposed a guide mechanism to enhance an NMT network with the ability to prioritize translation options presented in the form of XML annotations of source words. The mechanism is applied at every inference time-step, where the beam search is influenced with external suggestions coming from the attention model. Similarly, Zhang et al. (2018) exploit a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, from which they collect translation pieces. Then, the NMT model is modified to give an additional bonus to output sentences that contain the collecte"
2020.coling-main.348,W19-5402,0,0.0201923,"e-case where terminology is used in a system trained on generic data only. 1 Introduction High out-of-the-box quality for Neural Machine Translation (Bojar et al., 2016) has boosted the adoption of automatic translation by the industry and invigorated the research and development on domain adaption and integration of technology in human translation workflows. For instance, combination with translation memories (Bult´e and Tezcan, 2019; Xu et al., 2020), terminology handling (Hasler et al., 2018; Dinu et al., 2019), interactive translation (Peris and Casacuberta, 2019), post-editing modelling (Chatterjee et al., 2019) or dynamic adaptation (Farajian et al., 2017) are all different techniques to make machine translation part of real-life localization workflow. In this work, we focus on integrating terminology as a quick way to dynamically specialize a translation to a specific domain. Terminology is a key high quality asset maintained by language specialists as part of a translation project: it is a way to guarantee language consistency, certify translation accuracy and define constraints to human translation. Terminologists are putting a lot of effort to describe terms, including their morphology, their sy"
2020.coling-main.348,P18-1198,0,0.0126742,"a specific domain in a specific context. Terminology resources with all their sophistication have been the core building bricks and a continuous challenge to acquire in volume (Senellart et al., 2003) for rule-based engines. At the other extreme, they have been reduced to corpus or aligned “phrases” (Schwenk et al., 2008) for Statistical Machine Translation approaches, missing most of their intrinsic linguistic properties. In contrast, Neural Machine Translation operates on word and sentence representations in a continuous space so, on one hand, has access to deep actual linguistic knowledge (Conneau et al., 2018) and demonstrates a huge ability to generalize. But on the other hand, results are more difficult to interpret (Koehn and Knowles, 2017), and subsequently the translation process is far more complicated to control. Therefore, as for several other linguistic annotations, the challenge is how terminological information can be “passed” to the model. In this work, we extend existing work on terminology adaptation, show similarity with translation memory, and propose a new approach and new benchmark through a well-defined evaluation framework focusing on actual application of terminology and not ju"
2020.coling-main.348,N18-1119,0,0.0159609,"ained decoding enforces translation terms as decoding constraints applied at inference. Among others, Hokamp and Liu (2017) introduced grid beam search (GBS), an algorithm which employs a separate beam for each lexical constraint (translation term) aiming at ensuring the apparition of each given constraint in the translation hypothesis. The algorithm explores all possible constraints at each time-step, making sure not to generate a constraint that has already been generated in previous timesteps. The approach generates all the constraints in the final output. Other works (Hasler et al., 2018; Post and Vilar, 2018; Susanto et al., 2020) attempt to reduce the computational problem caused by using multiple beams in the inference, a well known weakness of this approach. Similar to the previous approach, constrained decoding does not consider target context when inserting translation terms, as it sets the target form and then produces a target context that fits this constraint. However, in a more realistic scenario, a source term may have multiple translation term inflections among which the MT engine should on-the-fly select the best one depending on the source and target context. Previously, Chatterjee e"
2020.coling-main.348,W08-0313,1,0.696081,"hich these terms apply, etc. From a human perspective, even though presentation and usage of dictionaries have evolved from ontology (as found in paper dictionary) to corpus-based presentation, looking up terms in a dictionary is the ultimate point of reference for validating the correct term for a specific domain in a specific context. Terminology resources with all their sophistication have been the core building bricks and a continuous challenge to acquire in volume (Senellart et al., 2003) for rule-based engines. At the other extreme, they have been reduced to corpus or aligned “phrases” (Schwenk et al., 2008) for Statistical Machine Translation approaches, missing most of their intrinsic linguistic properties. In contrast, Neural Machine Translation operates on word and sentence representations in a continuous space so, on one hand, has access to deep actual linguistic knowledge (Conneau et al., 2018) and demonstrates a huge ability to generalize. But on the other hand, results are more difficult to interpret (Koehn and Knowles, 2017), and subsequently the translation process is far more complicated to control. Therefore, as for several other linguistic annotations, the challenge is how terminolog"
2020.coling-main.348,2003.mtsummit-papers.46,1,0.276529,"inologists are putting a lot of effort to describe terms, including their morphology, their syntax, the semantic context in which these terms apply, etc. From a human perspective, even though presentation and usage of dictionaries have evolved from ontology (as found in paper dictionary) to corpus-based presentation, looking up terms in a dictionary is the ultimate point of reference for validating the correct term for a specific domain in a specific context. Terminology resources with all their sophistication have been the core building bricks and a continuous challenge to acquire in volume (Senellart et al., 2003) for rule-based engines. At the other extreme, they have been reduced to corpus or aligned “phrases” (Schwenk et al., 2008) for Statistical Machine Translation approaches, missing most of their intrinsic linguistic properties. In contrast, Neural Machine Translation operates on word and sentence representations in a continuous space so, on one hand, has access to deep actual linguistic knowledge (Conneau et al., 2018) and demonstrates a huge ability to generalize. But on the other hand, results are more difficult to interpret (Koehn and Knowles, 2017), and subsequently the translation process"
2020.coling-main.348,P16-1162,0,0.0398528,"terminologies with different translations according to the domain as can be seen in the next examples: accordance (noun) ECB/NEWS: conformidad (noun) EMEA: acuerdo (noun) EPPS/JRC: arreglo (noun) 4.3 move (verb) COMM: migrar (verb) ECB: pasar (verb) KDE4: mover (verb) NEWS: ascender (verb) move (noun) JRC: decisi´on (noun) KDE4: movimiento (noun) NEWS: maniobra (noun) Neural Machine Translation Our NMT models follow the state-of-the-art Transformer architecture described in Vaswani et al. (2017) implemented in the OpenNMT-tf13 toolkit. Before learning, we train a 32K joint byte-pair encoding (Sennrich et al., 2016) not applying on introduced placeholders. Note that all models are learnt using a joint source and target vocabulary and shared word embeddings to allow the injection of target words in the source stream. This is only required by one configuration but it enables a fair comparison and does not harm the rest of models. Additional details of our translation networks are given in Appendix A. 4.4 Experiments We evaluate the following configurations: • app: the target inflected term is appended to the source term. We use an additional parallel stream (factor) to indicate if each word is a term to in"
2020.ngt-1.25,D18-1045,0,0.0128552,"l. (2020) which shows that reducing the number of decoder layers can improve decoding speed at a very limited accuracy cost. We also keep the approach of running the models with a custom C++ runtime. This year we present CTranslate21 , an optimized and production-grade 1 https://github.com/OpenNMT/ CTranslate2 2 Teacher-student training We train our systems using a teacher-student approach (Kim and Rush, 2016). First, a large model (the teacher) is trained on all available bilingual data, including synthetic data such as backtranslations of monolingual target sentences (Sennrich et al., 2016; Edunov et al., 2018) and translations of monolingual source sentences (Zhang and Zong, 2016). Model ensembles are also typically used to build stronger teacher systems. Then, a small model (the student) is trained by means of minimizing the loss between the student and teacher systems with the goal of distilling the knowledge of the teacher (Kim and Rush, 2016; Zhang et al., 2018) into a smaller model with comparable accuracy results. Crego and Senellart (2016) show that student models can even outperform to some extent their teacher counterparts. Knowledge distillation is an effective approach to reduce the mode"
2020.ngt-1.25,D16-1139,0,0.0244369,"d Translation 2020 efficiency shared task. For WNMT 2018, we explored training and optimizations of small LSTM translation models combined with a customized runtime (Senellart et al., 2018). While this resulted in interesting decoding speed, there was still room for improvements in terms of quality, memory usage, and overall efficiency. For this 2020 edition, we focus on the standard Transformer architecture (Vaswani et al., 2017) that is now commonly used in production machine translation systems. Similar to our first participation, we train smaller models using the teacherstudent technique (Kim and Rush, 2016). We experiment with several encoder and decoder sizes following the work by Hongfei et al. (2020) which shows that reducing the number of decoder layers can improve decoding speed at a very limited accuracy cost. We also keep the approach of running the models with a custom C++ runtime. This year we present CTranslate21 , an optimized and production-grade 1 https://github.com/OpenNMT/ CTranslate2 2 Teacher-student training We train our systems using a teacher-student approach (Kim and Rush, 2016). First, a large model (the teacher) is trained on all available bilingual data, including synthet"
2020.ngt-1.25,P17-4012,1,0.518532,"the accuracy and efficiency results achieved by the submitted models. This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task. We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional optimizations and parallelization techniques, we create small, efficient, and highquality neural machine translation models. 1 Introduction This paper describes the OpenNMT (Klein et al., 2017) submissions to the Workshop on Neural Generation and Translation 2020 efficiency shared task. For WNMT 2018, we explored training and optimizations of small LSTM translation models combined with a customized runtime (Senellart et al., 2018). While this resulted in interesting decoding speed, there was still room for improvements in terms of quality, memory usage, and overall efficiency. For this 2020 edition, we focus on the standard Transformer architecture (Vaswani et al., 2017) that is now commonly used in production machine translation systems. Similar to our first participation, we train"
2020.ngt-1.25,P12-3005,0,0.113224,"Missing"
2020.ngt-1.25,W19-5333,0,0.0250952,"Missing"
2020.ngt-1.25,W18-6301,0,0.0193471,"sentences. We set the sampling weights of the selected data (a), (b), and (c) to 5, 2, and 2 respectively. That is, we consider a larger number of sentences synthesized from the English part of the bilingual data than from ParaCrawl or from the monolingual English data set. We use the OpenNMT-tf5 toolkit to train our student systems. Training is run on a single NVIDIA Tesla V100 GPU with an effective batch size of 25,000 tokens for the early epochs. Just before the final release, we train 10 additional epochs with a larger batch size by increasing the gradient update delay by a factor of 16 (Ott et al., 2018). Figure 1 shows the comparison with a larger batch size. We achieve an additional 0.1 to 0.2 BLEU using this 2 http://matrix.statmt.org/ http://statmt.org/wmt19/ translation-task.html 3 212 4 Due to the long decoding time of the teacher system, the English monolingual data was partially translated. The final data pool used for training consists of: (a) 7.4M bilingual data, (b) 26.1M ParaCrawl data, and (c) 127M English monolingual data. 5 https://github.com/OpenNMT/OpenNMT-tf Transformer Base (4:3 2xFFN) (6:3) (4:3) NEnc 6 4 6 4 NDec 6 3 3 3 h 8 8 8 8 dmodel 512 256 256 256 df f 2048 2048 102"
2020.ngt-1.25,P02-1040,0,0.106003,"Missing"
2020.ngt-1.25,D16-1160,0,0.0159878,"prove decoding speed at a very limited accuracy cost. We also keep the approach of running the models with a custom C++ runtime. This year we present CTranslate21 , an optimized and production-grade 1 https://github.com/OpenNMT/ CTranslate2 2 Teacher-student training We train our systems using a teacher-student approach (Kim and Rush, 2016). First, a large model (the teacher) is trained on all available bilingual data, including synthetic data such as backtranslations of monolingual target sentences (Sennrich et al., 2016; Edunov et al., 2018) and translations of monolingual source sentences (Zhang and Zong, 2016). Model ensembles are also typically used to build stronger teacher systems. Then, a small model (the student) is trained by means of minimizing the loss between the student and teacher systems with the goal of distilling the knowledge of the teacher (Kim and Rush, 2016; Zhang et al., 2018) into a smaller model with comparable accuracy results. Crego and Senellart (2016) show that student models can even outperform to some extent their teacher counterparts. Knowledge distillation is an effective approach to reduce the model size, thus lowering memory and computation requirements. 2.1 Teacher s"
2020.ngt-1.25,W18-6319,0,0.020918,"Missing"
2020.ngt-1.25,W18-2715,1,0.850979,"at are trained in a teacher-student setup. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional optimizations and parallelization techniques, we create small, efficient, and highquality neural machine translation models. 1 Introduction This paper describes the OpenNMT (Klein et al., 2017) submissions to the Workshop on Neural Generation and Translation 2020 efficiency shared task. For WNMT 2018, we explored training and optimizations of small LSTM translation models combined with a customized runtime (Senellart et al., 2018). While this resulted in interesting decoding speed, there was still room for improvements in terms of quality, memory usage, and overall efficiency. For this 2020 edition, we focus on the standard Transformer architecture (Vaswani et al., 2017) that is now commonly used in production machine translation systems. Similar to our first participation, we train smaller models using the teacherstudent technique (Kim and Rush, 2016). We experiment with several encoder and decoder sizes following the work by Hongfei et al. (2020) which shows that reducing the number of decoder layers can improve deco"
2020.ngt-1.25,P16-1009,0,0.0259392,"he work by Hongfei et al. (2020) which shows that reducing the number of decoder layers can improve decoding speed at a very limited accuracy cost. We also keep the approach of running the models with a custom C++ runtime. This year we present CTranslate21 , an optimized and production-grade 1 https://github.com/OpenNMT/ CTranslate2 2 Teacher-student training We train our systems using a teacher-student approach (Kim and Rush, 2016). First, a large model (the teacher) is trained on all available bilingual data, including synthetic data such as backtranslations of monolingual target sentences (Sennrich et al., 2016; Edunov et al., 2018) and translations of monolingual source sentences (Zhang and Zong, 2016). Model ensembles are also typically used to build stronger teacher systems. Then, a small model (the student) is trained by means of minimizing the loss between the student and teacher systems with the goal of distilling the knowledge of the teacher (Kim and Rush, 2016; Zhang et al., 2018) into a smaller model with comparable accuracy results. Crego and Senellart (2016) show that student models can even outperform to some extent their teacher counterparts. Knowledge distillation is an effective appro"
2020.wmt-1.63,N19-1423,0,0.00803713,"f concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common fashion in recent works (Feng et al., 2020; Chang et al., 2020). However, deep models suffer from computation complexity when applied onthe-fly for inference. We propose an extension of sent2vec (Pagliardini et al., 2018) to compute sentence representations that also inherits from the computationally efficient bilinear models (Mikolov et al., 2013a,b; Pennington et al., 2014). Similar to our work, Farajian et al. (2017) and Li et al. (2018) retrieve similar sentence to dynamically"
2020.wmt-1.63,P19-1294,0,0.0266447,"ed: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous work (Bulte and Tez"
2020.wmt-1.63,N19-1191,0,0.0170632,"based MT system. Our work, in contrast, integrates similar sentences in both source and target sides and employs similar translations found in parallel as well as monolingual data sets. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common fashion in recent works (Feng et al., 2020; Chang et al., 2020). However, deep models suffer from computation complexity when applied onthe-fly for inference. We propose an extension of sent2vec (Pagliardini et al., 2018) to compute sentence representations that also inherits from t"
2020.wmt-1.63,N18-1118,0,0.0172585,"xplored in Schwenk et al. (2019b), where the authors use multilingual sentence embeddings to retrieve pairs of similar sentences and train models uniquely with such sentences. Previously, Niehues et al. (2016) augmented input sentences with pre-translations generated by a phrase-based MT system. Our work, in contrast, integrates similar sentences in both source and target sides and employs similar translations found in parallel as well as monolingual data sets. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common f"
2020.wmt-1.63,P19-1175,0,0.129039,"improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous work (Bulte and Tezcan, 2019; Xu et al., 2020), we consider similar translations as external cues that can influence the translation process. We push this concept further: a) b"
2020.wmt-1.63,W17-4713,0,0.366859,"NVIDIA V100 GPU. We limit the length of training sentences to 300 BPE tokens (Sennrich et al., 2016c) in both source and target sides to enable the integration of similar sentences. We use a joint BPE-vocabulary of size 32K for both source and target texts. Inference is performed with a beam size of 5 using CTranslate210 , a custom C++ runtime inference engine for OpenNMT models that enables fast CPU decoding and also implements prefix decoding. For evaluation, we report BLEU (Papineni et al., 2002) scores computed by detokenized case-sensitive multi-bleu.perl11 . We re-implement the work of Farajian et al. (2017) as a contrastive model that we denote µadapt. Note that we only experiment with the basic version of this work, where the closest neighbours of the input sentence are first retrieved from the memory and then used to fine-tune a generic model during 15 additional iterations with a fixed learning rate of 0.0005; the fine-tuned model is then used to produce the translation of the given input sentence. In addition, Farajian et al. (2017) include a variant where learning rate and number of epochs are dynamically adapted considering sentence similarity. Adaptation is run on a sentenceby-sentence ba"
2020.wmt-1.63,P17-4012,1,0.0302657,"he French-side of the WikiMatrix data (Schwenk et al., 2019a). We randomly split the parallel corpora by keeping 500 sentences for validation, 1, 000 sentences for testing and the rest for training. All data is preprocessed using the OpenNMT tokenizer6 (conservative mode). Sentence Retrieval To identify similar translations using distributed representations, we use the faiss8 search toolkit (Johnson et al., 2019) through its Python API with exact FlatIP index. Translation Our NMT models rely on the Transformer base architecture of Vaswani et al. (2017), implemented in the OpenNMT-tf9 toolkit (Klein et al., 2017). We use the standard setting of Transformers for all experiments: size of word embedding: 512; size of hidden layers: 512; size of inner feed-forward layer: 2, 048; number of heads: 8; number of layers in the encoder or in the decoder: 6. In the tgt1 +STU scheme, token (508 cells) and STU (4 4 Freely available from http://opus.nlpl.eu http://data.statmt.org/news-crawl/ 6 https://github.com/OpenNMT/Tokenizer 5 519 7 Optimization experiments on a held-out development set are carried out for both models. 8 https://github.com/facebookresearch/ faiss 9 https://github.com/OpenNMT/OpenNMT-tf cells)"
2020.wmt-1.63,2016.amta-researchers.9,0,0.008848,"-the-fly priming compares to micro-adaptation (fine-tuning). Finally, we 1 https://github.com/jmcrego/cbon 516 Proceedings of the 5th Conference on Machine Translation (WMT), pages 516–527 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics show that our priming approach can also be used with monolingual data, providing a scenario where NMT can be effectively helped by large amounts of available data. Our proposal does not require to change the NMT architectures or algorithms, relying solely on input preprocessing and on prefix (forced) decoding (Santy et al., 2019; Knowles and Koehn, 2016), a feature already implemented in many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficient matrix factor (bilinear"
2020.wmt-1.63,kobus-etal-2017-domain,1,0.851148,"s been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous"
2020.wmt-1.63,2010.jec-1.4,1,0.473968,"arding the corpora used in this work4 (Tiedemann, 2012). Statistics are computed after splitting off punctuation. Corpus EPPS NEWS WIKI ECB EMEA JRC GNOME KDE4 WIKI NEWS #Sents (K) System Configurations Lmean English French Vocab (K) English Parallel Corpora 1,992.8 27.7 32.0 129.5 315.3 25.3 31.7 90.5 749.0 25.9 23.5 527.5 174.1 28.6 33.8 45.3 336.8 16.8 20.3 62.8 475.2 30.1 34.5 81.0 51.9 9.6 11.6 19.0 163.9 9.1 12.4 48.7 Monolingual Corpora 6,426.8 24.1 83,567.8 25.5 - French 149.2 96.7 506.6 53.5 68.9 83.5 21.6 64.7 1,626.3 3,444.1 Similarity For fuzzy matching FM we follow several works (Koehn and Senellart, 2010; Bulte and Tezcan, 2019; Xu et al., 2020) and keep the n-best matches when FM(s1 , s2 ) ≥ 0.5 with no approximation. Concerning S2V, the model is trained with default options during 20 epochs using all training data. We use an embedding dimension of 300 cells. Regarding CBON, we learn models using also the entire training data during one epoch (∼50,000 iterations). Similarly to S2V we use 10 negative samples per positive word to approximate the softmax, a batch size of 2k examples, and embedding size of 300 cells. We build CBON models using 3-grams and 4-grams to enable a comparison with sent"
2020.wmt-1.63,2020.lrec-1.302,0,0.0229122,"Missing"
2020.wmt-1.63,L18-1146,0,0.064128,"Missing"
2020.wmt-1.63,C16-1172,0,0.0170125,"Missing"
2020.wmt-1.63,N18-1049,0,0.0112408,"helped by large amounts of available data. Our proposal does not require to change the NMT architectures or algorithms, relying solely on input preprocessing and on prefix (forced) decoding (Santy et al., 2019; Knowles and Koehn, 2016), a feature already implemented in many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficient matrix factor (bilinear) models (Mikolov et al., 2013a,b; Pennington et al., 2014). Borrowing the notations of Pagliardini et al. (2018), training the model is formalized as an optimization problem: min U ,V This section describes our framework for priming neural MT with similar translations. We follow the work by (Bulte and Tezcan, 2019; Xu et al., 2020) and build a translation model that incorporates similar"
2020.wmt-1.63,P02-1040,0,0.121238,"000 and update the learning rate for every 8 iterations. Models are optimised during 300K iterations, using a single NVIDIA V100 GPU. We limit the length of training sentences to 300 BPE tokens (Sennrich et al., 2016c) in both source and target sides to enable the integration of similar sentences. We use a joint BPE-vocabulary of size 32K for both source and target texts. Inference is performed with a beam size of 5 using CTranslate210 , a custom C++ runtime inference engine for OpenNMT models that enables fast CPU decoding and also implements prefix decoding. For evaluation, we report BLEU (Papineni et al., 2002) scores computed by detokenized case-sensitive multi-bleu.perl11 . We re-implement the work of Farajian et al. (2017) as a contrastive model that we denote µadapt. Note that we only experiment with the basic version of this work, where the closest neighbours of the input sentence are first retrieved from the memory and then used to fine-tune a generic model during 15 additional iterations with a fixed learning rate of 0.0005; the fine-tuned model is then used to produce the translation of the given input sentence. In addition, Farajian et al. (2017) include a variant where learning rate and nu"
2020.wmt-1.63,D14-1162,0,0.118332,"many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficient matrix factor (bilinear) models (Mikolov et al., 2013a,b; Pennington et al., 2014). Borrowing the notations of Pagliardini et al. (2018), training the model is formalized as an optimization problem: min U ,V This section describes our framework for priming neural MT with similar translations. We follow the work by (Bulte and Tezcan, 2019; Xu et al., 2020) and build a translation model that incorporates similar translations from a translation memory (TM) to boost translation accuracy. In this work, TMs are parallel corpora containing translations falling in the same domain as test sentences. We first describe the methods employed in this work to compute sentence similarity."
2020.wmt-1.63,N18-1202,0,0.0171998,"monolingual data sets. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) further evaluated by Bawden et al. (2018) in the context of tackling discourse phenomena. Our work employs force decoding to allow including true translations in the decoder targetside. Thus, avoiding the error propagation problem (Ranzato et al., 2016) of longer sequences in auto-regressive models. Bapna and Firat (2019) propose a neural MT model that incorporates retrieved neighbours relying on local phrase level similarities. Using deep pre-trained models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Le et al., 2020; Conneau and Lample, 2019) to compute contextualized sentence representations has become common fashion in recent works (Feng et al., 2020; Chang et al., 2020). However, deep models suffer from computation complexity when applied onthe-fly for inference. We propose an extension of sent2vec (Pagliardini et al., 2018) to compute sentence representations that also inherits from the computationally efficient bilinear models (Mikolov et al., 2013a,b; Pennington et al., 2014). Similar to our work, Farajian et al. (2017) and Li et al. (2018"
2020.wmt-1.63,D19-3018,0,0.0124582,"by analyzing how on-the-fly priming compares to micro-adaptation (fine-tuning). Finally, we 1 https://github.com/jmcrego/cbon 516 Proceedings of the 5th Conference on Machine Translation (WMT), pages 516–527 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics show that our priming approach can also be used with monolingual data, providing a scenario where NMT can be effectively helped by large amounts of available data. Our proposal does not require to change the NMT architectures or algorithms, relying solely on input preprocessing and on prefix (forced) decoding (Santy et al., 2019; Knowles and Koehn, 2016), a feature already implemented in many NMT toolkits. The remainder of the paper is organized as follows: Section 2 gives details regarding our priming approach. The experimental framework is presented in Section 3. Results and discussion are respectively in Sections 4 and 5. We review related work in Section 6 and conclude in Section 7. 2 S2V: we use sent2vec3 (Pagliardini et al., 2018) to generate sentence embeddings. The network implements a simple but efficient unsupervised objective to train distributed representations for sentences. The model is based on efficie"
2020.wmt-1.63,2021.eacl-main.115,0,0.0345079,"Missing"
2020.wmt-1.63,W17-2619,0,0.0145288,"language (French in this work) and translate each sentence back into English to obtain synthetic parallel data. Similar to back-translation experiments in Sennrich et al. (2016b), we only use original (human-crafted) target-language data. We expect this to add less noise than incorporating synthetic target-language data into the NMT input. Once translated into English, the various priming approaches identify similar synthetic sentences and injects both the synthetic source and original target in the NMT input stream. Note that crosslingual sentence embedding models exist (Sabet et al., 2019; Schwenk and Douze, 2017; Conneau and Lample, 2019) but our preliminary experiments using these tools did not show satisfactory results. Thus, we exploit large collections of French texts for the News and Wikipedia domains (as detailed in Table 1) that we translate into English to enable similarity retrieval. Table 4 reports BLEU scores obtained by our best performing network CBON following the s+t5 scheme. The supplementary number of similar sentences (468 input sentences have similar translations) collected for the WIKI domain over parallel and mono521 lingual12 corpora (par+mon) yields an improvement of 2 BLEU poi"
2020.wmt-1.63,N16-1005,0,0.152513,"mputer vision priming has been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Fol"
2020.wmt-1.63,P16-1009,0,0.160789,"mputer vision priming has been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Fol"
2020.wmt-1.63,P16-1162,0,0.215406,"mputer vision priming has been broadly studied: for instance, in Rosenfeld et al. (2018), the authors introduce a cue about the presence of a certain class of object in an image that significantly improves object detection performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Fol"
2020.wmt-1.63,tiedemann-2012-parallel,0,0.0135836,"is section gives learning/inference details of the various systems used in this work. Corpora We experiment with the English-French language pair and data originating from eight domains, corresponding to texts from three European institutions: the European Parliament (EPPS), the European Medicines Agency (EMEA) and the European Central Bank (ECB); Legislative texts of the European Union (JRC); IT-domain corpora corresponding to KDE4 and GNOME; News Commentaries (NEWS); and parallel sentences extracted from Wikipedia (WIKI). Table 1 contains statistics regarding the corpora used in this work4 (Tiedemann, 2012). Statistics are computed after splitting off punctuation. Corpus EPPS NEWS WIKI ECB EMEA JRC GNOME KDE4 WIKI NEWS #Sents (K) System Configurations Lmean English French Vocab (K) English Parallel Corpora 1,992.8 27.7 32.0 129.5 315.3 25.3 31.7 90.5 749.0 25.9 23.5 527.5 174.1 28.6 33.8 45.3 336.8 16.8 20.3 62.8 475.2 30.1 34.5 81.0 51.9 9.6 11.6 19.0 163.9 9.1 12.4 48.7 Monolingual Corpora 6,426.8 24.1 83,567.8 25.5 - French 149.2 96.7 506.6 53.5 68.9 83.5 21.6 64.7 1,626.3 3,444.1 Similarity For fuzzy matching FM we follow several works (Koehn and Senellart, 2010; Bulte and Tezcan, 2019; Xu e"
2020.wmt-1.63,W17-4811,0,0.174236,"sed in this paper also addresses the unrelated word problem, at a much reduced computational cost. It considers both sides of similar translations (sk and tk ). Training streams take the form: src: tgt: sk ◦ ... ◦ s2 ◦ s1 ◦ s tk ◦ ... ◦ t2 ◦ t1 ◦ t In inference, target-side similar translations tk are used by the model as a target prefix. The initial steps of the beam search use the given prefix tk ◦ ... ◦ t2 ◦ t1 ◦ in forced decoding mode, returning to a regular beam search after the last ◦ token is generated. A similar strategy of concatenating previous and current sentences was explored by Tiedemann and Scherrer (2017) in the context of handling discourse phenomena. However, since we use true translation as prefixes, our strategy does not suffer from exposure bias (Ranzato et al., 2016) and the subsequent error propagation problem. Continuing on our running example, during inference the model receives: input: prefix: measles vaccin ◦ pertussis vaccin vaccin contre la rougeole ◦ the encoder embeds the input stream, and forcedecodes the target prefix, before starting the translation generation. Note that during beam search, the decoder has thus access both to all input tokens (sk and s) as well as to similar"
2020.wmt-1.63,2020.acl-main.144,1,0.145304,"n performance. Concerning language generation, Brown et al. (2020) use a combination of prompt and example to guide the GPT-3 network when performing a task, where the prompt is a sentence that describes the task (i.e. “Translate from English to French”); and is followed by an example of the task (i.e. “sea otter ; loutre de mer”). In the context of NMT, experiments reported (Sennrich et al., 2016a; Kobus et al., 2017; Dinu et al., 2019) aim at influencing translation inference with respectively politeness, domain and terminology constraints. More related to our work, (Bulte and Tezcan, 2019; Xu et al., 2020) introduce a simple and elegant framework where similar translations (cues) are used to prime an NMT model, effectively boosting translation accuracy. In all cases, priming is performed by injecting cues in the input stream prior to inference decoding. In this paper, we extend a framework that mimics the priming process in neural networks, in the context of machine translation. Following up on previous work (Bulte and Tezcan, 2019; Xu et al., 2020), we consider similar translations as external cues that can influence the translation process. We push this concept further: a) by proposing a nove"
2020.wmt-1.72,2020.acl-main.688,0,0.0315866,"c domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a random one, especially when adaptation data is scarce. Indeed, studies of transfer learning for NMT such as Artetxe et al. (2020); Aji et al. (2020) have confirmed this claim in extensive experiments. Full fine-tuning, that adapts all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and res"
2020.wmt-1.72,2020.acl-main.421,0,0.0420588,"g NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a random one, especially when adaptation data is scarce. Indeed, studies of transfer learning for NMT such as Artetxe et al. (2020); Aji et al. (2020) have confirmed this claim in extensive experiments. Full fine-tuning, that adapts all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly bec"
2020.wmt-1.72,2010.amta-papers.16,0,0.0611561,"Missing"
2020.wmt-1.72,D19-1165,0,0.370734,"baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and resource-expensive. Several recent studies (e.g. (Vilar, 2018; Wuebker et al., 2018; Michel and Neubig, 2018; Bapna and Firat, 2019)) have proposed more lightweight schemes to perform domain adaptation, while also preserving the value of pre-trained models. Our main inspiration is the latter work, whose proposal relies on small adapter components that are plugged in each hidden layer. These adapters are trained only with the in-domain data, keeping the pre-trained model frozen. Because these additional 617 Proceedings of the 5th Conference on Machine Translation (WMT), pages 617–628 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics adapters are very small compared to the size of the baseline mo"
2020.wmt-1.72,W17-4712,0,0.0824316,"e them to full fine-tuning on the one hand, and to two variants of the residual adapter architecture on the other hand. The reference methods included in our experiments are the following: • a system using “domain control” (Kobus et al., 2017). In this approach, domain information is introduced either as an additional token for each source sentence (DC-Tag) or in the form of a supplementary feature for each word (DC-Feat); • a system using lexicalized domain representations (Pham et al., 2019): word embeddings are composed of a generic and a domainspecific part (LDR); • the three proposals of Britz et al. (2017). TTM is a feature-based approach where the domain tag is introduced as an extra word on the target side. The training uses reference tags and inference is performed with predicted tags, just like for regular target words. DM is a multi-task learner where a domain classifier is trained on top of the MT encoder, so as to make it aware of domain differences; ADM is the adversarial version of DM, pushing the encoder towards learning domain-independent source representations. These methods only use domain labels in training. Model / Domain Mixed FT-Full DC-Tag DC-Feat LDR TTM DM ADM Res-Adap Res-A"
2020.wmt-1.72,W17-4713,0,0.0169783,"cation or domain normalization on the source or target side. A contribution of this study is an adversarial training scheme to normalize representations across domains and make the combination of multiple data sources more effective. Similar techniques (parameter sharing, automatic domain classification/normalization) are at play in Zeng et al. (2018): in this work, the lower layers of the MT use auxiliary classification tasks to disentangle domain-specific from domain-agnostic representations. These representations are first processed separately, then merged to compute the final translation. Farajian et al. (2017); Li et al. (2018) are two recent representatives of the instance-based approach: for each test sentence, a small adaptation corpus is collected based on similarity measures and used to fine-tune a mix-domain model. As shown in the former work, also adapting the training regime on a per sentence basis is crucial to make these techniques really effective. Finally, note that a distinct evolution of the residual adapter model of Bapna and Firat (2019) is presented in Sharaf et al. (2020), where meta-learning techniques are used to make fine-tuning more effective in a standard domain-adaptation se"
2020.wmt-1.72,N09-1068,0,0.0896083,"Missing"
2020.wmt-1.72,2012.eamt-1.60,0,0.0125413,"tal settings 3.1 Data and metrics We perform our experiments with two translation pairs involving multiple domains: English-French (En→Fr) and English-German (En→De). For the former pair, we use texts3 initially from 6 domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 ( M E D )4 , the European Central Bank corpus ( B A N K ) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus ( L A W ) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME and PHP from Opus collection (Tiedemann, 2009), collectively merged in a I T -domain, Ted Talks ( T A L K ) (Cettolo et al., 2012), and the Koran ( R E L ). Complementary experiments also use v12 of the News Commentary corpus ( N E W S ). Corpus statistics are in Table 1. En→De is a much larger task, for which we use corpora distributed for the News task of WMT205 including: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European Medicines Agency corpus ( M E D )6 , Press Release Database of European Commission corpus, News Commentary v15 corpus, Common Crawl corpus ( N E W S ), Europarl v10 ( G O V ), Tilde MODEL - czechtourism ( T O U R )7 , Paracrawl and Wikipedia Ma"
2020.wmt-1.72,2016.amta-researchers.10,0,0.019201,"lark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, using a topic vector to describe the 623 Model / Domain Mixed Res-Adap Res-Adap(2,4,6) Res-Adap(6) Res-Adap(4) Res-Adap(2) Res-Adap-WD Res-Adap-LR MED LAW BANK TALK IT REL AVG PARAMS 37.3 37.3 37.7 37.7 37.9 37.8 37.2 37.4 54.6 57.9 57 55.8 55.6 55.5 56.0 56.1 50.1 53.9 53 51.5 51.7 51.4 52.9 51.8 33.5 33.8 33.3 33.9 33.7 34 33.4 33.3 43.2 46.7 45 43.6 44.4 43.8 46.0 45.0 77.5 90.2 90 89.2 88.7 86.7 90.6 89.7 49.4 53.3 52.7 51.9 52 51.5 52.7 52.2 65M/0 65M/12M 65M/6M 65M/2M 65M/2M 65M/2M 65M/12M 65M/12M Table 4: Translation performance of various fine-"
2020.wmt-1.72,C18-1111,0,0.0172823,"and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model con"
2020.wmt-1.72,1983.tc-1.13,0,0.339315,"Missing"
2020.wmt-1.72,D08-1072,0,0.169703,"er regularization, which penalizes the output of the adapters, corresponding to the following objective: X 1 (− log(P (y|x)) #(x, y) x,y X +λ kADAP(i) (hi (x, y))k2 ) ¯= L i∈{1,..,6}⊗{enc,dec} Finally, another independent design choice relates to the training strategy for adapters. A first option is to generalize supervised domain adaptation to multi-domain adaptation and to proceed in two steps: (a) train a generic model with all the available data; (b) train each adapter layer with domain-specific data, keeping the generic model parameters unchanged. Another strategy is to adopt the view of Dredze and Crammer (2008), where the multi-domain setting is viewed as an instance of multi-task learning (Caruana, 1997) with each domain corresponding to a specific task. This suggests training all the parameters from scratch, as we would do in a multi-task mode. The generic parameters will still depend on all the available data, while each adapter will only be trained with the corresponding in-domain data. Figure 1: Highway residual adapter network 2.3 Gated Residual Adapters The basic architecture presented above rests on a rather simplistic view of “domains” as made of well-separated and unrelated pieces of texts"
2020.wmt-1.72,P13-2119,0,0.0200228,"apter model and open perspective to also make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parall"
2020.wmt-1.72,P12-2023,0,0.0258425,"Crammer, 2008; Finkel and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, using a topic vector to describe the 623 Model / Domain Mixed R"
2020.wmt-1.72,E14-1035,0,0.0224033,"and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, using a topic vector to describe the 623 Model / Domain Mixed Res-Adap Res-Adap(2,4,6"
2020.wmt-1.72,2015.mtsummit-papers.19,0,0.0200802,"common scenario in natural language processing (Dredze and Crammer, 2008; Finkel and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on the target side, us"
2020.wmt-1.72,D13-1176,0,0.0376761,"odel intact and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea using two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model and open perspective to also make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (201"
2020.wmt-1.72,P17-4012,1,0.831901,"Missing"
2020.wmt-1.72,kobus-etal-2017-domain,1,0.929226,"to its size; we then sample a batch of 12,288 tokens that is used to update the shared parameters and the parameters of the corresponding adapter. Models for En→De are larger and rely on embeddings as well as hidden layers of size 1024; each 621 3.3 Multi-domain systems In this section, we evaluate several proposals from the literature on multi-domain adaptation and compare them to full fine-tuning on the one hand, and to two variants of the residual adapter architecture on the other hand. The reference methods included in our experiments are the following: • a system using “domain control” (Kobus et al., 2017). In this approach, domain information is introduced either as an additional token for each source sentence (DC-Tag) or in the form of a supplementary feature for each word (DC-Feat); • a system using lexicalized domain representations (Pham et al., 2019): word embeddings are composed of a generic and a domainspecific part (LDR); • the three proposals of Britz et al. (2017). TTM is a feature-based approach where the domain tag is introduced as an extra word on the target side. The training uses reference tags and inference is performed with predicted tags, just like for regular target words. D"
2020.wmt-1.72,W17-3204,0,0.0198096,"lso make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of"
2020.wmt-1.72,L18-1146,0,0.0348477,"zation on the source or target side. A contribution of this study is an adversarial training scheme to normalize representations across domains and make the combination of multiple data sources more effective. Similar techniques (parameter sharing, automatic domain classification/normalization) are at play in Zeng et al. (2018): in this work, the lower layers of the MT use auxiliary classification tasks to disentangle domain-specific from domain-agnostic representations. These representations are first processed separately, then merged to compute the final translation. Farajian et al. (2017); Li et al. (2018) are two recent representatives of the instance-based approach: for each test sentence, a small adaptation corpus is collected based on similarity measures and used to fine-tune a mix-domain model. As shown in the former work, also adapting the training regime on a per sentence basis is crucial to make these techniques really effective. Finally, note that a distinct evolution of the residual adapter model of Bapna and Firat (2019) is presented in Sharaf et al. (2020), where meta-learning techniques are used to make fine-tuning more effective in a standard domain-adaptation setting. 5 tional co"
2020.wmt-1.72,2015.iwslt-evaluation.11,0,0.0287615,"4; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a rand"
2020.wmt-1.72,2020.ngt-1.5,0,0.0261148,"Missing"
2020.wmt-1.72,steinberger-etal-2006-jrc,0,0.0761424,"ility P (k|hL [t]) of domain k as the value for zk (hL [t]). Training gated residual adapters thus comprises three steps, instead of two for the baseline version: 3 Experimental settings 3.1 Data and metrics We perform our experiments with two translation pairs involving multiple domains: English-French (En→Fr) and English-German (En→De). For the former pair, we use texts3 initially from 6 domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 ( M E D )4 , the European Central Bank corpus ( B A N K ) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus ( L A W ) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME and PHP from Opus collection (Tiedemann, 2009), collectively merged in a I T -domain, Ted Talks ( T A L K ) (Cettolo et al., 2012), and the Koran ( R E L ). Complementary experiments also use v12 of the News Commentary corpus ( N E W S ). Corpus statistics are in Table 1. En→De is a much larger task, for which we use corpora distributed for the News task of WMT205 including: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European Medicines Agency corpus ( M E D )6 , Press Release Database of European Co"
2020.wmt-1.72,P18-2050,0,0.0197571,"all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and resource-expensive. Several recent studies (e.g. (Vilar, 2018; Wuebker et al., 2018; Michel and Neubig, 2018; Bapna and Firat, 2019)) have proposed more lightweight schemes to perform domain adaptation, while also preserving the value of pre-trained models. Our main inspiration is the latter work, whose proposal relies on small adapter components that are plugged in each hidden layer. These adapters are trained only with the in-domain data, keeping the pre-trained model frozen. Because these additional 617 Proceedings of the 5th Conference on Machine Translation (WMT), pages 617–628 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics adapters are very small compared to the"
2020.wmt-1.72,D18-1103,0,0.0223531,"Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are available for the domain of interest, one then adapts the pre-trained model by resuming training with the sole in-domain corpus. It is a conjecture that the pretrained model constitutes a better initialization than a random one, especially whe"
2020.wmt-1.72,P02-1040,0,0.109675,"luding: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European Medicines Agency corpus ( M E D )6 , Press Release Database of European Commission corpus, News Commentary v15 corpus, Common Crawl corpus ( N E W S ), Europarl v10 ( G O V ), Tilde MODEL - czechtourism ( T O U R )7 , Paracrawl and Wikipedia Matrix ( W E B ). Statistics are in Table 2. We randomly select in each corpus a development and a test set of 1,000 lines each and keep the rest for training.8 Development sets help choose the best model according to the average BLEU score (Papineni et al., 2002).9 1. learn a generic model with mixed corpora from multiple domains. 3.2 2. train a domain classifier on top of the encoder and decoder; during this step, the parameters of the generic model are frozen. This model computes the posterior domain probability P (k|hL [t]) for each word wt , based on the representation computed by the last layer. 3. train the parameters of adapters with indomain data separately for each domain, while freezing all the other parameters. 2 The term “word” is employed here by mere convenience, as systems only manipulate sub-lexical BPE units; furthermore, the values o"
2020.wmt-1.72,tiedemann-2012-parallel,0,0.0211147,"the encoder (resp. decoder) as input and use the posterior probability P (k|hL [t]) of domain k as the value for zk (hL [t]). Training gated residual adapters thus comprises three steps, instead of two for the baseline version: 3 Experimental settings 3.1 Data and metrics We perform our experiments with two translation pairs involving multiple domains: English-French (En→Fr) and English-German (En→De). For the former pair, we use texts3 initially from 6 domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 ( M E D )4 , the European Central Bank corpus ( B A N K ) (Tiedemann, 2012); The JRC-Acquis Communautaire corpus ( L A W ) (Steinberger et al., 2006), documentations for KDE, Ubuntu, GNOME and PHP from Opus collection (Tiedemann, 2009), collectively merged in a I T -domain, Ted Talks ( T A L K ) (Cettolo et al., 2012), and the Koran ( R E L ). Complementary experiments also use v12 of the News Commentary corpus ( N E W S ). Corpus statistics are in Table 1. En→De is a much larger task, for which we use corpora distributed for the News task of WMT205 including: European Central Bank corpus ( B A N K ), European Economic and Social Committee corpus ( E C O ), European"
2020.wmt-1.72,W18-5431,0,0.0197505,"ng. Likewise, it might be meaningful to explore ways to share subsets of adapters across domains. This, in turn, raises the issue of which layer(s) to adapt, a question that can be approached in the light of recent analyses of Transformers models, which conjecture that the higher layers encode 618 1 In the decoder, the stack of self-attention and cross encoder-decoder attention only counts as one attention layer and only corresponds to one residual adapter. global patterns with a more “semantic” interpretation, while the lower layers encode local patterns akin to morpho-syntactic information (Raganato and Tiedemann, 2018). A related question concerns the regularization of adapter layers to mitigate overfitting. Reducing the number of adapters, or their dimensions, is simple, but such choices are difficult to optimize numerically – an issue that becomes important as the number of domain grows. Less naive alternatives can also be entertained, such as applying weight decay or layer regularization to the adapter. Implementing these requires to modify the objective function in a way that still allows for a smooth optimization problem. For instance, weight decay applies a penalization on the weights of the adapters,"
2020.wmt-1.72,N18-2080,0,0.0270309,"ents. Full fine-tuning, that adapts all the parameters of a baseline model usually significantly improves the quality of the NMT for the chosen domain. However, it also yields large losses in translation quality for other domains, a phenomenon referred to as “catastrophic forgetting” in the neural network literature (McCloskey and Cohen, 1989). Therefore, a fully fine-tuned model is only useful to one target domain. As the number of domains to handle grows, training, and maintaining a separate model for each task can quickly become tedious and resource-expensive. Several recent studies (e.g. (Vilar, 2018; Wuebker et al., 2018; Michel and Neubig, 2018; Bapna and Firat, 2019)) have proposed more lightweight schemes to perform domain adaptation, while also preserving the value of pre-trained models. Our main inspiration is the latter work, whose proposal relies on small adapter components that are plugged in each hidden layer. These adapters are trained only with the in-domain data, keeping the pre-trained model frozen. Because these additional 617 Proceedings of the 5th Conference on Machine Translation (WMT), pages 617–628 c Online, November 19–20, 2020. 2020 Association for Computational Ling"
2020.wmt-1.72,P13-1082,0,0.0169293,"erogeneous sources is a common scenario in natural language processing (Dredze and Crammer, 2008; Finkel and Manning, 2009). It is thus no wonder that the design of multi-domain systems has been proposed for many tasks. In this short survey, we exclusively focus on machine translation; it is likely that similar methods (parameter sharing, instance selection/weighting, adversarial training, etc) have also been proposed for other tasks. Early approaches to multi-domain MT were proposed for statistical MT, either considering multiple data sources (eg. Banerjee et al. (2010); Clark et al. (2012); Sennrich et al. (2013); Huck et al. (2015)) or domains containing several topics (Eidelman et al., 2012; Hasler et al., 2014). Two main strategies emerge: feature-based methods, where domain labels are integrated through supplementary features; and instance-based methods, involving a measure of similarity between train and test domains. The former approach has also been adapted to NMT: Kobus et al. (2017); Tars and Fishel (2018) use an additional domain feature in an RNN model, in the form of an extra domain-token or of additional domain-features associated with each word. Chen et al. (2016) apply domain control on"
2020.wmt-1.72,D18-1041,0,0.0177965,"the work by Jiang et al. (2019), who consider a Transformer model containing both domain-specific and domain-agnostic heads. Britz et al. (2017) study three general techniques to take domain information into account in training: they rely on either domain classification or domain normalization on the source or target side. A contribution of this study is an adversarial training scheme to normalize representations across domains and make the combination of multiple data sources more effective. Similar techniques (parameter sharing, automatic domain classification/normalization) are at play in Zeng et al. (2018): in this work, the lower layers of the MT use auxiliary classification tasks to disentangle domain-specific from domain-agnostic representations. These representations are first processed separately, then merged to compute the final translation. Farajian et al. (2017); Li et al. (2018) are two recent representatives of the instance-based approach: for each test sentence, a small adaptation corpus is collected based on similarity measures and used to fine-tune a mix-domain model. As shown in the former work, also adapting the training regime on a per sentence basis is crucial to make these tec"
2020.wmt-1.72,D16-1163,0,0.0230272,"en perspective to also make adapted models more robust to label domain errors. 1 Introduction Owing to multiple improvements, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) nowadays delivers useful outputs for many language pairs. However, as many deep learning models, NMT systems need to be trained with sufficiently large amounts of data to reach their best performance. Therefore, the quality of the translation of NMT models is still limited in low-resource language or domain conditions (Duh et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). While many approaches have been proposed to improve the quality of NMT models in low-resource domains (see the recent survey of Chu and Wang (2018)), full fine-tuning (Luong and Manning, 2015; Neubig and Hu, 2018) of a generic baseline model remains the dominant supervised approach when adapting NMT models to specific domains. Under this view, building adapted systems is a two-step process: (a) one first trains NMT with the largest possible parallel corpora, aggregating texts from multiple, heterogeneous sources; (b) assuming that in-domain parallel documents are av"
C08-1115,E06-1005,1,\N,Missing
C08-1115,W07-0728,1,\N,Missing
C08-1115,W03-1729,1,\N,Missing
C08-1115,P07-1040,0,\N,Missing
C08-1115,W07-0732,1,\N,Missing
C08-1115,W07-0724,1,\N,Missing
C08-1115,W07-0718,0,\N,Missing
C08-1115,P07-1019,0,\N,Missing
C08-1115,W07-0717,1,\N,Missing
C08-1115,P03-1021,0,\N,Missing
D18-1328,S16-1081,0,0.0271883,"elated Work Attempts to measure the impact of translation divergences in MT have focused on the introduction of noise in sentence alignments (Goutte et al., 2012), showing that statistical MT is highly robust to noise, and that performance only degrades seriously at very high noise levels. In contrast, neural MTs seem to be more sensitive to noise (Chen et al., 2016), as they tend to assign high probabilities to rare events (Hassan et al., 2018). Efforts devoted to characterising the degree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sentence representations. The authors show that a simple SVM classifier exploiting such sentence representations achieves state-of-the-art results in a textual entailment task. With the same objective, the system of He and Lin (2016) uses multiple convolutional layers and models pairwise word interactions. Our work is inspired by Carpuat et al. (2017), who train a SVM-based cross-lingual divergence detector using word alignments and sentence length features."
D18-1328,D18-1549,0,0.0254976,"performance on a parallel corpus. Therefore, the quality of MT engines is heavily dependent on the amount but also the quality of available parallel sentences.1 Parallel texts are unfortunately, scarce resources: There are relatively few language pairs for which parallel corpora of large sizes exist, and even for those pairs, available corpora only concern few restricted domains. To alleviate the lack of parallel data, several approaches have been developed over the years. They range from methods using non-parallel, or comparable data (Zhao and 1 Recent work on neural MT (Lample et al., 2018; Artetxe et al., 2018) completely dispenses with parallel data, using unsupervised methods to obtain performance improvements over word-by-word statistical MT. These systems however lag far behind supervised systems, as considered in this work. What do you feel, Spock? Que ressentez-vous? What do you feel? How much do you get paid? T’es pay´e combien de l’heure? How much do you get paid per hour? That seems a lot. 40 livres? 40 pounds? I brought you french fries! Je t’ai rapport´e des saucisses! I brought you sausage! Table 1: Examples of semantically divergent parallel sentences. English (en), French (fr) and glos"
D18-1328,W17-3209,0,0.0693794,"egree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sentence representations. The authors show that a simple SVM classifier exploiting such sentence representations achieves state-of-the-art results in a textual entailment task. With the same objective, the system of He and Lin (2016) uses multiple convolutional layers and models pairwise word interactions. Our work is inspired by Carpuat et al. (2017), who train a SVM-based cross-lingual divergence detector using word alignments and sentence length features. Their work shows that an NMT system trained only on non-divergent sentences yields slightly better translation scores, while requiring less training time. A follow-up study by the same authors (Vyas et al., 2018) achieves even better results, using the neural architecture of He and Lin (2016). Our work differs from theirs as we 2 https://github.com/jmcrego/similarity Figure 1: Illustration of the model. It computes the similarity of any source-target sentence pair (s, t), where s = (s1"
D18-1328,2016.amta-researchers.8,0,0.233467,"nglais, 2018; Grover and Mitra, 2017; Schwenk, 2018) to techniques that produce synthetic parallel data from monolingual corpora (Sennrich et al., 2016a; Chinea-Rios et al., 2017), using automated alignment/translation engines that are prone to the introduction of noise in the resulting parallel sentences. Mismatches in parallel sentences extracted from translated texts are also reported (Tiedemann, 2011; Xu and Yvon, 2016). This problem is mostly ignored in MT, where parallel sentences are considered to convey the exact same meaning; yet it seems particularly important for neural MT engines (Chen et al., 2016). Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the network are used to identify and fix some partial divergences, yielding a"
D18-1328,W17-4714,0,0.0456606,"Missing"
D18-1328,W04-3208,0,0.0366978,"Missing"
D18-1328,2012.amta-papers.7,0,0.04818,"with a different, arguably simpler, topology. We model sentence similarity by means of optimising a loss function based on word alignments. Furthermore, the network predicts word similarity scores that we further use to correct divergent sentences. 3 Neural Divergence Classifier The architecture of our network is inspired by the work on word alignment of Legrand et al. (2016), using however contextual, rather than fixed, word embeddings (see Figure 1). Related Work Attempts to measure the impact of translation divergences in MT have focused on the introduction of noise in sentence alignments (Goutte et al., 2012), showing that statistical MT is highly robust to noise, and that performance only degrades seriously at very high noise levels. In contrast, neural MTs seem to be more sensitive to noise (Chen et al., 2016), as they tend to assign high probabilities to rare events (Hassan et al., 2018). Efforts devoted to characterising the degree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sent"
D18-1328,C18-1122,0,0.0465598,"Missing"
D18-1328,P17-3003,0,0.0250778,"Missing"
D18-1328,N16-1108,0,0.0341746,"they tend to assign high probabilities to rare events (Hassan et al., 2018). Efforts devoted to characterising the degree of semantic equivalence between two snippets of texts in the same or different languages are presented (Agirre et al., 2016). In (Mueller and Thyagarajan, 2016), a monolingual sentence similarity network is proposed, making use of a simple LSTM layer to compute sentence representations. The authors show that a simple SVM classifier exploiting such sentence representations achieves state-of-the-art results in a textual entailment task. With the same objective, the system of He and Lin (2016) uses multiple convolutional layers and models pairwise word interactions. Our work is inspired by Carpuat et al. (2017), who train a SVM-based cross-lingual divergence detector using word alignments and sentence length features. Their work shows that an NMT system trained only on non-divergent sentences yields slightly better translation scores, while requiring less training time. A follow-up study by the same authors (Vyas et al., 2018) achieves even better results, using the neural architecture of He and Lin (2016). Our work differs from theirs as we 2 https://github.com/jmcrego/similarity"
D18-1328,W16-2207,0,0.213438,"Missing"
D18-1328,C14-1055,0,0.109632,"are in bold letters. Table 1 gives some examples of English-French parallel sentences that are not completely semantically equivalent, extracted from the OpenSubtitles corpus (Lison and Tiedemann, 2016). Multiples types of translation divergences are found in parallel corpora: Additional segments are included on either side of the parallel sentences (first and second rows) most likely due to errors in sentence segmentation; Some translations may be completely uncorrelated (third row); Inaccurate translations also exist (fourth row). Note that divergent translations can be due various reasons (Li et al., 2014), the study of which is beyond the 2967 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2967–2973 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics scope of this paper. In this work, we present an unsupervised method for building cross-lingual sentence embeddings based on modelling word similarity, relying on a neural architecture (see § 3) that is able to identify several types of common cross-lingual divergences. The resulting embeddings are then used to measure semantic equivalence between sentenc"
D18-1328,L16-1147,0,0.182552,"ed in this work. What do you feel, Spock? Que ressentez-vous? What do you feel? How much do you get paid? T’es pay´e combien de l’heure? How much do you get paid per hour? That seems a lot. 40 livres? 40 pounds? I brought you french fries! Je t’ai rapport´e des saucisses! I brought you sausage! Table 1: Examples of semantically divergent parallel sentences. English (en), French (fr) and gloss of French (gl). Divergences are in bold letters. Table 1 gives some examples of English-French parallel sentences that are not completely semantically equivalent, extracted from the OpenSubtitles corpus (Lison and Tiedemann, 2016). Multiples types of translation divergences are found in parallel corpora: Additional segments are included on either side of the parallel sentences (first and second rows) most likely due to errors in sentence segmentation; Some translations may be completely uncorrelated (third row); Inaccurate translations also exist (fourth row). Note that divergent translations can be due various reasons (Li et al., 2014), the study of which is beyond the 2967 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2967–2973 c Brussels, Belgium, October 31 - November"
D18-1328,J05-4003,0,0.208936,"Missing"
D18-1328,P18-2037,0,0.0386913,"Missing"
D18-1328,P16-1009,0,0.0359762,"processed with OpenNMT5 , performing minimal tokenisation. After tokenisation, each out-of-vocabulary word is mapped to a special UNK token, assuming a vocabulary containing the 50, 000 more frequent words. 4.2 Neural Divergence Word embeddings of Es = Et = 256 cells are initialised using fastText,6 further aligned by means of MUSE7 following the unsupervised 4 http://paracrawl.eu/ http://opennmt.net 6 https://github.com/facebookresearch/fastText 7 https://github.com/facebookresearch/MUSE 5 Neural Translation In addition to the basic tokenisation detailed above, we perform Byte-Pair Encoding (Sennrich et al., 2016b) with 30000 merge operations learned by joining both language sides. Neural systems are based on the open-source project OpenNMT; using a Transformer model similar to the model of Vaswani et al. (2017): both encoder and decoder have 6 layers; Multi-head attention is performed over 8 head; the hidden layer size is 512; and the inner layer of feed forward network is of size 2048. Word embeddings have 512 cells. We set the dropout probability to 0.1 and the batch size to 3072. The optimiser is Lazy Adam with β1 = 0.9, β2 = 0.98,  = 10−9 , warmup steps = 4000. Training stops after 30 epochs. 5"
D18-1328,P16-1162,0,0.0649685,"processed with OpenNMT5 , performing minimal tokenisation. After tokenisation, each out-of-vocabulary word is mapped to a special UNK token, assuming a vocabulary containing the 50, 000 more frequent words. 4.2 Neural Divergence Word embeddings of Es = Et = 256 cells are initialised using fastText,6 further aligned by means of MUSE7 following the unsupervised 4 http://paracrawl.eu/ http://opennmt.net 6 https://github.com/facebookresearch/fastText 7 https://github.com/facebookresearch/MUSE 5 Neural Translation In addition to the basic tokenisation detailed above, we perform Byte-Pair Encoding (Sennrich et al., 2016b) with 30000 merge operations learned by joining both language sides. Neural systems are based on the open-source project OpenNMT; using a Transformer model similar to the model of Vaswani et al. (2017): both encoder and decoder have 6 layers; Multi-head attention is performed over 8 head; the hidden layer size is 512; and the inner layer of feed forward network is of size 2048. Word embeddings have 512 cells. We set the dropout probability to 0.1 and the batch size to 3072. The optimiser is Lazy Adam with β1 = 0.9, β2 = 0.98,  = 10−9 , warmup steps = 4000. Training stops after 30 epochs. 5"
D18-1328,N18-1136,0,0.168216,"Missing"
D18-1328,L16-1099,1,0.844686,"ystrangroup.com ‡ LIMSI, CNRS, Universit´e Paris-Saclay 91405 Orsay, France firstname.lastname@limsi.fr Abstract Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Gr´egoire and Langlais, 2018; Grover and Mitra, 2017; Schwenk, 2018) to techniques that produce synthetic parallel data from monolingual corpora (Sennrich et al., 2016a; Chinea-Rios et al., 2017), using automated alignment/translation engines that are prone to the introduction of noise in the resulting parallel sentences. Mismatches in parallel sentences extracted from translated texts are also reported (Tiedemann, 2011; Xu and Yvon, 2016). This problem is mostly ignored in MT, where parallel sentences are considered to convey the exact same meaning; yet it seems particularly important for neural MT engines (Chen et al., 2016). Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores,"
D19-5225,kobus-etal-2017-domain,1,0.858772,"loy a set of grammatical constraints (tense, voice and person) which introduce syntactic/semantic variations in translations. Thus, our method aims at enhancing translation diversity, a major drawback highlighted in back-translated data (Edunov et al., 2018). Similar to our work, side constraints have already been used on neural models in a number of different scenarios. To the best of our knowledge, side constraints were first employed to control politeness in a NMT by (Sennrich et al., 2016a). Domain-adapted translations using a unique network enhanced with side constraints is presented in (Kobus et al., 2017). We consider 4 constraints regarding POS classes: noun, verb, adjective and adverb. For each constraint we build 3 clusters containing the set of words with H (high), M (medium) and L (low) frequency as computed over the training data. This is, the set of nouns occurring with highest frequency are arranged in the NH class, verbs with lower frequencies in VL, etc. We set the frequency thresholds to satisfy that the three clusters of a POS class have approximately the same number of occurrences in the training corpus. Note that when creating synthetic corpora, side constraint values are randoml"
D19-5225,P02-1040,0,0.107735,"ese sentences of our datasets (aligned to English). This is, we back-translate the Japanese side of the Japanese-English (JaRuNC) corpus to extend the data available for the Russian→Japanese translation direction. Equivalently, we backtranslate the Russian side of the Russian-English (JaRuNC and NC) corpora to increase the amount of data available for the Japanese→Russian direction. Thus, building new synthetic Japanese*-Russian and Russian*-Japanese corpora. 13 Since our model is multi-lingual, we don’t need additional networks 13 6 Evaluation All our results are computed following the BLEU (Papineni et al., 2002) score. Validation sets are used to select our best performing networks, while results shown in Table 5 are computed for the official test sets. As it can be seen, all our experiments to alleviate data scarcity boosted translation performance. A light decrease in accuracy is observed when using SC data for Russian→Japanese translation. The improvement is remarkable for the Japanese→Russian task for which the BLEU score is doubled from 7 to more than 14 points. We use * to denote synthetic data 192 System base +FT(JaRuNC,NC) +FT(JaRuNC,NC,BT,SYN) +FT(JaRuNC,NC,BT,SYN,SC) Ru-Ja 9.76 12.10 15.89"
D19-5225,N16-1005,0,0.376749,"air constitute very challenging conditions. A rather common situation in the translation industry, that motivated us to explore techniques that can help in the construction from scratch of efficient NMT engines. We present systems built using only the data provided by the organisers for both translation directions (Russian↔Japanese) and using the Transformer network introduced by (Vaswani et al., 2017). We enhance the baseline systems with several experiments that aim at alleviating the data scarcity problem. More precisely we run experiments following the back-translation method proposed by (Sennrich et al., 2016b) in which target Resources Datasets used for the evaluation can be found listed in the shared task web site1 . WAT organisers kindly provide a manually aligned, cleaned and filtered Japanese↔Russian, Japanese↔English and English↔Russian train, development and test corpora (JaRuNC)2 as well as a news domain Russian↔English corpus (NC)3 . In addition, use of the next out-of-domain data is encouraged: • Japanese↔English Wikipedia articles related to Kyoto (KFTT)4 . • Japanese↔English Subtitles (JESC)5 , • Japanese↔English asian scientific paper abstracts (ASPEC)6 , 1 lotus.kuee.kyoto-u.ac.jp/WA"
D19-5225,D18-1045,0,0.0162792,"т приезжает завтра Side Constraints Table 3: French-German sentence pair with frequency constraints. We propose a method to generate synthetic parallel data that uses a set of side constraints. Side constraints are used to guide the NMT model to produce distinct word translation alternatives based on their frequency in the training corpora. Furthermore, we employ a set of grammatical constraints (tense, voice and person) which introduce syntactic/semantic variations in translations. Thus, our method aims at enhancing translation diversity, a major drawback highlighted in back-translated data (Edunov et al., 2018). Similar to our work, side constraints have already been used on neural models in a number of different scenarios. To the best of our knowledge, side constraints were first employed to control politeness in a NMT by (Sennrich et al., 2016a). Domain-adapted translations using a unique network enhanced with side constraints is presented in (Kobus et al., 2017). We consider 4 constraints regarding POS classes: noun, verb, adjective and adverb. For each constraint we build 3 clusters containing the set of words with H (high), M (medium) and L (low) frequency as computed over the training data. Th"
D19-5225,P16-1009,0,0.511566,"air constitute very challenging conditions. A rather common situation in the translation industry, that motivated us to explore techniques that can help in the construction from scratch of efficient NMT engines. We present systems built using only the data provided by the organisers for both translation directions (Russian↔Japanese) and using the Transformer network introduced by (Vaswani et al., 2017). We enhance the baseline systems with several experiments that aim at alleviating the data scarcity problem. More precisely we run experiments following the back-translation method proposed by (Sennrich et al., 2016b) in which target Resources Datasets used for the evaluation can be found listed in the shared task web site1 . WAT organisers kindly provide a manually aligned, cleaned and filtered Japanese↔Russian, Japanese↔English and English↔Russian train, development and test corpora (JaRuNC)2 as well as a news domain Russian↔English corpus (NC)3 . In addition, use of the next out-of-domain data is encouraged: • Japanese↔English Wikipedia articles related to Kyoto (KFTT)4 . • Japanese↔English Subtitles (JESC)5 , • Japanese↔English asian scientific paper abstracts (ASPEC)6 , 1 lotus.kuee.kyoto-u.ac.jp/WA"
D19-5225,P16-1162,0,0.761133,"air constitute very challenging conditions. A rather common situation in the translation industry, that motivated us to explore techniques that can help in the construction from scratch of efficient NMT engines. We present systems built using only the data provided by the organisers for both translation directions (Russian↔Japanese) and using the Transformer network introduced by (Vaswani et al., 2017). We enhance the baseline systems with several experiments that aim at alleviating the data scarcity problem. More precisely we run experiments following the back-translation method proposed by (Sennrich et al., 2016b) in which target Resources Datasets used for the evaluation can be found listed in the shared task web site1 . WAT organisers kindly provide a manually aligned, cleaned and filtered Japanese↔Russian, Japanese↔English and English↔Russian train, development and test corpora (JaRuNC)2 as well as a news domain Russian↔English corpus (NC)3 . In addition, use of the next out-of-domain data is encouraged: • Japanese↔English Wikipedia articles related to Kyoto (KFTT)4 . • Japanese↔English Subtitles (JESC)5 , • Japanese↔English asian scientific paper abstracts (ASPEC)6 , 1 lotus.kuee.kyoto-u.ac.jp/WA"
D19-5615,P00-1037,0,0.526819,"Missing"
D19-5615,P01-1023,0,0.322072,"ent ordering. We made publicly available the transformer extension presented in this paper1 . 1 Introduction Data-to-text generation is an important task in natural language generation (NLG). It refers to the task of automatically producing a descriptive text from non-linguistic structured data (tables, database records, spreadsheets, etc.). Table 1 illustrates an example of data-to-text NLG, with statistics of a NBA basketball game (top) and the corresponding game summary (bottom). Traditional approaches perform the summary generation in two separate steps: content selection (“what to say”) (Duboue and McKeown, 2001, 2003) and surface realization (“how to say it”) (Stent et al., 2004; Reiter et al., 2005). After the emergence of sequence-to-sequence (S2S) 1. We adapt the Transformer (Vaswani et al., 2017) architecture by modifying the input table representation (record embedding) and introducing an additional objective function (content selection modelling). 2. We create synthetic data following two data augmentation techniques and investigate 1 https://github.com/gongliym/ data2text-transformer 148 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 148–156 c Hong Kon"
D19-5615,W03-1016,0,0.246106,"Missing"
D19-5615,P16-1154,0,0.0450671,"to choose a suitable template to render the content. More recently, work on this topic has focused on end-to-end generation models. Konstas and Lapata (2012) described an end-to-end generation model which jointly models content selection and surface realization. Mei et al. (2015) proposed a neural encoder-aligner-decoder model which first encodes the entire input record dataset then the aligner module performs the content selection for the decoder to generate output summary. Some other work extends the encoder-decoder model to be able to copy words directly from the input (Yang et al., 2016; Gu et al., 2016; Gulcehre et al., 2016). Wiseman et al. (2017) investigates different data-to-text generation approaches and introduces a new corpus (ROTOW IRE, see Table 1) 3 Data-to-Text Generation Model In this section, we first formulate the data-totext generation problem and introduce our datato-text generation baseline model. Next, we detail the extensions introduced to our baseline network, namely Record Embedding and Content Selection Modelling. Problem Statement The objective of data-to-text generation is to generate a descriptive summary given structured data. Input of the model consists of a table"
D19-5615,P98-2209,0,0.386396,"to-text generation task along with a series of automatic measures for the contentoriented evaluation. Based on (Wiseman et al., 2017), Puduppully et al. (2019) incorporates content selection and planing mechanisms into the encoder-decoder system and improves the stateof-the-art on the ROTOW IRE dataset. their impacts on different evaluation metrics. We show that our model outperforms current state-of-the-art systems on BLEU, content selection precision and content ordering metrics. 2 Related Work Automatic summary generation has been a topic of interest for a long time (Reiter and Dale, 1997; Tanaka-Ishii et al., 1998). It has interesting applications in many different domains, such as sport game summary generation (Barzilay and Lapata, 2005; Liang et al., 2009), weather-forecast generation (Reiter et al., 2005) and recipe generation (Yang et al., 2016). Traditional data-to-text generation approaches perform the summary generation in two separate steps: content selection and surface realization. For content selection, a number of approaches were proposed to automatically select the elements of content and extract ordering constraints from an aligned corpus of input data and output summaries (Duboue and McKe"
D19-5615,P16-1014,0,0.0250577,"able template to render the content. More recently, work on this topic has focused on end-to-end generation models. Konstas and Lapata (2012) described an end-to-end generation model which jointly models content selection and surface realization. Mei et al. (2015) proposed a neural encoder-aligner-decoder model which first encodes the entire input record dataset then the aligner module performs the content selection for the decoder to generate output summary. Some other work extends the encoder-decoder model to be able to copy words directly from the input (Yang et al., 2016; Gu et al., 2016; Gulcehre et al., 2016). Wiseman et al. (2017) investigates different data-to-text generation approaches and introduces a new corpus (ROTOW IRE, see Table 1) 3 Data-to-Text Generation Model In this section, we first formulate the data-totext generation problem and introduce our datato-text generation baseline model. Next, we detail the extensions introduced to our baseline network, namely Record Embedding and Content Selection Modelling. Problem Statement The objective of data-to-text generation is to generate a descriptive summary given structured data. Input of the model consists of a table of records (see Table 1"
D19-5615,N12-1093,0,0.0259739,"ual dependencies between input data items. For surface realization, Stent et al. (2004) proposed to transform the input data into an intermediary structure and then to generate natural language text from it; Reiter et al. (2005) presented a method to generate text using consistent data-to-word rules. Angeli et al. (2010) broke up the two steps into a sequence of local decisions where they used two classifiers to select content form database and another classifier to choose a suitable template to render the content. More recently, work on this topic has focused on end-to-end generation models. Konstas and Lapata (2012) described an end-to-end generation model which jointly models content selection and surface realization. Mei et al. (2015) proposed a neural encoder-aligner-decoder model which first encodes the entire input record dataset then the aligner module performs the content selection for the decoder to generate output summary. Some other work extends the encoder-decoder model to be able to copy words directly from the input (Yang et al., 2016; Gu et al., 2016; Gulcehre et al., 2016). Wiseman et al. (2017) investigates different data-to-text generation approaches and introduces a new corpus (ROTOW IR"
D19-5615,D17-1239,0,0.575701,"representation (record embedding) and introducing an additional objective function (content selection modelling). 2. We create synthetic data following two data augmentation techniques and investigate 1 https://github.com/gongliym/ data2text-transformer 148 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 148–156 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d for the data-to-text generation task along with a series of automatic measures for the contentoriented evaluation. Based on (Wiseman et al., 2017), Puduppully et al. (2019) incorporates content selection and planing mechanisms into the encoder-decoder system and improves the stateof-the-art on the ROTOW IRE dataset. their impacts on different evaluation metrics. We show that our model outperforms current state-of-the-art systems on BLEU, content selection precision and content ordering metrics. 2 Related Work Automatic summary generation has been a topic of interest for a long time (Reiter and Dale, 1997; Tanaka-Ishii et al., 1998). It has interesting applications in many different domains, such as sport game summary generation (Barzila"
D19-5615,D16-1128,0,0.050921,"Missing"
D19-5615,P09-1011,0,0.115289,"(2019) incorporates content selection and planing mechanisms into the encoder-decoder system and improves the stateof-the-art on the ROTOW IRE dataset. their impacts on different evaluation metrics. We show that our model outperforms current state-of-the-art systems on BLEU, content selection precision and content ordering metrics. 2 Related Work Automatic summary generation has been a topic of interest for a long time (Reiter and Dale, 1997; Tanaka-Ishii et al., 1998). It has interesting applications in many different domains, such as sport game summary generation (Barzilay and Lapata, 2005; Liang et al., 2009), weather-forecast generation (Reiter et al., 2005) and recipe generation (Yang et al., 2016). Traditional data-to-text generation approaches perform the summary generation in two separate steps: content selection and surface realization. For content selection, a number of approaches were proposed to automatically select the elements of content and extract ordering constraints from an aligned corpus of input data and output summaries (Duboue and McKeown, 2001, 2003). In (Barzilay and Lapata, 2005), the content selection is treated as a collective classification problem which allows the system"
D19-5615,P02-1040,0,0.105549,"Missing"
D19-5615,P04-1011,0,0.357397,"d in this paper1 . 1 Introduction Data-to-text generation is an important task in natural language generation (NLG). It refers to the task of automatically producing a descriptive text from non-linguistic structured data (tables, database records, spreadsheets, etc.). Table 1 illustrates an example of data-to-text NLG, with statistics of a NBA basketball game (top) and the corresponding game summary (bottom). Traditional approaches perform the summary generation in two separate steps: content selection (“what to say”) (Duboue and McKeown, 2001, 2003) and surface realization (“how to say it”) (Stent et al., 2004; Reiter et al., 2005). After the emergence of sequence-to-sequence (S2S) 1. We adapt the Transformer (Vaswani et al., 2017) architecture by modifying the input table representation (record embedding) and introducing an additional objective function (content selection modelling). 2. We create synthetic data following two data augmentation techniques and investigate 1 https://github.com/gongliym/ data2text-transformer 148 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 148–156 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Lingui"
E12-2003,P07-2045,0,0.00303578,"Table 1 presents statistics of these in-domain data. The data extracted from HAL were used to adapt a generic system to the scientific literature domain. The generic system was mostly trained on data provided for the shared task of Sixth Workshop on Statistical Machine Translation6 (WMT 2011), described in Table 2. Table 3 presents results showing, in the English–French direction, the impact on the statistical engine of introducing the resources extracted from HAL, as well as the impact of domain adaptation techniques. The baseline statistical engine is a standard PBSMT system based on Moses (Koehn et al., 2007) and the SRILM tookit (Stolcke, 2002). Is was trained and tuned only on WMT11 data (out-of-domain). Incorporating the HAL data into the language model and tuning the system on the HAL development set, Domain Lg Monolingual data Train cs En 2.5 M Fr 761 k phys En 2.1 M Fr 662 k 54 M 19 M 50 M 17 M 457 k 274 k 646 k 292 k Table 1: Statistics for the parallel training, development, and test data sets extracted from thesis abstracts contained in HAL, as well as monolingual data extracted from all documents in HAL, in computer science (cs) and physics (phys). The following statistics are given for"
E12-2003,2008.iwslt-papers.6,1,0.861276,"f the corpus: the number of sentences, the number of running words (after tokenisation) and the number of words in the vocabulary (M and k stand for millions and thousands, respectively). yielded a gain of more than 7 BLEU points, in both domains (computer science and physics). Including the theses abstracts in the parallel training corpus, a further gain of 2.3 BLEU points is observed for computer science, and 3.1 points for physics. The last experiment performed aims at increasing the amount of in-domain parallel texts by translating automatically in-domain monolingual data, as suggested by Schwenk (2008). The synthesised bitext does not bring new words into the system, but increases the probability of indomain bilingual phrases. By adding a synthetic bitext of 12 million words to the parallel training data, we observed a gain of 0.5 BLEU point for computer science, and 0.7 points for physics. Although not shown here, similar results were obtained in the French–English direction. The French–English system is actually slightly better than the English–French one as it is an easier translation direction. 13 Translation Model Language Model Tuning Domain wmt11 wmt11+hal wmt11+hal wmt11+hal wmt11 h"
I17-2046,D16-1139,0,0.0233101,"Missing"
I17-2046,W17-3204,0,0.054482,"Missing"
I17-2046,2016.amta-researchers.10,0,0.0551937,"Missing"
I17-2046,P02-1040,0,0.0975088,"Missing"
I17-2046,K16-1029,0,0.0557737,"Missing"
I17-2046,P15-1001,0,0.0623444,"Missing"
I17-2046,P16-1159,0,0.0457533,"Missing"
I17-2046,Q16-1027,0,0.0375008,"Missing"
kobus-etal-2017-domain,P10-2041,0,\N,Missing
kobus-etal-2017-domain,W07-0733,0,\N,Missing
kobus-etal-2017-domain,2005.eamt-1.19,0,\N,Missing
kobus-etal-2017-domain,I08-2089,0,\N,Missing
kobus-etal-2017-domain,N16-1005,0,\N,Missing
kobus-etal-2017-domain,2016.amta-researchers.10,0,\N,Missing
kobus-etal-2017-domain,C16-1170,0,\N,Missing
kobus-etal-2017-domain,tiedemann-2012-parallel,0,\N,Missing
P17-4012,D15-1166,0,0.935439,"e current hidden state to produce a prediction p(wt |w1:t−1 , x) of the next word. This prediction is then fed back into the target RNN. Introduction Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community benefited greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) or travatar (Neubig, 2013) for syntax-based SMT, NMT toolkits can provide"
P17-4012,K16-1002,0,0.159928,"Missing"
P17-4012,P13-4016,0,0.0466914,"n-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community benefited greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) or travatar (Neubig, 2013) for syntax-based SMT, NMT toolkits can provide a foundation to build upon. A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efficient and accurate enough to be used in production contexts. Currently there are several existing NMT implementations. Many systems such as those developed in industry by Google, Microsoft, and Baidu, are closed source, and are unlikely to be released with unrestricted licenses. Many other systems such as GroundHog, Blocks, neuralmonkey, tensorflow-seq2seq, lamtram, and our own seq2seq-a"
P17-4012,W16-2209,0,0.0203922,"Table 1: Translation speed in source tokens per second for the Torch CPU/GPU implementations and for the multithreaded CPU C implementation. (Run with Intel i7/GTX 1080) Figure 3: 3D Visualization of OpenNMT source embedding from the TensorBoard visualization system. tion within the code. To test whether this approach would allow novel feature development we experimented with two case studies. to basic seq2seq models. We next discuss a case study to demonstrate that OpenNMT is extensible to future variants. Case Study: Factored Neural Translation In feature-based factored neural translation (Sennrich and Haddow, 2016), instead of generating a word at each time step, the model generates both word and associated features. For instance, the system might include words and separate case features. This extension requires modifying both the inputs and the output of the decoder to generate multiple symbols. In OpenNMT both of these aspects are abstracted from the core translation code, and therefore factored translation simply modifies the input network to instead process the featurebased representation, and the output generator network to instead produce multiple conditionally independent predictions. Multiple Mo"
P17-4012,N16-1012,1,0.109534,"ion in the 4 other languages. Corpus was tokenized using shared Byte Pair Encoding of 32k. Comparative results between multi-way translation and each of the 20 independent training are presented in Table 2. The systematically large improvement shows that language pair benefits from training jointly with the other language pairs. Additionally we have found interest from the community in using OpenNMT for non-standard MT tasks like sentence document summarization dialogue response generation (chatbots), among others. Using OpenNMT, we were able to replicate the sentence summarization results of Chopra et al. (2016), reaching a ROUGE-1 score of 33.13 on the Gigaword data. We have also trained a model on 14 million sentences of the OpenSubtitles data set based on the work Vinyals and Le (2015), achieving comparable perplexity. Table 3: Performance Results for EN→DE on WMT15 tested on newstest2014. Both system 2x500 RNN, embedding size 300, 13 epochs, batch size 64, beam size 5. We compare on a 50k vocabulary and a 32k BPE setting. OpenNMT shows improvements in speed and accuracy compared to Nematus. kenization, (b) has extremely simple, languageindependent tokenization rules. The tokenizer can also perfor"
P17-4012,P16-5005,0,0.0397599,"urrent neural network (RNN). Upon seeing the heosi symbol, the final time step initializes a target blue RNN. At each target time step, attention is applied over the source RNN and combined with the current hidden state to produce a prediction p(wt |w1:t−1 , x) of the next word. This prediction is then fed back into the target RNN. Introduction Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT"
P17-4012,1983.tc-1.13,0,0.689675,"Missing"
P17-4012,N16-1174,0,0.14961,"is a speech-to-text recognition system based on the work of Chan et al. (2015). This system has been implemented directly in OpenNMT by replacing the source encoder with a Pyrimidal source model. Case Study: Attention Networks The use of attention over the encoder at each step of translation is crucial for the model to perform well. The default method is to utilize the global attention mechanism. However there are many other types of attention that have recently proposed including local attention (Luong et al., 2015), sparse-max attention (Martins and Astudillo, 2016), hierarchical attention (Yang et al., 2016) among others. As this is simply a module in OpenNMT it can easily be substituted. Recently the Harvard group developed a structured attention approach, that utilizes graphical model inference to compute this attention. The method is quite computationally complex; however as it is modularized by the Torch interface, it can be used in OpenNMT to substitute for standard attention. 4.3 4.4 Additional Tools Finally we briefly summarize some of the additional tools that extend OpenNMT to make it more beneficial to the research community. Tokenization We aimed for OpenNMT to be a standalone project"
P17-4012,P10-4002,0,0.00659525,"and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community benefited greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) or travatar (Neubig, 2013) for syntax-based SMT, NMT toolkits can provide a foundation to build upon. A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efficient and accurate enough to be used in production contexts. Currently there are several existing NMT implementations. Many systems such as those developed in industry by Google, Microsoft, and Baidu, are closed source, and are unlikely to be released with unrestricted licenses. Many other systems such as GroundHog, Blocks, neuralmonkey, tensorflow-seq2seq, lam"
P17-4012,P07-2045,0,0.0670209,"models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community benefited greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) or travatar (Neubig, 2013) for syntax-based SMT, NMT toolkits can provide a foundation to build upon. A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efficient and accurate enough to be used in production contexts. Currently there are several existing NMT implementations. Many systems such as those developed in industry by Google, Microsoft, and Baidu, are closed source, and are unlikely to be released with unrestricted licenses. Many other systems such as Ground"
P17-4012,P16-1162,0,\N,Missing
W03-1729,W03-1719,0,0.0341237,"ser Dictionary that contains words found in the training corpora, but not in the SYSTRAN dictionary. Although each of these corpora was segmented according to its own standard, we made a single UD containing all the words gathered in all corpora. Although the ranking of the SYSTRAN segmenter is different in the four open tracks, SYSTRAN’s segmentation performance is quite comparable across the four corpora. This is to be compared to the scores obtained by other participants, where good performance was typically obtained on one corpus only. SYSTRAN scores for the 4 tracks are shown in Table 3 (Sproat and Emerson, 2003). Track ASo CTBo HKo PKo 3.2 R P F Roov Riv 0.915 0.894 0.904 0.426 0.926 0.891 0.877 0.884 0.733 0.925 0.898 0.860 0.879 0.616 0.920 0.905 0.869 0.886 0.503 0.934 Table 3. SYSTRAN’s Scores in the Bakeoff Discussions The segmentation differences between the reference corpora and SYSTRAN’s results are further analyzed. Table 4 shows the partition of divergences between OAS, CAS-T, and CAS-R strings:3 ASo CTBo HKo PKo Total Same OAS CAS-T CAS-R 11,985 10,970 76 448 491 39,922 35,561 231 2,419 1,711 34,959 31,397 217 1,436 1,909 17,194 15,554 82 615 943 Table 4. Count of OAS and CAS Divergence Th"
W03-1729,W03-1726,0,\N,Missing
W07-0732,2005.mtsummit-papers.28,1,0.773912,"Missing"
W07-0732,E06-1032,1,0.256679,"Missing"
W07-0732,W07-0728,0,0.527007,"Missing"
W07-0732,P07-2045,1,\N,Missing
W08-0313,2007.mtsummit-papers.18,1,0.861805,"Missing"
W08-0313,2003.mtsummit-tttt.3,0,0.312688,"ese systems seem to indicate that they have reached a level of performance allowing a human being to understand the automatic translations and to answer complicated questions on its content (Jones, 2008). In a joint project between the University of Le Mans and the company SYSTRAN, we try to build similar general purpose SMT systems for European languages. In the final version, these systems 2 Architecture of the system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is"
W08-0313,P07-2045,0,0.015397,"Missing"
W08-0313,P02-1038,0,0.182453,"ean languages. In the final version, these systems 2 Architecture of the system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. 119 Proceedings of the Third Workshop on Statistical Machine Translation, pages 119–122, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexic"
W08-0313,J03-1002,0,0.00432248,"indicate that they have reached a level of performance allowing a human being to understand the automatic translations and to answer complicated questions on its content (Jones, 2008). In a joint project between the University of Le Mans and the company SYSTRAN, we try to build similar general purpose SMT systems for European languages. In the final version, these systems 2 Architecture of the system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses S"
W08-0313,2006.iwslt-papers.2,1,0.884061,"Missing"
W08-0313,W07-0725,1,0.408966,"s: first, Moses is run and a 1000-best list is generated for each sentence. The parameters of Moses are tuned on devtest2006 for the Europarl task and nc-devtest2007 for the news task, using the cmert tool. These 1000-best lists are then rescored with a continuous space 5-gram LM and the weights of the feature functions are optimized again using the numerical optimization toolkit Condor (Berghen and Bersini, 2005). Note that this step operates only on the 1000-best lists, no re-decoding is performed. This basic architecture of the system is identical to the one used in the 2007 WMT evaluation(Schwenk, 2007a). 2.1 Translation model In the frame work of the 2008 WMT shared task, two parallel corpora were provided: the Europarl corpus (about 33M words) and the newscommentary corpus (about 1.2M words). It is known that the minutes of the debates of the European parliament use a particular jargon and these texts alone do not seem to be the appropriate to build a French/English SMT system for other texts. The more general news-commentary corpus is unfortunately rather small in size. Therefore, with the goal to build a general purpose system, we investigated whether more bilingual resources are availa"
W08-0313,2003.mtsummit-papers.53,0,0.070136,"Missing"
W08-0313,N03-1017,0,\N,Missing
W08-0327,E06-1032,1,0.868211,"Missing"
W08-0327,W07-0728,0,0.0519587,"Missing"
W08-0327,C08-1115,1,\N,Missing
W08-0327,W08-0313,1,\N,Missing
W08-0327,W07-0733,1,\N,Missing
W08-0327,N07-1064,0,\N,Missing
W08-0327,W07-0732,1,\N,Missing
W09-0419,2003.mtsummit-papers.46,1,0.715957,"s retained as a bilingual entry. Otherwise, the candidate is excluded. Given that a bilingual entry with a same lemma may have various inflectional forms in corpus, we then sum the lemma counts. Finally, in the current setup, we only keep the most frequent translation for each source. For our secondary submission for EnglishFrench, we extracted such entries from both the News Commentary and the Europarl corpus. Figure 2: Extraction pipeline: from parallel texts to bilingual dictionary 3.1 Manual customization through dictionary entries 3.3 The Systran system provides a dictionary coding tool (Senellart et al., 2003). This tool allows the manual task of coding entries to be partially automated with the use of monolingual dictionaries and probabilistic context-free grammars, while allowing the user to fine-tune it by correcting the automatic coding and/or add more features. However, this remains first of all a time-consuming task. Moreover, it is not easy for humans to select the best translation among a set of alternatives, let alone assign them probabilities. Last but not least, the beneficial effect on translation is not guaranteed (especially, the effect on the rule-based dependency analysis). Validati"
W09-0419,N07-1064,0,0.109192,"Cedex France Philipp Koehn** of Informatics University of Edinburgh 10 Crichton Street, Edinburgh United Kingdom ** School Abstract We describe here the two Systran/University of Edinburgh submissions for WMT2009. They involve a statistical post-editing model with a particular handling of named entities (English to French and German to English) and the extraction of phrasal rules (English to French). 1 Figure 1: Translation with PBMT post-editing Introduction Previous results had shown a rather satisfying performance for hybrid systems such as the Statistical Phrase-based Post-Editing (SPE) (Simard et al., 2007) combination in comparison with purely phrase-based statistical models, reaching similar BLEU scores and often receiving better human judgement (German to English at WMT2007) against the BLEU metric. This last result was in accordance with the previous acknowledgment (Callison-Burch et al., 2006) that systems of too differing structure could not be compared reliably with BLEU. We participated in the recent Workshop on Machine Translation (WMT’09) in the language pairs English to French and German to English. On the one hand we trained a PostEditing system with an additional special treatment t"
W09-0419,C08-1115,1,0.828838,"ries to parallel sentences 4: translate training corpus with current dictionary 5: for each entry do 6: translate all relevant sentences with current dictionary, plus this entry 7: compute BLEU scores without and with the entry 8: end for 9: Select entries with better/worse sentences ratio above threshold 10: add these entries to current dictionary 11: end for 5 Conclusion and future work We presented a few improvements to the Statistical Post Editing setup. They are part of an effort to better integrate a linguistic, rule-based system and the statistical correcting layer also illustrated in (Ueffing et al., 2008). Moreover, we presented a dictionary extraction setup which resulted in an improvement of 2 to 3 BLEU points over the baseline rule-based system when in-domain,as can be seen in table 4. This however improved translation very little on the ”news” domain which was used for evaluation. We think that is a different issue, namely of domain adaptation. In order to push further this rule-extraction approach and according to our previous work (Dugast et al., 2007) (Dugast et al., 2008), the most promising would probably be the use of alternative meanings and a language model to decode the best trans"
W09-0419,E06-1032,1,0.838034,"Missing"
W09-0419,W08-0309,1,0.744222,"to avoid the loss of entities such as dates and numbers. On the other hand we trained an additional English-to-French system (as a secondary submission) that made use of automatically extracted linguistic entries. In this paper, we will present both approaches. The latter is part of ongoing work motivated by the desire to both make use of corpus statistics and keep the advantage of the often (relative to automatic metrics’s scores) higher rank in human judgement given to rulebased systems on out-of-domain data, as seen on the WMT 2008 results for both English to French and German to English (Callison-Burch et al., 2008). 2 Statistical Post Editing systems 2.1 Baseline The basic setup is identical to the one described in (Dugast et al., 2007). A statistical translation model is trained between the rule-based translation of the source-side and the target-side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Figure 1 shows the translation process. Here are a few additional details which tend to improve training and limit unwanted statistical effects in translation: • Named e"
W09-0419,D08-1064,0,0.0138034,"ce analysis. 2.2 Trimming In a statistical translation model, trimming of the phrase table had been shown to be beneficial (Johnson et al., 2007). For our post-editing model, we can afford to perform an even more aggressive trimming of the phrase table, since the rule-based system already provides us with a translation and we only aim at correcting the most frequent errors. Therefore, we suppress all unique phrase pairs before calculating the probabilities for the final phrase table. 2.3 Avoiding the loss of entities Deleted and spurious content is a well known problem for statistical models (Chiang et al., 2008). Though we do not know of any study proving it, it seems obvious that Named Entities that would be either deleted or added to the output out of nowhere is an especially problematic kind of 111 POS Noun Adverb Verb English college level on bail badmouth French niveau d’´etudes universitaires sous caution m´edire de headword English level on badmouth headword French niveau sous m´edire Table 2: Example dictionary entries (word alignment using GIZA++ and use of common heuristics to extract phrase pairs (Koehn et al., 2007)) to extract phrase pairs. At this stage the ”phrases” are plain word sequ"
W09-0419,W07-0732,1,0.888185,"secondary submission) that made use of automatically extracted linguistic entries. In this paper, we will present both approaches. The latter is part of ongoing work motivated by the desire to both make use of corpus statistics and keep the advantage of the often (relative to automatic metrics’s scores) higher rank in human judgement given to rulebased systems on out-of-domain data, as seen on the WMT 2008 results for both English to French and German to English (Callison-Burch et al., 2008). 2 Statistical Post Editing systems 2.1 Baseline The basic setup is identical to the one described in (Dugast et al., 2007). A statistical translation model is trained between the rule-based translation of the source-side and the target-side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Figure 1 shows the translation process. Here are a few additional details which tend to improve training and limit unwanted statistical effects in translation: • Named entities are replaced by special tokens on both sides. By reducing vocabulary and combined with the next item mentioned, this"
W09-0419,W08-0327,1,0.830087,"Missing"
W09-0419,D07-1103,0,0.0151521,"ce analysis) target inflection. Motivations for adding phrasal dictionary entries (compound words) are twofold: first, just as for statistical translation models which went from word-based to phrase-based models, it helps solve disambiguation and non-literal translations. Second, as the rule-based engine makes use of a syntactic analysis of a source sentence, adding unambiguous phrasal chunks as entries will reduce the overall syntactic ambiguity and lead to a better source analysis. 2.2 Trimming In a statistical translation model, trimming of the phrase table had been shown to be beneficial (Johnson et al., 2007). For our post-editing model, we can afford to perform an even more aggressive trimming of the phrase table, since the rule-based system already provides us with a translation and we only aim at correcting the most frequent errors. Therefore, we suppress all unique phrase pairs before calculating the probabilities for the final phrase table. 2.3 Avoiding the loss of entities Deleted and spurious content is a well known problem for statistical models (Chiang et al., 2008). Though we do not know of any study proving it, it seems obvious that Named Entities that would be either deleted or added t"
W09-0419,P07-2045,1,0.0161351,"target language and all entities in the target language originate from the source language. This point is discussed in section 2.2. We will discuss some of these details further in the upcoming sections. Due to time constraints, we did not use the Giga French-English Parallel corpus provided for the workshop. We only made use of the News Commentary and the Europarl corpora. We used additional in-domain news corpora to train 5 grams language models, according to the baseline recommendations. Weights for these separate models were tuned through the Mert algorithm provided in the Moses toolkit (Koehn et al., 2007), using the provided news tuning set. 3 Rule Extraction The baseline Systran rule-based system is more or less a linguistic-oriented system that makes use of a dependency analysis, general transfer rules and dictionary entries, and finally a synthesis/reordering stage. The dictionary entries have long been the main entry point for customization of the system. Such lexical translation rules are fully linguistically coded dictionary entries, with the following features attached: part-of-speech, inflection category, headword and possibly some semantic tags. Table 2 displays a sample of manually-e"
W09-0423,D07-1090,0,0.0203199,"adding this data improves the overall system and they were not used in the final system, in order to keep the phrase-table small. We also performed experiments with the provided so-called bilingual French/English Gigaword corpus (575M English words in release 3). Again, we were not able to achieve any improvement by adding this data to the training material of the translation model. These findings are somehow surprising since it was eventually believed by the community that adding large amounts of bitexts should improve the translation model, as it is usually observed for the language model (Brants et al., 2007). In addition to these human generated bitexts, we also integrated a high quality bilingual dictionary from SYSTRAN. The entries of the dictionary were directly added to the bitexts. This technique has the potential advantage that the dictionary words could improve the alignments of these words when they also appear in the other bitexts. However, it is not guaranteed that multi-word expressions will be correctly aligned by GIZA++ and that only meaningful translations will actually appear in the phrase-table. A typical example is fire engine – camion de pompiers, for which the individual consti"
W09-0423,H93-1039,0,0.895104,"Missing"
W09-0423,W07-0732,1,0.833621,"rg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system During the last years statistical post-editing systems have shown to achieve very competitive performance (Simard et al., 2007; Dugast et al., 2007). The main idea of this techniques is to use 2 The source is available at http://www.cs.cmu. edu/˜qing/ 132 Corpus SMT system Eparl+NC Eparl+NC+dict Eparl+NC+dict+AFP SPE system SYSTRAN Eparl+NC Eparl+NC+AFP # En words Dev09a Dev09b Test09 41.6M 44.0M 51.7M 21.89 22.28 22.21 21.78 22.35# 21.43 23.80 24.13 23.88 44.2M 53.3M 18.68 23.03 22.95 18.84 23.15 23.15∗ 20.29 24.36 24.62 Table 3: Case sensitive NIST BLEU scores for the English-French systems. “NC” denotes the newscommentary bitexts, “dict” denotes SYSTRAN’s bilingual dictionary and “AFP” the automatically aligned news texts (∗ =primary,"
W09-0423,W08-0509,0,0.0200328,"ngual dictionary and “AFP” the automatically aligned news texts (∗ =primary, # =contrastive system) are given in Table 2. Adding the new news-train08 monolingual data had an important impact on the quality of the LM, even when the Gigaword data is already included. Data Vocabulary size Eparl+news + LDC Gigaword + Hansard and UN news-train08 alone all French 407k 248.8 142.2 137.5 165.0 120.6 The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).2 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. This previously caused problems when adding the entries of the bilingual dictionary to the bitexts. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses are tuned on news-dev2009a, using the cmert tool. The basic architecture of the system is identical to the one used in the 2008 WMT evaluation (Schwenk et al., 2008), but we did not use two pass decoding and n-best list rescoring with a continuous space language model. The results of t"
W09-0423,2003.mtsummit-tttt.3,0,0.0883033,"tems that include the additional AFP texts exhibit a bad generalisation behavior. We provide also the performance of the different systems on the official test set, calculated after the evaluation. In most of the cases, the observed improvements carry over on the test set. English 299k 416.7 194.9 187.5 245.9 174.8 Table 2: Perplexities on the development data of various language models. 4 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture"
W09-0423,P07-2045,0,0.0198073,"Missing"
W09-0423,P02-1038,0,0.0848608,"e development data of various language models. 4 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system During the last years statistical post-editing systems have shown to achieve very competitive performance (Simard et al., 2007; Dugast et al., 2007). The main idea of this techniques is to use 2 The source is available at http://www.cs.cmu. edu/˜qing/ 132 Corpus SMT system Eparl+NC Eparl+NC+dict Eparl+NC+dict+AFP SPE system"
W09-0423,J03-1002,0,0.0119318,"e additional AFP texts exhibit a bad generalisation behavior. We provide also the performance of the different systems on the official test set, calculated after the evaluation. In most of the cases, the observed improvements carry over on the test set. English 299k 416.7 194.9 187.5 245.9 174.8 Table 2: Perplexities on the development data of various language models. 4 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system D"
W09-0423,E09-1003,1,0.863217,"ines 580934–581316 and 599839–600662. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 130–134, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 130 French Gigaword English translations used as queries per day articles parallel sentences with extra words at ends candidate sentence pairs parallel sentences SMT FR EN length comparison tail removal + number / table removing 174M words WER 133M words 10.3M words 9.3M words +−5 day articles from English Gigaword Figure 1: Architecture of the parallel sentence extraction system (Rauf and Schwenk, 2009). (Brown et al., 1993). In comparison to our previous work (Schwenk et al., 2008), we also included all verbs in the French subjonctif and pass´e simple tense. In fact, those tenses seem to be frequently used in news material. In total about 10,000 verbs, 1,500 adjectives/adverbs and more than 100,000 noun forms were added. 2.2 word error rate is described in detail in (Rauf and Schwenk, 2009). 2.3 Monolingual data The French and English target language models were trained on all provided monolingual data. We realized that the news-train08 corpora contained some foreign texts, in particular in"
W09-0423,W08-0313,1,0.883458,"m based on the Moses decoder and a statistical post-editing system using SYSTRAN’s rule-based system. We also investigated techniques to automatically extract additional bilingual texts from comparable corpora. 1 Introduction This paper describes the machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2009 WMT shared task evaluation. This work was performed in cooperation with the company SYSTRAN. We only consider the translation between French and English (in both directions). The main differences to the previous year’s system (Schwenk et al., 2008) are as follows: better usage of SYSTRAN’s bilingual dictionary in the statistical system, less bilingual training data, additional language model training data (news-train08 as distributed by the organizers), usage of comparable corpora to improve the translation model, and development of a statistical post-editing system (SPE). These different components are described in the following. 2 Used Resources In the frame work of the 2009 WMT shared translation task many resources were made available. The following sections describe how they were used to train the translation and language models of"
W09-0423,2008.iwslt-papers.6,1,0.811009,"human evaluation. With respect to the SMT system, we were not able to improve the translation model by adding large amounts of bitexts, although different 133 sources were available (Canadian Hansard, UN or WEB data). Eventually these corpora are too noisy or out-of-domain. On the other hand, the integration of a high quality bilingual dictionary was helpful, as well as the automatic alignment of news texts from comparable corpora. Future work will concentrate on the integration of previously successful techniques, in particular continuous space language models and lightlysupervised training (Schwenk, 2008). We also believe that the tokenization could be improved, in particular for the French sources texts. Numbers, dates and other numerical expressions could be translated by a rule-based system. System combination has recently shown to provide important improvements of translation quality. We are currently working on a combination of the SMT and SPE system. It may be also interesting to add a third (hierarchical) MT system. 7 Acknowledgments This work has been partially funded by the French Government under the project I NSTAR (ANR JCJC06 143038) and the by the Higher Education Commission, Paki"
W09-0423,W07-0728,0,0.0556224,"arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system During the last years statistical post-editing systems have shown to achieve very competitive performance (Simard et al., 2007; Dugast et al., 2007). The main idea of this techniques is to use 2 The source is available at http://www.cs.cmu. edu/˜qing/ 132 Corpus SMT system Eparl+NC Eparl+NC+dict Eparl+NC+dict+AFP SPE system SYSTRAN Eparl+NC Eparl+NC+AFP # En words Dev09a Dev09b Test09 41.6M 44.0M 51.7M 21.89 22.28 22.21 21.78 22.35# 21.43 23.80 24.13 23.88 44.2M 53.3M 18.68 23.03 22.95 18.84 23.15 23.15∗ 20.29 24.36 24.62 Table 3: Case sensitive NIST BLEU scores for the English-French systems. “NC” denotes the newscommentary bitexts, “dict” denotes SYSTRAN’s bilingual dictionary and “AFP” the automatically aligned ne"
W09-0423,N03-1017,0,\N,Missing
W11-2142,J04-2004,0,0.0163665,"e a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all 360 source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor. 2.3 LIMSI-CNRS Single System 2.3.1 System overview The LIMSI system is built with n-code2 , an open source statistical machine translation system based on bilingual n-grams. 2.3.2 n-code Overview In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model w"
W11-2142,J07-2003,0,0.0225833,"and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase"
W11-2142,W08-0310,1,0.899949,"Missing"
W11-2142,P07-1019,0,0.0206925,"or Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded ph"
W11-2142,P02-1040,0,0.102913,"Missing"
W11-2142,E03-1076,0,0.0231201,"l table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogeneous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 For the German→English task, RWTH conducted experiments comparing the standard phrase extraction with the phrase training technique described in Section 2.1.3. Further experiments included the use of additional language model training data, reranking of n-best lists generated by the phrase-based system, and different optimization criteria. A considerable increase in translation quality can be achieved by application of German compound splitting (Koehn and Knight, 20"
W11-2142,W07-0732,1,0.818478,"a statistical post editing (SPE) component. The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k − 800k entries per language pair). The basic setup of the SPE component is identical to the one described in (L. Dugast and Koehn, 2007). A statistical translation model is trained on the rule-based translation of the source and the target side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Moreover, the following measures − limiting unwanted statistical effects − were applied: • Named entities are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by t"
W11-2142,E06-1005,1,0.83205,"d 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. A deeper description will be also given in the WMT11 system combination paper of RWTH Aachen University. For this task only the A2L framework has been used. 4 Experiments We tried different system combinations with different sets of single systems and different optimization criteria. As RWTH has two different translation systems, we pu"
W11-2142,W09-0435,1,0.836207,"ystem applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract also phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test sentences. Therefore,"
W11-2142,J03-1002,1,0.00747756,"joint translation by combining the knowledge of the four project partners. Each group develop and maintain their own different machine translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algor"
W11-2142,P03-1021,0,0.147772,"etups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tab"
W11-2142,P07-2045,0,0.0131162,"parallel corpus (whose target is identical to the source). This was added to the parallel text in order to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. • Phrase pairs appearing less than 2 times were pruned. The SPE language model was trained 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W11-2142,2007.tmi-papers.21,0,0.0203552,"lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net/ ger (Schmid, 1994). In addition, the system applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract als"
W11-2142,N04-4026,0,0.0225696,"ew In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the news"
W11-2142,W05-0836,1,0.901096,"rd heuristic phrase extraction techniques, performing force alignment phrase training (FA) gives an improvement in BLEU on newstest2008 and newstest2009, but a degradation in TER. The addition of LDC Gigaword corpora (+GW) to the language model training data shows improvements in both BLEU and TER. Reranking was done on 1000-best lists generated by the the best available 359 Preprocessing System Overview The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation. Optimization with regard to the BLEU score is done using Minimum Error Rate Training as described by Venugopal et al. (2005). The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a GIZA++ Word Alignment. We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained on the Gigaword corpus. Reordering is performed based on continuous and non-continuous POS rules to cover short and long-range reorderings. The long-range reordering rules were also applied to the training corpus and phrase extraction was performed on the resulting reordering lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net"
W11-2142,W10-1738,1,0.832324,"are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institut"
W11-2142,P10-1049,1,0.823271,"ons. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and"
W11-2142,W06-3110,1,0.850765,"ase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and IBM-1 lexicon models in both normal and inverse direction. These models are combined in a log-linear fashion and the scaling factors are tuned in the same manner as the baseline system (using TER−4BLEU on newstest2009). The final table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each"
W11-2142,2008.iwslt-papers.8,1,0.820865,"preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hie"
W11-2142,D08-1076,0,\N,Missing
W12-3140,J04-2004,0,0.0800912,"nslation model relies on a specific decomposition of the joint probability of a sentence pair P(s, t) using the n-gram assumption: a sentence pair is decomposed into a sequence of bilingual units called tuples, defining a joint segmentation of the source and target. In the approach of (Mari˜no et al., 2006), this segmentation is a by-product of source reordering which ultimately derives from initial word and phrase alignments. 2.3.1 An Overview of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model"
W12-3140,J07-2003,0,0.0283293,"322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting"
W12-3140,W08-0310,1,0.935329,"Missing"
W12-3140,2010.iwslt-papers.6,0,0.0806631,"as last year5 and thus the same target language model as detailed in (Allauzen et al., 2011). For English, we took advantage of our in-house text processing tools for tokenization and detokenization steps (D´echelotte et al., 2008) and our system was built in ”true-case”. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which is detrimental both at training and decoding time. Thus, the German side was normalized using a specific pre-processing scheme (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 2010), which notably aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. 2.4 SYSTRAN Software, Inc. Single System The data submitted by SYSTRAN were obtained by a system composed of the standard SYSTRAN MT engine in combination with a statistical post editing (SPE) component. 4 http://geek.kyloo.net/software 5 The fifth edition of the English Gigaword (LDC2011T07) was not used. 325 The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has alwa"
W12-3140,P07-1019,0,0.0343944,"on criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The"
W12-3140,E03-1076,0,0.55884,"ranslation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single Syst"
W12-3140,W07-0732,1,0.793396,"rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k 800k entries per language pair). The SYSTRAN phrase-based SPE component views the output of the rule-based system as the source language, and the (human) reference translation as the target language, see (L. Dugast and Koehn, 2007). It performs corrections and adaptions learned from the 5-gram language model trained on the parallel target-to-target corpus. Moreover, the following measures - limiting unwanted statistical effects - were applied: • Named entities, time and numeric expressions are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by the rule-based engine. • The intersection of both vocabularies (i.e. vocabularies of the rule-based output and the reference translation) is"
W12-3140,N12-1005,1,0.858653,"bilingual pairs, which means that the underlying vocabulary can be quite large. Unfortunately, the parallel data available to train these models are typically smaller than the corresponding monolingual corpora used to train target language models. It is very likely then, that such models should face severe estimation problems. In such setting, using neural network language 3 Part-of-speech labels for English and German are computed using the TreeTagger (Schmid, 1995). 2 http://ncode.limsi.fr/ 324 model techniques seem all the more appropriate. For this study, we follow the recommendations of Le et al. (2012), who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words. This yields a word factored translation model that can be estimated in a continuous space using the SOUL architecture (Le et al., 2011). The design and integration of a SOUL model for large SMT tasks is far from easy, given the computational cost of computing n-gram probabilities. The solution used here was to resort to a two pass approach: the first pass uses a conventional back-off n-gram model to produce a k-best list; in the second pass, t"
W12-3140,J06-4004,1,0.843277,"Missing"
W12-3140,E06-1005,1,0.849031,"glish LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 4 Experiments This year, we tried different sets of single systems for system combination. As RWTH has two different translation systems, we put the output of both systems into system combination. Although both systems have the same preprocessing and language model, their hypotheses differ because of their different decoding approach."
W12-3140,D09-1022,1,0.869178,"done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using t"
W12-3140,2011.iwslt-evaluation.9,1,0.871665,"ocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We us"
W12-3140,P10-2041,0,0.0181873,"ound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single System quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-bas"
W12-3140,W09-0435,1,0.860476,"Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as i"
W12-3140,W08-0303,1,0.84967,"very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters o"
W12-3140,2011.iwslt-papers.6,1,0.847434,"he Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Further"
W12-3140,W11-2124,1,0.868397,"length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known"
W12-3140,J03-1002,1,0.00977827,"RO partner trained their systems on the parallel Europarl and News Commentary corpora. All single systems were tuned on the newstest2009 or newstest2010 development set. The newstest2011 dev set was used to train the system combination parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram langu"
W12-3140,P03-1021,0,0.230828,"n (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out"
W12-3140,P07-2045,0,0.00885526,"to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. The SPE language model was trained on 2M bilingual phrases from the news/Europarl corpora, provided as training data for WMT 2012. An additional language model built from 15M phrases of the English LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W12-3140,W08-1006,0,0.100843,"ained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. For the test sentences, the reordering based on parts-of-speech and trees allows us to change the word order in the source sentence so that the sentence can be translated more easily. In addition, we build reordering lattices for all trainin"
W12-3140,2007.tmi-papers.21,0,0.266844,"the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are"
W12-3140,N04-4026,0,0.0510078,"of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the n"
W12-3140,W05-0836,1,0.92366,"rmed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system a"
W12-3140,W10-1738,1,0.875756,"by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 P"
W12-3140,2008.iwslt-papers.8,1,0.841876,"parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2."
W12-3140,W11-2135,1,\N,Missing
W12-3140,W10-1704,1,\N,Missing
W12-3140,D08-1076,0,\N,Missing
W17-4722,D15-1166,0,0.0436414,"rent hidden state hi and a context vector ci that aims at capturing relevant source-side information. Neural MT System Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Crego et al., 2016; Wu et al., 2016). NMT has now become a widely-applied technique for machine translation, as well as an effective approach 1 Figure 2 illustrates the attention layer. It implements the ""general"" attentional architecture from (Luong et al., 2015). The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the context vector ct . Hence, global alignment weights at are derived by http://opennmt.net 265 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 265–270 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Figure 1: Schematic view of our MT network. comparing the current target hidden state ht with each source hidden state hs : at (s) = P be stacked. More details about our system can be found in (Crego e"
W17-4722,P10-2041,0,0.021134,"at training work described in Table 3 is built as continuation of the model at epoch 13 on Table 2. Table 3 shows also BLEU scores over newstest2017 for the best performing network. As for the experiments detailed in Table 2, once all splits of the synthetic corpus were used to train our models with learning rate always set to 1.0 (5 epochs for German→English and 8 epochs for English→German), we began a decay mode. In this case, we decided to reduce the amount of training examples from 9 to 5 millions due to time restrictions. To select the training data we employed the algorithm detailed in (Moore and Lewis, 2010). It aims at identifying sentences in a generic corpus that are closer to domainsuch replacement is called a merge, and the number of merges is a tuneable parameter. Encodings were computed over the union of both German and English training corpora after preprocessing, aiming at improving consistency between source and target segmentations. Finally, case information was considered by the network as an additional feature. It allowed us to work with a lowercased vocabulary and treat recasing as a separate problem (Crego et al., 2016). 3.2 Training Details All experiments employ the NMT system de"
W17-4722,P16-1009,0,0.116247,"ith standard tokenisation tools. German words were further preprocessed to split compounds, following a similar algorithm as the built-in for Moses. Additional monolingual data was also used for both German and English available for the shared task: News Crawl: articles from 2016. Basic statistics of the tokenised data are available in Table 1. We used a byte pair encoding technique2 (BPE) to segment word forms and achieve openvocabulary translation with a fixed vocabulary of 30, 000 source and target tokens. BPE was originally devised as a compression algorithm, adapted to word segmentation (Sennrich et al., 2016b). It recursively replaces frequent consecutive bytes with a symbol that does not occur elsewhere. Each Figure 2: Attention layer of the MT network. Note that for the sake of simplicity figure 1 illustrates a two-layers LSTM encoder/decoder while any arbitrary number of LSTM layers can 2 https://github.com/rsennrich/ subword-nmt 266 #sents Parallel En 4.6M De 4.6M Monolingual En 20,6M De 34,7M #words vocab. Lmean 103.7M 104.5M 627k 836k 22.6 22.8 463,6M 620,8M 1.18M 3.36M 22.5 17.8 using multi-bleu.perl3 . Training time per epoch is also shown in row Time measured in number of hours. As expec"
W17-4722,P16-1162,0,0.319344,"ith standard tokenisation tools. German words were further preprocessed to split compounds, following a similar algorithm as the built-in for Moses. Additional monolingual data was also used for both German and English available for the shared task: News Crawl: articles from 2016. Basic statistics of the tokenised data are available in Table 1. We used a byte pair encoding technique2 (BPE) to segment word forms and achieve openvocabulary translation with a fixed vocabulary of 30, 000 source and target tokens. BPE was originally devised as a compression algorithm, adapted to word segmentation (Sennrich et al., 2016b). It recursively replaces frequent consecutive bytes with a symbol that does not occur elsewhere. Each Figure 2: Attention layer of the MT network. Note that for the sake of simplicity figure 1 illustrates a two-layers LSTM encoder/decoder while any arbitrary number of LSTM layers can 2 https://github.com/rsennrich/ subword-nmt 266 #sents Parallel En 4.6M De 4.6M Monolingual En 20,6M De 34,7M #words vocab. Lmean 103.7M 104.5M 627k 836k 22.6 22.8 463,6M 620,8M 1.18M 3.36M 22.5 17.8 using multi-bleu.perl3 . Training time per epoch is also shown in row Time measured in number of hours. As expec"
W17-4722,P17-4012,1,\N,Missing
W18-1817,D17-1151,0,0.135168,"ston, March 17 - 21, 2018 |Page 180 System BLEU-cased uedin-nmt-ensemble LMU-nmt-reranked-wmt17-en-de SYSTRAN-single (OpenNMT) 28.3 27.1 26.7 Table 1: Top 3 on English-German newstest2017 WMT17. System Nematus ONMT Speed tok/sec Train Trans BLEU System newstest14 newstest17 3221 5254 18.25 19.34 seq2seq Sockeye ONMT 22.19 23.23 [19.34] 25.55 25.06 [22.69] 252 457 Table 2: Performance results for EN→DE on WMT15 tested on newstest2014. Both systems 2x500 RNN, embedding size 300, 13 epochs, batch size 64, beam size 5. We compare on a 32k BPE setting. Table 3: OpenNMT’s performance as reported by Britz et al. (2017) and Hieber et al. (2017) (bracketed) compared to our best results. ONMT used 32k BPE, 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. Extensible Data, Models, and Search In addition to plain text, OpenNMT also supports different input types including models with discrete features (Sennrich and Haddow, 2016), models with non-sequential input such as tables, continuous data such as speech signals, and multi-dimensional data such as images. To support these different input modalities the library implements image encoder (Xu et al., 2015; Deng et al., 2017) and audio"
W18-1817,D14-1179,0,0.0444157,"Missing"
W18-1817,N16-1012,1,0.881204,"Missing"
W18-1817,P16-5005,0,0.0299161,"ce and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques. OpenNMT has been used in several production MT systems, modiﬁed for numerous research papers, and is implemented across several deep learning frameworks. 1 Introduction Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT"
W18-1817,P10-4002,0,0.0336078,"and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community beneﬁted greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) for syntax-based SMT, NMT toolkits can provide a foundation to build upon. A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efﬁcient and accurate enough to be used in production contexts. With these goals in mind, in this work we present an open-source toolkit for developing neural machine translation systems, known as OpenNMT (http://opennmt.net). Since its launch in December 2016, OpenNMT has become a collection of implementations targeting both academia and industry. The system is designed to be simple to use"
W18-1817,W17-3518,0,0.0185142,"7), data-toProceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 181 System GNMT 4 layers GNMT 8 layers WMT reference ONMT newstest14 newstest15 23.7 24.4 20.6 23.2 26.5 27.6 24.9 26.0 Table 4: Comparison with GNMT on EN→DE. ONMT used 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. System T2T ONMT T2T GNMT (rnn) ONMT (rnn) newstest14 newstest17 27.3 26.8 24.6 23.2 27.8 28.0 25.1 Table 5: Transformer Results on English-German newstest14 and newstest17. We use 6-layer transformer with model size of 512. document (Wiseman et al., 2017; Gardent et al., 2017), and transliteration (Ameur et al., 2017), to name a few of many applications. Additional Tools OpenNMT packages several additional tools, including: 1) reversible tokenizer, which can also perform Byte Pair Encoding (BPE) (Sennrich et al., 2015); 2) loading and exporting word embeddings; 3) translation server which enables showcase results remotely; and 4) visualization tools for debugging or understanding, such as beam search visualization, proﬁler and TensorBoard logging. 5 Experiments OpenNMT achieves competitive results against other systems, e.g. in the recent WMT 2017 translation task,"
W18-1817,P16-1154,0,0.0123343,"g models with discrete features (Sennrich and Haddow, 2016), models with non-sequential input such as tables, continuous data such as speech signals, and multi-dimensional data such as images. To support these different input modalities the library implements image encoder (Xu et al., 2015; Deng et al., 2017) and audio encoders (Chan et al., 2015). OpenNMT implements various attention types including general, dot product, and concatenation (Luong et al., 2015a; Britz et al., 2017). This also includes recent extensions to these standard modules such as the copy mechanism (Vinyals et al., 2015; Gu et al., 2016), which is widely used in summarization and generation applications. The newer implementations of OpenNMT have also been updated to include support for recent innovations in non-recurrent translation models. In particular recent support has been added for convolution translation (Gehring et al., 2017) and the attention-only transformer network (Vaswani et al., 2017). Finally, the translation code allows for user customization. In addition to out-of-vocabulary (OOV) handling (Luong et al., 2015b), OpenNMT also allows beam search with various normalizations including length and attention coverag"
W18-1817,E17-3017,0,0.116372,"8 |Page 180 System BLEU-cased uedin-nmt-ensemble LMU-nmt-reranked-wmt17-en-de SYSTRAN-single (OpenNMT) 28.3 27.1 26.7 Table 1: Top 3 on English-German newstest2017 WMT17. System Nematus ONMT Speed tok/sec Train Trans BLEU System newstest14 newstest17 3221 5254 18.25 19.34 seq2seq Sockeye ONMT 22.19 23.23 [19.34] 25.55 25.06 [22.69] 252 457 Table 2: Performance results for EN→DE on WMT15 tested on newstest2014. Both systems 2x500 RNN, embedding size 300, 13 epochs, batch size 64, beam size 5. We compare on a 32k BPE setting. Table 3: OpenNMT’s performance as reported by Britz et al. (2017) and Hieber et al. (2017) (bracketed) compared to our best results. ONMT used 32k BPE, 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. Extensible Data, Models, and Search In addition to plain text, OpenNMT also supports different input types including models with discrete features (Sennrich and Haddow, 2016), models with non-sequential input such as tables, continuous data such as speech signals, and multi-dimensional data such as images. To support these different input modalities the library implements image encoder (Xu et al., 2015; Deng et al., 2017) and audio encoders (Chan et al., 20"
W18-1817,P07-2045,0,0.0157885,"models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community beneﬁted greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) for syntax-based SMT, NMT toolkits can provide a foundation to build upon. A toolkit should aim to provide a shared framework for developing and comparing open-source systems, while at the same time being efﬁcient and accurate enough to be used in production contexts. With these goals in mind, in this work we present an open-source toolkit for developing neural machine translation systems, known as OpenNMT (http://opennmt.net). Since its launch in December 2016, OpenNMT has become a collection of implementations targeting both academia and ind"
W18-1817,W17-4505,1,0.845813,"016) and structured attention (Kim et al., 2017) with minimal change of code. As another example, in order to get feature-based factored neural translation (Sennrich and Haddow, 2016) we simply need to modify the input network to process the feature-based representation, and the output network to produce multiple conditionally independent predictions. We have seen instances of this use in published research. In addition to machine translation (Levin et al., 2017; Ha et al., 2017; Ma et al., 2017), researchers have employed OpenNMT for parsing (van Noord and Bos, 2017), document summarization (Ling and Rush, 2017), data-toProceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 181 System GNMT 4 layers GNMT 8 layers WMT reference ONMT newstest14 newstest15 23.7 24.4 20.6 23.2 26.5 27.6 24.9 26.0 Table 4: Comparison with GNMT on EN→DE. ONMT used 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. System T2T ONMT T2T GNMT (rnn) ONMT (rnn) newstest14 newstest17 27.3 26.8 24.6 23.2 27.8 28.0 25.1 Table 5: Transformer Results on English-German newstest14 and newstest17. We use 6-layer transformer with model size of 512. document (Wiseman et al., 2017; Ga"
W18-1817,D15-1166,0,0.827632,"ed in several production MT systems, modiﬁed for numerous research papers, and is implemented across several deep learning frameworks. 1 Introduction Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community beneﬁted greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based SMT and CDec (Dyer et al., 2010) for syntax-based SMT, NMT toolkits can provide a foundation to build upo"
W18-1817,W17-4751,0,0.016057,"tion module, we can implement local attention (Luong et al., 2015a), sparse-max attention (Martins and Astudillo, 2016) and structured attention (Kim et al., 2017) with minimal change of code. As another example, in order to get feature-based factored neural translation (Sennrich and Haddow, 2016) we simply need to modify the input network to process the feature-based representation, and the output network to produce multiple conditionally independent predictions. We have seen instances of this use in published research. In addition to machine translation (Levin et al., 2017; Ha et al., 2017; Ma et al., 2017), researchers have employed OpenNMT for parsing (van Noord and Bos, 2017), document summarization (Ling and Rush, 2017), data-toProceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 181 System GNMT 4 layers GNMT 8 layers WMT reference ONMT newstest14 newstest15 23.7 24.4 20.6 23.2 26.5 27.6 24.9 26.0 Table 4: Comparison with GNMT on EN→DE. ONMT used 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. System T2T ONMT T2T GNMT (rnn) ONMT (rnn) newstest14 newstest17 27.3 26.8 24.6 23.2 27.8 28.0 25.1 Table 5: Transformer Results on English-"
W18-1817,W16-2209,0,0.108277,"19.34] 25.55 25.06 [22.69] 252 457 Table 2: Performance results for EN→DE on WMT15 tested on newstest2014. Both systems 2x500 RNN, embedding size 300, 13 epochs, batch size 64, beam size 5. We compare on a 32k BPE setting. Table 3: OpenNMT’s performance as reported by Britz et al. (2017) and Hieber et al. (2017) (bracketed) compared to our best results. ONMT used 32k BPE, 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. Extensible Data, Models, and Search In addition to plain text, OpenNMT also supports different input types including models with discrete features (Sennrich and Haddow, 2016), models with non-sequential input such as tables, continuous data such as speech signals, and multi-dimensional data such as images. To support these different input modalities the library implements image encoder (Xu et al., 2015; Deng et al., 2017) and audio encoders (Chan et al., 2015). OpenNMT implements various attention types including general, dot product, and concatenation (Luong et al., 2015a; Britz et al., 2017). This also includes recent extensions to these standard modules such as the copy mechanism (Vinyals et al., 2015; Gu et al., 2016), which is widely used in summarization and"
W18-1817,D17-1239,1,0.846882,"on (Ling and Rush, 2017), data-toProceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 181 System GNMT 4 layers GNMT 8 layers WMT reference ONMT newstest14 newstest15 23.7 24.4 20.6 23.2 26.5 27.6 24.9 26.0 Table 4: Comparison with GNMT on EN→DE. ONMT used 2-layers bi-RNN of 1024, embedding size 512, dropout 0.1 and max length 100. System T2T ONMT T2T GNMT (rnn) ONMT (rnn) newstest14 newstest17 27.3 26.8 24.6 23.2 27.8 28.0 25.1 Table 5: Transformer Results on English-German newstest14 and newstest17. We use 6-layer transformer with model size of 512. document (Wiseman et al., 2017; Gardent et al., 2017), and transliteration (Ameur et al., 2017), to name a few of many applications. Additional Tools OpenNMT packages several additional tools, including: 1) reversible tokenizer, which can also perform Byte Pair Encoding (BPE) (Sennrich et al., 2015); 2) loading and exporting word embeddings; 3) translation server which enables showcase results remotely; and 4) visualization tools for debugging or understanding, such as beam search visualization, proﬁler and TensorBoard logging. 5 Experiments OpenNMT achieves competitive results against other systems, e.g. in the recent WMT"
W18-2715,P17-2091,0,0.201752,"print, we applied basic optimization techniques to reduce the final size of our models. Our strategy for the shared task was to take advantage of four main optimization techniques: (a) sequence-level distillation, in particular cross-class distillation from a transformer model (Vaswani et al., 2017) to an RNN, (b) architecture search, changing the structure of the network by increasing the size of the most efficient modules, reducing the size of the most costly modules and replacing default gated units, (c) specialized precomputation such as reducing dynamically the runtime target vocabulary (Shi and Knight, 2017), and (d) quantization and faster matrix operations, based on the work of Devlin (2017) and gemmlowp2 . All of these methods are employed in a special-purpose C++-based decoder CTranslate3 . The complete training workflow including data preparation and distillation is described in Section 2. Inference techniques and quantization are described in Section 3. Our experiments compare the different approaches in terms of speed and accuracy. A meta Introduction As neural machine translation becomes more widely deployed in production environments, it becomes also increasingly important to serve trans"
W18-2715,W18-2701,0,0.0219747,"a preparation and distillation is described in Section 2. Inference techniques and quantization are described in Section 3. Our experiments compare the different approaches in terms of speed and accuracy. A meta Introduction As neural machine translation becomes more widely deployed in production environments, it becomes also increasingly important to serve translations models in a way as fast and as memory-efficient as possible, both on dedicated GPU and on standard CPU hardwares. The WNMT 2018 shared task1 focused on comparing different systems on both accuracy and computational efficiency (Birch et al., 2018). This paper describes the entry for the OpenNMT system to this competition. Our specific interest was to explore the different techniques for training and optimizing CPU models for very high throughput while preserving highest possible accuracy compared to state-of-the-art. While we did not put real focus on memory and docker size foot2 1 https://sites.google.com/site/wnmt18/ shared-task 3 https://github.com/google/gemmlowp https://github.com/OpenNMT/CTranslate 122 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 122–128 c Melbourne, Australia, July 20, 2018"
W18-2715,D17-1300,0,0.0327723,"ategy for the shared task was to take advantage of four main optimization techniques: (a) sequence-level distillation, in particular cross-class distillation from a transformer model (Vaswani et al., 2017) to an RNN, (b) architecture search, changing the structure of the network by increasing the size of the most efficient modules, reducing the size of the most costly modules and replacing default gated units, (c) specialized precomputation such as reducing dynamically the runtime target vocabulary (Shi and Knight, 2017), and (d) quantization and faster matrix operations, based on the work of Devlin (2017) and gemmlowp2 . All of these methods are employed in a special-purpose C++-based decoder CTranslate3 . The complete training workflow including data preparation and distillation is described in Section 2. Inference techniques and quantization are described in Section 3. Our experiments compare the different approaches in terms of speed and accuracy. A meta Introduction As neural machine translation becomes more widely deployed in production environments, it becomes also increasingly important to serve translations models in a way as fast and as memory-efficient as possible, both on dedicated"
W18-2715,D16-1139,1,0.873957,"uccessful for reducing the size of neural models. We considered the transformer network as our teacher network. We used OpenNMT-tf 6 to train two transformer based systems: base and large described in Table 2 with their evaluation in Table 3. For both, the learning rate is set to 2.0 and warmup steps 8000, we average the last 8 checkpoints to get the final model. Our baseline system outperforms the provided baseline Sockeye model by +0.37 BLEU on newstest2014. 2.2 Distillation to RNN To train our smaller student system, we follow the sequence-level knowledge distillation approach described by Kim and Rush (2016). First, we build the full transformer as above. Next, we use the teacher system to retranslate all the training source sentences to generate a set of simplified target sentences. Then, we use this simplified corpus (original source and newly generated target) to train a student system. The student system can be assigned with smaller network size, in our case a RNN-based sequence-to-sequence model similar to Bahdanau et al. (2014) Results from Crego and Senellart (2016) show that the distillation process not only improves the throughput of the student models and reduce their size, but can also"
W18-2715,P17-4012,1,0.866484,"Missing"
W18-2715,P07-2045,0,0.0250883,"Missing"
W18-2715,D15-1166,0,0.146064,"Missing"
W18-2715,N18-2074,0,0.0351052,"d Senellart (2016) who reported that student models could outperform their teacher for reference RNN-based model. (b) We compare quantitatively different quantizations, and (c) we give an improved algorithm to dynamically select target vocabulary for a given batch. Finally, we also report several complementary experiments that resulted in systems inside of the pareto convex border. For instance, we compare using 8-bit quantization to 16-bit quantization. 2 Teacher Model: Transformer Transformer networks (Vaswani et al., 2017) are the current state-of-the art in many machine translation tasks (Shaw et al., 2018). The network directly models the representations of each sentence with a self-attention mechanism. Hence much longer term dependencies than with standard sequence-to-sequence models can be learned, which is especially important for language pairs like English-German. In addition, transformer allows to easily parallelise the MLE training process across multiple GPUs. However, a large number of parameters are needed by the network to obtain its best performance. In order to reduce the model size, we applied knowledge distillation, a technique that has proven successful for reducing the size of"
W18-3914,W16-4819,0,0.165364,"combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN architecture based on (Kim et al., 2016) using filters of increasing size 1*50, 2*50, 3*100, 4*100, 5*100, 6*100, 7*100 which gave a better F1 score 0.483 and ranked 2nd in the competition (Belinkov and Glass, 2016). Character-level CNNs present the advantage to be fast and able to learn local representations, which can be likened to char n-grams features successfully used by SVMs. One reason advanced by (Sadat et al., 2014) for the efficiency of character-level representations for Arabic Dialect Identification in speech transcription is that a great part of the variation between Arabic dialects is based on their affixes. However, as noticed by the organisers, Arabic speakers distinguish Arabic dialects not only according to words but also on the basis of speech cues absent from written transcripts. So f"
W18-3914,W16-4802,0,0.0233465,"s work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 128 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 128–136 Santa Fe, New Mexico, USA, August 20, 2018. scores ranging from 0.495 to 0.513. Three teams reported experiments with neural network architectures that were finally not submitted as their models following other machine learning methods obtained higher accuracy scores, QCRI (Eldesouki et al., 2016), GW LT3 (Zirikly et al., 2016) and tufbasfs (C¸o¨ ltekin and Rama, 2016). Two teams submitted systems training neural networks: the team cgli competed using a character-level Convolutional Neural Network (CNN) with the combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN architecture based on (Kim et al., 2016) usi"
W18-3914,W16-4828,0,0.0137054,"scribed in (Ali et al., 2014). The best performing systems obtained F1 This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 128 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 128–136 Santa Fe, New Mexico, USA, August 20, 2018. scores ranging from 0.495 to 0.513. Three teams reported experiments with neural network architectures that were finally not submitted as their models following other machine learning methods obtained higher accuracy scores, QCRI (Eldesouki et al., 2016), GW LT3 (Zirikly et al., 2016) and tufbasfs (C¸o¨ ltekin and Rama, 2016). Two teams submitted systems training neural networks: the team cgli competed using a character-level Convolutional Neural Network (CNN) with the combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used"
W18-3914,W16-4824,0,0.0164015,"scores, QCRI (Eldesouki et al., 2016), GW LT3 (Zirikly et al., 2016) and tufbasfs (C¸o¨ ltekin and Rama, 2016). Two teams submitted systems training neural networks: the team cgli competed using a character-level Convolutional Neural Network (CNN) with the combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN architecture based on (Kim et al., 2016) using filters of increasing size 1*50, 2*50, 3*100, 4*100, 5*100, 6*100, 7*100 which gave a better F1 score 0.483 and ranked 2nd in the competition (Belinkov and Glass, 2016). Character-level CNNs present the advantage to be fast and able to learn local representations, which can be likened to char n-grams features successfully used by SVMs. One reason advanced by (Sadat et al., 2014) for the efficiency of character-level representations for Arabic Dialect Identification in speech transcription is that"
W18-3914,D14-1181,0,0.00370822,"g from 0.495 to 0.513. Three teams reported experiments with neural network architectures that were finally not submitted as their models following other machine learning methods obtained higher accuracy scores, QCRI (Eldesouki et al., 2016), GW LT3 (Zirikly et al., 2016) and tufbasfs (C¸o¨ ltekin and Rama, 2016). Two teams submitted systems training neural networks: the team cgli competed using a character-level Convolutional Neural Network (CNN) with the combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN architecture based on (Kim et al., 2016) using filters of increasing size 1*50, 2*50, 3*100, 4*100, 5*100, 6*100, 7*100 which gave a better F1 score 0.483 and ranked 2nd in the competition (Belinkov and Glass, 2016). Character-level CNNs present the advantage to be fast and able to learn local representations, which can be likened to char"
W18-3914,W16-4801,0,0.176055,"Missing"
W18-3914,W14-5904,0,0.0722905,"per a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN architecture based on (Kim et al., 2016) using filters of increasing size 1*50, 2*50, 3*100, 4*100, 5*100, 6*100, 7*100 which gave a better F1 score 0.483 and ranked 2nd in the competition (Belinkov and Glass, 2016). Character-level CNNs present the advantage to be fast and able to learn local representations, which can be likened to char n-grams features successfully used by SVMs. One reason advanced by (Sadat et al., 2014) for the efficiency of character-level representations for Arabic Dialect Identification in speech transcription is that a great part of the variation between Arabic dialects is based on their affixes. However, as noticed by the organisers, Arabic speakers distinguish Arabic dialects not only according to words but also on the basis of speech cues absent from written transcripts. So for the DSL shared task 2017 (Zampieri et al., 2017), the ADI dataset contained twice more utterances than in 2016 and not only speech transcriptions but also acoustic features corresponding to sentences, namely i-"
W18-3914,J14-1006,0,0.0681215,"Missing"
W18-3914,W17-1201,0,0.167572,"Missing"
W18-3914,N16-1178,0,0.0236066,"challenge, but also end-to-end systems directly working on acoustic representation of speech data. 3.2.1 SVM In order to compare traditional machine learning and neural network approaches, we trained a multi-class Support Vector Machine (SVM) classifier using a radial basis function. We used the freely available LIBSVM1 software (Chang and Lin, 2011). 3.2.2 Multi-Input CNN (run 1) Our search for a simple and fast architecture to independently learn input embeddings of different type and combine them oriented us towards the Multi Group Convolutional Neural Network, also called MultiInput CNN (Zhang et al., 2016). Initially designed to join different word embeddings, these models allow the input embeddings to come from various sources and not to share the same dimensionality. Therefore, our first run is a Multi-Input CNN that we tailored to take as input the lexical, phonetic and acoustic data proposed for the challenge: it independently learns char embeddings and 4 phone embeddings by running convolutions with various filter sizes, respectively char-level convolutions on word transcripts and phone-level convolutions on phone transcripts. Then it concatenates the 5 resulting embeddings and the given a"
W18-3914,W16-4804,0,0.0143195,"e best performing systems obtained F1 This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 128 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 128–136 Santa Fe, New Mexico, USA, August 20, 2018. scores ranging from 0.495 to 0.513. Three teams reported experiments with neural network architectures that were finally not submitted as their models following other machine learning methods obtained higher accuracy scores, QCRI (Eldesouki et al., 2016), GW LT3 (Zirikly et al., 2016) and tufbasfs (C¸o¨ ltekin and Rama, 2016). Two teams submitted systems training neural networks: the team cgli competed using a character-level Convolutional Neural Network (CNN) with the combination of filters 3*128, 4*128, 5*128 (convention adopted throughout the whole article: filter size*number of filters) inspired from (Kim, 2014) which gave a macro F1 score of 0.433. They also reported in their paper a Long-Short Term Memory network (LSTM) with pre-compiled word embeddings giving as F1 score 0.423 after bug correction (Guggilla, 2016). The team mitsls used another character-level CNN ar"
W18-6485,P18-4020,0,0.0697091,"Missing"
W18-6485,P07-2045,0,0.0111924,"esearch/MUSE 6 https://github.com/clab/fast align 7 https://github.com/TALP-UPC/FreeLing.git 5 http://opennmt.net 936 Figure 2: BLEU score of the best submission of each participant measured for the neural MT system trained with 100M tokens. Score is averaged over the six blind test sets. the sentence pairs on the 1 billion word GermanEnglish Paracrawl corpus. Scores do not have to be meaningful, except that higher scores indicate better quality. The performance of the submissions is evaluated by sub-sampling 10 million and 100 million word corpora based on these scores, training statistical (Koehn et al., 2007) and neural (Junczys-Dowmunt et al., 2018) MT systems with these corpora, and assessing translation quality on six blind test sets8 using the BLEU (Papineni et al., 2002) score. Figure 2 displays the score of the best submission of each individual participant corresponding to the 100 million tokens corpus using the neural MT system. BLEU score is averaged over the six blind test sets. As it can be seen, very similar results were obtained by most of the participants. Accuracy results fall within a margin of 3 points BLEU for the first 16 classified. 6 in our objective as we built a very simple"
W18-6485,W16-2207,0,0.0375384,"Missing"
W18-6485,P02-1040,0,0.109343,"ch participant measured for the neural MT system trained with 100M tokens. Score is averaged over the six blind test sets. the sentence pairs on the 1 billion word GermanEnglish Paracrawl corpus. Scores do not have to be meaningful, except that higher scores indicate better quality. The performance of the submissions is evaluated by sub-sampling 10 million and 100 million word corpora based on these scores, training statistical (Koehn et al., 2007) and neural (Junczys-Dowmunt et al., 2018) MT systems with these corpora, and assessing translation quality on six blind test sets8 using the BLEU (Papineni et al., 2002) score. Figure 2 displays the score of the best submission of each individual participant corresponding to the 100 million tokens corpus using the neural MT system. BLEU score is averaged over the six blind test sets. As it can be seen, very similar results were obtained by most of the participants. Accuracy results fall within a margin of 3 points BLEU for the first 16 classified. 6 in our objective as we built a very simple network that was able to filter out divergent sentence pairs. Only assisted by a very simple filtering technique using rules based on length and language identification."
W18-6485,D18-1328,1,0.826078,"(nets and nett ) that compute in context representations of source (si ) and target words (tj ). L(src, tgt) = I X The model is composed of 2 Bi-directional LSTM subnetworks, nets and nett , which respectively encode source and target sentences. Since both nets and nett take the same form we describe + i=1 J X j=1 935   src log 1 + eaggrs (i,S)∗Yi +   tgt log 1 + eaggrt (j,S)∗Yj (3) where Yisrc and Yjtgt are vectors with reference labels containing −1 when the word is present in the translated sentence, and +1 for divergent (unpaired) words. Further details on the network can be found in (Pham et al., 2018). 3.1 most frequent words of each language are used as vocabulary. Each out-of-vocabulary word is mapped to a special UNK token. Word embeddings (LTs and LTt ) are initialised using fastText4 , further aligned by means of MUSE5 following the unsupervised method detailed in (Lample et al., 2018). Size of embeddings is Es = Et = 256 cells. Both Bi-LSTM use 256-dimensional hidden representations (E = 512). We use r = 1.0. Optimisation of the parameters is done using the stochastic gradient descent method along with gradient clipping (rescaling gradients whose norm exceeds a threshold) to avoid th"
W98-0611,P98-2198,1,0.821724,"f the text-based a u t o m a t o n construction, is in the possibility to have immediate concordances. It is hardly thinkable parse sequentially a 10 million word corpus. Moreover, with many levels of aut o m a t a (sometimes more than 20): the size of the developed main a u t o m a t o n becomes quickly huge, that we cannot re-compute for each concordance. Thus, we have chosen to index each s u b - a u t o m a t o n independently, with a dependency graph (cf below) like makefile, we only re-compute, the modified graph, and those depending on it. T h e index parsing algorithm is described in (Senellart, 1998a). It allows us, to obtain concordances on the whole corpus, in a mean time less than ls on a average personal computer. 3. C o n c o r d a n c e m a n a g e r . We have shown during all along this paper: that the way the concordance are sorted has a great importance. Under I N T E X , we can sort concordances according to the recognized sequence, to the right, or left context, and any combination of the three parameters. Moreover, when we put variables in the automaton, we must are able to validate recognized sequence linked to the variable in 84 6 Conclusion T h e simplicity with which we c"
W98-0611,C98-2193,1,\N,Missing
