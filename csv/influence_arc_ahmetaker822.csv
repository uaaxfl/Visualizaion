aker-etal-2012-assessing,W10-1806,0,\N,Missing
aker-etal-2012-assessing,D08-1027,0,\N,Missing
aker-etal-2012-assessing,J11-2010,0,\N,Missing
aker-etal-2012-assessing,P08-1080,0,\N,Missing
aker-etal-2012-assessing,P11-1122,0,\N,Missing
aker-etal-2012-assessing,kaisser-lowe-2008-creating,0,\N,Missing
aker-etal-2012-light,W02-1037,0,\N,Missing
aker-etal-2012-light,D09-1040,0,\N,Missing
aker-etal-2012-light,C04-1138,0,\N,Missing
aker-etal-2012-light,P99-1067,0,\N,Missing
aker-etal-2012-light,P01-1008,0,\N,Missing
aker-etal-2012-light,J05-4003,0,\N,Missing
aker-etal-2012-light,R11-1106,0,\N,Missing
aker-etal-2012-light,P06-2095,0,\N,Missing
aker-etal-2012-light,N06-1058,0,\N,Missing
aker-etal-2012-light,P06-1144,0,\N,Missing
aker-etal-2012-light,P06-1011,0,\N,Missing
aker-etal-2012-light,P99-1068,0,\N,Missing
aker-etal-2012-light,N06-1003,0,\N,Missing
aker-etal-2012-light,P08-1116,0,\N,Missing
aker-etal-2012-light,C10-2054,0,\N,Missing
aker-etal-2012-light,N04-1034,0,\N,Missing
aker-etal-2014-bilingual,steinberger-etal-2012-dgt,0,\N,Missing
aker-etal-2014-bilingual,C00-2163,0,\N,Missing
aker-etal-2014-bilingual,P07-1108,0,\N,Missing
aker-etal-2014-bilingual,P06-1011,0,\N,Missing
aker-etal-2014-bilingual,J03-1002,0,\N,Missing
aker-etal-2014-bilingual,P09-1018,0,\N,Missing
aker-etal-2014-bilingual,W13-2502,0,\N,Missing
aker-etal-2014-bilingual,R13-1074,1,\N,Missing
aker-etal-2014-bilingual,C12-2003,1,\N,Missing
aker-etal-2014-bilingual,P13-1040,1,\N,Missing
aker-etal-2014-bootstrapping,J93-2004,0,\N,Missing
aker-etal-2014-bootstrapping,steinberger-etal-2012-dgt,0,\N,Missing
aker-etal-2014-bootstrapping,C00-2163,0,\N,Missing
aker-etal-2014-bootstrapping,H01-1035,0,\N,Missing
aker-etal-2014-bootstrapping,E03-1035,0,\N,Missing
aker-etal-2014-bootstrapping,petrov-etal-2012-universal,0,\N,Missing
aker-etal-2014-bootstrapping,N03-1028,0,\N,Missing
aker-etal-2014-bootstrapping,J03-1002,0,\N,Missing
aker-etal-2014-bootstrapping,P13-1040,1,\N,Missing
aker-etal-2014-bootstrapping,gimenez-marquez-2004-svmtool,0,\N,Missing
aker-etal-2017-extensible,P13-4021,0,\N,Missing
aker-etal-2017-simple,P12-1091,0,\N,Missing
aker-etal-2017-simple,D14-1082,0,\N,Missing
aker-etal-2017-simple,P14-1023,0,\N,Missing
aker-etal-2017-simple,D13-1170,0,\N,Missing
aker-etal-2017-simple,S16-1003,0,\N,Missing
aker-etal-2017-simple,P16-2064,1,\N,Missing
aker-etal-2017-simple,S17-2087,0,\N,Missing
aker-etal-2017-simple,S17-2085,0,\N,Missing
aker-etal-2017-simple,S17-2080,0,\N,Missing
aker-etal-2017-simple,S17-2081,0,\N,Missing
aker-etal-2017-simple,D11-1147,0,\N,Missing
aker-gaizauskas-2010-model,W08-1407,1,\N,Missing
aker-gaizauskas-2010-model,P07-1126,0,\N,Missing
aker-gaizauskas-2010-model,W04-1013,0,\N,Missing
aker-gaizauskas-2010-model,P08-1032,0,\N,Missing
aker-gaizauskas-2010-model,R09-1002,1,\N,Missing
C12-2003,aker-etal-2012-light,1,0.808146,"repare the parallel phrases used to train and test the SVM classifier. For each language pair we split the corpus into two parts: a training set and a test set. The test set contains 10K parallel sentences. The training set contains 99K sentences for EN-DE, 423K for EN-EL and 53K sentences for EN-LV. 4.1.2 Comparable Corpora We used comparable corpora in English-Greek, English-Latvian and English-German language pairs. These corpora were collected from news articles using a light weight approach that only compares titles and date of publication of two articles to judge them for comparability (Aker et al., 2012). The corpora are aligned at the document level and are detailed in Table 1. language pair EN-DE EN-EL EN-LV document pairs 66K 122K 87K EN sentences 623K 1600K 1122K target sentences 533K 313K 285K EN words 14837K 27300K 18704K target words 6769K 8258K 5356K Table 1: Size of comparable corpora. 4.2 Phrase Extraction for Classifier Training and Testing On both parallel training and testing data sets (see Section 4.1.1) we separately applied GIZA++ to obtain the word alignment information used in our parallel phrase extraction method (see Section 2.1). Then we ran the training example extractio"
C12-2003,aswani-gaizauskas-2010-english,1,0.688491,"rposes Dictionaries mostly fail to return translation entries for named entities (NEs) or specialized terminology. Because of this we also use cognate-based methods to perform the mapping between source and target words or vice versa. We only apply the cognate-based methods for the firstWordTranslationScore and lastWordTranslationScore features. For these two features it is easy to compare the first or the last words from both the source and target phrases. The score of the cognate methods becomes the translation score for the features. We adopt several string similarity measures described in Aswani and Gaizauskas (2010): (1) Longest Common Subsequence Ratio, (2) Longest Common Substring, (3) Dice Similarity, (4) Needleman-Wunsch Distance and (5) Levenshtein Distance. Each of these measures returns a score between 0 and 1. We use a weighted linear combination of the scores to compute the final score. We learn the weights using linear regression over training data consisting of pairs of truely and falsely aligned city names available from Wikipedia1 . For the truely aligned named entities we assign a score of 1 and for the falsely aligned ones a score of 0. We take the cognate similarity score as the translati"
C12-2003,P01-1008,0,0.076278,"es or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target la"
C12-2003,N06-1003,0,0.0261749,"re not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible"
C12-2003,P05-1033,0,0.119284,"e pairs are generated from arbitrary source and target language sentence pairs. 24 2.1 Training Example Extraction We use whatever parallel data is available for a language pair to extract training examples for the SVM classifier. To get positive training examples (parallel phrases), we first align the parallel sentence pairs using the Giza++ toolkit (Och and Ney, 2000, 2003) in both directions and then refine the alignments using a “grow-diag-final-and” strategy. Then, we extract all phrases, as defined in the statistical machine translation literature (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005), and take these phrases as positive examples. j Let S denote a sentence, Si the i-th word in S and Si the subsequence of words in S from j j0 position i to j. Given a word-aligned sentence pair 〈S, T 〉, 〈Si , Ti 0 〉 is a phrase iff: • Sk is aligned to Tk0 for some k ∈ [i, j] and k0 ∈ [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ [i, j] and k0 ∈ / [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ / [i, j] and k0 ∈ [i 0 , j 0 ] To get negative training examples (non-parallel phrases), for each sentence pair, we enumerate all segments on the source side and on the target side, the length"
C12-2003,W11-1209,0,0.0789101,"is an under-resourced language, while for Greek and German text resources are more readily available. Considering all three languages allows us to directly compare our method’s performance on resource-rich and under-resourced languages. We perform two different tests. First, we evaluate the performance of the classifier on phrases extracted from held-out parallel data using standard measures such as recall, precision and accuracy. Secondly, we test whether the phrases extracted by our method from comparable corpora lead to improved SMT quality, as measured using BLEU (Papineni et al., 2002) . Hewavitharana and Vogel (2011) also adopt a classification approach for phrase extraction. However, their approach requires manual intervention in data preparation, whereas we perform the preparation of training and testing data fully automatically. In addition, Hewavitharana and Vogel (2011) do not report any SMT performance evaluation of their approach, so it is difficult to estimate how useful their approach is for the actual task it is meant to improve. We test the impact of our extracted phrases on the performance of an SMT system, which allows us to draw conclusions about the likely utility of our approach for SMT in"
C12-2003,ion-2012-pexacc,0,0.0248044,"better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S and for T and then compute all possible phrase pairings co"
C12-2003,N06-1058,0,0.0195444,"ficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we"
C12-2003,W04-3250,0,0.194912,"Missing"
C12-2003,N03-1017,0,0.0331396,"nguages. During testing candidate phrase pairs are generated from arbitrary source and target language sentence pairs. 24 2.1 Training Example Extraction We use whatever parallel data is available for a language pair to extract training examples for the SVM classifier. To get positive training examples (parallel phrases), we first align the parallel sentence pairs using the Giza++ toolkit (Och and Ney, 2000, 2003) in both directions and then refine the alignments using a “grow-diag-final-and” strategy. Then, we extract all phrases, as defined in the statistical machine translation literature (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005), and take these phrases as positive examples. j Let S denote a sentence, Si the i-th word in S and Si the subsequence of words in S from j j0 position i to j. Given a word-aligned sentence pair 〈S, T 〉, 〈Si , Ti 0 〉 is a phrase iff: • Sk is aligned to Tk0 for some k ∈ [i, j] and k0 ∈ [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ [i, j] and k0 ∈ / [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ / [i, j] and k0 ∈ [i 0 , j 0 ] To get negative training examples (non-parallel phrases), for each sentence pair, we enumerate all segments on the source side"
C12-2003,2007.tmi-papers.12,0,0.0404297,"cases, such as for under-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach."
C12-2003,W02-1018,0,0.0390414,"nder-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase"
C12-2003,D09-1040,0,0.0161754,"ystems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S and for T and then co"
C12-2003,J05-4003,0,0.0603772,"Missing"
C12-2003,P06-1011,0,0.106121,"h parallel resources (corpora). However, in many cases, such as for under-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is"
C12-2003,J04-4002,0,0.0446812,"ing candidate phrase pairs are generated from arbitrary source and target language sentence pairs. 24 2.1 Training Example Extraction We use whatever parallel data is available for a language pair to extract training examples for the SVM classifier. To get positive training examples (parallel phrases), we first align the parallel sentence pairs using the Giza++ toolkit (Och and Ney, 2000, 2003) in both directions and then refine the alignments using a “grow-diag-final-and” strategy. Then, we extract all phrases, as defined in the statistical machine translation literature (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005), and take these phrases as positive examples. j Let S denote a sentence, Si the i-th word in S and Si the subsequence of words in S from j j0 position i to j. Given a word-aligned sentence pair 〈S, T 〉, 〈Si , Ti 0 〉 is a phrase iff: • Sk is aligned to Tk0 for some k ∈ [i, j] and k0 ∈ [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ [i, j] and k0 ∈ / [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ / [i, j] and k0 ∈ [i 0 , j 0 ] To get negative training examples (non-parallel phrases), for each sentence pair, we enumerate all segments on the source side and on the target s"
C12-2003,C00-2163,0,0.75263,"etween parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S and for T and then compute all possible phrase pairings consisting of one phrase from S and one phrase from T. In the test phase we use a binary SVM classifier to determine for each generated phrase pair whether it is or is not parallel. The SVM classifier is trained using phrase pairs taken from parallel data word aligned using Giza++ (Och and Ney, 2000, 2003). We have tested our approach on the English-German, English-Greek and English-Latvian language pairs. Latvian is an under-resourced language, while for Greek and German text resources are more readily available. Considering all three languages allows us to directly compare our method’s performance on resource-rich and under-resourced languages. We perform two different tests. First, we evaluate the performance of the classifier on phrases extracted from held-out parallel data using standard measures such as recall, precision and accuracy. Secondly, we test whether the phrases extracted"
C12-2003,J03-1002,0,0.0125168,"Missing"
C12-2003,P02-1040,0,0.0910989,"language pairs. Latvian is an under-resourced language, while for Greek and German text resources are more readily available. Considering all three languages allows us to directly compare our method’s performance on resource-rich and under-resourced languages. We perform two different tests. First, we evaluate the performance of the classifier on phrases extracted from held-out parallel data using standard measures such as recall, precision and accuracy. Secondly, we test whether the phrases extracted by our method from comparable corpora lead to improved SMT quality, as measured using BLEU (Papineni et al., 2002) . Hewavitharana and Vogel (2011) also adopt a classification approach for phrase extraction. However, their approach requires manual intervention in data preparation, whereas we perform the preparation of training and testing data fully automatically. In addition, Hewavitharana and Vogel (2011) do not report any SMT performance evaluation of their approach, so it is difficult to estimate how useful their approach is for the actual task it is meant to improve. We test the impact of our extracted phrases on the performance of an SMT system, which allows us to draw conclusions about the likely u"
C12-2003,P06-2095,0,0.0256329,"ora). However, in many cases, such as for under-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generat"
C12-2003,skadina-etal-2012-collecting,1,0.887104,"Missing"
C12-2003,P08-1116,0,0.0191513,"chine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S"
C18-1284,aker-etal-2017-simple,1,0.861026,"1 As described in Section 2, prior work on tweet veracity classification investigated a wide range of features and classification methods. Most influential is the feature set proposed by Castillo et al. (2011), which we have adopted for training a classification model. We experimented with various classifiers such as simple decision trees, k-nn, etc. but achieved best performance with Random Forests, more precisely, we use a range of 33 different features varying from syntactical, semantic, indicator, user-specific and message-specific categories. Details of our features can be obtained from (Aker et al., 2017b). 5.2 Stance Aware Baseline: B2 Following the approach of Liu et al. (2015), we integrate stance as several features, additional to those used in the first baseline. Since there are four different stance classes, the following additional feature(s) are used: 3364 Relative-Stance-Score Percentage of supporting, denying, questioning, and commenting stances extracted from tweets within a rumour. To obtain this feature we count e.g. how many individual tweets express support for the rumour and divide it by the total number of tweets. We have in total four features, one of each class. Similar to"
C18-1284,S17-2082,0,0.50435,"in the stance distribution over time. Therefore, after training models for true and false rumours, we can build a binary veracity classifier by comparing sequence occurrence probabilities for the two cases. This paper investigates whether and to what extent rumour veracity classification can be predicted on the basis of crowd stance. While there is a body of work on automatic veracity classification such as (Castillo et al., 2011; Kwon et al., 2013; Vosoughi, 2015; Wu et al., 2015; Ma et al., 2015; Lukasik et al., 2016), only few have applied stance information as a feature (Liu et al., 2015; Enayet and El-Beltagy, 2017), and none of these studies investigated the collective power of the crowd as a source of stance information, which can then be used as a feature in rumour veracity classification. Here we argue that the content of the posts is not necessarily helpful towards determining the veracity of a story, as the content can be either misleading or inaccurate. We believe it is the aggregation of stances which can provide useful information to determine the veracity. Despite the fact that some of the users will be inevitably mistaken, and sharing the wrong stance, we argue that the aggregation of stances"
C18-1284,D15-1311,1,0.856867,"t of the rumour until a replying tweet occurred. Accordingly, observation alphabet of MSHMM λ0 is defined as a random vector o = (X, x), where X is a space index (i.e. a stance) and x ∈ R1 the processed timestamp. Multi spaced observation probability of o is defined as X b(o) = wg Ng (x) (8) g∈X Additionally, state set S, state transition matrix A and starting state probability vector π of system λ0 are defined analogue to system λ. 5 Evaluation Settings To evaluate the performance of the classification framework leave-one-event-out cross validation was performed, following the methodology of Lukasik et al. (2015). This means that we train on n − 1 events (on all rumours within these events) and test it on an unseen nth event (on all rumours within this event). We use F1 score to measure classifier performance and compare our results against two baselines. The first one does not make use of stance, whereas the second one integrates stance as one of a set of features. These two baselines have been selected to simulate the impact of collective stance i.e., that collective stance have positive impact on rich traditional feature engineered approaches. However, our results (see Section 6) show that it is im"
C18-1284,P16-2064,1,0.900532,"Missing"
C18-1284,D11-1147,0,0.668644,"Missing"
C18-1284,S17-2087,0,0.0357009,"The algorithm aimed at producing positive seeds to be shown to users first. 3361 Rumour veracity classification has also been studied in the RumourEval shared task at SemEval 2017 (Derczynski et al., 2017a). Subtask B consisted in determining if each of the rumours in the dataset were true, false or remained unverified. It considered two different settings, one closed where participants could not make use of external knowledge bases and another open where use of external resources was allowed. Participants viewed the task either as a three-way (Enayet and El-Beltagy, 2017; Wang et al., 2017; Singh et al., 2017) or two-way (Chen et al., 2017; Srivastava et al., 2017), single tweet classification task. The winning system (Enayet and El-Beltagy, 2017) added features more specific to the distribution of stance labels in the tweets replying to the source tweet (percentage of reply tweets classified as either support, deny or query). The survey paper of Zubiaga et al. (2018) provides an extensive summary of current work on rumour verification but also related task such as detection of rumours as well as stance classification of messages involved in rumours. In this work we propose to use stance only to ta"
C18-1284,S17-2085,0,0.0369485,"be shown to users first. 3361 Rumour veracity classification has also been studied in the RumourEval shared task at SemEval 2017 (Derczynski et al., 2017a). Subtask B consisted in determining if each of the rumours in the dataset were true, false or remained unverified. It considered two different settings, one closed where participants could not make use of external knowledge bases and another open where use of external resources was allowed. Participants viewed the task either as a three-way (Enayet and El-Beltagy, 2017; Wang et al., 2017; Singh et al., 2017) or two-way (Chen et al., 2017; Srivastava et al., 2017), single tweet classification task. The winning system (Enayet and El-Beltagy, 2017) added features more specific to the distribution of stance labels in the tweets replying to the source tweet (percentage of reply tweets classified as either support, deny or query). The survey paper of Zubiaga et al. (2018) provides an extensive summary of current work on rumour verification but also related task such as detection of rumours as well as stance classification of messages involved in rumours. In this work we propose to use stance only to tackle the rumour verification task. As highlighted earlie"
C18-1284,S17-2086,0,0.0415238,"m to block rumours. The algorithm aimed at producing positive seeds to be shown to users first. 3361 Rumour veracity classification has also been studied in the RumourEval shared task at SemEval 2017 (Derczynski et al., 2017a). Subtask B consisted in determining if each of the rumours in the dataset were true, false or remained unverified. It considered two different settings, one closed where participants could not make use of external knowledge bases and another open where use of external resources was allowed. Participants viewed the task either as a three-way (Enayet and El-Beltagy, 2017; Wang et al., 2017; Singh et al., 2017) or two-way (Chen et al., 2017; Srivastava et al., 2017), single tweet classification task. The winning system (Enayet and El-Beltagy, 2017) added features more specific to the distribution of stance labels in the tweets replying to the source tweet (percentage of reply tweets classified as either support, deny or query). The survey paper of Zubiaga et al. (2018) provides an extensive summary of current work on rumour verification but also related task such as detection of rumours as well as stance classification of messages involved in rumours. In this work we propose to"
D10-1047,R09-1002,1,0.933025,"asured with the final parameters after training to optimise ROUGE-2 with the three different heuristics and expanding five nodes in each step. score, R, and the loss is simply 1 - R. The training problem is to solve ˆ = arg min ∆(ˆ λ y, r) , Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): ● (4) λ ˆ and r are where with a slight abuse of notation, y taken to range over the corpus of many documentsets and summaries. To optimise the weights we use the minimum error rate training (MERT) technique (Och, 2003), as used for training statistical machine translation systems. This approach is a first order optimization method using Powell search to find the parameters which minimise the loss on the training data. MERT requires n-best lists which it uses to approximate the full space of possible outcomes. We use the A* search algorithm to construct these n-b"
D10-1047,P10-1127,1,0.336084,"eters after training to optimise ROUGE-2 with the three different heuristics and expanding five nodes in each step. score, R, and the loss is simply 1 - R. The training problem is to solve ˆ = arg min ∆(ˆ λ y, r) , Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): ● (4) λ ˆ and r are where with a slight abuse of notation, y taken to range over the corpus of many documentsets and summaries. To optimise the weights we use the minimum error rate training (MERT) technique (Och, 2003), as used for training statistical machine translation systems. This approach is a first order optimization method using Powell search to find the parameters which minimise the loss on the training data. MERT requires n-best lists which it uses to approximate the full space of possible outcomes. We use the A* search algorithm to construct these n-best lists,5 and use MERT to"
D10-1047,aker-gaizauskas-2010-model,1,0.872562,"eters after training to optimise ROUGE-2 with the three different heuristics and expanding five nodes in each step. score, R, and the loss is simply 1 - R. The training problem is to solve ˆ = arg min ∆(ˆ λ y, r) , Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): ● (4) λ ˆ and r are where with a slight abuse of notation, y taken to range over the corpus of many documentsets and summaries. To optimise the weights we use the minimum error rate training (MERT) technique (Och, 2003), as used for training statistical machine translation systems. This approach is a first order optimization method using Powell search to find the parameters which minimise the loss on the training data. MERT requires n-best lists which it uses to approximate the full space of possible outcomes. We use the A* search algorithm to construct these n-best lists,5 and use MERT to"
D10-1047,W08-1405,0,0.0350931,"Missing"
D10-1047,J97-1003,0,0.104665,"oblem of finding the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a training set consisting of pairs of document sets with model or reference summaries – typically human authored extractive or abstractive summaries. Search is typically performed by a greedy algorithm which selects each sentence in decreasing order of model score until the desired summary length is reached (see, e.g., Saggion (2005)) or using heuristic strategies based on position in document or lexical clues (Edmundson, 1969; Brandow et al., 1995; Hearst, 1997; Ouyang et al., 2010).1 We show in this paper that the search problem can be solved optimally and efficiently using A* search (Russell et al., 1995). Assuming the model only uses features local to each sentence in the summary, our algorithm finds the best scoring extractive summary up to a given length in words. Framing summarization as search suggests that many of the popular training techniques are maximising the wrong objective. These approaches train a classifier, regression or ranking model to distinguish between good and bad sentences under an evaluation metric, e.g., ROUGE (Lin, 2004)."
D10-1047,W01-0100,0,0.699137,"ve training algorithm which directly maximises the quality of the best summary, rather than assuming a sentence-level decomposition as in earlier work. Our approach leads to significantly better results than earlier techniques across a number of evaluation metrics. 1 Introduction Multi-document summarization aims to present multiple documents in form of a short summary. This short summary can be used as a replacement for the original documents to reduce, for instance, the time a reader would spend if she were to read the original documents. Following dominant trends in summarization research (Mani, 2001), we focus solely on extractive summarization which simplifies the summarization task to the problem of identifying a subset of units from the document collection (here sentences) which are concatenated to form the summary. Most multi-document summarization systems define a model which assigns a score to a candidate summary based on the features of the sentences included in the summary. The research challenges are then twofold: 1) the search problem of finding the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a trai"
D10-1047,P03-1021,0,0.0638436,"nd -SU4. The paper is structured as follows. Section 2 presents the summarization model. Next in section 3 we present an A* search algorithm for finding the best scoring (argmax) summary under the model with a constraint on the maximum summary length. We show that this algorithm performs search efficiently, even for very large document sets composed of many sentences. The second contribution of the paper is a new training method which directly optimises the summarization system, and is presented in section 4. This uses the minimum error-rate training (MERT) technique from machine translation (Och, 2003) to optimise the summariser’s output to an arbitrary evaluation metric. Section 5 describes our experimental setup and section 6 the results. Finally we conclude in section 7. 2 Summarization Model Extractive multi-document summarization aims to find the most important sentences from a set of documents, which are then collated and presented to the user in form of a short summary. Following the predominant approach to data-driven summarisation, we define a linear model which scores summaries as the weighted sum of their features, s(y|x) = Φ(x, y) · λ , (1) where x is the document set, composed"
D10-1047,W04-1013,0,\N,Missing
kurtic-etal-2012-corpus,torreira-ernestus-2010-nijmegen,0,\N,Missing
kurtic-etal-2012-corpus,sikveland-etal-2010-spontal,0,\N,Missing
kurtic-etal-2012-corpus,ernestus-etal-2014-nijmegen,0,\N,Missing
kurtic-etal-2012-corpus,wittenburg-etal-2006-elan,0,\N,Missing
L16-1494,W15-4631,0,0.150136,"ary of reader comments. Furthermore, the evaluations proposed so far, despite in several cases being called user studies, are not task-based evaluations that might let us understand how well systems are meeting user needs. A different, but promising, line of work, not yet deployed in summarization systems, is that on argument mining. Much of reader comment is argumentative and one appealing type of summary is one that would summarise the main points of contention in comments, something it is not clear an extractive summary could do. Work by e.g. Ghosh et al., (2014) Habernal et al. (2014) and Swanson et al. (2015) amongst others, focuses on defining and identifying key argumentative units and their relations. They mention summarization of argumentative texts as one potential application of their work. However, they do not specify what an end-user summary of reader comment on news might be like. In this paper we make three contributions to advancing work in this area. First, we offer a specification of one possible summary type for reader comment based on the notions of viewpoint and issue, which we define below (Section 2.). Second, we propose a task-based evaluation framework in which users are offere"
L16-1494,W14-2106,0,\N,Missing
L16-1663,C10-2054,0,0.0319886,"e translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2008, Marton et al., 2009, Aker et al., 2012) Without a doubt the web is the largest source that can be used to gather comparable corpora (Resnik and Smith, 2003, Huang et al., 2010). Newswire have been also explored to gather comparable corpora. The ACCURAT1 project, for instance, collected comparable corpora for 12 different European languages using the web, through Wikipedia and News articles (Skadiņa et al., 2012a, Skadiņa et al., 2012b). A follow up project, TaaS2, expanded this idea to all EU languages. In both projects it has been shown that comparable corpora have positive impact on SMT as well as in creation bilingual terminology resources. Apart from those mentioned studies and projects there have been many workshops focusing solely on building and using compara"
L16-1663,N06-1058,0,0.0351334,"collected from the United Nation or the European parliament and are mostly about the legal domain (Koehn, 2005). However, often parallel resources are not readily available for underresourced languages or other specific narrow domains. This leads to under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2008, Marton et al., 2009, Aker et al., 2012) Without a doubt the web is the largest source that can be used to gather comparable corpora (Resnik and Smith, 2003, Huang et al., 2010). Newswire have been also explored to gather comparable corpora. The ACCURAT1 project, for instance, collected comparable corpora for 12 different European languages using the web, through Wikipedia and News articles (Skadiņa et al., 2012a, Skadiņa et al., 2012b). A follow up project, TaaS2, expanded this idea to all EU languages."
L16-1663,2005.mtsummit-papers.11,0,0.0796689,"nguage, comparable corpora that involve more than one language can be created, e.g. English- Arabic - Persian, English - Arabic - Urdu, English – Urdu - Persian, etc. Upon request the data can be provided for research purposes. Keywords: Comparable Corpora for Arabic, Urdu, Persian and English 1. Paper Statistical Machine Translation (SMT) relies heavily on the quality and quantity of bilingual parallel corpora (Brown et al., 1993, Och and Ney, 2002, Koehn, 2010). Most known parallel corpora are collected from the United Nation or the European parliament and are mostly about the legal domain (Koehn, 2005). However, often parallel resources are not readily available for underresourced languages or other specific narrow domains. This leads to under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2"
L16-1663,J10-4005,0,0.0195026,"ges. The data has been collected over a period of a year, entails Arabic, Persian and Urdu languages. Furthermore using the English as a pivot language, comparable corpora that involve more than one language can be created, e.g. English- Arabic - Persian, English - Arabic - Urdu, English – Urdu - Persian, etc. Upon request the data can be provided for research purposes. Keywords: Comparable Corpora for Arabic, Urdu, Persian and English 1. Paper Statistical Machine Translation (SMT) relies heavily on the quality and quantity of bilingual parallel corpora (Brown et al., 1993, Och and Ney, 2002, Koehn, 2010). Most known parallel corpora are collected from the United Nation or the European parliament and are mostly about the legal domain (Koehn, 2005). However, often parallel resources are not readily available for underresourced languages or other specific narrow domains. This leads to under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006, Munteanu and M"
L16-1663,2007.tmi-papers.12,0,0.0384404,"parallel corpora are collected from the United Nation or the European parliament and are mostly about the legal domain (Koehn, 2005). However, often parallel resources are not readily available for underresourced languages or other specific narrow domains. This leads to under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2008, Marton et al., 2009, Aker et al., 2012) Without a doubt the web is the largest source that can be used to gather comparable corpora (Resnik and Smith, 2003, Huang et al., 2010). Newswire have been also explored to gather comparable corpora. The ACCURAT1 project, for instance, collected comparable corpora for 12 different European languages using the web, through Wikipedia and News articles (Skadiņa et al., 2012a, Skadiņa et al., 2012b). A follow up project, TaaS2, expanded thi"
L16-1663,D09-1040,0,0.0572156,"Missing"
L16-1663,W02-1037,0,0.0300802,"l corpora (Brown et al., 1993, Och and Ney, 2002, Koehn, 2010). Most known parallel corpora are collected from the United Nation or the European parliament and are mostly about the legal domain (Koehn, 2005). However, often parallel resources are not readily available for underresourced languages or other specific narrow domains. This leads to under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2008, Marton et al., 2009, Aker et al., 2012) Without a doubt the web is the largest source that can be used to gather comparable corpora (Resnik and Smith, 2003, Huang et al., 2010). Newswire have been also explored to gather comparable corpora. The ACCURAT1 project, for instance, collected comparable corpora for 12 different European languages using the web, through Wikipedia and News articles (Skadiņa et al., 20"
L16-1663,J05-4003,0,0.0571274,"creasing the number of weeks to extract SMT relevant data.  Analyse how similar events are reported in various languages.  Analyse how topics evolve within a language over the time as well as cross-lingually. To the best of our knowledge such data that has been collected over a period of a year, is big in volume, entails Arabic, Persian and Urdu languages as well as the settings enabling researchers to perform different analyses have not been reported by earlier studies. 4193 2. Related Work There have been number of studies in literature that reported comparable corpora for English-Arabic (Munteanu and Marcu, 2005, Abdul-Rauf and Schwenk, 2009). The most recent comparable corpora that was built in Arabic, was by (Saad et al., 2013). For the Persian the most commonly known corpora was collected by Hashemi et al. (2010) and Hashemi and Shakery (2014). In regards to the Urdu language we are not aware of any comparable corpora. Unlike related work we report comparable corpora covering not only Arabic and Persian but also Urdu. Furthermore, unlike related studies the genre from which our data is obtained is the same in all data sets, namely news. Because of this feature our data can be split into different"
L16-1663,P02-1038,0,0.220579,"rsian, Urdu} languages. The data has been collected over a period of a year, entails Arabic, Persian and Urdu languages. Furthermore using the English as a pivot language, comparable corpora that involve more than one language can be created, e.g. English- Arabic - Persian, English - Arabic - Urdu, English – Urdu - Persian, etc. Upon request the data can be provided for research purposes. Keywords: Comparable Corpora for Arabic, Urdu, Persian and English 1. Paper Statistical Machine Translation (SMT) relies heavily on the quality and quantity of bilingual parallel corpora (Brown et al., 1993, Och and Ney, 2002, Koehn, 2010). Most known parallel corpora are collected from the United Nation or the European parliament and are mostly about the legal domain (Koehn, 2005). However, often parallel resources are not readily available for underresourced languages or other specific narrow domains. This leads to under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006,"
L16-1663,P99-1067,0,0.23308,"gual parallel corpora (Brown et al., 1993, Och and Ney, 2002, Koehn, 2010). Most known parallel corpora are collected from the United Nation or the European parliament and are mostly about the legal domain (Koehn, 2005). However, often parallel resources are not readily available for underresourced languages or other specific narrow domains. This leads to under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2008, Marton et al., 2009, Aker et al., 2012) Without a doubt the web is the largest source that can be used to gather comparable corpora (Resnik and Smith, 2003, Huang et al., 2010). Newswire have been also explored to gather comparable corpora. The ACCURAT1 project, for instance, collected comparable corpora for 12 different European languages using the web, through Wikipedia and News ar"
L16-1663,J03-3002,0,0.0473581,"under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2008, Marton et al., 2009, Aker et al., 2012) Without a doubt the web is the largest source that can be used to gather comparable corpora (Resnik and Smith, 2003, Huang et al., 2010). Newswire have been also explored to gather comparable corpora. The ACCURAT1 project, for instance, collected comparable corpora for 12 different European languages using the web, through Wikipedia and News articles (Skadiņa et al., 2012a, Skadiņa et al., 2012b). A follow up project, TaaS2, expanded this idea to all EU languages. In both projects it has been shown that comparable corpora have positive impact on SMT as well as in creation bilingual terminology resources. Apart from those mentioned studies and projects there have been many workshops focusing solely on build"
L16-1663,P06-2095,0,0.0422066,"993, Och and Ney, 2002, Koehn, 2010). Most known parallel corpora are collected from the United Nation or the European parliament and are mostly about the legal domain (Koehn, 2005). However, often parallel resources are not readily available for underresourced languages or other specific narrow domains. This leads to under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2008, Marton et al., 2009, Aker et al., 2012) Without a doubt the web is the largest source that can be used to gather comparable corpora (Resnik and Smith, 2003, Huang et al., 2010). Newswire have been also explored to gather comparable corpora. The ACCURAT1 project, for instance, collected comparable corpora for 12 different European languages using the web, through Wikipedia and News articles (Skadiņa et al., 2012a, Skadiņa et al., 2"
L16-1663,skadina-etal-2012-collecting,1,0.854839,"nu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2008, Marton et al., 2009, Aker et al., 2012) Without a doubt the web is the largest source that can be used to gather comparable corpora (Resnik and Smith, 2003, Huang et al., 2010). Newswire have been also explored to gather comparable corpora. The ACCURAT1 project, for instance, collected comparable corpora for 12 different European languages using the web, through Wikipedia and News articles (Skadiņa et al., 2012a, Skadiņa et al., 2012b). A follow up project, TaaS2, expanded this idea to all EU languages. In both projects it has been shown that comparable corpora have positive impact on SMT as well as in creation bilingual terminology resources. Apart from those mentioned studies and projects there have been many workshops focusing solely on building and using comparable corpora (BUCC3). The amount and diverse studies and efforts show that 1 http://www.accurat-project.eu/ 2 http://www.taas-project.eu/ 3 https://comparable.limsi.fr/ comparable corpora is certainly a useful resource to determine helpful"
L16-1663,2012.eamt-1.49,0,0.144173,"nu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2008, Marton et al., 2009, Aker et al., 2012) Without a doubt the web is the largest source that can be used to gather comparable corpora (Resnik and Smith, 2003, Huang et al., 2010). Newswire have been also explored to gather comparable corpora. The ACCURAT1 project, for instance, collected comparable corpora for 12 different European languages using the web, through Wikipedia and News articles (Skadiņa et al., 2012a, Skadiņa et al., 2012b). A follow up project, TaaS2, expanded this idea to all EU languages. In both projects it has been shown that comparable corpora have positive impact on SMT as well as in creation bilingual terminology resources. Apart from those mentioned studies and projects there have been many workshops focusing solely on building and using comparable corpora (BUCC3). The amount and diverse studies and efforts show that 1 http://www.accurat-project.eu/ 2 http://www.taas-project.eu/ 3 https://comparable.limsi.fr/ comparable corpora is certainly a useful resource to determine helpful"
L16-1663,P08-1116,0,0.0264052,"(Koehn, 2005). However, often parallel resources are not readily available for underresourced languages or other specific narrow domains. This leads to under-performing machine translation systems in those sparse data settings. To overcome the low availability of parallel resources the machine translation community has recognized the potential of using comparable resources as training data (Rapp, 1999, Munteanu and Marcu, 2002, Sharoff et al., 2006, Munteanu and Marcu, 2006, Kumano et al., 2007, Kauchak and Barzilay, 2006, Callison-Burch et al., 2006, Barzilay and McKeown, 2001, Nakov, 2008, Zhao et al., 2008, Marton et al., 2009, Aker et al., 2012) Without a doubt the web is the largest source that can be used to gather comparable corpora (Resnik and Smith, 2003, Huang et al., 2010). Newswire have been also explored to gather comparable corpora. The ACCURAT1 project, for instance, collected comparable corpora for 12 different European languages using the web, through Wikipedia and News articles (Skadiņa et al., 2012a, Skadiņa et al., 2012b). A follow up project, TaaS2, expanded this idea to all EU languages. In both projects it has been shown that comparable corpora have positive impact on SMT as"
L18-1617,W15-0503,0,0.0511938,"umes of textual data has the potential to revolutionarise our access to information. Argument based search for information would for example facilitate individual and organisational decision-making, make learning more efficient, enable quicker reporting on present and past events, to name just a few broad applications. Even more important is argument mining in the multi-lingual context, by which argument based retrieval would be available to people in the language of their preference. Current studies report methods for argument mining in legal documents (Reed et al., 2008), persuasive essays (Nguyen and Litman, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015), discussion fora (Swanson et al., 2015), political debates (Lippi and Torroni, 2016a) and news (Sardianos et al., 2015; Al-Khatib et al., 2016). In terms of methodology, supervised machine learning is a central technique used in all these studies. This assumes the availability of data sets – argumentative texts – to train and test the argument mining models. Such data sets are readily available in English and – although in comparably smaller quantities – in very few European languages such as German or Italian. Languages other than"
L18-1617,reed-etal-2008-language,0,0.0399533,"6b). Identifying arguments in large volumes of textual data has the potential to revolutionarise our access to information. Argument based search for information would for example facilitate individual and organisational decision-making, make learning more efficient, enable quicker reporting on present and past events, to name just a few broad applications. Even more important is argument mining in the multi-lingual context, by which argument based retrieval would be available to people in the language of their preference. Current studies report methods for argument mining in legal documents (Reed et al., 2008), persuasive essays (Nguyen and Litman, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015), discussion fora (Swanson et al., 2015), political debates (Lippi and Torroni, 2016a) and news (Sardianos et al., 2015; Al-Khatib et al., 2016). In terms of methodology, supervised machine learning is a central technique used in all these studies. This assumes the availability of data sets – argumentative texts – to train and test the argument mining models. Such data sets are readily available in English and – although in comparably smaller quantities – in very few European languages suc"
L18-1617,D15-1050,0,0.171136,"ess to information. Argument based search for information would for example facilitate individual and organisational decision-making, make learning more efficient, enable quicker reporting on present and past events, to name just a few broad applications. Even more important is argument mining in the multi-lingual context, by which argument based retrieval would be available to people in the language of their preference. Current studies report methods for argument mining in legal documents (Reed et al., 2008), persuasive essays (Nguyen and Litman, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015), discussion fora (Swanson et al., 2015), political debates (Lippi and Torroni, 2016a) and news (Sardianos et al., 2015; Al-Khatib et al., 2016). In terms of methodology, supervised machine learning is a central technique used in all these studies. This assumes the availability of data sets – argumentative texts – to train and test the argument mining models. Such data sets are readily available in English and – although in comparably smaller quantities – in very few European languages such as German or Italian. Languages other than these are currently neglected. Due to this lack of data the r"
L18-1617,W15-0508,0,0.023043,"decision-making, make learning more efficient, enable quicker reporting on present and past events, to name just a few broad applications. Even more important is argument mining in the multi-lingual context, by which argument based retrieval would be available to people in the language of their preference. Current studies report methods for argument mining in legal documents (Reed et al., 2008), persuasive essays (Nguyen and Litman, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015), discussion fora (Swanson et al., 2015), political debates (Lippi and Torroni, 2016a) and news (Sardianos et al., 2015; Al-Khatib et al., 2016). In terms of methodology, supervised machine learning is a central technique used in all these studies. This assumes the availability of data sets – argumentative texts – to train and test the argument mining models. Such data sets are readily available in English and – although in comparably smaller quantities – in very few European languages such as German or Italian. Languages other than these are currently neglected. Due to this lack of data the research and development of argumentation mining outside English and few European languages is very limited, rendering m"
L18-1617,W15-4631,0,0.0225785,"ch for information would for example facilitate individual and organisational decision-making, make learning more efficient, enable quicker reporting on present and past events, to name just a few broad applications. Even more important is argument mining in the multi-lingual context, by which argument based retrieval would be available to people in the language of their preference. Current studies report methods for argument mining in legal documents (Reed et al., 2008), persuasive essays (Nguyen and Litman, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015), discussion fora (Swanson et al., 2015), political debates (Lippi and Torroni, 2016a) and news (Sardianos et al., 2015; Al-Khatib et al., 2016). In terms of methodology, supervised machine learning is a central technique used in all these studies. This assumes the availability of data sets – argumentative texts – to train and test the argument mining models. Such data sets are readily available in English and – although in comparably smaller quantities – in very few European languages such as German or Italian. Languages other than these are currently neglected. Due to this lack of data the research and development of argumentation"
P10-1127,R09-1002,1,0.572386,"ility of GPS (Global Position System) equipped cameras and phones, as well as by the widespread use of online social sites. The majority of these images are indexed with GPS coordinates (latitude and longitude) only and/or have minimal captions. This typically small amount of textual information associated with the image is of limited usefulness for image indexing, organization and search. Therefore methods which could automatically supplement the information available for image indexing and lead to improved image retrieval would be extremely useful. Following the general approach proposed by Aker and Gaizauskas (2009), in this paper we describe a method for automatic image captioning or caption enhancement starting with only a scene or subject type and a set of place names pertaining to an image – for example hchurch, {St. Paul’s,London}i. Scene type and place names can be obtained automatically given GPS coordinates and compass information using techniques such as those described in Xin et al. (2010) – that task is not the focus of this paper. Our method applies only to images of static features of the built or natural landscape, i.e. objects with persistent geo-coordinates, such as buildings and mountain"
P10-1127,aker-gaizauskas-2010-model,1,0.832858,"ute the values for the different features. This gives information about each feature’s value for each sentence. Then the ROUGE scores and feature score values for every sentence were input to the linear regression algorithm to train the weights. Given the weights, Equation 2 is used to compute the final score for each sentence. The final sentence scores are used to sort the sentences in the descending order. This sorted list is then used by the summarizer to generate the final summary as described in Aker and Gaizauskas (2009). Data sets For evaluation we use the image collection described in Aker and Gaizauskas (2010). The image collection contains 310 different images with manually assigned toponyms. The images cover 60 of the 107 object types identified from Wikipedia (see Table 2). For each image there are up to four short descriptions or model summaries. The model summaries were created manually based on image descriptions taken from VirtualTourist and contain a minimum of 190 and a maximum of 210 words. An example model summary about the Eiffel Tower is shown in Table 4. 23 of this image collection was used to train the weights and the remaining 31 (105 images) for evaluation. To generate automatic ca"
P10-1127,H05-1091,0,0.0274904,"20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1 coherence redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1 grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0 Dependency patterns have been exploited in various language processing applications. In information extraction, for instance, dependency patterns have been used to extract relevant information from text resources (Yangarber et al., 2000; Sudo et al., 2001; Culotta and Sorensen, 2004; Stevenson and Greenwood, 2005; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2009). However, dependency patterns have not been used extensively in summarization tasks. We are aware only of the work described in Nobata et al. (2002) who used dependency patterns in combination with other features to generate extracts in a single document summarization task. The authors found that when learning weights in a simple feature weigthing scheme, the weight assigned to dependency patterns was lower than that assigned to other features. The small contribution of the dependency patterns may have been due to the small number of documents they used to deri"
P10-1127,P04-1054,0,0.0522634,"1.2 6.7 5.7 4.0 10.2 6.0 0.5 2.6 3.3 focus 72.1 49.3 51.2 20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1 coherence redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1 grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0 Dependency patterns have been exploited in various language processing applications. In information extraction, for instance, dependency patterns have been used to extract relevant information from text resources (Yangarber et al., 2000; Sudo et al., 2001; Culotta and Sorensen, 2004; Stevenson and Greenwood, 2005; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2009). However, dependency patterns have not been used extensively in summarization tasks. We are aware only of the work described in Nobata et al. (2002) who used dependency patterns in combination with other features to generate extracts in a single document summarization task. The authors found that when learning weights in a simple feature weigthing scheme, the weight assigned to dependency patterns was lower than that assigned to other features. The small contribution of the dependency patterns may have be"
P10-1127,P07-1126,0,0.0269608,"tures the Wikipedia baseline summaries obtained better scores than our automated summaries. This comparison show that there is a gap to fill in order to obtain better readable summaries. 5 Related Work Our approach has an advantage over related work in automatic image captioning in that it requires only GPS information associated with the image in order to generate captions. Other attempts towards automatic generation of image captions generate captions based on the immediate textual context of the image with or without consideration of image related features such as colour, shape or texture (Deschacht and Moens, 2007; Mori et al., 2000; Barnard and Forsyth, 2001; Duygulu et al., 2002; Barnard et al., 2003; Pan et al., 2004; Feng and Lapata, 2008; Satoh et al., 1999; Berg et al., 2005). However, Marsch & White (2003) argue that the content of an image and its immediate text have little semantic agreement and this can, according to Purves et al. (2008), be misleading to image retrieval. Furthermore, these approaches assume that the image has been obtained from a document. In cases where there is no document associated with the image, which is the scenario we are principally concerned with, these techniques"
P10-1127,P08-1032,0,0.0102871,"fill in order to obtain better readable summaries. 5 Related Work Our approach has an advantage over related work in automatic image captioning in that it requires only GPS information associated with the image in order to generate captions. Other attempts towards automatic generation of image captions generate captions based on the immediate textual context of the image with or without consideration of image related features such as colour, shape or texture (Deschacht and Moens, 2007; Mori et al., 2000; Barnard and Forsyth, 2001; Duygulu et al., 2002; Barnard et al., 2003; Pan et al., 2004; Feng and Lapata, 2008; Satoh et al., 1999; Berg et al., 2005). However, Marsch & White (2003) argue that the content of an image and its immediate text have little semantic agreement and this can, according to Purves et al. (2008), be misleading to image retrieval. Furthermore, these approaches assume that the image has been obtained from a document. In cases where there is no document associated with the image, which is the scenario we are principally concerned with, these techniques are not applicable. 1256 Table 7: Readability evaluation results: Each cell shows the percentage of summaries scoring the ranking s"
P10-1127,W04-1013,0,0.0785449,"ed sentences and finally the last two sentences from the “surrounding” and “visiting” categories. However, in cases where we have not reached the summary word limit because of uncovered categories, i.e. there were not, for instance, sentences about “location”, we add to the end of the summary the next top sentence from the ranked list that was not taken. 3.2 Sentence Selection To compute the final score for each sentence Aker and Gaizauskas (2009) use a linear function with weighted features: n X Sscore = ( 4 Evaluation To evaluate our approach we used two different assessment methods: ROUGE (Lin, 2004) and manual readability. In the following we first describe the data sets used in each of these evaluations, and then we present the results of each assessment. 4.1 f eaturei ∗ weighti ) (2) i=1 We use the same approach, but whereas the feature weights they use are experimentally set rather than learned, we learn the weights using linear regression instead. We used 23 of the 310 images from our image set (see Section 4.1) to train the weights. The image descriptions from this data set are used as model summaries. Our training data contains for each image a set of image descriptions taken from"
P10-1127,nobata-etal-2002-summarization,0,0.024792,"3 5.0 27.1 8.8 1.2 8.1 3.1 grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0 Dependency patterns have been exploited in various language processing applications. In information extraction, for instance, dependency patterns have been used to extract relevant information from text resources (Yangarber et al., 2000; Sudo et al., 2001; Culotta and Sorensen, 2004; Stevenson and Greenwood, 2005; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2009). However, dependency patterns have not been used extensively in summarization tasks. We are aware only of the work described in Nobata et al. (2002) who used dependency patterns in combination with other features to generate extracts in a single document summarization task. The authors found that when learning weights in a simple feature weigthing scheme, the weight assigned to dependency patterns was lower than that assigned to other features. The small contribution of the dependency patterns may have been due to the small number of documents they used to derive their dependency patterns – they gathered dependency patterns from only ten domain specific documents which are unlikely to be sufficient to capture repeated features in a domain"
P10-1127,P05-1047,0,0.0210837,"5 2.6 3.3 focus 72.1 49.3 51.2 20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1 coherence redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1 grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0 Dependency patterns have been exploited in various language processing applications. In information extraction, for instance, dependency patterns have been used to extract relevant information from text resources (Yangarber et al., 2000; Sudo et al., 2001; Culotta and Sorensen, 2004; Stevenson and Greenwood, 2005; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2009). However, dependency patterns have not been used extensively in summarization tasks. We are aware only of the work described in Nobata et al. (2002) who used dependency patterns in combination with other features to generate extracts in a single document summarization task. The authors found that when learning weights in a simple feature weigthing scheme, the weight assigned to dependency patterns was lower than that assigned to other features. The small contribution of the dependency patterns may have been due to the small number of d"
P10-1127,H01-1009,0,0.135272,"We replicate the experiments of Aker and Gaizauskas and generate a bi-gram language model for each object type corpus. In later sections we use LM to refer to these models. 2.3 Dependency patterns We use the same object type corpora to derive dependency patterns. Our patterns are derived from dependency trees which are obtained using the Stanford parser1 . Each article in each object type corpus was pre-processed by sentence splitting and named entity tagging2 . Then each sentence was parsed by the Stanford dependency parser to obtain relational patterns. As with the chain model introduced by Sudo et al. (2001) our relational patterns are concentrated on the verbs in the sentences and contain n+1 words (the verb and n words in direct or indirect relation with the verb). The number n is experimentally set to two words. For illustration consider the sentence shown in Table 3 that is taken from an article in the bridge corpus. The first two rows of the table show the original sentence and its form after named entity tagging. The next step in processing is to replace any occurrence of a string denoting the object type by the term “OBJECTTYPE” as shown in the third row of Table 3. The final two rows of t"
P10-1127,C00-2136,0,0.00873769,"SLMD clarity 72.6 50.5 53.6 21.7 30.0 31.4 1.2 6.7 5.7 4.0 10.2 6.0 0.5 2.6 3.3 focus 72.1 49.3 51.2 20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1 coherence redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1 grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0 Dependency patterns have been exploited in various language processing applications. In information extraction, for instance, dependency patterns have been used to extract relevant information from text resources (Yangarber et al., 2000; Sudo et al., 2001; Culotta and Sorensen, 2004; Stevenson and Greenwood, 2005; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2009). However, dependency patterns have not been used extensively in summarization tasks. We are aware only of the work described in Nobata et al. (2002) who used dependency patterns in combination with other features to generate extracts in a single document summarization task. The authors found that when learning weights in a simple feature weigthing scheme, the weight assigned to dependency patterns was lower than that assigned to other features. The small cont"
P13-1040,C12-2003,1,0.851838,"ich has a translation in the target term to the length of the source term, expressed as a percentage. For compound terms we proceed as with percentageOfTranslatedWords. • longestNotTranslatedUnitInPercentage returns the percentage of the number of words within the longest sequence of source words which have no translations in the target term. 4.2 Cognate based features Dictionaries mostly fail to return translation entries for named entities (NEs) or specialized terminology. Because of this we also use cognate based methods to perform the mapping between source and target words or vice versa. Aker et al. (2012) have applied (1) Longest Common Subsequence Ratio, (2) Longest Common Substring Ratio, (3) Dice Similarity, (4) Needleman-Wunsch Distance and (5) Levenshtein Distance in order to extract parallel phrases from comparable corpora. We adopt these measures within our classifier. Each of them returns a score between 0 and 1. • Longest Common Subsequence Ratio (LCSR): The longest common subsequence (LCS) measure measures the longest common non-consecutive sequence of characters between two strings. For instance, the words “dollars” and “dolari” share a sequence of 5 non-consecutive characters in th"
P13-1040,aswani-gaizauskas-2010-english,0,0.0268202,"urce (asymmetric approaches), and by the extent to which they rely on linguistic knowledge as opposed to simply statistical techniques. We focus on techniques for bilingual term extraction from comparable corpora – collections of source-target language document pairs that are not direct translations but are topically related. We 402 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 402–411, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Onaizan and Knight (2002), Knight and Graehl (1998), Udupa et. al. (2008) and Aswani and Gaizauskas (2010). Very few approaches have treated term alignment as a classification problem suitable for machine learning (ML) techniques. So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way. However, it naturally lends itself to being viewed as a classification task, assuming a symmetric approach, since the different information sources mentioned above can be treated as features and each source-target language potential term pairing can be treated as an instance to be fed to a binary classifier which decides whether to align them or no"
P13-1040,bouamor-etal-2012-identifying,0,0.18231,"lated Work Previous studies have investigated the extraction of bilingual terms from parallel and comparable corpora. For instance, Kupiec (1993) uses statistical techniques and extracts bilingual noun phrases from parallel corpora tagged with terms. Daille et al. (1994), Fan et al. (2009) and Okita et al. (2010) also apply statistical methods to extract terms/phrases from parallel corpora. In addition to statistical methods Daille et al. use word translation information between two words within the extracted terms as a further indicator of the correct alignment. More recently, Bouamor et al. (2012) use vector space models to align terms. The entries in the vectors are co-occurrence statistics between the terms computed over the entire corpus. Bilingual term alignment methods that work on comparable corpora use essentially three sorts of information: (1) cognate information, typically estimated using some sort of transliteration similarity measure (2) context congruence, a measure of the extent to which the words that the source term co-occurs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate"
P13-1040,C02-1011,0,0.2773,"action from comparable corpora – collections of source-target language document pairs that are not direct translations but are topically related. We 402 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 402–411, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Onaizan and Knight (2002), Knight and Graehl (1998), Udupa et. al. (2008) and Aswani and Gaizauskas (2010). Very few approaches have treated term alignment as a classification problem suitable for machine learning (ML) techniques. So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way. However, it naturally lends itself to being viewed as a classification task, assuming a symmetric approach, since the different information sources mentioned above can be treated as features and each source-target language potential term pairing can be treated as an instance to be fed to a binary classifier which decides whether to align them or not. Our work differs from that of Cao and Li (2002) in several ways. First they consider only terms consisting of nounnoun pairs. Secondly for a given source language term"
P13-1040,W97-0119,0,0.0907569,"ilingual term alignment methods that work on comparable corpora use essentially three sorts of information: (1) cognate information, typically estimated using some sort of transliteration similarity measure (2) context congruence, a measure of the extent to which the words that the source term co-occurs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate term and (3) translation of component words in the term and/or in context words, where some limited dictionary exists. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al. (2007), Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. Transliteration and cognate-based information is exploited in Al4 Feature extraction To align or map source and target terms we use an SVM binary classifier (Joachims, 2002) with a linear kernel and the trade-off between training error and margin parameter c = 10. Within the classifier we use language dependent and independent features described in the following sections. 4.1 Dictionary based features The dictionary based features are language dependent and are"
P13-1040,C10-2055,0,0.0341372,"ssentially three sorts of information: (1) cognate information, typically estimated using some sort of transliteration similarity measure (2) context congruence, a measure of the extent to which the words that the source term co-occurs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate term and (3) translation of component words in the term and/or in context words, where some limited dictionary exists. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al. (2007), Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. Transliteration and cognate-based information is exploited in Al4 Feature extraction To align or map source and target terms we use an SVM binary classifier (Joachims, 2002) with a linear kernel and the trade-off between training error and margin parameter c = 10. Within the classifier we use language dependent and independent features described in the following sections. 4.1 Dictionary based features The dictionary based features are language dependent and are computed using bilingual dictionaries which are created with GIZA++ (Och"
P13-1040,steinberger-etal-2012-dgt,0,0.01748,"Transliteration and cognate-based information is exploited in Al4 Feature extraction To align or map source and target terms we use an SVM binary classifier (Joachims, 2002) with a linear kernel and the trade-off between training error and margin parameter c = 10. Within the classifier we use language dependent and independent features described in the following sections. 4.1 Dictionary based features The dictionary based features are language dependent and are computed using bilingual dictionaries which are created with GIZA++ (Och and Ney, 2000; Och and Ney, 2003). The DGT-TM parallel data (Steinberger et al., 2012) was input to GIZA++ to obtain the dictionaries. Dictionary entries have the form hs, ti , pi i, where s is a source word, ti is the i-th translation of s in the dictionary and pi is the probability that s is translated by ti , the pi ’s summing to 1 for each s in the dictionary. From the dictionaries we removed all entries with pi &lt; 0.05. In addition we also removed 403 the target. We also compute another feature averagePercentageOfTranslatedWords which builds the average between the feature values of percentageOfTranslatedWords from source to target and target to source. Thus in total we hav"
P13-1040,P93-1003,0,0.0725045,"recision and F-measure. We run this evaluation on all 20 language pairs. Secondly, we test the system’s performance on obtaining bilingual terms from comparable corpora. This second test simulates the situation of using the term alignment system in a real world scenario. For this evaluation we collected English-German comparable corpora from Wikipedia, performed monolingual term tagging and ran our tool over the term tagged corpora to extract bilingual terms. 3 Related Work Previous studies have investigated the extraction of bilingual terms from parallel and comparable corpora. For instance, Kupiec (1993) uses statistical techniques and extracts bilingual noun phrases from parallel corpora tagged with terms. Daille et al. (1994), Fan et al. (2009) and Okita et al. (2010) also apply statistical methods to extract terms/phrases from parallel corpora. In addition to statistical methods Daille et al. use word translation information between two words within the extracted terms as a further indicator of the correct alignment. More recently, Bouamor et al. (2012) use vector space models to align terms. The entries in the vectors are co-occurrence statistics between the terms computed over the entire"
P13-1040,E03-1035,0,0.0198491,"on Bilingual terminologies are important for various applications of human language technologies, including cross-language information search and retrieval, statistical machine translation (SMT) in narrow domains and computer-aided assistance to human translators. Automatic construction of bilingual terminology mappings has been investigated in many earlier studies and various methods have been applied to this task. These methods may be distinguished by whether they work on parallel or comparable corpora, by whether they assume monolingual term recognition in source and target languages (what Moore (2003) calls symmetrical approaches) or only in the source (asymmetric approaches), and by the extent to which they rely on linguistic knowledge as opposed to simply statistical techniques. We focus on techniques for bilingual term extraction from comparable corpora – collections of source-target language document pairs that are not direct translations but are topically related. We 402 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 402–411, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Onaizan and Knight (2002), Kni"
P13-1040,P07-1084,0,0.0630059,"Missing"
P13-1040,C00-2163,0,0.209216,"10) the context of text units is used to identify term mappings. Transliteration and cognate-based information is exploited in Al4 Feature extraction To align or map source and target terms we use an SVM binary classifier (Joachims, 2002) with a linear kernel and the trade-off between training error and margin parameter c = 10. Within the classifier we use language dependent and independent features described in the following sections. 4.1 Dictionary based features The dictionary based features are language dependent and are computed using bilingual dictionaries which are created with GIZA++ (Och and Ney, 2000; Och and Ney, 2003). The DGT-TM parallel data (Steinberger et al., 2012) was input to GIZA++ to obtain the dictionaries. Dictionary entries have the form hs, ti , pi i, where s is a source word, ti is the i-th translation of s in the dictionary and pi is the probability that s is translated by ti , the pi ’s summing to 1 for each s in the dictionary. From the dictionaries we removed all entries with pi &lt; 0.05. In addition we also removed 403 the target. We also compute another feature averagePercentageOfTranslatedWords which builds the average between the feature values of percentageOfTransla"
P13-1040,J03-1002,0,0.00610398,"text units is used to identify term mappings. Transliteration and cognate-based information is exploited in Al4 Feature extraction To align or map source and target terms we use an SVM binary classifier (Joachims, 2002) with a linear kernel and the trade-off between training error and margin parameter c = 10. Within the classifier we use language dependent and independent features described in the following sections. 4.1 Dictionary based features The dictionary based features are language dependent and are computed using bilingual dictionaries which are created with GIZA++ (Och and Ney, 2000; Och and Ney, 2003). The DGT-TM parallel data (Steinberger et al., 2012) was input to GIZA++ to obtain the dictionaries. Dictionary entries have the form hs, ti , pi i, where s is a source word, ti is the i-th translation of s in the dictionary and pi is the probability that s is translated by ti , the pi ’s summing to 1 for each s in the dictionary. From the dictionaries we removed all entries with pi &lt; 0.05. In addition we also removed 403 the target. We also compute another feature averagePercentageOfTranslatedWords which builds the average between the feature values of percentageOfTranslatedWords from source"
P13-1040,W10-4006,0,0.0326927,"rpora. This second test simulates the situation of using the term alignment system in a real world scenario. For this evaluation we collected English-German comparable corpora from Wikipedia, performed monolingual term tagging and ran our tool over the term tagged corpora to extract bilingual terms. 3 Related Work Previous studies have investigated the extraction of bilingual terms from parallel and comparable corpora. For instance, Kupiec (1993) uses statistical techniques and extracts bilingual noun phrases from parallel corpora tagged with terms. Daille et al. (1994), Fan et al. (2009) and Okita et al. (2010) also apply statistical methods to extract terms/phrases from parallel corpora. In addition to statistical methods Daille et al. use word translation information between two words within the extracted terms as a further indicator of the correct alignment. More recently, Bouamor et al. (2012) use vector space models to align terms. The entries in the vectors are co-occurrence statistics between the terms computed over the entire corpus. Bilingual term alignment methods that work on comparable corpora use essentially three sorts of information: (1) cognate information, typically estimated using"
P13-1040,P95-1050,0,0.284864,"ire corpus. Bilingual term alignment methods that work on comparable corpora use essentially three sorts of information: (1) cognate information, typically estimated using some sort of transliteration similarity measure (2) context congruence, a measure of the extent to which the words that the source term co-occurs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate term and (3) translation of component words in the term and/or in context words, where some limited dictionary exists. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al. (2007), Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. Transliteration and cognate-based information is exploited in Al4 Feature extraction To align or map source and target terms we use an SVM binary classifier (Joachims, 2002) with a linear kernel and the trade-off between training error and margin parameter c = 10. Within the classifier we use language dependent and independent features described in the following sections. 4.1 Dictionary based features The dictionary based features are l"
P13-1040,W02-0505,0,\N,Missing
P13-1040,C94-1084,0,\N,Missing
P13-1040,J98-4003,0,\N,Missing
paramita-etal-2012-correlation,W99-0625,0,\N,Missing
paramita-etal-2012-correlation,W06-2810,0,\N,Missing
paramita-etal-2012-correlation,W11-1212,0,\N,Missing
paramita-etal-2012-correlation,W09-1605,0,\N,Missing
paramita-etal-2012-correlation,N10-1063,0,\N,Missing
paramita-etal-2012-correlation,J05-4003,0,\N,Missing
paramita-etal-2012-correlation,W11-2206,0,\N,Missing
paramita-etal-2012-correlation,D07-1036,0,\N,Missing
paramita-etal-2012-correlation,N09-2031,0,\N,Missing
paramita-etal-2012-correlation,zesch-etal-2008-extracting,0,\N,Missing
R09-1002,W08-1407,1,0.850161,"t applicable. In this paper, we propose a technique for automatic image captioning or caption enhancement starting with only a set of place names pertaining to an image. The technique applies just to images of static features of the built or natural landscape (e.g. buildings, mountains, etc.) and not to images of objects which move about in such landscapes (e.g. people, cars, clouds, etc.). Our approach is based on extractive multi-document summarization techniques, where the documents to be summarized are web-documents retrieved using the place names associated with an image. In earlier work [1] we have shown that in this scenario query-based summaries outperform generic summaries, i.e. extractive summaries of multiple web pages retrieved using the place names which bias the summarizer to include sentences mentioning these place names tend to be better than generic summaries of the same pages. However, the resulting summaries were still far from ideal. We examined information selected by humans for inclusion in a caption from the same place-nameretrieved web-documents made available to the summarizer and observed high levels of agreement between humans on which information to include"
R09-1002,P07-1126,0,0.171497,"has grown immensely, facilitated by the development of cheap digital hardware and the availability of online image sharing social sites. Many of these images are tagged only with place names or contain minimal captions that include locational information. This small amount of textual information associated with the image is of limited usefulness for image indexing, organization and search. What would be useful is a means to generate or augment captions automatically based on existing data. Attempts towards automatic generation of image captions have been previously reported. Deschacht & Moens [6] and Mori et al. [14] generate image captions automatically by analyzing image-related text from the immediate context of the image, e.g. the surrounding text in HTML documents. The authors identify named entities and other noun phrases in the imagerelated text and assign these to the image as captions. Other approaches create image captions by taking into consideration image features (colour, shape and texture) as well as image-related text [22, 14, 4, 7, 3, 15, 8]. These approaches analyze only the immediate textual context of the image. However, generating image captions based on the immedi"
R09-1002,P08-1032,0,0.088501,"aptions automatically based on existing data. Attempts towards automatic generation of image captions have been previously reported. Deschacht & Moens [6] and Mori et al. [14] generate image captions automatically by analyzing image-related text from the immediate context of the image, e.g. the surrounding text in HTML documents. The authors identify named entities and other noun phrases in the imagerelated text and assign these to the image as captions. Other approaches create image captions by taking into consideration image features (colour, shape and texture) as well as image-related text [22, 14, 4, 7, 3, 15, 8]. These approaches analyze only the immediate textual context of the image. However, generating image captions based on the immediate context of the image can result in an image description which does not describe the image at all. Marsch & White [13] argue that the content of an image and its immediate text have little semantic agreement and this can, according to Purves et al. [16], be misleading to image retrieval. Furthermore, these approaches assume that the image has been obtained from a document. In cases where there is no document associated with the image, which is the scenario we are"
R09-1002,C92-2082,0,0.0139715,"Missing"
R09-1002,W04-1013,0,0.0342275,"n anyway they want, resulting in image captions of different length, coherence, focus, grammaticality etc. To ensure a good standard for our model captions we asked 11 human subjects to generate up to four model captions per object by modifying Virtualtourist captions. The modifications included deleting personal information, ensuring consistency and coherence of the text and generating a summary of 190-210 words in length (because our automatic summaries have similar word counts). An example model summary about Parc Guell is shown in Table 6. For comparison between summaries the ROUGE metric [11] is used. ROUGE compares automatically generated summaries against human-created reference summaries and can be used to estimate content coverage in an automatically generated summary. Following the Document Understanding Conference (DUC) [5] evaluation standards we use ROUGE 2 and ROUGE SU4 as evaluation metrics. ROUGE 2 gives recall scores for bi-gram overlap between the automatically generated summaries and the reference ones. ROUGE SU4 allows bi-grams to be composed of non-contiguous words, with a maximum of four words between the bi-grams. 3 www.virtualtourist.com Table 4: ROUGE scores fo"
R09-1002,C00-1072,0,0.220542,"ation that humans appear to have a conceptual model of what is salient regarding a specific object type, the question arises as to whether we can represent or approximate such a conceptual model in a way that allows us to improve content selection for our caption summaries. While there are many ways this could be done, one simple way is to view a corpus of descriptions of objects of a given type as containing an implicit model of that type and use language models derived from the corpus to bias sentence selection by an extractive summarizer. In this paper we explore the use of signature words [12] and language models [21] to represent such concep6 International Conference RANLP 2009 - Borovets, Bulgaria, pages 6–11 tual models and investigate their impact on the quality of automatically generated image captions. Our results show that using these conceptual models does indeed improve the results over those of a standard query-based summarizer. In the following we first describe how the object type corpora were collected (section 2) and how language models are generated from these corpora (section 3). Next, we describe the set of our images, their categorization by object type and the re"
R11-1011,W04-1013,0,0.0188887,"Missing"
R11-1011,P04-3020,0,0.0462382,"the generation of summaries depends on a wide range of issues, such as the summarization input, output or purpose. In particular, the type of text 2 Related Work A great number of techniques have been proven to be effective for generating summaries automatically. Such approaches include template creation (Oakes and Paice, 1999), statistical techniques (Teng et al., 2008; Lloret and Palomar, 2009), discourse analysis (Marcu, 1999; Teufel 77 Proceedings of Recent Advances in Natural Language Processing, pages 77–83, Hissar, Bulgaria, 12-14 September 2011. and Moens, 2002), graph-based methods (Mihalcea, 2004; Plaza et al., 2008), and machine learning algorithms (Fattah and Ren, 2008; Schilder and Kondadadi, 2008). Moreover, new scenarios, such as the generation of summaries that can be used as image captions (Aker and Gaizauskas, 2009; Plaza et al., 2010; Aker and Gaizauskas, 2010a), have recently drawn special attention in recent years. In particular, this image caption generation task has been automatically approached by analyzing image-related text from the immediate context of the image, for instance, the surrounding text in HTML documents (Mori et al., 2000; Deschacht and Moens, 2007). In th"
R11-1011,W08-2008,1,0.903529,"Missing"
R11-1011,R09-1002,1,0.851942,"tive for generating summaries automatically. Such approaches include template creation (Oakes and Paice, 1999), statistical techniques (Teng et al., 2008; Lloret and Palomar, 2009), discourse analysis (Marcu, 1999; Teufel 77 Proceedings of Recent Advances in Natural Language Processing, pages 77–83, Hissar, Bulgaria, 12-14 September 2011. and Moens, 2002), graph-based methods (Mihalcea, 2004; Plaza et al., 2008), and machine learning algorithms (Fattah and Ren, 2008; Schilder and Kondadadi, 2008). Moreover, new scenarios, such as the generation of summaries that can be used as image captions (Aker and Gaizauskas, 2009; Plaza et al., 2010; Aker and Gaizauskas, 2010a), have recently drawn special attention in recent years. In particular, this image caption generation task has been automatically approached by analyzing image-related text from the immediate context of the image, for instance, the surrounding text in HTML documents (Mori et al., 2000; Deschacht and Moens, 2007). In these approaches, named entities and other noun phrases in the image-related text are identified and assigned to the image as captions. Similar to these approaches, our aim is to produce summaries capable of providing a brief descrip"
R11-1011,P10-1127,1,0.660235,"ch approaches include template creation (Oakes and Paice, 1999), statistical techniques (Teng et al., 2008; Lloret and Palomar, 2009), discourse analysis (Marcu, 1999; Teufel 77 Proceedings of Recent Advances in Natural Language Processing, pages 77–83, Hissar, Bulgaria, 12-14 September 2011. and Moens, 2002), graph-based methods (Mihalcea, 2004; Plaza et al., 2008), and machine learning algorithms (Fattah and Ren, 2008; Schilder and Kondadadi, 2008). Moreover, new scenarios, such as the generation of summaries that can be used as image captions (Aker and Gaizauskas, 2009; Plaza et al., 2010; Aker and Gaizauskas, 2010a), have recently drawn special attention in recent years. In particular, this image caption generation task has been automatically approached by analyzing image-related text from the immediate context of the image, for instance, the surrounding text in HTML documents (Mori et al., 2000; Deschacht and Moens, 2007). In these approaches, named entities and other noun phrases in the image-related text are identified and assigned to the image as captions. Similar to these approaches, our aim is to produce summaries capable of providing a brief description for an image of an object related to the t"
R11-1011,P08-2052,0,0.016462,"put, output or purpose. In particular, the type of text 2 Related Work A great number of techniques have been proven to be effective for generating summaries automatically. Such approaches include template creation (Oakes and Paice, 1999), statistical techniques (Teng et al., 2008; Lloret and Palomar, 2009), discourse analysis (Marcu, 1999; Teufel 77 Proceedings of Recent Advances in Natural Language Processing, pages 77–83, Hissar, Bulgaria, 12-14 September 2011. and Moens, 2002), graph-based methods (Mihalcea, 2004; Plaza et al., 2008), and machine learning algorithms (Fattah and Ren, 2008; Schilder and Kondadadi, 2008). Moreover, new scenarios, such as the generation of summaries that can be used as image captions (Aker and Gaizauskas, 2009; Plaza et al., 2010; Aker and Gaizauskas, 2010a), have recently drawn special attention in recent years. In particular, this image caption generation task has been automatically approached by analyzing image-related text from the immediate context of the image, for instance, the surrounding text in HTML documents (Mori et al., 2000; Deschacht and Moens, 2007). In these approaches, named entities and other noun phrases in the image-related text are identified and assigned"
R11-1011,aker-gaizauskas-2010-model,1,0.773328,"ch approaches include template creation (Oakes and Paice, 1999), statistical techniques (Teng et al., 2008; Lloret and Palomar, 2009), discourse analysis (Marcu, 1999; Teufel 77 Proceedings of Recent Advances in Natural Language Processing, pages 77–83, Hissar, Bulgaria, 12-14 September 2011. and Moens, 2002), graph-based methods (Mihalcea, 2004; Plaza et al., 2008), and machine learning algorithms (Fattah and Ren, 2008; Schilder and Kondadadi, 2008). Moreover, new scenarios, such as the generation of summaries that can be used as image captions (Aker and Gaizauskas, 2009; Plaza et al., 2010; Aker and Gaizauskas, 2010a), have recently drawn special attention in recent years. In particular, this image caption generation task has been automatically approached by analyzing image-related text from the immediate context of the image, for instance, the surrounding text in HTML documents (Mori et al., 2000; Deschacht and Moens, 2007). In these approaches, named entities and other noun phrases in the image-related text are identified and assigned to the image as captions. Similar to these approaches, our aim is to produce summaries capable of providing a brief description for an image of an object related to the t"
R11-1011,P07-1126,0,0.0256246,"aph-based methods (Mihalcea, 2004; Plaza et al., 2008), and machine learning algorithms (Fattah and Ren, 2008; Schilder and Kondadadi, 2008). Moreover, new scenarios, such as the generation of summaries that can be used as image captions (Aker and Gaizauskas, 2009; Plaza et al., 2010; Aker and Gaizauskas, 2010a), have recently drawn special attention in recent years. In particular, this image caption generation task has been automatically approached by analyzing image-related text from the immediate context of the image, for instance, the surrounding text in HTML documents (Mori et al., 2000; Deschacht and Moens, 2007). In these approaches, named entities and other noun phrases in the image-related text are identified and assigned to the image as captions. Similar to these approaches, our aim is to produce summaries capable of providing a brief description for an image of an object related to the tourist domain, for instance the Eiffel Tower. Instead of analyzing the text surrounding the image (which may be not available), we use documents obtained from the web using the place name as query. In order to achieve this goal, we rely on the corresponding human written descriptions or summaries to capture which"
R11-1011,J02-4002,0,0.157932,"Missing"
R19-1002,R19-2002,1,0.873989,"Missing"
R19-1002,balahur-etal-2010-sentiment,0,0.0796272,"Missing"
R19-1002,L18-1008,0,0.0415676,"aches and features. Before using the tweets in decision making, we also apply a simple preprocessing on them. In the following, we briefly outline these. 5.2.2 5.1 5.2.3 TF-IDF In this case, we simply use the training data to create a vocabulary of terms and use this vocabulary to extract features from each tweet. We use tf-idf representation for each vocabulary term. Preprocessing Embeddings We use the ArkTokenizer (Gimpel et al., 2011) to tokenize the tweets. In addition to tokenization, we do lowercasing and remove digits if available in text. Finally, we also use fasttext based embedding (Mikolov et al., 2018) vectors which are trained on common crawl having 600 billion tokens. 5.2 We investigate 8 classifiers for our task including Multi-Layer Perceptron (MLPC), Support Vector Machine with linear (LSVC) and rbf (SVC) kernel, K Nearest Neighbour (KNN), Logistic Regression (LR), Random Forest (RF), XGBoost (XGB) and Decision Tree (DT). In addition, we also fine-tune BERT-base model (Devlin et al., 2018). Each classifier, except the BERT, has been trained and tested on each possible combination of the three feature types. 5.3 Features We extract nine features for each tweet and divide them into Struc"
R19-1002,S16-1001,0,0.0374605,"wn in Figure 1. In contrary to that, the bad news is defined as when it relates to the low subjective topic and include negative overtones such as death, injury, defeat, loss and is not beneficial for an individual, a group or society. An example of bad news is shown in Figure 2. Based on these definitions/guidelines we have gathered our dataset (see next Section) of tweets containing the good and bad labels. Related Work In terms of classifying tweets into the good and bad classes no prior work exists. The closest studies to our work, are those performing sentiment classification in Twitter (Nakov et al., 2016; Rosenthal et al., 2017). Kouloumpis et al. (2011) use n-gram, lexicon, part of speech and micro-blogging features for detecting sentiment in tweets. Similar features are used by Go (2009). More recently researchers also investigated deep learning strategies to tackle the tweet level sentiment problem (Severyn and Moschitti, 2015; Ren et al., 2016). Twitter is multi-lingual and in Mozetiˇc et al. (2016) the idea of multi-lingual sentiment classification is investigated. The task, as well as approaches proposed for determining tweet level sentiment, are nicely summarized in the survey paper of"
R19-1002,P11-2008,0,0.182169,"Missing"
R19-1002,S17-2088,0,0.0764101,"Missing"
R19-1002,S15-2079,0,0.0788259,"Missing"
S16-1092,S12-1051,0,0.0861557,"tasets. 1 Introduction Semantic Textual Similarity (STS) is a metric which aims to determine the likeness between two short textual entities. Therefore, STS is widely used in many research areas such as Natural Language Processing, for a large amount of tasks like Information Retrieval (IR), Natural Language Understanding (NLU) or even Machine Translation (MT) evaluation, for which STS allows capturing more information than traditional metrics based on n-grams match like BLEU (Papineni et al., 2002). Part of the SemEval campaign, STS competition benefits from a growing interest over the year (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015). In 2016, participants had the possibility to compete in two tasks: (i) STS split into ’STS Core’ and ’Cross-lingual STS’ which are respectively an English monolingual and English/Spanish bilingual subtasks; (ii) iSTS which focuses on the interpretable aspect of STS assessment. Introduced for the first time in 2015, iSTS became a standalone task this year. This paper describes our participation in the STS Core subtask and is organised as follows: we first describe in Section 2 methods we have adapted to tackle the task. Next, we"
S16-1092,S13-1004,0,0.0712362,"n Semantic Textual Similarity (STS) is a metric which aims to determine the likeness between two short textual entities. Therefore, STS is widely used in many research areas such as Natural Language Processing, for a large amount of tasks like Information Retrieval (IR), Natural Language Understanding (NLU) or even Machine Translation (MT) evaluation, for which STS allows capturing more information than traditional metrics based on n-grams match like BLEU (Papineni et al., 2002). Part of the SemEval campaign, STS competition benefits from a growing interest over the year (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015). In 2016, participants had the possibility to compete in two tasks: (i) STS split into ’STS Core’ and ’Cross-lingual STS’ which are respectively an English monolingual and English/Spanish bilingual subtasks; (ii) iSTS which focuses on the interpretable aspect of STS assessment. Introduced for the first time in 2015, iSTS became a standalone task this year. This paper describes our participation in the STS Core subtask and is organised as follows: we first describe in Section 2 methods we have adapted to tackle the task. Next, we present and discuss"
S16-1092,S14-2010,0,0.0671161,"milarity (STS) is a metric which aims to determine the likeness between two short textual entities. Therefore, STS is widely used in many research areas such as Natural Language Processing, for a large amount of tasks like Information Retrieval (IR), Natural Language Understanding (NLU) or even Machine Translation (MT) evaluation, for which STS allows capturing more information than traditional metrics based on n-grams match like BLEU (Papineni et al., 2002). Part of the SemEval campaign, STS competition benefits from a growing interest over the year (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015). In 2016, participants had the possibility to compete in two tasks: (i) STS split into ’STS Core’ and ’Cross-lingual STS’ which are respectively an English monolingual and English/Spanish bilingual subtasks; (ii) iSTS which focuses on the interpretable aspect of STS assessment. Introduced for the first time in 2015, iSTS became a standalone task this year. This paper describes our participation in the STS Core subtask and is organised as follows: we first describe in Section 2 methods we have adapted to tackle the task. Next, we present and discuss our results (Section"
S16-1092,S15-2045,0,0.0133173,"etric which aims to determine the likeness between two short textual entities. Therefore, STS is widely used in many research areas such as Natural Language Processing, for a large amount of tasks like Information Retrieval (IR), Natural Language Understanding (NLU) or even Machine Translation (MT) evaluation, for which STS allows capturing more information than traditional metrics based on n-grams match like BLEU (Papineni et al., 2002). Part of the SemEval campaign, STS competition benefits from a growing interest over the year (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015). In 2016, participants had the possibility to compete in two tasks: (i) STS split into ’STS Core’ and ’Cross-lingual STS’ which are respectively an English monolingual and English/Spanish bilingual subtasks; (ii) iSTS which focuses on the interpretable aspect of STS assessment. Introduced for the first time in 2015, iSTS became a standalone task this year. This paper describes our participation in the STS Core subtask and is organised as follows: we first describe in Section 2 methods we have adapted to tackle the task. Next, we present and discuss our results (Section 3). Finally, Section 4"
S16-1092,P14-1023,0,0.035884,"stores the outputs of all the compared regions of the input sentences. The final output of the system generates an output similarity score through a final log-softmax layer, which receives the accumulated vector. System parameters (window size, number of filters, learning rate, regularization parameter, hidden units) are maintained with respect to the original work in which the system is presented. 2.3 Word2Vec Word embeddings using Word2vec (Mikolov et al., 2013) have been extensively used to measure the semantic similarity between words. Our word embeddings comprise the vectors published by Baroni et al. (2014). To measure the similarity between a pair of sentences we first remove from each sentence stop-words as well as punctuations, query for each word its vector representation and create a averaged sum of the word vectors. The number of remaining words in each sentence is used to average that sentence. Finally, we use the resulting averaged sum vectors and determine their similarity using cosine. 2.4 Model Combination In this section, we present the experiments to combine all methods described in previous sections. We formulated the problem as a regres611 sion task where we are given multiple fea"
S16-1092,D15-1181,0,0.0323202,"Missing"
S16-1092,P02-1040,0,0.0946109,"s slightly better than using CNN only. Our results also show that the performance of our systems varies between the datasets. 1 Introduction Semantic Textual Similarity (STS) is a metric which aims to determine the likeness between two short textual entities. Therefore, STS is widely used in many research areas such as Natural Language Processing, for a large amount of tasks like Information Retrieval (IR), Natural Language Understanding (NLU) or even Machine Translation (MT) evaluation, for which STS allows capturing more information than traditional metrics based on n-grams match like BLEU (Papineni et al., 2002). Part of the SemEval campaign, STS competition benefits from a growing interest over the year (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirrea et al., 2015). In 2016, participants had the possibility to compete in two tasks: (i) STS split into ’STS Core’ and ’Cross-lingual STS’ which are respectively an English monolingual and English/Spanish bilingual subtasks; (ii) iSTS which focuses on the interpretable aspect of STS assessment. Introduced for the first time in 2015, iSTS became a standalone task this year. This paper describes our participation in the STS Core subta"
S16-1092,D14-1162,0,0.0779087,"stem is a Convolutional Neural Network (CNN), which generates a similarity score for each pair of sen610 tences. More specifically, we replicated the system presented in (He et al., 2015), using previous SemEval data for training the network and generating similarity values between 0 and 5, for each of the test sentences given by the organizers. In the following subsections, we briefly summarize the CNN system, although specific details can be found in the cited paper. Word Representation Words in the sentences to be compared are first transformed into vectors using the GloVe word embeddings (Pennington et al., 2014). These embeddings are trained on 840 billion tokens, and the resulting vectors are 300dimensional. Hence, each sentence sent with n words will be transformed into a matrix Msent ∈ Rn×300 . Hence, senti:j denotes the word embeddings of words i to j inside the [k] sentence, senti denotes the k-th dimension of [k] word embedding i and senti:j the k-th dimension of words i to j inside the sentence. Sentence modelling The technique makes use of two different types of filters for extracting features from the sentences: holistic and per-dimension. Holistic filters generate a vector representing a ”t"
S16-1092,P13-4014,1,0.828496,"Missing"
S16-1092,Q14-1018,0,0.0259121,"n this paper. We adapted methods from related work that have been applied on the monolingual STS task and used their combined version on the new 2016 monolingual task. Methods adapted and the strategy we used to combine them are described in the following sections. 2.1 Monolingual Alignment We use an alignment-based approach, which was among the top-perfoming submissions in the past year’s task (Sultan et al., 2015). Sentence similarity score is computed in two separate steps. First, an alignment between related words in the input sentences is established using Monolingual Word Aligner (MWA) (Sultan et al., 2014). Next, sentence similarity is calculated based on the proportion of aligned content words: nac (S 1 ) + nac (S 2 ) sim(S , S ) = nc (S 1 ) + nc (S 2 ) 1 2 (1) where nc (S i ) and nac (S i ) are the number of content words and the number of aligned content words in sentence i, respectively. MWA makes alignment decisions based on lexical similarity and contextual evidence. Lexical similarity component identifies word pairs that are possible candidates for alignment. Context words are considered as evidence for alignment if they are lexically similar and have the same or equivalent syntactic rel"
S16-1092,S15-2027,0,0.0268312,"n that those methods have been reported separately a natural question that arises from this is what the performance is when those methods are combined. This is exactly what we did and what we propose in this paper. We adapted methods from related work that have been applied on the monolingual STS task and used their combined version on the new 2016 monolingual task. Methods adapted and the strategy we used to combine them are described in the following sections. 2.1 Monolingual Alignment We use an alignment-based approach, which was among the top-perfoming submissions in the past year’s task (Sultan et al., 2015). Sentence similarity score is computed in two separate steps. First, an alignment between related words in the input sentences is established using Monolingual Word Aligner (MWA) (Sultan et al., 2014). Next, sentence similarity is calculated based on the proportion of aligned content words: nac (S 1 ) + nac (S 2 ) sim(S , S ) = nc (S 1 ) + nc (S 2 ) 1 2 (1) where nc (S i ) and nac (S i ) are the number of content words and the number of aligned content words in sentence i, respectively. MWA makes alignment decisions based on lexical similarity and contextual evidence. Lexical similarity compo"
S19-2147,N16-1138,0,0.0499239,"n propagation. Initial work on rumour detection and stance classification (Qazvinian et al., 2011) was succeeded by more elaborate systems and annotation schemas (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016). Vosoughi (2015) demonstrated the value of making use of propagation information, i.e. the ensuing discussion, in rumour verification. Stance detection is the task of classifying a text according to the position it takes with respect to a statement. Research supports the importance of this subtask as a first step to 6 veracity identification. (Ferreira and Vlachos, 2016; Enayet and El-Beltagy, 2017). Crowd response, stance and the details of rumour propagation feature in the work by Chen et al. (2016) as well as the most successful system in RumourEval 2017 (Enayet and El-Beltagy, 2017), and the highest performing systems in RumourEval 2019. 1.2 Datasets for rumour verification The UK fact-checking charity Full Fact provides a roadmap7 for development of automated fact checking. They cite open and shared evaluation as one of their five principles for international collaboration, demonstrating the continuing relevance of shared tasks in this area. Shared data"
S19-2147,S17-2083,1,0.672765,"4 (15) 0.4895 (4) 0.1272 (19) 0.3267 (16) 0.3537 (14) 0.3875 (10) 0.4384 (6) 0.3927 (9) 0.6067 (2) 0.4792 (5) 0.3699 (12) 0.4298 (8) 0.3326 0.6846 0.2165 0.1845 0.7857 0.2530 0.3364 0.7806 0.4929 0.3089 0.7698 - 0.2241 0.7115 0.2234 uses the same features as the stance classification system but produces a single output per branch. The veracity prediction for the thread is then decided using majority voting over per-branch outcomes. Stance classification baseline For subtask A we released a Keras (Chollet et al., 2015) implementation of branchLSTM, the winning system of RumourEval 2017 Task A (Kochkina et al., 2017). This system uses the conversation structure by splitting it into linear branches. It is a neural network architecture that uses LSTM layer(s) to process sequences of tweets, outputting a stance label at each time step. Each tweet is represented by the average of its word vectors 11 concatenated with a number of extra features. This baseline was outperformed by 3 submitted systems (BLCU NLP, BUT-FIT, eventAI). 4.2 Subtask B, RMSE 0.6078 (1) 0.7642 (2) 0.8012 (3) 0.8179 (5) 0.8081 (4) 0.8623 (7) 0.8623 (7) 0.8623 (7) 0.8678 (8) 0.8264 (6) 0.9148 (9) - Table 5: Results table. Ranking is in brac"
S19-2147,C18-1288,1,0.869514,"Missing"
S19-2147,P18-1184,0,0.0196785,"cite open and shared evaluation as one of their five principles for international collaboration, demonstrating the continuing relevance of shared tasks in this area. Shared datasets are a crucial part of the joint endeavour. Datasets for rumour resolution are still relatively few, and likely to be in increasing demand. In addition to the data from RumourEval 2017, the dataset released by Kwon et al. (2017) is also suitable for veracity classification. It includes 51 true rumours and 60 false rumours, where each rumour includes a stream of tweets associated with it. Twitter 15 and 16 datasets (Ma et al., 2018) contain claim propagation trees and combine tasks of rumour detection and verification in one four-way classification task (Non-rumour, True, False, Unverified). A Sina Weibo corpus is also available (Wu et al., 2015), in which 5000 posts are classified for veracity, but responses are not available. Partially generated statistical claim checking data is now becoming available in the context of the FEVER shared task, mentioned above, but is not suitable for this type of work. Twitter continues to be a highly relevant platform for rumour verification, being popular with the public as well as po"
S19-2147,N18-1202,0,0.0133727,"t al., 2018) 10 https://github.com/kochkinaelena/ RumourEval2019 11 We are using word2vec (Mikolov et al., 2013) model pretrained on the GoogleNews dataset (300d) 851 training of bidirectional representations to provide additional context. They experiment with different parameter settings and if the model increased overall performance it was added to the classifier. Interestingly the best performing system in task A (BLCU-NLP) and the third best (CLEARumor) also use pre-trained contextual embedding representations with BLCU-NLP using OpenAI GPT (Radford et al., 2018) and ClEARumor using ELMo (Peters et al., 2018). While most systems use single tweets or pairs of tweets (sourceresponse) as their underlying structure to operate on, BLCU-NLP employ an inference chain-based system for this paper. Thus they consider the conversation thread starting with a source tweet, followed by replies, in which each one responds to an earlier one in time sequence. They take each conversation thread as an inference chain and concentrate on utilizing it to solve the problem of class imbalance in subtask A and training data scarcity in subtask B. They also have augmented the training data with external public datasets. Ot"
S19-2147,D11-1147,0,0.461343,"ient skepticism and/or anxiety so as to motivate finding out the actual truth” (Zubiaga et al., 2016). One can distinguish several component to a rumour resolution pipeline such as rumour detection, rumour tracking and stance classification, leading to the final outcome of determining the veracity of a rumour (Zubiaga et al., 2018). Thus what characterises rumour verification compared to other types of fact checking is time sensitivity and the importance of dynamic interactions between users, their stance and information propagation. Initial work on rumour detection and stance classification (Qazvinian et al., 2011) was succeeded by more elaborate systems and annotation schemas (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016). Vosoughi (2015) demonstrated the value of making use of propagation information, i.e. the ensuing discussion, in rumour verification. Stance detection is the task of classifying a text according to the position it takes with respect to a statement. Research supports the importance of this subtask as a first step to 6 veracity identification. (Ferreira and Vlachos, 2016; Enayet and El-Beltagy, 2017). Crowd response, stance and the details o"
S19-2147,D15-1312,0,0.023098,"Hostage-taker in supermarket siege killed, reports say. #ParisAttacks LINK [true] Veracity prediction. Example 2: u1: OMG. #Prince rumoured to be performing in Toronto today. Exciting! [false] Table 1: Examples of source tweets with veracity value of local features. Fact checking is a broad complex task, challenging the resourcefulness of even a human expert. Claims such as ”we send the EU 350 million a week” which is partially true would need to be decomposed into statements to be checked against knowledge bases and multiple sources. Ways of automating fact checking has inspired researchers (Vlachos and Riedel, 2015) and has resulted in a new shared task FEVER.6 Other research has focused on stylistic tells of untrustworthiness in the source itself (Conroy et al., 2015; Singhania et al., 2017). Rumour verification is a particular case of fact checking. Rumours are “circulating stories of questionable veracity, which are apparently credible but hard to verify, and produce sufficient skepticism and/or anxiety so as to motivate finding out the actual truth” (Zubiaga et al., 2016). One can distinguish several component to a rumour resolution pipeline such as rumour detection, rumour tracking and stance classi"
sanchan-etal-2017-automatic,D10-1007,0,\N,Missing
sanchan-etal-2017-automatic,C00-1072,0,\N,Missing
skadina-etal-2012-collecting,W06-2810,0,\N,Missing
skadina-etal-2012-collecting,E06-1020,0,\N,Missing
skadina-etal-2012-collecting,W11-1217,0,\N,Missing
skadina-etal-2012-collecting,E09-1003,0,\N,Missing
skadina-etal-2012-collecting,W07-1702,0,\N,Missing
skadina-etal-2012-collecting,J03-3002,0,\N,Missing
skadina-etal-2012-collecting,W09-1605,0,\N,Missing
skadina-etal-2012-collecting,P02-1040,0,\N,Missing
skadina-etal-2012-collecting,J05-4003,0,\N,Missing
skadina-etal-2012-collecting,C10-1073,0,\N,Missing
skadina-etal-2012-collecting,R11-1106,0,\N,Missing
skadina-etal-2012-collecting,P07-2045,0,\N,Missing
skadina-etal-2012-collecting,P12-3016,1,\N,Missing
skadina-etal-2012-collecting,2005.mtsummit-papers.11,0,\N,Missing
skadina-etal-2012-collecting,aker-etal-2012-light,1,\N,Missing
skadina-etal-2012-collecting,su-babych-2012-development,1,\N,Missing
skadina-etal-2012-collecting,pinnis-2012-latvian,1,\N,Missing
skadina-etal-2012-collecting,C10-2054,0,\N,Missing
skadina-etal-2012-collecting,P00-1056,0,\N,Missing
W08-1407,P07-1126,0,0.167376,"Missing"
W08-1407,W04-1013,0,0.121202,"ed with coordinates (latitude and longitude) and compass information, that show things with fixed locations (e.g. buildings, mountains, etc.). This paper reports an initial study that aims to assess the viability of a state-of-the-art multi-document summarizer for automatic captioning of geo-referenced images. The automatic captioning procedure requires summarizing multiple web documents that contain information related to images’ location. We use SUMMA (Saggion and Gaizauskas, 2005) to generate generic and query-based multi-document summaries and evaluate them using ROUGE evaluation metrics (Lin, 2004) relative to human generated summaries. Results show that, even though query-based summaries perform better than generic ones, they are still not selecting the information that human participants do. In particular, the areas of interest that human summaries display (history, travel information, etc.) are not contained in the query-based summaries. For our future work in automatic image captioning this result suggests that developing the query-based summarizer further and biasing it to account for user-specific requirements will prove worthwhile. 1 Attempts towards automatic generation of image"
W08-1407,W01-0100,0,0.162697,"mprovements. documents, however, the challenge lies in being able to summarize unrestricted web documents. Various multi-document summarization tools have been developed: SUMMA (Saggion and Gaizauskas, 2005), MEAD (Radev et al., 2004), CLASSY (Conroy et al., 2005), CATS (Farzinder et al., 2005) and the system of Boros et al. (2001), to name just a few. These systems generate either generic or query-based summaries or both. Generic summaries address a broad readership whereas query-based summaries are preferred by specific groups of people aiming for quick knowledge gain about specific topics (Mani, 2001). SUMMA and MEAD generate both generic and query-based multi-document summaries. Boros et al. (2001) create only generic summaries, while CLASSY and CATS create only query-based summaries from multiple documents. The performance of these tools has been reported for DUC tasks1 . As Sekine and Nobata (2003) note, although DUC tasks provide a common evaluation standard, they are restricted in topic and are somewhat idealized. For our purposes the summarizer needs to create summaries from unrestricted web input, for which there are no previous performance reports. For this reason we evaluate the p"
W08-1407,W03-0509,0,\N,Missing
W14-4802,aker-etal-2012-light,1,0.843294,"contain translations for terms the user seeks. 3 For example Bess´e et al. (1997) define term as “a lexical unit consisting of one or more than one word which represents a concept inside a domain”; ISO 1087-1:2000 defines term as “verbal designation of a general concept in a specific subject field”. 12 In this section we detail only the domain classification component of BiTES as it is the component that has the most direct implications for the research questions addressed in the paper and as the underlying methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al., 2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin¸a et al., 2012; Aker et al., 2013; Aker et al., 2014b; Aker et al., 2014a). 2.2 Domain Classification 2.2.1 Domain classification scheme Despite the existence of various domain classification schemes, the TaaS project has created its own domain classification for several reasons. First, the TaaS platform requires a suitable classification system which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users working in terminology management and machine translation. The project conducted a user study to i"
W14-4802,P13-1040,1,0.835823,"fine term as “a lexical unit consisting of one or more than one word which represents a concept inside a domain”; ISO 1087-1:2000 defines term as “verbal designation of a general concept in a specific subject field”. 12 In this section we detail only the domain classification component of BiTES as it is the component that has the most direct implications for the research questions addressed in the paper and as the underlying methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al., 2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin¸a et al., 2012; Aker et al., 2013; Aker et al., 2014b; Aker et al., 2014a). 2.2 Domain Classification 2.2.1 Domain classification scheme Despite the existence of various domain classification schemes, the TaaS project has created its own domain classification for several reasons. First, the TaaS platform requires a suitable classification system which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users working in terminology management and machine translation. The project conducted a user study to identify the set of required domains. Various classification systems were considered,"
W14-4802,aker-etal-2014-bootstrapping,1,0.841188,"ical unit consisting of one or more than one word which represents a concept inside a domain”; ISO 1087-1:2000 defines term as “verbal designation of a general concept in a specific subject field”. 12 In this section we detail only the domain classification component of BiTES as it is the component that has the most direct implications for the research questions addressed in the paper and as the underlying methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al., 2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin¸a et al., 2012; Aker et al., 2013; Aker et al., 2014b; Aker et al., 2014a). 2.2 Domain Classification 2.2.1 Domain classification scheme Despite the existence of various domain classification schemes, the TaaS project has created its own domain classification for several reasons. First, the TaaS platform requires a suitable classification system which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users working in terminology management and machine translation. The project conducted a user study to identify the set of required domains. Various classification systems were considered, including the Dewe"
W14-4802,aker-etal-2014-bilingual,1,0.795415,"ical unit consisting of one or more than one word which represents a concept inside a domain”; ISO 1087-1:2000 defines term as “verbal designation of a general concept in a specific subject field”. 12 In this section we detail only the domain classification component of BiTES as it is the component that has the most direct implications for the research questions addressed in the paper and as the underlying methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al., 2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin¸a et al., 2012; Aker et al., 2013; Aker et al., 2014b; Aker et al., 2014a). 2.2 Domain Classification 2.2.1 Domain classification scheme Despite the existence of various domain classification schemes, the TaaS project has created its own domain classification for several reasons. First, the TaaS platform requires a suitable classification system which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users working in terminology management and machine translation. The project conducted a user study to identify the set of required domains. Various classification systems were considered, including the Dewe"
W14-4802,P13-1052,0,0.0626131,"Missing"
W14-4802,drouin-2004-detection,0,0.0434918,"n the information used to extract terms: approaches using purely linguistic information, approaches using purely statistical information and those using combinations of both. An analysis of different approaches is given by Pazienza et al. (2005). For the most part, however, such approaches make the assumption that domain-specific, and perhaps also non-domain-specific, collections of texts are available. Justeson and Katz (1995), for example, assume that term frequency of a limited sort of noun phrases in domainspecific texts is sufficicent to indicate termhood. Others such as Chung (2003) and Drouin (2004) look at statistical contrasts between domain-specific and general comparison or reference corpus. See also 18 (Kim et al., 2009; Marciniak and Mykowiecka, 2013; Kilgariff, 2014). By contrast our approach does not presuppose the existence of documents pre-classified by domain (though we could benefit from this). Rather our approach starts by classifying a document into a domain and then extracting terms from it and assigning them the domain of the document. Utsuro et al. (2006) and Kida et al. (2007) extract terms from web-documents. The domain specification of a term is determined in two stag"
W14-4802,E14-2014,0,0.0204856,"An analysis of different approaches is given by Pazienza et al. (2005). For the most part, however, such approaches make the assumption that domain-specific, and perhaps also non-domain-specific, collections of texts are available. Justeson and Katz (1995), for example, assume that term frequency of a limited sort of noun phrases in domainspecific texts is sufficicent to indicate termhood. Others such as Chung (2003) and Drouin (2004) look at statistical contrasts between domain-specific and general comparison or reference corpus. See also 18 (Kim et al., 2009; Marciniak and Mykowiecka, 2013; Kilgariff, 2014). By contrast our approach does not presuppose the existence of documents pre-classified by domain (though we could benefit from this). Rather our approach starts by classifying a document into a domain and then extracting terms from it and assigning them the domain of the document. Utsuro et al. (2006) and Kida et al. (2007) extract terms from web-documents. The domain specification of a term is determined in two stage approach. In the first stage for a term under inspection web-documents which mention the term are collected. Then these documents are divided into two sets: domain relevant and"
W14-4802,skadina-etal-2012-collecting,1,0.870619,"Missing"
W14-4802,W12-0102,0,0.0205244,"seeks. 3 For example Bess´e et al. (1997) define term as “a lexical unit consisting of one or more than one word which represents a concept inside a domain”; ISO 1087-1:2000 defines term as “verbal designation of a general concept in a specific subject field”. 12 In this section we detail only the domain classification component of BiTES as it is the component that has the most direct implications for the research questions addressed in the paper and as the underlying methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al., 2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin¸a et al., 2012; Aker et al., 2013; Aker et al., 2014b; Aker et al., 2014a). 2.2 Domain Classification 2.2.1 Domain classification scheme Despite the existence of various domain classification schemes, the TaaS project has created its own domain classification for several reasons. First, the TaaS platform requires a suitable classification system which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users working in terminology management and machine translation. The project conducted a user study to identify the set of required domains. Vario"
W14-5406,P08-1032,0,0.0244764,"atasets annotated with a fixed set of labels as training data. For example, Duygulu et al. (2002) investigated learning from images annotated with a set of keywords, posing the problem as a machine translation task between image regions and textual labels. Gupta and Davis (2008) includes some semantic information by incorporating prepositions and comparative adjectives, which also requires manual annotation as no such data is readily available. Recent work has moved beyond learning image annotation from constrained text labels to learning from real world texts, for example from news captions (Feng and Lapata, 2008) and sports articles (Socher and Fei-Fei, 2010). There is also recent interest in treating texts as richer sources of information than just simple bags of keywords, for example with the use of semantic hierarchies for object recognition (Marszałek and Schmid, 2008; Deng et al., 2012b) and the inclusion of attributes for a richer representation (Lampert et al., 2009; Farhadi et al., 2009). Another line of recent work uses textual descriptions of images for various vision tasks, for example for recognizing butterfly species from butterfly descriptions (Wang et al., 2009) and discovering attribut"
W14-5406,padro-stanilovsky-2012-freeling,0,0.0297307,"Missing"
W14-5406,D11-1041,0,\N,Missing
W15-4635,W09-4613,0,0.0161401,"rsection set between the terms/words in the segment and in the comment. len returns the number of entries in the given set. • Jaccard: jaccard = len(I(S, C)) len(U (S, C)) (2) where U (S, C) is the union set between the terms/words in the segment and comment. • NE overlap: N Eoverlap = len(I(S, C)) len(U (S, C)) (3) where I(S, C) is the intersection set between the named entities (NEs) in the segment and in the comment and U (S, C) is the NEs union set. • DISCO 1 + DISCO 2: DISCO (DIStributionally similar words using CO-occurrences) assumes words with similar meaning occur in similar context (Kolb, 2009). Using large text collections such as the BNC corpora or Wikipedia, distributional similarity between words is computed by using a simple context window of size ±3 words for counting co-occurrences. DISCO computes two different similarities between words: DISCO1 and DISCO2. In DISCO1 when two words are directly compared for exact similarity DISCO simply retrieves their word vectors from the large text collections and computes the similarity according to Lin’s information theoretic measure (Lin, 1998). DISCO2 compares words based on their sets of distributional similar words. 3.2.2 Computing S"
W15-4635,P98-2127,0,0.0148338,"lar words using CO-occurrences) assumes words with similar meaning occur in similar context (Kolb, 2009). Using large text collections such as the BNC corpora or Wikipedia, distributional similarity between words is computed by using a simple context window of size ±3 words for counting co-occurrences. DISCO computes two different similarities between words: DISCO1 and DISCO2. In DISCO1 when two words are directly compared for exact similarity DISCO simply retrieves their word vectors from the large text collections and computes the similarity according to Lin’s information theoretic measure (Lin, 1998). DISCO2 compares words based on their sets of distributional similar words. 3.2.2 Computing Similarity Linking Score Using a linear function, we combine the scores of each of these features (cosine to DISCO) to produce a final similarity score for a commentsegment pair: Score = n X f eaturei ∗ weighti (4) i=1 where weighti is the weight associated with the ith feature. The weights are trained based on linear regression using the Weka package and the training data described in the following section. 4 3.2.3 Training Data Obtaining training data requires manual effort and human involvement and"
W15-4635,C98-2122,0,\N,Missing
W16-3605,W16-6610,1,0.619014,"of comments in themselves, and as a target for what an automated system might deliver online. Misra et al. (2015) have created manual summaries of short dialogue sequences, extracted from different conversations on similar issues on debat49 relating it with human judgements on system output quality. If it cannot be validated, the challenge arises to develop a metric better suited to this evaluation need. Our summary corpus has already proved useful in providing insights for system development, and for training and evaluation. We have used group annotations to evaluate a clustering algorithm (Aker et al., 2016a); used back-links to inform the training of a cluster labeling algorithm (Aker et al., 2016b); used the summaries as references in evaluating system outputs (with ROUGE as metric), and to inform human assessors in a task-based system evaluation (Barker et al., 2016). Acknowledgments The authors would like to thank the European Commission for supporting this work, carried out as part of the FP7 SENSEI project, grant reference: FP7-ICT-610916. We would also like to thank all the annotators without whose work the SENSEI corpus would not have been created, Jonathan Foster for his help in recruit"
W16-3605,W16-2802,1,0.902481,"oint or assertion already expressed. Issues are questions on which multiple viewpoints are possible; e.g., the issue of whether reducing bin collection to once every three weeks is a good idea, or whether reducing bin collection will lead to an increase in vermin. Issues are very often implicit, i.e not directly expressed in the comments (e.g., the issue of whether reducing bin collection will lead to an increase in vermin is never explicitly mentioned yet this is clearly what comments 1-4 are addressing). A fuller account of this issue-based framework for analysing reader comment is given in Barker and Gaizauskas (2016). Aside from argumentative content, reader comments exhibit other features as well. For example, commenters may seek clarification about facts (e.g. comment 4 where the commenter asks Is Bury going to provide larger bins for families . . . ?). But these clarifications are typically carried out in the broader context of making an argument, i.e. advancing evidence to support a viewpoint. Comments may also express jokes or emotion, though these too are often in the service of advancing some viewpoint (e.g. sarcasm or as in comments 4 and 6 emotive terms like lamebrained and crazy clearly indicati"
W16-3605,L16-1494,1,0.816562,"judgements on system output quality. If it cannot be validated, the challenge arises to develop a metric better suited to this evaluation need. Our summary corpus has already proved useful in providing insights for system development, and for training and evaluation. We have used group annotations to evaluate a clustering algorithm (Aker et al., 2016a); used back-links to inform the training of a cluster labeling algorithm (Aker et al., 2016b); used the summaries as references in evaluating system outputs (with ROUGE as metric), and to inform human assessors in a task-based system evaluation (Barker et al., 2016). Acknowledgments The authors would like to thank the European Commission for supporting this work, carried out as part of the FP7 SENSEI project, grant reference: FP7-ICT-610916. We would also like to thank all the annotators without whose work the SENSEI corpus would not have been created, Jonathan Foster for his help in recruiting annotators and The Guardian for allowing us access to their materials. Finally, thanks to our anonymous reviewers whose comments and suggestions have helped us to improve the paper. Even so, there are limitations to the work done which give pointers to further wor"
W16-3605,W14-2106,0,0.0607293,"ctive summary. A significant weakness of such summaries is that they fail to capture the essential argument-oriented nature of these multiway conversations, since single comments taken from topically distinct clusters do not reflect the argumentative structure of the conversation. In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015). If such elements and relations can be automatically extracted then they could serve as the basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. To the best of our knowledge, however, none have proposed how, given an analysis in terms of their theory, one might produce a summary of a full reader comment set. Researchers are beginning to explore how to generate summaries of extended ar"
W16-3605,W15-4631,0,0.029459,"ch summaries is that they fail to capture the essential argument-oriented nature of these multiway conversations, since single comments taken from topically distinct clusters do not reflect the argumentative structure of the conversation. In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015). If such elements and relations can be automatically extracted then they could serve as the basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. To the best of our knowledge, however, none have proposed how, given an analysis in terms of their theory, one might produce a summary of a full reader comment set. Researchers are beginning to explore how to generate summaries of extended argumentative conversations in social media, su"
W16-3605,W02-0406,0,0.189098,"Missing"
W16-3605,W04-1013,0,0.0550558,"Missing"
W16-3605,N15-1046,0,0.194431,"hey fail to capture the essential argument-oriented nature of these multiway conversations, since single comments taken from topically distinct clusters do not reflect the argumentative structure of the conversation. In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015). If such elements and relations can be automatically extracted then they could serve as the basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. To the best of our knowledge, however, none have proposed how, given an analysis in terms of their theory, one might produce a summary of a full reader comment set. Researchers are beginning to explore how to generate summaries of extended argumentative conversations in social media, such as those found in"
W16-3605,D08-1081,0,0.0695176,"Missing"
W16-6610,aker-etal-2014-bootstrapping,1,0.783301,"ferences. From this pool, 20 comment clusters were randomly selected. Table 1 provides statistics on the two evaluation sets. 3 Method Our labeling approach is supervised and we refer to it as SCL (Supervised Cluster Labeler). Using the entire set of manually annotated Guardian articles, we collect training data to build a regression model for extracting labels for automatic clusters. To do this we first extract terms2 from the arti2 Terms are noun phrase-like word sequences and are extracted using POS-tag grammars such as NN NN. We use the automatically generated POS-tag grammars reported by Aker et al. (2014). cle as well as comments and represent them with features. Each term is assigned a score between 0 and 1, where 0 indicates a term that is a poor label for a cluster, and 1 a term that makes an excellent label. We obtain the score using human summaries generated for the Guardian articles. For these human summaries we have the information about which sentences in the summary links to which human clusters. If the question is to answer whether the term X is a good label for the Y cluster, then we collect the sentences from the human summaries that are linked to that Y cluster and compare that te"
W16-6610,W16-3605,1,0.738625,"present the topic (Lau et al., 2011; Mei et al., 2007). In most studies, such textual labels are still extractive, i.e. the methods rely on labels being present within the textual sources (Lau et al., 2011; Mei et al., 2007). To overcome this limitation, many studies use external resources, most notably Wikipedia, for deriving topic labels. Hulpus et al. (2013), for example, present a graph-based approach to labeling using DBpedia concepts. An advantage of such approaches is the potential to provide labels that are more abstract, and hence more akin to labels humans might produce. Aker et al. (2016) apply such an approach to the online news domain, and evaluate it via an information retrieval task (similar to the evaluation in Aletras et al. (2014)). However, low recall figures were reported due to the abstractedness of the labels. Joty et al. (2013) also argue that external resources like Wikipedia titles are too broad for their e-mail and blog domain, as shown by the fact that none of the human-created labels in their development set appears in a Wikipedia title. Chang et al. (2015) use human generated labels for social media posts in Google+, suggesting that post-internal information"
W16-6610,P14-1023,0,0.00994184,"raph): the number of occurrences of a term in the article sentences 2–6. • #Term in sentences after 6 (main text body): the number of occurrences of a term in the final portion of the article (from the 7th sentence to the end of the article). • #Term in the entire article: the number of occurrences of a term in the entire article. • Article centroid similarity: the cosine similarity (Salton and Lesk, 1968) between the term and the article centroid. The similarity is based on Word2Vec word embeddings: each word is represented by a 400-dimensional word embedding. We use the vectors published by Baroni et al. (2014). To compute the similarity of term:document pair, we remove stop-words and punctuation from each, then query for each remaining word’s vector representation using the Word2Vec, and create a sum of the word vectors. We use the resulting sum vectors to compute their cosine similarity. Features In the cluster labeling approach we use several features extracted from the news article and the comments. To investigate to what extent our intuition about the relevance of the news article for labelling comment clusters is justified and craft features, we analysed a set of 1.7K Guardian news articles al"
W16-6610,P11-1154,0,0.0310324,". Identifying topics in comment streams is vitally important to providing an overview of what readers are saying. However, merely clustering comments is not enough: topic clusters should also be given labels that accurately reflect their content, and that are accessible to users. Producing “good labels” is challenging, as what constitutes a good label is not well defined. A 61 common method of labelling topic clusters with the top-n key terms characterising the topic is reported as less suitable than generating “textual labels” not consisting of key terms, to meaningfully represent the topic (Lau et al., 2011; Mei et al., 2007). In most studies, such textual labels are still extractive, i.e. the methods rely on labels being present within the textual sources (Lau et al., 2011; Mei et al., 2007). To overcome this limitation, many studies use external resources, most notably Wikipedia, for deriving topic labels. Hulpus et al. (2013), for example, present a graph-based approach to labeling using DBpedia concepts. An advantage of such approaches is the potential to provide labels that are more abstract, and hence more akin to labels humans might produce. Aker et al. (2016) apply such an approach to th"
W17-5108,D09-1040,0,0.0122197,"y the statistical machine translation (SMT) methods, we explore the idea of comparable corpora to obtain argumentative data sets. A comparable corpus contains pairs of documents written in two different languages. The document pairs usually share the same topic but the documents in a pair are not necessarily entirely translations of each other. However, they may share few sentences that are translation of each other. Related work has shown the usefulness of such corpora for training SMT system for under-resourced languages, cross-lingual information retrieval and assisted machine translation (Marton et al., 2009; Aker et al., 2013; Hashemi and Shakery, 2014; Kumano et al., 2007; Sharoff et al., 2006; Aker et al., 2012; Skadin¸a et al., 2012; Munteanu and Marcu, 2005, 2002; Rapp, 1999). Given the difficulty and the cost of creating an argumentative corpus, extracting arguments from comparable corpora by automatically mapping arguments from the source language corpus to their translations in the target language seems an attractive avenue. In this work, we take a preliminary step to evaluating the viability of such an approach. This paper reports on the first Mandarin argumentative corpus that is obtain"
W17-5108,W14-2109,0,0.0857698,"for the research community. Overall the paper contributes the following: • We introduce a new task of creating multilingual argumentative corpora based on the idea of mapping argumentative sentences between articles that are comparable. Our manually generated data can be used to evaluate performance of automatic approaches. • We establish and evaluate the possibility of obtaining argumentative corpora in any language with lower cost. To this end we propose a first baseline system for mapping English argumentative sentences into Mandarin. 2 Data We work with the argumentative data published by Aharoni et al. (2014). The data contains the annotation of English Wikipedia articles for topic specific claims called Context Dependent Claims (CDCs) and premises referred as Context Dependent Evidence (CDE). A topic is a short phrase and frames the discussion within the article (Levy et al., 2014). A CDC is a general, concise statement that directly supports or contests the given topic (Levy et al., 2014). A CDE is a text segment that directly supports a claim in the context of the topic (Rinott et al., 2015). The data released in 2014 contains 1392 labeled claims for 33 different topics, and 1291 labeled premis"
W17-5108,W02-1037,0,0.0413344,"Missing"
W17-5108,aker-etal-2012-light,1,0.82443,"entative data sets. A comparable corpus contains pairs of documents written in two different languages. The document pairs usually share the same topic but the documents in a pair are not necessarily entirely translations of each other. However, they may share few sentences that are translation of each other. Related work has shown the usefulness of such corpora for training SMT system for under-resourced languages, cross-lingual information retrieval and assisted machine translation (Marton et al., 2009; Aker et al., 2013; Hashemi and Shakery, 2014; Kumano et al., 2007; Sharoff et al., 2006; Aker et al., 2012; Skadin¸a et al., 2012; Munteanu and Marcu, 2005, 2002; Rapp, 1999). Given the difficulty and the cost of creating an argumentative corpus, extracting arguments from comparable corpora by automatically mapping arguments from the source language corpus to their translations in the target language seems an attractive avenue. In this work, we take a preliminary step to evaluating the viability of such an approach. This paper reports on the first Mandarin argumentative corpus that is obtained using comparable corpora. We make use of the existing corpora, in which English documents are annotated f"
W17-5108,J05-4003,0,0.0700437,"ntains pairs of documents written in two different languages. The document pairs usually share the same topic but the documents in a pair are not necessarily entirely translations of each other. However, they may share few sentences that are translation of each other. Related work has shown the usefulness of such corpora for training SMT system for under-resourced languages, cross-lingual information retrieval and assisted machine translation (Marton et al., 2009; Aker et al., 2013; Hashemi and Shakery, 2014; Kumano et al., 2007; Sharoff et al., 2006; Aker et al., 2012; Skadin¸a et al., 2012; Munteanu and Marcu, 2005, 2002; Rapp, 1999). Given the difficulty and the cost of creating an argumentative corpus, extracting arguments from comparable corpora by automatically mapping arguments from the source language corpus to their translations in the target language seems an attractive avenue. In this work, we take a preliminary step to evaluating the viability of such an approach. This paper reports on the first Mandarin argumentative corpus that is obtained using comparable corpora. We make use of the existing corpora, in which English documents are annotated for arguments, i.e. where sentences within the doc"
W17-5108,P13-1040,1,0.840589,"hine translation (SMT) methods, we explore the idea of comparable corpora to obtain argumentative data sets. A comparable corpus contains pairs of documents written in two different languages. The document pairs usually share the same topic but the documents in a pair are not necessarily entirely translations of each other. However, they may share few sentences that are translation of each other. Related work has shown the usefulness of such corpora for training SMT system for under-resourced languages, cross-lingual information retrieval and assisted machine translation (Marton et al., 2009; Aker et al., 2013; Hashemi and Shakery, 2014; Kumano et al., 2007; Sharoff et al., 2006; Aker et al., 2012; Skadin¸a et al., 2012; Munteanu and Marcu, 2005, 2002; Rapp, 1999). Given the difficulty and the cost of creating an argumentative corpus, extracting arguments from comparable corpora by automatically mapping arguments from the source language corpus to their translations in the target language seems an attractive avenue. In this work, we take a preliminary step to evaluating the viability of such an approach. This paper reports on the first Mandarin argumentative corpus that is obtained using comparable"
W17-5108,W15-0503,0,0.015001,"m Source to Target Languages Ahmet Aker Huangpan ZHANG University of Duisburg-Essen University of Duisburg-Essen a.aker@is.inf.uni-due.de huangpan.zhang@stud.uni-due.de Abstract Argument mining is a new, but rapidly growing area of research within Computational Linguistic that has gained a great popularity in the last five years. For instance, since 2014 the meeting of the Association for Computational Linguistics (ACL) is hosting a workshop specifically dedicated to Argument Mining.1 Current studies report methods for argument mining in legal documents (Reed et al., 2008), persuasive essays (Nguyen and Litman, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015), discussion fora (Swanson et al., 2015), political debates (Lippi and Torroni, 2016) and news (Sardianos et al., 2015; AlKhatib et al., 2016). In terms of methodology, supervised machine learning is a central technique used in all these studies. This assumes the availability of data sets – argumentative texts – to train and test the argument mining models. Such data sets are readily available in English and – although in comparably smaller quantities – in very few European languages such as German or Italian. Languages other than th"
W17-5108,C16-1324,0,0.046209,"Missing"
W17-5108,W14-2107,0,0.0607381,"Missing"
W17-5108,P99-1067,0,0.111602,"n in two different languages. The document pairs usually share the same topic but the documents in a pair are not necessarily entirely translations of each other. However, they may share few sentences that are translation of each other. Related work has shown the usefulness of such corpora for training SMT system for under-resourced languages, cross-lingual information retrieval and assisted machine translation (Marton et al., 2009; Aker et al., 2013; Hashemi and Shakery, 2014; Kumano et al., 2007; Sharoff et al., 2006; Aker et al., 2012; Skadin¸a et al., 2012; Munteanu and Marcu, 2005, 2002; Rapp, 1999). Given the difficulty and the cost of creating an argumentative corpus, extracting arguments from comparable corpora by automatically mapping arguments from the source language corpus to their translations in the target language seems an attractive avenue. In this work, we take a preliminary step to evaluating the viability of such an approach. This paper reports on the first Mandarin argumentative corpus that is obtained using comparable corpora. We make use of the existing corpora, in which English documents are annotated for arguments, i.e. where sentences within the documents are marked a"
W17-5108,N16-2003,0,0.0240507,"fora (Swanson et al., 2015), political debates (Lippi and Torroni, 2016) and news (Sardianos et al., 2015; AlKhatib et al., 2016). In terms of methodology, supervised machine learning is a central technique used in all these studies. This assumes the availability of data sets – argumentative texts – to train and test the argument mining models. Such data sets are readily available in English and – although in comparably smaller quantities – in very few European languages such as German or Italian. Languages other than these are currently neglected. We are only aware of the study conducted by Chow (2016) who manually annotated Chinese news editorial paragraphs about whether they contain an argument or not. However, the boundaries of the arguments and their claims and premises were not annotated. Due to this lack of data the research and development of argumentation mining outside English and few European languages is very limited, rendering multi-lingual argument mining and language independent argument based search impossible. In this research we aim to fill this gap. We aim to map existing argument annotations from a source language to a target language. For this purpose an ideal situation"
W17-5108,reed-etal-2008-language,0,0.0290565,"Projection of Argumentative Corpora from Source to Target Languages Ahmet Aker Huangpan ZHANG University of Duisburg-Essen University of Duisburg-Essen a.aker@is.inf.uni-due.de huangpan.zhang@stud.uni-due.de Abstract Argument mining is a new, but rapidly growing area of research within Computational Linguistic that has gained a great popularity in the last five years. For instance, since 2014 the meeting of the Association for Computational Linguistics (ACL) is hosting a workshop specifically dedicated to Argument Mining.1 Current studies report methods for argument mining in legal documents (Reed et al., 2008), persuasive essays (Nguyen and Litman, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015), discussion fora (Swanson et al., 2015), political debates (Lippi and Torroni, 2016) and news (Sardianos et al., 2015; AlKhatib et al., 2016). In terms of methodology, supervised machine learning is a central technique used in all these studies. This assumes the availability of data sets – argumentative texts – to train and test the argument mining models. Such data sets are readily available in English and – although in comparably smaller quantities – in very few European languages such"
W17-5108,P07-2045,0,0.00805994,"target languages. For this reason, an automatic approach to argument matching is mandatory in order to achieve larger data set sizes for multi-lingual argument mining approaches. In addition, successfull automatation of matching would open up the possibiity of creation argumentative corpora from any less-resourced language for which comparable corpora are available. To evaluate the viability of an automatic approach and create a first benchmark we also performed a simple automatic mapping of English CDCs and CDEs into Mandarin. Our approach relies on automatic machine translation using MOSES (Koehn et al., 2007) and Google translate4 . We trained MOSES using the publicly available parallel corpora from the HIT IR-lab5 . For each English article we first translate all CDCs and CDEs into Mandarin. Next, we compare each of those translated argumentative pieces of text with every sentence from the corresponding Mandarin article. Our comparison is based on cosine similarity without stop-word removal. To perform tokenisation we used THULAC6 an efficient Chi• No article to match an English one: In this case there is no Mandarin article to match an English one. In most cases this is due to the topic of the a"
W17-5108,D15-1050,0,0.213169,"of Duisburg-Essen University of Duisburg-Essen a.aker@is.inf.uni-due.de huangpan.zhang@stud.uni-due.de Abstract Argument mining is a new, but rapidly growing area of research within Computational Linguistic that has gained a great popularity in the last five years. For instance, since 2014 the meeting of the Association for Computational Linguistics (ACL) is hosting a workshop specifically dedicated to Argument Mining.1 Current studies report methods for argument mining in legal documents (Reed et al., 2008), persuasive essays (Nguyen and Litman, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015), discussion fora (Swanson et al., 2015), political debates (Lippi and Torroni, 2016) and news (Sardianos et al., 2015; AlKhatib et al., 2016). In terms of methodology, supervised machine learning is a central technique used in all these studies. This assumes the availability of data sets – argumentative texts – to train and test the argument mining models. Such data sets are readily available in English and – although in comparably smaller quantities – in very few European languages such as German or Italian. Languages other than these are currently neglected. We are only aware of the study c"
W17-5108,2007.tmi-papers.12,0,0.0345458,"idea of comparable corpora to obtain argumentative data sets. A comparable corpus contains pairs of documents written in two different languages. The document pairs usually share the same topic but the documents in a pair are not necessarily entirely translations of each other. However, they may share few sentences that are translation of each other. Related work has shown the usefulness of such corpora for training SMT system for under-resourced languages, cross-lingual information retrieval and assisted machine translation (Marton et al., 2009; Aker et al., 2013; Hashemi and Shakery, 2014; Kumano et al., 2007; Sharoff et al., 2006; Aker et al., 2012; Skadin¸a et al., 2012; Munteanu and Marcu, 2005, 2002; Rapp, 1999). Given the difficulty and the cost of creating an argumentative corpus, extracting arguments from comparable corpora by automatically mapping arguments from the source language corpus to their translations in the target language seems an attractive avenue. In this work, we take a preliminary step to evaluating the viability of such an approach. This paper reports on the first Mandarin argumentative corpus that is obtained using comparable corpora. We make use of the existing corpora, i"
W17-5108,P06-2095,0,0.0475547,"orpora to obtain argumentative data sets. A comparable corpus contains pairs of documents written in two different languages. The document pairs usually share the same topic but the documents in a pair are not necessarily entirely translations of each other. However, they may share few sentences that are translation of each other. Related work has shown the usefulness of such corpora for training SMT system for under-resourced languages, cross-lingual information retrieval and assisted machine translation (Marton et al., 2009; Aker et al., 2013; Hashemi and Shakery, 2014; Kumano et al., 2007; Sharoff et al., 2006; Aker et al., 2012; Skadin¸a et al., 2012; Munteanu and Marcu, 2005, 2002; Rapp, 1999). Given the difficulty and the cost of creating an argumentative corpus, extracting arguments from comparable corpora by automatically mapping arguments from the source language corpus to their translations in the target language seems an attractive avenue. In this work, we take a preliminary step to evaluating the viability of such an approach. This paper reports on the first Mandarin argumentative corpus that is obtained using comparable corpora. We make use of the existing corpora, in which English docume"
W17-5108,C14-1141,0,0.149158,"an ZHANG University of Duisburg-Essen University of Duisburg-Essen a.aker@is.inf.uni-due.de huangpan.zhang@stud.uni-due.de Abstract Argument mining is a new, but rapidly growing area of research within Computational Linguistic that has gained a great popularity in the last five years. For instance, since 2014 the meeting of the Association for Computational Linguistics (ACL) is hosting a workshop specifically dedicated to Argument Mining.1 Current studies report methods for argument mining in legal documents (Reed et al., 2008), persuasive essays (Nguyen and Litman, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015), discussion fora (Swanson et al., 2015), political debates (Lippi and Torroni, 2016) and news (Sardianos et al., 2015; AlKhatib et al., 2016). In terms of methodology, supervised machine learning is a central technique used in all these studies. This assumes the availability of data sets – argumentative texts – to train and test the argument mining models. Such data sets are readily available in English and – although in comparably smaller quantities – in very few European languages such as German or Italian. Languages other than these are currently neglected. We are onl"
W17-5108,skadina-etal-2012-collecting,1,0.899169,"Missing"
W17-5108,W15-4631,0,0.0203669,"17. 2017 Association for Computational Linguistics gumentative corpus of Mandarin, also containing projected argumentative sentences from English to Mandarin comparable articles. ments are annotated for arguments and where every sentence in the source document had a translation in the target document. In this case one could easily map any argumentative annotation from the source language to the target one. However, parallel data are sparse. In particular there exist no annotated argumentative corpora with parallel documents in any other language, expect the one described by Peldszus and Stede (2015) who report argumentative microtexts corpora in German that is also translated into English. Instead, inspired by the statistical machine translation (SMT) methods, we explore the idea of comparable corpora to obtain argumentative data sets. A comparable corpus contains pairs of documents written in two different languages. The document pairs usually share the same topic but the documents in a pair are not necessarily entirely translations of each other. However, they may share few sentences that are translation of each other. Related work has shown the usefulness of such corpora for training"
W17-5112,P12-2041,0,0.00927411,"a process that involves the following steps, each of which is a research area in itself addressed by several studies: identifying argumentative segments in text (Moens et al., 2007; Wyner et al., 2012; Park and Cardie, 2014; Goudas et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Swanson et al., 2015; Sardianos et al., 2015; Lawrence et al., 2014), clustering reˇ curring arguments (Boltuˇzi´c and Snajder, 2015; Misra et al., 2015), classification of premises as supporting (pro) or rejecting (contra) (Stab and Gurevych, 2014b; Nguyen and Litman, 2015), determining argument structure (Cabrio and Villata, 2012; Lawrence et al., 2014; Ghosh et al., 2014) and mapping arguments into pre-defined argument schemas (Feng and Hirst, 2011). In terms of methods all these studies rely on supervised machine learning. Among the different classification approaches applied Support Vector Machines, Na¨ıve Bayes and Logistic Regression are the most common ones. Also different feature types have been investigated for the different steps of the argument mining task. Among the features types the prominent ones are structural, lexical, syntactic, indicators and contextual features as summarized by Stab and Gurevych (20"
W17-5112,P11-1099,0,0.0441203,"ying argumentative segments in text (Moens et al., 2007; Wyner et al., 2012; Park and Cardie, 2014; Goudas et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Swanson et al., 2015; Sardianos et al., 2015; Lawrence et al., 2014), clustering reˇ curring arguments (Boltuˇzi´c and Snajder, 2015; Misra et al., 2015), classification of premises as supporting (pro) or rejecting (contra) (Stab and Gurevych, 2014b; Nguyen and Litman, 2015), determining argument structure (Cabrio and Villata, 2012; Lawrence et al., 2014; Ghosh et al., 2014) and mapping arguments into pre-defined argument schemas (Feng and Hirst, 2011). In terms of methods all these studies rely on supervised machine learning. Among the different classification approaches applied Support Vector Machines, Na¨ıve Bayes and Logistic Regression are the most common ones. Also different feature types have been investigated for the different steps of the argument mining task. Among the features types the prominent ones are structural, lexical, syntactic, indicators and contextual features as summarized by Stab and Gurevych (2014b). Given this variety of work on argument mining time is ripe for an extensive comparative analysis of the performance o"
W17-5112,W14-2106,0,0.0105445,"h of which is a research area in itself addressed by several studies: identifying argumentative segments in text (Moens et al., 2007; Wyner et al., 2012; Park and Cardie, 2014; Goudas et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Swanson et al., 2015; Sardianos et al., 2015; Lawrence et al., 2014), clustering reˇ curring arguments (Boltuˇzi´c and Snajder, 2015; Misra et al., 2015), classification of premises as supporting (pro) or rejecting (contra) (Stab and Gurevych, 2014b; Nguyen and Litman, 2015), determining argument structure (Cabrio and Villata, 2012; Lawrence et al., 2014; Ghosh et al., 2014) and mapping arguments into pre-defined argument schemas (Feng and Hirst, 2011). In terms of methods all these studies rely on supervised machine learning. Among the different classification approaches applied Support Vector Machines, Na¨ıve Bayes and Logistic Regression are the most common ones. Also different feature types have been investigated for the different steps of the argument mining task. Among the features types the prominent ones are structural, lexical, syntactic, indicators and contextual features as summarized by Stab and Gurevych (2014b). Given this variety of work on argument"
W17-5112,D14-1181,0,0.00245328,"Missing"
W17-5112,W14-2111,0,0.0869607,"is.inf.uni-due.de alfred.sliwa.92, yuan.ma, ruishen.liu, niravkumar.borad seyedeh.ziyaei, mina.ghobadi@stud.uni-due.de Abstract online products (Wyner et al., 2012), social media (Goudas et al., 2014) and news articles (Sardianos et al., 2015). Argument mining is a process that involves the following steps, each of which is a research area in itself addressed by several studies: identifying argumentative segments in text (Moens et al., 2007; Wyner et al., 2012; Park and Cardie, 2014; Goudas et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Swanson et al., 2015; Sardianos et al., 2015; Lawrence et al., 2014), clustering reˇ curring arguments (Boltuˇzi´c and Snajder, 2015; Misra et al., 2015), classification of premises as supporting (pro) or rejecting (contra) (Stab and Gurevych, 2014b; Nguyen and Litman, 2015), determining argument structure (Cabrio and Villata, 2012; Lawrence et al., 2014; Ghosh et al., 2014) and mapping arguments into pre-defined argument schemas (Feng and Hirst, 2011). In terms of methods all these studies rely on supervised machine learning. Among the different classification approaches applied Support Vector Machines, Na¨ıve Bayes and Logistic Regression are the most common"
W17-5112,C14-1141,0,0.163973,"Niravkumar Borad, Seyedeh Fatemeh Ziyaei, Mina Ghbadi University of Duisburg-Essen a.aker@is.inf.uni-due.de alfred.sliwa.92, yuan.ma, ruishen.liu, niravkumar.borad seyedeh.ziyaei, mina.ghobadi@stud.uni-due.de Abstract online products (Wyner et al., 2012), social media (Goudas et al., 2014) and news articles (Sardianos et al., 2015). Argument mining is a process that involves the following steps, each of which is a research area in itself addressed by several studies: identifying argumentative segments in text (Moens et al., 2007; Wyner et al., 2012; Park and Cardie, 2014; Goudas et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Swanson et al., 2015; Sardianos et al., 2015; Lawrence et al., 2014), clustering reˇ curring arguments (Boltuˇzi´c and Snajder, 2015; Misra et al., 2015), classification of premises as supporting (pro) or rejecting (contra) (Stab and Gurevych, 2014b; Nguyen and Litman, 2015), determining argument structure (Cabrio and Villata, 2012; Lawrence et al., 2014; Ghosh et al., 2014) and mapping arguments into pre-defined argument schemas (Feng and Hirst, 2011). In terms of methods all these studies rely on supervised machine learning. Among the different classification appro"
W17-5112,N15-1046,0,0.011769,", mina.ghobadi@stud.uni-due.de Abstract online products (Wyner et al., 2012), social media (Goudas et al., 2014) and news articles (Sardianos et al., 2015). Argument mining is a process that involves the following steps, each of which is a research area in itself addressed by several studies: identifying argumentative segments in text (Moens et al., 2007; Wyner et al., 2012; Park and Cardie, 2014; Goudas et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Swanson et al., 2015; Sardianos et al., 2015; Lawrence et al., 2014), clustering reˇ curring arguments (Boltuˇzi´c and Snajder, 2015; Misra et al., 2015), classification of premises as supporting (pro) or rejecting (contra) (Stab and Gurevych, 2014b; Nguyen and Litman, 2015), determining argument structure (Cabrio and Villata, 2012; Lawrence et al., 2014; Ghosh et al., 2014) and mapping arguments into pre-defined argument schemas (Feng and Hirst, 2011). In terms of methods all these studies rely on supervised machine learning. Among the different classification approaches applied Support Vector Machines, Na¨ıve Bayes and Logistic Regression are the most common ones. Also different feature types have been investigated for the different steps of"
W17-5112,W15-0503,0,0.0385016,"news articles (Sardianos et al., 2015). Argument mining is a process that involves the following steps, each of which is a research area in itself addressed by several studies: identifying argumentative segments in text (Moens et al., 2007; Wyner et al., 2012; Park and Cardie, 2014; Goudas et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Swanson et al., 2015; Sardianos et al., 2015; Lawrence et al., 2014), clustering reˇ curring arguments (Boltuˇzi´c and Snajder, 2015; Misra et al., 2015), classification of premises as supporting (pro) or rejecting (contra) (Stab and Gurevych, 2014b; Nguyen and Litman, 2015), determining argument structure (Cabrio and Villata, 2012; Lawrence et al., 2014; Ghosh et al., 2014) and mapping arguments into pre-defined argument schemas (Feng and Hirst, 2011). In terms of methods all these studies rely on supervised machine learning. Among the different classification approaches applied Support Vector Machines, Na¨ıve Bayes and Logistic Regression are the most common ones. Also different feature types have been investigated for the different steps of the argument mining task. Among the features types the prominent ones are structural, lexical, syntactic, indicators and"
W17-5112,W14-2105,0,0.0356875,"et Aker, Alfred Sliwa, Yuan Ma, Ruishen Liu Niravkumar Borad, Seyedeh Fatemeh Ziyaei, Mina Ghbadi University of Duisburg-Essen a.aker@is.inf.uni-due.de alfred.sliwa.92, yuan.ma, ruishen.liu, niravkumar.borad seyedeh.ziyaei, mina.ghobadi@stud.uni-due.de Abstract online products (Wyner et al., 2012), social media (Goudas et al., 2014) and news articles (Sardianos et al., 2015). Argument mining is a process that involves the following steps, each of which is a research area in itself addressed by several studies: identifying argumentative segments in text (Moens et al., 2007; Wyner et al., 2012; Park and Cardie, 2014; Goudas et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Swanson et al., 2015; Sardianos et al., 2015; Lawrence et al., 2014), clustering reˇ curring arguments (Boltuˇzi´c and Snajder, 2015; Misra et al., 2015), classification of premises as supporting (pro) or rejecting (contra) (Stab and Gurevych, 2014b; Nguyen and Litman, 2015), determining argument structure (Cabrio and Villata, 2012; Lawrence et al., 2014; Ghosh et al., 2014) and mapping arguments into pre-defined argument schemas (Feng and Hirst, 2011). In terms of methods all these studies rely on supervised machine learning."
W17-5112,reed-etal-2008-language,0,0.00957166,"e an implementation of argument mining into different applications such as argument based search. 1 Introduction Argument mining refers to the automatic extraction of arguments from natural texts. An argument consists of a claim (also referred to as the conclusion of the argument) and several pieces of evidence called premises that support or reject the claim (Lippi and Torroni, 2016). As a research area argument mining has seen a rapid progress in the last three-to-five years (Lippi and Torroni, 2015). Current studies report methods for argument mining in legal documents (Moens et al., 2007; Reed et al., 2008), persuasive essays (Nguyen and Litman, 2015; Stab and Gurevych, 2014b), Wikipedia articles (Levy et al., 2014), user comments (Park and Cardie, 2014), 91 Proceedings of the 4th Workshop on Argument Mining, pages 91–96 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Google News corpus consisting of 3 million 300dimension English word vectors 1 . respect to two argument mining tasks: (1) identifying argumentative segments in text, i.e. the classification of textual units (usually sentences) into claims, premises or none and (2) the prediction of argument"
W17-5112,C14-1142,0,0.511391,"(Goudas et al., 2014) and news articles (Sardianos et al., 2015). Argument mining is a process that involves the following steps, each of which is a research area in itself addressed by several studies: identifying argumentative segments in text (Moens et al., 2007; Wyner et al., 2012; Park and Cardie, 2014; Goudas et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Swanson et al., 2015; Sardianos et al., 2015; Lawrence et al., 2014), clustering reˇ curring arguments (Boltuˇzi´c and Snajder, 2015; Misra et al., 2015), classification of premises as supporting (pro) or rejecting (contra) (Stab and Gurevych, 2014b; Nguyen and Litman, 2015), determining argument structure (Cabrio and Villata, 2012; Lawrence et al., 2014; Ghosh et al., 2014) and mapping arguments into pre-defined argument schemas (Feng and Hirst, 2011). In terms of methods all these studies rely on supervised machine learning. Among the different classification approaches applied Support Vector Machines, Na¨ıve Bayes and Logistic Regression are the most common ones. Also different feature types have been investigated for the different steps of the argument mining task. Among the features types the prominent ones are structural, lexical,"
W17-5112,D14-1006,0,0.126345,"(Goudas et al., 2014) and news articles (Sardianos et al., 2015). Argument mining is a process that involves the following steps, each of which is a research area in itself addressed by several studies: identifying argumentative segments in text (Moens et al., 2007; Wyner et al., 2012; Park and Cardie, 2014; Goudas et al., 2014; Levy et al., 2014; Lippi and Torroni, 2015; Swanson et al., 2015; Sardianos et al., 2015; Lawrence et al., 2014), clustering reˇ curring arguments (Boltuˇzi´c and Snajder, 2015; Misra et al., 2015), classification of premises as supporting (pro) or rejecting (contra) (Stab and Gurevych, 2014b; Nguyen and Litman, 2015), determining argument structure (Cabrio and Villata, 2012; Lawrence et al., 2014; Ghosh et al., 2014) and mapping arguments into pre-defined argument schemas (Feng and Hirst, 2011). In terms of methods all these studies rely on supervised machine learning. Among the different classification approaches applied Support Vector Machines, Na¨ıve Bayes and Logistic Regression are the most common ones. Also different feature types have been investigated for the different steps of the argument mining task. Among the features types the prominent ones are structural, lexical,"
W17-5112,W15-4631,0,0.0310242,"Missing"
W18-5505,P05-1065,0,0.072812,"system set by Alexa.com (a subsidiary of Amazon) that audits and publishes the frequency of visits on various websites. The Alexa ranking is the geometric mean of reach and page views, averaged over a period of three months. Google PageRank is a link analysis algorithm that assigns a numerical weight to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of measuring its relative importance within the set. Twitter Popularity is calculated as an average of the scores for the following two metrics: 1 Article popularity 2.3 Ease of reading As described by Schwarm and Ostendorf (2005) the readability level is used to characterize the educational level a reader needs to understand a text. This topic has been in research since 1930 and several automatic solutions have been proposed to determine the readability level of an input text (Vajjala and Meurers, 2013; Xia et al., 2016; Schwarm and Ostendorf, 2005). The core concept in these studies is to use machine learning along with feature engineering covering lexical, structural, and heuristic based features. We followed this core concept and used Random Forest with features inspired by earlier studies. This approach achieved h"
W18-5505,W13-2907,0,0.0467341,"Missing"
W18-5505,W16-0502,0,0.0221317,"Missing"
W18-5518,W18-2501,0,0.0259331,"Missing"
W18-5518,N18-1202,0,0.0134935,"extracting few keywords from the claim and use them to find candidate sentences in the Wikipedia dump. Searching for the Candidate Sentences We use three types of queries to search for candidate sentences for a claim: • Type 1: For each keyword we split the keyword phrase into individual words and create a query that searches within the Wikipedia page titles requiring all the individual words to be found. Extracting Keywords We use Named Entity Recognition (NER), Constituency Parsing and Dependency Parsing to extract keywords from each claim. For NER we use the neural network model created by Peters et al. (2018). We use all found named entities as keywords. For the Constituency Parsing we use the neural network model created by Stern et al. (2017). We extract all NP tagged phrases from the first two recursion layers as keywords because we found that this finds mostly subjects and objects of a sentence. These two neural networks both use the AllenNLP library (Gardner et al., 2018). For the dependency parsing we use the Standford Dependency Parser (Chen and Manning, 2014). We extract all subject and object phrases as keywords. The NE recognition is our main source for keywords extraction while the othe"
W18-5518,P17-1076,0,0.0131162,"s We use three types of queries to search for candidate sentences for a claim: • Type 1: For each keyword we split the keyword phrase into individual words and create a query that searches within the Wikipedia page titles requiring all the individual words to be found. Extracting Keywords We use Named Entity Recognition (NER), Constituency Parsing and Dependency Parsing to extract keywords from each claim. For NER we use the neural network model created by Peters et al. (2018). We use all found named entities as keywords. For the Constituency Parsing we use the neural network model created by Stern et al. (2017). We extract all NP tagged phrases from the first two recursion layers as keywords because we found that this finds mostly subjects and objects of a sentence. These two neural networks both use the AllenNLP library (Gardner et al., 2018). For the dependency parsing we use the Standford Dependency Parser (Chen and Manning, 2014). We extract all subject and object phrases as keywords. The NE recognition is our main source for keywords extraction while the other two systems provide additional keywords that either have not been found by the NER or that are not named entities in the first place. Ex"
W18-5518,N18-1074,0,0.0376577,"Missing"
W18-5518,N03-1033,0,0.0309184,"Missing"
W18-5518,D15-1075,0,0.0210601,"Missing"
W18-5518,D14-1082,0,0.0149882,"R), Constituency Parsing and Dependency Parsing to extract keywords from each claim. For NER we use the neural network model created by Peters et al. (2018). We use all found named entities as keywords. For the Constituency Parsing we use the neural network model created by Stern et al. (2017). We extract all NP tagged phrases from the first two recursion layers as keywords because we found that this finds mostly subjects and objects of a sentence. These two neural networks both use the AllenNLP library (Gardner et al., 2018). For the dependency parsing we use the Standford Dependency Parser (Chen and Manning, 2014). We extract all subject and object phrases as keywords. The NE recognition is our main source for keywords extraction while the other two systems provide additional keywords that either have not been found by the NER or that are not named entities in the first place. Example of the keywords being extracted from claims shown in Table 2 are shown in Table 1. • Type 2: We split all keywords into individual words and combine those into one query searching within the Wikipedia page titles to find sentences on those pages where as many words as possible match the title of the page. • Type 3: We com"
