2020.coling-main.208,N19-1423,0,0.079282,"t−1 ), with tokens coming from a vocabulary, xi ∈ V. If x = (x1 , . . . , x|x |) represents the text sequence, pθ typically takes the form |x| pθ (x) = Πt=1 pθ (xt |x1 , . . . , xt−1 ). If p∗ (x) denotes the reference distribution and D denotes a finite set of text sequences from p∗ , TGM estimates parameters θ by minimizing the following objective function: (j) L(pθ , D) = − |D ||x | X X (j) (j) (j) (j) log pθ (xt |x1 , . . . , xi , . . . , xt−1 ). (1) j=1 t=1 Notice that TGM can be a non-neural model (e.g., n-gram LM) and based on nontraditional LM objective (e.g., masked language modeling (Devlin et al., 2019; Song et al., 2019)). In this survey, we focus primarily on TGMs for English that are neural and based on traditional LM objective, as they are successful in generating coherent paragraphs of English text. 2.2 Generating text from TGM Given a sub-sequence (prefix), x1:k ∼ p∗ , the task of generating text from TGM is to use pθ ˆ k+1:N ∼ pθ (.|x1:k ) such that the resulting completion to conditionally decode a continuation, x (x1 , . . . , xk , x ˆk+1 , . . . , x ˆN ) resembles a sample from p∗ (Welleck et al., 2020). In a news article generation task, the prefix can be headlines and the contin"
2020.coling-main.208,2020.emnlp-demos.25,0,0.119591,"Missing"
2020.coling-main.208,P18-1082,0,0.368552,"e understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area. 1 Introduction Current state-of-the-art text generative models (TGMs) excel in producing text that approaches the style of human language, especially in terms of grammaticality, fluency, coherency, and usage of real world knowledge (Radford et al., 2019; Zellers et al., 2019; Keskar et al., 2019; Bakhtin et al., 2020; Brown et al., 2020). TGMs are useful in a wide variety of applications, including story generation (Fan et al., 2018), conversational response generation (Zhang et al., 2020), code auto-completion (Solaiman et al., 2019), and radiology report generation (Liu et al., 2019a). However, TGMs can also be misused for fake news generation (Zellers et al., 2019; Brown et al., 2020; Uchendu et al., 2020), fake product reviews generation (Adelani et al., 2020), and spamming/phishing. (Weiss, 2019). Thus, it is important to build tools that can minimize the threats posed by the misuse of TGMs. The commonly used approach to combat the threats posed by the misuse of TGMs is to formulate the problem of distinguishing text"
2020.coling-main.208,P19-3019,0,0.401161,"e platform (email client, social media) on which TGM is applied (Solaiman et al., 2019); (ii) data-efficient, that is, needs as few examples as possible from the TGM used by the attacker (Zellers et al., 2019); (iii) generalizable, that is, detects text generated by different modeling choices of the TGM used by the attacker such as model architecture, TGM training data, TGM conditioning prompt length, model size, and text decoding method (Solaiman et al., 2019; Bakhtin et al., 2020; Uchendu et al., 2020); and (iv) interpretable, that is, detector decisions need to be understandable to humans (Gehrmann et al., 2019); and (v) robust, that is, detector can handle adversarial examples (Wolff, 2020). Given the importance of this problem, there has been a flurry of research recently from both NLP and ML communities on This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2296 Proceedings of the 28th International Conference on Computational Linguistics, pages 2296–2309 Barcelona, Spain (Online), December 8-13, 2020 building useful detectors. However, there is currently no work that provides a literature review of ex"
2020.coling-main.208,J95-2003,0,0.311964,"phical and grammatical errors (10) and (11). We note that such errors (especially spelling errors) are not unusual in online reviews, including those by humans. (10) Once they are on they aren’t wrinkled or lose they shape. (11) Had to unplug thing to get the hard drive to work. Would rather have don batteries in the olden days.. Incoherence: There are 3 false positive reviews that seem incoherent. The movie review (12) switches the focus of the discourse between actors (Sophia and Duchovny) and story line in an incoherent fashion, which violates the theory of centering in discourse analysis (Grosz et al., 1995; Gehrmann et al., 2019). (12) ... Sophia Loren plays ‘Marion’ a ‘showgirl’ that is picked on by the establishment for her wild style. ... Duchovny’s character is also ‘On the line’ in the business world. ... The storyline is so intriguing and unpredictable. ... Sophia Loren’s acting is just awesome and her wardrobe is just perfect! If you love sex and nud**y, you will be greatly pleased. 6 Future Research Directions In this section, we discuss a set of future research directions, which can help in building useful detectors. 6.1 Leveraging auxiliary signals Existing detectors do not exploit au"
2020.coling-main.208,P16-2057,0,0.0121447,"ardrobe is just perfect! If you love sex and nud**y, you will be greatly pleased. 6 Future Research Directions In this section, we discuss a set of future research directions, which can help in building useful detectors. 6.1 Leveraging auxiliary signals Existing detectors do not exploit auxiliary signals about the textual source. 6 For example, the RoBERTa detector studied in §5 ignores the auxiliary signals about the review (e.g., helpfulness) and the product (e.g., description). Such auxiliary signals can be complementary to linguistic signals from the textual source for the detection task (Hovy, 2016; Solaiman et al., 2019). Given the rapidly evolving research 6 Concurrent with our work, Tan et al., (2020) propose a detector that spots machine generated news articles by utilizing news body, images, and captions associated with the news articles. 2304 in building intelligent TGMs that narrows the gap between machine and human distribution of natural language text, auxiliary signals could play a crucial role in mitigating the threats posed by TGMs. 6.2 Assessing veracity of the text Existing detectors have an assumption that the fake text is determined by the source (e.g., TGM) that generat"
2020.coling-main.208,2020.acl-main.164,0,0.078475,"meters (117M, 345M, 762M, 1542M) and different sampling techniques (pure sampling, top-k sampling, and top-p sampling). They observe that generations from the larger GPT-2 models are difficult to detect compared to that of the smaller models, which indicates that the larger the TGM, the closer the style of the generated text with that of human written text. Top-k samples are easier to detect while nucleus samples are harder to detect. This result stems from the fact that top-k sampler typically over-generates common words, leaving statistical anomalies that are easily spotted by the detector (Ippolito et al., 2020). Additionally, Solaiman et al., (2019) fine-tune the GPT-2 model on Amazon product reviews and show that the text generated by fine-tuned GPT-2 model is harder to detect as fine-tuned domain specific TGMs are more human-like than general purpose TGM (i.e., the original 2300 GPT-2 model). Detecting machine configuration: Tay et al., (2020) study the extent to which different modeling choices (decoding method, TGM model size, prompt length) leave artifacts (detectable signatures that arise from modeling choices) in the generated text. They propose the task of identifying the TGM modeling choice"
2020.coling-main.208,W19-5333,0,0.040777,"Missing"
2020.coling-main.208,2020.cl-2.8,0,0.0213049,"304 in building intelligent TGMs that narrows the gap between machine and human distribution of natural language text, auxiliary signals could play a crucial role in mitigating the threats posed by TGMs. 6.2 Assessing veracity of the text Existing detectors have an assumption that the fake text is determined by the source (e.g., TGM) that generated the text. This assumption does not hold true in two practical scenarios: (i) real text autogenerated in a process similar to that of fake text, and (ii) adversaries creating fake text by modifying articles originating from legitimate human sources. Schuster et al., (2020) show that existing detectors perform poorly in these two scenarios as they rely too much on distributional features, which cannot help in distinguishing texts from similar sources. Hence, we call for more research on detectors that assess the veracity of machine generated text by consulting external sources, like knowledge bases (Thorne and Vlachos, 2018) and diffusion network (Vosoughi et al., 2018), instead of relying only on the source. 6.3 Building generalizable detectors Existing detectors exhibit poor cross-domain accuracy, that is, they are not generalizable to different publication fo"
2020.coling-main.208,P19-1355,0,0.0276575,"eddit, and news sources). As an exception, GROVER (Zellers et al., 2019) is trained on millions of news article only. Such trained TGMs can also be fine-tuned on a domain-specific corpus for the LM task to generate text that matches the respective domain reasonably. For example, Adelani et al., (2020) fine-tune the GPT-2 model on the specific domain of product reviews to generate fake reviews, which mimics the style of a human review. Training cost: Training TGMs with billions of parameters on millions of documents requires a huge computational budget (Zellers et al., 2019), high energy cost (Strubell et al., 2019), and long training time (Brown et al., 2020). Unfortunately, it is not yet a standard practice to report financial (vs. energy vs. computational) budget in every research publication. This makes it hard for us to perform TGM training feasibility studies. One exception is the work done by Zellers et al., (2019), where they explicitly mention that their proposed TGM model, GROVER, took two weeks of training with a cost of $25K (including the cost of data collection). We note that even though this may be an expensive budget, it is by no means outside the reach of even low-resource organizations,"
2020.coling-main.208,P19-1159,0,0.0271804,"e., W is the smallest subset with P p (x|x1 , . . . , xt−1 ) &gt;= p. Thus, the number of candidate tokens considered varies dynamically θ x∈W depending on the context, and the resulting text is reasonably natural with less repetitions. Recently, Massarelli et al., (2020) show that top-k and top-p sampler tend to generate more nonfactual sentences, as corroborated by Wikipedia. 2.3 Social impacts of TGMs Bias: Unsurprisingly, a TGM can capture and amplify the societal biases (over-generalized beliefs about a particular group of people, e.g., Group X are bad drivers) present in the training data (Sun et al., 2019; Nadeem et al., 2020). Solaiman et al., (2019) and Brown et al., (2020) show that TGMs reflect gender bias (e.g., favoring males over females), racial bias (e.g., favoring white over black people), and religious bias (e.g., favoring Christians over Muslims). Although TGMs can be used as a tool to study how patterns in the training data can translate to these unintended biases in the model outputs (Solaiman et al., 2019), the biases can cause harm to the people in relevant groups in many ways (Crawford, 2017). Beneficial usage: TGMs are used to create task-specific systems, such as question an"
2020.coling-main.208,2020.emnlp-main.163,0,0.0337319,"Directions In this section, we discuss a set of future research directions, which can help in building useful detectors. 6.1 Leveraging auxiliary signals Existing detectors do not exploit auxiliary signals about the textual source. 6 For example, the RoBERTa detector studied in §5 ignores the auxiliary signals about the review (e.g., helpfulness) and the product (e.g., description). Such auxiliary signals can be complementary to linguistic signals from the textual source for the detection task (Hovy, 2016; Solaiman et al., 2019). Given the rapidly evolving research 6 Concurrent with our work, Tan et al., (2020) propose a detector that spots machine generated news articles by utilizing news body, images, and captions associated with the news articles. 2304 in building intelligent TGMs that narrows the gap between machine and human distribution of natural language text, auxiliary signals could play a crucial role in mitigating the threats posed by TGMs. 6.2 Assessing veracity of the text Existing detectors have an assumption that the fake text is determined by the source (e.g., TGM) that generated the text. This assumption does not hold true in two practical scenarios: (i) real text autogenerated in a"
2020.coling-main.208,2020.acl-main.25,0,0.0752992,"written text. Top-k samples are easier to detect while nucleus samples are harder to detect. This result stems from the fact that top-k sampler typically over-generates common words, leaving statistical anomalies that are easily spotted by the detector (Ippolito et al., 2020). Additionally, Solaiman et al., (2019) fine-tune the GPT-2 model on Amazon product reviews and show that the text generated by fine-tuned GPT-2 model is harder to detect as fine-tuned domain specific TGMs are more human-like than general purpose TGM (i.e., the original 2300 GPT-2 model). Detecting machine configuration: Tay et al., (2020) study the extent to which different modeling choices (decoding method, TGM model size, prompt length) leave artifacts (detectable signatures that arise from modeling choices) in the generated text. They propose the task of identifying the TGM modeling choice given the text generated by TGM. They show that a classifier can be trained to predict the modeling choice well beyond the chance level, which ascertains that text generated by TGM may be more sensitive to TGM modeling choices than previously thought. They also find that the proposed detection task of identifying text generated by differe"
2020.coling-main.208,C18-1283,0,0.0243412,"ext. This assumption does not hold true in two practical scenarios: (i) real text autogenerated in a process similar to that of fake text, and (ii) adversaries creating fake text by modifying articles originating from legitimate human sources. Schuster et al., (2020) show that existing detectors perform poorly in these two scenarios as they rely too much on distributional features, which cannot help in distinguishing texts from similar sources. Hence, we call for more research on detectors that assess the veracity of machine generated text by consulting external sources, like knowledge bases (Thorne and Vlachos, 2018) and diffusion network (Vosoughi et al., 2018), instead of relying only on the source. 6.3 Building generalizable detectors Existing detectors exhibit poor cross-domain accuracy, that is, they are not generalizable to different publication formats (Wikipedia, books, news sources) (Bakhtin et al., 2019). Beyond publication formats and topics (e.g., politics, sports), the detector should also transfer to unseen TGM settings such as model architecture, different decoding methods (e.g., top-k, top-p), model size, different prefix lengths, and training data (Bakhtin et al., 2020; Uchendu et al., 20"
2020.coling-main.208,2020.emnlp-main.673,0,0.263591,"t approaches the style of human language, especially in terms of grammaticality, fluency, coherency, and usage of real world knowledge (Radford et al., 2019; Zellers et al., 2019; Keskar et al., 2019; Bakhtin et al., 2020; Brown et al., 2020). TGMs are useful in a wide variety of applications, including story generation (Fan et al., 2018), conversational response generation (Zhang et al., 2020), code auto-completion (Solaiman et al., 2019), and radiology report generation (Liu et al., 2019a). However, TGMs can also be misused for fake news generation (Zellers et al., 2019; Brown et al., 2020; Uchendu et al., 2020), fake product reviews generation (Adelani et al., 2020), and spamming/phishing. (Weiss, 2019). Thus, it is important to build tools that can minimize the threats posed by the misuse of TGMs. The commonly used approach to combat the threats posed by the misuse of TGMs is to formulate the problem of distinguishing text generated by TGMs and human written text as a classification task. The classifier, henceforth called detector, can be used to automatically remove machine generated text from online platforms such as social media, e-commerce, email clients, and government forums, when the intenti"
2020.coling-main.208,D19-1221,0,0.055982,"Missing"
2020.coling-main.208,2020.acl-demos.30,0,0.118394,"error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area. 1 Introduction Current state-of-the-art text generative models (TGMs) excel in producing text that approaches the style of human language, especially in terms of grammaticality, fluency, coherency, and usage of real world knowledge (Radford et al., 2019; Zellers et al., 2019; Keskar et al., 2019; Bakhtin et al., 2020; Brown et al., 2020). TGMs are useful in a wide variety of applications, including story generation (Fan et al., 2018), conversational response generation (Zhang et al., 2020), code auto-completion (Solaiman et al., 2019), and radiology report generation (Liu et al., 2019a). However, TGMs can also be misused for fake news generation (Zellers et al., 2019; Brown et al., 2020; Uchendu et al., 2020), fake product reviews generation (Adelani et al., 2020), and spamming/phishing. (Weiss, 2019). Thus, it is important to build tools that can minimize the threats posed by the misuse of TGMs. The commonly used approach to combat the threats posed by the misuse of TGMs is to formulate the problem of distinguishing text generated by TGMs and human written text as a classifica"
2020.emnlp-main.472,cotterell-callison-burch-2014-multi,0,0.0310173,"9.00 92.00 — — 92.50 ASTD 72.00 66.00 73.00 — — 78.50 SemEv 63.00 60.00 69.00 — — 70.50 ASC — — — — 76.67 90.86 OFF — — — 90.51 — 91.47 Table 7: Evaluation of MARBERT on external tasks. 10 Related Work Dialectal Arabic Data and Models. Much of the early work on Arabic varieties focused on collecting data for main varieties such as Egyptian and Levantine (Diab et al., 2010; Elfardy and Diab, 2012; Al-Sabbagh and Girju, 2012; Sadat et al., 2014; Zaidan and Callison-Burch, 2011). Many works developed models for detecting 2-3 dialects (Elfardy and Diab, 2013; Zaidan and CallisonBurch, 2011, 2014; Cotterell and Callison-Burch, 2014). Larger datasets, mainly based on Twitter, were recently introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018; Bouamor et al., 2019a). Our dataset is orders of magnitude larger than other datasets, F1P N was defined by SemEval-2017 as the macro F1 over the positive and negative classes only while neglecting the neutral class. 20 more balanced, and more diverse. It is also, by far, the most fine-grained. Geolocation, Variation, and MTL. Research on geolocation is also relevant, whether based on text (Roller et al., 2012; Graham et al., 2014; Han et al.,"
2020.emnlp-main.472,2020.acl-main.79,0,0.0330337,"ic research has shown how language varies across geographical regions, even for areas as small as different parts of the same city (Labov, 1964; Trudgill, 1974). These pioneering studies often used field work data from a handful of individuals and focused on small sets of carefully chosen features, often phonological. Inspired by this early work, researchers have used geographically tagged social media data from hundreds of thousands of users to predict user location (Paul and Dredze, 2011; Amitay et al., 2004; Han et al., 2014; Rahimi et al., 2017; Huang and Carley, 2019b; Tian et al., 2020; Zhong et al., 2020) or to develop language identification tools (Lui and Baldwin, 2012; Zubiaga et al., 2016; Jurgens et al., 2017a; Dunn and Adams, 2020). Whether it is possible at all to predict the micro-varieties 2 of the same general 1 Our labeled data and models will be available at: https: //github.com/UBC-NLP/microdialects. 2 We use micro-variety and micro-dialect interchangeably. language is a question that remains, to the best of our knowledge, unanswered. In this work, our goal is to investigate this specific question by introducing the novel task of Micro-Dialect Identification (MDI). Given a single"
2020.emnlp-main.472,W14-5307,0,0.0393652,"Missing"
2020.emnlp-main.472,W19-4637,1,0.856044,"Missing"
2020.ngt-1.20,2012.eamt-1.60,0,0.0114488,"Open Parallel Corpus Project (OPUS) (Tiedemann, 2012). OPUS7 contains more than 2.7 billion parallel sentences in 90 languages. The specific corpus we extracted consists of data from multiple domains and sources including: ParaCrawl project (Espl`a-Gomis et al., 2019), EUbookshop (Skadin¸sˇ et al., 2014), Tilde Model (Rozis and Skadinˇs, 2017), translation memories (DGT) (Steinberger et al., 2013), OpenSubtitles (Creutz, 2018), SciELO Parallel (Soares et al., 2018), JRC-Acquis Multilingual (Steinberger et al., 2006), Tanzil (Zarrabi-Zadeh, 2007), Eu6 7 roparl Parallel (Koehn, 2005), TED 2013 (Cettolo et al., 2012), Wikipedia (Wołk and Marasek, 2014), Tatoeba 8 , QCRI Educational Domain (Abdelali et al., 2014), GNOME localization files, 9 Global Voices, 10 KDE4, 11 , Ubuntu, 12 and Multilingual Bible (Christodouloupoulos and Steedman, 2015). To train our models, we extract more than 77.7M parallel (i.e., English-Portuguese) sentences from the whole collection. The extracted dataset comprises more than 1.5B English tokens and 1.4B Portuguese tokens. More details about the training dataset are given in Table 2. 3.3 Pre-Processing Pre-processing is an important step in building any MT model as it can signi"
2020.ngt-1.20,L18-1218,0,0.0165703,"fficient English-Portuguese MT models that can possibly work across different text domains, we make use of a large dataset of parallel English-Portuguese sentences extracted from the Open Parallel Corpus Project (OPUS) (Tiedemann, 2012). OPUS7 contains more than 2.7 billion parallel sentences in 90 languages. The specific corpus we extracted consists of data from multiple domains and sources including: ParaCrawl project (Espl`a-Gomis et al., 2019), EUbookshop (Skadin¸sˇ et al., 2014), Tilde Model (Rozis and Skadinˇs, 2017), translation memories (DGT) (Steinberger et al., 2013), OpenSubtitles (Creutz, 2018), SciELO Parallel (Soares et al., 2018), JRC-Acquis Multilingual (Steinberger et al., 2006), Tanzil (Zarrabi-Zadeh, 2007), Eu6 7 roparl Parallel (Koehn, 2005), TED 2013 (Cettolo et al., 2012), Wikipedia (Wołk and Marasek, 2014), Tatoeba 8 , QCRI Educational Domain (Abdelali et al., 2014), GNOME localization files, 9 Global Voices, 10 KDE4, 11 , Ubuntu, 12 and Multilingual Bible (Christodouloupoulos and Steedman, 2015). To train our models, we extract more than 77.7M parallel (i.e., English-Portuguese) sentences from the whole collection. The extracted dataset comprises more than 1.5B English t"
2020.ngt-1.20,W19-6721,0,0.0264513,"Missing"
2020.ngt-1.20,N18-1170,0,0.0803552,"pted Portuguese Translations English sentence Accepted Portuguese Translations Table 1: English sentences with their Portuguese translation samples from shared task training split. More recently, advances in neural machine translation (NMT) have spurred interest in paraphrase generation (Sutskever et al., 2014; Luong and Manning, 2015; Aharoni et al., 2019). For example, Prakash et al. (2016) employ a stacked residual LSTM network to learn a sequence-to-sequence model on paraphrase data. A parpahrase model with adversarial training is presented by (Li et al., 2017). Wieting and Gimpel (2017); Iyyer et al. (2018) propose a translation-based paraphrasing system, which is based on NTM to translate one side of a parallel corpus. Paraphrase generation with pivot NMT is used by (Mallinson et al., 2017; Yu et al., 2018). 3 2 Related Work 3.1 We focus our related work overview on the task of paraphrase generation and its intersection with machine translation. Paraphrasing is the task of expressing the same textual units (e.g. sentence) with alternative forms using different words while keeping the original meaning intact. 5 Over the last few years, MT has been the dominant approach for paraphrase generation."
2020.ngt-1.20,D14-1181,0,0.00533185,"Missing"
2020.ngt-1.20,2005.mtsummit-papers.11,0,0.0175018,"nces extracted from the Open Parallel Corpus Project (OPUS) (Tiedemann, 2012). OPUS7 contains more than 2.7 billion parallel sentences in 90 languages. The specific corpus we extracted consists of data from multiple domains and sources including: ParaCrawl project (Espl`a-Gomis et al., 2019), EUbookshop (Skadin¸sˇ et al., 2014), Tilde Model (Rozis and Skadinˇs, 2017), translation memories (DGT) (Steinberger et al., 2013), OpenSubtitles (Creutz, 2018), SciELO Parallel (Soares et al., 2018), JRC-Acquis Multilingual (Steinberger et al., 2006), Tanzil (Zarrabi-Zadeh, 2007), Eu6 7 roparl Parallel (Koehn, 2005), TED 2013 (Cettolo et al., 2012), Wikipedia (Wołk and Marasek, 2014), Tatoeba 8 , QCRI Educational Domain (Abdelali et al., 2014), GNOME localization files, 9 Global Voices, 10 KDE4, 11 , Ubuntu, 12 and Multilingual Bible (Christodouloupoulos and Steedman, 2015). To train our models, we extract more than 77.7M parallel (i.e., English-Portuguese) sentences from the whole collection. The extracted dataset comprises more than 1.5B English tokens and 1.4B Portuguese tokens. More details about the training dataset are given in Table 2. 3.3 Pre-Processing Pre-processing is an important step in buil"
2020.ngt-1.20,abdelali-etal-2014-amara,0,0.0181619,"llel sentences in 90 languages. The specific corpus we extracted consists of data from multiple domains and sources including: ParaCrawl project (Espl`a-Gomis et al., 2019), EUbookshop (Skadin¸sˇ et al., 2014), Tilde Model (Rozis and Skadinˇs, 2017), translation memories (DGT) (Steinberger et al., 2013), OpenSubtitles (Creutz, 2018), SciELO Parallel (Soares et al., 2018), JRC-Acquis Multilingual (Steinberger et al., 2006), Tanzil (Zarrabi-Zadeh, 2007), Eu6 7 roparl Parallel (Koehn, 2005), TED 2013 (Cettolo et al., 2012), Wikipedia (Wołk and Marasek, 2014), Tatoeba 8 , QCRI Educational Domain (Abdelali et al., 2014), GNOME localization files, 9 Global Voices, 10 KDE4, 11 , Ubuntu, 12 and Multilingual Bible (Christodouloupoulos and Steedman, 2015). To train our models, we extract more than 77.7M parallel (i.e., English-Portuguese) sentences from the whole collection. The extracted dataset comprises more than 1.5B English tokens and 1.4B Portuguese tokens. More details about the training dataset are given in Table 2. 3.3 Pre-Processing Pre-processing is an important step in building any MT model as it can significantly affect the end results. We remove punctuation and tokenize all data with the Moses token"
2020.ngt-1.20,N19-1388,0,0.0222545,"k. In Section 3, we describe the data we use for both training and fine-tuning our models. Section 4 presents the proposed MT system. Section 5 describes our different methods. We discuss our results in Section 6, and conclude in Section 7. English sentence Accepted Portuguese Translations English sentence Accepted Portuguese Translations Table 1: English sentences with their Portuguese translation samples from shared task training split. More recently, advances in neural machine translation (NMT) have spurred interest in paraphrase generation (Sutskever et al., 2014; Luong and Manning, 2015; Aharoni et al., 2019). For example, Prakash et al. (2016) employ a stacked residual LSTM network to learn a sequence-to-sequence model on paraphrase data. A parpahrase model with adversarial training is presented by (Li et al., 2017). Wieting and Gimpel (2017); Iyyer et al. (2018) propose a translation-based paraphrasing system, which is based on NTM to translate one side of a parallel corpus. Paraphrase generation with pivot NMT is used by (Mallinson et al., 2017; Yu et al., 2018). 3 2 Related Work 3.1 We focus our related work overview on the task of paraphrase generation and its intersection with machine transl"
2020.ngt-1.20,P05-1074,0,0.0584396,"t NMT is used by (Mallinson et al., 2017; Yu et al., 2018). 3 2 Related Work 3.1 We focus our related work overview on the task of paraphrase generation and its intersection with machine translation. Paraphrasing is the task of expressing the same textual units (e.g. sentence) with alternative forms using different words while keeping the original meaning intact. 5 Over the last few years, MT has been the dominant approach for paraphrase generation. For instance, Barzilay and McKeown (2001); Pang et al. (2003) use multiple translations of the same text to train a paraphrase system. Similarly, Bannard and Callison-Burch (2005) use an MT phrase table to mapping an English sentences to various non-English sentences. 5 https://dictionary.cambridge.org/ dictionary/english/paraphrase is my explanation clear? - minha explicac¸a˜ o est´a clara? - minha explicac¸a˜ o e´ clara? - a minha explicac¸a˜ o e´ clara? - est´a clara minha explicac¸a˜ o? - minha explanac¸a˜ o est´a clara? - e´ clara minha explicac¸a˜ o? you look so pretty! - vocˆe est´a t˜ao linda! - vocˆe est´a t˜ao bonita! - vocˆe est´a muito linda! - vocˆe est´a muito bonita! - vocˆe parece t˜ao linda! - vocˆe parece t˜ao bonita! Data Shared task data As part of"
2020.ngt-1.20,2015.iwslt-evaluation.11,0,0.00905814,"f overview of related work. In Section 3, we describe the data we use for both training and fine-tuning our models. Section 4 presents the proposed MT system. Section 5 describes our different methods. We discuss our results in Section 6, and conclude in Section 7. English sentence Accepted Portuguese Translations English sentence Accepted Portuguese Translations Table 1: English sentences with their Portuguese translation samples from shared task training split. More recently, advances in neural machine translation (NMT) have spurred interest in paraphrase generation (Sutskever et al., 2014; Luong and Manning, 2015; Aharoni et al., 2019). For example, Prakash et al. (2016) employ a stacked residual LSTM network to learn a sequence-to-sequence model on paraphrase data. A parpahrase model with adversarial training is presented by (Li et al., 2017). Wieting and Gimpel (2017); Iyyer et al. (2018) propose a translation-based paraphrasing system, which is based on NTM to translate one side of a parallel corpus. Paraphrase generation with pivot NMT is used by (Mallinson et al., 2017; Yu et al., 2018). 3 2 Related Work 3.1 We focus our related work overview on the task of paraphrase generation and its intersect"
2020.ngt-1.20,P01-1008,0,0.37244,"slation-based paraphrasing system, which is based on NTM to translate one side of a parallel corpus. Paraphrase generation with pivot NMT is used by (Mallinson et al., 2017; Yu et al., 2018). 3 2 Related Work 3.1 We focus our related work overview on the task of paraphrase generation and its intersection with machine translation. Paraphrasing is the task of expressing the same textual units (e.g. sentence) with alternative forms using different words while keeping the original meaning intact. 5 Over the last few years, MT has been the dominant approach for paraphrase generation. For instance, Barzilay and McKeown (2001); Pang et al. (2003) use multiple translations of the same text to train a paraphrase system. Similarly, Bannard and Callison-Burch (2005) use an MT phrase table to mapping an English sentences to various non-English sentences. 5 https://dictionary.cambridge.org/ dictionary/english/paraphrase is my explanation clear? - minha explicac¸a˜ o est´a clara? - minha explicac¸a˜ o e´ clara? - a minha explicac¸a˜ o e´ clara? - est´a clara minha explicac¸a˜ o? - minha explanac¸a˜ o est´a clara? - e´ clara minha explicac¸a˜ o? you look so pretty! - vocˆe est´a t˜ao linda! - vocˆe est´a t˜ao bonita! - voc"
2020.ngt-1.20,D15-1166,0,0.0130008,"basic model for 5 epochs. 14 We will refer to the model resulting from this fine-tuning process simply as the finetuned model. In this section, we first describe the architecture of our models. We then explain the different ways we train the models on various subsets of the data. 4.1 Architecture Our models are mainly based on a Convolutional Neural Network (CNN) architecture (Kim, 2014; Gehring et al., 2017). This convolutional architecture exploits BPE (Sennrich et al., 2016). The architecture is as follows: 20 layers in the encoder and 20 layers in the decoder, a multiplicative attention (Luong et al., 2015) in every decoder layer, a kernel width of 3 for both the encoder and the decoder, a hidden size 512, and an embedding size of 512, and 256 for the encoder and decoder layers respectively. We use a Fairseq implementation (Ott et al., 2019). 4.2 Basic En↔Pt Models We trained two MT models, English-to-Portuguese (En→Pt) and Portuguese-to-English (Pt→En), on 4 V100 GPUs, following the setup described in Ott et al. (2018). For both models, the learning rate was set to 0.25, a dropout of 0.2, and a maximum tokens of 4, 000 for each mini-batch. We train our models on the 77.7M parallel sentences of"
2020.ngt-1.20,E17-1083,0,0.012223,"e recently, advances in neural machine translation (NMT) have spurred interest in paraphrase generation (Sutskever et al., 2014; Luong and Manning, 2015; Aharoni et al., 2019). For example, Prakash et al. (2016) employ a stacked residual LSTM network to learn a sequence-to-sequence model on paraphrase data. A parpahrase model with adversarial training is presented by (Li et al., 2017). Wieting and Gimpel (2017); Iyyer et al. (2018) propose a translation-based paraphrasing system, which is based on NTM to translate one side of a parallel corpus. Paraphrase generation with pivot NMT is used by (Mallinson et al., 2017; Yu et al., 2018). 3 2 Related Work 3.1 We focus our related work overview on the task of paraphrase generation and its intersection with machine translation. Paraphrasing is the task of expressing the same textual units (e.g. sentence) with alternative forms using different words while keeping the original meaning intact. 5 Over the last few years, MT has been the dominant approach for paraphrase generation. For instance, Barzilay and McKeown (2001); Pang et al. (2003) use multiple translations of the same text to train a paraphrase system. Similarly, Bannard and Callison-Burch (2005) use an"
2020.ngt-1.20,2020.ngt-1.28,0,0.344647,"on. However, many possible translations of a given input text can be acceptable. This situation is common in online language learning applications such as Duolingo,1 Babbel2 , and Busuu.3 In applications of this type, learning happens via translation-based activities while evaluation is performed by comparing learners’ responses to a large set of human acceptable translations. Figure 1 shows an example of a typical situation extracted from the Duolingo application. The main set up of the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE 2020) (Mayhew et al., 2020) is such that one starts with a set of English sentences (prompts) and then generates highcoverage sets of plausible translations in the five target languages: Portuguese, Hungarian, Japanese, Korean, and Vietnamese. For instance, if we want to translate the English (En) sentence “is my explanation clear?” to Portuguese (Pt), all the translated Portuguese sentences illustrated in Table 1 would be acceptable.4 Limited training data. One challenge for training a sufficiently effective model we faced is the limited size of the source training data released by organizers (4, 000 source English sen"
2020.ngt-1.20,N19-4009,0,0.136531,"e models on various subsets of the data. 4.1 Architecture Our models are mainly based on a Convolutional Neural Network (CNN) architecture (Kim, 2014; Gehring et al., 2017). This convolutional architecture exploits BPE (Sennrich et al., 2016). The architecture is as follows: 20 layers in the encoder and 20 layers in the decoder, a multiplicative attention (Luong et al., 2015) in every decoder layer, a kernel width of 3 for both the encoder and the decoder, a hidden size 512, and an embedding size of 512, and 256 for the encoder and decoder layers respectively. We use a Fairseq implementation (Ott et al., 2019). 4.2 Basic En↔Pt Models We trained two MT models, English-to-Portuguese (En→Pt) and Portuguese-to-English (Pt→En), on 4 V100 GPUs, following the setup described in Ott et al. (2018). For both models, the learning rate was set to 0.25, a dropout of 0.2, and a maximum tokens of 4, 000 for each mini-batch. We train our models on the 77.7M parallel sentences of the OPUS dataset described in Section 3. Validation is performed on the development data from STAPLE 2020 (Mayhew et al., 2020). 4.3 5 In order to enhance the 1-to-n En-Pt translation, we propose three methods based on the previously discu"
2020.ngt-1.20,W18-6301,0,0.0241109,"volutional architecture exploits BPE (Sennrich et al., 2016). The architecture is as follows: 20 layers in the encoder and 20 layers in the decoder, a multiplicative attention (Luong et al., 2015) in every decoder layer, a kernel width of 3 for both the encoder and the decoder, a hidden size 512, and an embedding size of 512, and 256 for the encoder and decoder layers respectively. We use a Fairseq implementation (Ott et al., 2019). 4.2 Basic En↔Pt Models We trained two MT models, English-to-Portuguese (En→Pt) and Portuguese-to-English (Pt→En), on 4 V100 GPUs, following the setup described in Ott et al. (2018). For both models, the learning rate was set to 0.25, a dropout of 0.2, and a maximum tokens of 4, 000 for each mini-batch. We train our models on the 77.7M parallel sentences of the OPUS dataset described in Section 3. Validation is performed on the development data from STAPLE 2020 (Mayhew et al., 2020). 4.3 5 In order to enhance the 1-to-n En-Pt translation, we propose three methods based on the previously discussed MT models (see section 4). These methods are n-Best prediction, multi-checkpoint translation, and paraphrasing. 5.1 5.2 Paraphrasing Paraphrasing is an effective data augmentati"
2020.ngt-1.20,N03-1024,0,0.213548,"stem, which is based on NTM to translate one side of a parallel corpus. Paraphrase generation with pivot NMT is used by (Mallinson et al., 2017; Yu et al., 2018). 3 2 Related Work 3.1 We focus our related work overview on the task of paraphrase generation and its intersection with machine translation. Paraphrasing is the task of expressing the same textual units (e.g. sentence) with alternative forms using different words while keeping the original meaning intact. 5 Over the last few years, MT has been the dominant approach for paraphrase generation. For instance, Barzilay and McKeown (2001); Pang et al. (2003) use multiple translations of the same text to train a paraphrase system. Similarly, Bannard and Callison-Burch (2005) use an MT phrase table to mapping an English sentences to various non-English sentences. 5 https://dictionary.cambridge.org/ dictionary/english/paraphrase is my explanation clear? - minha explicac¸a˜ o est´a clara? - minha explicac¸a˜ o e´ clara? - a minha explicac¸a˜ o e´ clara? - est´a clara minha explicac¸a˜ o? - minha explanac¸a˜ o est´a clara? - e´ clara minha explicac¸a˜ o? you look so pretty! - vocˆe est´a t˜ao linda! - vocˆe est´a t˜ao bonita! - vocˆe est´a muito linda"
2020.ngt-1.20,N18-2082,0,0.0208291,"Missing"
2020.ngt-1.20,L18-1546,0,0.0153251,"odels that can possibly work across different text domains, we make use of a large dataset of parallel English-Portuguese sentences extracted from the Open Parallel Corpus Project (OPUS) (Tiedemann, 2012). OPUS7 contains more than 2.7 billion parallel sentences in 90 languages. The specific corpus we extracted consists of data from multiple domains and sources including: ParaCrawl project (Espl`a-Gomis et al., 2019), EUbookshop (Skadin¸sˇ et al., 2014), Tilde Model (Rozis and Skadinˇs, 2017), translation memories (DGT) (Steinberger et al., 2013), OpenSubtitles (Creutz, 2018), SciELO Parallel (Soares et al., 2018), JRC-Acquis Multilingual (Steinberger et al., 2006), Tanzil (Zarrabi-Zadeh, 2007), Eu6 7 roparl Parallel (Koehn, 2005), TED 2013 (Cettolo et al., 2012), Wikipedia (Wołk and Marasek, 2014), Tatoeba 8 , QCRI Educational Domain (Abdelali et al., 2014), GNOME localization files, 9 Global Voices, 10 KDE4, 11 , Ubuntu, 12 and Multilingual Bible (Christodouloupoulos and Steedman, 2015). To train our models, we extract more than 77.7M parallel (i.e., English-Portuguese) sentences from the whole collection. The extracted dataset comprises more than 1.5B English tokens and 1.4B Portuguese tokens. More"
2020.ngt-1.20,tiedemann-2012-parallel,0,0.130272,"Missing"
2020.ngt-1.20,C16-1275,0,0.0198043,"a we use for both training and fine-tuning our models. Section 4 presents the proposed MT system. Section 5 describes our different methods. We discuss our results in Section 6, and conclude in Section 7. English sentence Accepted Portuguese Translations English sentence Accepted Portuguese Translations Table 1: English sentences with their Portuguese translation samples from shared task training split. More recently, advances in neural machine translation (NMT) have spurred interest in paraphrase generation (Sutskever et al., 2014; Luong and Manning, 2015; Aharoni et al., 2019). For example, Prakash et al. (2016) employ a stacked residual LSTM network to learn a sequence-to-sequence model on paraphrase data. A parpahrase model with adversarial training is presented by (Li et al., 2017). Wieting and Gimpel (2017); Iyyer et al. (2018) propose a translation-based paraphrasing system, which is based on NTM to translate one side of a parallel corpus. Paraphrase generation with pivot NMT is used by (Mallinson et al., 2017; Yu et al., 2018). 3 2 Related Work 3.1 We focus our related work overview on the task of paraphrase generation and its intersection with machine translation. Paraphrasing is the task of e"
2020.ngt-1.20,W17-0235,0,0.0173764,"nce Accepted Portuguese Translations English sentence Accepted Portuguese Translations Table 1: English sentences with their Portuguese translation samples from shared task training split. More recently, advances in neural machine translation (NMT) have spurred interest in paraphrase generation (Sutskever et al., 2014; Luong and Manning, 2015; Aharoni et al., 2019). For example, Prakash et al. (2016) employ a stacked residual LSTM network to learn a sequence-to-sequence model on paraphrase data. A parpahrase model with adversarial training is presented by (Li et al., 2017). Wieting and Gimpel (2017); Iyyer et al. (2018) propose a translation-based paraphrasing system, which is based on NTM to translate one side of a parallel corpus. Paraphrase generation with pivot NMT is used by (Mallinson et al., 2017; Yu et al., 2018). 3 2 Related Work 3.1 We focus our related work overview on the task of paraphrase generation and its intersection with machine translation. Paraphrasing is the task of expressing the same textual units (e.g. sentence) with alternative forms using different words while keeping the original meaning intact. 5 Over the last few years, MT has been the dominant approach for p"
2020.ngt-1.20,P16-1162,0,0.0172521,"dman, 2015). To train our models, we extract more than 77.7M parallel (i.e., English-Portuguese) sentences from the whole collection. The extracted dataset comprises more than 1.5B English tokens and 1.4B Portuguese tokens. More details about the training dataset are given in Table 2. 3.3 Pre-Processing Pre-processing is an important step in building any MT model as it can significantly affect the end results. We remove punctuation and tokenize all data with the Moses tokenizer (Koehn et al., 2007). We also use joint Byte-Pair Encoding (BPE) with 60K split operations for subword segmentation (Sennrich et al., 2016). https://sharedtask.duolingo.com/#data. http://opus.nlpl.eu/ 171 8 www.tatoeba.org www.10n.gnome.org 10 www.globalvoices.org/ 11 www.i18n.kde.org 12 www.translations.launchpad.net 9 4 Models we experiment with using the STAPLE-based STRAIN parallel dataset from the previous subsection to fine-tune our En→Pt basic model for 5 epochs. 14 We will refer to the model resulting from this fine-tuning process simply as the finetuned model. In this section, we first describe the architecture of our models. We then explain the different ways we train the models on various subsets of the data. 4.1 Archi"
2020.ngt-1.20,skadins-etal-2014-billions,0,0.0551098,"Missing"
2020.osact-1.17,2020.osact-1.3,1,0.694397,"judge as more suitable carriers of hateful content. Table 2 shows samples of the offensive and hateful seeds. Table 3 shows examples of seeds in our initial larger set that we filtered out since these are less likely to carry negative meaning (whether offensive or hateful). To extend the offensive and hateful tweets, we use 500K randomly sampled, unlabeled, tweets from (Abdul-Mageed et al., 2019) that each have at least one occurrence of the trigger word Ya and at least one occurrence of a word from either of our two seed lexica (i.e., the offensive and hateful seeds). 8 We then apply AraNet (Abdul-Mageed et al., 2020) on this 500K collection and keep only tweets assigned negative sentiment labels. Tweets that 7 matically extracted dataset, to which we refer to as augmented (AUG). 6 Original tweets can be run-on sentences, lack proper grammatical structures or punctuation. In presented translation, for readability, while we maintain the meaning as much as possible, we render grammatical, well-structured sentence. AraNet (Abdul-Mageed et al., 2020) assigns only positive and negative sentiment labels. In other words, it does not assign neutral labels. 8 The 500K collection is extracted via searching a larger"
2020.osact-1.17,W18-4416,0,0.0166194,"p highly accurate deep learning models for the two tasks of offensive content and hate speech detection. The rest of the paper is organized as follows: We introduce related works in Section 2., shared task data and our datasets in Section 3., our models in Section 4., and we conclude in Section 5.. 2. Related Work Thematic Focus: Research on undesirable content shows that social media users sometimes utilize profane, obscene, or offensive language (Jay and Janschewitz, 2008; Wiegand et al., 2018); aggression (Kumar et al., 2018; Modha et al., 2018); toxic content (Georgakopoulos et al., 2018; Fortuna et al., 2018; Zampieri et al., 2019), and bullying (Dadvar et al., 2013; Agrawal and Awekar, 2018; Fortuna et al., 2018). Overarching Applications: Several works have taken as their target detecting these types of negative content with a goal to build applications for (1) content filtering or (2) quantifying the intensity of polarization (Barber´a and Sood, 2015; Conover et al., 2011), (3) classifying trolls and propaganda accounts that often use offensive language (Darwish et al., 2017), (4) identifying hate speech that may correlate with hate crimes (Nobata et al., 2016), and (5) detecting signals of co"
2020.osact-1.17,W18-4401,0,0.0176946,"ffective models on the two downstream tasks of offensive and hate speech. • We develop highly accurate deep learning models for the two tasks of offensive content and hate speech detection. The rest of the paper is organized as follows: We introduce related works in Section 2., shared task data and our datasets in Section 3., our models in Section 4., and we conclude in Section 5.. 2. Related Work Thematic Focus: Research on undesirable content shows that social media users sometimes utilize profane, obscene, or offensive language (Jay and Janschewitz, 2008; Wiegand et al., 2018); aggression (Kumar et al., 2018; Modha et al., 2018); toxic content (Georgakopoulos et al., 2018; Fortuna et al., 2018; Zampieri et al., 2019), and bullying (Dadvar et al., 2013; Agrawal and Awekar, 2018; Fortuna et al., 2018). Overarching Applications: Several works have taken as their target detecting these types of negative content with a goal to build applications for (1) content filtering or (2) quantifying the intensity of polarization (Barber´a and Sood, 2015; Conover et al., 2011), (3) classifying trolls and propaganda accounts that often use offensive language (Darwish et al., 2017), (4) identifying hate speech tha"
2020.osact-1.17,malmasi-zampieri-2017-detecting,0,0.0240654,"Missing"
2020.osact-1.17,W18-4423,0,0.0112038,"he two downstream tasks of offensive and hate speech. • We develop highly accurate deep learning models for the two tasks of offensive content and hate speech detection. The rest of the paper is organized as follows: We introduce related works in Section 2., shared task data and our datasets in Section 3., our models in Section 4., and we conclude in Section 5.. 2. Related Work Thematic Focus: Research on undesirable content shows that social media users sometimes utilize profane, obscene, or offensive language (Jay and Janschewitz, 2008; Wiegand et al., 2018); aggression (Kumar et al., 2018; Modha et al., 2018); toxic content (Georgakopoulos et al., 2018; Fortuna et al., 2018; Zampieri et al., 2019), and bullying (Dadvar et al., 2013; Agrawal and Awekar, 2018; Fortuna et al., 2018). Overarching Applications: Several works have taken as their target detecting these types of negative content with a goal to build applications for (1) content filtering or (2) quantifying the intensity of polarization (Barber´a and Sood, 2015; Conover et al., 2011), (3) classifying trolls and propaganda accounts that often use offensive language (Darwish et al., 2017), (4) identifying hate speech that may correlate with"
2020.osact-1.17,W17-3008,0,0.0535267,"e and hate speech models off affective models (i.e., we fine-tune already trained sentiment and emotion models on both the offensive and hate speech tasks). (2) 1 Of course hand-crafted features can also be added to a representation fed into a deep learning model. However, we do not do this here. We apply BERT language models on these two tasks. We also (3) automatically augment offensive and hate speech training data using a simple data enrichment method. Arabic Offensive Content: Very few works have been applied to the Arabic language, focusing on detecting offensive language. For example, (Mubarak et al., 2017) develop a list of obscene words and hashtags using patterns common in offensive and rude communications to label a dataset of 1,100 tweets. Mubarak and Darwish (2019) applied character n-gram FasText model on a large dataset (3.3M tweets) of offensive content. Our work is similar to Mubarak and Darwish (2019) in that we also automatically augment training data based on an initial seed lexicon. 3. Data In our experiments, we use two types of data: (1) data distributed by the Offensive Language Detection shared task and (2) an automatically collected dataset that we develop (Section 3.1.). The"
2020.osact-1.17,S19-2010,0,0.017012,"learning models for the two tasks of offensive content and hate speech detection. The rest of the paper is organized as follows: We introduce related works in Section 2., shared task data and our datasets in Section 3., our models in Section 4., and we conclude in Section 5.. 2. Related Work Thematic Focus: Research on undesirable content shows that social media users sometimes utilize profane, obscene, or offensive language (Jay and Janschewitz, 2008; Wiegand et al., 2018); aggression (Kumar et al., 2018; Modha et al., 2018); toxic content (Georgakopoulos et al., 2018; Fortuna et al., 2018; Zampieri et al., 2019), and bullying (Dadvar et al., 2013; Agrawal and Awekar, 2018; Fortuna et al., 2018). Overarching Applications: Several works have taken as their target detecting these types of negative content with a goal to build applications for (1) content filtering or (2) quantifying the intensity of polarization (Barber´a and Sood, 2015; Conover et al., 2011), (3) classifying trolls and propaganda accounts that often use offensive language (Darwish et al., 2017), (4) identifying hate speech that may correlate with hate crimes (Nobata et al., 2016), and (5) detecting signals of conflict, which are often"
2020.osact-1.3,N16-3003,0,0.0610908,"Missing"
2020.osact-1.3,abdul-mageed-diab-2012-awatif,1,0.7067,"function applied to the last layer (with logits) in each model. Figure 2 shows two examples of using the tool as Python library. F1 73.47 81.62 AraNet as a Command-Line and Interactive Tool: AraNet provides scripts supporting both command-line and interactive mode. Command-line mode accepts a text or file path. Interaction mode is good for quick interactive line-by-line experiments and also pipeline re-directions. Table 6: Model performance on irony detection. 3.5. TRAIN 61,555 39,044 100,599 Sentiment We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects (Abdul-Mageed and Diab, 2012; Abdulla et al., 2013; Abdul-Mageed et al., 2014b; Nabil et al., 2015; Kiritchenko et al., 2016; Aly and Atiya, 2013; Salameh et al., 2015; Rosenthal et al., 2017; Alomari et al., 2017; Mohammad et al., 2018; Baly et al., 2019). Table 8 shows all the corpora we use. The datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of labels to binary"
2020.osact-1.3,R11-1096,1,0.696523,"paring across different Arabic social media NLP tasks, by providing one way to test new models against AraNet predictions (i.e., model-based comparisons). Our toolkit can be used to make important discoveries about the Arab world, a vast geographical region of strategic importance. It can enhance also enhance our understating of Arabic online communities, and the Arabic digital culture in general. Related Works As we pointed out earlier, there are several works on some of the tasks but less on others. By far, Arabic sentiment analysis has been the most popular task. Works focused on both MSA (Abdul-Mageed et al., 2011; Abdul-Mageed et al., 2014a) and dialects (Nabil et al., 2015; ElSahar and El-Beltagy, 2015; Al Sallab et al., 2015; Al-Moslmi et al., 2018; Al-Smadi et al., 2019; Al-Ayyoub et al., 2019; Farha and Magdy, 2019). A number of studies have been published on dialect detection, including (Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014; Elfardy and Diab, 2013; Cotterell and CallisonBurch, 2014). Some works took as their target the tasks of age detection (Zaghouani and Charfi, 2018; Rangel et al., 2019), gender detection (Zaghouani and Charfi, 2018; Rangel et al., 2019), irony ident"
2020.osact-1.3,W19-4622,0,0.0718047,"ieties spoken by a wide population of ∼ 400 million native speakers covering a vast geographical region (shown in Figure 1), no such suite of tools currently exists. Many works have focused on sentiment analysis, e.g., (Abdul-Mageed et al., 2014a; Nabil et al., 2015; ElSahar and El-Beltagy, 2015; Al Sallab et al., 2015; Al-Moslmi et al., 2018; Al-Smadi et al., 2019; Al-Ayyoub et al., 2019; Farha and Magdy, 2019) and dialect identification (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014; Cotterell and Callison-Burch, 2014; Zhang and Abdul-Mageed, 2019b; Bouamor et al., 2019a). However, there is rarity of tools for other tasks such as gender and age detection. This motivates our toolkit, which we hope can meet the current critical need for studying Arabic communities online. This is especially valuable given the waves of protests, uprisings, and revolutions that have swept the region during the last decade. Figure 2: AraNet usage and output as a Python library. the somewhat ephemeral nature of parts of some types of these data. In particular, many tasks are developed based on social media posts such as tweets that are distributed under restrictive conditions. For"
2020.osact-1.3,cotterell-callison-burch-2014-multi,0,0.0218646,"ribe in each section. For Arabic, a collection of languages and varieties spoken by a wide population of ∼ 400 million native speakers covering a vast geographical region (shown in Figure 1), no such suite of tools currently exists. Many works have focused on sentiment analysis, e.g., (Abdul-Mageed et al., 2014a; Nabil et al., 2015; ElSahar and El-Beltagy, 2015; Al Sallab et al., 2015; Al-Moslmi et al., 2018; Al-Smadi et al., 2019; Al-Ayyoub et al., 2019; Farha and Magdy, 2019) and dialect identification (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014; Cotterell and Callison-Burch, 2014; Zhang and Abdul-Mageed, 2019b; Bouamor et al., 2019a). However, there is rarity of tools for other tasks such as gender and age detection. This motivates our toolkit, which we hope can meet the current critical need for studying Arabic communities online. This is especially valuable given the waves of protests, uprisings, and revolutions that have swept the region during the last decade. Figure 2: AraNet usage and output as a Python library. the somewhat ephemeral nature of parts of some types of these data. In particular, many tasks are developed based on social media posts such as tweets t"
2020.osact-1.3,N19-1423,0,0.0696443,"Missing"
2020.osact-1.3,N04-4038,0,0.147522,"Missing"
2020.osact-1.3,P13-2081,0,0.0292028,"Arab countries. Our different datasets cover varying regions of the Arab world as we describe in each section. For Arabic, a collection of languages and varieties spoken by a wide population of ∼ 400 million native speakers covering a vast geographical region (shown in Figure 1), no such suite of tools currently exists. Many works have focused on sentiment analysis, e.g., (Abdul-Mageed et al., 2014a; Nabil et al., 2015; ElSahar and El-Beltagy, 2015; Al Sallab et al., 2015; Al-Moslmi et al., 2018; Al-Smadi et al., 2019; Al-Ayyoub et al., 2019; Farha and Magdy, 2019) and dialect identification (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014; Cotterell and Callison-Burch, 2014; Zhang and Abdul-Mageed, 2019b; Bouamor et al., 2019a). However, there is rarity of tools for other tasks such as gender and age detection. This motivates our toolkit, which we hope can meet the current critical need for studying Arabic communities online. This is especially valuable given the waves of protests, uprisings, and revolutions that have swept the region during the last decade. Figure 2: AraNet usage and output as a Python library. the somewhat ephemeral nature of parts of some type"
2020.osact-1.3,2020.osact-1.17,1,0.786073,"Missing"
2020.osact-1.3,W19-4621,0,0.0141818,"Arabic social media processing. Figure 1: A map of Arab countries. Our different datasets cover varying regions of the Arab world as we describe in each section. For Arabic, a collection of languages and varieties spoken by a wide population of ∼ 400 million native speakers covering a vast geographical region (shown in Figure 1), no such suite of tools currently exists. Many works have focused on sentiment analysis, e.g., (Abdul-Mageed et al., 2014a; Nabil et al., 2015; ElSahar and El-Beltagy, 2015; Al Sallab et al., 2015; Al-Moslmi et al., 2018; Al-Smadi et al., 2019; Al-Ayyoub et al., 2019; Farha and Magdy, 2019) and dialect identification (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014; Cotterell and Callison-Burch, 2014; Zhang and Abdul-Mageed, 2019b; Bouamor et al., 2019a). However, there is rarity of tools for other tasks such as gender and age detection. This motivates our toolkit, which we hope can meet the current critical need for studying Arabic communities online. This is especially valuable given the waves of protests, uprisings, and revolutions that have swept the region during the last decade. Figure 2: AraNet usage and output as a Python library."
2020.osact-1.3,S16-1004,0,0.0590955,"Missing"
2020.osact-1.3,S18-1001,0,0.0293446,"supporting both command-line and interactive mode. Command-line mode accepts a text or file path. Interaction mode is good for quick interactive line-by-line experiments and also pipeline re-directions. Table 6: Model performance on irony detection. 3.5. TRAIN 61,555 39,044 100,599 Sentiment We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects (Abdul-Mageed and Diab, 2012; Abdulla et al., 2013; Abdul-Mageed et al., 2014b; Nabil et al., 2015; Kiritchenko et al., 2016; Aly and Atiya, 2013; Salameh et al., 2015; Rosenthal et al., 2017; Alomari et al., 2017; Mohammad et al., 2018; Baly et al., 2019). Table 8 shows all the corpora we use. The datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of labels to binary tags in the set {‘positive0 , ‘negative0 } using the following rules: Figure 3: AraNet usage examples as command-line mode, pipeline, and interactive mode. • Map {Positive, Pos, or High-Pos} to ‘positive’ Ar"
2020.osact-1.3,D15-1299,0,0.340702,"nt identities across different geographic regions. In this work, we propose AraNet , a suit of tools that has the promise to play such a role of Arabic social media processing. Figure 1: A map of Arab countries. Our different datasets cover varying regions of the Arab world as we describe in each section. For Arabic, a collection of languages and varieties spoken by a wide population of ∼ 400 million native speakers covering a vast geographical region (shown in Figure 1), no such suite of tools currently exists. Many works have focused on sentiment analysis, e.g., (Abdul-Mageed et al., 2014a; Nabil et al., 2015; ElSahar and El-Beltagy, 2015; Al Sallab et al., 2015; Al-Moslmi et al., 2018; Al-Smadi et al., 2019; Al-Ayyoub et al., 2019; Farha and Magdy, 2019) and dialect identification (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014; Cotterell and Callison-Burch, 2014; Zhang and Abdul-Mageed, 2019b; Bouamor et al., 2019a). However, there is rarity of tools for other tasks such as gender and age detection. This motivates our toolkit, which we hope can meet the current critical need for studying Arabic communities online. This is especially valuable given the wav"
2020.osact-1.3,pasha-etal-2014-madamira,0,0.101524,"Missing"
2020.osact-1.3,W15-3202,0,0.0619543,"Missing"
2020.osact-1.3,W18-1104,1,0.857805,"Missing"
2020.osact-1.3,2020.osact-1.6,1,0.863471,"Missing"
2020.osact-1.3,P13-2088,0,0.0273242,"ry. F1 73.47 81.62 AraNet as a Command-Line and Interactive Tool: AraNet provides scripts supporting both command-line and interactive mode. Command-line mode accepts a text or file path. Interaction mode is good for quick interactive line-by-line experiments and also pipeline re-directions. Table 6: Model performance on irony detection. 3.5. TRAIN 61,555 39,044 100,599 Sentiment We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects (Abdul-Mageed and Diab, 2012; Abdulla et al., 2013; Abdul-Mageed et al., 2014b; Nabil et al., 2015; Kiritchenko et al., 2016; Aly and Atiya, 2013; Salameh et al., 2015; Rosenthal et al., 2017; Alomari et al., 2017; Mohammad et al., 2018; Baly et al., 2019). Table 8 shows all the corpora we use. The datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of labels to binary tags in the set {‘positive0 , ‘negative0 } using the following rules: Figure 3: AraNet usage examples as command-lin"
2020.osact-1.3,L18-1535,0,0.0245925,"Missing"
2020.osact-1.3,S17-2088,0,0.0242472,"and Interactive Tool: AraNet provides scripts supporting both command-line and interactive mode. Command-line mode accepts a text or file path. Interaction mode is good for quick interactive line-by-line experiments and also pipeline re-directions. Table 6: Model performance on irony detection. 3.5. TRAIN 61,555 39,044 100,599 Sentiment We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects (Abdul-Mageed and Diab, 2012; Abdulla et al., 2013; Abdul-Mageed et al., 2014b; Nabil et al., 2015; Kiritchenko et al., 2016; Aly and Atiya, 2013; Salameh et al., 2015; Rosenthal et al., 2017; Alomari et al., 2017; Mohammad et al., 2018; Baly et al., 2019). Table 8 shows all the corpora we use. The datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of labels to binary tags in the set {‘positive0 , ‘negative0 } using the following rules: Figure 3: AraNet usage examples as command-line mode, pipeline, and interactive mode. • Map"
2020.osact-1.3,N15-1078,0,0.0297695,"aNet as a Command-Line and Interactive Tool: AraNet provides scripts supporting both command-line and interactive mode. Command-line mode accepts a text or file path. Interaction mode is good for quick interactive line-by-line experiments and also pipeline re-directions. Table 6: Model performance on irony detection. 3.5. TRAIN 61,555 39,044 100,599 Sentiment We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects (Abdul-Mageed and Diab, 2012; Abdulla et al., 2013; Abdul-Mageed et al., 2014b; Nabil et al., 2015; Kiritchenko et al., 2016; Aly and Atiya, 2013; Salameh et al., 2015; Rosenthal et al., 2017; Alomari et al., 2017; Mohammad et al., 2018; Baly et al., 2019). Table 8 shows all the corpora we use. The datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of labels to binary tags in the set {‘positive0 , ‘negative0 } using the following rules: Figure 3: AraNet usage examples as command-line mode, pipeline, and"
2020.osact-1.3,L18-1111,0,0.0214629,"cy drop could be that, for this tweet-level task, some tweets from the same user occur across our TRAIN/DEV/TEST splits. This was unavoidable since Arab-Tweet is distributed without user ids, thus not making it possible for us to prevent user-level data leakage into the two tweet-level classification tasks of age and gender we report here. We alleviate this issue for gender by annotating and developing on UBC-Gender where we control for user-level data distribution across the splits as explained earlier. Data and Models Age and Gender Arab-Tweet. For modeling age and gender, we use ArapTweet (Zaghouani and Charfi, 2018) 4 , which we will refer to as Arab-Tweet. Arab-tweet comprises 11 Arabic regions from 17 different countries. 5 For each region, data from 100 Twitter users were crawled. Users needed to have posted at least 2,000 tweets and were selected based on an initial list of seed words characteristic of each region.   /barsha/ ‘many’ The seed list included words such as éQK . for Tunisian Arabic and YK @ð /wayed/ ‘many’ for Gulf Arabic. (Zaghouani and Charfi, 2018) employed human annotators to verify that users do belong to each respective region. Annotators also assigned gender labels from the set"
2020.osact-1.3,P11-2007,0,0.0209136,"erent datasets cover varying regions of the Arab world as we describe in each section. For Arabic, a collection of languages and varieties spoken by a wide population of ∼ 400 million native speakers covering a vast geographical region (shown in Figure 1), no such suite of tools currently exists. Many works have focused on sentiment analysis, e.g., (Abdul-Mageed et al., 2014a; Nabil et al., 2015; ElSahar and El-Beltagy, 2015; Al Sallab et al., 2015; Al-Moslmi et al., 2018; Al-Smadi et al., 2019; Al-Ayyoub et al., 2019; Farha and Magdy, 2019) and dialect identification (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014; Cotterell and Callison-Burch, 2014; Zhang and Abdul-Mageed, 2019b; Bouamor et al., 2019a). However, there is rarity of tools for other tasks such as gender and age detection. This motivates our toolkit, which we hope can meet the current critical need for studying Arabic communities online. This is especially valuable given the waves of protests, uprisings, and revolutions that have swept the region during the last decade. Figure 2: AraNet usage and output as a Python library. the somewhat ephemeral nature of parts of some types of these data. In particular, m"
2020.osact-1.3,J14-1006,0,0.0727481,"Missing"
2020.osact-1.3,W19-4637,1,0.905476,"collection of languages and varieties spoken by a wide population of ∼ 400 million native speakers covering a vast geographical region (shown in Figure 1), no such suite of tools currently exists. Many works have focused on sentiment analysis, e.g., (Abdul-Mageed et al., 2014a; Nabil et al., 2015; ElSahar and El-Beltagy, 2015; Al Sallab et al., 2015; Al-Moslmi et al., 2018; Al-Smadi et al., 2019; Al-Ayyoub et al., 2019; Farha and Magdy, 2019) and dialect identification (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014; Cotterell and Callison-Burch, 2014; Zhang and Abdul-Mageed, 2019b; Bouamor et al., 2019a). However, there is rarity of tools for other tasks such as gender and age detection. This motivates our toolkit, which we hope can meet the current critical need for studying Arabic communities online. This is especially valuable given the waves of protests, uprisings, and revolutions that have swept the region during the last decade. Figure 2: AraNet usage and output as a Python library. the somewhat ephemeral nature of parts of some types of these data. In particular, many tasks are developed based on social media posts such as tweets that are distributed under rest"
2020.osact-1.6,W17-3008,0,0.0335421,"Missing"
2020.osact-1.6,S19-2081,0,0.0234585,"Missing"
2020.osact-1.6,N16-2013,0,0.0922269,"Missing"
2020.osact-1.6,W17-3012,0,0.015054,"ws: In Section 2., we review related literature. Building dangerous lexica used to collect our datasets is discussed in Section 3.3.. We describe our annotation in Section 4.1.. We present our models in Section 5., and conclude in Section 6.. 2. † Both authors contributed equally. 1 https://en.oxforddictionaries.com/ definition/threat 2 https://help.twitter.com/en/ rules-and-policies/twitter-rules Related work Detection of offensive language in natural languages has recently attracted the interest of multiple researchers. However, the space of abusive language is vast and has its own nuances. Waseem et al. (2017) classify abusive 40 ` SemEval2019 competition, HatEval (Oscar Garibo, 2019). This shared task addresses the problem of multilingual detection of hate speech against immigrants and women in Twitter. language along two dimensions: directness (the level to which it is directed to a specific person or organization or not) and explicitness (the degree to which it is explicit). Jay and Janschewitz (2008) categorize offensive language to three categories: Vulgar, Pornographic, and Hateful. The Hateful category includes offensive language such as threats as well as language pertaining to class, race,"
2020.osact-1.6,W16-5618,0,0.0566333,"Missing"
2020.osact-1.6,S19-2010,0,0.0460698,"Missing"
2020.osact-1.6,W17-3013,0,0.0274821,"Missing"
2020.osact-1.6,W17-3001,0,\N,Missing
2020.sigmorphon-1.16,P16-1038,0,0.0233822,"gre hin hun ice jpn kor lit rum vie 33.56 24.00 41.33 30.89 34.89 25.33 24.00 22.67 20.89 30.22 11.78 30.67 26.00 20.00 32.00 9.31 5.65 12.07 7.73 12.69 5.19 5.13 6.76 5.30 11.12 3.73 9.17 7.75 5.52 13.75 avg 27.22 8.06 7 Conclusion Table 9: Development set results for monolingual models. Multilingual training is a crucial component in our system. Our approach is closely related to multilingual neural machine translation (Johnson et al., 2017b), where a single model is trained to translate between multiple source and target languages. Others have also explored multilingual approaches to G2P. Deri and Knight (2016) use multilingual G2P conversion for the purpose of adapting models from high-resource languages to train weighted finite-state transducers for related low-resource languages. Ni et al. (2018) experiment with multilingual training for deep learning models. They use pretrained character embeddings with LSTM encoder-decoders in order to train multilingual G2P models for Chinese, Japanese, Korean and Thai. In contrast to Ni et al. (2018), we inspect multilingual training in the context of transformer models. For our second model, whose training data is augmented from Wikipedia, we use a selftaini"
2020.sigmorphon-1.16,P15-1166,0,0.0363059,"the shared task training data for each of the 15 individual languages. To ease this bottleneck, we decided to view the task through a multilingual machine translation lens where we build a single model mapping from input to output across all the languages simultaneously. In this, we hypothesized that a multilingual model would allow for shared representations across the various languages that may be more powerful than individual representations of monolingual models. Abundant evidence now exists for approaching machine translation tasks from a multilingual perspective (Johnson et al., 2017a; Dong et al., 2015; Firat et al., 2016), which inspired our choice. In order to make use of unlabeled data, we also explore a straightforward self-training approach. In particular, we employ our trained models to convert sequences of multilingual unlabeled graphemes, taken from Wikipedia data, into multilingual phonemes. We then select sequences of phonemes predicted with our models above a certain confidence threshold to augment the shared task training data, thus re-training our models with larger (gold and silver) training data from scratch. Our models are based on the Transformer architecture which exploits"
2020.sigmorphon-1.16,N16-1101,0,0.020224,"aining data for each of the 15 individual languages. To ease this bottleneck, we decided to view the task through a multilingual machine translation lens where we build a single model mapping from input to output across all the languages simultaneously. In this, we hypothesized that a multilingual model would allow for shared representations across the various languages that may be more powerful than individual representations of monolingual models. Abundant evidence now exists for approaching machine translation tasks from a multilingual perspective (Johnson et al., 2017a; Dong et al., 2015; Firat et al., 2016), which inspired our choice. In order to make use of unlabeled data, we also explore a straightforward self-training approach. In particular, we employ our trained models to convert sequences of multilingual unlabeled graphemes, taken from Wikipedia data, into multilingual phonemes. We then select sequences of phonemes predicted with our models above a certain confidence threshold to augment the shared task training data, thus re-training our models with larger (gold and silver) training data from scratch. Our models are based on the Transformer architecture which exploits effective self-atten"
2020.sigmorphon-1.16,2020.sigmorphon-1.2,0,0.132547,"me (G2P) conversion is an important component of both speech recognition and synthesis. In G2P conversion, sequences of graphemes (the symbols used to write words) are mapped to corresponding phonemes (pronunciation symbols, e.g., symbols of the International Phonetic Alphabet). Members of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON) have proposed a G2P shared task (SIGMORPHON 2020 Shared Task 1) 1 involving multiple languages. In this paper, we describe our submissions to the shared task. Organizers provide an overview of the task and submitted systems in Gorman et al. (2020) (this volume). 1 The shared task webpage is accessible at: https: //sigmorphon.github.io/sharedtasks/2020/task1. The task was introduced with data from 10 languages, with an additional 5 ‘surprise’ languages released during the task timeline. Our goal was to develop an effective system based on modern deep learning methods as a solution. However, deep learning technologies work best with sufficiently large training data. Hence, a clear challenge we came across is the limited size of the shared task training data for each of the 15 individual languages. To ease this bottleneck, we decided to v"
2020.sigmorphon-1.16,P17-4012,0,0.0398098,"ww.opengrm.org/twiki/bin/view/GRM. https://github.com/pytorch/fairseq. gual models, then we introduce our semisupervised models (also multilingual). Our semi-supervised models follow a self-training set up. We now explain each of these models. 3.1 Supervised, Multilingual Models We use a multilingual approach where we train a single model on data from all 15 languages. For this purpose, we prepend a token comprising a language code (e.g. fre) to each grapheme sequence source. For our implementation, we use the PyTorch Transformer architecture in the OpenNMT Neural Machine Translation Toolkit (Klein et al., 2017). We set the model hyper-parameters as shown in Table 3, which follows those adopted by Vaswani et al. (2017). Hyper-Parameter Number of layers Hidden state size Word embedding size Hidden feed-forward size Number of self-attention heads Optimizer Dropout probability Number of training steps Table 3: parameters. Multilingual Value 6 512 512 2,048 8 Adam 0.1 200K Transformer 3.2.1 Language arm bul dut fre geo gre hin hun ice kor lit rum Total hyperWe train the model with 3 different random seeds, and at inference we employ an ensemble consisting of the models from 4 training checkpoints (at 50k"
2020.sigmorphon-1.16,2020.lrec-1.521,0,0.459033,"baselines. The first is a pair n-gram model encoded as a weighted finitestate transducer (FST), implemented using the OpenGRMtoolkit 4 . The second is a biLSTM encoder-decoder sequence model implemented using the Fairseq toolkit 5 . The third is a Transformer model also implemented using the Fairseq toolkit. Organizer-provided shared task baselines are shown in Table 2 as WER and PER averages over the 15 languages. We now introduce our models. Task Data, Evaluation, and Baselines The data provided by the organizers of the shared task are extracted from Wiktionary 2 using the WikiPron library (Lee et al., 2020), and consist of 4,050 gold labeled graphemephoneme pairs for each of 15 languages, split into a training set (3,600 per language) and a development set (450 per language). The blind test data comprise 450 sources for each language. The data involves languages in the set {Adyghe (ady), Armenian (arm), Bulgarian (bul), Dutch (dut), French (fre), Georgian (geo), Modern Greek (gre), Hindi (hin), Hungarian (hun), Icelandic (ice), Japanese hiragana (jpn), Korean (kor), Lithuanian (lit), Romanian (rum), Vietnamese (vie)}. 3 This set of languages employ a variety of writing systems: alphabets (e.g. F"
2020.wanlp-1.7,N16-1138,0,0.0480243,"oping automatic methods for fake news detection has mainly followed two lines of research as categorized in the literature (Thorne and Vlachos, 2018; Potthast et al., 2018). First, work that compares a claim against an evidence from (trusted) collections of factual information whether the evidence is a sentence (i.e. fact-checking modeled as textual entailment) or a full document (i.e. stance detection between a claim-document pair). This includes work that created synthetic claims verified against Wikipedia (Thorne et al., 2018), and naturally occurring claims verified against news articles (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (Joty et al., 2018), or debate websites (Chen et al., 2019). These datasets are labeled using 2 tags (true, false) (Alhindi et al., 2018) 3 tags (supported, refuted, not-enough-information) (Thorne et al., 2018), or 4 tags (agree, disagree, discuss, unrelated) (Pomerleau and Rao, 2017). They vary in size from 300 claims (Fer1 2 Models and data are at: https://github.com/UBC-NLP/wanlp2020_arabic_fake_news_detection. We use the terms “true” and “legitimate” interchangeably to refer to stories that are not “fake”. 70 reira and Vlachos, 2016) to 185, 0"
2020.wanlp-1.7,W19-4614,0,0.0214556,"influenced to believe fake stories. More recently, concerns have also been raised about possible abuse of machine-generated text such as by GPT3 (Brown et al., 2020) for deceiving Figure 1: Our proposed methods. Left: Machine generreaders. ation of manipulated text. Top Right: manipulated text In the Arab context, Arab countries have detection model (MTD). Bottom Right: fake news dehad their share of misinformation. This tection model (FND). wordi : original word. wordj : is especially the case due to the sweepsubstituted word. ing waves of uprisings and popular protests (Torres et al., 2018; Helwe et al., 2019). Although there has been considerable research investigating the This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 69 Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 69–84 Barcelona, Spain (Online), December 12, 2020 legitimacy, or lack thereof, of news in many languages (Conroy et al., 2015; Kim et al., 2018; Bondielli and Marcelloni, 2019), work on the Arabic language is still lagging behind. In this paper, we first report an approach to automatically generate manipu"
2020.wanlp-1.7,2020.acl-main.761,1,0.884033,"s (supported, refuted, not-enough-information) (Thorne et al., 2018), or 4 tags (agree, disagree, discuss, unrelated) (Pomerleau and Rao, 2017). They vary in size from 300 claims (Fer1 2 Models and data are at: https://github.com/UBC-NLP/wanlp2020_arabic_fake_news_detection. We use the terms “true” and “legitimate” interchangeably to refer to stories that are not “fake”. 70 reira and Vlachos, 2016) to 185, 000 claims (Thorne et al., 2018). Approaches on developing models to predict claim veracity using these datasets include hierarchical attention networks (Ma et al., 2019), pointer networks (Hidey et al., 2020), graph-based reasoning (Zhou et al., 2019; Zhong et al., 2019), and (similar to our methods) fine-tuning of pre-trained transformers (Hidey et al., 2020; Zhong et al., 2019). Style-Based Detection. The second line of research focuses on analyzing the linguistic features of a claim to determine its veracity without considering external factual information. This approach is based on investigating linguistic characteristics of fake content in comparison to true content. In news and various fact-checked political claims, Rashkin et al. (2017) found that first and second person pronouns, superlati"
2020.wanlp-1.7,D18-1452,0,0.020188,"Missing"
2020.wanlp-1.7,2020.fever-1.2,0,0.361572,"on of claim veracity. Arabic Work. All of the datasets described above, however, are in English with limited availability of similar ones in other languages such as Arabic. Available Arabic datasets cover tasks such as determining claim check-worthiness of tweets (Barr´on-Cede˜no et al., 2020), news and claims from fact-checking websites (Elsayed et al., 2019), and translated political claims from English (Nakov et al., 2018). In addition, there are datasets for stance and factuality prediction of claims from news or social media with or without the evidence retrieval task (Baly et al., 2018; Khouja, 2020; Elsayed et al., 2019; Alkhair et al., 2019; Darwish et al., 2017). These corpora are created by either using credibility of publishers as proxy for veracity (true/false) then manually annotating the stance between a claim-document pair (agree, disagree, discuss, unrelated) (Baly et al., 2018) or by manual alteration of true claims to generate fake ones about the same topic (Khouja, 2020)–all requiring a manual, slow, and labor-intensive process. We alleviate this by introducing our simple and scalable approach for automatic generation of Arabic manipulated text, including potential fake stor"
2020.wanlp-1.7,D19-6604,0,0.0267378,"e, fake news datasets. Thorne et al. (2019) showed that current fact-checking systems are vulnerable to adversarial attacks by doing simple alteration to the training data. To increase robustness of such systems, previous work has extended available fake news datasets both manually and automatically using lexical substitution (Alzantot et al., 2018), rule-based alterations (Ribeiro et al., 2018), phrasal addition and temporal reasoning (Hidey et al., 2020), or using transformer models such as GPT-2 (Radford et al., 2019) and Grover (Zellers et al., 2019) for claim and news article generation (Niewinski et al., 2019; Zellers et al., 2019). As a way to increase our understanding and trust in fact-checking systems, Atanasova et al. (2020) developed a transformer-based model for generating fact-checking textual explanations along with the prediction of claim veracity. Arabic Work. All of the datasets described above, however, are in English with limited availability of similar ones in other languages such as Arabic. Available Arabic datasets cover tasks such as determining claim check-worthiness of tweets (Barr´on-Cede˜no et al., 2020), news and claims from fact-checking websites (Elsayed et al., 2019), and"
2020.wanlp-1.7,pasha-etal-2014-madamira,0,0.0842389,"Missing"
2020.wanlp-1.7,C18-1287,0,0.0419075,"Missing"
2020.wanlp-1.7,P18-1022,0,0.0294704,"est of the paper is organized as follows: Section 2 provides an overview of related work. In Section 3, we describe the two true2 news datasets used in this work. Section 4 is about our methods for generating manipulated text (and potentially fake news stories). Section 5 describes our human annotation study. In Section 6, we present our detection models. We conclude in Section 7. 2 Related Work Knowledge-Based Fact Checking. Recent work on developing automatic methods for fake news detection has mainly followed two lines of research as categorized in the literature (Thorne and Vlachos, 2018; Potthast et al., 2018). First, work that compares a claim against an evidence from (trusted) collections of factual information whether the evidence is a sentence (i.e. fact-checking modeled as textual entailment) or a full document (i.e. stance detection between a claim-document pair). This includes work that created synthetic claims verified against Wikipedia (Thorne et al., 2018), and naturally occurring claims verified against news articles (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (Joty et al., 2018), or debate websites (Chen et al., 2019). These datasets are labeled using 2 tags"
2020.wanlp-1.7,D17-1317,0,0.0173036,"attention networks (Ma et al., 2019), pointer networks (Hidey et al., 2020), graph-based reasoning (Zhou et al., 2019; Zhong et al., 2019), and (similar to our methods) fine-tuning of pre-trained transformers (Hidey et al., 2020; Zhong et al., 2019). Style-Based Detection. The second line of research focuses on analyzing the linguistic features of a claim to determine its veracity without considering external factual information. This approach is based on investigating linguistic characteristics of fake content in comparison to true content. In news and various fact-checked political claims, Rashkin et al. (2017) found that first and second person pronouns, superlatives, modal adverbs, and hedging are more prevalent in fake content, while concrete and comparative figures, and assertive words are more widespread in truthful content. Other work found the properties of deceptive language to differ between domains (P´erez-Rosas et al., 2018). Misleading content itself has been classified into sub-categories such as (a) the 3 types of fake (serious fabrication, hoaxes, and satire) (Rubin et al., 2015), (b) propaganda and its different techniques (Da San Martino et al., 2019), and (c) misinformation and dis"
2020.wanlp-1.7,P18-1079,0,0.0254901,"matic fake news detection models was possible as the afore-mentioned datasets became available. More related to our work, previous work has focused on developing methods to automatically generate more robust, and large-scale, fake news datasets. Thorne et al. (2019) showed that current fact-checking systems are vulnerable to adversarial attacks by doing simple alteration to the training data. To increase robustness of such systems, previous work has extended available fake news datasets both manually and automatically using lexical substitution (Alzantot et al., 2018), rule-based alterations (Ribeiro et al., 2018), phrasal addition and temporal reasoning (Hidey et al., 2020), or using transformer models such as GPT-2 (Radford et al., 2019) and Grover (Zellers et al., 2019) for claim and news article generation (Niewinski et al., 2019; Zellers et al., 2019). As a way to increase our understanding and trust in fact-checking systems, Atanasova et al. (2020) developed a transformer-based model for generating fact-checking textual explanations along with the prediction of claim veracity. Arabic Work. All of the datasets described above, however, are in English with limited availability of similar ones in ot"
2020.wanlp-1.7,C18-1283,0,0.0116417,"an external dataset. The rest of the paper is organized as follows: Section 2 provides an overview of related work. In Section 3, we describe the two true2 news datasets used in this work. Section 4 is about our methods for generating manipulated text (and potentially fake news stories). Section 5 describes our human annotation study. In Section 6, we present our detection models. We conclude in Section 7. 2 Related Work Knowledge-Based Fact Checking. Recent work on developing automatic methods for fake news detection has mainly followed two lines of research as categorized in the literature (Thorne and Vlachos, 2018; Potthast et al., 2018). First, work that compares a claim against an evidence from (trusted) collections of factual information whether the evidence is a sentence (i.e. fact-checking modeled as textual entailment) or a full document (i.e. stance detection between a claim-document pair). This includes work that created synthetic claims verified against Wikipedia (Thorne et al., 2018), and naturally occurring claims verified against news articles (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (Joty et al., 2018), or debate websites (Chen et al., 2019). These datasets"
2020.wanlp-1.7,N18-1074,0,0.0128316,"lude in Section 7. 2 Related Work Knowledge-Based Fact Checking. Recent work on developing automatic methods for fake news detection has mainly followed two lines of research as categorized in the literature (Thorne and Vlachos, 2018; Potthast et al., 2018). First, work that compares a claim against an evidence from (trusted) collections of factual information whether the evidence is a sentence (i.e. fact-checking modeled as textual entailment) or a full document (i.e. stance detection between a claim-document pair). This includes work that created synthetic claims verified against Wikipedia (Thorne et al., 2018), and naturally occurring claims verified against news articles (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (Joty et al., 2018), or debate websites (Chen et al., 2019). These datasets are labeled using 2 tags (true, false) (Alhindi et al., 2018) 3 tags (supported, refuted, not-enough-information) (Thorne et al., 2018), or 4 tags (agree, disagree, discuss, unrelated) (Pomerleau and Rao, 2017). They vary in size from 300 claims (Fer1 2 Models and data are at: https://github.com/UBC-NLP/wanlp2020_arabic_fake_news_detection. We use the terms “true” and “legitimate” int"
2020.wanlp-1.7,D19-1292,0,0.0109084,"e, Politifact.com introduced 6 levels: pants-on-fire, false, mostly-false, half-true, mostly-true and true. These different levels have been exploited in previous work, with a goal to automate this more challenging six-way classification task (Rashkin et al., 2017; Wang, 2017; Alhindi et al., 2018). Automatic Generation of Data. The development of automatic fake news detection models was possible as the afore-mentioned datasets became available. More related to our work, previous work has focused on developing methods to automatically generate more robust, and large-scale, fake news datasets. Thorne et al. (2019) showed that current fact-checking systems are vulnerable to adversarial attacks by doing simple alteration to the training data. To increase robustness of such systems, previous work has extended available fake news datasets both manually and automatically using lexical substitution (Alzantot et al., 2018), rule-based alterations (Ribeiro et al., 2018), phrasal addition and temporal reasoning (Hidey et al., 2020), or using transformer models such as GPT-2 (Radford et al., 2019) and Grover (Zellers et al., 2019) for claim and news article generation (Niewinski et al., 2019; Zellers et al., 201"
2020.wanlp-1.7,P17-2067,0,0.0615235,"(c) misinformation and disinformation (Ireton and Posetti, 2018). The differences between these different categories depend on many factors such as genre and domain, targeted audience, and deceptive intent (Rubin et al., 2015; Rashkin et al., 2017). In addition to categories, truth was classified to more than two levels. For example, Politifact.com introduced 6 levels: pants-on-fire, false, mostly-false, half-true, mostly-true and true. These different levels have been exploited in previous work, with a goal to automate this more challenging six-way classification task (Rashkin et al., 2017; Wang, 2017; Alhindi et al., 2018). Automatic Generation of Data. The development of automatic fake news detection models was possible as the afore-mentioned datasets became available. More related to our work, previous work has focused on developing methods to automatically generate more robust, and large-scale, fake news datasets. Thorne et al. (2019) showed that current fact-checking systems are vulnerable to adversarial attacks by doing simple alteration to the training data. To increase robustness of such systems, previous work has extended available fake news datasets both manually and automaticall"
2020.wanlp-1.7,2020.acl-main.549,0,0.023777,"Missing"
2020.wanlp-1.7,P19-1085,0,0.0123419,"n) (Thorne et al., 2018), or 4 tags (agree, disagree, discuss, unrelated) (Pomerleau and Rao, 2017). They vary in size from 300 claims (Fer1 2 Models and data are at: https://github.com/UBC-NLP/wanlp2020_arabic_fake_news_detection. We use the terms “true” and “legitimate” interchangeably to refer to stories that are not “fake”. 70 reira and Vlachos, 2016) to 185, 000 claims (Thorne et al., 2018). Approaches on developing models to predict claim veracity using these datasets include hierarchical attention networks (Ma et al., 2019), pointer networks (Hidey et al., 2020), graph-based reasoning (Zhou et al., 2019; Zhong et al., 2019), and (similar to our methods) fine-tuning of pre-trained transformers (Hidey et al., 2020; Zhong et al., 2019). Style-Based Detection. The second line of research focuses on analyzing the linguistic features of a claim to determine its veracity without considering external factual information. This approach is based on investigating linguistic characteristics of fake content in comparison to true content. In news and various fact-checked political claims, Rashkin et al. (2017) found that first and second person pronouns, superlatives, modal adverbs, and hedging are more p"
2020.wanlp-1.9,2020.wanlp-1.26,0,0.0901457,"Missing"
2020.wanlp-1.9,2020.wanlp-1.22,0,0.0733884,"Missing"
2020.wanlp-1.9,bouamor-etal-2014-multidialectal,1,0.809064,"received more attention relatively recently (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004). A majority of DA computational efforts have targeted creating resources for country or regionally specific dialects (Gadalla et al., 1997; Diab et al., 2010; Al-Sabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). The expansion into multi-dialectal data sets and models to identify them was initially done at the regional level (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). A number of Arabic dialect identification shared tasks were organized as part of the VarDial workshop. These focused on regional varieties such as Egyptian, Gulf, Levantine, and North African based on speech broadcast transcriptions (Malmasi et al., 2016) but also acoustic features (Zampieri et al., 2017) and phonetic features (Zampieri et al., 2018) extracted from raw audio. Althobaiti (2020) presents a recent survey of computational work on Arabic dialects. An early effort for creating finer grained parallel dialectal corpus and lexicon was done under the Multi Arabi"
2020.wanlp-1.9,W19-4622,1,0.341438,"translation. Their data was also used for dialectal identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. One issue with the MADAR data in the context of identification is that it was commissioned and not naturally occurring. Concurrently, larger Twitter-based datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018). Researchers are also starting to introduce DA datasets labeled for socio-pragmatics, e.g., (Abbes et al., 2020; Mubarak et al., 2020). The MADAR shared task (Bouamor et al., 2019) comprised two subtasks, one focusing on 21 Arab countries exploiting Twitter data manually labeled at the user level, and another on 25 Arab cities mentioned above. During the same time as NADI, Abdul-Mageed et al. (2020) describe data and models at country, province, and city levels. The NADI shared task follows these pioneering works by availing data to the (Arabic) NLP community, and encouraging work on Arabic dialects. Similar to the MADAR shared task, we include a country-level dialect identification task (Subtask 1), and a sub-country dialect identification task (Subtask 2). However, ou"
2020.wanlp-1.9,2020.lrec-1.165,0,0.0905249,"2 Related Work As we explained in Section 1, Arabic could be viewed as comprised of 3 main types: CA, MSA, and DA. While CA and MSA have been studied and taught extensively, DA has only received more attention relatively recently (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004). A majority of DA computational efforts have targeted creating resources for country or regionally specific dialects (Gadalla et al., 1997; Diab et al., 2010; Al-Sabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). The expansion into multi-dialectal data sets and models to identify them was initially done at the regional level (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). A number of Arabic dialect identification shared tasks were organized as part of the VarDial workshop. These focused on regional varieties such as Egyptian, Gulf, Levantine, and North African based on speech broadcast transcriptions (Malmasi et al., 2016) but also acoustic features (Zampieri et al., 2017) and phonetic features (Zampieri et al., 2018) extracted from raw audio. Alth"
2020.wanlp-1.9,2020.wanlp-1.27,0,0.141485,"Missing"
2020.wanlp-1.9,W14-3911,0,0.018515,"tensively, DA has only received more attention relatively recently (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004). A majority of DA computational efforts have targeted creating resources for country or regionally specific dialects (Gadalla et al., 1997; Diab et al., 2010; Al-Sabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). The expansion into multi-dialectal data sets and models to identify them was initially done at the regional level (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). A number of Arabic dialect identification shared tasks were organized as part of the VarDial workshop. These focused on regional varieties such as Egyptian, Gulf, Levantine, and North African based on speech broadcast transcriptions (Malmasi et al., 2016) but also acoustic features (Zampieri et al., 2017) and phonetic features (Zampieri et al., 2018) extracted from raw audio. Althobaiti (2020) presents a recent survey of computational work on Arabic dialects. An early effort for creating finer grained parallel dialectal corpus and lexicon was done"
2020.wanlp-1.9,2020.wanlp-1.28,0,0.0931316,"Missing"
2021.acl-long.551,W19-4622,0,0.0954076,". (2020). Default baseline is AraBERT. † settings of ANT. 4.4 Dialect Identification Arabic dialect identification can be performed at different levels of granularity, including binary (i.e., MSA-DA), regional (e.g., Gulf, Levantine), country level (e.g., Algeria, Morocco), and recently province level (e.g., the Egyptian province of Cairo, the Saudi province of Al-Madinah) (Abdul-Mageed et al., 2020a, 2021b). Datasets. We fine-tune our models on the following datasets: Arabic Online Commentary (AOC) (Zaidan and Callison-Burch, 2014), ArSarcasmDia (Farha and Magdy, 2020),15 MADAR (sub-task 2) (Bouamor et al., 2019), NADI-2020 (Abdul-Mageed et al., 2020a), and QADI (Abdelali et al., 2020). Details about these datasets are in Table D.1. Baselines. Our baselines are marked in Table 7 caption. Details about the baselines are in Table D.2. Results. As Table 7 shows, our models outperform all SOTA as well as the baseline AraBERT across all classification levels with sizeable margins. These results reflect the powerful and diverse dialectal representation of MARBERT, enabling it to serve wider communities. Although ARBERT is developed mainly for MSA, it also outperforms all other models. 4.5 Named Entity Recog"
2021.acl-long.551,2020.acl-main.747,0,0.642391,"score (77.40) across all six task clusters, outperforming all other models including XLMRLarge (∼ 3.4× larger size). Our models are publicly available at https://github.com/UBCNLP/marbert and ARLUE will be released through the same repository. 1 Introduction Language models (LMs) exploiting self-supervised learning such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019a) have recently emerged as powerful transfer learning tools that help improve a very wide range of natural language processing (NLP) tasks. Multilingual LMs such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (XLM-R) (Conneau et al., 2020) have also been introduced, but are usually outperformed by monolingual models pre-trained with larger vocabulary and bigger language-specific datasets (Virtanen et al., 2019; Antoun et al., 2020; Dadas et al., 2020; † All authors contributed equally. de Vries et al., 2019; Le et al., 2020; Martin et al., 2020; Nguyen and Tuan Nguyen, 2020). Since LMs are costly to pre-train, it is important to keep in mind the end goals they will serve once developed. For example, (i) in addition to their utility on ‘standard’ data, it is useful to endow them with ability to excel on wider real world settings"
2021.acl-long.551,P13-1153,0,0.0268225,"baselines are in Table D.2. Results. As Table 7 shows, our models outperform all SOTA as well as the baseline AraBERT across all classification levels with sizeable margins. These results reflect the powerful and diverse dialectal representation of MARBERT, enabling it to serve wider communities. Although ARBERT is developed mainly for MSA, it also outperforms all other models. 4.5 Named Entity Recognition We fine-tune the models on five NER datasets. Datasets. We use ACE03NW and ACE03BN (Mitchell et al., 2004), ACE04NW (Mitchell et al., 2004), ANERcorp (Benajiba and Rosso, 2007), and TW-NER (Darwish, 2013). Table E.1 shows the 7093 15 ArSarcasmDia carries regional dialect labels. Dataset SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT ANERcorp ACE04NW ACE03BN ACE03NW TW-NER 88.77 91.47 94.92 91.20 65.34 86.78 86.37 91.23 81.40 36.83 87.24 89.93 53.97 87.24 49.16 89.94 89.89 85.41 90.62 54.44 89.13 89.03 91.94 88.09 41.26 84.38 88.24 96.18 90.09 59.17 80.64 85.02 79.05 87.76 66.67 Table 8: NER results in F1 . SOTA by Khalifa and Shaalan (2019). distribution of named entity classes across the five datasets. Baseline. We compare our results with SOTA presented by Khalifa and Shaalan (2019) and fol"
2021.acl-long.551,N19-1423,0,0.636423,"ies of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-theart results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLMRLarge (∼ 3.4× larger size). Our models are publicly available at https://github.com/UBCNLP/marbert and ARLUE will be released through the same repository. 1 Introduction Language models (LMs) exploiting self-supervised learning such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019a) have recently emerged as powerful transfer learning tools that help improve a very wide range of natural language processing (NLP) tasks. Multilingual LMs such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (XLM-R) (Conneau et al., 2020) have also been introduced, but are usually outperformed by monolingual models pre-trained with larger vocabulary and bigger language-specific datasets (Virtanen et al., 2019; Antoun et al., 2020; Dadas et al., 2020; † All authors contributed equally. de Vries et al., 2019; Le et al., 2020; Martin et al., 2020; Nguyen and Tuan N"
2021.acl-long.551,2020.wanlp-1.27,0,0.585,"Missing"
2021.acl-long.551,W18-3930,1,0.857664,"Missing"
2021.acl-long.551,2020.osact-1.5,0,0.524015,"ad/arabic-sentiment-twitter. SOTA ArSAS (3) ASTD (3) SemEval (3) AraNETSent (2) ArSarcSent (3) AraSenTi (3) BBN (3) SYTS (3) TwSaad (2) SAMAR (5) AWATIF (4) TwAbdullah (2) Table 3: SA results (I) in F1 PN . ? Obeid et al. (2020); † AbdulIndividual Downstream Tasks Datasets. We fine-tune the language models on all publicly available SA datasets we could find in addition to those we acquired directly from authors. In total, we have the following 17 MSA and DA datasets: AJGT (Alomari et al., 2017), AraNETSent (Abdul-Mageed et al., 2020b), AraSenTi-Tweet (Al-Twairesh et al., 2017), ArSarcasmSent (Farha and Magdy, 2020), ArSAS (Elmadany et al., 2018), ArSenDLev (Baly et al., 2019), ASTD (Nabil et al., 2015), AWATIF (Abdul-Mageed and Diab, 2012), BBNS & SYTS (Salameh et al., 2015), CAMelSent (Obeid et al., 2020), HARD (Elnagar et al., 2018), LABR (Aly and Atiya, 2013), TwitterAbdullah (Abdulla et al., 2013), TwitterSaad ,14 and SemEval2017 (Rosenthal et al., 2017). Details about the datasets and their splits are in Section A.1. Baselines. We compare to the STOA listed in Table 3 and Table 4 captions. For all datasets with no baseline in Table 3, we consider AraBERT our baseline. Details about SA baselines are"
2021.acl-long.551,2021.eacl-main.65,1,0.695067,"dels are also significantly better than AraBERT, ? the currently best-performing Arabic pre-trained LM. We also introduced AraLU, a large and diverse benchmark for Arabic NLU composed of 42 datasets thematically organized into six main task clusters. ARLUE fills a critical gap in Arabic and multilingual NLP, and promises to help propel innovation and facilitate meaningful comparisons in the field. Our models are publicly available. We also plan to publicly release our ARLUE benchmark. In the future, we plan to explore self-training our language models as a way to improve performance following Khalifa et al. (2021). We also plan to investigate developing more energy efficient models. Acknowledgements We gratefully acknowledges support from the Natural Sciences and Engineering Research Council of Canada, the Social Sciences and Humanities Research Council of Canada, Canadian Foundation for Innovation, Compute Canada and UBC ARCSockeye (https://doi.org/10.14288/SOCKEYE). We also thank the Google TFRC program for providing us with free TPU access. 7096 shared task. In Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 97–110, Barcelona, Spain (Online). Association for Computational"
2021.acl-long.551,S16-1004,0,0.0664168,"Missing"
2021.acl-long.551,2020.emnlp-main.382,0,0.409462,"Missing"
2021.acl-long.551,2020.lrec-1.302,0,0.0630717,"Missing"
2021.acl-long.551,2020.acl-main.653,0,0.080522,"Missing"
2021.acl-long.551,2021.ccl-1.108,0,0.0340968,"Missing"
2021.acl-long.551,W19-5327,0,0.176496,"der rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-theart results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLMRLarge (∼ 3.4× larger size). Our models are publicly available at https://github.com/UBCNLP/marbert and ARLUE will be released through the same repository. 1 Introduction Language models (LMs) exploiting self-supervised learning such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019a) have recently emerged as powerful transfer learning tools that help improve a very wide range of natural language processing (NLP) tasks. Multilingual LMs such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (XLM-R) (Conneau et al., 2020) have also been introduced, but are usually outperformed by monolingual models pre-trained with larger vocabulary and bigger language-specific datasets (Virtanen et al., 2019; Antoun et al., 2020; Dadas et al., 2020; † All authors contributed equally. de Vries et al., 2019; Le et al., 2020; Martin et al., 2020; Nguyen and Tuan Nguyen, 2020). Since LMs are co"
2021.acl-long.551,S18-1001,0,0.0699423,"Missing"
2021.acl-long.551,2020.findings-emnlp.92,0,0.0379156,"Missing"
2021.acl-long.551,D16-1264,0,0.146499,"Missing"
2021.acl-long.551,2020.tacl-1.54,0,0.0134146,"Related Work English and Multilingual LMs. Pre-trained LMs exploiting a self-supervised objective with masking such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) have revolutionized NLP. Multilingual versions of these models such as mBERT and XLM-RoBERTa (Conneau et al., 2020) were also pre-trained. Other models with different objectives and/or architectures such as ALBERT (Lan et al., 2019), T5 (Raffel et al., 2020) and its multilingual version, mT5 (Xue et al., 2021), and GPT3 (Brown et al., 2020) were also introduced. More information about BERT-inspired LMs can be found in Rogers et al. (2020). Non-English LMs. Several models dedicated to individual languages other than English have been developed. These include AraBERT (Antoun et al., 2020) and ArabicBERT (Safaya et al., 2020) for Arabic, Bertje for Dutch (de Vries et al., 2019), CamemBERT (Martin et al., 2020) and FlauBERT (Le et al., 2020) for French, PhoBERT for Vietnamese (Nguyen and Tuan Nguyen, 2020), and the models presented by Virtanen et al. (2019) for Finnish, Dadas et al. (2020) for Polish, and Malmsten et al. (2020) for Swedish. Pyysalo et al. (2020) also create monolingual LMs for 42 languages exploiting Wikipedia dat"
2021.acl-long.551,2020.semeval-1.271,0,0.210453,"articles from 24 Arab countries), and 1.5B words Corpus from El-Khair (2016) (5M articles extracted from 10 news sources). Antoun et al. (2020) evaluate AraBERT on three Arabic downstream tasks. These are (1) sentiment analysis from six different datasets: HARD (Elnagar et al., 2018), ASTD (Nabil et al., 2015), ArsenTDLev (Baly et al., 2019), LABR (Aly and Atiya, 2013), and ArSaS (Elmadany et al., 2018). (2) NER, with the ANERcorp (Benajiba and Rosso, 2007), and (3) Arabic QA, on Arabic-SQuAD and ARCD (Mozannar et al., 2019) datasets. Another Arabic LM that was also introduced is ArabicBERT (Safaya et al., 2020), which is similarly based on BERT architecture. ArabicBERT was pretrained on two datasets only, Arabic Wikipedia and 7089 Source Size #Tokens Books (Hindawi) 650MB 72.5M El-Khair 16GB 1.6B Gigawords 10GB 1.1B OSIAN 2.8GB 292.6M OSCAR-MSA 31GB 3.4B OSCAR-Egyptian 32MB 3.8M Wiki 1.4GB 156.5M Total 61GB 6.5B Table 1: ARBERT ’s pre-train resources. Arabic OSACAR (Su´arez et al., 2019). Since both of these datasets are already included in AraBERT, and Arabic OSACAR1 has significant duplicates, we compare to AraBERT only. GigaBERT (Lan et al., 2020), an Arabic and English LM designed with code-swit"
2021.americasnlp-1.30,2020.wmt-1.42,1,0.858242,"Missing"
2021.americasnlp-1.30,2020.emnlp-main.615,0,0.0311047,"Missing"
2021.americasnlp-1.30,N19-1423,0,0.0688334,"Missing"
2021.americasnlp-1.30,2020.coling-main.351,0,0.50441,"plays the translation model. role of student. With this design, the teacher model 266 needs only monolingual data and does not have to rely on large parallel data. 2.2 MT of Indigenous Languages Unlike high-resource languages such as English and French, Indigenous languages are often lowresource. Due to this, it is common that researchers of Indigenous languages adopt methods that can fare well in low-resource scenarios. This includes using the Transformer architecture and its variants in both low-resource (Adebara et al., 2021, 2020; Przystupa and Abdul-Mageed, 2019) and Indigenous language (Feldman and Coto-Solano, 2020; Orife, 2020; Le and Sadat, 2020) settings. Despite the fact that Indigenous languages face difficulties similar to most low-resource languages, there are some challenges specific to Indigenous languages. As Mager et al. (2018) point out, some Indigenous languages have complex morphological systems and some have various non-standardized orthographic conventions. For example, Micher (2018) shows that in Inuktitut, an Indigineous language in North America with a complex morphological system, a corpus of one million tokens, there are about 225K different types for Inuktitut while about 30K types"
2021.americasnlp-1.30,P84-1044,0,0.263925,"Missing"
2021.americasnlp-1.30,W18-6325,0,0.0149383,"d the dataset it exploits for the downstream Indigenous MT task but that very large space for improvement still exists. The rest of the paper is organized as follows: In Section 2, we introduce recent MT work in lowresource and Indigenous languages settings. In Section 3, we describe how we develop our new language model for ten Indigenous languages. In Section 4, we describe our NMT models. We conclude in Section 5. 2 2.1 Related Work Low-Resource MT Transfer learning is another method that can boost the performance of MT on low-resource languages (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018). The rationale behind one approach to transfer learning is that knowledge obtained while translating high-resource languages may be transferable to translation of lowresource languages. In Zoph et al. (2016), a parent model is first trained on a high-resource language pair (i.e., French to English) then a child model is trained on a low-resource language pair (i.e., Uzbek to English). The Uzbek-English model has 10.7 BLEU score without parent model and 15.0 with the parent model. It is also shown that the more similar the two source languages, the more performance gain is possible. For exampl"
2021.americasnlp-1.30,W17-4123,0,0.0254154,"a multitude of challenges for handling Indigenous languages. Complexity of the morphological systems of some of these languages and lack of standard orthography for writing them are among these challenges (Mager et al., 2018; Littell et al., 2018). The most fundamental issue facing NLP efforts, however, remains the lack of digital textual data that can be exploited for systems development. In this work, we describe a scenario usually faced when trying to develop NLP systems for Indigenous languages and we focus on machine translation (MT). We adopt a neural machine translation approach (NMT) (Koehn, 2017) as our method. We show that, in spite of its recent success on many 1 https://github.com/UBC-NLP/IndT5 Figure 1: A map of the ten Indigenous languages covered by IndT5, our text-to-text Transformer model, and our IndCorpus dataset. The languages are mainly spoken in five Latin American countries. contexts, NMT still struggles in very low-resource settings involving Indigenous languages. This is due to the core difficulty of lack of parallel textual data, but also even monolingual data. Although our main goal in this work in particular is to develop translation models from Spanish to several I"
2021.americasnlp-1.30,P18-1007,0,0.0176016,"us, a collection of ten Indigenous languages and Spanish comprising 1.17 GB of text (∼5.37M sentences), to pre-train IndT5. IndCorpus is collected from both Wikipedia and the Bible. Table 2 provides the size and number of sentences for each language in our dataset. 3.2 IndT5 Vocabulary The T5 (Raffel et al., 2019) model is based on a vocabulary acquired by the SentencePiece library2 using English, French, German, and Romanian web pages from “Colossal Clean Crawled Corpus"" (or C4 for short). We use a similar procedure to create our Indigenous languages vocabulary. Namely, we use SentencePiece (Kudo, 2018) to encode text as WordPiece (Sennrich et al., 2016b) tokens with a vocabulary size of 100K WordPieces extracted from IndCorpus. 3.3 Unsupervised Pre-Training We leverage our unlabeled Indigenous corpus, IndCorpus, to pre-train IndT5. For that, we use a denoising objective (Raffel et al., 2019) that does not require labels. The main idea is feeding the model with corrupted (masked) versions of the original sentence, and training it to reconstruct the original sentence. Inspired by BERT’s objective (i.e., masked language model) (Devlin et al., 2019), the denoising objective (Raffel et al., 2019"
2021.americasnlp-1.30,C18-1222,0,0.0277424,"on to the AmericasNLP 2021 Shared Task on Open Machine Translation. IndT5 and IndCorpus are publicly available for research.1 1 Introduction Indigenous languages are starting to attract attention in the field of natural language processing (NLP), with the number of related publications growing in recent years (Mager et al., 2018). In spite of this interest, there remains a multitude of challenges for handling Indigenous languages. Complexity of the morphological systems of some of these languages and lack of standard orthography for writing them are among these challenges (Mager et al., 2018; Littell et al., 2018). The most fundamental issue facing NLP efforts, however, remains the lack of digital textual data that can be exploited for systems development. In this work, we describe a scenario usually faced when trying to develop NLP systems for Indigenous languages and we focus on machine translation (MT). We adopt a neural machine translation approach (NMT) (Koehn, 2017) as our method. We show that, in spite of its recent success on many 1 https://github.com/UBC-NLP/IndT5 Figure 1: A map of the ten Indigenous languages covered by IndT5, our text-to-text Transformer model, and our IndCorpus dataset. Th"
2021.americasnlp-1.30,I17-2050,0,0.0175718,"our new language model and the dataset it exploits for the downstream Indigenous MT task but that very large space for improvement still exists. The rest of the paper is organized as follows: In Section 2, we introduce recent MT work in lowresource and Indigenous languages settings. In Section 3, we describe how we develop our new language model for ten Indigenous languages. In Section 4, we describe our NMT models. We conclude in Section 5. 2 2.1 Related Work Low-Resource MT Transfer learning is another method that can boost the performance of MT on low-resource languages (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018). The rationale behind one approach to transfer learning is that knowledge obtained while translating high-resource languages may be transferable to translation of lowresource languages. In Zoph et al. (2016), a parent model is first trained on a high-resource language pair (i.e., French to English) then a child model is trained on a low-resource language pair (i.e., Uzbek to English). The Uzbek-English model has 10.7 BLEU score without parent model and 15.0 with the parent model. It is also shown that the more similar the two source languages, the more performance gain"
2021.americasnlp-1.30,P02-1040,0,0.111123,"Missing"
2021.americasnlp-1.30,W17-4770,0,0.0242373,"Missing"
2021.americasnlp-1.30,W19-5431,1,0.795224,"enefit the final the role of teacher and translation model plays the translation model. role of student. With this design, the teacher model 266 needs only monolingual data and does not have to rely on large parallel data. 2.2 MT of Indigenous Languages Unlike high-resource languages such as English and French, Indigenous languages are often lowresource. Due to this, it is common that researchers of Indigenous languages adopt methods that can fare well in low-resource scenarios. This includes using the Transformer architecture and its variants in both low-resource (Adebara et al., 2021, 2020; Przystupa and Abdul-Mageed, 2019) and Indigenous language (Feldman and Coto-Solano, 2020; Orife, 2020; Le and Sadat, 2020) settings. Despite the fact that Indigenous languages face difficulties similar to most low-resource languages, there are some challenges specific to Indigenous languages. As Mager et al. (2018) point out, some Indigenous languages have complex morphological systems and some have various non-standardized orthographic conventions. For example, Micher (2018) shows that in Inuktitut, an Indigineous language in North America with a complex morphological system, a corpus of one million tokens, there are about 2"
2021.americasnlp-1.30,P16-1009,0,0.0352816,"corporating linguistic knowltax parser and a dictionary to generate parallel edge, and knowledge distillation. data. By reordering target-language sentences into Since the main bottleneck of low-resource MT is the lack of abundant parallel textual data, data aug- source-language syntactic structure and then mapping target-language words into source-language mentation is straightforwardly a potential method words with a dictionary, the size of parallel data is to enhance the model performance. Back translaenlarged and translation performance is improved. tion is a way to augment parallel data (Sennrich et al., 2016a). By training a target-to-source translaBaziotis et al. (2020) leverage a language model tion model with original data and feeding in mono- to help enhance the performance of the translation lingual data of target language, synthetic parallel model. Similar to the idea of knowledge distillation data is generated. If the target language is rich in (Hinton et al., 2015), a teacher model and a student textual data, much synthetic parallel data can be model are trained where the language model plays added into training data and may benefit the final the role of teacher and translation model play"
2021.americasnlp-1.30,P16-1162,0,0.0200352,"corporating linguistic knowltax parser and a dictionary to generate parallel edge, and knowledge distillation. data. By reordering target-language sentences into Since the main bottleneck of low-resource MT is the lack of abundant parallel textual data, data aug- source-language syntactic structure and then mapping target-language words into source-language mentation is straightforwardly a potential method words with a dictionary, the size of parallel data is to enhance the model performance. Back translaenlarged and translation performance is improved. tion is a way to augment parallel data (Sennrich et al., 2016a). By training a target-to-source translaBaziotis et al. (2020) leverage a language model tion model with original data and feeding in mono- to help enhance the performance of the translation lingual data of target language, synthetic parallel model. Similar to the idea of knowledge distillation data is generated. If the target language is rich in (Hinton et al., 2015), a teacher model and a student textual data, much synthetic parallel data can be model are trained where the language model plays added into training data and may benefit the final the role of teacher and translation model play"
2021.americasnlp-1.30,P19-1021,0,0.0167582,"language pair (i.e., French to English) then a child model is trained on a low-resource language pair (i.e., Uzbek to English). The Uzbek-English model has 10.7 BLEU score without parent model and 15.0 with the parent model. It is also shown that the more similar the two source languages, the more performance gain is possible. For example, a SpanishEnglish MT model has 16.4 BLEU score without parent model and 31.0 with French-English parent model. The performance gain is much more than when transferring French-English parent model to the more distant context of the Uzbek-English child model. Sennrich and Zhang (2019) argue that instead of using hyperparameters that work in high-resource settings, there should be a set of hyperparameters specific to the low-resource scenario. For example, keeping the vocabulary size small, training a model with relatively small capacity, and having smaller batch size may be beneficial to model performance. When building a vocabulary with BPE, by reducing the the number of merge operations, a smaller vocabulary can be obtained and an inclusion of lowfrequency (sub)words can be avoided. Inclusion of inclusion of low-frequency (sub)words could otherwise negatively influencing"
2021.americasnlp-1.30,D19-1143,0,0.0128495,"e may be beneficial to model performance. When building a vocabulary with BPE, by reducing the the number of merge operations, a smaller vocabulary can be obtained and an inclusion of lowfrequency (sub)words can be avoided. Inclusion of inclusion of low-frequency (sub)words could otherwise negatively influencing representation learning effectiveness. A number of methods and techniques have been proposed to mitigate the effects of having rather small datasets for machine translation. These inLeveraging linguistic knowledge for data augclude data augmentation, transfer learning, hypermentation, Zhou et al. (2019) use a rule-based synparameter tuning, incorporating linguistic knowltax parser and a dictionary to generate parallel edge, and knowledge distillation. data. By reordering target-language sentences into Since the main bottleneck of low-resource MT is the lack of abundant parallel textual data, data aug- source-language syntactic structure and then mapping target-language words into source-language mentation is straightforwardly a potential method words with a dictionary, the size of parallel data is to enhance the model performance. Back translaenlarged and translation performance is improved."
2021.americasnlp-1.30,D16-1163,0,0.0283522,"how the utility of our new language model and the dataset it exploits for the downstream Indigenous MT task but that very large space for improvement still exists. The rest of the paper is organized as follows: In Section 2, we introduce recent MT work in lowresource and Indigenous languages settings. In Section 3, we describe how we develop our new language model for ten Indigenous languages. In Section 4, we describe our NMT models. We conclude in Section 5. 2 2.1 Related Work Low-Resource MT Transfer learning is another method that can boost the performance of MT on low-resource languages (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018). The rationale behind one approach to transfer learning is that knowledge obtained while translating high-resource languages may be transferable to translation of lowresource languages. In Zoph et al. (2016), a parent model is first trained on a high-resource language pair (i.e., French to English) then a child model is trained on a low-resource language pair (i.e., Uzbek to English). The Uzbek-English model has 10.7 BLEU score without parent model and 15.0 with the parent model. It is also shown that the more similar the two source languages,"
2021.calcs-1.6,2020.findings-emnlp.206,0,0.445798,"based on equivalence constraint theory (Poplack, 1980). They observe that the default distribution of synthetic code-mixed sentences created by their method can be quite different from the distribution of real code-mixed sentences in terms of code-mixing measures. This distribution gap can be largely bridged by post-processing the generated code-mixed sentences by binning them into switch point fraction bins and appropriately sampling from these bins. However, the method depends on availability of a word alignment model, which can be erroneous for distant languages (e.g., Hindi and Chinese) (Gupta et al., 2020). Winata et al. (2019) show that a Seq2Seq model with a copy mechanism can be trained to consume parallel monolingual data (concatenated) as input and produce code-mixed data as output, that is distributionally similar to real code-mixed data. Their method needs an external NMT system to obtain monolingual fragment from code-switched text and is expensive to scale to more language pairs. Garg et al. (2018) introduces a novel RNN unit for an RNN based language model that includes separate components to focus on each language in codeswitched text. They utilize training data generated from SeqGAN"
2021.calcs-1.6,2020.emnlp-main.472,1,0.757167,"Missing"
2021.calcs-1.6,W17-7509,0,0.0274576,"y adapting the text by removing mentions, hashtags, emojis, emoticons as well as non-meaning bearing constituents such as URLs. 2.3 Our approach to the English-Hinglish MT task is simple. We first identify the best text-to-text Transformer model on the validation set and follow a curriculum learning procedure to finetune the model for the downstream task. The curriculum learning procedure works such that we first finetune the model using synthetic code-mixed data from our generation method, then further finetune on the gold code-mixed data. This training recipe has been explored previously by Choudhury et al. (2017) and Pratapa et al. (2018) to build code-mixed language models. Curriculum learning itself has been explored previously for different NLP tasks such as parsing (Spitkovsky et al., 2010) and language modeling (Graves et al., 2017). We now present our proposed method to generate synthetic code-mixed text for a given language pair. For our method, we assume having access to large amounts of bitext from a given pair of languages (LG1 and LG2 ) for which we need to generate code-mixed data. Let Bi = {xi , yi } denote the bitext data, where xi and yi correspond to sentences in LG1 and LG2 , respecti"
2021.calcs-1.6,L18-1548,0,0.225933,"bitexts by taking the Hindi sentence from parallel English-Hindi data and changing the script of Hindi from native script (Devanagari) to Roman script while keeping the English sentence intact. Although the resulting Hindi sentence is monolingual, the generated bitexts can help mT5 model to learn the semantics of Romanized Hindi language (mT5 model might be pretrained on native Hindi), along with the relationships between English and romanized Hindi language. To this end, we exploit two large parallel data sources for English-Hindi pairs (Hindi in native script) — IIT Bombay Parallel corpus (Kunchukuttan et al., 2018) (1.49M bitexts) and OPUS corpus (17.2M bitexts). We utilize the Aksharamukha tool to convert native Hindi to romanized Hindi.6 5.2.4 Social Media Adaptation We adapt a publicly available English-Hinglish social media dataset, PHINC (Srivastava and Singh, 2020), to our task. PHINC consists of 13, 738 manually annotated English-Hinglish code-mixed sentences, mainly sourced from social media platforms such as Twitter and Facebook. It covers a wide range of topics (such as sports and politics) and has high quality text (e.g., it handles spelling variations and filters abusive and ambiguous senten"
2021.calcs-1.6,D17-1126,0,0.0526666,"Missing"
2021.calcs-1.6,P19-4007,0,0.0269065,"xt instance, thereby creating a shuffled code-mixed corpus. We train a word2vec model on this shuffled code-mixed Multilingual Pretrained Models Neural models pretrained on monolingual data using a self-supervised objective such as BERT (Devlin et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020) have become integral to NLP systems as they serve as a good starting point for building SOTA models for diverse monolingual tasks. Recently, there is increasing attention to pretraining neural models on multilingual data, resulting in models such as mBERT (Devlin et al., 2019), XLM (Conneau et al., 2019), mBART (Liu et al., 2020) and mT5 (Xue et al., 2021). Especially, generative multilingual models such as mBART (Liu et al., 2020) and mT5 (Xue et al., 2021) can be utilized directly without additional neural network components to solve summarization, MT, and other natural language generation tasks. These generative models are trained using a self-supervised pretraining objective based on span-corruption objective (mBART and mT5) and sentence shuffling objective (mBART). Training data for these models are prepared by concatenating monolingual texts from multiple languages (e.g., 25 for mBART,"
2021.calcs-1.6,2020.acl-main.703,0,0.0400429,"maine”, “ye”, “kabhi”, “nah”, “dekhi”, “I’ve never”, “never seen”, “seen it”, “maine ye”, “ye kabhi”, “kabhi nah”, “nah dekhi”}. A shuffled code-mixed sentence can be, “I’ve ye_kabhi never seen_it seen never_seen it kabhi_nah I’ve_never maine_ye ye kabhi nah dekhi maine nah_dekhi”. We create one shuffled code-mixed sentence per bitext instance, thereby creating a shuffled code-mixed corpus. We train a word2vec model on this shuffled code-mixed Multilingual Pretrained Models Neural models pretrained on monolingual data using a self-supervised objective such as BERT (Devlin et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020) have become integral to NLP systems as they serve as a good starting point for building SOTA models for diverse monolingual tasks. Recently, there is increasing attention to pretraining neural models on multilingual data, resulting in models such as mBERT (Devlin et al., 2019), XLM (Conneau et al., 2019), mBART (Liu et al., 2020) and mT5 (Xue et al., 2021). Especially, generative multilingual models such as mBART (Liu et al., 2020) and mT5 (Xue et al., 2021) can be utilized directly without additional neural network components to solve summarization, MT, and othe"
2021.calcs-1.6,L18-1218,0,0.0225407,"Missing"
2021.calcs-1.6,N19-1423,0,0.0282525,"e”, “never”, “seen”, “it”, “maine”, “ye”, “kabhi”, “nah”, “dekhi”, “I’ve never”, “never seen”, “seen it”, “maine ye”, “ye kabhi”, “kabhi nah”, “nah dekhi”}. A shuffled code-mixed sentence can be, “I’ve ye_kabhi never seen_it seen never_seen it kabhi_nah I’ve_never maine_ye ye kabhi nah dekhi maine nah_dekhi”. We create one shuffled code-mixed sentence per bitext instance, thereby creating a shuffled code-mixed corpus. We train a word2vec model on this shuffled code-mixed Multilingual Pretrained Models Neural models pretrained on monolingual data using a self-supervised objective such as BERT (Devlin et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020) have become integral to NLP systems as they serve as a good starting point for building SOTA models for diverse monolingual tasks. Recently, there is increasing attention to pretraining neural models on multilingual data, resulting in models such as mBERT (Devlin et al., 2019), XLM (Conneau et al., 2019), mBART (Liu et al., 2020) and mT5 (Xue et al., 2021). Especially, generative multilingual models such as mBART (Liu et al., 2020) and mT5 (Xue et al., 2021) can be utilized directly without additional neural network components to solve"
2021.calcs-1.6,2020.tacl-1.47,0,0.111061,"a shuffled code-mixed corpus. We train a word2vec model on this shuffled code-mixed Multilingual Pretrained Models Neural models pretrained on monolingual data using a self-supervised objective such as BERT (Devlin et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020) have become integral to NLP systems as they serve as a good starting point for building SOTA models for diverse monolingual tasks. Recently, there is increasing attention to pretraining neural models on multilingual data, resulting in models such as mBERT (Devlin et al., 2019), XLM (Conneau et al., 2019), mBART (Liu et al., 2020) and mT5 (Xue et al., 2021). Especially, generative multilingual models such as mBART (Liu et al., 2020) and mT5 (Xue et al., 2021) can be utilized directly without additional neural network components to solve summarization, MT, and other natural language generation tasks. These generative models are trained using a self-supervised pretraining objective based on span-corruption objective (mBART and mT5) and sentence shuffling objective (mBART). Training data for these models are prepared by concatenating monolingual texts from multiple languages (e.g., 25 for mBART, 107 for mT5). It is not cl"
2021.calcs-1.6,W18-3817,0,0.74051,"bc.ca Abstract et al., 2020). Building NLP systems that can handle code-mixing is challenging as the space of valid grammatical and lexical configurations can be large due to presence of syntactic structures from more than one linguistic system (Pratapa et al., 2018). In this work, we focus on building a machine translation (MT) system that converts a monolingual sequence of words into a code-mixed sequence. More specifically, we focus on translating from English to Hindi code-mixed with English (i.e., Hinglish). In the literature, work has been done on translating from Hinglish into English (Dhar et al., 2018; Srivastava and Singh, 2020). To illustrate both directions, we provide Figure 1. The Figure presents sample translation pairs for Hinglish to English as well as English to Hinglish. The challenges for solving this task include: (i) lack of Hindi data in roman script (words highlighted in cyan color), (ii) non-standard spellings (e.g., ‘isme’ vs ‘is me’), (iii) token-level ambiguity across the two languages (e.g., Hindi ‘main’ vs. English ‘main’), (iv) non-standard casing (e.g., ROTTEN TOMATOES), (v) informal writing style, and (vi) paucity of English-Hinglish parallel data. Compared with Hin"
2021.calcs-1.6,D18-1346,0,0.0209816,"ns and appropriately sampling from these bins. However, the method depends on availability of a word alignment model, which can be erroneous for distant languages (e.g., Hindi and Chinese) (Gupta et al., 2020). Winata et al. (2019) show that a Seq2Seq model with a copy mechanism can be trained to consume parallel monolingual data (concatenated) as input and produce code-mixed data as output, that is distributionally similar to real code-mixed data. Their method needs an external NMT system to obtain monolingual fragment from code-switched text and is expensive to scale to more language pairs. Garg et al. (2018) introduces a novel RNN unit for an RNN based language model that includes separate components to focus on each language in codeswitched text. They utilize training data generated from SeqGAN along with syntactic features (e.g., Part-of-Speech tags, Brown word clusters, language ID feature) to train their RNN based language model. Their method involves added cost to train SeqGAN model and expensive to scale to more language pairs. replacing aligned named entities and noun phrases from English; and (ii) training a Seq2Seq model to take English sentence as input and produce the code-mixed senten"
2021.calcs-1.6,S15-2001,0,0.0637929,"Missing"
2021.calcs-1.6,2021.eacl-demos.24,0,0.547372,"uage pairs. replacing aligned named entities and noun phrases from English; and (ii) training a Seq2Seq model to take English sentence as input and produce the code-mixed sentences created in the first phase. Their approach depends on the availability of a word alignment tool, a part-of-speech tagger, and knowledge of what constituents to replace in order to create a code-mixed sentence. By contrast, our proposed method based on bilingual word embeddings to generate code-mixed data does not require external software such as a word alignment tool, part-of-speech tagger, or constituency parser. Rizvi et al. (2021) develops the toolkit for code-mixed data generation for a given language pair using two linguistic theories: equivalence constraint (codemixing following the grammatical structure of both the languages) and matrix language theory (McClure, 1995) (code-mixing by fixing a language that lends grammatical structure while other language lends its vocabulary). For comparison, we use this tool to implement the code-mixed data generation method based on equivalence constraint theory. 2.2 Code-Mixed MT Building MT systems involving code-mixed language is a less researched area. Existing MT systems tra"
2021.calcs-1.6,N10-1116,0,0.0205762,"le. We first identify the best text-to-text Transformer model on the validation set and follow a curriculum learning procedure to finetune the model for the downstream task. The curriculum learning procedure works such that we first finetune the model using synthetic code-mixed data from our generation method, then further finetune on the gold code-mixed data. This training recipe has been explored previously by Choudhury et al. (2017) and Pratapa et al. (2018) to build code-mixed language models. Curriculum learning itself has been explored previously for different NLP tasks such as parsing (Spitkovsky et al., 2010) and language modeling (Graves et al., 2017). We now present our proposed method to generate synthetic code-mixed text for a given language pair. For our method, we assume having access to large amounts of bitext from a given pair of languages (LG1 and LG2 ) for which we need to generate code-mixed data. Let Bi = {xi , yi } denote the bitext data, where xi and yi correspond to sentences in LG1 and LG2 , respectively. Let ngrams(n, xi , yi ) denote the set of unique n-grams in xi and yi . Let cumulative-ngrams(n, xi , yi ) = ∪j=n j=1 ngrams(j, xi , yi ) denote the cumulative set of unique n-gra"
2021.calcs-1.6,2020.wnut-1.7,0,0.240398,"l., 2020). Building NLP systems that can handle code-mixing is challenging as the space of valid grammatical and lexical configurations can be large due to presence of syntactic structures from more than one linguistic system (Pratapa et al., 2018). In this work, we focus on building a machine translation (MT) system that converts a monolingual sequence of words into a code-mixed sequence. More specifically, we focus on translating from English to Hindi code-mixed with English (i.e., Hinglish). In the literature, work has been done on translating from Hinglish into English (Dhar et al., 2018; Srivastava and Singh, 2020). To illustrate both directions, we provide Figure 1. The Figure presents sample translation pairs for Hinglish to English as well as English to Hinglish. The challenges for solving this task include: (i) lack of Hindi data in roman script (words highlighted in cyan color), (ii) non-standard spellings (e.g., ‘isme’ vs ‘is me’), (iii) token-level ambiguity across the two languages (e.g., Hindi ‘main’ vs. English ‘main’), (iv) non-standard casing (e.g., ROTTEN TOMATOES), (v) informal writing style, and (vi) paucity of English-Hinglish parallel data. Compared with Hinglish to English translation,"
2021.calcs-1.6,K19-1026,0,0.0162435,"constraint theory (Poplack, 1980). They observe that the default distribution of synthetic code-mixed sentences created by their method can be quite different from the distribution of real code-mixed sentences in terms of code-mixing measures. This distribution gap can be largely bridged by post-processing the generated code-mixed sentences by binning them into switch point fraction bins and appropriately sampling from these bins. However, the method depends on availability of a word alignment model, which can be erroneous for distant languages (e.g., Hindi and Chinese) (Gupta et al., 2020). Winata et al. (2019) show that a Seq2Seq model with a copy mechanism can be trained to consume parallel monolingual data (concatenated) as input and produce code-mixed data as output, that is distributionally similar to real code-mixed data. Their method needs an external NMT system to obtain monolingual fragment from code-switched text and is expensive to scale to more language pairs. Garg et al. (2018) introduces a novel RNN unit for an RNN based language model that includes separate components to focus on each language in codeswitched text. They utilize training data generated from SeqGAN along with syntactic"
2021.calcs-1.8,bouamor-etal-2014-multidialectal,0,0.022673,"esults in Section 8. We conclude in Section 9. 2 DA Arabic MT Resources. There are also efforts to develop dialectal Arabic MT resources. For example, Meftouh et al. (2015) present the Parallel Arabic Dialect Corpus (PADIC),2 which is a multi-dialect corpus including MSA, Algerian, Tunisian, Palestinian, and Syrian. Recently, Sajjad et al. (2020a) also introduced AraBench, an evaluation suite for dialectal Arabic to English MT. AraBench consists of five publicly available datasets: Arabic-Dialect/English Parallel Text (APT) (Zbib et al., 2012), Multi-dialectal Parallel Corpus of Arabic (MDC) (Bouamor et al., 2014), MADAR Corpus (Bouamor et al., 2018), Qatari-English speech corpus (Elmahdy et al., 2014), and the English Bible translated into MSA.3 Related Work A thread of research on code-mixed MT focuses on automatically generating synthetic code-mixed data to improve the downstream task. This includes attempts to generate linguistically-motivated sequences (Pratapa et al., 2018). Some work leverages sequence-to-sequence (S2S) models (Winata et al., 2019) to generate code-mixing exploiting an external neural MT system, while others (Garg et al., 2018) use a recurrent neural network along with data gene"
2021.calcs-1.8,elmahdy-etal-2014-development,0,0.15848,"forts to develop dialectal Arabic MT resources. For example, Meftouh et al. (2015) present the Parallel Arabic Dialect Corpus (PADIC),2 which is a multi-dialect corpus including MSA, Algerian, Tunisian, Palestinian, and Syrian. Recently, Sajjad et al. (2020a) also introduced AraBench, an evaluation suite for dialectal Arabic to English MT. AraBench consists of five publicly available datasets: Arabic-Dialect/English Parallel Text (APT) (Zbib et al., 2012), Multi-dialectal Parallel Corpus of Arabic (MDC) (Bouamor et al., 2014), MADAR Corpus (Bouamor et al., 2018), Qatari-English speech corpus (Elmahdy et al., 2014), and the English Bible translated into MSA.3 Related Work A thread of research on code-mixed MT focuses on automatically generating synthetic code-mixed data to improve the downstream task. This includes attempts to generate linguistically-motivated sequences (Pratapa et al., 2018). Some work leverages sequence-to-sequence (S2S) models (Winata et al., 2019) to generate code-mixing exploiting an external neural MT system, while others (Garg et al., 2018) use a recurrent neural network along with data generated by a sequence generative adversarial network (SeqGAN) and grammatical information su"
2021.calcs-1.8,D18-1346,0,0.0206948,"Multi-dialectal Parallel Corpus of Arabic (MDC) (Bouamor et al., 2014), MADAR Corpus (Bouamor et al., 2018), Qatari-English speech corpus (Elmahdy et al., 2014), and the English Bible translated into MSA.3 Related Work A thread of research on code-mixed MT focuses on automatically generating synthetic code-mixed data to improve the downstream task. This includes attempts to generate linguistically-motivated sequences (Pratapa et al., 2018). Some work leverages sequence-to-sequence (S2S) models (Winata et al., 2019) to generate code-mixing exploiting an external neural MT system, while others (Garg et al., 2018) use a recurrent neural network along with data generated by a sequence generative adversarial network (SeqGAN) and grammatical information such as from a part of speech tagger to generate code-mixed sequences. These methods have dependencies and can be 3 Code-Switching Shared Task The goal of the shared tasks on machine translation in code-switching settings4 is to encourage building MT systems that translate a source sentence into a target sentence while one of the directions contains 2 https://sites.google.com/site/ torjmanepnr/6-corpus 3 The United Bible Societies https://www.bible.com 4 h"
2021.calcs-1.8,2021.eacl-main.298,1,0.797223,"Missing"
2021.calcs-1.8,J82-2005,0,0.604454,"Missing"
2021.calcs-1.8,P14-2125,0,0.0232078,"f MSAEA sentences and their human and machine translations. We highlight problematic translations caused by nixing of Egyptian Arabic with MSA. Through work related to the shared task, we target the following three main research questions: costly to scale beyond one language pair. Arabic MT. For Arabic, some work has focused on translating between MSA and Arabic dialects. For instance, Zbib et al. (2012) studied the impact of combined dialectal and MSA data on dialect/MSA to English MT performance. Sajjad et al. (2013) uses MSA as a pivot language for translating Arabic dialects into English. Salloum et al. (2014) investigate the effect of sentence-level dialect identification and several linguistic features for MSA/dialect-English translation. Guellil et al. (2017) propose an neural machine translation (NMT) system for Arabic dialects using a vanilla recurrent neural networks (RNN) encoder-decoder model for translating Algerian Arabic written in a mixture of Arabizi and Arabic characters into MSA. Baniata et al. (2018) present an NMT system to translate Levantine (Jordanian, Syrian, and Palestinian) and Maghrebi (Algerian, Moroccan, Tunisia) to MSA, and MSA to English. Farhan et al. (2020), propose un"
2021.calcs-1.8,2020.tacl-1.47,0,0.0213816,"uare-root learning rate scheduler with a value of 5e−4 and 4, 000 warmup updates. For the loss function, we use label smoothed cross entropy with a smoothing strength of 0.1. We run the Moses tokenizer (Koehn et al., 2007) on our input before passing data to the model. For vocabulary, we use a joint Byte-Pair Encoding (BPE) (Sennrich et al., 2015) vocabulary with 64K split operations for subword segmentation. 5.2 Data Splits and Pre-Processing Pre-Trained Seq2Seq Language Models We also fine-tune two state-of-the-art pre-trained multlingual generative models, mT5 (Xue et al., 2020) and mBART (Liu et al., 2020) on DA-Train for 100 epochs. We use early stopping during finetuning and identify the best model on DA-Dev. We use the HuggingFace (Wolf et al., 2020) implemenData Splits. For our experiments, we split the MSA and DA data as follows: 5 We do not make use of the Maghrebi data due to the considerable linguistic differences between Maghrebi and the the Egyptian dialect we target in this work. 59 4 HA Ó Ó  JJ . Ë@ á  ¯PA«  ð Y» AJK á  ¯PA«  . á  ¯ Source: S2ST we don’t know for sure and the girls don’t know finn . mT5 we can’t make sure and we don’t know where the girls are mBART we don’t"
2021.calcs-1.8,Y15-1004,0,0.022345,"questions. We also develop powerful models for translating from MSAEA to English. The rest of the paper is organized as follows: Section 2 discusses related work. The shared task is described in Section 3. Section 4 describes external parallel data we exploit to build our models. Section 5 presents the proposed MT models. Section 6 presents our experiments, and our different settings. We provide evaluation on Dev data in Section 7 and official results in Section 8. We conclude in Section 9. 2 DA Arabic MT Resources. There are also efforts to develop dialectal Arabic MT resources. For example, Meftouh et al. (2015) present the Parallel Arabic Dialect Corpus (PADIC),2 which is a multi-dialect corpus including MSA, Algerian, Tunisian, Palestinian, and Syrian. Recently, Sajjad et al. (2020a) also introduced AraBench, an evaluation suite for dialectal Arabic to English MT. AraBench consists of five publicly available datasets: Arabic-Dialect/English Parallel Text (APT) (Zbib et al., 2012), Multi-dialectal Parallel Corpus of Arabic (MDC) (Bouamor et al., 2014), MADAR Corpus (Bouamor et al., 2018), Qatari-English speech corpus (Elmahdy et al., 2014), and the English Bible translated into MSA.3 Related Work A"
2021.calcs-1.8,tiedemann-2012-parallel,0,0.0439843,"Missing"
2021.calcs-1.8,N19-4009,0,0.0234411,"f, Levantine, Nile, and Maghrebi). In our work, we use only the Gulf, Levantine, and Nile (Egyptian) dialects, and exclude Maghrebi.5 Qatari-English Speech Corpus. This parallel corpus comprises 14.7k Qatari-English sentences collected by Elmahdy et al. (2014) from talk-show programs and Qatari TV series. More details about all our parallel dialectal-English datasets are in Table 3. 4.3 5 5.1 MT Models From-Scratch Seq2Seq Models We train our models on the MSA-English parallel data described in section 4.1 on MSA-Train with a Transformer (Vaswani et al., 2017) model as implemented in Fairseq (Ott et al., 2019). For that, we follow Ott et al. (2018) in using 6 blocks for each of the encoder and decoder parts. We use a learning rate of 0.25, a dropout of 0.3, and a batch size 4, 000 tokens. For the optimizer, we use Adam (Kingma and Ba, 2014) with beta coefficients of 0.9 and 0.99 which control an exponential decay rate of running averages, with a weight decay of 10−4 . We also apply an inverse square-root learning rate scheduler with a value of 5e−4 and 4, 000 warmup updates. For the loss function, we use label smoothed cross entropy with a smoothing strength of 0.1. We run the Moses tokenizer (Koeh"
2021.calcs-1.8,W18-6301,0,0.0274412,"ur work, we use only the Gulf, Levantine, and Nile (Egyptian) dialects, and exclude Maghrebi.5 Qatari-English Speech Corpus. This parallel corpus comprises 14.7k Qatari-English sentences collected by Elmahdy et al. (2014) from talk-show programs and Qatari TV series. More details about all our parallel dialectal-English datasets are in Table 3. 4.3 5 5.1 MT Models From-Scratch Seq2Seq Models We train our models on the MSA-English parallel data described in section 4.1 on MSA-Train with a Transformer (Vaswani et al., 2017) model as implemented in Fairseq (Ott et al., 2019). For that, we follow Ott et al. (2018) in using 6 blocks for each of the encoder and decoder parts. We use a learning rate of 0.25, a dropout of 0.3, and a batch size 4, 000 tokens. For the optimizer, we use Adam (Kingma and Ba, 2014) with beta coefficients of 0.9 and 0.99 which control an exponential decay rate of running averages, with a weight decay of 10−4 . We also apply an inverse square-root learning rate scheduler with a value of 5e−4 and 4, 000 warmup updates. For the loss function, we use label smoothed cross entropy with a smoothing strength of 0.1. We run the Moses tokenizer (Koehn et al., 2007) on our input before pas"
2021.calcs-1.8,K19-1026,0,0.0193636,"f five publicly available datasets: Arabic-Dialect/English Parallel Text (APT) (Zbib et al., 2012), Multi-dialectal Parallel Corpus of Arabic (MDC) (Bouamor et al., 2014), MADAR Corpus (Bouamor et al., 2018), Qatari-English speech corpus (Elmahdy et al., 2014), and the English Bible translated into MSA.3 Related Work A thread of research on code-mixed MT focuses on automatically generating synthetic code-mixed data to improve the downstream task. This includes attempts to generate linguistically-motivated sequences (Pratapa et al., 2018). Some work leverages sequence-to-sequence (S2S) models (Winata et al., 2019) to generate code-mixing exploiting an external neural MT system, while others (Garg et al., 2018) use a recurrent neural network along with data generated by a sequence generative adversarial network (SeqGAN) and grammatical information such as from a part of speech tagger to generate code-mixed sequences. These methods have dependencies and can be 3 Code-Switching Shared Task The goal of the shared tasks on machine translation in code-switching settings4 is to encourage building MT systems that translate a source sentence into a target sentence while one of the directions contains 2 https://"
2021.calcs-1.8,W19-6621,0,0.0419975,"Missing"
2021.calcs-1.8,P02-1040,0,0.108832,"Missing"
2021.calcs-1.8,2020.emnlp-demos.6,0,0.024961,"moothing strength of 0.1. We run the Moses tokenizer (Koehn et al., 2007) on our input before passing data to the model. For vocabulary, we use a joint Byte-Pair Encoding (BPE) (Sennrich et al., 2015) vocabulary with 64K split operations for subword segmentation. 5.2 Data Splits and Pre-Processing Pre-Trained Seq2Seq Language Models We also fine-tune two state-of-the-art pre-trained multlingual generative models, mT5 (Xue et al., 2020) and mBART (Liu et al., 2020) on DA-Train for 100 epochs. We use early stopping during finetuning and identify the best model on DA-Dev. We use the HuggingFace (Wolf et al., 2020) implemenData Splits. For our experiments, we split the MSA and DA data as follows: 5 We do not make use of the Maghrebi data due to the considerable linguistic differences between Maghrebi and the the Egyptian dialect we target in this work. 59 4 HA Ó Ó  JJ . Ë@ á  ¯PA«  ð Y» AJK á  ¯PA«  . á  ¯ Source: S2ST we don’t know for sure and the girls don’t know finn . mT5 we can’t make sure and we don’t know where the girls are mBART we don’t know where to make sure Source: S2ST and we don’t know where the girls are QK A« AK@   ¬Q«@ ÑêªJK Ó  ¯ñÓ  JË@ áÓ ùÖÞ QË@ à@ ñkB@  KQË@ .  È"
2021.calcs-1.8,P18-1143,0,0.0234398,"h, an evaluation suite for dialectal Arabic to English MT. AraBench consists of five publicly available datasets: Arabic-Dialect/English Parallel Text (APT) (Zbib et al., 2012), Multi-dialectal Parallel Corpus of Arabic (MDC) (Bouamor et al., 2014), MADAR Corpus (Bouamor et al., 2018), Qatari-English speech corpus (Elmahdy et al., 2014), and the English Bible translated into MSA.3 Related Work A thread of research on code-mixed MT focuses on automatically generating synthetic code-mixed data to improve the downstream task. This includes attempts to generate linguistically-motivated sequences (Pratapa et al., 2018). Some work leverages sequence-to-sequence (S2S) models (Winata et al., 2019) to generate code-mixing exploiting an external neural MT system, while others (Garg et al., 2018) use a recurrent neural network along with data generated by a sequence generative adversarial network (SeqGAN) and grammatical information such as from a part of speech tagger to generate code-mixed sequences. These methods have dependencies and can be 3 Code-Switching Shared Task The goal of the shared tasks on machine translation in code-switching settings4 is to encourage building MT systems that translate a source se"
2021.calcs-1.8,2020.coling-main.447,0,0.258888,"k is described in Section 3. Section 4 describes external parallel data we exploit to build our models. Section 5 presents the proposed MT models. Section 6 presents our experiments, and our different settings. We provide evaluation on Dev data in Section 7 and official results in Section 8. We conclude in Section 9. 2 DA Arabic MT Resources. There are also efforts to develop dialectal Arabic MT resources. For example, Meftouh et al. (2015) present the Parallel Arabic Dialect Corpus (PADIC),2 which is a multi-dialect corpus including MSA, Algerian, Tunisian, Palestinian, and Syrian. Recently, Sajjad et al. (2020a) also introduced AraBench, an evaluation suite for dialectal Arabic to English MT. AraBench consists of five publicly available datasets: Arabic-Dialect/English Parallel Text (APT) (Zbib et al., 2012), Multi-dialectal Parallel Corpus of Arabic (MDC) (Bouamor et al., 2014), MADAR Corpus (Bouamor et al., 2018), Qatari-English speech corpus (Elmahdy et al., 2014), and the English Bible translated into MSA.3 Related Work A thread of research on code-mixed MT focuses on automatically generating synthetic code-mixed data to improve the downstream task. This includes attempts to generate linguistic"
2021.calcs-1.8,N12-1006,0,0.435266,"ally, we take as our objective translating between Modern Standard Arabic (MSA) mixed with Egyptian Arabic (EA) (source; collectively abbreviated here as MSAEA) into English (target). Table1 shows two examples of MSAEA sentences and their human and machine translations. We highlight problematic translations caused by nixing of Egyptian Arabic with MSA. Through work related to the shared task, we target the following three main research questions: costly to scale beyond one language pair. Arabic MT. For Arabic, some work has focused on translating between MSA and Arabic dialects. For instance, Zbib et al. (2012) studied the impact of combined dialectal and MSA data on dialect/MSA to English MT performance. Sajjad et al. (2013) uses MSA as a pivot language for translating Arabic dialects into English. Salloum et al. (2014) investigate the effect of sentence-level dialect identification and several linguistic features for MSA/dialect-English translation. Guellil et al. (2017) propose an neural machine translation (NMT) system for Arabic dialects using a vanilla recurrent neural networks (RNN) encoder-decoder model for translating Algerian Arabic written in a mixture of Arabizi and Arabic characters int"
2021.eacl-main.298,P17-1067,1,0.891347,"Missing"
2021.eacl-main.298,2020.acl-main.747,0,0.0953523,"Missing"
2021.eacl-main.298,2020.osact-1.17,1,0.828348,"Missing"
2021.eacl-main.65,P17-1178,0,0.0183196,"PXA¯ èPY®K . ð (magically; idiomatic expression) FT Pred. PER LOC PER LOC PER PER PER Table 8: Sample false positives mitigated by self-training. These were correctly predicted as the unnamed entity “O” by the self-trained model. no. (1) (2) (3) (4) (5) (6) Word à@ ñkB@ ú «X@Q.ÊË XCm.Ì &apos;@ ø Ym.× È QK X àA ¯ ø Pð Q m .Ì &apos;@ áK P àñ Gold ORG FT ORG ST O PER PER O PER PER O PER PER O PER PER O PER PER O settings when gold labels are rare in the target language. Hajmohammadi et al. (2015) proposed a combination of active learning and self-training for cross-lingual sentiment classification. Pan et al. (2017) made use of self-training for named entity tagging and linking across 282 different languages. Lastly, Dong and de Melo (2019) employed selftraining to improve zero-shot cross-lingual classification with mBERT (Devlin et al., 2018). 7 Table 9: NER task. Sample false negatives produced by self-training. using English-resources only using two BiLSTM networks to learn common and language-specific features. Xie et al. (2018) made use of bilingual word embeddings with self-attention to learn crosslingual NER for low-resource languages. Multilingual extensions of LMs have emerged through joint pre-"
2021.eacl-main.65,W10-2606,0,0.0484166,"ecome useful for few-shot and zero-shot cross-lingual settings, where there is little or no access to labeled data in the target language. For instance Conneau et al. (2019) evaluated a cross-lingual version of RoBERTa (Liu et al., 2019), namely XLM-R, on cross-lingual learning across different tasks such as question answering, text classification, and named entity recognition. Self-Training. Self-Training is a semisupervised technique to improve learning using unlabeled data. Self-training has been successfully applied to NER (Kozareva et al., 2005), POS tagging (Wang et al., 2007), parsing (Sagae, 2010) and text classification (Van Asch and Daelemans, 2016). Self-training has also been applied in cross-lingual Conclusion Even though pre-trained language models have improved many NLP tasks, they still need labeled data for fine-tuning. We show how self-training can boost the performance of pre-trained language models in zero- and few-shot settings on various Arabic varieties. We apply our approach to two sequence labeling tasks (NER and POS), establishing new state-of-the-art results on both. Through in-depth error analysis and an ablation study, we uncover why our models work and where they"
2021.eacl-main.65,J14-2008,1,0.856192,"Missing"
2021.eacl-main.65,W15-3205,0,0.0141075,"ncorrectly in different contexts by the self-trained model. Context of correct classifiRelated Work Sequence Labeling. Recent work on sequence labeling usually involves using a word- or characterlevel neural network with a CRF layer (Lample et al., 2016; Ma and Hovy, 2016). These architectures have also been applied to Arabic sequence tagging (Gridach, 2016; Alharbi et al., 2018; Khalifa and Shaalan, 2019; Al-Smadi et al., 2020; El Bazi and Laachfoubi, 2019), producing better or comparable results to classical rule-based approaches (Shaalan and Oudah, 2014). We refer the interested reader to (Shoufan and Alameri, 2015) and (AlAyyoub et al., 2018) for surveys on Arabic NLP. Pre-trained Language Models. Language models, based on Transformers (Vaswani et al., 2017), and pre-trained with the masked language modeling (MLM) objective have seen wide use in various NLP tasks. Examples include BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), MASS (Song et al., 2019), and ELECTRA (Clark et al., 2020). While they have been applied to several tasks, including text classification, question answering, named entity recognition (Conneau et al., 2019), and POS tagging (Tsai et al., 2019), a sufficiently large amount"
2021.eacl-main.65,D19-1374,0,0.146817,"t al., 2019), pre-trained on large amounts of unlabeled data and then fine-tuned on labeled taskspecific data, has become a popular approach for 1 Our code and fine-tuned models can be accessed at: https://github.com/mohammadKhalifa/ zero-shot-arabic-dialects improving downstream task performance. This pre-training then fine-tuning scheme has been successfully applied to several tasks, including question answering (Yang et al., 2019), social meaning detection (Abdul-Mageed et al., 2020d), text classification (Liu et al., 2019), named entity recognition (NER), and part-of-speech (POS) tagging (Tsai et al., 2019; Conneau et al., 2019). The same setup also works well for cross-lingual learning (Lample and Conneau, 2019; Conneau et al., 2019). Given that it is very expensive to glean labeled resources for all language varieties and dialects, a question arises: “How can we leverage resourcerich dialects to develop models nuanced to downstream tasks for resource-scarce ones?”. In this work, we aim to answer this particular question by applying self-training to unlabeled target dialect data. We empirically show that self-training is indeed an effective strategy in zero-shot (where no gold dialectal data a"
2021.eacl-main.65,D18-1034,0,0.0281985,"old labels are rare in the target language. Hajmohammadi et al. (2015) proposed a combination of active learning and self-training for cross-lingual sentiment classification. Pan et al. (2017) made use of self-training for named entity tagging and linking across 282 different languages. Lastly, Dong and de Melo (2019) employed selftraining to improve zero-shot cross-lingual classification with mBERT (Devlin et al., 2018). 7 Table 9: NER task. Sample false negatives produced by self-training. using English-resources only using two BiLSTM networks to learn common and language-specific features. Xie et al. (2018) made use of bilingual word embeddings with self-attention to learn crosslingual NER for low-resource languages. Multilingual extensions of LMs have emerged through joint pre-training on multiple languages. Examples include mBERT (Devlin et al., 2018), XLM (Lample and Conneau, 2019) and XLMRoBERTa (Conneau et al., 2019). Such multilingual models have become useful for few-shot and zero-shot cross-lingual settings, where there is little or no access to labeled data in the target language. For instance Conneau et al. (2019) evaluated a cross-lingual version of RoBERTa (Liu et al., 2019), namely"
2021.eacl-main.65,P19-1226,0,0.030812,"Missing"
2021.eacl-main.65,P11-2007,0,0.0403459,"tch size of 16. We typically fine-tune for 20 epochs, keeping the best model on the development set for testing. We report results on the test split for each dataset, across the two tasks. For all BiLSTM experiments, we use the same hyper-parameters as (Khalifa and Shaalan, 2019). For the standard fine-tuning experiments, we use the same train/development/test split as in (Khalifa and Shaalan, 2019) for NER, and the same split provided by (Darwish et al., 2018) for POS tagging. For all the self-training experiments, we use the dialect subset of the Arabic online news commentary (AOC) dataset (Zaidan and Callison-Burch, 2011), comprising the EGY, GLF, and LEV varieties limiting to equal sizes of 9K examples per dialect (total =27K) 3 . We use the split from (Elaraby and Abdul-Mageed, 2018) of AOC, removing the dialect labels and just using the comments themselves for our self-training. Each iteration involved fine-tuning the model for K = 5 epochs. As a stopping criterion, we use early stopping with patience of 10 epochs. Other hyper-parameters are set as listed before. For selecting confident samples, we experiment with a fixed number of top samples S = [50, 100, 200] and selection based on a probability threshol"
2021.eacl-main.65,J14-1006,0,0.04407,"Missing"
2021.nlp4if-1.9,N18-2004,1,0.0700937,"a, 5 Qatar Computing Research Institute, HBKU tariq@cs.columbia.edu, amal@gatech.edu, alimoham@buffalo.edu muhammad.mageed@ubc.ca, pnakov@hbku.edu.qa Abstract Our work here contributes to these efforts a new dataset and baseline results on it. In particular, we create a new dataset for stance detection of claims collected from a number of websites covering different domains such as politics, health, and economics. The websites cover several Arab countries, which enables wider applicability of our dataset. This compares favorably to previous work for Arabic stance detection such as the work of Baly et al. (2018), who focused on a single country. We use the websites as our source to collect true and false claims, and we carefully crawl web articles related to these claims. Using the claim–article pairs, we then manually assign stance labels to the articles. By stance we mean whether an article agrees, disagrees, discusses a claim or it is just unrelated. This allows us to exploit the resulting dataset to build models that automatically identify the stance with respect to a given claim, which is an important component of fact-checking and fake news detection systems. To develop these models, we resort"
2021.nlp4if-1.9,W18-5521,1,0.830685,"te range to two months before and after the date of the claim, prepending named entities and removing extra clauses using parse trees. In order to emphasize the presence of the main entity(s) in the claim, we extracted named entities using the Arabic NER corpus by Benajiba et al. (2007) and Stanford’s CoreNLP Arabic NER tagger (Manning et al., 2014). We further used Stanford’s CoreNLP Arabic parser to extract the first verb phrase (VP) and all its preceeding tokens in the claim, as this has been shown to improve document retrieval results for claim verification, especially for lengthy claims (Chakrabarty et al., 2018). For the two examples shown above, we would keep the claims until the comma for the first example and the word and for the second one, and we would consider those as the queries. 3.3 Stance Annotation We set up the annotation task as follows: given a claim–article pair, what is the stance of the document towards the claim? The stance was to be annotated using one of the following labels: agree, disagree, discuss, or unrelated, which were also used in previous work (Pomerleau and Rao, 2017; Baly et al., 2018). 6 We used Google News as a reference of news sources for the three countries of the"
2021.nlp4if-1.9,N19-1423,0,0.158717,"ation). These circumstances motivate a need to develop tools for detecting fake news online, including for a region with opposing forces and ongoing conflicts such as the Arab world. Our contributions can be summarized as follows: 1. We release a new multi-domain, multi-country dataset labeled for both stance and veracity. 2. We introduce a multi-query related document retrieval approach for claims from diverse topics in Arabic, resulting in a dataset with balanced label distributions across classes. 3. We compare our dataset to two other Arabic stance detection datasets using four BERTbased (Devlin et al., 2019) models. 1 The data can be found at http://github.com/ Tariq60/arastance. 57 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 57–65 June 6, 2021. ©2021 Association for Computational Linguistics 2 3 Related Work AraStance Construction We constructed our AraStance dataset similarly to the way this was done for the English Fake News Challenge (FNC) dataset (Pomerleau and Rao, 2017) and for the Arabic dataset of Baly et al. (2018). Our dataset contains true and false claims, where each claim is paired with one or more documents. Each claim–article pair has a stance label:"
2021.nlp4if-1.9,N18-1070,1,0.852606,"social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset (Ferreira and Vlachos, 2016), which itself pairs 300 claims to 2,500 articles. In FNC, this pairing is done at random, and it yielded a large number of unrelated claim– article pairs. There are several approaches attempting to predict the stance on the FNC dataset using LSTMs, memory networks, and transformers (Hanselowski et al., 2018; Conforti et al., 2018; Mohtarami et al., 2018; Zhang et al., 2019; Schiller et al., 2021; Schütz et al., 2021). There are two datasets for Arabic stance detection with respect to claims. The first one collected their false claims from a single political source (Baly et al., 2018), while we cover three sources from multiple countries and topics. They retrieved relevant documents and annotated the claim–article pairs using the four labels listed earlier (i.e., agree, disagree, discuss, unrelated). They also annotated “rationales,” which are segments in the articles where the stance is most strongly expressed. The other Arabic dataset by Kh"
2021.nlp4if-1.9,D19-1452,1,0.871488,"Missing"
2021.nlp4if-1.9,2020.wanlp-1.7,1,0.751705,"s and annotated the claim–article pairs using the four labels listed earlier (i.e., agree, disagree, discuss, unrelated). They also annotated “rationales,” which are segments in the articles where the stance is most strongly expressed. The other Arabic dataset by Khouja (2020) uses headlines from news sources and generated true and false claims by modifying the headlines. They used a three-class labeling scheme of stance by merging the discuss and the unrelated classes in one class called other. Our work is also related to detecting machinegenerated and manipulated text (Jawahar et al., 2020; Nagoudi et al., 2020). 3.1 Claim Collection and Preprocessing We collected false claims from three fact-checking websites: A RAANEWS2 , DABEGAD3 , and N ORU MORS 4 , based in the UAE, Egypt, and Saudi Arabia, respectively. The claims were from 2012 to 2018 and covered multiple domains such as politics, sports, and health. As the three fact-checking websites only debunk false claims, we looked for another source for true claims: following Baly et al. (2018), we collected true claims from the Arabic website of R EUTERS5 , assuming that their content was trustworthy. We added topic and date restrictions when collecti"
2021.nlp4if-1.9,2020.semeval-1.271,0,0.0348712,"which all the experimental downstream three datasets are derived. Also, it seems that ARBERT and MARBERT are better than the other two models at predicting the stance between a pair of sentences, as it is the case with the Khouja (2020) dataset. Models We fine-tuned the following four models for each of the three Arabic datasets: 1. Multilingual BERT (mBERT), base size, which is trained on the Wikipedias of 100 different languages, including Arabic (Devlin et al., 2019). 2. ArabicBERT, base size, which is trained on 8.2 billion tokens from the OSCAR corpus8 as well as on the Arabic Wikipedia (Safaya et al., 2020). 3. ARBERT, which is trained on 6.2 billion tokens of mostly Modern Standard Arabic text (Abdul-Mageed et al., 2020). 4. MARBERT, which is trained on one billion Arabic tweets, which in turn use both Modern Standard Arabic and Dialectal Arabic (AbdulMageed et al., 2020). 8 Results http://oscar-corpus.com 62 Model Baly et al. (2018) Dataset D Ds U Acc F1 A mBERT ArabicBERT ARBERT MARBERT .63 .58 .56 .44 0 .14 .14 .14 .11 .24 .30 .23 .84 .82 .83 .78 .73 .69 .70 .62 .40 .45 .46 .40 Khouja (2020) Dataset A D O Acc F1 A D AraStance Ds U Acc F1 .74 .74 .81 .80 .81 .85 .85 .85 .68 .75 .82 .80 .58 .5"
2021.nlp4if-1.9,W18-5501,0,0.0218529,"ation (e.g., for, against, neutral) of a text segment towards a topic, usually a controversial one such as abortion or gun control (Mohammad et al., 2016; Abbott et al., 2016). Another one models the relation (e.g., agree, disagree, discuss, unrelated) between two pieces of text (Hardalov et al., 2021b; Ferreira and Vlachos, 2016). The latter definition is used in automatic fact-checking, fake news detection, and rumour verification (Vlachos and Riedel, 2014). There are several English datasets that model fact-checking as a stance detection task on text from multiple genres such as Wikipedia (Thorne et al., 2018), news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset (Ferreira and Vlachos, 2016), which itself pairs 300 claims to 2,500 articles. In FNC, this pairing is done at random, and it yielded a large number of unrelated claim– article pairs. There are several approaches attempting to predict the stance on the FNC dataset using LSTMs, memory net"
2021.nlp4if-1.9,W14-2508,0,0.0214982,", and (iii) stance annotations. Stance detection started as a standalone task, unrelated to fact-checking (Küçük and Can, 2020). One type of stance models the relation (e.g., for, against, neutral) of a text segment towards a topic, usually a controversial one such as abortion or gun control (Mohammad et al., 2016; Abbott et al., 2016). Another one models the relation (e.g., agree, disagree, discuss, unrelated) between two pieces of text (Hardalov et al., 2021b; Ferreira and Vlachos, 2016). The latter definition is used in automatic fact-checking, fake news detection, and rumour verification (Vlachos and Riedel, 2014). There are several English datasets that model fact-checking as a stance detection task on text from multiple genres such as Wikipedia (Thorne et al., 2018), news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset (Ferreira and Vlachos, 2016), which itself pairs 300 claims to 2,500 articles. In FNC, this pairing is done at random, and it yield"
2021.nlp4if-1.9,N16-1138,0,0.0188798,"d. Below, we decribe the three steps of building AraStance: (i) claim collection and pre-processing, (ii) relevant document retrieval, and (iii) stance annotations. Stance detection started as a standalone task, unrelated to fact-checking (Küçük and Can, 2020). One type of stance models the relation (e.g., for, against, neutral) of a text segment towards a topic, usually a controversial one such as abortion or gun control (Mohammad et al., 2016; Abbott et al., 2016). Another one models the relation (e.g., agree, disagree, discuss, unrelated) between two pieces of text (Hardalov et al., 2021b; Ferreira and Vlachos, 2016). The latter definition is used in automatic fact-checking, fake news detection, and rumour verification (Vlachos and Riedel, 2014). There are several English datasets that model fact-checking as a stance detection task on text from multiple genres such as Wikipedia (Thorne et al., 2018), news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset"
2021.nlp4if-1.9,S19-2147,0,0.0579166,"ortion or gun control (Mohammad et al., 2016; Abbott et al., 2016). Another one models the relation (e.g., agree, disagree, discuss, unrelated) between two pieces of text (Hardalov et al., 2021b; Ferreira and Vlachos, 2016). The latter definition is used in automatic fact-checking, fake news detection, and rumour verification (Vlachos and Riedel, 2014). There are several English datasets that model fact-checking as a stance detection task on text from multiple genres such as Wikipedia (Thorne et al., 2018), news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset (Ferreira and Vlachos, 2016), which itself pairs 300 claims to 2,500 articles. In FNC, this pairing is done at random, and it yielded a large number of unrelated claim– article pairs. There are several approaches attempting to predict the stance on the FNC dataset using LSTMs, memory networks, and transformers (Hanselowski et al., 2018; Conforti et al., 2018; Mohtarami et al., 2018; Zhang et al"
2021.nlp4if-1.9,C18-1158,0,0.118668,"u and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset (Ferreira and Vlachos, 2016), which itself pairs 300 claims to 2,500 articles. In FNC, this pairing is done at random, and it yielded a large number of unrelated claim– article pairs. There are several approaches attempting to predict the stance on the FNC dataset using LSTMs, memory networks, and transformers (Hanselowski et al., 2018; Conforti et al., 2018; Mohtarami et al., 2018; Zhang et al., 2019; Schiller et al., 2021; Schütz et al., 2021). There are two datasets for Arabic stance detection with respect to claims. The first one collected their false claims from a single political source (Baly et al., 2018), while we cover three sources from multiple countries and topics. They retrieved relevant documents and annotated the claim–article pairs using the four labels listed earlier (i.e., agree, disagree, discuss, unrelated). They also annotated “rationales,” which are segments in the articles where the stance is most str"
2021.nlp4if-1.9,2021.emnlp-main.710,1,0.734252,"ee, discuss, or unrelated. Below, we decribe the three steps of building AraStance: (i) claim collection and pre-processing, (ii) relevant document retrieval, and (iii) stance annotations. Stance detection started as a standalone task, unrelated to fact-checking (Küçük and Can, 2020). One type of stance models the relation (e.g., for, against, neutral) of a text segment towards a topic, usually a controversial one such as abortion or gun control (Mohammad et al., 2016; Abbott et al., 2016). Another one models the relation (e.g., agree, disagree, discuss, unrelated) between two pieces of text (Hardalov et al., 2021b; Ferreira and Vlachos, 2016). The latter definition is used in automatic fact-checking, fake news detection, and rumour verification (Vlachos and Riedel, 2014). There are several English datasets that model fact-checking as a stance detection task on text from multiple genres such as Wikipedia (Thorne et al., 2018), news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pa"
2021.nlp4if-1.9,2020.coling-main.208,1,0.748171,"eved relevant documents and annotated the claim–article pairs using the four labels listed earlier (i.e., agree, disagree, discuss, unrelated). They also annotated “rationales,” which are segments in the articles where the stance is most strongly expressed. The other Arabic dataset by Khouja (2020) uses headlines from news sources and generated true and false claims by modifying the headlines. They used a three-class labeling scheme of stance by merging the discuss and the unrelated classes in one class called other. Our work is also related to detecting machinegenerated and manipulated text (Jawahar et al., 2020; Nagoudi et al., 2020). 3.1 Claim Collection and Preprocessing We collected false claims from three fact-checking websites: A RAANEWS2 , DABEGAD3 , and N ORU MORS 4 , based in the UAE, Egypt, and Saudi Arabia, respectively. The claims were from 2012 to 2018 and covered multiple domains such as politics, sports, and health. As the three fact-checking websites only debunk false claims, we looked for another source for true claims: following Baly et al. (2018), we collected true claims from the Arabic website of R EUTERS5 , assuming that their content was trustworthy. We added topic and date res"
2021.nlp4if-1.9,2020.fever-1.2,0,0.153909,"18; Zhang et al., 2019; Schiller et al., 2021; Schütz et al., 2021). There are two datasets for Arabic stance detection with respect to claims. The first one collected their false claims from a single political source (Baly et al., 2018), while we cover three sources from multiple countries and topics. They retrieved relevant documents and annotated the claim–article pairs using the four labels listed earlier (i.e., agree, disagree, discuss, unrelated). They also annotated “rationales,” which are segments in the articles where the stance is most strongly expressed. The other Arabic dataset by Khouja (2020) uses headlines from news sources and generated true and false claims by modifying the headlines. They used a three-class labeling scheme of stance by merging the discuss and the unrelated classes in one class called other. Our work is also related to detecting machinegenerated and manipulated text (Jawahar et al., 2020; Nagoudi et al., 2020). 3.1 Claim Collection and Preprocessing We collected false claims from three fact-checking websites: A RAANEWS2 , DABEGAD3 , and N ORU MORS 4 , based in the UAE, Egypt, and Saudi Arabia, respectively. The claims were from 2012 to 2018 and covered multiple"
2021.nlp4if-1.9,P14-5010,0,0.00308725,"J ¢  K ÕæK Ï @ YJ« á  JKQË@ á  J kYÖ ÈCg áÓ    ÐAK @ èQå« èYÖÏ ZAÖÏ @ ð á .ÊË@ PAm'. Õæ Lungs of smokers are cleaned by smelling the steam of milk and water for ten days To remedy this, we boosted the quality of the retrieved documents by restricting the date range to two months before and after the date of the claim, prepending named entities and removing extra clauses using parse trees. In order to emphasize the presence of the main entity(s) in the claim, we extracted named entities using the Arabic NER corpus by Benajiba et al. (2007) and Stanford’s CoreNLP Arabic NER tagger (Manning et al., 2014). We further used Stanford’s CoreNLP Arabic parser to extract the first verb phrase (VP) and all its preceeding tokens in the claim, as this has been shown to improve document retrieval results for claim verification, especially for lengthy claims (Chakrabarty et al., 2018). For the two examples shown above, we would keep the claims until the comma for the first example and the word and for the second one, and we would consider those as the queries. 3.3 Stance Annotation We set up the annotation task as follows: given a claim–article pair, what is the stance of the document towards the claim?"
2021.nlp4if-1.9,S16-1003,0,0.0863519,"Missing"
2021.wanlp-1.2,2020.lrec-1.768,0,0.0398327,"es can also be used in comparisons against contextual embeddings (Peters et al., 2018) and embeddings acquired from language models such as BERT (Devlin et al., 2019). For example, it can be used in evaluation settings with Arabic language models such as AraBert (Antoun et al., 2020), GigaBERT (Lan et al., 2020b), and the recently developed ARBERT and MARBERT (Abdul-Mageed et al., 2020a). More generally, our efforts are motivated by the fact that the study of Arabic dialects and computational Arabic dialect processing is a nascent area with several existing gaps to fill (Bouamor et al., 2019; Abbes et al., 2020; Abdul-Mageed et al., 2020b, 2021, 2020c). The rest of the paper is organized as follows. In Section 2, we describe how DiaLex was constructed. Section 3 offers our methodical generation of a testbank for evaluating word embeddings. In Section 4, we provide a case study for evaluating various word embedding models, some of which are newly developed by us. Section 5 is about related work. Section 6 is where we conclude and present future directions. 2 Benchmark Construction DiaLex consists of a set of word pairs in five different Arabic dialects and for six different semantic and syntactic rel"
2021.wanlp-1.2,W19-4622,0,0.0226343,"ab, 2019). Our resources can also be used in comparisons against contextual embeddings (Peters et al., 2018) and embeddings acquired from language models such as BERT (Devlin et al., 2019). For example, it can be used in evaluation settings with Arabic language models such as AraBert (Antoun et al., 2020), GigaBERT (Lan et al., 2020b), and the recently developed ARBERT and MARBERT (Abdul-Mageed et al., 2020a). More generally, our efforts are motivated by the fact that the study of Arabic dialects and computational Arabic dialect processing is a nascent area with several existing gaps to fill (Bouamor et al., 2019; Abbes et al., 2020; Abdul-Mageed et al., 2020b, 2021, 2020c). The rest of the paper is organized as follows. In Section 2, we describe how DiaLex was constructed. Section 3 offers our methodical generation of a testbank for evaluating word embeddings. In Section 4, we provide a case study for evaluating various word embedding models, some of which are newly developed by us. Section 5 is about related work. Section 6 is where we conclude and present future directions. 2 Benchmark Construction DiaLex consists of a set of word pairs in five different Arabic dialects and for six different semant"
2021.wanlp-1.2,L18-1577,1,0.787513,"equence root extraction method. Furthermore, they demonstrated how one can leverage the root information alongside a simple slot-based morphological decomposition to improve upon word embedding representations as evaluated through word similarity, word analogy, and language modeling tasks. In this paper, we have demonstrated the effectiveness of our benchmark DiaLex in the evaluation of a chosen set of these available word embedding models and compared them to newlydeveloped ones by us as we explain in the next section. word. Both CBOW and SKIP-Gram architecture are investigated in this work. Abdul-Mageed et al. (2018) build an SKIP-G model using ∼ 234M tweets, with vector dimensions at 300. The authors, however, do not exploit their model in downstream tasks. Abu Farha and Magdy (2019) built two word embeddings models (CBOW and SKIP-Gram) exploiting 250M tweets. The authors used the models in the context of training the sentiment analysis system Mazajak. The dimensions of each embedding vector in the Mazajak models are at 300. More recently, El-Haj (2020) developed Habibi, a Multi Dialect Multi-National Arabic Song Lyrics Corpus which comprises more than 30, 000 Arabic song lyrics from 18 Arab countries an"
2021.wanlp-1.2,2021.acl-long.551,1,0.798033,"Missing"
2021.wanlp-1.2,chen-eisele-2012-multiun,0,0.0342428,"available Arabic word embeddings is increasing rapidly. Some of these are strictly trained using textual corpora written in MSA, while others were trained using dialectal data. We review the most popular embedding models we are aware of here. Zahran et al. (2015) built three models for Arabic word embeddings (CBOW, SKIP-G, and GloVe). To train these models, they used a large collection of MSA texts totaling ∼ 5.8B words. The sources used include Arabic Wikipedia, Arabic Gigaword (Parker et al., 2009), Open Source Arabic Corpora (OSAC) (Saad and Ashour, 2010), OPUS (Tiedemann, 2012), MultiUN (Chen and Eisele, 2012), and a few others. Soliman et al. (2017) proposed AraVec a set of Arabic word embedding models. It consist of six word embedding models built on top of three different Arabic content domains; Wikipedia Arabic, World Wide Web pages, and Tweets with more than 3.3 billion sults across the different word relationships. For top-1 performance, one or another of our models scores best. For top-5 results, our Twitter250K-MC100 (Ours-MC-100) acquires best performance for most dialects. Exceptions are EGY and TUN dialects. Ours-MC-100 also performs best on all but TUN dialect. These results show the ne"
2021.wanlp-1.2,2020.wanlp-1.9,1,0.752607,"le.2 Beyond evaluation of word embeddings, we envision DiaLex as a basis for creating multidialectal Arabic resources that can facilitate study of the semantics and syntax of Arabic dialects, for example for pedagogical applications (Mubarak et al., 2020). More broadly, we hope DiaLex will contribute to efforts for integrating dialects in the Arabic language curriculum (Al-Batal, 2017). DiaLex can also be used to complement a growing interest in contextual word embeddings (Peters et al., 2018) and self-supervised language models (Devlin et al., 2019), including in Arabic (Antoun et al., 2020; Abdul-Mageed et al., 2020a; Lan et al., 2020a). Extensions of DiaLex can also be valuable for NLP, for example, DiaLex can be easily translated into MSA, other Arabic dialects, English, or other languages. This extension can enable evaluation of word-level translation systems, including in cross-lingual settings (Aldarmaki et al., 2018; Aldarmaki and Diab, 2019). Our resources can also be used in comparisons against contextual embeddings (Peters et al., 2018) and embeddings acquired from language models such as BERT (Devlin et al., 2019). For example, it can be used in evaluation settings with Arabic language models s"
2021.wanlp-1.2,2021.wanlp-1.28,1,0.836033,"Missing"
2021.wanlp-1.2,C16-1228,0,0.0261576,"ing different types of corpora (polar and non-polar) and evaluated them using their proposed methods. To the best of our knowledge, our proposed benchmark is the first benchmark developed in various Arabic dialects and that can be used to perform intrinsic evaluation of Arabic word embedding with respect to those dialects. as the authors themselves point out, translating an English benchmark is not the best strategy to evaluate Arabic embeddings. Zahran et al. (2015) also consider extrinsic evaluation on two NLP tasks, namely query expansion for Information Retrieval and short answer grading. Dahou et al. (2016) used the analogy questions from Zahran et al. (2015) after correcting some Arabic spelling mistakes resulting from the translation and after adding new analogy questions to make up for the inadequacy of the English questions for the Arabic language. They also performed an extrinsic evaluation using sentiment analysis. Finally, Al-Rfou et al. (2013) generated word embeddings for 100 different languages, including Arabic, and evaluated the embeddings using part-of-speech tagging, however the evaluation was done only for a handful of European languages. Elrazzaz et al. (2017) built a benchmark i"
2021.wanlp-1.2,2020.emnlp-main.472,1,0.731802,"le.2 Beyond evaluation of word embeddings, we envision DiaLex as a basis for creating multidialectal Arabic resources that can facilitate study of the semantics and syntax of Arabic dialects, for example for pedagogical applications (Mubarak et al., 2020). More broadly, we hope DiaLex will contribute to efforts for integrating dialects in the Arabic language curriculum (Al-Batal, 2017). DiaLex can also be used to complement a growing interest in contextual word embeddings (Peters et al., 2018) and self-supervised language models (Devlin et al., 2019), including in Arabic (Antoun et al., 2020; Abdul-Mageed et al., 2020a; Lan et al., 2020a). Extensions of DiaLex can also be valuable for NLP, for example, DiaLex can be easily translated into MSA, other Arabic dialects, English, or other languages. This extension can enable evaluation of word-level translation systems, including in cross-lingual settings (Aldarmaki et al., 2018; Aldarmaki and Diab, 2019). Our resources can also be used in comparisons against contextual embeddings (Peters et al., 2018) and embeddings acquired from language models such as BERT (Devlin et al., 2019). For example, it can be used in evaluation settings with Arabic language models s"
2021.wanlp-1.2,N19-1423,0,0.0232837,"d dialectal Arabic word embeddings will also be publicly available.2 Beyond evaluation of word embeddings, we envision DiaLex as a basis for creating multidialectal Arabic resources that can facilitate study of the semantics and syntax of Arabic dialects, for example for pedagogical applications (Mubarak et al., 2020). More broadly, we hope DiaLex will contribute to efforts for integrating dialects in the Arabic language curriculum (Al-Batal, 2017). DiaLex can also be used to complement a growing interest in contextual word embeddings (Peters et al., 2018) and self-supervised language models (Devlin et al., 2019), including in Arabic (Antoun et al., 2020; Abdul-Mageed et al., 2020a; Lan et al., 2020a). Extensions of DiaLex can also be valuable for NLP, for example, DiaLex can be easily translated into MSA, other Arabic dialects, English, or other languages. This extension can enable evaluation of word-level translation systems, including in cross-lingual settings (Aldarmaki et al., 2018; Aldarmaki and Diab, 2019). Our resources can also be used in comparisons against contextual embeddings (Peters et al., 2018) and embeddings acquired from language models such as BERT (Devlin et al., 2019). For example"
2021.wanlp-1.2,W19-4621,0,0.0231495,"pon word embedding representations as evaluated through word similarity, word analogy, and language modeling tasks. In this paper, we have demonstrated the effectiveness of our benchmark DiaLex in the evaluation of a chosen set of these available word embedding models and compared them to newlydeveloped ones by us as we explain in the next section. word. Both CBOW and SKIP-Gram architecture are investigated in this work. Abdul-Mageed et al. (2018) build an SKIP-G model using ∼ 234M tweets, with vector dimensions at 300. The authors, however, do not exploit their model in downstream tasks. Abu Farha and Magdy (2019) built two word embeddings models (CBOW and SKIP-Gram) exploiting 250M tweets. The authors used the models in the context of training the sentiment analysis system Mazajak. The dimensions of each embedding vector in the Mazajak models are at 300. More recently, El-Haj (2020) developed Habibi, a Multi Dialect Multi-National Arabic Song Lyrics Corpus which comprises more than 30, 000 Arabic song lyrics from 18 Arab countries and six Arabic dialects for singers. Habibi contains 500, 000 sentences (song verses) with more than 3.5 million words. Moreover, the authors provided a 300 dimension CBOW w"
2021.wanlp-1.2,2020.lrec-1.580,1,0.757089,"els (CBOW and SKIP-Gram) exploiting 250M tweets. The authors used the models in the context of training the sentiment analysis system Mazajak. The dimensions of each embedding vector in the Mazajak models are at 300. More recently, El-Haj (2020) developed Habibi, a Multi Dialect Multi-National Arabic Song Lyrics Corpus which comprises more than 30, 000 Arabic song lyrics from 18 Arab countries and six Arabic dialects for singers. Habibi contains 500, 000 sentences (song verses) with more than 3.5 million words. Moreover, the authors provided a 300 dimension CBOW word embeddings of the corpus. Doughman et al. (2020) built a set of word embeddings learnt from three large Lebanese news archives, which collectively consist of 609, 386 scanned newspaper images and spanning a total of 151 years, ranging from 1933 till 2011. To train the word embeddings, Optical Character Recognition (OCR) was employed to transcribe the scanned news archives, and various archive-level as well as decade-level word embeddings were trained. In addition, models were also built using a mixture of Arabic and English data. For example, Lachraf et al. (2019) presented AraEngVec an Arabic-English cross-lingual word embedding models. To"
2021.wanlp-1.2,2020.lrec-1.165,0,0.022149,"em to newlydeveloped ones by us as we explain in the next section. word. Both CBOW and SKIP-Gram architecture are investigated in this work. Abdul-Mageed et al. (2018) build an SKIP-G model using ∼ 234M tweets, with vector dimensions at 300. The authors, however, do not exploit their model in downstream tasks. Abu Farha and Magdy (2019) built two word embeddings models (CBOW and SKIP-Gram) exploiting 250M tweets. The authors used the models in the context of training the sentiment analysis system Mazajak. The dimensions of each embedding vector in the Mazajak models are at 300. More recently, El-Haj (2020) developed Habibi, a Multi Dialect Multi-National Arabic Song Lyrics Corpus which comprises more than 30, 000 Arabic song lyrics from 18 Arab countries and six Arabic dialects for singers. Habibi contains 500, 000 sentences (song verses) with more than 3.5 million words. Moreover, the authors provided a 300 dimension CBOW word embeddings of the corpus. Doughman et al. (2020) built a set of word embeddings learnt from three large Lebanese news archives, which collectively consist of 609, 386 scanned newspaper images and spanning a total of 151 years, ranging from 1933 till 2011. To train the wo"
2021.wanlp-1.2,Q18-1014,0,0.0283312,"r integrating dialects in the Arabic language curriculum (Al-Batal, 2017). DiaLex can also be used to complement a growing interest in contextual word embeddings (Peters et al., 2018) and self-supervised language models (Devlin et al., 2019), including in Arabic (Antoun et al., 2020; Abdul-Mageed et al., 2020a; Lan et al., 2020a). Extensions of DiaLex can also be valuable for NLP, for example, DiaLex can be easily translated into MSA, other Arabic dialects, English, or other languages. This extension can enable evaluation of word-level translation systems, including in cross-lingual settings (Aldarmaki et al., 2018; Aldarmaki and Diab, 2019). Our resources can also be used in comparisons against contextual embeddings (Peters et al., 2018) and embeddings acquired from language models such as BERT (Devlin et al., 2019). For example, it can be used in evaluation settings with Arabic language models such as AraBert (Antoun et al., 2020), GigaBERT (Lan et al., 2020b), and the recently developed ARBERT and MARBERT (Abdul-Mageed et al., 2020a). More generally, our efforts are motivated by the fact that the study of Arabic dialects and computational Arabic dialect processing is a nascent area with several exist"
2021.wanlp-1.2,W19-4610,0,0.0206694,"ions results only for this model for space limi16 Some works have also investigated the utility of using morphological knowledge to enhance word embeddings. For example, Erdmann and Habash (2018) demonstrated that out-of-context rule-based knowledge of morphological structure can complement what word embeddings can learn about morphology from words’ in-context behaviors. They quantified the value of leveraging subword information when learning embeddings and the further value of noise reduction techniques targeting the sparsity caused by complex morphology such as in the Arabic language case. El-Kishky et al. (2019) tackled the problem of root extraction from words in the Semitic language family. They proposed a constrained sequenceto-sequence root extraction method. Furthermore, they demonstrated how one can leverage the root information alongside a simple slot-based morphological decomposition to improve upon word embedding representations as evaluated through word similarity, word analogy, and language modeling tasks. In this paper, we have demonstrated the effectiveness of our benchmark DiaLex in the evaluation of a chosen set of these available word embedding models and compared them to newlydevelop"
2021.wanlp-1.2,2020.osact-1.2,0,0.0343074,"Missing"
2021.wanlp-1.2,P17-2072,1,0.836511,"n many languages including Arabic. Due to their importance, it is vital to be able to evaluate word embeddings, and various methods have been proposed for evaluating them. These methods can be broadly categorized into intrinsic evaluation methods and extrinsic evaluation ones. For extrinsic evaluation, word embeddings are assessed based on performance in downstream applications. For intrinsic evaluation, they are assessed based on how well they capture syntactic and semantic relations between words. Although there exists a benchmark for evaluating modern standard Arabic (MSA) word embeddings (Elrazzaz et al., 2017), no such resource that we know of exists for Arabic dialects. This makes it difficult to measure progress on Arabic dialect processing. In this paper, our goal is to facilitate intrinsic evaluation of dialectal Arabic word embeddings. To this end, we build a new benchmark spanning five different Arabic dialects, from Eastern, Middle, and Western Arab World. Namely, our benchmark covers Algerian (ALG), Egyptian (EGY), Lebanese (LEB), SyrIntroduction Word embeddings are the backbone of modern natural language processing (NLP) systems. They encode semantic and syntactic relations between words b"
2021.wanlp-1.2,D14-1162,0,0.101447,"Arabic dialect processing. In this paper, our goal is to facilitate intrinsic evaluation of dialectal Arabic word embeddings. To this end, we build a new benchmark spanning five different Arabic dialects, from Eastern, Middle, and Western Arab World. Namely, our benchmark covers Algerian (ALG), Egyptian (EGY), Lebanese (LEB), SyrIntroduction Word embeddings are the backbone of modern natural language processing (NLP) systems. They encode semantic and syntactic relations between words by representing them in a low-dimensional space. Many techniques have been proposed to learn such embeddings (Pennington et al., 2014; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013) from large text corpora. As of today, a 1 https://github.com/UBC-NLP/dialex. 11 Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 11–20 Kyiv, Ukraine (Virtual), April 19, 2021. ian (SYR), and Tunisian (TUN). Figure 1 shows a map of the five Arab countries covered by DiaLex. For each one of these dialects, DiaLex consists of a set of word pairs that are syntactically or semantically related by one of six different relations: Male to Female, Singular to Plural, Singular to Dual, Antonym, Comparative, and Genitive to P"
2021.wanlp-1.2,W18-5806,0,0.0120909,"0) acquires best performance for most dialects. Exceptions are EGY and TUN dialects. Ours-MC-100 also performs best on all but TUN dialect. These results show the necessity of developing dialectal resources for the various varieties and that a model trained on large MSA data such as that of Zahran et al. (2015) is quite sub-optimal. Table 4 shows per-relation results evaluation of the Twitter-250K-MC100 model. We show allrelations results only for this model for space limi16 Some works have also investigated the utility of using morphological knowledge to enhance word embeddings. For example, Erdmann and Habash (2018) demonstrated that out-of-context rule-based knowledge of morphological structure can complement what word embeddings can learn about morphology from words’ in-context behaviors. They quantified the value of leveraging subword information when learning embeddings and the further value of noise reduction techniques targeting the sparsity caused by complex morphology such as in the Arabic language case. El-Kishky et al. (2019) tackled the problem of root extraction from words in the Semitic language family. They proposed a constrained sequenceto-sequence root extraction method. Furthermore, they"
2021.wanlp-1.2,N18-1202,0,0.0554067,"addition to the benchmark of word pairs, our newlydeveloped dialectal Arabic word embeddings will also be publicly available.2 Beyond evaluation of word embeddings, we envision DiaLex as a basis for creating multidialectal Arabic resources that can facilitate study of the semantics and syntax of Arabic dialects, for example for pedagogical applications (Mubarak et al., 2020). More broadly, we hope DiaLex will contribute to efforts for integrating dialects in the Arabic language curriculum (Al-Batal, 2017). DiaLex can also be used to complement a growing interest in contextual word embeddings (Peters et al., 2018) and self-supervised language models (Devlin et al., 2019), including in Arabic (Antoun et al., 2020; Abdul-Mageed et al., 2020a; Lan et al., 2020a). Extensions of DiaLex can also be valuable for NLP, for example, DiaLex can be easily translated into MSA, other Arabic dialects, English, or other languages. This extension can enable evaluation of word-level translation systems, including in cross-lingual settings (Aldarmaki et al., 2018; Aldarmaki and Diab, 2019). Our resources can also be used in comparisons against contextual embeddings (Peters et al., 2018) and embeddings acquired from langu"
2021.wanlp-1.2,W19-4605,1,0.810715,"er, the authors provided a 300 dimension CBOW word embeddings of the corpus. Doughman et al. (2020) built a set of word embeddings learnt from three large Lebanese news archives, which collectively consist of 609, 386 scanned newspaper images and spanning a total of 151 years, ranging from 1933 till 2011. To train the word embeddings, Optical Character Recognition (OCR) was employed to transcribe the scanned news archives, and various archive-level as well as decade-level word embeddings were trained. In addition, models were also built using a mixture of Arabic and English data. For example, Lachraf et al. (2019) presented AraEngVec an Arabic-English cross-lingual word embedding models. To train their bilingual models, they used a large dataset with more than 93 million pairs of Arabic-English parallel sentences (with more than 1.8 billion words) mainly extracted from the Open Parallel Corpus Project (OPUS) (Tiedemann, 2012). In order to train the models, they have chosen CBOW and SKIPGram as an architecture. Indeed, they propose three methods for pre-processing the opus dataset: parallel sentences, word-level alignment and random shuffling. Both extrinsic and intrinsic evaluations for the different A"
2021.wanlp-1.2,2020.emnlp-main.382,0,0.0412904,"rd embeddings, we envision DiaLex as a basis for creating multidialectal Arabic resources that can facilitate study of the semantics and syntax of Arabic dialects, for example for pedagogical applications (Mubarak et al., 2020). More broadly, we hope DiaLex will contribute to efforts for integrating dialects in the Arabic language curriculum (Al-Batal, 2017). DiaLex can also be used to complement a growing interest in contextual word embeddings (Peters et al., 2018) and self-supervised language models (Devlin et al., 2019), including in Arabic (Antoun et al., 2020; Abdul-Mageed et al., 2020a; Lan et al., 2020a). Extensions of DiaLex can also be valuable for NLP, for example, DiaLex can be easily translated into MSA, other Arabic dialects, English, or other languages. This extension can enable evaluation of word-level translation systems, including in cross-lingual settings (Aldarmaki et al., 2018; Aldarmaki and Diab, 2019). Our resources can also be used in comparisons against contextual embeddings (Peters et al., 2018) and embeddings acquired from language models such as BERT (Devlin et al., 2019). For example, it can be used in evaluation settings with Arabic language models such as AraBert (Ant"
2021.wanlp-1.2,D15-1036,0,0.227876,"rall, DiaLex consists of over 3, 000 word pairs in those five dialects, evenly distributed. To the best of our knowledge, DiaLex is the first benchmark that can be used to assess the quality of Arabic word embeddings in the five dialects it covers. To be able to use DiaLex to evaluate Arabic word embeddings, we generate a set of word analogy questions from the word pairs in DiaLex. A word analogy question is generated from two word pairs from a given relation. These questions have recently become the standard in intrinsic evaluation of word embeddings (Mikolov et al., 2013a; Gao et al., 2014; Schnabel et al., 2015). To demonstrate the usefulness of DiaLex in evaluating Arabic word embeddings, we use it to evaluate a set of existing and new Arabic word embeddings. We conclude that both available and newly-developed word embedding models have moderate-to-serious coverage issues and are not sufficiently representative of the respective dialects under study. In addition to the benchmark of word pairs, our newlydeveloped dialectal Arabic word embeddings will also be publicly available.2 Beyond evaluation of word embeddings, we envision DiaLex as a basis for creating multidialectal Arabic resources that can f"
2021.wanlp-1.2,tiedemann-2012-parallel,0,0.195755,"rd Embeddings The number of available Arabic word embeddings is increasing rapidly. Some of these are strictly trained using textual corpora written in MSA, while others were trained using dialectal data. We review the most popular embedding models we are aware of here. Zahran et al. (2015) built three models for Arabic word embeddings (CBOW, SKIP-G, and GloVe). To train these models, they used a large collection of MSA texts totaling ∼ 5.8B words. The sources used include Arabic Wikipedia, Arabic Gigaword (Parker et al., 2009), Open Source Arabic Corpora (OSAC) (Saad and Ashour, 2010), OPUS (Tiedemann, 2012), MultiUN (Chen and Eisele, 2012), and a few others. Soliman et al. (2017) proposed AraVec a set of Arabic word embedding models. It consist of six word embedding models built on top of three different Arabic content domains; Wikipedia Arabic, World Wide Web pages, and Tweets with more than 3.3 billion sults across the different word relationships. For top-1 performance, one or another of our models scores best. For top-5 results, our Twitter250K-MC100 (Ours-MC-100) acquires best performance for most dialects. Exceptions are EGY and TUN dialects. Ours-MC-100 also performs best on all but TUN d"
2021.wanlp-1.2,2020.coling-demos.15,0,0.0189252,"t of existing and new Arabic word embeddings. We conclude that both available and newly-developed word embedding models have moderate-to-serious coverage issues and are not sufficiently representative of the respective dialects under study. In addition to the benchmark of word pairs, our newlydeveloped dialectal Arabic word embeddings will also be publicly available.2 Beyond evaluation of word embeddings, we envision DiaLex as a basis for creating multidialectal Arabic resources that can facilitate study of the semantics and syntax of Arabic dialects, for example for pedagogical applications (Mubarak et al., 2020). More broadly, we hope DiaLex will contribute to efforts for integrating dialects in the Arabic language curriculum (Al-Batal, 2017). DiaLex can also be used to complement a growing interest in contextual word embeddings (Peters et al., 2018) and self-supervised language models (Devlin et al., 2019), including in Arabic (Antoun et al., 2020; Abdul-Mageed et al., 2020a; Lan et al., 2020a). Extensions of DiaLex can also be valuable for NLP, for example, DiaLex can be easily translated into MSA, other Arabic dialects, English, or other languages. This extension can enable evaluation of word-leve"
2021.wanlp-1.28,bouamor-etal-2014-multidialectal,1,0.782333,"A has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018). These datasets come"
2021.wanlp-1.28,W19-4622,1,0.537072,"classification level next. 3.1 Country-level Classification • Subtask 1.1: Country-level MSA. The goal of Subtask 1.1 is to identify country level MSA from short written sentences (tweets). NADI 2021 Subtask 1.1 is novel since no previous works focused on teasing apart MSA by country of origin. • Subtask 1.2: Country-level DA. Subtask 1.2 is similar to Subtask 1.1, but focuses on identifying country level dialect from tweets. Subtask 1.2 is similar to previous works that have also taken country as their target (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018; Bouamor et al., 2019; Abdul-Mageed et al., 2020b). We provided labeled data to NADI 2021 participants with specific training (TRAIN) and development (DEV) splits. Each of the 21 labels corresponding to the 21 countries is represented in both TRAIN and DEV. Teams could score their models through an online system (codalab) on the DEV set before the deadline. We released our TEST set of unlabeled tweets shortly before the system submission deadline. We then invited participants to submit their predictions to the online scoring system housing the gold TEST set labels. Table 2 shows the distribution of the TRAIN, DEV,"
2021.wanlp-1.28,2020.lrec-1.165,0,0.0729724,"Arabic has three main categories: CA, MSA, and DA. While CA and MSA have been studied extensively (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004), DA has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occur"
2021.wanlp-1.28,W14-3911,0,0.0207609,"2000; Holes, 2004), DA has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018"
2021.wanlp-1.28,2021.wanlp-1.32,0,0.0786186,"Missing"
2021.wanlp-1.28,L16-1679,1,0.731098,"5. 2 Related Work As we explained in Section 1, Arabic has three main categories: CA, MSA, and DA. While CA and MSA have been studied extensively (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004), DA has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it"
2021.wanlp-1.28,N19-4002,1,0.803112,"2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018). These datasets come from the Twitter domain, and hence are naturally-occurring. Several works have also focused on sociopragmatics meaning exploiting dialectal data. These include sentiment analysis (Abdul-Mageed et al., 2014), emotion (Alhuzali et al., 2018), age and gender (Abbes et al., 2020), offe"
2021.wanlp-1.28,W14-5904,0,0.0248499,", and a high-level description of submitted systems in Section 5. 2 Related Work As we explained in Section 1, Arabic has three main categories: CA, MSA, and DA. While CA and MSA have been studied extensively (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004), DA has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue wit"
2021.wanlp-1.28,C18-1113,1,0.850076,"., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018). These datasets come from the Twitter domain, and hence are naturally-occurring. Several works have also focused on sociopragmatics meaning exploiting dialectal data. These include sentiment analysis (Abdul-Mageed et al., 2014), emotion (Alhuzali et al., 2018), age and gender (Abbe"
2021.wanlp-1.28,2021.wanlp-1.35,0,0.0644144,"Missing"
2021.wanlp-1.28,L18-1111,0,0.109857,"2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018). These datasets come from the Twitter domain, and hence are naturally-occurring. Several works have also focused on sociopragmatics meaning exploiting dialectal data. These include sentiment analysis (Abdul-Mageed et al., 2014), emotion (Alhuzali et al., 2018), age and gender (Abbes et al., 2020), offensive language (Mubarak et al., 2020), and sarcasm (Abu Farha and Magdy, 2020). Concurrent with our work, (Abdul-Mageed et al., 2020c) also describe data and models at country, province, and city levels. 1 The dataset is accessible via our GitHub at: https: //github.com/UBC-NLP/nadi. 245 The fir"
2021.wanlp-1.28,P11-2007,0,0.0259273,"ell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004), DA has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zagh"
2021.wanlp-1.28,W17-1201,0,0.0998386,"Missing"
2021.wanlp-1.28,W18-3901,0,0.0455917,"Missing"
abdul-mageed-diab-2012-awatif,J94-2004,0,\N,Missing
abdul-mageed-diab-2012-awatif,W11-0413,1,\N,Missing
abdul-mageed-diab-2012-awatif,W10-1401,0,\N,Missing
abdul-mageed-diab-2012-awatif,P11-2103,1,\N,Missing
abdul-mageed-diab-2012-awatif,W06-2915,0,\N,Missing
abdul-mageed-diab-2012-awatif,P99-1032,0,\N,Missing
abdul-mageed-diab-2014-sana,W04-3253,0,\N,Missing
abdul-mageed-diab-2014-sana,J90-1003,0,\N,Missing
abdul-mageed-diab-2014-sana,W11-0413,1,\N,Missing
abdul-mageed-diab-2014-sana,W12-3705,1,\N,Missing
abdul-mageed-diab-2014-sana,baccianella-etal-2010-sentiwordnet,0,\N,Missing
abdul-mageed-diab-2014-sana,kamps-etal-2004-using,0,\N,Missing
abdul-mageed-diab-2014-sana,C04-1200,0,\N,Missing
abdul-mageed-diab-2014-sana,P11-2103,1,\N,Missing
abdul-mageed-diab-2014-sana,P02-1053,0,\N,Missing
abdul-mageed-diab-2014-sana,P97-1023,0,\N,Missing
abdul-mageed-diab-2014-sana,R11-1096,1,\N,Missing
abdul-mageed-diab-2014-sana,abdul-mageed-diab-2012-awatif,1,\N,Missing
abdul-mageed-diab-2014-sana,S12-1033,0,\N,Missing
D16-1217,abdul-mageed-diab-2014-sana,1,0.828748,"MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields competitive results (Abdul-Mageed and Diab, 2014; Mohammad et al., 2016). However, translated texts tend to lose sentiment information such that the translated data is more neutral than the source language (Salameh et al., 2015). It is less obvious how well expressions of emotion or subjective well-being translate between languages and cultures; the words for liking a phone or TV may be more similar across cultures than the ones for finding life and relationships satisfying, or work meaningful and engaging. 2.2 Well-being In contrast to classic sentiment analysis, well-being is not restricted to positive and negative emotion. In 2011, the p"
D16-1217,W12-3709,0,0.0172632,"g camp is developing methods to estimate personality and emotion, asking “how does she feel?” rather than “how much does she like the product?” (Mohammad and Kiritchenko, 2015; Park et al., 2014). In social media, the well-being of individuals as well as communities has been studied, on various platforms such as Facebook and Twitter (Bollen et al., 2011; Schwartz et al., 2013; Eichstaedt et al., 2015; Schwartz et al., 2016). 2.1 Translating sentiment Past work has, on the whole, regarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-spec"
D16-1217,D08-1014,0,0.0225301,"s well as communities has been studied, on various platforms such as Facebook and Twitter (Bollen et al., 2011; Schwartz et al., 2013; Eichstaedt et al., 2015; Schwartz et al., 2016). 2.1 Translating sentiment Past work has, on the whole, regarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields"
D16-1217,P12-3005,0,0.0677526,"Missing"
D16-1217,P07-1123,0,0.096535,"rk et al., 2014). In social media, the well-being of individuals as well as communities has been studied, on various platforms such as Facebook and Twitter (Bollen et al., 2011; Schwartz et al., 2013; Eichstaedt et al., 2015; Schwartz et al., 2016). 2.1 Translating sentiment Past work has, on the whole, regarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic,"
D16-1217,N15-1078,0,0.0358916,"t languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields competitive results (Abdul-Mageed and Diab, 2014; Mohammad et al., 2016). However, translated texts tend to lose sentiment information such that the translated data is more neutral than the source language (Salameh et al., 2015). It is less obvious how well expressions of emotion or subjective well-being translate between languages and cultures; the words for liking a phone or TV may be more similar across cultures than the ones for finding life and relationships satisfying, or work meaningful and engaging. 2.2 Well-being In contrast to classic sentiment analysis, well-being is not restricted to positive and negative emotion. In 2011, the psychologist Martin Selig2043 man proposed PERMA (Seligman, 2011), a fivedimensional model of well-being where ‘P’ stands for positive emotion, ‘E’ is engagement, ‘R’ is positive re"
D16-1217,D14-1121,1,0.829336,"irani, 1996) models to predict the average annotation score for each of the crowdsourced PERMA labels. Separate models, each consisting of regression weights for each term in the lexicon, were built for each of the ten (five positive and five negative) PERMA components in both English and Spanish1 . Each model was validated using 10-fold cross validation, with Pearson correlations averaged over the 10 positive/negative PERMA components. Re1 Available at www.wwbp.org. 2044 sults are presented in Table 1. The models were then transformed into a predictive lexicon using the methods described in (Sap et al., 2014), where the weights in the lexicon were derived from the above Lasso regression model. Model Spanish English r 0.36 0.36 Table 1: Performance as measured by Pearson r correlation averaged over the 10 positive/negative PERMA components using 10-fold cross validation. 3.3 Translating the models We used Google Translate to translate both the original English and Spanish Tweets and the words in the models. We also created versions of the translated models in which we manually corrected apparent translation errors for 25 terms with the largest regression coefficients for each of the 10 PERMA compon"
D16-1217,D08-1058,0,0.0554644,"egarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields competitive results (Abdul-Mageed and Diab, 2014; Mohammad et al., 2016). However, translated texts tend to lose sentiment information such that the translated data is more neutral than the source language (Salameh et al., 2015)."
L18-1577,al-sabbagh-girju-2012-yadac,0,0.0599478,"udied in Arabic NLP. Our work seeks to take a first step toward enabling the bridging of this important gap. 2.2. 2 Nizar Habash (personal communication, December, 2011) also points out he found this classification to be the ‘most common’ in the literature and hence he opted for it in his book. . Computational Treatment of Arabic Dialects. Early NLP work on Arabic dialects focused on collecting datasets that would enable the investigation of these 3654 Country Egypt dialects. A number of these pioneering studies focused on collecting data from blogs (Diab et al., 2010; Elfardy and Diab, 2012; Al-Sabbagh and Girju, 2012; Sadat et al., 2014), the general Web (Al-Sabbagh and Girju, 2012), comments on online news sites (Zaidan and CallisonBurch, 2011), or building dialectal lexica (Diab et al., 2014). Other works have dealt with detecting one or more of the Levantine, Gulf, and Egyptian dialects (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014; Cotterell and Callison-Burch, 2014). Works covering other dialects (e.g., from countries like Tunisia, the Sudan, Qatar, Bahrain) include (Sadat et al., 2014), although they exploited small datasets (mostly < 5K sentences from eac"
L18-1577,cotterell-callison-burch-2014-multi,0,0.0780348,"g datasets that would enable the investigation of these 3654 Country Egypt dialects. A number of these pioneering studies focused on collecting data from blogs (Diab et al., 2010; Elfardy and Diab, 2012; Al-Sabbagh and Girju, 2012; Sadat et al., 2014), the general Web (Al-Sabbagh and Girju, 2012), comments on online news sites (Zaidan and CallisonBurch, 2011), or building dialectal lexica (Diab et al., 2014). Other works have dealt with detecting one or more of the Levantine, Gulf, and Egyptian dialects (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014; Cotterell and Callison-Burch, 2014). Works covering other dialects (e.g., from countries like Tunisia, the Sudan, Qatar, Bahrain) include (Sadat et al., 2014), although they exploited small datasets (mostly < 5K sentences from each country). Closer to our work is (Mubarak and Darwish, 2014) who report collecting a dataset of 123 million tweets covering Egyptian, Levantine, Iraqi, Maghrebi dialects. Our work follows (Mubarak and Darwish, 2014)’s lead, while developing a dataset almost twice the size (and from as twice countries). In addition, our work compares preferably to (Mubarak and Darwish, 2014) in that our data has more n"
L18-1577,diab-etal-2014-tharwa,0,0.0559697,"found this classification to be the ‘most common’ in the literature and hence he opted for it in his book. . Computational Treatment of Arabic Dialects. Early NLP work on Arabic dialects focused on collecting datasets that would enable the investigation of these 3654 Country Egypt dialects. A number of these pioneering studies focused on collecting data from blogs (Diab et al., 2010; Elfardy and Diab, 2012; Al-Sabbagh and Girju, 2012; Sadat et al., 2014), the general Web (Al-Sabbagh and Girju, 2012), comments on online news sites (Zaidan and CallisonBurch, 2011), or building dialectal lexica (Diab et al., 2014). Other works have dealt with detecting one or more of the Levantine, Gulf, and Egyptian dialects (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014; Cotterell and Callison-Burch, 2014). Works covering other dialects (e.g., from countries like Tunisia, the Sudan, Qatar, Bahrain) include (Sadat et al., 2014), although they exploited small datasets (mostly < 5K sentences from each country). Closer to our work is (Mubarak and Darwish, 2014) who report collecting a dataset of 123 million tweets covering Egyptian, Levantine, Iraqi, Maghrebi dialects. Our work"
L18-1577,elfardy-diab-2012-simplified,0,0.34824,"ally, with fewer structural and syntactic differences (Bateson, 1967; Ryding, 2005). DA is a collection of arbitrarily defined (Versteegh, 2001; Habash, 2010) variations, although geography does play a role in the classification of DA. Most Arabic varieties remained primarily spoken for a long time. With the advent of the internet and the proliferation of social media, Arabic dialects found their way to online written form (Abdul-Mageed, 2015). Early computational studies of Arabic dialects have depended on data collected from blogs and comments on online news sites, e.g., (Diab et al., 2010; Elfardy and Diab, 2012). Due to the costly efforts associated with labeling the data with dialect tags, these pioneering works have focused on a few varieties like Egyptian or Levantine (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014). User-provided location information in the Twitter microblogging platform have made it possible to collect data with nuanced geographical labels in ways not previously possible, see e.g., (Jurgens et al., 2017). We depend on these cues to label our dataset with location tags as a proxy for the relevant dialects. Although no agreement exists as t"
L18-1577,P13-2081,0,0.61341,"hy does play a role in the classification of DA. Most Arabic varieties remained primarily spoken for a long time. With the advent of the internet and the proliferation of social media, Arabic dialects found their way to online written form (Abdul-Mageed, 2015). Early computational studies of Arabic dialects have depended on data collected from blogs and comments on online news sites, e.g., (Diab et al., 2010; Elfardy and Diab, 2012). Due to the costly efforts associated with labeling the data with dialect tags, these pioneering works have focused on a few varieties like Egyptian or Levantine (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014). User-provided location information in the Twitter microblogging platform have made it possible to collect data with nuanced geographical labels in ways not previously possible, see e.g., (Jurgens et al., 2017). We depend on these cues to label our dataset with location tags as a proxy for the relevant dialects. Although no agreement exists as to where dialectal boundaries should be drawn, some proposals have been made. Figure 1 shows only one such classifications of Arabic dialects. The vast geographic extension the Arab world"
L18-1577,W17-1225,0,0.0112472,"port collecting a dataset of 123 million tweets covering Egyptian, Levantine, Iraqi, Maghrebi dialects. Our work follows (Mubarak and Darwish, 2014)’s lead, while developing a dataset almost twice the size (and from as twice countries). In addition, our work compares preferably to (Mubarak and Darwish, 2014) in that our data has more nuanced labels (i.e., at the city level). Also related to our research is recent work on discriminating similar languages, e.g., via the VarDial workshop (Malmasi et al., 2016; Zampieri et al., 2017) where some works focused on Arabic (Malmasi and Zampieri, 2017; Ionescu and Butnaru, 2017). Some works also focus on Arabic dialect identification in speech transcripts, e.g., (Malmasi and Zampieri, 2016). Again, our work has wider scope and coverage. We now turn to describing our dataset. 3. Kuwait Oman Palestine Qatar KSA UAE Yemen All https://github.com/geopy/geopy. A list of these third-party geocoders and other sources can be found at: https://github.com/geopy/geopy/tree/ master/geopy/geocoders. 5 https://nominatim.openstreetmap.org/ 4 Jordan Dataset In order to develop our dataset, we exploit several in-house corpora (i.e., a total of > 1 billion tweets) covering the 10 Arab"
L18-1577,P17-2009,0,0.035379,"nal studies of Arabic dialects have depended on data collected from blogs and comments on online news sites, e.g., (Diab et al., 2010; Elfardy and Diab, 2012). Due to the costly efforts associated with labeling the data with dialect tags, these pioneering works have focused on a few varieties like Egyptian or Levantine (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014). User-provided location information in the Twitter microblogging platform have made it possible to collect data with nuanced geographical labels in ways not previously possible, see e.g., (Jurgens et al., 2017). We depend on these cues to label our dataset with location tags as a proxy for the relevant dialects. Although no agreement exists as to where dialectal boundaries should be drawn, some proposals have been made. Figure 1 shows only one such classifications of Arabic dialects. The vast geographic extension the Arab world constitutes naturally translates into rich and varied linguistic tradition, thus making nuanced study of Arabic dialects an attractive object of scientific investigation. In this work, we take a step in this direction by collecting a large dataset of Arabic dialects. We focus"
L18-1577,W16-4814,0,0.0750759,"follows (Mubarak and Darwish, 2014)’s lead, while developing a dataset almost twice the size (and from as twice countries). In addition, our work compares preferably to (Mubarak and Darwish, 2014) in that our data has more nuanced labels (i.e., at the city level). Also related to our research is recent work on discriminating similar languages, e.g., via the VarDial workshop (Malmasi et al., 2016; Zampieri et al., 2017) where some works focused on Arabic (Malmasi and Zampieri, 2017; Ionescu and Butnaru, 2017). Some works also focus on Arabic dialect identification in speech transcripts, e.g., (Malmasi and Zampieri, 2016). Again, our work has wider scope and coverage. We now turn to describing our dataset. 3. Kuwait Oman Palestine Qatar KSA UAE Yemen All https://github.com/geopy/geopy. A list of these third-party geocoders and other sources can be found at: https://github.com/geopy/geopy/tree/ master/geopy/geocoders. 5 https://nominatim.openstreetmap.org/ 4 Jordan Dataset In order to develop our dataset, we exploit several in-house corpora (i.e., a total of > 1 billion tweets) covering the 10 Arab countries from the set {Oman, Egypt, Iraq, Jordan, Kuwait, Palestine, Qatar, KSA, UAE, and Yemen}. Our inhouse dat"
L18-1577,W17-1222,0,0.0142014,"ak and Darwish, 2014) who report collecting a dataset of 123 million tweets covering Egyptian, Levantine, Iraqi, Maghrebi dialects. Our work follows (Mubarak and Darwish, 2014)’s lead, while developing a dataset almost twice the size (and from as twice countries). In addition, our work compares preferably to (Mubarak and Darwish, 2014) in that our data has more nuanced labels (i.e., at the city level). Also related to our research is recent work on discriminating similar languages, e.g., via the VarDial workshop (Malmasi et al., 2016; Zampieri et al., 2017) where some works focused on Arabic (Malmasi and Zampieri, 2017; Ionescu and Butnaru, 2017). Some works also focus on Arabic dialect identification in speech transcripts, e.g., (Malmasi and Zampieri, 2016). Again, our work has wider scope and coverage. We now turn to describing our dataset. 3. Kuwait Oman Palestine Qatar KSA UAE Yemen All https://github.com/geopy/geopy. A list of these third-party geocoders and other sources can be found at: https://github.com/geopy/geopy/tree/ master/geopy/geocoders. 5 https://nominatim.openstreetmap.org/ 4 Jordan Dataset In order to develop our dataset, we exploit several in-house corpora (i.e., a total of > 1 billion t"
L18-1577,W16-4801,0,0.105811,"Missing"
L18-1577,W14-3601,0,0.14534,"Web (Al-Sabbagh and Girju, 2012), comments on online news sites (Zaidan and CallisonBurch, 2011), or building dialectal lexica (Diab et al., 2014). Other works have dealt with detecting one or more of the Levantine, Gulf, and Egyptian dialects (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014; Cotterell and Callison-Burch, 2014). Works covering other dialects (e.g., from countries like Tunisia, the Sudan, Qatar, Bahrain) include (Sadat et al., 2014), although they exploited small datasets (mostly < 5K sentences from each country). Closer to our work is (Mubarak and Darwish, 2014) who report collecting a dataset of 123 million tweets covering Egyptian, Levantine, Iraqi, Maghrebi dialects. Our work follows (Mubarak and Darwish, 2014)’s lead, while developing a dataset almost twice the size (and from as twice countries). In addition, our work compares preferably to (Mubarak and Darwish, 2014) in that our data has more nuanced labels (i.e., at the city level). Also related to our research is recent work on discriminating similar languages, e.g., via the VarDial workshop (Malmasi et al., 2016; Zampieri et al., 2017) where some works focused on Arabic (Malmasi and Zampieri,"
L18-1577,D14-1162,0,0.0912469,"ters to only 2. We build a skip-gram model with 300 dimensions, with a minimal word count = 100 words, and a window size of 5 words on each side of a target word. We use the gensim 6 implementation for the word2vec tool 7 . 5.2. Hand-Picked Examples In order to demonstrate the capacity and richness of our word vectors model, particularly in terms of dialectal word coverage, we ask our annotators to identify a list of dialectal words from the data Building Word Vectors Model Distributed representations of language at various levels of granularity, e.g., words and phrases (Mikolov et al., 2013; Pennington et al., 2014) or sentences (Kiros et al., 2015) boost performance on various NLP tasks. Zahran et al. (2015) pioneered efforts to build 6 https://radimrehurek.com/gensim/models/ word2vec.html. 7 https://code.google.com/archive/p/ word2vec. 3656 Country Arabic Egypt ø @P@  ¯QªÓ . àA«Yg ø YªJK.  ®Ó  ÐAJ KAÓ ÐCËAë Õæ ¨Pð èñK @ ËAK ËAm Q  éQ « àñÓQK KSA UAE English Most similar words how I don’t know men pass by nothing don’t we sleep this talk name it boy yes sitting how are you? go sip they talk  , àA Ê«  ,ðXQK , èY»  ,úæ¯ñËX I ¯ñËX .     &apos; Ym. . ,ðXQK. , ñ¯QªÓ ,úæ¯ñËX , ¬PAªÓ"
L18-1577,W14-5904,0,0.196354,"k seeks to take a first step toward enabling the bridging of this important gap. 2.2. 2 Nizar Habash (personal communication, December, 2011) also points out he found this classification to be the ‘most common’ in the literature and hence he opted for it in his book. . Computational Treatment of Arabic Dialects. Early NLP work on Arabic dialects focused on collecting datasets that would enable the investigation of these 3654 Country Egypt dialects. A number of these pioneering studies focused on collecting data from blogs (Diab et al., 2010; Elfardy and Diab, 2012; Al-Sabbagh and Girju, 2012; Sadat et al., 2014), the general Web (Al-Sabbagh and Girju, 2012), comments on online news sites (Zaidan and CallisonBurch, 2011), or building dialectal lexica (Diab et al., 2014). Other works have dealt with detecting one or more of the Levantine, Gulf, and Egyptian dialects (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014; Cotterell and Callison-Burch, 2014). Works covering other dialects (e.g., from countries like Tunisia, the Sudan, Qatar, Bahrain) include (Sadat et al., 2014), although they exploited small datasets (mostly < 5K sentences from each country). Closer to"
L18-1577,P11-2007,0,0.116115,"he classification of DA. Most Arabic varieties remained primarily spoken for a long time. With the advent of the internet and the proliferation of social media, Arabic dialects found their way to online written form (Abdul-Mageed, 2015). Early computational studies of Arabic dialects have depended on data collected from blogs and comments on online news sites, e.g., (Diab et al., 2010; Elfardy and Diab, 2012). Due to the costly efforts associated with labeling the data with dialect tags, these pioneering works have focused on a few varieties like Egyptian or Levantine (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and CallisonBurch, 2014). User-provided location information in the Twitter microblogging platform have made it possible to collect data with nuanced geographical labels in ways not previously possible, see e.g., (Jurgens et al., 2017). We depend on these cues to label our dataset with location tags as a proxy for the relevant dialects. Although no agreement exists as to where dialectal boundaries should be drawn, some proposals have been made. Figure 1 shows only one such classifications of Arabic dialects. The vast geographic extension the Arab world constitutes naturally translates"
L18-1577,J14-1006,0,0.0775506,"bic dialects focused on collecting datasets that would enable the investigation of these 3654 Country Egypt dialects. A number of these pioneering studies focused on collecting data from blogs (Diab et al., 2010; Elfardy and Diab, 2012; Al-Sabbagh and Girju, 2012; Sadat et al., 2014), the general Web (Al-Sabbagh and Girju, 2012), comments on online news sites (Zaidan and CallisonBurch, 2011), or building dialectal lexica (Diab et al., 2014). Other works have dealt with detecting one or more of the Levantine, Gulf, and Egyptian dialects (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014; Cotterell and Callison-Burch, 2014). Works covering other dialects (e.g., from countries like Tunisia, the Sudan, Qatar, Bahrain) include (Sadat et al., 2014), although they exploited small datasets (mostly < 5K sentences from each country). Closer to our work is (Mubarak and Darwish, 2014) who report collecting a dataset of 123 million tweets covering Egyptian, Levantine, Iraqi, Maghrebi dialects. Our work follows (Mubarak and Darwish, 2014)’s lead, while developing a dataset almost twice the size (and from as twice countries). In addition, our work compares preferably to (Mubarak and Darwi"
L18-1577,W17-1201,0,0.0925642,"Missing"
P11-2103,C04-1200,0,0.263084,"Missing"
P11-2103,W10-1401,0,0.0195633,"ictator, stepped down). Subjective text is further classified with sentiment or polarity. For sentiment classification, the task refers to identifying whether the subjective text is positive (e.g., What an excellent camera!), negative (e.g., I hate this camera!), neutral (e.g., I believe there will be a meeting.), or, sometimes, mixed (e.g., It is good, but I hate it!) texts. Most of the SSA literature has focused on English and other Indio-European languages. Very few studies have addressed the problem for morphologically rich languages (MRL) such as Arabic, Hebrew, 587 Turkish, Czech, etc. (Tsarfaty et al., 2010). MRL pose significant challenges to NLP systems in general, and the SSA task is expected to be no exception. The problem is even more pronounced in some MRL due to the lack in annotated resources for SSA such as labeled corpora, and polarity lexica. In the current paper, we investigate the task of sentence-level SSA on Modern Standard Arabic (MSA) texts from the newswire genre. We run experiments on three different pre-processing settings based on tokenized text from the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) and employ both language-independent and Arabicspecific, morphology-bas"
P11-2103,P99-1032,0,0.711929,"Missing"
P11-2103,J04-3002,0,0.0583472,"configuration schemes, with each underlined. Features: The features we employed are of two main types: Language-independent features and Morphological features. Language-Independent Features: This group of features has been employed in various SSA studies. Domain: Following (Wilson et al., 2009), we apply a feature indicating the domain of the document to which a sentence belongs. As mentioned earlier, each sentence has a document domain label manually associated with it. 2 A detailed account of issues related to the annotation task will appear in a separate publication. 588 UNIQUE: Following Wiebe et al. (2004) we apply a unique feature. Namely words that occur in our corpus with an absolute frequency &lt; 5, are replaced with the token ”UNIQUE”. N-GRAM: We run experiments with N-grams ≤ 4 and all possible combinations of them. ADJ: For subjectivity classification, we follow Bruce & Wiebe’s (1999) in adding a binary has adjective feature indicating whether or not any of the adjectives in our manually created polarity lexicon exists in a sentence. For sentiment classification, we apply two features, has POS adjective and has NEG adjective, each of these binary features indicate whether a POS or NEG adje"
P11-2103,J94-2004,0,0.0435343,"notated on the sentence level. We also describe an automatic SSA tagging system that exploits the annotated data. We investigate the impact of different levels of preprocessing settings on the SSA classification task. We show that by explicitly accounting for the rich morphology the system is able to achieve significantly higher levels of performance. 1 Introduction Subjectivity and Sentiment Analysis (SSA) is an area that has been witnessing a flurry of novel research. In natural language, subjectivity refers to expression of opinions, evaluations, feelings, and speculations (Banfield, 1982; Wiebe, 1994) and thus incorporates sentiment. The process of subjectivity classification refers to the task of classifying texts into either objective (e.g., Mubarak stepped down) or subjective (e.g., Mubarak, the hateful dictator, stepped down). Subjective text is further classified with sentiment or polarity. For sentiment classification, the task refers to identifying whether the subjective text is positive (e.g., What an excellent camera!), negative (e.g., I hate this camera!), neutral (e.g., I believe there will be a meeting.), or, sometimes, mixed (e.g., It is good, but I hate it!) texts. Most of th"
P11-2103,J09-3003,0,0.0195244,"r lemma citation forms, for instance in case of verbs it is the 3rd person masculine singular perfective form; and (3) Stem, which is the surface form minus inflectional morphemes, it should be noted that this configuration may result in non proper Arabic words (a la IR stemming). Table 1 illustrates examples of the three configuration schemes, with each underlined. Features: The features we employed are of two main types: Language-independent features and Morphological features. Language-Independent Features: This group of features has been employed in various SSA studies. Domain: Following (Wilson et al., 2009), we apply a feature indicating the domain of the document to which a sentence belongs. As mentioned earlier, each sentence has a document domain label manually associated with it. 2 A detailed account of issues related to the annotation task will appear in a separate publication. 588 UNIQUE: Following Wiebe et al. (2004) we apply a unique feature. Namely words that occur in our corpus with an absolute frequency &lt; 5, are replaced with the token ”UNIQUE”. N-GRAM: We run experiments with N-grams ≤ 4 and all possible combinations of them. ADJ: For subjectivity classification, we follow Bruce & Wi"
P17-1067,P11-2102,0,0.0152851,"Missing"
P17-1067,P14-1062,0,0.078461,"Missing"
P17-1067,D14-1181,0,0.00781316,"Missing"
P17-1067,P13-2087,0,0.0140046,"Missing"
P17-1067,P10-3008,0,0.201778,"Missing"
P17-1067,D15-1278,0,0.0238346,"Missing"
P17-1067,pak-paroubek-2010-twitter,0,0.0336727,"Missing"
P17-1067,D15-1280,0,0.0223252,"Missing"
P17-1067,P12-3005,0,0.0118018,"acters of > 2 to 2] and user mentions (as detected by a string starting with an “@” sign). We then performed a manual inspection of a random sample of 1,000 tweets from the data and found no evidence of any remaining tweet duplicates. Next, even though the emotion hashtags themselves are exclusively in English, we observe the data do have tweets in languages other than English. This is due to code-switching, but also to the fact that our data dates back to 2009 and Twitter did not allow use of hashtags for several non-English languages until 2012. To filter out non-English, we use the langid (Lui and Baldwin, 2012) (https://github.com/ saffsd/langid.py) library to assign language tags to the tweets. Since the common wisdom in the literature (e.g., (Mohammad, 2012; Wang et al., 2012)) is to restrict data to hashtags occurring in final position of a tweet, we investigate correlations between a tweet’s relevance and emotion hashtag location in Section 4 and test models exclusively on data with hashtags occurring in final position. We also only use tweets conamong the various emotion types. The eight sectors are meant to capture that there are eight primary emotion dimensions arranged as four pairs of oppos"
P17-1067,P11-1015,0,0.13061,"Missing"
P17-1067,E12-1049,0,0.0299375,"than (Mohammad, 2012) and (Volkova and Bachrach, 2016) and the range of emotions we target is much more fine grained than (Mohammad, 2012; Wang et al., 2012; Volkova and Bachrach, 2016) since we model 24 emotion types, rather than focus on ≤ 7 basic emotions. Our emotion modeling relies on distant supervision (Read, 2005; Mintz et al., 2009), the approach of using cues in data (e.g., hashtags or emoticons) as a proxy for “ground truth” labels as we explained above. Distant supervision has been investigated by a number of researchers for emotion detection (Tanaka et al., 2005; Mohammad, 2012; Purver and Battersby, 2012; Wang et al., 2012; Pak and Paroubek, 2010; Yang et al., 2007) and for other semantic tasks such as sentiment analysis (Read, 2005; Go et al., 2009) and sarcasm detection (Gonz´alez-Ib´anez et al., 2011). In these works, authors successfully use emoticons and/or hashtags as marks to label data after performing varying degrees of data quality assurance. We take a similar approach, using a larger collection of tweets, richer emotion definitions, and stronger filtering for tweet quality. The remainder of the paper is organized as follows: We first overview related literature in Section 2, descri"
P17-1067,N13-1090,0,0.0234662,"Missing"
P17-1067,P09-1113,0,0.225266,"a, however, is costly and so it is desirable to develop labeled emotion data without annotators. While the proliferation of social media has made it possible for us to acquire large datasets with implicit labels in the form of hashtags (Mohammad and Kiritchenko, 2015), such labels are noisy and reliable. In this work, we seek to enable deep learning by creating a large dataset of fine-grained emotions using Twitter data. More specifically, we harness cues in Twitter data in the form of emotion hashtags as a way to build a labeled emotion dataset that we then exploit using distant supervision (Mintz et al., 2009) (the use of hashtags as a surrogate for annotator-generated emotion labels) to build emotion models grounded in psychology. We construct such a dataset and exploit it using powerful deep learning methods to build accurate, high coverage models for emotion prediction. Overall, we make the following contributions: 1) Grounded in psychological theory of emotions, we build a large-scale, high quality dataset of tweets labeled with emotions. Key to this are methods to ensure data quality, 2) we validate the data collection method using human annotations, 3) we develop powerful deep learning models"
P17-1067,P05-2008,0,0.018251,"Missing"
P17-1067,D13-1170,0,0.0237764,"Missing"
P17-1067,S12-1033,0,0.0655914,"uch, developing emotion detection models is important; they have a wide array of applications, ranging from building nuanced virtual assistants that cater for the emotions of their users to detecting the emotions of social media users in order to understand their mental and/or physical health. 1 https://en.oxforddictionaries.com/ definition/emotion. 718 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 718–728 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1067 example, (Mohammad, 2012) shows that by using a simple domain adaptation method to train a classifier on their data they are able to improve both precision and recall on the SemEval-2007 (Strapparava and Mihalcea, 2007) dataset. As the author points out, this is another premise that the selflabeled hashtags acquired from Twitter are consistent, to some degree, with the emotion labels given by the trained human judges who labeled the SemEval-2007 data. As pointed out earlier, (Wang et al., 2012) randomly sample a set of 400 tweets from their data and human-label as relevant/irrelevant, as a way to verify the distant su"
P17-1067,S07-1013,0,0.201123,"Missing"
P17-1067,P15-1150,0,0.0612687,"Missing"
P17-1067,D15-1167,0,0.0234391,"Missing"
P17-1067,C14-1018,0,0.0178222,"Missing"
P17-1067,P14-1146,0,0.114972,"Missing"
P17-1067,P16-1148,0,0.0350034,"Missing"
P17-1067,N16-2011,0,0.160285,"Missing"
P17-1067,L16-1183,0,0.0483304,"Missing"
R11-1096,baccianella-etal-2010-sentiwordnet,0,0.0463714,"o identify waste, fraud and abuse in the health care system, etc. Below is an example: • HW LEX: This is a list of adjectives comprising all gradable and dynamic adjectives, both manually prepared and automatically extracted, by (Hatzivassiloglou and Wiebe, 2000)4 . • ”The House plan is projected to guarantee coverage for 96 percent of Americans at a cost of more than $1 trillion over the next 10 years, according to the nonpartisan Congressional Budget Office.” (OBJ) 4 4.1 • SentiWN LEX: This lexicon is composed of all positive and negative entries with a score > 0.25 5 from SentiWordnet 3.0 (Baccianella et al., 2010). • SentiWN Strong LEX: This lexicon is composed of all positive and negative entries with a score > 0.50 6 from Sentiwordnet 3.0. Approach Features The following are the set of features we apply: TOPIC: We apply a feature indicating the topic/dimension of the each paragraph. UNIQUE: Following Wiebe et al. (2004), to account for the frequency of words’ effect, we include a unique feature. Namely words that occur in our corpus with a frequency ≤3 are replaced with the token ”UNIQUE”. N-GRAM: We run experiments with N-grams ≤3 and all possible combinations of them. Thus, we employ N-gram combina"
R11-1096,W09-3012,0,0.017285,"gap. We present a new labeled corpus of professional articles collected from major Websites focused on the Obama health reform plan (OHRP). We introduce a new annotation scheme that incorporates subjectivity as well as topics directly related to the OHRP and describe a highlysuccessful SSA system that exploits the annotation. In the process, we introduce a number of novel features and a wide-coverage polarity lexicon for the health domain. 1 Introduction In recent years, searches and processing of data beyond the limiting level of surface words are becoming more important than it used to be (Diab et al., 2009). One of the areas that has been witnessing a swelling interest is that of Subjectivity and sentiment analysis (SSA). Subjectivity in natural language refers to aspects of language used to express opinions, feelings, evaluations, and speculations (Banfield, 1982; Wiebe, 1994) and it, thus, incorporates sentiment. Subjectivity classification refers to the task of classifying texts into either Objective (e.g., The Obama Health Committe submitted a report last week.) or Subjective. Subjective text is further classified with sentiment or polarity. For sentiment classification, the task refers to i"
R11-1096,C00-1044,0,0.345814,"o 97 percent of residents having some form of coverage, said Alan Sager, director of the Health Reform Program at Boston University’s School of Public Health.” (OBJ) Funding: Descriptions of the Funding dimension included that the plan (1) will not add a dime to the deficit and is paid for upfront, (2) creates an independent commission of doctors and medical experts to identify waste, fraud and abuse in the health care system, etc. Below is an example: • HW LEX: This is a list of adjectives comprising all gradable and dynamic adjectives, both manually prepared and automatically extracted, by (Hatzivassiloglou and Wiebe, 2000)4 . • ”The House plan is projected to guarantee coverage for 96 percent of Americans at a cost of more than $1 trillion over the next 10 years, according to the nonpartisan Congressional Budget Office.” (OBJ) 4 4.1 • SentiWN LEX: This lexicon is composed of all positive and negative entries with a score > 0.25 5 from SentiWordnet 3.0 (Baccianella et al., 2010). • SentiWN Strong LEX: This lexicon is composed of all positive and negative entries with a score > 0.50 6 from Sentiwordnet 3.0. Approach Features The following are the set of features we apply: TOPIC: We apply a feature indicating the"
R11-1096,P99-1032,0,0.232166,"Missing"
R11-1096,J04-3002,0,0.0741584,"for 96 percent of Americans at a cost of more than $1 trillion over the next 10 years, according to the nonpartisan Congressional Budget Office.” (OBJ) 4 4.1 • SentiWN LEX: This lexicon is composed of all positive and negative entries with a score > 0.25 5 from SentiWordnet 3.0 (Baccianella et al., 2010). • SentiWN Strong LEX: This lexicon is composed of all positive and negative entries with a score > 0.50 6 from Sentiwordnet 3.0. Approach Features The following are the set of features we apply: TOPIC: We apply a feature indicating the topic/dimension of the each paragraph. UNIQUE: Following Wiebe et al. (2004), to account for the frequency of words’ effect, we include a unique feature. Namely words that occur in our corpus with a frequency ≤3 are replaced with the token ”UNIQUE”. N-GRAM: We run experiments with N-grams ≤3 and all possible combinations of them. Thus, we employ N-gram combinations, as follows:(1) 1g, (2) 2g, (3) 3g, (4) 1g+2g, (5) 1g+3g, (6) 2g+3g, (7) 1g+2g+3g. POLARITY LEX: We apply a binary has polar feature indicating whether or not any of the polarized entries in a polarity lexicon. We compare the performance of a number of polarity lexicons, including a manually labeled lexicon"
R11-1096,J94-2004,0,0.0170752,"ssful SSA system that exploits the annotation. In the process, we introduce a number of novel features and a wide-coverage polarity lexicon for the health domain. 1 Introduction In recent years, searches and processing of data beyond the limiting level of surface words are becoming more important than it used to be (Diab et al., 2009). One of the areas that has been witnessing a swelling interest is that of Subjectivity and sentiment analysis (SSA). Subjectivity in natural language refers to aspects of language used to express opinions, feelings, evaluations, and speculations (Banfield, 1982; Wiebe, 1994) and it, thus, incorporates sentiment. Subjectivity classification refers to the task of classifying texts into either Objective (e.g., The Obama Health Committe submitted a report last week.) or Subjective. Subjective text is further classified with sentiment or polarity. For sentiment classification, the task refers to identifying whether a subjective text is positive (e.g., Obama’s reform plan will solve all our health problems!), negative (e.g., The proposed ideas will lead to definite failure!), neutral (e.g., The president may make changes to some of the ideas proposed.), and, sometimes,"
R13-1001,A00-2013,0,0.435736,"Missing"
R13-1001,W96-0102,0,0.0395104,"earning method that does not abstract rules from the data, but rather keeps all training data. During training, the learner stores the training instances without abstraction. Given a new instance, the classifier finds the k nearest neighbors in the training set and chooses their most frequent class for the new instance. MBL has been shown to have a suitable bias for NLP problems (Daelemans et al., 1999; Daelemans and van den Bosch, 2005) since it does not abstract over irregularities or subregularities. For each of the two classification tasks (i.e., segmentation and POS tagging), we use MBT (Daelemans et al., 1996), a memory-based POS tagger that has access to previous tagging decisions in addition to an expressive feature set. 4.2 5 Segmentation 5.1 Setup We define segmentation as an IOB classification task, where each letter in a word is tagged with a label indicating its place in a segment. The tagset is {B-SEG, I-SEG, O}, where B is a tag assigned to the beginning of a segment, I denotes the inside of a segment, and O spaces between surface form words. Data Sets and Splits We use segmentation and POS data from the Penn Arabic Treebank (PATB) (Maamouri et al., 2004), specifically, we use the followin"
R13-1001,N10-1105,1,0.843769,"Missing"
R13-1001,W95-0107,0,0.361934,"Missing"
R13-1001,N04-4038,1,0.820658,"Missing"
R13-1001,P08-2030,1,0.909449,"Missing"
R13-1001,W10-1401,0,0.122729,"Missing"
R13-1001,P05-1071,0,0.0971224,"Missing"
R13-1001,P09-2056,0,0.0503576,"Missing"
S19-2136,W18-3504,0,0.068307,"Missing"
S19-2136,N19-1144,0,0.0359268,"eral text classification tasks, including detecting offensive and hateful language. For example, Pitsilis et al. (2018) use recurrent 775 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 775–781 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics 3,876 tweets (2,407 individual, 1,074 group, and 395 other). We similarly provide one example for each of these classes in Table 1. ensemble models for solving the data imbalance problem. In this paper, we describe our submissions to SemEval-2019 task 6 (OffenseEval) (Zampieri et al., 2019b). We focus on sub-tasks B and C. The Offensive Language Identification Dataset (Zampieri et al., 2019a), the data released by the organizers for each of these sub-tasks, is extremely imbalanced (see Section 2). We propose effective methods for developing models exploiting the data. Our main contributions are: (1) we experiment with a number of simple data augmentation methods to alleviate class imbalance, and (2) we apply a number of classical machine learning methods in the context of ensembling to develop highly successful models for each of the competition sub-tasks. Our work shows the ut"
S19-2136,S19-2010,0,0.0356195,"eral text classification tasks, including detecting offensive and hateful language. For example, Pitsilis et al. (2018) use recurrent 775 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 775–781 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics 3,876 tweets (2,407 individual, 1,074 group, and 395 other). We similarly provide one example for each of these classes in Table 1. ensemble models for solving the data imbalance problem. In this paper, we describe our submissions to SemEval-2019 task 6 (OffenseEval) (Zampieri et al., 2019b). We focus on sub-tasks B and C. The Offensive Language Identification Dataset (Zampieri et al., 2019a), the data released by the organizers for each of these sub-tasks, is extremely imbalanced (see Section 2). We propose effective methods for developing models exploiting the data. Our main contributions are: (1) we experiment with a number of simple data augmentation methods to alleviate class imbalance, and (2) we apply a number of classical machine learning methods in the context of ensembling to develop highly successful models for each of the competition sub-tasks. Our work shows the ut"
S19-2188,P17-1067,1,0.851569,"etworks (RNN) and convolutional neural networks (CNN). RNNs are good summarizers of sequential information such as language, yet suffer from gradient issues when sequences are very long. Long-Short Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) have been proposed to solve this issue, and so we employ them. Bidirectional LSTM (Bi-LSTM) where information is summarized from both left to right and vice versa and combined to form a single representation has also worked well on many tasks such as named entity recognition (Limsopatham and Collier, 2016), but also text classification (Abdul-Mageed and Ungar, 2017; Elaraby and Abdul-Mageed, 2018). As such, we also investigate Bi-LSTMs on the task. Attention mechanism has also been proposed to improve machine translation (Bahdanau et al., 2014), but was also applied successfully to various other tasks such as speech recognition, image captioning generation, and text classification (Xu et al., 2015; Chorowski et al., 2015; Baziotis et al., 2018; Rajendran et al., 2019). We employ a simple attention mechanism (Zhou et al., 2016b) to the output vector of the (Bi-)LSTM layer. Although CNNs have initially been proposed for image tasks, they have also been sh"
S19-2188,S18-1037,0,0.0499733,"Missing"
S19-2188,W18-3930,1,0.851971,"l neural networks (CNN). RNNs are good summarizers of sequential information such as language, yet suffer from gradient issues when sequences are very long. Long-Short Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) have been proposed to solve this issue, and so we employ them. Bidirectional LSTM (Bi-LSTM) where information is summarized from both left to right and vice versa and combined to form a single representation has also worked well on many tasks such as named entity recognition (Limsopatham and Collier, 2016), but also text classification (Abdul-Mageed and Ungar, 2017; Elaraby and Abdul-Mageed, 2018). As such, we also investigate Bi-LSTMs on the task. Attention mechanism has also been proposed to improve machine translation (Bahdanau et al., 2014), but was also applied successfully to various other tasks such as speech recognition, image captioning generation, and text classification (Xu et al., 2015; Chorowski et al., 2015; Baziotis et al., 2018; Rajendran et al., 2019). We employ a simple attention mechanism (Zhou et al., 2016b) to the output vector of the (Bi-)LSTM layer. Although CNNs have initially been proposed for image tasks, they have also been shown to work well for texts (e.g.,"
S19-2188,S19-2145,0,0.0282817,"data size on the task, and (3) we probe our models with an attention mechanism coupled with a simple visualization method to discover meaningful contributions of various lexical features to the learning task. The rest of the paper is organized as follows: data are described in Section 2, Section 3 describes our methods, followed by experiments in Section 4. Next, we explain the results in detail and our submission to SemEval-2019 Task4 in Section 4. We present attention-based visualizations in Section 5, and conclude in Section 6. 2 Data Hyperpartisan news detection is the SemEval2019 task 4 (Kiesel et al., 2019). The task is set up as binary classification where data released by organizers are labeled with the tagset 1072 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1072–1077 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics Labels-by-Publisher Hyperpartisan Non- Hyperpartisan Total Labels-by-Article Train Dev Test Total Train Test Total 383,151 416,849 800,000 66,849 33,151 100,000 50,000 50,000 100,000 500,000 500,000 1,000,000 214 366 580 24 41 65 238 407 645 Table 1: Distribution of labels over our data spli"
S19-2188,D14-1181,0,0.00729621,"Missing"
S19-2188,D16-1076,0,0.0516194,"Missing"
S19-2188,W16-5102,0,0.0185949,"y a range of variations and combinations of recurrent neural networks (RNN) and convolutional neural networks (CNN). RNNs are good summarizers of sequential information such as language, yet suffer from gradient issues when sequences are very long. Long-Short Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) have been proposed to solve this issue, and so we employ them. Bidirectional LSTM (Bi-LSTM) where information is summarized from both left to right and vice versa and combined to form a single representation has also worked well on many tasks such as named entity recognition (Limsopatham and Collier, 2016), but also text classification (Abdul-Mageed and Ungar, 2017; Elaraby and Abdul-Mageed, 2018). As such, we also investigate Bi-LSTMs on the task. Attention mechanism has also been proposed to improve machine translation (Bahdanau et al., 2014), but was also applied successfully to various other tasks such as speech recognition, image captioning generation, and text classification (Xu et al., 2015; Chorowski et al., 2015; Baziotis et al., 2018; Rajendran et al., 2019). We employ a simple attention mechanism (Zhou et al., 2016b) to the output vector of the (Bi-)LSTM layer. Although CNNs have ini"
S19-2188,C14-1019,0,0.0317919,"lt in dangerous real world consequences (Akpan, 2016), or possibly undermine the very democratic bases of modern societies (Qiu et al., 2017; Allcott and Gentzkow, 2017). Several approaches have been employed for detecting fake stories online, including detecting the sources that are highly polarized (or hyperpartisan)] (Potthast et al., 2017). Detecting whether a source is extremely biased for or against a given party can be an effective step toward identifying fake news. Most research on news orientation prediction employed machine learning methods based on feature engineering. For example, Pla and Hurtado (2014) use features such as text n-grams, part-ofspeech tags, hashtags, etc. with an SVM classifier to tackle political tendency identification in twitter. Potthast et al. (2017) investigate the writing style of hyperpartisan and mainstream news using a random forest classifier (Koppel et al., 2007). Further, Preot¸iuc-Pietro et al. (2017) use a linear regression algorithm to categorize Twitter users into a fine-grained political group. The authors were able to show a relationship between language use and political orientation. Nevertheless, previous works have not considered the utility of deep lea"
S19-2188,P17-1068,0,0.0735943,"Missing"
S19-2188,D15-1206,0,0.0341324,"where information is summarized from both left to right and vice versa and combined to form a single representation has also worked well on many tasks such as named entity recognition (Limsopatham and Collier, 2016), but also text classification (Abdul-Mageed and Ungar, 2017; Elaraby and Abdul-Mageed, 2018). As such, we also investigate Bi-LSTMs on the task. Attention mechanism has also been proposed to improve machine translation (Bahdanau et al., 2014), but was also applied successfully to various other tasks such as speech recognition, image captioning generation, and text classification (Xu et al., 2015; Chorowski et al., 2015; Baziotis et al., 2018; Rajendran et al., 2019). We employ a simple attention mechanism (Zhou et al., 2016b) to the output vector of the (Bi-)LSTM layer. Although CNNs have initially been proposed for image tasks, they have also been shown to work well for texts (e.g., (Kim, 2014)) and so we employ a CNN. In addition, neural network architectures that combine different neural 1073 network architectures have shown their advantage in text classification (e.g., sentiment analysis). For example, improvements on text classification accuracy were observed applying a model bu"
S19-2188,C16-1329,0,0.0507445,"Missing"
S19-2188,P16-2034,0,0.0307517,"ked well on many tasks such as named entity recognition (Limsopatham and Collier, 2016), but also text classification (Abdul-Mageed and Ungar, 2017; Elaraby and Abdul-Mageed, 2018). As such, we also investigate Bi-LSTMs on the task. Attention mechanism has also been proposed to improve machine translation (Bahdanau et al., 2014), but was also applied successfully to various other tasks such as speech recognition, image captioning generation, and text classification (Xu et al., 2015; Chorowski et al., 2015; Baziotis et al., 2018; Rajendran et al., 2019). We employ a simple attention mechanism (Zhou et al., 2016b) to the output vector of the (Bi-)LSTM layer. Although CNNs have initially been proposed for image tasks, they have also been shown to work well for texts (e.g., (Kim, 2014)) and so we employ a CNN. In addition, neural network architectures that combine different neural 1073 network architectures have shown their advantage in text classification (e.g., sentiment analysis). For example, improvements on text classification accuracy were observed applying a model built on a combination of Bi-LSTM and two-dimensional CNN (2DCNN) compared to separate RNN and CNN models (Zhou et al., 2016a). Moreo"
W11-0413,W09-3012,1,0.485063,"es and provide examples from our corpus exemplifying the different phenomena. Throughout the paper, we discuss expression of subjectivity in natural language, combining various previously scattered insights belonging to many branches of linguistics. 1 Introduction As the volume of web data continues to phenomenally increase, researchers are becoming more interested in mining that data and making the information therein accessible to end-users in various innovative ways. As a result, searches and processing of data beyond the limiting level of surface words are becoming increasingly important (Diab et al., 2009). The sentiment expressed in Web data specifically continues to be of high interest and value to internet users, businesses, and governmental bodies. Thus, the area of Subjectivity and sentiment analysis (SSA) has been witnessing a flurry of novel research. Subjectivity in natural language refers to aspects of language used to express opinions, feelings, evaluations, and speculations (Banfield, 1982; Wiebe, In spite of the great interest in SSA, only few studies have been conducted on morphologicallyrich languages (MRL) (i.e., languages in which significant information concerning syntactic uni"
W11-0413,C04-1200,0,0.359557,"Missing"
W11-0413,W06-2915,0,0.119935,"lad” which is a private state (i.e., a state that is not subject to direct verification) (Quirk et al., 1974). 3.2 Good & Bad News News can be good or bad. For example, whereas ”Five persons were killed in a car accident” is bad news, ”It is sunny and warm today in Chicago” is good news. Our coders were instructed not to consider good news positive nor bad news negative if they think the sentences expressing them are objectively reporting information. Thus, bad news and good news can be OBJ as is the case in both examples. 3.3 Perspective Some sentences are written from a certain perspective (Lin et al., 2006) or point of view. Consider the two sentences (1) ”Israeli soldiers, our heroes, are keen on protecting settlers” and (2) ”Palestinian freedom fighters are willing to attack these Israeli targets”. Sentence (1) is written from an Israeli perspective, while sentence (2) is written from a Palestinian perspective. The perspective from which a sentence is written interplays with how sentiment is assigned. Sentence (1) is considered positive from an Israeli perspective, yet the act of protecting settlers is considered negative from a Palestinian perspective. Similarly, attacking Israeli targets may"
W11-0413,2007.sigdial-1.5,0,0.00828931,"teration: w ybdw An Altktm Al*y AHAT bzyArp byryz AlY AndwnysyA kAn yhdf AlY tfAdy AvArp rdwd fEl mEAdyp fy AlblAd. English: It seems that the secrecy surrounding Peres’s visit to Indonesia was aimed at avoiding negative reactions in the country.   k QÓ B@ ù¢« @ é@ ñªË@ àA¢J.¯ à @ l . P B@ úÎ«ð (21) K  B@ É¿ ZA®£A .AîD JÓ úÎ« HB . Transliteration: wElY AlArjH An qbTAn AlgwASp AETY AlAmr bATfA’ kl AlAlAt ElY mtnhA. English: Most likely the submarine’s captain ordered turning off all the machines on board. Some S-NEUT cases are examples of arguing that something is true or should be done (Somasundaran et al., 2007). (22) is an illustrative example:  Ö Ï A¯ ,AëPQ» @ð ,AîDÊ ¯ (22)  Ë éÊ ÐAm Ì '@ ¡® JË@ ú ¯ I  ® JÖ Ï @ ú¯ AÖß @ ð . éJ ¢® JË@ HA Transliteration: qlthA, wAkrrhA, fAlm$klp lyst fy AlnfT AlxAm wInmA fy Alm$tqAt AlnfTyp. English: I said, and I repeat it, the problem is not in crude oil but rather in oil derivatives. Example 22 was, however, initially tagged as OBJ. Later, the two annotators agreed to assign it an S-NEUT tag. 6 Related Work There are a number of datasets annotated for SSA. Most relevant to us is work on the news genre. (Wiebe et al., 2005) describe a fine-grained n"
W11-0413,W10-1401,0,0.0354738,"continues to be of high interest and value to internet users, businesses, and governmental bodies. Thus, the area of Subjectivity and sentiment analysis (SSA) has been witnessing a flurry of novel research. Subjectivity in natural language refers to aspects of language used to express opinions, feelings, evaluations, and speculations (Banfield, 1982; Wiebe, In spite of the great interest in SSA, only few studies have been conducted on morphologicallyrich languages (MRL) (i.e., languages in which significant information concerning syntactic units and relations are expressed at the word-level (Tsarfaty et al., 2010)). Arabic, Hebrew, Turkish, Czech, and Basque are examples of MRLs. SSA work on MRLs has been hampered by lack of annotated data. In the current paper we report efforts to manually annotate a corpus of Modern Standard Arabic (MSA), a morphologically-rich variety of Arabic, e.g., (Diab et al., 2007; Habash et al., 2009). The corpus is a collection of documents from the newswire genre covering several domains such as politics and sports. We label the data at the sentence level. Our annotation guidelines explicitly incorporate linguisticallymotivated information. 110 Proceedings of the Fifth Law"
W11-0413,P99-1032,0,0.496906,"Missing"
W11-0413,J94-2004,0,0.362688,"Missing"
W12-3705,abdul-mageed-diab-2012-awatif,1,0.518069,"lexicon. For sentiment classification, we apply two features, has POS adjective and has NEG adjective. These binary features indicate whether a POS or NEG adjective from the lexicon occurs in a sentence. 4.4 Dialectal Arabic Features Dialect: We apply the two gold language variety features, {MSA, DA}, on the Twitter data set to represent whether the tweet is in MSA or in a dialect. 4.5 Genre Specific Features Gender: Inspired by gender variation research exploiting social media data (e.g., (Herring, 1996)), we apply three gender (GEN) features corresponding to the set {MALE, FEMALE, UNKNOWN}. Abdul-Mageed and Diab (2012a) suggest that there is a relationship between politeness strategies and sentiment expression. And gender variation research in social media shows that expression of linguistic politeness (Brown and Levinson, 1987) differs based on the gender of the user. User ID: The user ID (UID) labels are inspired by research on Arabic Twitter showing that a considerable share of tweets is produced by organizations such as news agencies (Abdul-Mageed et al., 2011a) as opposed to lay users. We hence employ two features from the set {PERSON, ORGANIZATION} to classification of the Twitter data set. The assum"
W12-3705,P11-2103,1,0.514872,"dbaths in Syria are horrifying!), neutral (e.g., Obama may sign the bill.), or, sometimes, mixed (e.g., The iPad is cool, but way too expensive). In this work, we address two main issues in Subjectivity and Sentiment Analysis (SSA): First, SSA has mainly been conducted on a small number of genres such as newspaper text, customer reports, 19 and blogs. This excludes, for example, social media genres (such as Wikipedia Talk Pages). Second, despite increased interest in the area of SSA, only few attempts have been made to build SSA systems for morphologically-rich languages (Abbasi et al., 2008; Abdul-Mageed et al., 2011b), i.e. languages in which a significant amount of information concerning syntactic units and relations is expressed at the word-level, such as Finnish or Arabic. We thus aim at partially bridging these two gaps in research by developing an SSA system for Arabic, a morphologically highly complex languages (Diab et al., 2007; Habash et al., 2009). We present SAMAR, a sentence-level SSA system for Arabic social media texts. We explore the SSA task on four different genres: chat, Twitter, Web forums, and Wikipedia Talk Pages. These genres vary considerably in terms of their functions and the lan"
W12-3705,R11-1096,1,0.934465,"dbaths in Syria are horrifying!), neutral (e.g., Obama may sign the bill.), or, sometimes, mixed (e.g., The iPad is cool, but way too expensive). In this work, we address two main issues in Subjectivity and Sentiment Analysis (SSA): First, SSA has mainly been conducted on a small number of genres such as newspaper text, customer reports, 19 and blogs. This excludes, for example, social media genres (such as Wikipedia Talk Pages). Second, despite increased interest in the area of SSA, only few attempts have been made to build SSA systems for morphologically-rich languages (Abbasi et al., 2008; Abdul-Mageed et al., 2011b), i.e. languages in which a significant amount of information concerning syntactic units and relations is expressed at the word-level, such as Finnish or Arabic. We thus aim at partially bridging these two gaps in research by developing an SSA system for Arabic, a morphologically highly complex languages (Diab et al., 2007; Habash et al., 2009). We present SAMAR, a sentence-level SSA system for Arabic social media texts. We explore the SSA task on four different genres: chat, Twitter, Web forums, and Wikipedia Talk Pages. These genres vary considerably in terms of their functions and the lan"
W12-3705,W07-0812,1,0.795461,"HsnAt in the TOK setting. POS tagging: Since we use only the base forms of words, the question arises whether we lose meaningful morphological information and consequently whether we could represent this information in the POS tags instead. Thus, we use two sets of POS features that are specific to Arabic: the reduced tag set (RTS) and the extended reduced tag set (ERTS) (Diab, 2009). The RTS is composed of 42 tags and reflects only number for nouns and some tense information for verbs whereas the ERTS comprises 115 tags and enriches the RTS with gender, number, and definiteness information. Diab (2007b; 2007a) shows that using the ERTS improves results for higher processing tasks such as base phrase chunking of Arabic. 4.3 Standard Features This group includes two features that have been employed in various SSA studies. 22 Unique: Following Wiebe et al. (2004), we apply a UNIQUE (Q) feature: We replace low frequency words with the token ”UNIQUE”. Experiments showed that setting the frequency threshold to 3 yields the best results. Polarity Lexicon (PL): The lexicon (cf. section 3) is used in two different forms for the two tasks: For subjectivity classification, we follow Bruce and Wiebe ("
W12-3705,C04-1200,0,0.213208,"Missing"
W12-3705,P02-1053,0,0.00892885,"nt for SSA. More specifically, we concentrate on two questions: Since we need to reduce word forms to base forms to combat data sparseness, is it more useful to use tokenization or lemmatization? And given that the part-of-speech (POS) tagset for Arabic contains a fair amount of morphological information, how much of this information is useful for SSA? More specifically, we investigate two different reduced tagsets, the RTS and the ERTS. For more detailed information see section 4. RQ2 addresses the impact of using two standard features, frequently employed in SSA studies (Wiebe et al., 2004; Turney, 2002), on social media data, which exhibit DA usage and text length variations, e.g. in twitter data. First, we investigate the utility of applying a UNIQUE feature (Wiebe et al., 2004) where low frequency words below a threshold are replaced with the token ”UNIQUE”. Given that our data includes very short posts (e.g., twitter data has a limit of only 140 characters per tweet), it is questionable whether the UNIQUE feature will be useful or whether it replaces too many content words. Second, we test whether a polarity lexicon extracted in a standard domain using Modern Standard Arabic (MSA) transfe"
W12-3705,P99-1032,0,0.404161,"Missing"
W12-3705,J04-3002,0,0.0593286,"tion that is important for SSA. More specifically, we concentrate on two questions: Since we need to reduce word forms to base forms to combat data sparseness, is it more useful to use tokenization or lemmatization? And given that the part-of-speech (POS) tagset for Arabic contains a fair amount of morphological information, how much of this information is useful for SSA? More specifically, we investigate two different reduced tagsets, the RTS and the ERTS. For more detailed information see section 4. RQ2 addresses the impact of using two standard features, frequently employed in SSA studies (Wiebe et al., 2004; Turney, 2002), on social media data, which exhibit DA usage and text length variations, e.g. in twitter data. First, we investigate the utility of applying a UNIQUE feature (Wiebe et al., 2004) where low frequency words below a threshold are replaced with the token ”UNIQUE”. Given that our data includes very short posts (e.g., twitter data has a limit of only 140 characters per tweet), it is questionable whether the UNIQUE feature will be useful or whether it replaces too many content words. Second, we test whether a polarity lexicon extracted in a standard domain using Modern Standard Arabi"
W12-3705,W03-1017,0,0.249658,"Missing"
W17-1318,P89-1010,0,0.191987,"nce. Often times, contexts where negation is employed would be more SUBJ than OBJ, based on the distribution of negation particles in the TRAIN and DEV data. Examples of negation particles that occur more frequently in the SUBJ class are lA (which negates imperfective verbs) lm (which negates things in the past) and ln (which negates things in the future). 3.2 Subjectivity with Syntactically Motivated Feature Selection In order to further investigate the utility of functional segments for subjectivity classification, we perform a set of experiments based on pointwise mutual information (PMI) (Church and Hanks, 1989; Church and Hanks, 1990) feature selection focused at these segments. PMI is a statistical measure of the co-occurrence of two events that captures the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions. The PMI between a functional segment ‘fs’ and its class ‘c’ (e.g., the OBJ vs. the SUBJ class) is: Restriction: In situations where it is necessarily to be precise, restrict a statement, stress a position, etc., employment of restriction particles is useful. Restriction particles like &lt;lA (Eng., ‘except’) and &lt;nmA (Eng. ‘"
W17-1318,J90-1003,0,0.134241,"ts where negation is employed would be more SUBJ than OBJ, based on the distribution of negation particles in the TRAIN and DEV data. Examples of negation particles that occur more frequently in the SUBJ class are lA (which negates imperfective verbs) lm (which negates things in the past) and ln (which negates things in the future). 3.2 Subjectivity with Syntactically Motivated Feature Selection In order to further investigate the utility of functional segments for subjectivity classification, we perform a set of experiments based on pointwise mutual information (PMI) (Church and Hanks, 1989; Church and Hanks, 1990) feature selection focused at these segments. PMI is a statistical measure of the co-occurrence of two events that captures the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions. The PMI between a functional segment ‘fs’ and its class ‘c’ (e.g., the OBJ vs. the SUBJ class) is: Restriction: In situations where it is necessarily to be precise, restrict a statement, stress a position, etc., employment of restriction particles is useful. Restriction particles like &lt;lA (Eng., ‘except’) and &lt;nmA (Eng. ‘but for’) are used more w"
W17-1318,W11-0413,1,0.816165,"revolutionary!), negative (e.g., The Syria war is terrifying!), neutral (e.g., The new models may be released next week.), or, sometimes, mixed (e.g., I really like this phone, but it is way too expensive!). The field of subjectivity and sentiment analysis (SSA) is a very vibrant one and there has been a flurry of research on especially the English language (Wiebe et al., 2004; Liu, 2010; Dave et al., 2003; Pang and Lee, 2008; Chaovalit and Zhou, 2005; Zhuang et al., 2006). By now, there is also a fair amount of work on morphologically rich languages (MRL) (Tsarfaty et al., 2010) like Arabic (Abdul-Mageed and Diab, 2011; Abdul-Mageed et al., 2011; Abdul-Mageed 147 Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 147–156, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Dataset ATB1V4.1 ATB2V3.1 ATB3V3.2 ALL ways to break down the lexical space based on relevant, if not comparable, grammatical grounds? That is the question we seek to answer in the current work. Overall, we make the following contributions: (1) We present a new human-labeled ATB dataset for SSA; (2) We introduce a new syntactically motivated feature selection method for SSA on"
W17-1318,abdul-mageed-diab-2012-awatif,1,0.926686,"Missing"
W17-1318,P11-2103,1,0.922186,"g., The Syria war is terrifying!), neutral (e.g., The new models may be released next week.), or, sometimes, mixed (e.g., I really like this phone, but it is way too expensive!). The field of subjectivity and sentiment analysis (SSA) is a very vibrant one and there has been a flurry of research on especially the English language (Wiebe et al., 2004; Liu, 2010; Dave et al., 2003; Pang and Lee, 2008; Chaovalit and Zhou, 2005; Zhuang et al., 2006). By now, there is also a fair amount of work on morphologically rich languages (MRL) (Tsarfaty et al., 2010) like Arabic (Abdul-Mageed and Diab, 2011; Abdul-Mageed et al., 2011; Abdul-Mageed 147 Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 147–156, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Dataset ATB1V4.1 ATB2V3.1 ATB3V3.2 ALL ways to break down the lexical space based on relevant, if not comparable, grammatical grounds? That is the question we seek to answer in the current work. Overall, we make the following contributions: (1) We present a new human-labeled ATB dataset for SSA; (2) We introduce a new syntactically motivated feature selection method for SSA on Arabic that can arguably al"
W17-1318,R13-1001,1,0.903843,"Missing"
W17-1318,W13-1608,0,0.0490124,"Missing"
W17-1318,P13-2088,0,0.0411728,"Missing"
W17-1318,D15-1299,0,0.0283602,"Missing"
W17-1318,refaee-rieser-2014-arabic,0,0.0396719,"Missing"
W17-1318,S16-1077,0,0.0227282,"Missing"
W17-1318,S14-2009,0,0.0698327,"Missing"
W17-1318,S15-2078,0,0.0647329,"Missing"
W17-1318,N15-1078,0,0.0461353,"Missing"
W17-1318,W10-1401,0,0.0161495,"ositive (e.g., The new machines are revolutionary!), negative (e.g., The Syria war is terrifying!), neutral (e.g., The new models may be released next week.), or, sometimes, mixed (e.g., I really like this phone, but it is way too expensive!). The field of subjectivity and sentiment analysis (SSA) is a very vibrant one and there has been a flurry of research on especially the English language (Wiebe et al., 2004; Liu, 2010; Dave et al., 2003; Pang and Lee, 2008; Chaovalit and Zhou, 2005; Zhuang et al., 2006). By now, there is also a fair amount of work on morphologically rich languages (MRL) (Tsarfaty et al., 2010) like Arabic (Abdul-Mageed and Diab, 2011; Abdul-Mageed et al., 2011; Abdul-Mageed 147 Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 147–156, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Dataset ATB1V4.1 ATB2V3.1 ATB3V3.2 ALL ways to break down the lexical space based on relevant, if not comparable, grammatical grounds? That is the question we seek to answer in the current work. Overall, we make the following contributions: (1) We present a new human-labeled ATB dataset for SSA; (2) We introduce a new syntactically motiv"
W17-1318,P02-1053,0,0.053295,"Missing"
W17-1318,J04-3002,0,0.0380901,".) vs. those that express opinions, feelings, evaluations, and speculations (Banfield, 1982; Wiebe, 1994) and hence are subjective. Subjective language is further classified based on its sentiment into positive (e.g., The new machines are revolutionary!), negative (e.g., The Syria war is terrifying!), neutral (e.g., The new models may be released next week.), or, sometimes, mixed (e.g., I really like this phone, but it is way too expensive!). The field of subjectivity and sentiment analysis (SSA) is a very vibrant one and there has been a flurry of research on especially the English language (Wiebe et al., 2004; Liu, 2010; Dave et al., 2003; Pang and Lee, 2008; Chaovalit and Zhou, 2005; Zhuang et al., 2006). By now, there is also a fair amount of work on morphologically rich languages (MRL) (Tsarfaty et al., 2010) like Arabic (Abdul-Mageed and Diab, 2011; Abdul-Mageed et al., 2011; Abdul-Mageed 147 Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 147–156, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Dataset ATB1V4.1 ATB2V3.1 ATB3V3.2 ALL ways to break down the lexical space based on relevant, if not comparable, grammatical ground"
W17-1318,J94-2004,0,0.160973,"Missing"
W18-1104,R09-1010,0,0.0321819,"the utility of our phrase based data acquisition approach, and the advantage of our models. 6.5 Negative Results with MT In absence of labeled data, MT can be been used for converting labeled data from a source language (often English) into one or more target languages for classification. Although, to the best of our knowledge, there are currently no attempts to exploit MT for emotion detection, there have been successful efforts on the (conceptually relevant) task of sentiment analysis. Examples of sentiment systems employing MT include Hiroshi et al. (2004) (Japanese), Wan (2008) (Chinese), Brooke et al. (2009); Smith et al. (2016) (Spanish), Mihalcea et al. (2007) (Romanian), and Mohammad et al. (2016) (Arabic). Clearly, MT has its limitations. Hence, whether MT will be as useful for emotion as it proved to be for sentiment is in our view an interesting question. As a first attempt to explore answers, we experiment with the MT-DIST data described in Section 3 under two settings: (a) We train exclusively on MT-DIST and test on LAMA-DINA, and (b) We merge MTDIST with the training split of LAMA-DINA to form a single training set that we refer to as MTD2. Again, we use the same settings as described in"
W18-1104,L18-1577,1,0.808669,"d is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the list of seed expressions used, improve on the manual annotation study, and empirically validate the method on the practical emotion modeling task both on our data and on an external dataset. Our work also has affinity to works on Arabic text classification (Abdul-Mageed et al., 2011; Refaee and Rieser, 2014; Abdul-Mageed et al., 2014; Nabil et al., 2015; Salameh et al., 2015; Abdul-Mageed, 2017, 2018; Alshehri et al., 2018; Abdul-Mageed et al., 2018), but we focus on emotion. emotion that is over double the size of their data (i.e., 7, 268 vs. 2, 984 tweets), (3) we introduce a hybrid supervision method and apply it to develop promising emotion detection models using a powerful deep gated recurrent neural network (GRU), and (4) we explore the utility of MT in the context of emotion detection, hoping our data-driven findings will lead to work enhancing our understanding of emotion. The remainder of the paper is organized as follows: Section 2 is a review of related work. Section 3 is an overview of the different datasets acquired and used"
W18-1104,P11-2103,1,0.876709,"Missing"
W18-1104,P17-1067,1,0.948241,"l suffers from the bottleneck of labeled data. This is true for the Arabic language. With the exception of Abdul-Mageed et al. (2016) who develop data for Ekman’s (Ekman, 1992) 6 basic emotions {anger, disgust, fear, joy, sadness, surprise} and another dataset released very recently as part of SemEval 2018 (Mohammad and Kiritchenko, 2018) that focuses on the 4 emotions 25 Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 25–35 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics Abdul-Mageed and Ungar (2017). For example, Mohammad (2012) collects a corpus of 50, 000 tweets using seed words corresponding to the 6 Ekman emotions and exploits it for building emotion models. More recently, Mohammad and Bravo-Marquez (2017) label a dataset of 7, 097 tweets with emotion intensity tags for the four emotions {anger, fear, joy, sadness} using a method they refer to as best-worst annotation (Kiritchenko and Mohammad, 2016). They describe the method as producing reliable labels. In a similar vein, Wang et al. (2012) collect a large emotion corpus (N= 5 million) for 5 of Ekman’s 6 basic emotions (skipping di"
W18-1104,C04-1071,0,0.071016,"gains on the SE18 external dataset further demonstrate the utility of our phrase based data acquisition approach, and the advantage of our models. 6.5 Negative Results with MT In absence of labeled data, MT can be been used for converting labeled data from a source language (often English) into one or more target languages for classification. Although, to the best of our knowledge, there are currently no attempts to exploit MT for emotion detection, there have been successful efforts on the (conceptually relevant) task of sentiment analysis. Examples of sentiment systems employing MT include Hiroshi et al. (2004) (Japanese), Wan (2008) (Chinese), Brooke et al. (2009); Smith et al. (2016) (Spanish), Mihalcea et al. (2007) (Romanian), and Mohammad et al. (2016) (Arabic). Clearly, MT has its limitations. Hence, whether MT will be as useful for emotion as it proved to be for sentiment is in our view an interesting question. As a first attempt to explore answers, we experiment with the MT-DIST data described in Section 3 under two settings: (a) We train exclusively on MT-DIST and test on LAMA-DINA, and (b) We merge MTDIST with the training split of LAMA-DINA to form a single training set that we refer to a"
W18-1104,D15-1299,0,0.073461,"different in that we use seed expressions, rather than hashtags. Our data collection method is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the list of seed expressions used, improve on the manual annotation study, and empirically validate the method on the practical emotion modeling task both on our data and on an external dataset. Our work also has affinity to works on Arabic text classification (Abdul-Mageed et al., 2011; Refaee and Rieser, 2014; Abdul-Mageed et al., 2014; Nabil et al., 2015; Salameh et al., 2015; Abdul-Mageed, 2017, 2018; Alshehri et al., 2018; Abdul-Mageed et al., 2018), but we focus on emotion. emotion that is over double the size of their data (i.e., 7, 268 vs. 2, 984 tweets), (3) we introduce a hybrid supervision method and apply it to develop promising emotion detection models using a powerful deep gated recurrent neural network (GRU), and (4) we explore the utility of MT in the context of emotion detection, hoping our data-driven findings will lead to work enhancing our understanding of emotion. The remainder of the paper is organized as follows: Section 2"
W18-1104,N16-1095,0,0.0275146,"n Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 25–35 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics Abdul-Mageed and Ungar (2017). For example, Mohammad (2012) collects a corpus of 50, 000 tweets using seed words corresponding to the 6 Ekman emotions and exploits it for building emotion models. More recently, Mohammad and Bravo-Marquez (2017) label a dataset of 7, 097 tweets with emotion intensity tags for the four emotions {anger, fear, joy, sadness} using a method they refer to as best-worst annotation (Kiritchenko and Mohammad, 2016). They describe the method as producing reliable labels. In a similar vein, Wang et al. (2012) collect a large emotion corpus (N= 5 million) for 5 of Ekman’s 6 basic emotions (skipping disgust), but adding love and thankfulness using a seed set of 131 hashtags representing these emotions. The authors then randomly sample 400 tweets and label them manually with a tag from the set relevant, irrelevant. Abdul-Mageed and Ungar (2017) also collect a large dataset of English tweets using 665 hashtags representing 24 different types of emotions. The authors also perform a manual annotation study show"
W18-1104,P10-3008,0,0.0282429,"these emotions. The authors then randomly sample 400 tweets and label them manually with a tag from the set relevant, irrelevant. Abdul-Mageed and Ungar (2017) also collect a large dataset of English tweets using 665 hashtags representing 24 different types of emotions. The authors also perform a manual annotation study showing the utility of using hashtags as labels. Other work includes Yan and Turtle (2016) who use crowdsourcing and lab-controlled conditions to label a dataset of 15, 553 tweets that they then exploit to build baseline models. Related to our work is also scholarship on mood (Nguyen, 2010; De Choudhury et al., 2012) which also depend on collecting data using seed words. Our work also falls under distant supervision, but is different in that we use seed expressions, rather than hashtags. Our data collection method is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the list of seed expressions used, improve on the manual annotation study, and empirically validate the method on the practical emotion modeling task both on our data and on an external dataset. Our work"
W18-1104,P04-1035,0,0.0180555,"Missing"
W18-1104,P07-1123,0,0.0627695,"oach, and the advantage of our models. 6.5 Negative Results with MT In absence of labeled data, MT can be been used for converting labeled data from a source language (often English) into one or more target languages for classification. Although, to the best of our knowledge, there are currently no attempts to exploit MT for emotion detection, there have been successful efforts on the (conceptually relevant) task of sentiment analysis. Examples of sentiment systems employing MT include Hiroshi et al. (2004) (Japanese), Wan (2008) (Chinese), Brooke et al. (2009); Smith et al. (2016) (Spanish), Mihalcea et al. (2007) (Romanian), and Mohammad et al. (2016) (Arabic). Clearly, MT has its limitations. Hence, whether MT will be as useful for emotion as it proved to be for sentiment is in our view an interesting question. As a first attempt to explore answers, we experiment with the MT-DIST data described in Section 3 under two settings: (a) We train exclusively on MT-DIST and test on LAMA-DINA, and (b) We merge MTDIST with the training split of LAMA-DINA to form a single training set that we refer to as MTD2. Again, we use the same settings as described in 5 with both the online classifiers and GRUs, and 7 Con"
W18-1104,P09-1113,0,0.0819144,"e annotators received no training, but were given samples of annotated sentences to illustrate Ekman’s 6 types of emotions. Annotators also labeled the data for mixed-emotion and no-emotion. In addition, annotators were required to assign emotion intensity tags from the set {low, medium, high} to all emotion-carrying sentences (thus excluding sentences tagged with no-emotion). Our work differs from these in that we focus on Arabic and the Twitter domain. A number of works use emotion hashtags (e.g., #happy, #sad) as a way of automatically labeling data for emotion (i.e., distant supervision) (Mintz et al., 2009). These include Mohammad (2012); Mohammad and Kiritchenko (2015); Wang et al. (2012); Volkova and Bachrach (2016); 3 Data Building LAMA: We collect a dataset of Arabic tweets from the Twitter public stream ex26 ploiting the Twitter API 1 using a seed set of emotion-carrying expressions following AbdulMageed et al. (2016). More specifically, we use a list of seeds for each of the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. As such, we add anticipation and trust to the 6 categories Abdul-Mageed et al. (2016) work with. In this ap"
W18-1104,refaee-rieser-2014-arabic,0,0.018628,"ur work also falls under distant supervision, but is different in that we use seed expressions, rather than hashtags. Our data collection method is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the list of seed expressions used, improve on the manual annotation study, and empirically validate the method on the practical emotion modeling task both on our data and on an external dataset. Our work also has affinity to works on Arabic text classification (Abdul-Mageed et al., 2011; Refaee and Rieser, 2014; Abdul-Mageed et al., 2014; Nabil et al., 2015; Salameh et al., 2015; Abdul-Mageed, 2017, 2018; Alshehri et al., 2018; Abdul-Mageed et al., 2018), but we focus on emotion. emotion that is over double the size of their data (i.e., 7, 268 vs. 2, 984 tweets), (3) we introduce a hybrid supervision method and apply it to develop promising emotion detection models using a powerful deep gated recurrent neural network (GRU), and (4) we explore the utility of MT in the context of emotion detection, hoping our data-driven findings will lead to work enhancing our understanding of emotion. The remainder"
W18-1104,S17-2088,0,0.0592273,"Missing"
W18-1104,S18-1001,0,0.40644,"ee of emotion arousal when an emotion exists) as a single task (rather then two stages). We believe a single stage set up can cause annotator cognitive overload and empirically show how a more simplified, two-stage annotation process yields higher annotator inter-rater reliability.We then proceed to show the utility of exploiting data acquired with our method to develop emotion detection models, including supervised, distant supervised, and hybridly-supervised (i.e., a mixture of supervised and distant supervised). We also validate our method of data acquisition on an external dataset (i.e., (Mohammad and Kiritchenko, 2018)), further proving its usefulness in capturing emotion signal. Finally, training on machine translation (MT) data, we acquire initial results that may be suggesting emotion does not translate (i.e., it may not be possible to successfully build emotion detection systems using MT). Overall, we offer the following contributions: (1) We extend a first-person seed phrase approach introduced by (Abdul-Mageed et al., 2016) for emotion data collection from 6 to 8 emotion categories, and improve on the annotation procedure, acquiring higher agreement between the judges, (2) we introduce a new dataset f"
W18-1104,N15-1078,0,0.0619384,"e use seed expressions, rather than hashtags. Our data collection method is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the list of seed expressions used, improve on the manual annotation study, and empirically validate the method on the practical emotion modeling task both on our data and on an external dataset. Our work also has affinity to works on Arabic text classification (Abdul-Mageed et al., 2011; Refaee and Rieser, 2014; Abdul-Mageed et al., 2014; Nabil et al., 2015; Salameh et al., 2015; Abdul-Mageed, 2017, 2018; Alshehri et al., 2018; Abdul-Mageed et al., 2018), but we focus on emotion. emotion that is over double the size of their data (i.e., 7, 268 vs. 2, 984 tweets), (3) we introduce a hybrid supervision method and apply it to develop promising emotion detection models using a powerful deep gated recurrent neural network (GRU), and (4) we explore the utility of MT in the context of emotion detection, hoping our data-driven findings will lead to work enhancing our understanding of emotion. The remainder of the paper is organized as follows: Section 2 is a review of relate"
W18-1104,S12-1033,0,0.340576,"a. This is true for the Arabic language. With the exception of Abdul-Mageed et al. (2016) who develop data for Ekman’s (Ekman, 1992) 6 basic emotions {anger, disgust, fear, joy, sadness, surprise} and another dataset released very recently as part of SemEval 2018 (Mohammad and Kiritchenko, 2018) that focuses on the 4 emotions 25 Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 25–35 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics Abdul-Mageed and Ungar (2017). For example, Mohammad (2012) collects a corpus of 50, 000 tweets using seed words corresponding to the 6 Ekman emotions and exploits it for building emotion models. More recently, Mohammad and Bravo-Marquez (2017) label a dataset of 7, 097 tweets with emotion intensity tags for the four emotions {anger, fear, joy, sadness} using a method they refer to as best-worst annotation (Kiritchenko and Mohammad, 2016). They describe the method as producing reliable labels. In a similar vein, Wang et al. (2012) collect a large emotion corpus (N= 5 million) for 5 of Ekman’s 6 basic emotions (skipping disgust), but adding love and th"
W18-1104,D16-1217,1,0.763424,"ase based data acquisition approach, and the advantage of our models. 6.5 Negative Results with MT In absence of labeled data, MT can be been used for converting labeled data from a source language (often English) into one or more target languages for classification. Although, to the best of our knowledge, there are currently no attempts to exploit MT for emotion detection, there have been successful efforts on the (conceptually relevant) task of sentiment analysis. Examples of sentiment systems employing MT include Hiroshi et al. (2004) (Japanese), Wan (2008) (Chinese), Brooke et al. (2009); Smith et al. (2016) (Spanish), Mihalcea et al. (2007) (Romanian), and Mohammad et al. (2016) (Arabic). Clearly, MT has its limitations. Hence, whether MT will be as useful for emotion as it proved to be for sentiment is in our view an interesting question. As a first attempt to explore answers, we experiment with the MT-DIST data described in Section 3 under two settings: (a) We train exclusively on MT-DIST and test on LAMA-DINA, and (b) We merge MTDIST with the training split of LAMA-DINA to form a single training set that we refer to as MTD2. Again, we use the same settings as described in 5 with both the onli"
W18-1104,S17-1007,0,0.0384645,"fear, joy, sadness, surprise} and another dataset released very recently as part of SemEval 2018 (Mohammad and Kiritchenko, 2018) that focuses on the 4 emotions 25 Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 25–35 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics Abdul-Mageed and Ungar (2017). For example, Mohammad (2012) collects a corpus of 50, 000 tweets using seed words corresponding to the 6 Ekman emotions and exploits it for building emotion models. More recently, Mohammad and Bravo-Marquez (2017) label a dataset of 7, 097 tweets with emotion intensity tags for the four emotions {anger, fear, joy, sadness} using a method they refer to as best-worst annotation (Kiritchenko and Mohammad, 2016). They describe the method as producing reliable labels. In a similar vein, Wang et al. (2012) collect a large emotion corpus (N= 5 million) for 5 of Ekman’s 6 basic emotions (skipping disgust), but adding love and thankfulness using a seed set of 131 hashtags representing these emotions. The authors then randomly sample 400 tweets and label them manually with a tag from the set relevant, irrelevant"
W18-1104,S07-1013,0,0.173257,"as follows: Section 2 is a review of related work. Section 3 is an overview of the different datasets acquired and used in our work. Section 4 is a description of both the first-person seed phrase approach to data acquisition and the annotation study we performed. Section 5 is about our methods, and 6 is where we introduce our models and describe negative experiments with MT. We conclude in Section 7. 2 Related Work There is a small, but growing, body of NLP literature on emotion. A number of papers have focused on creating datasets for emotion detection. The SemEval 2007 Affective Text task (Strapparava and Mihalcea, 2007) focused on emotion annotation and classification where a dataset of 1, 250 news headlines was human labeled with the 6 basic emotions of Ekman (Ekman, 1972) and provided to participants. Similarly, Aman and Szpakowicz (2007) describe an emotion annotation and classification task on blog post data of 4, 090 sentences. The data were collected with identified emotion seeds words. Aman and Szpakowicz (2007) point out that the annotators received no training, but were given samples of annotated sentences to illustrate Ekman’s 6 types of emotions. Annotators also labeled the data for mixed-emotion"
W18-1104,P16-1148,0,0.164395,"types of emotions. Annotators also labeled the data for mixed-emotion and no-emotion. In addition, annotators were required to assign emotion intensity tags from the set {low, medium, high} to all emotion-carrying sentences (thus excluding sentences tagged with no-emotion). Our work differs from these in that we focus on Arabic and the Twitter domain. A number of works use emotion hashtags (e.g., #happy, #sad) as a way of automatically labeling data for emotion (i.e., distant supervision) (Mintz et al., 2009). These include Mohammad (2012); Mohammad and Kiritchenko (2015); Wang et al. (2012); Volkova and Bachrach (2016); 3 Data Building LAMA: We collect a dataset of Arabic tweets from the Twitter public stream ex26 ploiting the Twitter API 1 using a seed set of emotion-carrying expressions following AbdulMageed et al. (2016). More specifically, we use a list of seeds for each of the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. As such, we add anticipation and trust to the 6 categories Abdul-Mageed et al. (2016) work with. In this approach, we collect all tweets where a seed phrase appears in the tweet body text. Note this approach is only cond"
W18-1104,D08-1058,0,0.0421448,"t further demonstrate the utility of our phrase based data acquisition approach, and the advantage of our models. 6.5 Negative Results with MT In absence of labeled data, MT can be been used for converting labeled data from a source language (often English) into one or more target languages for classification. Although, to the best of our knowledge, there are currently no attempts to exploit MT for emotion detection, there have been successful efforts on the (conceptually relevant) task of sentiment analysis. Examples of sentiment systems employing MT include Hiroshi et al. (2004) (Japanese), Wan (2008) (Chinese), Brooke et al. (2009); Smith et al. (2016) (Spanish), Mihalcea et al. (2007) (Romanian), and Mohammad et al. (2016) (Arabic). Clearly, MT has its limitations. Hence, whether MT will be as useful for emotion as it proved to be for sentiment is in our view an interesting question. As a first attempt to explore answers, we experiment with the MT-DIST data described in Section 3 under two settings: (a) We train exclusively on MT-DIST and test on LAMA-DINA, and (b) We merge MTDIST with the training split of LAMA-DINA to form a single training set that we refer to as MTD2. Again, we use t"
W18-1104,J04-3002,0,0.195303,"Missing"
W18-1104,N16-2011,0,0.101397,"ilar vein, Wang et al. (2012) collect a large emotion corpus (N= 5 million) for 5 of Ekman’s 6 basic emotions (skipping disgust), but adding love and thankfulness using a seed set of 131 hashtags representing these emotions. The authors then randomly sample 400 tweets and label them manually with a tag from the set relevant, irrelevant. Abdul-Mageed and Ungar (2017) also collect a large dataset of English tweets using 665 hashtags representing 24 different types of emotions. The authors also perform a manual annotation study showing the utility of using hashtags as labels. Other work includes Yan and Turtle (2016) who use crowdsourcing and lab-controlled conditions to label a dataset of 15, 553 tweets that they then exploit to build baseline models. Related to our work is also scholarship on mood (Nguyen, 2010; De Choudhury et al., 2012) which also depend on collecting data using seed words. Our work also falls under distant supervision, but is different in that we use seed expressions, rather than hashtags. Our data collection method is most similar to Abdul-Mageed et al. (2016), who also use phrase seeds to acquire tweets for Ekman’s 6 basic emotions, but we extend the work to 8 emotions, expand the"
W18-1104,Q17-1021,0,0.0494919,"Missing"
W18-3930,L18-1577,1,0.90337,"viation of σ = 0.05. Our models are trained using the Keras (Chollet and others, 2015) library with a Tensorflow (Abadi et al., 2016) backend. We train each of our 6 deep learning classifiers across 3 different settings pertaining the way we initialize the embeddings for the input layer in each network. The three embedding settings are: 1. Random embeddings: Where we initialize the input layer randomly. 2. AOC-based embeddings: We make use of the ∼ 3M unlabeled comments in AOC by training a “continuous bag of words” (CBOW) (Mikolov et al., 2013) model exploiting them. We adopt the settings in Abdul-Mageed et al. (2018) for training our model to acquire 300 dimensional word vectors. 1 The benchmarked data can be obtained by emailing the authors. See also project repository at: https://github. com/UBC-NLP/aoc_id. 269 Binary Three-way Method Dev Test Dev Test Traditional Classifiers Baseline (majority class in Train) 58.75 58.75 58.75 58.75 Logistic Regression (1+2+3 grams) 84.18 83.71 86.91 85.75 Naive Bayes (1+2+3 grams) 84.97 84.53 87.51 87.81 SVM (1+2+3 grams) 82.79 82.41 85.51 84.27 Logistic Regression (1+2+3 grams TF-IDF) 83.96 83.24 86.71 85.51 Naive Bayes (1+2+3 grams TF-IDF) 83.52 82.91 86.61 86.87 84"
W18-3930,P10-1010,0,0.0139798,"language, and dialect, identification. These technologies are useful for applications ranging from monitoring health and well-being (Yepes et al., 2015; Nguyen et al., 2016; Nguyen et al., 2017; Abdul-Mageed et al., 2017), to real-time disaster operation management (Sakaki et al., 2010; Palen and Hughes, 2018), and analysis of human mobility (Hawelka et al., 2014; Jurdak et al., 2015; Louail et al., 2014). Language identification is also an enabling technology that can help automatically filter foreign text in some tasks (Lui and Baldwin, 2012), acquire multilingual data (e.g., from the web) (Abney and Bird, 2010), including to enhance tasks like machine translation (Ling et al., 2013). Arabic. In this paper our focus is on Arabic, a term that refers to a wide collection of varieties. These varieties are the result of the interweave between the native languages of the Middle East and North Africa and Arabic itself. Modern Standard Arabic (MSA), the modern variety of the language used in pan-Arab news outlets like AlJazeera and in educational circles in the Arab world, differs phonetically, phonologically, lexically, and syntactically from the varieties spoken in everyday communication by native speaker"
W18-3930,W16-4819,0,0.0584923,"Missing"
W18-3930,cotterell-callison-burch-2014-multi,0,0.103749,"lects remained mostly spoken. Dialects started to find their way in written form with the spread of social media, thus affording an opportunity for researchers to use these data for NLP. This motivated Zaidan and Callison-Burch (2014) to create a large-scale repository of Arabic texts, the Arabic Online Commentary (AOC). The resource is composed of ∼ 3M MSA and dialectal comments on a number of Arabic news sites. A portion of the data (> 108K comments) is manually annotated via crowdsourcing. The dataset was exploited for dialect identification in Zaidan and Callison-Burch (2014) and later in Cotterell and Callison-Burch (2014). These works, however, pre-date the current boom in NLP where deep neural networks enable better learning (given sufficiently large training data). Cotterell and Callison-Burch (2014) use n-fold cross validation in their work, thus making it costly to adopt the same data split procedure to develop deep learning models. This is the case since deep models can take long times to train and optimize. For this reason, it is desirable to benchmark the AOC dataset for deep learning research. This motivates our work. We also ask the empirical question: To what extent can we tease apart the Arabic vari"
W18-3930,D14-1154,0,0.0689988,"Missing"
W18-3930,P13-2081,0,0.24136,"Missing"
W18-3930,W14-3911,0,0.0468869,"Missing"
W18-3930,habash-etal-2012-conventional,0,0.0427862,"e language (Diab et al., 2010; Habash, 2010; Abdul-Mageed, 2015; Abdul-Mageed, 2017). These ‘everyday’ varieties constitute the dialects of Arabic. Examples of these are Egyptian (EGY), Gulf (GLF), Levantine (LEV), and Moroccan (MOR). In addition to MSA and dialects, Classical Arabic also exists and is the variety of historical literary texts and religious discourse. Arabic Dialects. Language varieties, including those of Arabic, can be categorized based on shared linguistic features. For Arabic, one classical categorization is based on geographical locations. For example, in addition to MSA, Habash et al. (2012), provides 5 main categories, as shown in Figure 1. This same classification is also common in the literature, and includes: This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/. Licence details: http: 263 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 263–274 Santa Fe, New Mexico, USA, August 20, 2018. • Egyptian: The variety spoken in Egypt, which is widely spread due to the historical impact of Egyptian media • Gulf: A variety spoken primarily in Saudi Arabia, UAE, Kuwait a"
W18-3930,D15-1254,0,0.389257,"Missing"
W18-3930,W17-0221,0,0.0233574,"Missing"
W18-3930,P17-2009,0,0.0425259,"Missing"
W18-3930,D14-1181,0,0.00688602,"LSTM), (3) convolutional LSTM (CLSTM), (4) bidirectional LSTM (BiLSTM), (5) bidirectional gated recurrent units (BiGRU), and (6) 266 BiLSTM with attention. While there are other variations of how some of these models learn (Vaswani et al., 2017), we believe these models with the variations we exploit form a strong basis for our benchmarking objective. In all our models, we use pre-trained word vectors based on word2vec to initialize the networks. We then fine-tune weights during learning. Word-based Convolutional Neural Network (CNN): This model is conceptually similar to the one described in Kim (2014), and has the following architecture: • Input layer: an input layer to map word sequence w into a sequence vector x where xw is a realvalued vector (xw ∈ Rdemb , with demb = 300 in all our models) initialized from external embedding model and tuned during training. The embedding layer is followed by a dropout rate of 0.5 for regularization (in this case to prevent co-adaptation between hidden units). • Convolution layer: Two 1-D convolution operations are applied in parallel to the input layer to map input sequence x into a hidden sequence h. A filter k ∈ Rwdemb is applied to a window of conca"
W18-3930,E17-1087,0,0.0243344,"Missing"
W18-3930,P13-1018,0,0.0175537,"lications ranging from monitoring health and well-being (Yepes et al., 2015; Nguyen et al., 2016; Nguyen et al., 2017; Abdul-Mageed et al., 2017), to real-time disaster operation management (Sakaki et al., 2010; Palen and Hughes, 2018), and analysis of human mobility (Hawelka et al., 2014; Jurdak et al., 2015; Louail et al., 2014). Language identification is also an enabling technology that can help automatically filter foreign text in some tasks (Lui and Baldwin, 2012), acquire multilingual data (e.g., from the web) (Abney and Bird, 2010), including to enhance tasks like machine translation (Ling et al., 2013). Arabic. In this paper our focus is on Arabic, a term that refers to a wide collection of varieties. These varieties are the result of the interweave between the native languages of the Middle East and North Africa and Arabic itself. Modern Standard Arabic (MSA), the modern variety of the language used in pan-Arab news outlets like AlJazeera and in educational circles in the Arab world, differs phonetically, phonologically, lexically, and syntactically from the varieties spoken in everyday communication by native speakers of the language (Diab et al., 2010; Habash, 2010; Abdul-Mageed, 2015; A"
W18-3930,P12-3005,0,0.0364436,"asive use of social media strongly motivates need for technologies like language, and dialect, identification. These technologies are useful for applications ranging from monitoring health and well-being (Yepes et al., 2015; Nguyen et al., 2016; Nguyen et al., 2017; Abdul-Mageed et al., 2017), to real-time disaster operation management (Sakaki et al., 2010; Palen and Hughes, 2018), and analysis of human mobility (Hawelka et al., 2014; Jurdak et al., 2015; Louail et al., 2014). Language identification is also an enabling technology that can help automatically filter foreign text in some tasks (Lui and Baldwin, 2012), acquire multilingual data (e.g., from the web) (Abney and Bird, 2010), including to enhance tasks like machine translation (Ling et al., 2013). Arabic. In this paper our focus is on Arabic, a term that refers to a wide collection of varieties. These varieties are the result of the interweave between the native languages of the Middle East and North Africa and Arabic itself. Modern Standard Arabic (MSA), the modern variety of the language used in pan-Arab news outlets like AlJazeera and in educational circles in the Arab world, differs phonetically, phonologically, lexically, and syntacticall"
W18-3930,P17-2033,0,0.0440776,"Missing"
W18-3930,P18-1187,0,0.0247074,"Missing"
W18-3930,W14-5313,0,0.0519129,"Missing"
W18-3930,D15-1206,0,0.0381582,"ows: zt = σ(Wz .[ht−1 , xt ]) rt = σ(Wr .[ht−1 , xt ]) ˜ t = tanh(W.[rt ∗ ht−1 , xt ]) h ˜t ht = (1 − zt ) ∗ ht−1 + zt ∗ h (3) The Bidirectional GRU (BiGRUs) can be obtained by combining two GRUs, each looking at a different direction similar to the case of BiLSTMs above. We employ the same regularization techniques applied to the LSTM and BiLSTM networks. Attention-based BiLSTM Recently, using an attention mechanism with a neural networks has resulted in notable success in a wide range of NLP tasks, such as machine translation, speech recognition, and image captioning (Bahdanau et al., 2014; Xu et al., 2015; Chorowski et al., 2015). In this section, we describe an attention mechanism that we employ in one of our models (BiLSTM) that turned out to perform well without attention, hoping the mechanism will further improve model performance. We use a simple implementation inspired by Zhou et al. (2016) where attention is applied to the output vector of the LSTM layer. If H is a matrix consisting of output vectors [h1 , h2 , ..hT ] (where T is the sentence length), we can compute the attention vector α of the sequence as follows: et = tanh(ht ) exp(et ) αt = T Σi=1 exp(ei ) 268 (4) Finally, the repre"
W18-3930,W15-3821,0,0.114542,"Missing"
W18-3930,P11-2007,0,0.203852,"Missing"
W18-3930,J14-1006,0,0.303938,"n Saudi Arabia, UAE, Kuwait and Qatar • Iraqi: The variety spoken by the people of Iraq • Levantine: The variety spoken primarily by the Levant (i.e., people of Syria, Lebanon, and Palestine) • Maghrebi: The variety spoken by people of North Africa, excluding Egypt Figure 1: One categorization of Arabic dialects (Zaidan and Callison-Burch, 2011) Arabic dialectal data. For a long time, Arabic dialects remained mostly spoken. Dialects started to find their way in written form with the spread of social media, thus affording an opportunity for researchers to use these data for NLP. This motivated Zaidan and Callison-Burch (2014) to create a large-scale repository of Arabic texts, the Arabic Online Commentary (AOC). The resource is composed of ∼ 3M MSA and dialectal comments on a number of Arabic news sites. A portion of the data (> 108K comments) is manually annotated via crowdsourcing. The dataset was exploited for dialect identification in Zaidan and Callison-Burch (2014) and later in Cotterell and Callison-Burch (2014). These works, however, pre-date the current boom in NLP where deep neural networks enable better learning (given sufficiently large training data). Cotterell and Callison-Burch (2014) use n-fold cro"
W18-3930,P16-2034,0,0.0278485,"gularization techniques applied to the LSTM and BiLSTM networks. Attention-based BiLSTM Recently, using an attention mechanism with a neural networks has resulted in notable success in a wide range of NLP tasks, such as machine translation, speech recognition, and image captioning (Bahdanau et al., 2014; Xu et al., 2015; Chorowski et al., 2015). In this section, we describe an attention mechanism that we employ in one of our models (BiLSTM) that turned out to perform well without attention, hoping the mechanism will further improve model performance. We use a simple implementation inspired by Zhou et al. (2016) where attention is applied to the output vector of the LSTM layer. If H is a matrix consisting of output vectors [h1 , h2 , ..hT ] (where T is the sentence length), we can compute the attention vector α of the sequence as follows: et = tanh(ht ) exp(et ) αt = T Σi=1 exp(ei ) 268 (4) Finally, the representation vector for input text v is computed by a weighted summation over all the time steps, using obtained attention scores as weights. v = ΣTi=1 αi hi (5) The vector v is an encoded representation of the whole input text. This representation is passed to the sof tmax layer for classification."
W18-6250,P09-1113,0,0.0223428,"Missing"
W18-6250,P17-1067,1,0.914608,"Missing"
W18-6250,W18-1104,1,0.791532,"Missing"
W18-6250,D14-1162,0,0.0811634,". We use the Twitter dataset released by the organizers of the “Implicit Emotion” task, as described in the previous section. The data are partitioned into 153, 383 tweets for training, 9591 tweets for validation, and 28, 757 data points for testing. The training and validation sets were provided early for system development, while the test set was released one week before the deadline of system submission. The full details of the dataset can be found in Klinger et al. (2018). We now describe our methods in the nesxt section. 3 3.1 3.2.1 Character and/or Word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016) have boosted performance on a host of NLP tasks. Most state of the art systems now finetune these embeddings as a simple transfer learning technique targeting the first layer of a network (McCann et al., 2017). We make use of one such pre-trained embeddings (fastText) to identify the utility of tuning its learned weights on the task. FastText: The first embedding model is fastText 2 (Bojanowski et al., 2016), which builds representations based on characters, rather than only words, thus alleviating issues of complex morphology characetrestic of many languages like Ar"
W18-6250,N18-1202,0,0.0783798,"Missing"
W18-6250,S07-1013,0,0.0783134,"Missing"
W18-6250,S18-1002,0,0.0219904,"ngs to prevent co-adaptation of hidden units’ activation, and L2 − norm: we also apply an L2-norm regularization with a small value (0.0001) on the hidden units layer to prevent the network from over-fitting on training set. Each of the networks has a single hidden layer. Network architectures and hyper-parameters are listed in Table 1. Methods Pre-processing We adopt a simple pre-processing scheme, similar to most of the pre-trained models we employ. This involves lowercasing all text and filtering out urls and user mentions. We also split clusters of emojis into individual emojis, following Duppada et al. (2018). For our vocabulary V, we retain the top 100k words and then remove all words occurring &lt; 2 times, which leaves |V |= 23, 656. 3.2 Systems With Simple Embeddings Models We develop a host of models based on deep neural networks, using some of these models as our baseline models. As an additional baseline, we compare to Klinger et al. (2018) who propose a model based on Logistic Regression with a bag of word unigrams (BOW). All our deep learning models are based on variations of recurrent neural networks (RNNs), which have proved useful for several NLP tasks. RNNs are able to capture sequential"
W18-6250,P16-1148,0,0.188393,"Missing"
W18-6250,D17-1169,0,0.177209,"Missing"
W18-6250,W18-6206,0,0.0214288,"Missing"
W19-4637,W17-1318,1,0.862176,"Missing"
W19-4637,W18-3930,1,0.844256,"the MADAR shared task 2, twitter user dialect identification (Bouamor et al., 2019). Previous works on Arabic (e.g., Zaidan and Callison-Burch (2011, 2014); Elfardy and Diab (2013); Cotterell and Callison-Burch (2014)) have primarily targeted cross-country regional varieties such as Egyptian, Gulf, and Levantine, in addition to Modern Standard Arabic (MSA). These To solve Arabic dialect identification, many researchers developed models based on computational linguistics and machine learning (Elfardy and Diab, 2013; Salloum et al., 2014; Cotterell and Callison-Burch, 2014), and deep learning (Elaraby and Abdul-Mageed, 2018). In this paper, we focus on using state-of-the-arts deep learning architectures to identify Arabic dialects of Twitter users at the country level. We use the MADAR twitter corpus (Bouamor et al., 2019), comprising 21 country-level dialect labels. Namely, we employ unidirectional Gated Recurrent Unit (GRU) (Cho et al., 2014) as our baseline and pre-trained Multilingual Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) to identify dialect classes for individual tweets (which we then port at user level). We also apply semi-supervised learning to augment our tra"
W19-4637,L18-1577,1,0.854813,"Missing"
W19-4637,elfardy-diab-2012-simplified,0,0.0678162,"Missing"
W19-4637,P13-2081,0,0.313951,"uages in global settings (see Jauhiainen et al. (2018) for a survey), there has not been extensive research on teasing apart similar languages or language varieties (Zampieri et al., 2018). This is the case for Arabic, the term used to collectively refer to a large number of varieties with a vast population of native speakers (∼ 300 million). For this reason, we focus on detecting fine-grained Arabic dialect as part of our contribution to the MADAR shared task 2, twitter user dialect identification (Bouamor et al., 2019). Previous works on Arabic (e.g., Zaidan and Callison-Burch (2011, 2014); Elfardy and Diab (2013); Cotterell and Callison-Burch (2014)) have primarily targeted cross-country regional varieties such as Egyptian, Gulf, and Levantine, in addition to Modern Standard Arabic (MSA). These To solve Arabic dialect identification, many researchers developed models based on computational linguistics and machine learning (Elfardy and Diab, 2013; Salloum et al., 2014; Cotterell and Callison-Burch, 2014), and deep learning (Elaraby and Abdul-Mageed, 2018). In this paper, we focus on using state-of-the-arts deep learning architectures to identify Arabic dialects of Twitter users at the country level. We"
W19-4637,abdul-mageed-diab-2012-awatif,1,0.911468,"Missing"
W19-4637,al-sabbagh-girju-2012-yadac,0,0.208129,"Missing"
W19-4637,W16-4814,0,0.03153,"Missing"
W19-4637,W14-3601,0,0.264166,"Missing"
W19-4637,W19-4622,0,0.480882,"ntiment analysis (Abdul-Mageed, 2017b,a). Although several works have focused on detecting languages in global settings (see Jauhiainen et al. (2018) for a survey), there has not been extensive research on teasing apart similar languages or language varieties (Zampieri et al., 2018). This is the case for Arabic, the term used to collectively refer to a large number of varieties with a vast population of native speakers (∼ 300 million). For this reason, we focus on detecting fine-grained Arabic dialect as part of our contribution to the MADAR shared task 2, twitter user dialect identification (Bouamor et al., 2019). Previous works on Arabic (e.g., Zaidan and Callison-Burch (2011, 2014); Elfardy and Diab (2013); Cotterell and Callison-Burch (2014)) have primarily targeted cross-country regional varieties such as Egyptian, Gulf, and Levantine, in addition to Modern Standard Arabic (MSA). These To solve Arabic dialect identification, many researchers developed models based on computational linguistics and machine learning (Elfardy and Diab, 2013; Salloum et al., 2014; Cotterell and Callison-Burch, 2014), and deep learning (Elaraby and Abdul-Mageed, 2018). In this paper, we focus on using state-of-the-arts"
W19-4637,L18-1576,0,0.022181,"Missing"
W19-4637,W14-5904,0,0.165576,"Missing"
W19-4637,cotterell-callison-burch-2014-multi,0,0.107333,"(see Jauhiainen et al. (2018) for a survey), there has not been extensive research on teasing apart similar languages or language varieties (Zampieri et al., 2018). This is the case for Arabic, the term used to collectively refer to a large number of varieties with a vast population of native speakers (∼ 300 million). For this reason, we focus on detecting fine-grained Arabic dialect as part of our contribution to the MADAR shared task 2, twitter user dialect identification (Bouamor et al., 2019). Previous works on Arabic (e.g., Zaidan and Callison-Burch (2011, 2014); Elfardy and Diab (2013); Cotterell and Callison-Burch (2014)) have primarily targeted cross-country regional varieties such as Egyptian, Gulf, and Levantine, in addition to Modern Standard Arabic (MSA). These To solve Arabic dialect identification, many researchers developed models based on computational linguistics and machine learning (Elfardy and Diab, 2013; Salloum et al., 2014; Cotterell and Callison-Burch, 2014), and deep learning (Elaraby and Abdul-Mageed, 2018). In this paper, we focus on using state-of-the-arts deep learning architectures to identify Arabic dialects of Twitter users at the country level. We use the MADAR twitter corpus (Bouamo"
W19-4637,P14-2125,0,0.124359,"Missing"
W19-4637,L18-1111,0,0.142442,"Missing"
W19-4637,P11-2007,0,0.106679,"ral works have focused on detecting languages in global settings (see Jauhiainen et al. (2018) for a survey), there has not been extensive research on teasing apart similar languages or language varieties (Zampieri et al., 2018). This is the case for Arabic, the term used to collectively refer to a large number of varieties with a vast population of native speakers (∼ 300 million). For this reason, we focus on detecting fine-grained Arabic dialect as part of our contribution to the MADAR shared task 2, twitter user dialect identification (Bouamor et al., 2019). Previous works on Arabic (e.g., Zaidan and Callison-Burch (2011, 2014); Elfardy and Diab (2013); Cotterell and Callison-Burch (2014)) have primarily targeted cross-country regional varieties such as Egyptian, Gulf, and Levantine, in addition to Modern Standard Arabic (MSA). These To solve Arabic dialect identification, many researchers developed models based on computational linguistics and machine learning (Elfardy and Diab, 2013; Salloum et al., 2014; Cotterell and Callison-Burch, 2014), and deep learning (Elaraby and Abdul-Mageed, 2018). In this paper, we focus on using state-of-the-arts deep learning architectures to identify Arabic dialects of Twitte"
W19-4637,J14-1006,0,0.538447,"Missing"
W19-4637,W18-3901,0,0.0614286,"Missing"
