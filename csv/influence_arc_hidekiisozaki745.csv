2005.iwslt-1.15,J04-4002,0,0.0250193,") roughly means the word stem (inflectional endings). For example, “Would it be possible to ship it to Japan” becomes “Woul+ it be poss+ to ship it to Japa+” by prefix-4, and “+ould it be +ible to ship it to +apan” by suffix-4, where “+” at the end or beginning of a word denotes deletion. Prefix-4 and suffix-4 are likely to contribute to word alignment and language modeling, respectively. 3.2. Phrase-based Features Our system adopts a phrase-based translation model represented by phrase-based features, which are based on phrase translation pairs extracted by the method proposed by Och and Ney [4]. First, many-to-many word alignment is set by using both one-to-many and many-to-one word alignments generated by GIZA++ toolkit. In the experiment, we used prefix-4 for word-to-word alignment. Using prefix-4 produced better translations than the original form in preliminary experiments. Next, phrase pairs consistent with word alignment are extracted. The words in a legal phrase pair are only aligned to each other and not to words outside. Hereafter, we use count(˜ e) and count(f˜, e˜) to denote the number of extracted phrase e˜ and extracted phrase pair ( f˜, e˜), respectively. We used the f"
2005.iwslt-1.15,N03-1017,0,0.0336733,"xtracted source/target phrases # of source/target phrases appearing in the corpus • Phrase pair extraction probability, i.e., # of sentences phrase pairs extracted # of sentences phrase pairs appearing in the corpus • Adjusted Dice coefficient, which is an extension of the measure proposed in [5], i.e., Dice(f˜, e˜)log(count(f˜, e˜) + 1) 3.3. Word-level Features We used the following word-level features, where w(f |e) =  count(f, e) ,  f  count(f , e) I is the number of words in the translation and J is the number of words in the input sentence. • Lexical weight pw (f˜|˜ e) and pw (˜ e|f˜) [6], where pw (f˜|˜ e) = maxa ·  J  1 |{i|(i, j) ∈ a)}| j=1 ∀(i,j)∈a w(fj |ei ) (I˜ + 1)J˜ j w(f˜j |˜ ei ) i • Viterbi IBM Model 1 score p M1 (f˜|˜ e) and pM1 (˜ e|f˜), where pM1 (f˜|˜ e) = J˜  1 (I˜ + 1)J˜ max w(f˜j |˜ ei ) j i • Noisy OR gate pN OR (f˜|˜ e) and pN OR (˜ e|f˜) [7], where e) = pN OR (f˜|˜   (1 − (1 − w(f˜j |˜ ei ))) j i e, f˜) where • Deletion penalty p del (˜ pdel (˜ e, f˜) =  ˜ del(˜ eI1 , f˜j ) j 2 • Phrase extraction probability of source/target, i.e., J˜  I˜  1 ˜ del(˜ eI1 , f˜j ) =   1  0 i does not exist s.t. w(˜ ei |f˜j ) &gt; threshold otherwise. 3.4. Lexical"
2005.iwslt-1.15,N04-1033,0,0.0174663,"posed in [5], i.e., Dice(f˜, e˜)log(count(f˜, e˜) + 1) 3.3. Word-level Features We used the following word-level features, where w(f |e) =  count(f, e) ,  f  count(f , e) I is the number of words in the translation and J is the number of words in the input sentence. • Lexical weight pw (f˜|˜ e) and pw (˜ e|f˜) [6], where pw (f˜|˜ e) = maxa ·  J  1 |{i|(i, j) ∈ a)}| j=1 ∀(i,j)∈a w(fj |ei ) (I˜ + 1)J˜ j w(f˜j |˜ ei ) i • Viterbi IBM Model 1 score p M1 (f˜|˜ e) and pM1 (˜ e|f˜), where pM1 (f˜|˜ e) = J˜  1 (I˜ + 1)J˜ max w(f˜j |˜ ei ) j i • Noisy OR gate pN OR (f˜|˜ e) and pN OR (˜ e|f˜) [7], where e) = pN OR (f˜|˜   (1 − (1 − w(f˜j |˜ ei ))) j i e, f˜) where • Deletion penalty p del (˜ pdel (˜ e, f˜) =  ˜ del(˜ eI1 , f˜j ) j 2 • Phrase extraction probability of source/target, i.e., J˜  I˜  1 ˜ del(˜ eI1 , f˜j ) =   1  0 i does not exist s.t. w(˜ ei |f˜j ) &gt; threshold otherwise. 3.4. Lexical Reordering Features We used the following features to control the reordering of phrases: • Distortion model d(a i − bi−1 ) = exp−|ai −bi−1 −1 |, where ai denotes the starting position of the foreign phrase translated into the i-th English phrase, and b i−1 denotes the end position of"
2005.iwslt-1.15,N04-1021,0,0.050617,"Missing"
2005.iwslt-1.15,W02-1021,0,0.0178786,"on of the foreign phrase translated into the i-th English phrase, and b i−1 denotes the end position of the foreign phrase translated into the (i − 1)-th English phrase [6]. • Right monotone model P R (˜ e, f˜) (and left monotone ˜ model PL (˜ e, f )) inspired by Och’s scheme [8], where PR (f˜, e˜) = countR , count(f˜, e˜) and countR denotes the number of right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the"
2005.iwslt-1.15,P97-1047,0,0.0296466,"umber of right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam se"
2005.iwslt-1.15,J03-1005,0,0.0276819,"r of right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam search"
2005.iwslt-1.15,2003.mtsummit-papers.53,0,0.0219439,"right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam search sta"
2005.iwslt-1.15,C04-1006,0,0.0208502,"s in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam search stage, three kinds of pruning are performed to further reduce the search space [11]. First, observation pruning limits the number of phrase translation candidates to a maximum of N candidates. Second, threshold pruning is performed by computing the most likely partial hypothesis and by discarding hypotheses whose probability is lower than the maximu"
2005.iwslt-1.15,W05-0834,0,0.0109953,"e fly and then integrated with the preceding score for beam pruning. We estimated future cost as described in [13]. Exact costs for the phrase-based features and word level features can be calculated for each extracted phrase pair. For the language model features, their costs were approximated by using only output words contained by each phrase pair. The upper bound of lexical reordering feature costs can be computed beforehand by considering the possible permutations of phrase pairs for a given input. After generating a word graph, it is then pruned using the posterior probabilities of edges [15] to further reduce the number of duplicate translations for A ∗ search. An edge is pruned if its posterior score is lower than the highest posterior score in the graph by a certain amount. 5. Experiments To validate the use of the reportedly effective features, we conducted translation experiments using all features introduced in Section 3. Also, we conducted comparable experiments in both supplied and unrestricted data tracks to study the effectiveness of additional language resources. English data sets IWSLT (supplied) ATR WEB Gigaword Corpus size (words) 190,177 1,100,194 8,482,782 1,799,53"
2005.iwslt-1.15,2004.iwslt-evaluation.13,0,0.0125922,".5 28.6 Chinese IWSLT LDC 56.6 462 56.1 449 50.7 432 Table 4: Input language perplexity of trigram trained by supplied corpora and IWSLT datasets are similar, WEB is closer to IWSLT than Gigaword, and that LDC is very different from IWSLT. Since the collection is enormous in Gigaword, the vocabulary set is first limited to that observed in the English part of supplied corpus and the ATR database. Then for decoding, an actual n-gram language model is estimated on the fly by constraining the vocabulary set to that observed in a given test set. 5.3. Other Setups Following one of the best systems [17] in IWSLT 2004, feature function scaling factors λ j are trained using NIST scores [18] in a loss function of minimum error rate training, and development set 1 (CSTAR) was used for it. For Japanese and Korean, ITG constraints of lexical reordering were applied, and for Arabic and Chinese, simple window size constraints up to 7 were used. 5.4. Results Table 5 summarizes the overall results of the supplied/unrestricted data tracks. The scores of the table are obtained by the comparable conditions for each language pair while some are not the same as those released by the organizer. “m unrestric"
2005.iwslt-1.15,W03-1506,0,\N,Missing
2005.iwslt-1.15,P02-1040,0,\N,Missing
2005.iwslt-1.15,P02-1038,0,\N,Missing
2005.iwslt-1.15,J03-1002,0,\N,Missing
2005.iwslt-1.15,2006.iwslt-evaluation.14,1,\N,Missing
2005.iwslt-1.15,P03-1021,0,\N,Missing
2006.iwslt-evaluation.14,J03-1002,0,0.00515357,"on-terminals are always instantiated with phrase translation pairs. Thus, we will be able to reduce the number of rules induced from a bilingual corpus, which, in turn, help reducing the decoding complexity. Note that we do not imply arbitrary synchronous-CFGs are transformed into the target normalized form. The form simply restricts the grammar extracted from a bilingual corpus explained in Section 2.4. 2.4. Training The phrase extraction algorithm is based on those presented by [3]. First, many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ [6], in both directions and by combining the results based on a heuristic [7]. Second, phrase translation pairs are extracted from the word aligned corpus [3]. This method ) from a senexhaustively extracts phrase pairs (fjj+m , ei+n i J I tence pair (f1 , e1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases [4]: 2.6. Feature Functions Feature functions evaluated during the decoding procedure is summarized as count-based models, lexicon-based models, language model,"
2006.iwslt-evaluation.14,J04-4002,0,0.069757,"we will be able to reduce the number of rules induced from a bilingual corpus, which, in turn, help reducing the decoding complexity. Note that we do not imply arbitrary synchronous-CFGs are transformed into the target normalized form. The form simply restricts the grammar extracted from a bilingual corpus explained in Section 2.4. 2.4. Training The phrase extraction algorithm is based on those presented by [3]. First, many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ [6], in both directions and by combining the results based on a heuristic [7]. Second, phrase translation pairs are extracted from the word aligned corpus [3]. This method ) from a senexhaustively extracts phrase pairs (fjj+m , ei+n i J I tence pair (f1 , e1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases [4]: 2.6. Feature Functions Feature functions evaluated during the decoding procedure is summarized as count-based models, lexicon-based models, language model, reordering models and length-based models. 1. A phrase pair (f¯, e¯) const"
2006.iwslt-evaluation.14,W05-1507,0,0.0304273,"boxed indices indicate non-terminal alignment. One of the major differences to the algorithm presented in [4] is the restriction of the target normalized form in the last step. 2.5. Decoding by Top-down Parsing Decoding is performed by parsing on the source-side and by combining the projected target-side. A conventional method of parsing is a CKY-based method in which ordering is governed by the span-size of the source words [4]. One of the problem is the high computational complexity when integrated with ngram language model of the target-side especially when the ngram’s order is quite high [8]. The complexity lies on the possible “holes” in the target-side. One of the solution is to perform a binarization so that the target-side will not contain holes [9]. We applied an Earley-style top-down parsing approach described in [5] that is similar to [10]. The basic idea is to perform a top-down parsing in order so that the projected target-side is generated in a left-to-right manner. The search is guided with a push-down automaton which keeps track of the span-size of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder effici"
2006.iwslt-evaluation.14,N06-1033,0,0.0183208,"e last step. 2.5. Decoding by Top-down Parsing Decoding is performed by parsing on the source-side and by combining the projected target-side. A conventional method of parsing is a CKY-based method in which ordering is governed by the span-size of the source words [4]. One of the problem is the high computational complexity when integrated with ngram language model of the target-side especially when the ngram’s order is quite high [8]. The complexity lies on the possible “holes” in the target-side. One of the solution is to perform a binarization so that the target-side will not contain holes [9]. We applied an Earley-style top-down parsing approach described in [5] that is similar to [10]. The basic idea is to perform a top-down parsing in order so that the projected target-side is generated in a left-to-right manner. The search is guided with a push-down automaton which keeps track of the span-size of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most-likely translation. Our decoding algorithm can be regarded as an instance of Earley algorithm, but the predicted rule’s “dot” is moved sy"
2006.iwslt-evaluation.14,P02-1038,0,0.663191,"onal phrase-based model. In addition, our reranking algorithm further boosted the performance. = argmax P r(eI1 |f1J ) (1) eI1 = argmax P eI1 e′ I1 ′ P M  I J λ h (e , f ) m m 1 1 m=1 (2) P M J ′I′ exp m=1 λm hm (e 1 , f1 ) exp In this framework, the posterior probability P r(eI1 |f1J ) is directly maximized using a log-linear combination of feature functions hm (eI1 , f1J ), such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likelihood approach [1] or on a direct error minimization approach [2]. This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. 1. Introduction This paper describes the NTT statistical machine translation system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to f"
2006.iwslt-evaluation.14,P03-1021,0,0.0700907,"king algorithm further boosted the performance. = argmax P r(eI1 |f1J ) (1) eI1 = argmax P eI1 e′ I1 ′ P M  I J λ h (e , f ) m m 1 1 m=1 (2) P M J ′I′ exp m=1 λm hm (e 1 , f1 ) exp In this framework, the posterior probability P r(eI1 |f1J ) is directly maximized using a log-linear combination of feature functions hm (eI1 , f1J ), such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likelihood approach [1] or on a direct error minimization approach [2]. This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. 1. Introduction This paper describes the NTT statistical machine translation system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to form the output English sentence. The approach i"
2006.iwslt-evaluation.14,N03-1017,0,0.214202,"s dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likelihood approach [1] or on a direct error minimization approach [2]. This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. 1. Introduction This paper describes the NTT statistical machine translation system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to form the output English sentence. The approach is able to capture phrase-wise local-reordering, or possibly neighboring phrase reordering, but does not account for long-distance reordering of phrases. In the hierarchical phrase-based translation approach [4], translation is constituted by hierarchically combining phrases with the help of non-terminals embedded in phrases themselves. Each non-terminal represented in each phrase can capture reordering of phrases. Based"
2006.iwslt-evaluation.14,P05-1033,0,0.752177,"system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to form the output English sentence. The approach is able to capture phrase-wise local-reordering, or possibly neighboring phrase reordering, but does not account for long-distance reordering of phrases. In the hierarchical phrase-based translation approach [4], translation is constituted by hierarchically combining phrases with the help of non-terminals embedded in phrases themselves. Each non-terminal represented in each phrase can capture reordering of phrases. Based on the hierarchical phrase-based modeling, we adopted the left-to-right target generation method described in [5]. The method is able to generate translations efficiently, first, by simplifying the grammar so that the target-side takes a phrase-prefixed form, namely target normalized form. Our simplified grammar drastically reduces the number of rules extracted from a bilingual corpu"
2006.iwslt-evaluation.14,P06-1098,1,0.921865,"hen, reordered to form the output English sentence. The approach is able to capture phrase-wise local-reordering, or possibly neighboring phrase reordering, but does not account for long-distance reordering of phrases. In the hierarchical phrase-based translation approach [4], translation is constituted by hierarchically combining phrases with the help of non-terminals embedded in phrases themselves. Each non-terminal represented in each phrase can capture reordering of phrases. Based on the hierarchical phrase-based modeling, we adopted the left-to-right target generation method described in [5]. The method is able to generate translations efficiently, first, by simplifying the grammar so that the target-side takes a phrase-prefixed form, namely target normalized form. Our simplified grammar drastically reduces the number of rules extracted from a bilingual corpus empirically presented in [5]. Second, translation is generated in a left-to-right manner, similar to a phrase-based approach, using an Earley-style top-down parsing on the source-side. Coupled with the tarOur system consists of two parts. A hierarchical phrasebased translation system that generates a large n-best list. The"
2006.iwslt-evaluation.14,W06-3119,0,0.0424367,"side and by combining the projected target-side. A conventional method of parsing is a CKY-based method in which ordering is governed by the span-size of the source words [4]. One of the problem is the high computational complexity when integrated with ngram language model of the target-side especially when the ngram’s order is quite high [8]. The complexity lies on the possible “holes” in the target-side. One of the solution is to perform a binarization so that the target-side will not contain holes [9]. We applied an Earley-style top-down parsing approach described in [5] that is similar to [10]. The basic idea is to perform a top-down parsing in order so that the projected target-side is generated in a left-to-right manner. The search is guided with a push-down automaton which keeps track of the span-size of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most-likely translation. Our decoding algorithm can be regarded as an instance of Earley algorithm, but the predicted rule’s “dot” is moved synchronized with the left-to-right ordering of the projected target-side, not the left-to-right"
2006.iwslt-evaluation.14,P96-1041,0,0.0444429,"e hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases [4]: 2.6. Feature Functions Feature functions evaluated during the decoding procedure is summarized as count-based models, lexicon-based models, language model, reordering models and length-based models. 1. A phrase pair (f¯, e¯) constitutes a rule: X → f¯, e¯ 96 2.6.3. Language Model 2.6.1. Count-based Models hφ (f1J |eI1 , D) and hφ (eI1 |f1J , D) estisentences f1J and eI1 over a derivaWe used mixed-cased 5-gram language model estimated with modified Kneser-Ney smoothing [11]: Y pn (ei |ei−4 ei−3 ei−2 ei−1 ) (11) hlm (eI1 ) = log Main feature functions mate the likelihood of two tion tree D. We assume that the production rules in D are independent of each other: Y φ(γ|α) (4) hφ (f1J |eI1 , D) = log i 2.6.4. Reordering Models hγ,αi∈D In order to limit the reorderings, two feature functions are employed: X height(Di ) (12) hheight (eI1 , f1J , D) = φ(γ|α) is estimated through the relative frequency on a given bilingual corpus. count(γ, α) φ(γ|α) = P γ count(γ, α) (5) Di ∈back(D) hwidth (eI1 , f1J , D) = where count(·) represents the cooccurrence frequency of rules γ"
2006.iwslt-evaluation.14,P02-1034,0,0.0145487,"titute a translation: j=1 max t(fj |ei ) &lt; τdel 0≤i≤I  (9) The deletion model simply counts the number of words whose lexicon model probability is lower than a threshold τdel . Likewise, an insertion model is integrated that penalizes the inserted English words that do not account for any foreign words in an input: hins (eI1 , f1J ) = I  X i=1 max t(ei |fj ) &lt; τins 0≤j≤J  (14) hr (D) = rule(D) (15) This section explains our discriminative reranking method which further improves the quality of baseline MT system. Our reranking method basically follows the parse reranking method explained in [12]. We first generate a n-best list of candidate outputs (translations) from a baseline MT system, the hierarchical phrase-based translation described in Section 2. Then, a reranking model is trained by a ranking voted perceptron on a development set. Finally, in the process of decoding, we re-rank the n-best list of test data fed from the baseline MT using the trained reranking model. We adopted the above method, [12], with a BLEU-score-based weight update scheme. The reranking setting of MT is an ordinal regression procedure in each output pairs, similar to the parse reranking task, which can"
2006.iwslt-evaluation.14,takezawa-etal-2002-toward,0,0.0312538,"running GIZA++ in both directions, and by refining word alignment with a heuristic. Third, from three distinctly preprocessed corpora, rules are extracted using the algorithm presented in 2.4. In this step, preprocessed corpora are recovered into their original form. When recovered, punctuation marks on the source-side were removed together with corresponding word alignments. The idea is to induce better word alignments by considering non-punctuation corpus, together with punctuation preserved corpus. 4. Tasks The experiments were carried out on the Basic Travel Expression Corpus (BTEC) task [13]. BTEC is a multilingual corpus in traveling domain which was collected from phrase books for tourists. In the IWSLT 2006 open data track, the subsets of BTEC consists of training set and three development sets (Dev1 through Dev3) indicated in Table 1. Another development set (Dev4) and the final test set were provided in this track1 . The translation pairs set up for the task are: Arabic-to-English, Italian-to-English, Japanese-toEnglish and Chinese-to-English. The task description for the IWSLT 2006 evaluation campaign can be summarized as follows: 1. Spoken language, instead of written text"
2006.iwslt-evaluation.14,W06-3115,1,0.730654,"ey algorithm, but the predicted rule’s “dot” is moved synchronized with the left-to-right ordering of the projected target-side, not the left-to-right ordering on the source-side. The use of target normalized form further simplify the decodig procedure. Since the rule form does not allow any holes for the target-side, the integration with ngram language model is straightforward: the prefixed phrases are simply concatenated and intersected. Our decoder is based on an in-house developed phrasebased decoder which uses a bit vector to represent uncovered foreign word positions for each hypothesis [14]. We basically replaced the bit vector structure to the stack structure: Almost no modification was required for the word graph structure and the beam search strategy implemented for a phrase-based modeling, since the target-side’s prefixed phrases are simply concatenated. The use of a stack structure directly models a synchronous-CFG formalism realized as a push-down automation, while the bit vector implementation is conceptualized as a finite state transducer. (3) where X is a non-terminal, γ is a source-side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target-s"
2006.iwslt-evaluation.14,2005.iwslt-1.8,0,0.0199399,"erings. 5.2. Results on Hierarchical Phrase-based Translation We first compared our baseline hierarchical phrase-based translation against an in-house developed phrase-based translation that performed quite well for the shared task of “Workshop on Statistical Machine Translation” [14]. Table 3 shows the number of phrases and rules extracted from each task. The grammar size for our hierarchical phrase-based system is almost twice as large as the size of the phrase table for our phrase-based system. The phrase-based system employs a lexicalized reordering model to capture phrase-wise reordering [15]. For the hierarchical phrase-based system, spansize for each non-terminal was constrained to 7 for all tasks. Window-size constraints were set to 7 in the phrase-based system. As indicated in Table 4, our hierarchical phrasebased system outperforms the phrase-based system in all tasks. mPER 56.65 54.15 48.13 41.57 55.12 52.17 59.72 57.71 53.70 5.3. Results on Reranking Table 5 shows the reranking results for IWSLT2006. The rows of ‘1-best’ in Table 5 show the performance of our baseline MT system, hierarchical phrase-based system (contrast1 system). Then, the rows of ‘SC’ display the performa"
2007.iwslt-1.16,N07-1008,0,0.0150061,"parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a single k-best list combined with the ASR’s n-best list’s confidence measures. This paper is organized as follows: The overview of our decoder is presented in Section 2. We will describe the feature functions experimented in [3] together with additional features. The reranking system is described in Section 3. The reranker is biased to use a slightly different feature set to avoid over training. Both systems s"
2007.iwslt-1.16,P03-1021,0,0.0149653,"e feature functions experimented in [3] together with additional features. The reranking system is described in Section 3. The reranker is biased to use a slightly different feature set to avoid over training. Both systems share the same online training algorithm, but differ in that the decoder’s parameters are updated based on the dynamically generated candidate list, whereby the reranking training is based on a fixed translation candidate list. Section 4 presents the results for the evaluation campaign of IWSLT 2007. 2. Machine Translation System We use a linear feature combination approach [8] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w⊤ · h(f, e) (1) e where h(f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the ngram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. Under this maximization scenario, our system composed of two steps: The first step is a decoder that can efficiently generate k-best list of ca"
2007.iwslt-1.16,P05-1033,0,0.107755,"by seeking a maximum solution: eˆ = argmax w⊤ · h(f, e) (1) e where h(f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the ngram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. Under this maximization scenario, our system composed of two steps: The first step is a decoder that can efficiently generate k-best list of candidate translations in a left-to-right manner [1] based on the hierarchical phrase-based translation[9]. The second step rerank the k-best list using a reranking voted perceptron[2]. 2.1. Hierarchical Phrase-based Translation We use the hierarchical phrase-based translation approach, in which non-terminals are embedded in each phrase [9]. A translation is generated by hierarchically combining phrases using the non-terminals. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [10]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic str"
2007.iwslt-1.16,N03-1017,0,0.00424325,"te k-best list of candidate translations in a left-to-right manner [1] based on the hierarchical phrase-based translation[9]. The second step rerank the k-best list using a reranking voted perceptron[2]. 2.1. Hierarchical Phrase-based Translation We use the hierarchical phrase-based translation approach, in which non-terminals are embedded in each phrase [9]. A translation is generated by hierarchically combining phrases using the non-terminals. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [10]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Based on hierarchical phrase-based modeling, we adopted the left-to-right target generation method [1] which performed better than a phrase-based system in the last year’s evaluation[2]. This method is able to generate translations efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-t"
2007.iwslt-1.16,P98-2230,0,0.0172839,"s efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target side where ¯b is a string of terminals, or a phrase, and β is a (possibly empty) string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. Second, a translation is generated in a left-to-right manner, similar to phrase-based decoding using Earley-style topdown parsing on the source side [11, 1, 12]. The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the restcost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure, at the expense for expressiveness. Since the rule form does not allow any holes in the target side, the integration with an n-gram language model is"
2007.iwslt-1.16,W06-3119,0,0.0336527,"s efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target side where ¯b is a string of terminals, or a phrase, and β is a (possibly empty) string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. Second, a translation is generated in a left-to-right manner, similar to phrase-based decoding using Earley-style topdown parsing on the source side [11, 1, 12]. The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the restcost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure, at the expense for expressiveness. Since the rule form does not allow any holes in the target side, the integration with an n-gram language model is"
2007.iwslt-1.16,2004.iwslt-evaluation.13,0,0.0146149,"nd intersected with an n-gram. 2.2. Features 2.2.1. Baseline Features The hierarchical phrase-based translation system employs standard real valued value features: • n-gram language model to capture the fluency of the target side. • Hierarchical phrase translation probabilities in both directions, h(γ|¯bβ) and h(¯bβ|γ), estimated by relative counts, count(γ, ¯bβ) [9]. • Word-based lexically weighted models of hlex (γ|¯bβ) and hlex (¯bβ|γ) using lexical translation models[9]. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models [13]. • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling [1]. 2.2.2. Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system [3]. The features are designed with decoding efficiency in mind and are based on the word alignment structure preserved in hierarchical phrase translation pairs [14]. When hierarchical phrases are extracted, the word alignment is preserved. If multiple ei−1 ei f j−1 ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of"
2007.iwslt-1.16,W06-3108,0,0.0191772,"(¯bβ|γ) using lexical translation models[9]. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models [13]. • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling [1]. 2.2.2. Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system [3]. The features are designed with decoding efficiency in mind and are based on the word alignment structure preserved in hierarchical phrase translation pairs [14]. When hierarchical phrases are extracted, the word alignment is preserved. If multiple ei−1 ei f j−1 ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of sparse features for a phrase translation. X1 X2 f j−1 fj f j+1 f j+3 X3 f j+2 Figure 2: Example hierarchical features. word alignments are observed with the same source and target sides, only the most frequently observed word alignment is kept to reduce the grammar size. Using the word alignment structure inside hierarchical phrases, we employs following feature set. • Word pair features directly capture the source/target word co"
2007.iwslt-1.16,P06-1098,1,0.894326,"parse binary features — of the order of millions — are integrated during the search. This paper gives the details of the two steps and shows the results for the Evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2007. 1. Introduction This paper presents NTT Statistical Machine Translation System evaluated in the evaluation campaign of International Workshop on Spoken Language Translation (IWSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both sys"
2007.iwslt-1.16,P04-1007,0,0.0263178,"(ei−1 , fj−1 ), (ei , fj+1 )). • Insertion/deletion features are integrated in which no word alignment is associated in the target/source side. Inserted words are associated with all the words in the source sentence, such as (ei+1 , f1 ), ..., (ei+1 , fJ ) for the non-aligned word ei+1 with the source sentence f1J in Figure 1. In the same way, we use hierarchical phrase-wise deletion features by associating each inserted source word in a phrase to all the target words in the same phrase. • Target bigram features are also included to directly capture the fluency as in the n-gram language model [15], such as (ei−1 , ei ), (ei , ei+1 ), (ei+1 , ei+2 )... in Figure 1. • Hierarchical features capture dependencies the source words in a parent phrase to the source words in child phrases, such as (fj−1 , fj ), (fj−1 , fj+1 ), (fj+3 , fj ), (fj+3 , fj+1 ), (fj , fj+2 ) and (fj+1 , fj+2 ) as indicated by the arrows in Figure 2. The hierarchical features are extracted only for those source words that are aligned with the target side to limit the feature size. Algorithm 1 Online Training Algorithm for decoder T 1: 2: 3: 4: 5: 6: 7: 8: 9: Training data: T = {(f t , et )}t=1 m-best oracles: O = {}Tt"
2007.iwslt-1.16,E99-1010,0,0.0399514,"ed only for those source words that are aligned with the target side to limit the feature size. Algorithm 1 Online Training Algorithm for decoder T 1: 2: 3: 4: 5: 6: 7: 8: 9: Training data: T = {(f t , et )}t=1 m-best oracles: O = {}Tt=1 i=0 for n = 1, ..., N do for t = 1, ..., T do C t ← bestk (f t ; wi ) Ot ← oraclem (Ot ∪ C t ; et ) wi+1 = update wi using C t w.r.t. Ot i=i+1 end for end for PN T i w return i=1 NT In order to achieve the generalization capability, we introduce normalized tokens for each surface form [3]. • Word class/part-of-speech/named entity. Words are clustered by mkcls [16]. The part-of-speech (POS) and named entity (NE) tags are also integrated to capture linguistic characteristics when taggers are available. A unique word class is assigned to each surface form. However, multiple POS/NE are potentially assigned to each surface word. In our approach, we do not disambiguate labels, but simply collect a surface word to multiple tags dictionary. Those tags are integrated by first running a tagger on the training data. Then, a surface form to POS/NE dictionary is generated by collecting all possible tags for each word. • Synsets from WordNet. In order to represent s"
2007.iwslt-1.16,D07-1080,1,0.562399,"ken Language Translation (IWSLT) 2007. 1. Introduction This paper presents NTT Statistical Machine Translation System evaluated in the evaluation campaign of International Workshop on Spoken Language Translation (IWSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that"
2007.iwslt-1.16,P06-1091,0,0.0676827,"nese and Japanese. We consider all possible combination of those token types. For example, an English/Arabic word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its"
2007.iwslt-1.16,P05-1012,0,0.28923,"WSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a s"
2007.iwslt-1.16,P06-2098,0,0.0623452,"sed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a single k-best list combined with"
2007.iwslt-1.16,P06-1096,0,0.119827,"nese and Japanese. We consider all possible combination of those token types. For example, an English/Arabic word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its"
2007.iwslt-1.16,P02-1040,0,0.07649,"rmalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its beam search pruning and OOV. Thus, we cannot always assign scores to each reference translation. Therefore, possible oracle translations are mai"
2007.iwslt-1.16,W04-3201,0,0.0272233,"Liang et at. [18] presented a similar updating strategy in which parameters were updated toward an oracle translation found in C t , but ignored potentially better translations discovered in the past iterations. A new wi+1 is computed using the k-best list C t with respect to the oracle translations Ot (line 5). After N iterations, the algorithm returns an averaged weight vector to avoid overfitting (line 9). When updating parameters in line 5, we use the Margin Infused Relaxed Algorithm (MIRA) [4] which is an online version of the large-margin training algorithm for structured classification [20] that has been successfully used for dependency parsing [5] and joint-labeling/chunking [6]. Line 5 of the weight vector update procedure in Algorithm 1 is reAlgorithm 2 Online Training Algorithm for Reranker placed by the solution of: X 1 ˆ i+1 = argmin ||wi+1 − wi ||2 + C ξ(ˆ e, e′ ) w wi+1 2 ′ eˆ,e subject to si+1 (f t , eˆ) − si+1 (f t , e′ ) + ξ(ˆ e, e′ ) ≥ L(ˆ e , e′ ; et ) ′ ξ(ˆ e, e ) ≥ 0 ∀ˆ e ∈ Ot , ∀e′ ∈ C t (3)  ⊤ where si (f t , e) = wi · h(f t , e). ξ(·) is a non-negative slack variable and C ≥ 0 is a constant to control the influence to the objective function. A larger C implies"
2007.iwslt-1.16,P02-1034,0,0.0176638,"ain the best oracle translations O1T = eˆ1 , ..., eˆT that is treated as a “bed” document. The approximated BLEU for a hypothesized translation e′ for the training instance (f t , et ) is computed over the bed O1T except for eˆt , which is replaced by e′ : BLEU({ˆ e1 , ..., eˆt−1 , e′ , eˆt+1 , ..., eˆT }; E) The loss computed by the approximated BLEU measures the document-wise loss of substituting the correct translation eˆt Our reranking system is basically identical to the system presented in the last year’s IWSLT 2006 evaluation [2] that is based on the parse reranking method explained in [21]. We first generate n-best lists of candidate translations from the decoder, then train reranking model using the development set with additional features by ranking voted perceptron. Finally, during the testing, we rerank the k-best list of test data from the decoder by the parameters for the reranking. A separately trained reranking model is used for the ASR’s nbest list. The reranker selects the best translation out of the merged k · n-best list generated by translating all the sentences in the n-best list. 3.1. Features The reranking system employs a slightly different feature set from the"
2007.iwslt-1.16,takezawa-etal-2002-toward,0,0.068604,"n line 8 is based on an perceptron algorithm with the update amount scaled by the loss function L(·).  wn = wn + L(Rj , Ri ; et ) · h(f t , Rj ) − h(f t , Ri ) (5) As our loss function, we employed the difference of the approximated BLEU in Section 2.4, but used a set of 1-best translations from the decoder as our bed document, instead of oracle translations. The idea is to directly measure the gain or loss by selecting the translation different from the original 1-best translation of the decoder. 4. Evaluation 4.1. Data The major training data comes from IWSLT supplied data, a subset of BTEC[22]. We also used common bilingual data either in the public domain or from the LDC as indicated in Table 1. Additional data for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-Englis"
2007.iwslt-1.16,2005.mtsummit-papers.11,0,0.00566131,"ions from the decoder as our bed document, instead of oracle translations. The idea is to directly measure the gain or loss by selecting the translation different from the original 1-best translation of the decoder. 4. Evaluation 4.1. Data The major training data comes from IWSLT supplied data, a subset of BTEC[22]. We also used common bilingual data either in the public domain or from the LDC as indicated in Table 1. Additional data for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-English 277.24 271.39 51.29 13.45 Japanese-to-English Chinese-to-English 188.49 73.18 cellaneous text data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists large"
2007.iwslt-1.16,P03-1010,0,0.0124565,"ata for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-English 277.24 271.39 51.29 13.45 Japanese-to-English Chinese-to-English 188.49 73.18 cellaneous text data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists larger mismatch with the IWSLT condition, we extracted texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decode"
2007.iwslt-1.16,H05-1059,0,0.0194319,"data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists larger mismatch with the IWSLT condition, we extracted texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decoder, and by a rule-based Brill’s POS tagger for reranking. Arabic data is tokenized by simply isolating Arabic scripts. Italian data is POS tagged by treetagger [26]. Japanese/Chinese texts are POS tagged/NE chunked [27]. After tokenization, we removed all the punctuation marks in the source side of bilingual data and lowercased the texts. The English side of the bilingual data is case/punctuation preserved. 4.2. Task Adaptation As discussed in Section 4.1, we extracted bilingual data from various sources, ranging from in-domain travel related data to out-of-domain news, mi"
2007.iwslt-1.16,W03-1506,0,0.274806,"d texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decoder, and by a rule-based Brill’s POS tagger for reranking. Arabic data is tokenized by simply isolating Arabic scripts. Italian data is POS tagged by treetagger [26]. Japanese/Chinese texts are POS tagged/NE chunked [27]. After tokenization, we removed all the punctuation marks in the source side of bilingual data and lowercased the texts. The English side of the bilingual data is case/punctuation preserved. 4.2. Task Adaptation As discussed in Section 4.1, we extracted bilingual data from various sources, ranging from in-domain travel related data to out-of-domain news, miscellaneous texts and lexicons. Their characteristics are very different from the style in the IWSLT development and test conditions. Table 2 shows the development/test set perplexity of the source side language computed by the trigram of t"
2007.iwslt-1.16,N06-1014,0,0.0286367,"Missing"
2007.iwslt-1.16,2006.iwslt-evaluation.14,1,\N,Missing
2007.iwslt-1.16,C98-2225,0,\N,Missing
2008.iwslt-evaluation.13,D07-1080,1,0.906376,"Missing"
2008.iwslt-evaluation.13,2006.iwslt-evaluation.14,1,0.898254,"ted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSLT 2006 evaluation [3]: • Hierarchical phrase translation probabilities We employs Pegasos1 , a fast optimization algorithm for linear-kernel SVMs. It uses only k samples to calculate subgradient for optimization, so learning time of Pegasos does not depend on the training data size [5]. 3.2. Approximated BLEU We use approximated BLEU [3] to choose the best translation candidates, for optimizing the reranker in terms of BLEU [11]. The approximated BLEU is independently calculated on each translation candidate for each sentence in reranker training data during pre-processing, although original BLEU required document"
2008.iwslt-evaluation.13,2007.iwslt-1.16,1,0.843688,"entences O1T = {e11 , ..., eT1 }, the approximated BLEU on i-best translation candidate for t-th input sentence eti is calculated by substituting et1 with eti , i.e. the BLEU on the sentence set t t+1 T {e11 , ..., et−1 1 , ei , e1 , ..., e1 }. 3.3. Reranker features We use a large number of sparse binary features for reranking, as well as a real-valued feature (decoder score). • Lexical translation probabilities in phrase pairs 3.3.1. Word alignment features • Word-based insertion/deletion penalties We use source-target word pairs extracted by separately running IBM Model 1 in both direction [4]. In addition to source-target word unigram pairs, we used pairs of targetside word bigram and their corresponding source-side words in terms of the word alignment. We also include POS-based features, target-side word surfaces are replaced with their POS tags in the word alignment features above. Target-side (English) POS tags are automatically annotated by Brill Tagger. • Word 5-gram language model scores • Reordering penalties • Length penalties on both words and hierarchical phrases 3. Reranking Component Our reranking component is based on Ranking SVMs [1]. Each decoder k-best translation"
2008.iwslt-evaluation.13,P03-1021,0,0.0231724,"2 - work in the experiments, because they failed to capture useful context information in the current condition. We discuss these features using distinctive examples between reranker selections and decoder 1-bests. This paper is organized as follows: Section 2 brieﬂy describes our SMT decoder. Section 3 describes our reranking component and sparse features for reranking. Section 4 presents the results for the evaluation campaign of IWSLT 2008, followed by discussion in Section 5. 2. Machine Translation Component 2.1. Statistical Machine Translation We use a linear feature combination approach [6] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w&gt; · h(f, e) (1) e where h(f, e) is a feature vector. w is a weight vector that scales the contribution from each feature. Feature weights (i.e. elements of w) are optimized based on minimum error rate training [6]. 2.2. Hierarchical Phrase-based Approach Our SMT component employs the hierarchical phrase-based approach [7], in which the translation model is based on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hie"
2008.iwslt-evaluation.13,J07-2003,0,0.0606697,"on in Section 5. 2. Machine Translation Component 2.1. Statistical Machine Translation We use a linear feature combination approach [6] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w&gt; · h(f, e) (1) e where h(f, e) is a feature vector. w is a weight vector that scales the contribution from each feature. Feature weights (i.e. elements of w) are optimized based on minimum error rate training [6]. 2.2. Hierarchical Phrase-based Approach Our SMT component employs the hierarchical phrase-based approach [7], in which the translation model is based on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hierarchically combining phrases using non-terminals. Each production rule of SCFG takes the following form. X → hγ, α, ∼i (2) In the notation above, X is a non-terminal symbol, γ is a source-side string of terminal and non-terminal symbols, and α is a target-side one. γ and α share the same number of non-terminals whose one-to-one mapping is deﬁned by ∼. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by"
2008.iwslt-evaluation.13,N03-1017,0,0.0076853,"ased on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hierarchically combining phrases using non-terminals. Each production rule of SCFG takes the following form. X → hγ, α, ∼i (2) In the notation above, X is a non-terminal symbol, γ is a source-side string of terminal and non-terminal symbols, and α is a target-side one. γ and α share the same number of non-terminals whose one-to-one mapping is deﬁned by ∼. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by"
2008.iwslt-evaluation.13,J03-1002,0,0.00281422,"hrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSL"
2008.iwslt-evaluation.13,J04-4002,0,0.0283074,"hrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSL"
2008.iwslt-evaluation.13,P02-1040,0,0.0764379,"in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSLT 2006 evaluation [3]: • Hierarchical phrase translation probabilities We employs Pegasos1 , a fast optimization algorithm for linear-kernel SVMs. It uses only k samples to calculate subgradient for optimization, so learning time of Pegasos does not depend on the training data size [5]. 3.2. Approximated BLEU We use approximated BLEU [3] to choose the best translation candidates, for optimizing the reranker in terms of BLEU [11]. The approximated BLEU is independently calculated on each translation candidate for each sentence in reranker training data during pre-processing, although original BLEU required document-wise calculation and is not suitable for sentence-level reranking. Given 1-best translation outputs for T input sentences O1T = {e11 , ..., eT1 }, the approximated BLEU on i-best translation candidate for t-th input sentence eti is calculated by substituting et1 with eti , i.e. the BLEU on the sentence set t t+1 T {e11 , ..., et−1 1 , ei , e1 , ..., e1 }. 3.3. Reranker features We use a large number of spar"
2008.iwslt-evaluation.13,W03-1506,0,0.0710488,"Missing"
2008.iwslt-evaluation.13,C08-1137,0,\N,Missing
2008.iwslt-evaluation.13,2007.iwslt-1.8,0,\N,Missing
2008.iwslt-evaluation.13,P07-1002,0,\N,Missing
2008.iwslt-evaluation.13,W06-3110,0,\N,Missing
2008.iwslt-evaluation.13,P06-1066,0,\N,Missing
2008.iwslt-evaluation.13,P07-2045,0,\N,Missing
2008.iwslt-evaluation.13,N04-1022,0,\N,Missing
2008.iwslt-evaluation.13,2005.iwslt-1.16,0,\N,Missing
2008.iwslt-evaluation.13,W06-3119,0,\N,Missing
2008.iwslt-evaluation.13,I08-1066,0,\N,Missing
2008.iwslt-evaluation.13,2006.iwslt-evaluation.22,0,\N,Missing
2009.iwslt-papers.3,P02-1038,0,0.263208,"tained by our proposed methods is more stable in various conditions than that obtained by MERT. Our experimental results on the FrenchEnglish WMT08 shared task show that degrade of our proposed methods is smaller than that of MERT in case of small training data or out-of-domain test data. 1. Introduction The state of the art statistical machine translation systems have been modeled by the log-linear approach which is a generalization of the noizy-channel approach. This approach has achieved a lot of great advances because it has provided a natural extention to integrate many useful components [1]. To estimate the weights toward these components according to their performance, minimum error rate training (MERT) [2] was introduced by Och (2003). MERT improves statistical machine translation quality by optimizing the parameter of the log-linear function by using such automatic translation evaluation metrics as the BLEU scores [3]. To train a small number of real-valued features used on a standard phrase-based statistical machine translation system like Moses [16], MERT with BLEU-based objective function is very effective due to line-search algorithm proposed by Och (2003). However, MERT"
2009.iwslt-papers.3,P03-1021,0,0.0865125,"on the FrenchEnglish WMT08 shared task show that degrade of our proposed methods is smaller than that of MERT in case of small training data or out-of-domain test data. 1. Introduction The state of the art statistical machine translation systems have been modeled by the log-linear approach which is a generalization of the noizy-channel approach. This approach has achieved a lot of great advances because it has provided a natural extention to integrate many useful components [1]. To estimate the weights toward these components according to their performance, minimum error rate training (MERT) [2] was introduced by Och (2003). MERT improves statistical machine translation quality by optimizing the parameter of the log-linear function by using such automatic translation evaluation metrics as the BLEU scores [3]. To train a small number of real-valued features used on a standard phrase-based statistical machine translation system like Moses [16], MERT with BLEU-based objective function is very effective due to line-search algorithm proposed by Och (2003). However, MERT tends to overfit to training data because its objective function consists of no regularizer. To enhance generalization a"
2009.iwslt-papers.3,2001.mtsummit-papers.68,0,0.120111,"l machine translation systems have been modeled by the log-linear approach which is a generalization of the noizy-channel approach. This approach has achieved a lot of great advances because it has provided a natural extention to integrate many useful components [1]. To estimate the weights toward these components according to their performance, minimum error rate training (MERT) [2] was introduced by Och (2003). MERT improves statistical machine translation quality by optimizing the parameter of the log-linear function by using such automatic translation evaluation metrics as the BLEU scores [3]. To train a small number of real-valued features used on a standard phrase-based statistical machine translation system like Moses [16], MERT with BLEU-based objective function is very effective due to line-search algorithm proposed by Och (2003). However, MERT tends to overfit to training data because its objective function consists of no regularizer. To enhance generalization ability, we would like to use other state-of-the-art machine learning techniques for machine translation. Support vector machines (SVM) have proven to be power† This research was conducted as an internship program 2008"
2009.iwslt-papers.3,P05-1033,0,0.0849286,"Missing"
2009.iwslt-papers.3,P05-1034,0,0.0712134,"Missing"
2009.iwslt-papers.3,W00-1303,0,0.04046,"fective due to line-search algorithm proposed by Och (2003). However, MERT tends to overfit to training data because its objective function consists of no regularizer. To enhance generalization ability, we would like to use other state-of-the-art machine learning techniques for machine translation. Support vector machines (SVM) have proven to be power† This research was conducted as an internship program 2008 of NTT. Watanabe now belongs to National Institute of Information and Communications Technology (NICT) in Japan. ∗ Taro - 144 - ful tools for many tasks in natural language processing [6][7]. The core of the form consists of a smooth convex regularizer such as 12 ||w||2 and the empirical risk term of hinge loss. In this paper we present an approach to optimize the parameter of the log-liner model using the primal form of structural SVM [12]. We expect the convex regularizer or the factor of enlarging the margin (between the reference and the incorrect translation) of SVM to reduce the overfitting problem and enhance generalization ability. Using the BLEU scores to define the hinge loss, our proposed method also inherits the advantages of MERT, which enhance the BLEU scores of tra"
2009.iwslt-papers.3,D07-1080,1,0.888167,"ˆelsi−1 .m−li−1 .m } i−1 .b−li .b xi = { lil.m−l } i−1 .m end if until no more intersections add(I , xi1 ) add(I , max(I ) + 2) xbest = argminx∈I Obj(w )d, Cs1 , {rj , ˆ e∗j , fj }sj=1 ) w+ = (xbest − ) delete(I , max(I ) + 2) end for return w + (x − S e∗s }1 ) − BLEU({rs , ˆ es }1 )}. Q × {BLEU({rs , ˆ 1-slack formulation with margin-rescaling is constructed usS ing the corpus-wise BLEU loss function Δ({ˆ e∗s , ˆ es }1 ): λ argmin w2 + ξ. w,ξ≥0 2 (7) eS ) ∈ CS : s.t. ∀(ˆ e1 , · · · , ˆ S 1 S w, δhs  ≥ Δ({ˆ e∗s , ˆ es }1 ) − ξ. S s=1 Unlike the margin infused relaxed algorithm (MIRA) [9] and S-slack formulation, we can directly apply the corpus-wise BLEU to the SVM objective function without approximating the BLEU scores. 4.3. Optimization Algorithm 4.3.1. Line-search Algorithm Next we describe extended Och’s line-search algorithms for S-slack and 1-slack formulation of the Structural SVM. These pseudocodes for the line-search to optimize parameter w are given by Algorithm 3,4. In Algorithm 3,4 we can find the range of values along the direction vector d to which each candidate translation is assigned the best score. Algorithm 3 is the line-search algorithm for S-slack formul"
2009.iwslt-papers.3,P05-1012,0,0.13106,"Missing"
2009.iwslt-papers.3,P06-2098,0,0.029702,"Missing"
2009.iwslt-papers.3,2005.eamt-1.36,0,0.401502,"ce. In the second subsection we define loss function Δ(rs , ˆ es ) using the BLEU scores, and the third subsection extends Och’s line-search algorithm as the optimization algorithm for the SVM-based objective function. 4.1. Selecting the Configuration in the K-best list For structural SVM, we need to label a correct candidate translation for each source sentence from its K-best list of candidates, which is called a configuration. Since the BLEU scores are not cumulative, we cannot efficiently select the best configuration from the K-best list. So we approximate it by a greedy search algorithm [14]. This algorithm considers the impact on the training set score when selecting an alternative translation by subtracting the statistics for the current configuration choice from the accumulated statistics and adding those for the alternative and selects the translation which results in the highest score. Repeat this process and continue untill there are no configuration changes. The configuration obtained by this algorithm specifies the correct candidate for each K-best list, and the BLEU scores are the upper bound for the BLEU scores on the training set. 4.2. Loss Function for Rescaling 4.2.1"
2009.iwslt-papers.3,2006.iwslt-evaluation.14,1,0.930093,"Missing"
2009.iwslt-papers.3,P07-2045,0,0.0638114,"s approach has achieved a lot of great advances because it has provided a natural extention to integrate many useful components [1]. To estimate the weights toward these components according to their performance, minimum error rate training (MERT) [2] was introduced by Och (2003). MERT improves statistical machine translation quality by optimizing the parameter of the log-linear function by using such automatic translation evaluation metrics as the BLEU scores [3]. To train a small number of real-valued features used on a standard phrase-based statistical machine translation system like Moses [16], MERT with BLEU-based objective function is very effective due to line-search algorithm proposed by Och (2003). However, MERT tends to overfit to training data because its objective function consists of no regularizer. To enhance generalization ability, we would like to use other state-of-the-art machine learning techniques for machine translation. Support vector machines (SVM) have proven to be power† This research was conducted as an internship program 2008 of NTT. Watanabe now belongs to National Institute of Information and Communications Technology (NICT) in Japan. ∗ Taro - 144 - ful too"
2009.iwslt-papers.3,J03-1002,0,0.00413523,"i = argminˆes ∈Cs ˆe∗s { ˆelsi−1 .m−li−1 .m } i−1 .b−li .b xi = { lil.m−l } i−1 .m end if until no more intersections add(I , xi1 ) end for add(I , max(I ) + 2) xbest = argminx∈I Obj(w )d, CS1 , {rs , ˆ e∗s , fs }S1 ) return w + (xbest − )d + (x − • Three orientation types reordering model[17]: p(m|f , e) , p(s|f , e) , p(d|f , e) to capture the lexicalized information. • Word , Phrase penalty: To control the target length and the average length of the phrases. In this paper, we trained these small number of features and phrases were extracted using a typical approach [16] that ran GIZA++ [18]. We used a Katz smoothing 5-gram language model that was created using the SRILM toolkit [19]. 5.2. Data Set For experiments we used the French-English data provided for the Europarl-based WMT08 shared task. Europarl corpus was collected from the proceedings of European Parliament [20]. This training corpus contains about 1.3 M sentences. Parameters were tuned over the provided development set (dev2006) that consisted of 2000 sentences with one reference. We used two open test sets: Europarl test 2008, consisting of 2000 sentences with one reference, and News newstest 2008 (out-of-domain), co"
2009.iwslt-papers.3,2005.mtsummit-papers.11,0,0.00816147,"g model[17]: p(m|f , e) , p(s|f , e) , p(d|f , e) to capture the lexicalized information. • Word , Phrase penalty: To control the target length and the average length of the phrases. In this paper, we trained these small number of features and phrases were extracted using a typical approach [16] that ran GIZA++ [18]. We used a Katz smoothing 5-gram language model that was created using the SRILM toolkit [19]. 5.2. Data Set For experiments we used the French-English data provided for the Europarl-based WMT08 shared task. Europarl corpus was collected from the proceedings of European Parliament [20]. This training corpus contains about 1.3 M sentences. Parameters were tuned over the provided development set (dev2006) that consisted of 2000 sentences with one reference. We used two open test sets: Europarl test 2008, consisting of 2000 sentences with one reference, and News newstest 2008 (out-of-domain), consisting of 1500 sentences. Table 1 shows these contents in more detail. Table 1: The Data statistics For the 1-slack formulation we should calculate slopes m and intercepts b the same as the S-slack formulation, but, to avoid bias toward the line-search procedure by sentencewise BLEU,"
2009.iwslt-papers.3,N09-1025,0,0.107757,"Missing"
2009.iwslt-papers.3,W08-0304,0,0.108941,"Missing"
2009.iwslt-papers.3,D08-1076,0,0.0713832,"Missing"
2009.iwslt-papers.3,C08-1074,0,0.0530567,"Missing"
2009.iwslt-papers.3,J93-2003,0,0.0179364,"Missing"
2009.iwslt-papers.3,P02-1040,0,\N,Missing
2009.iwslt-papers.3,2005.iwslt-1.8,0,\N,Missing
2012.iwslt-keynotes.1,W12-4207,0,\N,Missing
C02-1053,P01-1041,1,0.897899,"Missing"
C02-1053,W00-1303,1,0.435633,"t summarization. Aone et al. (1998) and Kupiec et al. (1995) employed Bayesian classiﬁers, Mani et al. (1998), Nomoto et al. (1997), Lin (1999), and Okumura et al. (1999) used decision tree learning. However, most machine learning methods overﬁt the training data when many features are given. Therefore, we need to select features carefully. Support Vector Machines (SVMs) (Vapnik, 1995) is robust even when the number of features is large. Therefore, SVMs have shown good performance for text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2001), and dependency structure analysis (Kudo and Matsumoto, 2000). In this paper, we present an important sentence extraction technique based on SVMs. We veriﬁed the technique against the Text Summarization Challenge (TSC) (Fukushima and Okumura, 2001) corpus. 2 Important Sentence Extraction based on Support Vector Machines 2.1 Support Vector Machines (SVMs) SVM is a supervised learning algorithm for 2class problems. Training data is given by (x1 , y1 ), · · · , (xu , yu ), xj ∈ Rn , yj ∈ {+1, −1}. Here, xj is a feature vector of the j-th sample; yj is its class label, positive(+1) or negative(−1). SVM separates positive and negative examples by a hyperplan"
C02-1053,N01-1025,1,0.881428,"learning has attracted attention in the ﬁeld of automatic text summarization. Aone et al. (1998) and Kupiec et al. (1995) employed Bayesian classiﬁers, Mani et al. (1998), Nomoto et al. (1997), Lin (1999), and Okumura et al. (1999) used decision tree learning. However, most machine learning methods overﬁt the training data when many features are given. Therefore, we need to select features carefully. Support Vector Machines (SVMs) (Vapnik, 1995) is robust even when the number of features is large. Therefore, SVMs have shown good performance for text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2001), and dependency structure analysis (Kudo and Matsumoto, 2000). In this paper, we present an important sentence extraction technique based on SVMs. We veriﬁed the technique against the Text Summarization Challenge (TSC) (Fukushima and Okumura, 2001) corpus. 2 Important Sentence Extraction based on Support Vector Machines 2.1 Support Vector Machines (SVMs) SVM is a supervised learning algorithm for 2class problems. Training data is given by (x1 , y1 ), · · · , (xu , yu ), xj ∈ Rn , yj ∈ {+1, −1}. Here, xj is a feature vector of the j-th sample; yj is its class label, positive(+1) or negative(−1"
C02-1053,C00-2167,0,0.0456631,"Missing"
C02-1053,C96-2166,0,0.0378212,"r Machines Important sentence extraction can be regarded as a two-class problem: important or unimportant. However, the proportion of important sentences in training data will diﬀer from that in the test data. The number of important sentences in a document is determined by a summarization rate that is given at run-time. A simple solution for this problem is to rank sentences in a document. We use g(x) the distance from the hyperplane to x to rank the sentences. 2.3 Features We deﬁne the boolean features discussed below that are associated with sentence Si by taking past studies into account (Zechner, 1996; Nobata et al., 2001; Hirao et al., 2001; Nomoto and Matsumoto, 1997). We use 410 boolean variables for each Si . Where x = (x[1], · · ·, x[410]). A real-valued feature normalized between 0 and 1 is represented by 10 boolean variables. Each variable corresponds to an internal [i/10,(i + 1)/10) where i = 0 to 9. For example, Posd = 0.75 is represented by “0000000100” because 0.75 belongs to [7/10,8/10). Position of sentences We deﬁne three feature functions for the position of Si . First, Lead is a boolean that corresponds to the output of the lead-based method described below1 . Second, Posd"
C02-1053,P98-1009,0,\N,Missing
C02-1053,C98-1009,0,\N,Missing
C02-1054,W98-1120,0,\N,Missing
C02-1054,C00-2167,0,\N,Missing
C02-1054,P00-1042,0,\N,Missing
C02-1054,P01-1041,1,\N,Missing
C02-1054,N01-1025,0,\N,Missing
C04-1040,A00-2018,0,0.0187229,"th a telescope a He saw a girl with a telescope. Figure 1: A word dependency tree ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002). Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM is relatively robust. In Natural Language Processing, we use tens of thousands of words as features. Therefore, SVM often gives good performance. However, the accuracy of Yamada’s analyzer is lower than state-of-the-art phrase structure parsers such as Charniak’s Maximum-Entropy-Inspired Parser (MEIP) (Charniak, 2000) and Collins’ Model 3 parser. One reason is the lack of top-down information that is available in phrase structure parsers. In this paper, we show that the accuracy of the word dependency parser can be improved by adding a base-NP chunker, a Root-Node Finder, and a Prepositional Phrase (PP) Attachment Resolver. We introduce the base-NP chunker because base NPs are important components of a sentence and can be easily annotated. Since most words are contained in a base NP or are adjacent to a base NP, we expect that the introduction of base NPs will improve accuracy. We introduce the Root-Node F"
C04-1040,P02-1034,0,0.03485,"Missing"
C04-1040,P97-1003,0,0.0190629,"method based on an ‘IOB2’ chunking scheme. By the chunking, each word is tagged as – B: Beginning of a base NP, – I: Other elements of a base NP. – O: Otherwise. Please see Kudo’s paper for more details. • A Root-Node Finder (RNF): We will describe this later. • A Dependency Analyzer: It works just like Yamada’s Dependency Analyzer. • A PP-Attatchment Resolver (PPAR): This resolver improves the dependency accuracy of prepositions whose part-of-speech tags are IN or TO. The above procedures require a part-of-speech tagger. Here, we extract part-of-speech tags from the Collins parser’s output (Collins, 1997) for section 23 instead of reinventing a tagger. According to the document, it is the output of Ratnaparkhi’s tagger (Ratnaparkhi, 1996). Figure 2 shows the architecture of the system. PPAR’s output is used to rewrite the output of the Dependency Analyzer. 2.1 Finding root nodes When we use SVM, we regard root-node finding as a classification task: Root nodes are positive examples and other words are negative examples. For this classification, each word w i in a tagged sentence T = (w1 /p1 , . . . , wi /pi , . . . , wN /pN ) is characterized by a set of features. Since the given POS tags are s"
C04-1040,C96-1058,0,0.033018,"state-of-the-art phrase structure parsers because of the lack of top-down information given by phrase labels. This paper shows that the dependency analyzer can be improved by introducing a Root-Node Finder and a Prepositional-Phrase Attachment Resolver. Experimental results show that these modules based on Preference Learning give better scores than Collins’ Model 3 parser for these subproblems. We expect this method is also applicable to phrase structure parsers. 1 Introduction 1.1 Dependency Analysis Word dependency is important in parsing technology. Figure 1 shows a word dependency tree. Eisner (1996) proposed probabilistic models of dependency parsing. Collins (1999) used dependency analysis for phrase structure parsing. It is also studied by other researchers (Sleator and Temperley, 1991; Hockenmaier and Steedman, 2002). However, statistical dependency analysis of English sentences without phrase labels is not studied very much while phrase structure parsing is intensively studied. Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsu"
C04-1040,P02-1043,0,0.0136622,"and a Prepositional-Phrase Attachment Resolver. Experimental results show that these modules based on Preference Learning give better scores than Collins’ Model 3 parser for these subproblems. We expect this method is also applicable to phrase structure parsers. 1 Introduction 1.1 Dependency Analysis Word dependency is important in parsing technology. Figure 1 shows a word dependency tree. Eisner (1996) proposed probabilistic models of dependency parsing. Collins (1999) used dependency analysis for phrase structure parsing. It is also studied by other researchers (Sleator and Temperley, 1991; Hockenmaier and Steedman, 2002). However, statistical dependency analysis of English sentences without phrase labels is not studied very much while phrase structure parsing is intensively studied. Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM). They did not use phrase labels by considering annotation of documents in expert domains. SVM (Vapnik, 1995) has shown g"
C04-1040,C02-1054,1,0.892014,"that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM). They did not use phrase labels by considering annotation of documents in expert domains. SVM (Vapnik, 1995) has shown good performance in difHe girl with a telescope a He saw a girl with a telescope. Figure 1: A word dependency tree ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002). Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM is relatively robust. In Natural Language Processing, we use tens of thousands of words as features. Therefore, SVM often gives good performance. However, the accuracy of Yamada’s analyzer is lower than state-of-the-art phrase structure parsers such as Charniak’s Maximum-Entropy-Inspired Parser (MEIP) (Charniak, 2000) and Collins’ Model 3 parser. One reason is the lack of top-down information that is available in phrase structure parsers. In this paper, we show that the accurac"
C04-1040,N01-1025,0,0.173353,"udied. Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM). They did not use phrase labels by considering annotation of documents in expert domains. SVM (Vapnik, 1995) has shown good performance in difHe girl with a telescope a He saw a girl with a telescope. Figure 1: A word dependency tree ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002). Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM is relatively robust. In Natural Language Processing, we use tens of thousands of words as features. Therefore, SVM often gives good performance. However, the accuracy of Yamada’s analyzer is lower than state-of-the-art phrase structure parsers such as Charniak’s Maximum-Entropy-Inspired Parser (MEIP) (Charniak, 2000) and Collins’ Model 3 parser. One reason is the lack of top-down information that is available in phrase structure parsers. In this pape"
C04-1040,W02-2016,0,0.170951,"Missing"
C04-1040,J93-2004,0,0.0295554,"Missing"
C04-1040,W03-0402,0,0.0507802,"Missing"
C04-1040,P03-1029,0,0.0272678,"1 shows a word dependency tree. Eisner (1996) proposed probabilistic models of dependency parsing. Collins (1999) used dependency analysis for phrase structure parsing. It is also studied by other researchers (Sleator and Temperley, 1991; Hockenmaier and Steedman, 2002). However, statistical dependency analysis of English sentences without phrase labels is not studied very much while phrase structure parsing is intensively studied. Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM). They did not use phrase labels by considering annotation of documents in expert domains. SVM (Vapnik, 1995) has shown good performance in difHe girl with a telescope a He saw a girl with a telescope. Figure 1: A word dependency tree ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002). Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM is relatively robust"
C04-1040,P03-1005,1,0.81524,"g technology. Figure 1 shows a word dependency tree. Eisner (1996) proposed probabilistic models of dependency parsing. Collins (1999) used dependency analysis for phrase structure parsing. It is also studied by other researchers (Sleator and Temperley, 1991; Hockenmaier and Steedman, 2002). However, statistical dependency analysis of English sentences without phrase labels is not studied very much while phrase structure parsing is intensively studied. Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM). They did not use phrase labels by considering annotation of documents in expert domains. SVM (Vapnik, 1995) has shown good performance in difHe girl with a telescope a He saw a girl with a telescope. Figure 1: A word dependency tree ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002). Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM"
C04-1040,W96-0213,0,\N,Missing
C04-1040,J03-4003,0,\N,Missing
C04-1064,W99-0625,0,\N,Missing
C04-1064,J97-1003,0,\N,Missing
C04-1064,W02-2016,0,\N,Missing
C04-1064,W03-1004,0,\N,Missing
C04-1064,P02-1040,0,\N,Missing
C04-1064,W03-0507,0,\N,Missing
C04-1064,P03-1005,1,\N,Missing
D07-1080,P05-1012,0,0.310578,"ptimized toward a set of good translations found in the k-best list across iterations. The objective function is an approximated BLEU (Watanabe et al., 2006a) that scales the loss of a sentence BLEU to a documentwise loss. The parameters are trained using the 764 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 764–773, Prague, June 2007. 2007 Association for Computational Linguistics Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006). MIRA is successfully employed in dependency parsing (McDonald et al., 2005) or the joint-labeling/chunking task (Shimizu and Haas, 2006). Experiments were carried out on an Arabicto-English translation task, and we achieved significant improvements over conventional minimum error training with a small number of features. This paper is organized as follows: First, Section 2 introduces the framework of statistical machine translation. As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al., 2006b) originally proposed by Chiang (2005). In Section 3, a set of binary sparse features are defined"
D07-1080,J04-4002,0,0.0244586,"achine translation by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant"
D07-1080,2004.iwslt-evaluation.13,0,0.0101531,"concatenated and intersected with n-gram. 3 Features 3.1 Baseline Features The hierarchical phrase-based translation system employs standard numeric value features: • n-gram language model to capture the fluency of the target side. • Hierarchical phrase translation probabilities in ¯ and h(bβ|γ), ¯ both directions, h(γ|bβ) estimated ¯ by relative counts, count(γ, bβ). • Word-based lexically weighted models of ¯ and hlex (bβ|γ) ¯ hlex (γ|bβ) using lexical translation models. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models (Bender et al., 2004). • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling (Watanabe et al., 2006b). 3.2 Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system. We may use any binary features, such as   English word “violate” and Arabic    1 word “tnthk” appeared in e and f . h( f, e) =     0 otherwise. The features are designed by considering the decoding efficiency and are based on the word alignment structure preserved in hierarchical phrase translation pai"
D07-1080,P05-1033,0,0.854373,"ber of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a conventional s"
D07-1080,P06-1121,0,0.0405344,"training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a conventional small feature set. Bangalore et al. (2006) trained"
D07-1080,N03-1017,0,0.244563,"by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a"
D07-1080,P03-1021,0,0.59948,"on 2 introduces the framework of statistical machine translation. As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al., 2006b) originally proposed by Chiang (2005). In Section 3, a set of binary sparse features are defined including numeric features for our baseline system. Section 4 introduces an online large-margin training algorithm using MIRA with our key components. The experiments are presented in Section 5 followed by discussion in Section 6. 2 Statistical Machine Translation We use a log-linear approach (Och, 2003) in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax wT · h( f, e) (1) e where h( f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the n-gram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. 2.1 Hierarchical Phrase-based SMT Chiang (2005) introduced the hierarchical phrasebased translation approach, in which non-terminals are emb"
D07-1080,P02-1040,0,0.107918,"ed as “@@@@/@/@@”. We consider all possible combination of those token types. For example, the word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. 4 Online Large-Margin Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms (Tillmann and Zhang, 2006; Liang et al., 2006) in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, i.e. BLEU (Papineni et al., 2002). In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of ( f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot is updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its beam search pruning and OOV. Thus, we cannot always assign scores for each reference translation. Therefore, possible oracle translations are ma"
D07-1080,P04-1007,0,0.0559747,"of phrase translation pairs, but it is trivial to define the features over hierarchical phrases. X1 X2 f j−1 fj f j+3 X3 f j+1 f j+2 Figure 2: Example hierarchical features. same way, we will be able to include deletion features where a non-aligned source word is associated with the target sentence. However, this would lead to complex decoding in which all the translated words are memorized for each hypothesis, and thus not integrated in our feature set. 3.2.3 Target Bigram Features Target side bigram features are also included to directly capture the fluency as in the n-gram language model (Roark et al., 2004). For instance, bigram features of (ei−1 , ei ), (ei , ei+1 ), (ei+1 , ei+2 )... are observed in Figure 1. 3.2.4 Hierarchical Features In addition to the phrase motivated features, we included features inspired by the hierarchical structure. Figure 2 shows an example of hierarchical phrases in the source side, consisting of X 1 → E E D D D E f j−1 X 2 f j+3 , X 2 → f j f j+1 X 3 and X 3 → f j+2 . Hierarchical features capture the dependency of the source words in a parent phrase to the source words in child phrases, such as ( f j−1 , f j ), ( f j−1 , f j+1 ), ( f j+3 , f j ), ( f j+3 , f j+1 )"
D07-1080,P06-2098,0,0.147558,"st list across iterations. The objective function is an approximated BLEU (Watanabe et al., 2006a) that scales the loss of a sentence BLEU to a documentwise loss. The parameters are trained using the 764 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 764–773, Prague, June 2007. 2007 Association for Computational Linguistics Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006). MIRA is successfully employed in dependency parsing (McDonald et al., 2005) or the joint-labeling/chunking task (Shimizu and Haas, 2006). Experiments were carried out on an Arabicto-English translation task, and we achieved significant improvements over conventional minimum error training with a small number of features. This paper is organized as follows: First, Section 2 introduces the framework of statistical machine translation. As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al., 2006b) originally proposed by Chiang (2005). In Section 3, a set of binary sparse features are defined including numeric features for our baseline system. Section"
D07-1080,W04-3201,0,0.02528,"nslation are created, which amount to m × k large-margin constraints. In this online training, only active features constrained by Eq. 3 are kept and updated, unlike offline training in which all possible features have to be extracted and selected in advance. The Lagrange dual form of Eq. 3 is:   1 X maxα(·)≥0 − || α(ˆe, e′ ) h( f t , eˆ ) − h( f t , e′ ) ||2 2 eˆ ,e′ X + α(ˆe, e′ )L(ˆe, e′ ; et ) eˆ,e′ Margin Infused Relaxed Algorithm The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) is an online version of the large-margin training algorithm for structured classification (Taskar et al., 2004) that has been successfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006). The basic idea is to keep the norm of the updates to the weight vector as small as possible, considering a margin at least as large as the loss of the incorrect classification. Line 5 of the weight vector update procedure in Algorithm 1 is replaced by the solution of: X ˆ i+1 = argmin ||wi+1 − wi ||+ C ξ(ˆe, e′ ) w wi+1 eˆ ,e′ subject to si+1 ( f t , eˆ ) − si+1 ( f t , e′ ) + ξ(ˆe, e′ ) ≥ L(ˆe, e′ ; et ) ξ(ˆe, e′ ) ≥ 0 ∀ˆe ∈ Ot , ∀e′ ∈ Ct (3) n oT where si ( f"
D07-1080,P06-1091,0,0.438964,"an 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a conventional small feature set. Bangalore et al. (2006) trained the lexical choice model by using Conditional Random Fields (CRF) realized on a WFST. Their modeling was reduced to M"
D07-1080,P06-1098,1,0.329931,"oice model by using Conditional Random Fields (CRF) realized on a WFST. Their modeling was reduced to Maximum Entropy Markov Model (MEMM) to handle a large number of features which, in turn, faced the labeling bias problem (Lafferty et al., 2001). Tillmann and Zhang (2006) trained their feature set using an online discriminative algorithm. Since the decoding is still expensive, their online training approach is approximated by enlarging a merged kbest list one-by-one with a 1-best output. Liang et al. (2006) introduced an averaged perceptron algorithm, but employed only 1-best translation. In Watanabe et al. (2006a), binary features were trained only on a small development set using a variant of voted perceptron for reranking k-best translations. Thus, the improvement is merely relative to the baseline translation system, namely whether or not there is a good translation in their k-best. We present a method to estimate a large number of parameters — of the order of millions — using an online training algorithm. Although it was intuitively considered to be prone to overfitting, training on a small development set — less than 1K sentences — was sufficient to achieve improved performance. In this method,"
D07-1080,P98-2230,0,0.0117577,"string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. The use of phrase b¯ as a prefix maintains the strength of the phrasebase framework. A contiguous English side with a (possibly) discontiguous foreign language side preserves phrase-bounded local word reordering. At the same time, the target normalized framework still combines phrases hierarchically in a restricted manner. 2.3 Left-to-Right Target Generation Decoding is performed by parsing on the source side and by combining the projected target side. We applied an Earley-style top-down parsing approach (Wu and Wong, 1998; Watanabe et al., 2006b; Zollmann and Venugopal, 2006). The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure. Since the rule form does not allow any holes for the target side, the integration with an n-g"
D07-1080,W06-3108,0,0.164848,"Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling (Watanabe et al., 2006b). 3.2 Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system. We may use any binary features, such as   English word “violate” and Arabic    1 word “tnthk” appeared in e and f . h( f, e) =     0 otherwise. The features are designed by considering the decoding efficiency and are based on the word alignment structure preserved in hierarchical phrase translation pairs (Zens and Ney, 2006). When hierarchical phrases are extracted, the word alignment is preserved. If multiple word alignments are observed 766 ei−1 f j−1 ei ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of sparse features for a phrase translation. with the same source and target sides, only the frequently observed word alignment is kept to reduce the grammar size. 3.2.1 Word Pair Features Word pair features reflect the word correspondence in a hierarchical phrase. Figure 1 illustrates an example of sparse features for a phrase translation pair f j , ..., f j+2 and ei , ..., ei+3 1 . From the word al"
D07-1080,W06-3119,0,0.0385186,"one mapping between non-terminals in γ and β. The use of phrase b¯ as a prefix maintains the strength of the phrasebase framework. A contiguous English side with a (possibly) discontiguous foreign language side preserves phrase-bounded local word reordering. At the same time, the target normalized framework still combines phrases hierarchically in a restricted manner. 2.3 Left-to-Right Target Generation Decoding is performed by parsing on the source side and by combining the projected target side. We applied an Earley-style top-down parsing approach (Wu and Wong, 1998; Watanabe et al., 2006b; Zollmann and Venugopal, 2006). The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure. Since the rule form does not allow any holes for the target side, the integration with an n-gram language model is straightforward: the prefixed phr"
D07-1080,P06-1096,0,\N,Missing
D07-1080,J03-1002,0,\N,Missing
D07-1080,2006.iwslt-evaluation.14,1,\N,Missing
D07-1080,C98-2225,0,\N,Missing
D07-1083,W03-0423,0,0.0156617,"Missing"
D07-1083,W03-0425,0,0.0243821,"Missing"
D07-1083,P06-1027,0,0.1027,"ediction, which is not practical as regards the large data sets used for standard sequence labeling tasks in NLP. Another discriminative approach to semi-supervised SOL involves the incorporation of an entropy regularizer (Grand791 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 791–800, Prague, June 2007. 2007 Association for Computational Linguistics valet and Bengio, 2004). Semi-supervised conditional random fields (CRFs) based on a minimum entropy regularizer (SS-CRF-MER) have been proposed in (Jiao et al., 2006). With this approach, the parameter is estimated to maximize the likelihood of labeled data and the negative conditional entropy of unlabeled data. Therefore, the structured predictor is trained to separate unlabeled data well under the entropy criterion by parameter estimation. In contrast to these previous studies, this paper proposes a semi-supervised SOL framework based on a hybrid generative and discriminative approach. A hybrid approach was first proposed in a supervised learning setting (Raina et al., 2003) for text classification. (Fujino et al., 2005) have developed a semi-supervised"
D07-1083,N01-1025,0,0.229147,"Missing"
D07-1083,N03-1028,0,0.0610035,"bialgorithm for evaluating unseen samples, as well as that of standard CRFs, without any additional cost. 4 Experiments We examined our hybrid model (HySOL) by applying it to two sequence labeling tasks, named entity recognition (NER) and syntactic chunking (Chunking). We used the same Chunking and ‘English’ NER data as those used for the shared tasks of CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000) and CoNLL-2003 (Tjong Kim Sang and Meulder, 2003), respectively. For the baseline method, we performed a conditional random field (CRF), which is exactly the same training procedure described in (Sha and Pereira, 2003) with L-BFGS. Moreover, LOP-CRF (Smith et al., 2005) is also compared with our hybrid model, since the formalism of our hybrid model can be seen as an extension of LOP-CRFs as described in Section 3. For CRF, we used the Gaussian prior as the second term on the RHS in Equation (1), where δ 2 represents the hyper-parameter in the Gaussian prior. In contrast, for LOP-CRF and HySOL, we used the Dirichlet priors as the second term on the λ1 f(words ), f(lwords ), f(poss ), f(wtypes ), f(poss−1 , poss ), f(wtypes−1 , wtypes ), f(poss , poss+1 ), f(wtypes , wtypes+1 ), f(pref1s ), f(pref2s ), f(pref"
D07-1083,P05-1003,0,0.0828816,"which is equivalent to γj = 0 for all j, we obtain essentially the same objective function as that of the LOP-CRFs. Thus, our framework can also be seen as an extension of LOP-CRFs that enables us to incorporate unlabeled data. θys−1 ,ys θys ,xs , s=1 where θys−1 ,ys and θys ,xs represent the transition probability between states ys−1 and ys and the symbol emission probability of the s-th position of the corresponding input sequence, respectively, where θyS+1 ,xS+1 = 1. It can be seen that the formalization in the loglinear combination of our hybrid model is very similar to that of LOP-CRFs (Smith et al., 2005). In fact, if we only use a combination of discriminative 793 X n log R(y n |xn ; Λ, Θ, Γ)+log p(Γ). (3) where p(Γ) is a prior probability distribution of Γ. The value of Γ providing a global maximum of LHySOL (Γ|Θ) is guaranteed under an arbitrary fixed value in the Θ domain, since LHySOL (Γ|Θ) is a concave function of Γ. Thus, we can easily maximize Equation (3) by using a gradient-based optimization algorithm such as (bound constrained) L-BFGS (Liu and Nocedal, 1989). 3.2 Incorporating Unlabeled Data We cannot directly incorporate unlabeled data for discriminative training such as Equation"
D07-1083,N06-1012,0,0.081288,"ntences in training, development and test data, respectively, with four named entity tags, PERSON, LOCATION, ORGANIZATION and MISC, plus the ‘O’ tag. The unlabeled data consists of 17,003,926 words from 1,029,122 sentences. These data sets are exactly the same as those provided for the shared task of CoNLL-2003. We slightly extended the feature set of the supplied data by adding feature types such as ‘word type’, and word prefix and suffix. Examples of ‘word type’ include whether the word is capitalized, contains digit or contains punctuation, which basically follows the baseline features of (Sutton et al., 2006) without regular expressions. Note that, unlike several previous studies, we did not employ additional information from external resources such as gazetteers. All our features can be automatically extracted from the supplied data. For LOP-CRF and HySOL, we used four base discriminative models trained by CRFs with different feature sets. Table 1 shows the feature sets we used for training these models. The design of these feature sets was derived from a suggestion in (Smith et al., 2005), which exhibited the best performance in the several feature division. Note that the CRF for the comparison"
D07-1083,P06-1028,1,0.829929,"nsed to use the TREC corpus including WSJ unlabeled data that they used for their Chunking experiments (training and test data for Chunking is derived from WSJ). Therefore, we simply used the supplied unlabeled data of the CoNLL-2003 shared task for both NER and Chunking. If we consider the advantage of our approach, our hybrid model incorporating generative models seems rather intuitive, since it is sometimes difficult to find out a design of effective auxiliary problems for the target problem. Interestingly, the additional information obtained HySOL (ξ 0 =0.1,η 0 =0.0001) + w/ F-score opt. (Suzuki et al., 2006) + unlabeled data (17M → 27M words) + supplied gazetters + add dev. set for estimating Γ Fβ=1 87.20 88.02 88.41 88.90 89.27 (gain) (+0.82) (+0.39) (+0.49) (+0.37) Table 7: The HySOL performance with the Fscore optimization technique and some additional resources in NER (CoNLL-2003) experiments Fβ=1 (gain) HySOL (ξ 0 =0.1,η 0 =0.0001) 94.30 + w/ F-score opt. (Suzuki et al., 2006) 94.36 (+0.06) Table 8: The HySOL performance with the F-score optimization technique on Chunking (CoNLL-2000) experiments from unlabeled data appear different from each other. ASO-semi uses unlabeled data for construct"
D07-1083,W00-0726,0,0.0784923,"ation is independent of the number of models used in the hybrid model. In addition, after training, we can easily merge all the parameter values in a single parameter vector. This means that we can simply employ the Viterbialgorithm for evaluating unseen samples, as well as that of standard CRFs, without any additional cost. 4 Experiments We examined our hybrid model (HySOL) by applying it to two sequence labeling tasks, named entity recognition (NER) and syntactic chunking (Chunking). We used the same Chunking and ‘English’ NER data as those used for the shared tasks of CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000) and CoNLL-2003 (Tjong Kim Sang and Meulder, 2003), respectively. For the baseline method, we performed a conditional random field (CRF), which is exactly the same training procedure described in (Sha and Pereira, 2003) with L-BFGS. Moreover, LOP-CRF (Smith et al., 2005) is also compared with our hybrid model, since the formalism of our hybrid model can be seen as an extension of LOP-CRFs as described in Section 3. For CRF, we used the Gaussian prior as the second term on the RHS in Equation (1), where δ 2 represents the hyper-parameter in the Gaussian prior. In contrast, for LOP-CRF and HySOL"
D07-1083,W03-0419,0,0.0373196,"Missing"
D07-1083,P05-1001,0,\N,Missing
D09-1058,D07-1101,1,0.74923,"dency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank). Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al. (2008)’s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approach that inThis paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning app"
D09-1058,P99-1065,1,0.699382,"at the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3 , and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008). We will refer to this as the “cluster-based feature set” (CL). The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown in Table 1 were used to construct the hierarchical clusterings used within the approach. Note that when this feature set is used within the SSSCM approach, the same set of unlabeled data is used to both induce the clusters, and to estimate the genera"
D09-1058,C96-1058,0,0.336642,"describes how the parameters θj,a are trained on unlabeled data. Given parameters θj,a , we can simply define the functions q1 . . . qk to be log probabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent feat"
D09-1058,D07-1015,1,0.813809,". qk to be log probabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η −"
D09-1058,P08-1068,1,0.098982,"for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank). Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al. (2008)’s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approac"
D09-1058,E06-1011,0,0.589255,".mit.edu Abstract supervised methods for dependency parsing includes (Smith and Eisner, 2007; Koo et al., 2008; Wang et al., 2008). In particular, Koo et al. (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension"
D09-1058,W07-2216,0,0.0658637,"enerative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η − 1) + Pdj ˆj,a a=1 r . (6) This corresponds to a MA"
D09-1058,P05-1012,0,0.451267,"d l is the label of the dependency. We use h = 0 for the root of the sentence. We assume access to a set of labeled training examples, {xi , yi }N i=1 , and in addition a set of unlabeled examples, {x0i }M i=1 . In conditional log-linear models for dependency parsing (which are closely related to conditional random fields (Lafferty et al., 2001)), a distribution over dependency structures for a sentence x is defined as follows: 2.2 X (1) w · f (x, h, m, l) (h,m,l)∈y Here f (x, h, m, l) is a feature vector representing the dependency (h, m, l) in the context of the sentence x (see for example (McDonald et al., 2005a)). In this paper we extend the definition of g(x, y) to include features that are induced from unlabeled data. Specifically, we define g(x, y) = X The Generative Models We now describe how the generative models q1 . . . qk are defined, and how they are induced from unlabeled data. These models make direct use of the feature-vector definition f (x, y) used in the original, fully supervised, dependency parser. The first step is to partition the d features in f (x, y) into k separate feature vectors, r1 (x, y) . . . rk (x, y) (with the result that f is the concatenation of the k feature vectors"
D09-1058,H05-1066,0,0.116818,"Missing"
D09-1058,W06-1615,0,0.174516,"Missing"
D09-1058,J92-4003,0,0.385752,"parameters (w1 , v1 , q1 ). Note that it is possible to iterate the method—steps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008)—but in our experiments we only performed these steps once. 3 Second-order Parsing Models Extensions 3.1 Incorporating Cluster-Based Features Koo et al. (2008) describe a semi-supervised approach that incorporates cluster-based features, and that gives competitive results on dependency parsing benchmarks. The method is a two-stage approach. First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992). Second, a new feature set is constructed by representing words by bit-strings of various lengths, corresponding to clusters at different levels of the hierarchy. These features are combined with conventional features based on words and part-of-speech 1 We used a slightly modified version of 1-best MIRA, whose difference can be found in the third line in Eq. 7, namely, including L(yi , y). 554 (a) English dependency parsing Data set (WSJ Sec. IDs) # of sentences # of tokens Training (02–21) 39,832 950,028 Development (22) 1,700 40,117 Test (23) 2,012 47,377 Unlabeled 1,796,379 43,380,315 Corp"
D09-1058,W06-2920,0,0.027869,"Missing"
D09-1058,W96-0213,0,0.382393,"ther created through random sampling or by using a predefined subset of document IDs from the labeled training data. mately 4,000 times larger than the size of labeled training data. 4.2 Features 4.2.1 Baseline Features In general we will assume that the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3 , and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008). We will refer to this as the “cluster-based feature set” (CL). The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown"
D09-1058,D07-1070,0,0.0431948,"Missing"
D09-1058,D07-1014,0,0.0889514,"obabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η − 1) + Pdj ˆj,a a=1 r . ("
D09-1058,P08-1076,1,0.72351,"upervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more spe"
D09-1058,P08-1061,0,0.0411284,"Missing"
D09-1058,W03-3023,0,0.20564,"ent, test data (labeled data sets) and unlabeled data used in our experiments parameter-estimation method for the second-order parsing model. In particular, we perform the following optimizations on each update t = 1, ..., T for re-estimating w and v: min ||w(t+1) − w(t) ||+ ||v(t+1) − v(t) || ˆ ) ≥ L(yi , y ˆ) s.t. S(xi , yi ) − S(xi , y ˆ = arg maxy S(xi , y) + L(yi , y), y as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008). The English dependencyparsing data sets were constructed using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to dependency tree representations. We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for test. The Czech data sets were obtained from the predefined training/development/test partition in the PDT. The unlabeled data for English was derived from the Brown Laboratory for Linguistic Information Processing (BLLIP) Corpus (LDC2000T43)2 , giving a total of 1,796,379 sentences and 43,380,315 tokens. The raw text section of the PDT was used for Czech, giving 2,349,224 sentences and 39,336,"
D09-1058,J93-2004,0,\N,Missing
D09-1058,D07-1096,0,\N,Missing
D10-1092,W05-0909,0,0.242163,"Missing"
D10-1092,W10-1749,0,0.0898134,"on of the square root to NKT imply that Chinese word order is close to that of English, and they have to measure subtle word order mistakes. 951 Table 3: NTCIR-7 √ meta-evaluation: Effects of square root (b(x) = 1 − 1 − x) √ NKT NKT b(NKT) Spearman w/ adequacy 0.940 0.940 0.922 Pearson w/ adequacy 0.922 0.817 0.941 Spearman w/ fluency 0.887 0.865 0.858 Pearson w/ fluency 0.931 0.917 0.833 In spite of these differences, the two groups independently recognized the usefulness of rank correlations for automatic evaluation of translation quality for distant language pairs. In their WMT-2010 paper (Birch and Osborne, 2010), they multiplied NKT with the brevity penalty and interpolated it with BLEU for the WMT-2010 shared task. This fact implies that incomprehensible or misleading word order mistakes are rare in translation among European languages. 6 Conclusions When Statistical Machine Translation is applied to distant language pairs such as Japanese and English, word order becomes an important problem. SMT systems often fail to find an appropriate translation because of a large search space. Therefore, they often output misleading or incomprehensible sentences such as “A because B” vs. “B because A.” To penal"
D10-1092,E06-1032,0,0.0197708,"a-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an auto"
D10-1092,W07-0718,0,0.0467588,"f sentence-level scores. We used default settings for conventional metrics, but we tuned GTM (Melamed et al., 2007) with -e option. This option controls preferences on longer word runs. We also used the paraphrase database TERp (http://www.umiacs.umd. 949 edu/˜snover/terp) for METEOR (Banerjee and Lavie, 2005). 3.2 Meta-evaluation with WMT-07 data We developed our metric mainly for automatic evaluation of translation quality for distant language pairs such as Japanese-English, but we also want to know how well the metric works for similar language pairs. Therefore, we also use the WMT07 data (Callison-Burch et al., 2007) that covers only European language pairs. Callison-Burch et al. (2007) tried different human evaluation methods and showed detailed evaluation scores. The Europarl test set has 2,000 sentences, and The News Commentary test set has 2,007 sentences. This data has different language pairs: Spanish, French, German ⇒ English. We exclude CzechEnglish because there were so few systems (See the footnote of p. 146 in their paper). 4 4.1 Results Meta-evaluation with NTCIR-7 data Table 1 shows the main results of this paper. The left part has corpus-level meta-evaluation with adequacy. Error metrics, WE"
D10-1092,I05-2014,0,0.0369692,"l by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers (http://chasenlegacy.sourceforge.jp/) following Fujii et al. (2008) In JE translation, most Statistical Machine Translation (SMT) systems translate the Japanese sentence (J0) kare wa sono hon wo yonda node sekaishi ni kyoumi ga atta which means 944 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944–952, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Associat"
D10-1092,2007.mtsummit-papers.21,0,0.0340785,"eni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers"
D10-1092,W10-1736,1,0.720649,") was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks in the reference with those in the hypothesis. There are two popular rank correlation coefficients: Spearman’s ρ and Kendall’s τ (Kendall, 1975). In Isozaki et al. (2010), we used Kendall’s τ to measure the effectiveness of our Head Finalization rule as a preprocessor for English-to-Japanese translation, but we measured the quality of translation by using conventional metrics. It is not clear how well τ works as an automatic evaluation metric of translation quality. Moreover, Spearman’s ρ might work better than Kendall’s τ . As we discuss later, τ considers only the direction of the rank change, whereas ρ considers the distance of the change. The first objective of this paper is to examine which is the better metric for distant language pairs. The second objec"
D10-1092,N03-1020,0,0.0120162,"tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundar"
D10-1092,P02-1040,0,0.100666,"ese metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and A"
D10-1092,2006.amta-papers.25,0,0.103847,"evity Penalty (BP) min(1, exp(1 − r/h)), where r is the length of the reference and h is the length of the hypothesis. BLEU = BP × (p1 p2 p3 p4 )1/4 . Its range is [0, 1]. The BLEU score of (H0) with reference (R0) is 1.0×(11/11×9/10×6/9×4/8)1/4 = 0.740. Therefore, BLEU gives a very good score to this inadequate translation because it checks only ngrams and does not regard global word order. Since (R0) and (H0) look similar in terms of fluency, adequacy is more important than fluency in the translation between distant language pairs. Similarly, other popular scores such as NIST, PER, and TER (Snover et al., 2006) also give relatively good scores to this translation. NIST also considers only local word orders (n-grams). PER (Position-Independent Word Error Rate) was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks"
D10-1092,N03-2021,0,\N,Missing
H05-1019,W04-1003,0,0.0188577,"or topics 7-12, 13-18 and 25-30 on Ó  , Ó , and Ó .  respectively. Note that each human subject, A to E, was a retired professional journalist; that is, they shared a common background. Table 3 shows the Pearson’s correlation coefficient ( ) and Spearman’s rank correlation coefficient Ö for the human subjects. The results show that every pair has a high correlation. Therefore, changing the human subject has little influence as regards creating references and evaluating system summaries. The evaluation by human subjects is stable. This result agrees with DUC’s additional evaluation results (Harman and Over, 2004). However, the behavior of the correlations between humans with different backgrounds is uncertain. The correlation might be fragile if we introduce a human subject whose background is different from the others. 4.3 Compared Automatic Evaluation Methods We compared our method with ROUGE-N and ROUGE-L described below. We used only content words to calculate the ROUGE scores because the correlation coefficient decreased if we did not remove functional words. WSK-based method We use WSK instead of ESK in equation (6)-(8).   ;=&gt;Z1ADC ;=&gt;Z1A (16)  Here, function SU returns the number of"
H05-1019,C04-1077,1,0.811115,"luations require a huge effort and the cost is considerable. Moreover, we cannot automatically evaluate a new system even if we use the corpora built for these workshops, and we cannot conduct reevaluation experiments. To cope with this situation, there is a particular need to establish a high quality automatic evaluation method. Once this is done, we can expect great progress to be made on natural language generation. In this paper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more r"
H05-1019,C04-1064,1,0.847492,"luations require a huge effort and the cost is considerable. Moreover, we cannot automatically evaluate a new system even if we use the corpora built for these workshops, and we cannot conduct reevaluation experiments. To cope with this situation, there is a particular need to establish a high quality automatic evaluation method. Once this is done, we can expect great progress to be made on natural language generation. In this paper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more r"
H05-1019,N03-1020,0,0.306123,"eneration. In this paper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust. 2 Related Work Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes. One is the longest common subsequence (LCS) based approach (Hori et al., 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The other is the N-gram based approach (Papineni et al., 145 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natu"
H05-1019,P04-1077,0,0.382558,"hod for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust. 2 Related Work Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes. One is the longest common subsequence (LCS) based approach (Hori et al., 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The other is the N-gram based approach (Papineni et al., 145 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 145–152, Vancouver, Octo"
H05-1019,W04-1013,0,0.385578,"aper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust. 2 Related Work Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes. One is the longest common subsequence (LCS) based approach (Hori et al., 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The other is the N-gram based approach (Papineni et al., 145 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Languag"
H05-1019,P02-1040,0,0.0948615,"eam 1 0 a-DREAM cosmonaut-my great is my Becoming-DREAM Becoming-SPACEMAN Becoming-a Becoming-ambition 2 Becoming-an Becoming-astronaut Becoming-cosmonaut Becoming-dream Becoming-great 1 1 1 0 1 1     1 0 0 0 0  1   0  0  0 a-SPACEMAN 2 a-cosmonaut a-dream a-great a-is a-my an-DREAM an-SPACEMAN an-ambition an-astronaut an-is an-my 2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b; Soricut and Brill, 2004). Hori et. al (2003) proposed an automatic evaluation method for speech summarization based on word recognition accuracy. They reported that their method is superior to BLEU (Papineni et al., 2002) in terms of the correlation between human assessment and automatic evaluation. Lin (2004a; 2004b) and Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. They applied ROUGE-L to the evaluation of summarization and machine translation. The results showed that the LCS-based measure is comparable to Ngram-based automatic evaluation methods. However, these methods tend to be strongly influenced by word order. Various N-gram-based methods have been proposed since BLEU, which is now widely used for the evaluation of machine translation. Lin et al. (2003) proposed a"
H05-1019,P04-1078,0,0.0189371,"AN-dream   0 cosmonaut-DREAM   0 an 0 1 SPACEMAN-great cosmonaut-dream  0   0 1 astronaut 0 1 SPACEMAN-is 1 1 cosmonaut-great  0  cosmonaut 1 0 SPACEMAN-my cosmonaut-is 1 0    0  0 dream 1 0 a-DREAM cosmonaut-my great is my Becoming-DREAM Becoming-SPACEMAN Becoming-a Becoming-ambition 2 Becoming-an Becoming-astronaut Becoming-cosmonaut Becoming-dream Becoming-great 1 1 1 0 1 1     1 0 0 0 0  1   0  0  0 a-SPACEMAN 2 a-cosmonaut a-dream a-great a-is a-my an-DREAM an-SPACEMAN an-ambition an-astronaut an-is an-my 2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b; Soricut and Brill, 2004). Hori et. al (2003) proposed an automatic evaluation method for speech summarization based on word recognition accuracy. They reported that their method is superior to BLEU (Papineni et al., 2002) in terms of the correlation between human assessment and automatic evaluation. Lin (2004a; 2004b) and Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. They applied ROUGE-L to the evaluation of summarization and machine translation. The results showed that the LCS-based measure is comparable to Ngram-based automatic evaluation methods. However, these methods tend"
I08-1055,P98-1013,0,0.00595273,"urn to the existing similarity metrics and dictionaries to derive features that would be useful for why-QA. To train a ranker, we create a corpus of why-questions and answers and adopt one of the machine learning algorithms for ranking. The following sections describe the three types of features, the corpus creation, and the ranker training. The actual instances of the features, the corpus, and the ranker will be presented in Section 4. 3.1 Causal Expression Features With the increasing attention paid to SRL, we currently have a number of corpora, such as PropBank (Palmer, 2005) and FrameNet (Baker et al., 1998), that are tagged with semantic relations including a causal relation. Since text spans for such relations are annotated in the corpora, we can simply collect the spans marked by a causal relation as causal expressions. Since an answer candidate that has a matching expression for one of the collected causal expressions is likely to be expressing a cause as well, we can make the existence of each expression a feature. Although the collected causal expressions without any modification might be used to create features, for generality, it would be better to abstract them into syntactic patterns. F"
I08-1055,W03-1210,0,0.668216,"sality. The ranker is trained to maximize the QA performance with regards to a corpus of why-questions and answers, automatically tuning the weights of the features. This paper is organized as follows: Section 2 describes previous work on why-QA, and Section 3 describes our approach. Section 4 describes the implementation of our approach, and Section 5 presents the evaluation results. Section 6 summarizes and mentions future work. 2 Previous Work Although systems that can answer why-questions are emerging, they tend to have limitations in that they can answer questions only with causal verbs (Girju, 2003), in specific domains (Khoo et al., 418 2000), or questions covered by a specific knowledge base (Curtis et al., 2005). Recently, Verberne (2006; 2007a) has been intensively working on whyQA based on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). However, her approach requires manually annotated corpora with RST relations. When we look for fully implemented systems for generic “why X?” questions, we only find a small number of such systems. Since why-QA would be a challenging task when tackled straightforwardly, requiring common-sense knowledge and semantic interpretation of"
I08-1055,C02-1053,1,0.806673,"data. However, the performance gains begin to decrease relatively early, possibly indicating the limitation of our approach. Since our approach heavily relies on surface patterns, the use of syntactic and semantic features may be necessary. 200 300 400 500 600 700 800 900 Number of training samples Figure 3: Learning curve: Performance changes when answering Q1–Q100 with different sizes of training samples. Paragraphs are used as answer candidates. their quality itself may be to blame. Furthermore, analyzing the trained ranking models allows us to calculate the weights given to the features (Hirao et al., 2002). Table 3 shows the weights of the top-10 features. We also include in the table the weights of the Synonym Pair, MANCausal Expression and Cause Effect Pair features so that the role of all three types of features in our approach can be shown. The analyzed model was the one trained with all 1,000 questions in the WHYQA collection with paragraphs as answers. Just as suggested by Table 2, the Question-Candidate Cosine Similarity feature plays a key role, followed by automatically collected causal expression features. Figure 2 shows the distribution of the ranks of the first correct answers for a"
I08-1055,W05-0306,0,0.123026,"to et al., 2007). However, their performance is much lower (Mori et al., 2007) than that of factoid QA systems (Fukumoto et al., 2004; Voorhees and Dang, 2005). We consider that this low performance is due to the great amount of hand-crafting involved in the 1 http://research.nii.ac.jp/ntcir/ntcir-ws6/ws-en.html systems. Currently, most of the systems rely on hand-crafted patterns to extract and evaluate answer candidates (Fukumoto et al., 2007). Such patterns include typical cue phrases and POS-tag sequences related to causality, such as “because of” and “by reason of.” However, as noted in (Inui and Okumura, 2005), causes are expressed in various forms, and it is difficult to cover all such expressions by hand. Hand-crafting is also very costly. Some patterns may be more indicative of causes than others. Therefore, it may be useful to assign different weights to the patterns for better answer candidate extraction, but currently this must be done by hand (Mori et al., 2007). It is not clear whether the weights determined by hand are suitable. In this paper, we propose a corpus-based approach for why-QA in order to reduce this hand-crafting effort. We automatically collect causal expressions from corpora"
I08-1055,P00-1043,0,0.194898,"Missing"
I08-1055,W05-0628,0,0.0154759,"Missing"
I08-1055,J05-1004,0,0.0246144,"two types of features, we turn to the existing similarity metrics and dictionaries to derive features that would be useful for why-QA. To train a ranker, we create a corpus of why-questions and answers and adopt one of the machine learning algorithms for ranking. The following sections describe the three types of features, the corpus creation, and the ranker training. The actual instances of the features, the corpus, and the ranker will be presented in Section 4. 3.1 Causal Expression Features With the increasing attention paid to SRL, we currently have a number of corpora, such as PropBank (Palmer, 2005) and FrameNet (Baker et al., 1998), that are tagged with semantic relations including a causal relation. Since text spans for such relations are annotated in the corpora, we can simply collect the spans marked by a causal relation as causal expressions. Since an answer candidate that has a matching expression for one of the collected causal expressions is likely to be expressing a cause as well, we can make the existence of each expression a feature. Although the collected causal expressions without any modification might be used to create features, for generality, it would be better to abstra"
I08-1055,N04-3012,0,0.0410046,"that encode their relevance to the question, we can use this score or simply the rank of the retrieved document as a feature. A question and an answer candidate may be semantically expressing the same content with different expressions. The simplest case is when synonyms are used to describe the same content; e.g., when “arrest” is used instead of “apprehend.” For such cases, we can exploit existing thesauri. We can create a feature encoding whether synonyms of words in the question are found in the answer candidate. We could also use the value of semantic similarity and relatedness measures (Pedersen et al., 2004) or the existence of hypernym or hyponym relations as features. 3.3 Causal Relation Features There are semantic lexicons where a semantic relation between concepts is indicated. For example, the EDR dictionary3 shows whether a causal relation holds between two concepts; e.g., between “murder” and “arrest.” Using such dictionaries, we can create pairs of expressions, one indicating a cause and the other its effect. If we find an expression for a cause in the answer candidate and that for an effect in the question, it is likely that they hold a causal relation. Therefore, we can create a feature"
I08-1055,E06-3005,0,0.1239,"ora tagged with semantic relations. From the collected expressions, features are created to train an answer candidate ranker that maximizes the QA performance with regards to the corpus of why-questions and answers. NAZEQA, a Japanese why-QA system based on our approach, clearly outperforms a baseline that uses hand-crafted patterns with a Mean Reciprocal Rank (top-5) of 0.305, making it presumably the best-performing fully implemented why-QA system. 1 Introduction Following the trend of non-factoid QA, we are seeing the emergence of work on why-QA; e.g., answering generic “why X?” questions (Verberne, 2006). However, since why-QA is an inherently difficult problem, there have only been a small number of fully implemented systems dedicated to solving it. Recent systems at NTCIR-61 Question Answering Challenge (QAC-4) can handle why-questions (Fukumoto et al., 2007). However, their performance is much lower (Mori et al., 2007) than that of factoid QA systems (Fukumoto et al., 2004; Voorhees and Dang, 2005). We consider that this low performance is due to the great amount of hand-crafting involved in the 1 http://research.nii.ac.jp/ntcir/ntcir-ws6/ws-en.html systems. Currently, most of the systems"
I08-1055,C98-1013,0,\N,Missing
I08-2116,H05-1087,0,0.392191,"on binary classification. With this approach, we assume the independence of categories and design a binary classifier for each category that determines whether or not to assign a category label to data samples. Statistical classifiers such as the logistic regression model (LRM), the support vector machine (SVM), and naive Bayes are employed as binary classifiers (Joachims, 1998). In text categorization, the F1 -score is often used to evaluate classifier performance. Recently, methods for training binary classifiers to maximize the F1 -score have been proposed for SVM (Joachims, 2005) and LRM (Jansche, 2005). It was confirmed experimentally that these training methods were more effective for obtaining binary classifiers with better F1 -score performance than the minimum error rate and maximum likelihood used for training conventional classifiers, especially when there was a large imbalance between positive and negative samples. In multi-label categorization, macroand micro-averaged F1 -scores are often used to evaluate classification performance. Therefore, we can expect to improve multi-label classification performance by using binary classifiers trained to maximize the F1 -score. On the other h"
P01-1041,C00-2102,0,0.408393,"Missing"
P01-1041,C00-2167,0,0.151992,"Missing"
P01-1041,W98-1120,0,0.028368,"Missing"
P01-1041,W00-1301,0,0.0501777,"Missing"
P01-1041,P00-1042,0,0.454602,"Missing"
P01-1041,utsuro-sassano-2000-minimally,0,0.026024,"Missing"
P01-1041,C96-1072,0,\N,Missing
P01-1041,W99-0613,0,\N,Missing
P01-1041,M95-1013,0,\N,Missing
P03-2028,C02-1169,0,\N,Missing
P04-1016,W02-2016,0,0.0299937,"dded into an original kernel calculation process by using sub-structure mining algorithms. Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method. 1 Introduction Over the past few years, many machine learning methods have been successfully applied to tasks in natural language processing (NLP). Especially, state-of-the-art performance can be achieved with kernel methods, such as Support Vector Machine (Cortes and Vapnik, 1995). Examples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kern"
P04-1016,P03-1004,0,0.0485189,"can be embedded in an original kernel calculation process, which allows us to use the same calculation procedure as the conventional methods. The only difference between the original sequence kernels and the proposed method is that the latter calculates a statistical metric χ2 (u) by using a sub-structure mining algorithm in the kernel calculation. Third, although the kernel calculation, which unifies our proposed method, requires a longer training time because of the feature selection, the selected sub-sequences have a TRIE data structure. This means a fast calculation technique proposed in (Kudo and Matsumoto, 2003) can be simply applied to our method, which yields classification very quickly. In the classification part, the features (subsequences) selected in the learning part must be known. Therefore, we store the TRIE of selected sub-sequences and use them during classification. 5 Proposed Method Applied to Other Convolution Kernels We have insufficient space to discuss this subject in detail in relation to other convolution kernels. However, our proposals can be easily applied to tree kernels (Collins and Duffy, 2001) by using string encoding for trees. We enumerate nodes (labels) of tree in postorde"
P04-1016,C02-1150,0,0.0611917,"Missing"
P04-1016,P03-1005,1,0.935231,"ples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kernels are related to convolution kernels (Haussler, 1999), which provides the concept of kernels over discrete structures. Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object. That is, convolution kernels are well suited to NLP tasks in terms of both accuracy and concept. Unfortunately, experiments have shown that in some cases there is a critical issue with convolution kernels, especially in NLP tasks (Collins and Duffy, 2001; Cancedda et al., 2003;"
P05-1024,A00-2018,0,0.0491847,"was used as test data. As a baseline model, we used a shallow parser based on Conditional Random Fields (CRFs), very similar to that described in (Sha and Pereira, 2003). CRFs have shown remarkable results in a number of tagging and chunking tasks in NLP. n-best outputs were obtained by a combination of forward 194 Table 1: Results for section 23 of the WSJ Treebank LR/LP = labeled recall/precision. CBs is the average number of cross brackets per sentence. 0 CBs, and 2CBs are the percentage of sentences with 0 or ≤ 2 crossing brackets, respectively. COL99 = Model 2 of (Collins, 1999). CH00 = (Charniak, 2000), CO00=(Collins, 2000). CO02=(Collins and Duffy, 2002). Viterbi search and backward A* search. Note that this search algorithm yields optimal n-best results in terms of the CRFs score. Each sentence has at most 20 distinct parses. The log probability from the CRFs shallow parser was incorporated into the reranking. Following (Collins, 2000), the training set was split into 5 portions, and the CRFs shallow parser was trained on 4/5 of the data, then used to decode the remaining 1/5. The outputs of the base parser, which consist of base phrases, were converted into right-branching trees by assum"
P05-1024,P02-1034,0,0.816258,"and how it selects a small and relevant feature set efficiently. Two experiments on parse reranking show that our method achieves comparable or even better performance than kernel methods and also improves the testing efficiency. 1 Introduction formance. However, they are highly task dependent and require careful design to create the optimal feature set for each task. Kernel methods offer an elegant solution to these problems. They can work on a potentially huge or even infinite number of features without a loss of generalization. The best known kernel for modeling a tree is the tree kernel (Collins and Duffy, 2002), which argues that a feature vector is implicitly composed of the counts of subtrees. Although kernel methods are general and can cover almost all useful features, the set of subtrees that is used is extremely redundant. The main question addressed in this paper concerns whether it is possible to achieve a comparable or even better accuracy using just a small and non-redundant set of subtrees. In this paper, we present a new application of boosting for parse reranking. While tree kernel implicitly uses the all-subtrees representation, our boosting algorithm uses it explicitly. Although this s"
P05-1024,N01-1025,1,0.654592,"f base phrases, were converted into right-branching trees by assuming that two adjacent base phrases are in a parent-child relationship. Figure 4 shows an example of the tree for shallow parsing task. We also put two virtual nodes, left/right boundaries, to capture local transitions. The size parameter s and frequency parameter f were experimentally set at 6 and 5, respectively. Table 2 lists results on test data for the baseline CRFs parser, for several previous studies, and for our best model. Our model achieves a 94.12 Fmeasure, and outperforms the baseline CRFs parser and the SVMs parser (Kudo and Matsumoto, 2001). (Zhang et al., 2002) reported a higher F-measure with a generalized winnow using additional linguistic features. The accuracy of our model is very similar to that of (Zhang et al., 2002) without using such additional features. Table 3 shows the results for our best model per chunk type. TOP ADJP ADVP CONJP INTJ LST NP PP PRT SBAR VP Overall NP PRP (L) I VP (R) VBD (L) saw (R) NP DT (L) NN EOS a girl (R) Figure 4: Tree representation for shallow parsing Represented in a right-branching tree with two virtual nodes MODEL CRFs (baseline) 8 SVMs-voting (Kudo and Matsumoto, 2001) RW + linguistic f"
P05-1024,W04-3239,1,0.841064,"n each boosting iteration, we have to solve the following optimization problem: kˆ = argmax gain(tk ), k=1,...,m q ¯q ¯ ¯ ¯ where gain(tk ) = ¯ Wk+ − Wk− ¯. 192 It is non-trivial to find the optimal tree tkˆ that maximizes gain(tk ), since the number of subtrees is exponential to its size. In fact, the problem is known to be NP-hard (Yang, 2004). However, in real applications, the problem is manageable, since the maximum number of subtrees is usually bounded by a constant. To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004) 4.1 Efficient Enumeration of Trees Abe and Zaki independently proposed an efficient method, rightmost-extension, for enumerating all subtrees from a given tree (Abe et al., 2002; Zaki, 2002). First, the algorithm starts with a set of trees consisting of single nodes, and then expands a given tree of size (n−1) by attaching a new node to it to obtain trees of size n. However, it would be inefficient to expand nodes at arbitrary positions of the tree, as duplicated enumeration is inevitable. The algorithm, rightmost extension, avoids such duplicated enumerations by restricting the position of a"
P05-1024,N03-1028,0,0.046633,"Missing"
P05-1024,W00-0726,0,0.0242374,"Missing"
P05-1024,J03-4003,0,\N,Missing
P05-1024,P01-1010,0,\N,Missing
P05-1024,P02-1062,0,\N,Missing
P06-1028,N01-1025,0,0.386988,"γ2 ZD 1 ZD + , (γ 2 +1)·ZN 2 ZD , if δ(s∗j ) = 1 otherwise where ZN and ZD represent the numerator and denominator of Eq. 16, respectively. In the optimization process of the segmentation F-score objective function, we can efficiently calculate Eq. 15 by using the forward and backward Viterbi algorithm, which is almost the same as calculating Eq. 3 with a variant of the forwardbackward algorithm (Sha and Pereira, 2003). The same numerical optimization methods described in Sec. 3.3 can be employed for this optimization. 5 5.2 Features As regards the basic feature set for Chunking, we followed (Kudo and Matsumoto, 2001), which is the same feature set that provided the best result in CoNLL-2000. We expanded the basic features by using bigram combinations of the same types of features, such as words and part-of-speech tags, within window size 5. In contrast to the above, we used the original feature set for NER. We used features derived only from the data provided by CoNLL-2003 with the addition of character-level regular expressions of uppercases [A-Z], lowercases [a-z], digits [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] or others, and prefixes and suffixes of one to four letters. We also expanded the above basic features"
P06-1028,W03-0430,0,0.0241101,"et al., 2001) for representing a conditional model p(y|x), where both a set of inputs, x, and a set of outputs, y, display non-trivial interdependency. CRFs are basically defined as a discriminative model of Markov random fields conditioned on inputs (observations) x. Unlike generative models, CRFs model only the output y’s distribution over x. This allows CRFs to use flexible features such as complicated functions of multiple observations. The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing (Sha and Pereira, 2003) and information extraction (McCallum and Li, 2003). Since the introduction of CRFs, intensive research has been undertaken to boost their effectiveness. The first approach to estimating CRF parameters is the maximum likelihood (ML) criterion over conditional probability p(y|x) itself (Lafferty et al., 2001). The ML criterion, however, 217 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 217–224, c Sydney, July 2006. 2006 Association for Computational Linguistics − log p(y ∗k |xk ; λ). We minimize the following loss function for the ML criterion training of CRFs: P known in"
P06-1028,W00-0726,0,0.214004,"know, the maximum output can be efficiently calculated with the Viterbi algorithm, which is the same as calculating Eq. 1. Therefore, we can find the maximum incorrect output by using the A* algorithm (Hart et al., 1968), if the maximum output is the correct output, and by using the Viterbi algorithm otherwise. It may be feared that since the objective function is not differentiable everywhere for ψ = ∞, problems for optimization would occur. However, it has been shown (Le Roux and McDer4.2 Segmentation F-score Loss for SSTs The standard evaluation measure of SSTs is the segmentation F-score (Sang and Buchholz, 2000): Fγ = 220 γ2 (γ 2 + 1) · T P · F N + F P + (γ 2 + 1) · T P (13) Named Entity Recognition Text Chunking Seg.: NP VP NP VP PP Seg.: NP x: He reckons the current account deficit will narrow to only # 1.8 billion . y: B-NP B-VP B-NP I-NP I-NP I-NP B-VP I-VP B-PP B-NP I-NP I-NP I-NP O Dep.: y1 y2 y3 y4 y5 y6 y7 y8 y9 y10 y11 y12 y13 y14 ORG PER LOC x: United Nation official Ekeus Smith heads for Baghdad . y: B-ORG I-ORG O B-PER I-PER O O B-LOC O Dep.: y1 y2 y3 y4 y5 y6 y7 y8 y9 Figure 1: Examples of sequential segmentation tasks (SSTs): text chunking (Chunking) and named entity recognition (NER)."
P06-1028,W03-0419,0,0.0343031,"Missing"
P06-1028,W03-1019,0,0.151784,"ORG I-ORG O B-PER I-PER O O B-LOC O Dep.: y1 y2 y3 y4 y5 y6 y7 y8 y9 Figure 1: Examples of sequential segmentation tasks (SSTs): text chunking (Chunking) and named entity recognition (NER). where T P , F P and F N represent true positive, false positive and false negative counts, respectively. The individual evaluation units used to calculate T P , F N and P N , are not individual outputs yi or output sequences y, but rather segments. We need to define a segment-wise loss, in contrast to the standard CRF loss, which is sometimes referred to as an (entire) sequential loss (Kakade et al., 2002; Altun et al., 2003). First, we consider the point-wise decision w.r.t. Eq. 1, that is, yˆi = arg maxyi ∈Y1 g(y, x, i, λ). The point-wise discriminant function can be written as follows: g(y, x, i, λ) = max y 0 ∈Y|y |[yi ] 0 λ · F (y , x) viously reduces to Eq. 14 if the length of all segments is 1. Then, the segment-wise misclassification measure d(y ∗ , x, sj , λ) can be obtained simply by replacing the discriminant function of the entire sequence g(y, x, λ) with that of segmentwise g(y, x, sj , λ) in Eq. 7. Let s∗k be a segment sequence corresponding to the correct output y ∗k for a given xk , and S(xk ) be al"
P06-1028,N03-1028,0,0.775517,"CRFs) are a recently introduced formalism (Lafferty et al., 2001) for representing a conditional model p(y|x), where both a set of inputs, x, and a set of outputs, y, display non-trivial interdependency. CRFs are basically defined as a discriminative model of Markov random fields conditioned on inputs (observations) x. Unlike generative models, CRFs model only the output y’s distribution over x. This allows CRFs to use flexible features such as complicated functions of multiple observations. The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing (Sha and Pereira, 2003) and information extraction (McCallum and Li, 2003). Since the introduction of CRFs, intensive research has been undertaken to boost their effectiveness. The first approach to estimating CRF parameters is the maximum likelihood (ML) criterion over conditional probability p(y|x) itself (Lafferty et al., 2001). The ML criterion, however, 217 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 217–224, c Sydney, July 2006. 2006 Association for Computational Linguistics − log p(y ∗k |xk ; λ). We minimize the following loss functio"
P06-1028,H05-1087,0,0.0219381,"erage accuracies, or error rates, rather than a given task-specific evaluation measure. For example, sequential segmentation tasks (SSTs), such as text chunking and named entity recognition, are generally evaluated with the segmentation F-score. This inconsistency between the objective function during training and the task evaluation measure might produce a suboptimal result. In fact, to overcome this inconsistency, an SVM-based multivariate optimization method has recently been proposed (Joachims, 2005). Moreover, an F-score optimization method for logistic regression has also been proposed (Jansche, 2005). In the same spirit as the above studies, we first propose a generalization framework for CRF training that allows us to optimize directly not only the error rate, but also any evaluation measure. In other words, our framework can incorporate any evaluation measure of interest into the loss function and then optimize this loss function as the training objective function. Our proposed framework is fundamentally derived from an approach to (smoothed) error rate minimization well This paper proposes a framework for training Conditional Random Fields (CRFs) to optimize multivariate evaluation mea"
P06-1028,P03-1021,0,\N,Missing
P06-1078,W03-0430,0,0.0527793,"Missing"
P06-1078,H01-1034,0,0.251074,"Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan {sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp Abstract expressions and their categories. Unlike text data, speech data introduce automatic speech recognition (ASR) error problems to NER. Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while o"
P06-1078,C00-2167,0,0.0542981,"Missing"
P06-1078,W98-1120,0,0.0374624,", especially in precision, compared to simply applying text-based NER to the ASR results. 2 Manual transcription Speech data Transcriptions ASR ASR results NE labeling NE-labeled transcriptions Setting ASR confidence feature to 1 SVM-based NER Text-based training data NER is a kind of chunking problem that can be solved by classifying words into NE classes that consist of name categories and such chunking states as PERSON-BEGIN (the beginning of a person’s name) and LOCATION-MIDDLE (the middle of a location’s name). Many discriminative methods have been applied to NER, such as decision trees (Sekine et al., 1998), ME models (Borthwick, 1999; Chieu and Ng, 2003), and CRFs (McCallum and Li, 2003). In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002). We define three features for each word: the word itself, its part-of-speech tag, and its character type. We also use those features for the two preceding and succeeding words for context dependence and use 15 features when classifying a word. Each feature is represented by a binary value (1 or 0), for example, “whether the previous word is Japan,” and each word is class"
P06-1078,W03-0423,0,0.163739,"to NER. Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are hard to use in HMM-based models. To deal with ASR error problems in NER, Palmer and Ostendorf (2001) proposed an HMMbased NER method that explicitly models ASR errors using ASR confidence and rejects erroneous word"
P06-1078,N04-4010,0,0.0913895,"on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are hard to use in HMM-based models. To deal with ASR error problems in NER, Palmer and Ostendorf (2001) proposed an HMMbased NER method that explicitly models ASR errors using ASR confidence and rejects erroneous word hypotheses in the ASR results. Such rejection is especially effective when ASR accuracy is relatively low because many misrecognized words m"
P06-1078,H05-1062,0,0.023325,"Missing"
P06-1078,C02-1054,1,\N,Missing
P06-1098,P05-1066,0,0.0109351,"n Chinese engineers , construction design and construction methods of the recipient from . The Health and Welfare Ministry has decided to invoke the Disaster Relief Law in extending relief measures to the village and the city of Niigata . The Health and Welfare Ministry in that the Japanese people in the village are made law . The Health and Welfare Ministry decided to apply the Disaster Relief Law to the village in Niigata . Figure 3: Sample translations from two systems: Phrase and Normalized-2 as a set of rules that reorders the foreign language to match with English language sequentially. Collins et al. (2005) presented a method with hand-coded rules. Our method directly learns such serialization rules from a bilingual corpus without linguistic clues. The translation quality presented in Section 5 are rather low due to the limited size of the bilingual corpus, and also because of the linguistic difference of two languages. As our future work, we are in the process of experimenting our model for other languages with rich resources, such as Chinese and Arabic, as well as similar language pairs, such as French and English. Additional feature functions will be also investigated that were proved success"
P06-1098,W05-1507,0,0.0389301,"n-terminals in γ and α that are associated with ∼. Chiang (2005) proposed a hierarchical phrasebased translation model, a binary synchronousCFG, which restricted the form of production rules as follows: The integration with a ngram language model further increases the cost of decoding especially when incorporating a higher order ngram, such as 5-gram. In the hierarchical phrase-based model (Chiang, 2005), and an inversion transduction grammar (ITG) (Wu, 1997), the problem is resolved by restricting to a binarized form where at most two non-terminals are allowed in the righthand side. However, Huang et al. (2005) reported that the computational complexity for decoding amounted to O(J 3+3(n−1) ) with n-gram even using a hook technique. The complexity lies in memorizing the ngram’s context for each constituent. The order of ngram would be a dominant factor for higher order ngrams. • Only two types of non-terminals allowed: S and X. • Both of the strings γ and α must contain at least one terminal item. • Rules may have at most two non-terminals but non-terminals cannot be adjacent for the foreign language side γ. As an alternative to a binarized form, we present a target-normalized hierarchical phrasebas"
P06-1098,N03-1017,0,0.316076,"1I , f1J ) is a feature function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that e1I is segmented into a sequence of K phrases e¯ 1K . Each phrase e¯ k is transformed into f¯k . The translated phrases are reordered to form f1J . One of the benefits of the modeling is that the phrase translation unit preserves localized word reordering. However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordere"
P06-1098,W04-3250,0,0.0419224,"Missing"
P06-1098,P02-1038,0,0.0250315,"pled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models. Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system. 1 where hm (e1I , f1J ) is a feature function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that e1I is segmented into a sequence of K phrases e¯ 1K . Each phrase e¯ k is transformed into f¯k . The translated"
P06-1098,J03-1002,0,0.00440966,"ls but non-terminals cannot be adjacent for the foreign language side γ. As an alternative to a binarized form, we present a target-normalized hierarchical phrasebased translation model. The model is a class of a hierarchical phrase-based model, but constrained so that the English part of the right-hand side The production rules are induced from a bilingual corpus with the help of word alignments. To alleviate a data sparseness problem, glue rules are 778 added that prefer combining hierarchical phrases in a serial manner: D E S → S 1 X2 , S 1 X2 (5) E D (6) S → X1 , X1 model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Koehn et al., 2003). Second, phrase translation pairs are extracted from the word alignment corpus (Koehn et al., 2003). The method exhaustively extracts phrase j+m J I pairs ( f j , ei+n i ) from a sentence pair ( f1 , e1 ) that do not violate the word alignment constraints a: where boxed indices indicate non-terminal’s linkages represented in ∼. Our model is based on Chiang (2005)’s framework, but further restricts the form of production rules so that the aligned right-hand side α follows a GNF-like structure: D E ¯ ∼ X"
P06-1098,N04-4026,0,0.119691,"Missing"
P06-1098,J93-2003,0,0.00796207,"or is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that e1I is segmented into a sequence of K phrases e¯ 1K . Each phrase e¯ k is transformed into f¯k . The translated phrases are reordered to form f1J . One of the benefits of the modeling is that the phrase translation unit preserves localized word reordering. However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordered to match with an En1 Introduction In a classical statistical machine translation, a foreign language sentence f1J ="
P06-1098,P05-1033,0,0.605252,"ram language model is straightforward, since the model generates a translation in left-to-right order. Our decoder is based on an Earley-style top down parsing on the foreign language side. The projected English-side is generated in left-to-right order synchronized with the derivation of the foreign language side. The decoder’s implementation is taken after a decoder for an existing phrase-based model with a simple modification to account for production rules. Experimental results on a Japanese-toEnglish newswire translation task showed significant improvement against a phrase-based modeling. Chiang (2005) introduced a hierarchical phrasebased translation model that combined the strength of the phrase-based approach and a synchronous-CFG formalism (Aho and Ullman, 1969): A rewrite system initiated from a start symbol which synchronously rewrites paired nonterminals. Their translation model is a binarized synchronous-CFG, or a rank-2 of synchronousCFG, in which the right-hand side of a production rule contains at most two non-terminals. The form can be regarded as a phrase translation pair with at most two holes instantiated with other phrases. The hierarchically combined phrases provide a sort"
P06-1098,P03-1010,0,0.00564793,"f D, and height(Di ) and width(Di ) refer the height and width of subtree Di , respectively. In Figure 1(b), for instance, a rule of X 1 with non-terminals X 2 and X 4 , two rules X 2 and X 3 spanning two terminal symbols should be backtracked to proceed to X 4 . The rationale is that positive scaling factors prefer a deeper structure whereby negative scaling factors prefer a monotonized structure. 4.5 The bilingual corpus used for our experiments was obtained from an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs (refer to Table 1) (Utiyama and Isahara, 2003). From one-toone aligned sentences, 1,500 sentence pairs were sampled for a development set and a test set1 . Since the bilingual corpus is rather small, especially for the newspaper translation domain, Japanese/English dictionaries consisting of 1.3M entries were added into a training set to alleviate an OOV problem2 . Word alignments were annotated by a HMM translation model (Och and Ney, 2003). After Length-based Models Three trivial length-based feature functions were used in our experiment. hl (e1I ) = I (21) hr (D) = rule(D) (22) h p (D) = phrase(D) (23) 1 Japanese sentences were segment"
P06-1098,J97-3002,0,0.208821,"ectively. ∼ is a one-to-one correspondence for the non-terminals appeared in γ and α. Starting from an initial non-terminal, each rule rewrites non-terminals in γ and α that are associated with ∼. Chiang (2005) proposed a hierarchical phrasebased translation model, a binary synchronousCFG, which restricted the form of production rules as follows: The integration with a ngram language model further increases the cost of decoding especially when incorporating a higher order ngram, such as 5-gram. In the hierarchical phrase-based model (Chiang, 2005), and an inversion transduction grammar (ITG) (Wu, 1997), the problem is resolved by restricting to a binarized form where at most two non-terminals are allowed in the righthand side. However, Huang et al. (2005) reported that the computational complexity for decoding amounted to O(J 3+3(n−1) ) with n-gram even using a hook technique. The complexity lies in memorizing the ngram’s context for each constituent. The order of ngram would be a dominant factor for higher order ngrams. • Only two types of non-terminals allowed: S and X. • Both of the strings γ and α must contain at least one terminal item. • Rules may have at most two non-terminals but no"
P06-1098,P03-1019,0,0.100533,"ure function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that e1I is segmented into a sequence of K phrases e¯ 1K . Each phrase e¯ k is transformed into f¯k . The translated phrases are reordered to form f1J . One of the benefits of the modeling is that the phrase translation unit preserves localized word reordering. However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordered to match with an E"
P06-1098,P03-1021,0,\N,Missing
P07-2030,C02-1053,1,0.808016,"sed model and the best-performing baseline is significant (p<0.00001). The results also show that PMI is more effective in quiz-style ranking than any other measure. The fact that max is important probably means that the mere existence of a word that has a high PMI score is enough to raise the ranking of a hint. It is also interesting that Wikipedia gives better ranking, which is probably because people’s names and related keywords are close to each other in such descriptive texts. Analyzing the ranking model trained by the ranking SVM allows us to calculate the weights given to the features (Hirao et al., 2002). Table 2 shows the top-10 features in weights in absolute figures when all samples were used for training. It can be seen that high PMI values and words/semantic categories related to government or creation lead to easy hints, whereas semantic categories, such as birth and others (corresponding to the person in ‘a person from Tokyo’), lead to early hints. This supports our intuitive notion that birthplaces should be presented early for users to start thinking about a person. 6 Summary and Future Work This paper proposed ranking definitions of a person to automatically generate a “Who is this?"
P07-2030,1991.mtsummit-papers.16,0,0.0147351,"cal/dependency parser, http://chasen.org/˜taku/software/cabocha/) and extracted all content words to make binary features representing the existence of each content word. There are 2,156 BOW features in our data. As for the semantic features, we used the semantic categories in Nihongo Goi-Taikei. Since there are 2,715 semantic categories, we created 2,715 features representing the existence of each semantic category in the definition. Semantic categories were assigned to words in the definition by a morphological analyzer that comes with ALT/J-E, a Japanese-English machine translation system (Ikehara et al., 1991). In total, we have 4,991 features to represent each definition. We calculated all feature values for all definitions in our data to be used for the learning. 4.3 Training Ranking Models Using the reference ranking data, we trained a ranking model using the ranking SVM (Joachims, 2002) (with a linear kernel) that minimizes the pairwise ranking error among the definitions of each person. 5 Evaluation To evaluate the performance of the ranking model, following (Xu et al., 2004; Sun et al., 2005), we compared it with baselines that use only the scores of IR-related and positional features for ran"
P07-2030,P03-1069,0,0.056852,"with improved understanding and lasting motivation, which is useful for educational systems. In our approach, we train a ranker that learns from data the appropriate ranking of definitions based on features that encode the importance of keywords in a definition as well as its content. Experimental results show that our approach is significantly better in ranking definitions than baselines that use conventional information retrieval measures such as tf*idf and pointwise mutual information (PMI). 1 Introduction Appropriate ranking of sentences is important, as noted in sentence ordering tasks (Lapata, 2003), in effectively delivering content. Whether the task is to convey news texts or definitions, the objective is to make it easier for users to understand the content. However, just conveying it in an encyclopedia-like or temporal order may not be the best solution, considering that interaction between a system and a user improves understanding (Sugiyama et al., 1999) and that the cognitive load in receiving information is believed to correlate with memory fixation (Craik and Lockhart, 1972). In this paper, we discuss the idea of ranking definitions as a way to present people’s biographical info"
P08-1076,P05-1001,0,0.835839,"n traditional and important NLP tasks, namely part-of-speech (POS) tagging, syntactic chunking, and named entity recognition (NER). These are also typical supervised learning applications in NLP, and are referred to as sequential labeling and segmentation problems. In some cases, these tasks have relatively large amounts of labeled training data. In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL. In fact, few papers have succeeded in showing significantly better results than state-of-theart supervised learning. Ando and Zhang (2005) reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000) and NER with the CoNLL’03 shared task data (Tjong Kim Sang and Meulder, 2003). One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible. This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data. To achieve this, it is paramount to make the SSL method scalable with regard"
P08-1076,W03-0425,0,0.114297,"Missing"
P08-1076,P05-1046,0,0.0212165,"s that are expected to capture a good feature representation of the target problem. As regards syntactic chunking, JESS-CM significantly outperformed ASO-semi for the same 15M-word unlabeled data size obtained from the Wall Street Journal in 1991 as described in (Ando and Zhang, 2005). Unfortunately with NER, JESS-CM is slightly inferior to ASO-semi for the same 27M-word unlabeled data size extracted from the Reuters corpus. In fact, JESS-CM using 37M-words of unlabeled data provided a comparable result. We observed that ASOsemi prefers ‘nugget extraction’ tasks to ’field segmentation’ tasks (Grenager et al., 2005). We cannot provide details here owing to the space limitation. Intuitively, their word prediction auxiliary problems can capture only a limited number of characteristic behaviors because the auxiliary problems are constructed by a limited number of ‘binary’ classifiers. Moreover, we should remember that ASOsemi used the human knowledge that ‘named entities mostly consist of nouns or adjectives’ during the auxiliary problem construction in their NER experiments. In contrast, our results require no such additional knowledge or limitation. In addition, the design and training of auxiliary proble"
P08-1076,N01-1025,0,0.361443,"Missing"
P08-1076,N03-1028,0,0.433554,"e written as: L1 (λ0 |Θ) = 0 X n log P (y n |xn ; λ0 , Θ) + log p(λ0 ), where p(λ ) is a prior probability distribution of λ0 . Clearly, JESS-CM shown in Equation 2 has exactly the same form as Equation 1. With a fixed Θ, the log-likelihood, log pj , can be seen simply as the feature functions of JESS-CM as with fi . Therefore, embedded joint PMs do not violate the global convergence conditions. As a result, as with supervised CRFs, it is guaranteed that λ0 has a value that achieves the global maximum of L1 (λ0 |Θ). Moreover, we can obtain the same form of gradient as that of supervised CRFs (Sha and Pereira, 2003), that is, £ ¤ ∇L1 (λ0 |Θ) = EP˜ (Y,X ;λ0 ,Θ) h(Y, X ) X £ ¤ − EP (Y|xn ;λ0 ,Θ) h(Y, xn ) +∇ log p(λ0 ). n Thus, we can easily optimize L1 by using the forward-backward algorithm since this paper solely focuses on a sequence model and a gradient-based optimization algorithm in the same manner as those used in supervised CRF parameter estimation. We cannot naturally incorporate unlabeled data into standard discriminative learning methods since the correct outputs y for unlabeled data are unknown. On the other hand with a generative approach, a well-known way to achieve this incorporation is to"
P08-1076,P07-1096,0,0.101655,"621 Development 30–31/08/96 3,466 51,362 Test 06–07/12/96 3,684 46,435 Table 1: Details of training, development, and test data (labeled data set) used in our experiments data Tipster Reuters Corpus English Gigaword total In our experiments, we report POS tagging, syntactic chunking and NER performance incorporating up to 1G-words of unlabeled data. 3.1 Data Set To compare the performance with that of previous studies, we selected widely used test collections. For our POS tagging experiments, we used the Wall Street Journal in PTB III (Marcus et al., 1994) with the same data split as used in (Shen et al., 2007). For our syntactic chunking and NER experiments, we used exactly the same training, development and test data as those provided for the shared tasks of CoNLL’00 (Tjong Kim Sang and Buchholz, 2000) and CoNLL’03 (Tjong Kim Sang and Meulder, 2003), respectively. The training, development and test data are detailed in Table 11 . The unlabeled data for our experiments was taken from the Reuters corpus, TIPSTER corpus (LDC93T3C) and the English Gigaword corpus, third edition (LDC2007T07). As regards the TIP1 The second-order encoding used in our NER experiments is the same as that described in (Sha"
P08-1076,N06-1012,0,0.0174311,"of JESS-CM We used the same graph structure as the linear chain CRF for JESS-CM. As regards the design of the feature functions fi , Table 3 shows the feature templates used in our experiments. In the table, s indicates a focused token position. Xs−1:s represents the bi-gram of feature X obtained from s − 1 and s positions. {Xu }B u=A indicates that u ranges from A to B. For example, {Xu }s+2 u=s−2 is equal to five feature templates, {Xs−2 , Xs−1 , Xs , Xs+1 , Xs+2 }. ‘word type’ or wtp represents features of a word such as capitalization, the existence of digits, and punctuation as shown in (Sutton et al., 2006) without regular expressions. Although it is common to use external (a) POS tagging:(total 47 templates) [ys ], [ys−1:s ], {[ys , pf-Ns ], [ys , sf-Ns ]}9N =1 , {[ys , wdu ], [ys , wtpu ], [ys−1:s , wtpu ]}s+2 u=s−2 , {[ys , wdu−1:u ], [ys , wtpu−1:u ], [ys−1:s , wtpu−1:u ]}s+2 u=s−1 (b) Syntactic chunking: (total 39 templates) [ys ], [ys−1:s ], {[ys , wdu ], [ys , posu ], [ys , wdu , posu ], [ys−1:s , wdu ], [ys−1:s , posu ]}s+2 u=s−2 , {[ys , wdu−1:u ], [ys , posu−1:u ], {[ys−1:s , posu−1:u ]}s+2 u=s−1 , (c) NER: (total 79 templates) [ys ], [ys−1:s ], {[ys , wdu ], [ys , lwdu ], [ys , posu ]"
P08-1076,D07-1083,1,0.417996,". Second, we report the best current results for the widely used test 665 Proceedings of ACL-08: HLT, pages 665–673, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics collections described above. Third, we confirm that the use of more unlabeled data in SSL can really lead to further improvements. 2 Conditional Model for SSL We design our model for SSL as a natural semisupervised extension of conventional supervised conditional random fields (CRFs) (Lafferty et al., 2001). As our approach for incorporating unlabeled data, we basically follow the idea proposed in (Suzuki et al., 2007). 2.1 Conventional Supervised CRFs Let x ∈ X and y ∈ Y be an input and output, where X and Y represent the set of possible inputs and outputs, respectively. C stands for the set of cliques in an undirected graphical model G(x, y), which indicates the interdependency of a given x and y. y c denotes the output from the corresponding clique c. Each clique c ∈ C has a potential function Ψc . Then, the CRFs define the conditional probability p(y|x) as a product of Ψc s. In addition, let f = (f1, . . ., fI ) be a feature vector, and λ = (λ1, . . ., λI ) be a parameter vector, whose lengths are I. p("
P08-1076,W00-0726,0,0.6312,"nd are referred to as sequential labeling and segmentation problems. In some cases, these tasks have relatively large amounts of labeled training data. In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL. In fact, few papers have succeeded in showing significantly better results than state-of-theart supervised learning. Ando and Zhang (2005) reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000) and NER with the CoNLL’03 shared task data (Tjong Kim Sang and Meulder, 2003). One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible. This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data. To achieve this, it is paramount to make the SSL method scalable with regard to the size of unlabeled data. We first propose a scalable model for SSL. Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data (Marcus et al., 1994) for POS ta"
P08-1076,W03-0419,0,0.0369952,"Missing"
P08-1076,N03-1033,0,0.0458738,"Missing"
P08-1076,J93-2004,0,\N,Missing
P09-1093,P06-1048,0,0.241824,"del (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003) and Clarke et al. (2006) employed the linear model with simple combined features. They simply regard a sentence as a word sequence and structural information, such as full parse tree or dependency trees, are encoded in the sequence as features. The advantage of these methods over the tree trimming approach is that they have the potential to drop arbitrary words from the original sentence without the need to consider the boundaries determined by the tree structures. This approach is more suitable fo"
P09-1093,W04-3239,0,0.0304391,"s output by a state-of-the-art Japanese dependency parser contain at least one error (Kudo and Matsumoto, 2005). Even more, it is well known that if we parse a sentence whose source is different from the training data of the parser, the performance could be much worse. This critically degrades the overall performance of sentence compression. Moreover, summarization systems often have to process megabytes of documents. Parsers are still slow and users of on2 Generally, a dependency relation is defined between bunsetsu. Therefore, in order to identify word dependencies, we followed Kudo’s rule (Kudo and Matsumoto, 2004) 827 3.2.1 Intra-sentence Positional Term Weighting (IPTW) IDF is a global term weighting scheme in that it measures the significance score of a word in a text corpus, which could be extremely large. By contrast, this paper proposes another type of term weighting; it measures the positional significance score of a word within its sentence. Here, we assume the following hypothesis: demand summarization systems are not prepared to wait for parsing to finish. 3 A Syntax Free Sequence-oriented Sentence Compression Method As an alternative to syntactic parsing, we propose two novel features, intra-"
P09-1093,E06-1038,0,0.127006,"thods trim dependency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003) and Clarke et al. (2006) employed the linear model with simple combined features. They simply regard a sentence as a word sequence and structural information, such as full parse tree or dependency trees, are encoded in the sequence as features. The advantage of these methods over the tree trimming approach is that they have the potential to drop arbitrary words from the original sentence without the need to consider the boundaries determ"
P09-1093,P08-1035,0,0.413878,"Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1 Hereafter, we refer these compression processes as “tree trimming.” 826 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Chunk 1 推定 し suitei shi Chunk 2 推定 し た suitei shi ta Chunk 3 配点 について haiten Chunk 1 nitsuite 福武 Chunk 3 配点 について haiten ga Chunk 4 ta Chunk 2 が fukutake た nitsuite 福武 fukutake が ga Chunk 7 枝問 部分 の Compression Chunk 5 公表 し て い kouhyou shi te i センタ試験枝問 center shiken edamon edamon bub"
P09-1093,P02-1040,0,0.0783887,"Corpus and Evaluation Measures We randomly selected 1,000 lead sentences (a lead sentence is the first sentence of an article excluding the headline.) whose length (number of words) was greater than 30 words from the Mainichi Newspaper from 1994 to 2002. There were five different ideal compressions (reference compressions produced by human) for each sentence; all had a 0.6 compression rate. The average length of the input sentences was about 42 words and that of the reference compressions was about 24 words. For MCE learning, we selected the reference compression that maximize the BLEU score (Papineni et al., 2002) (= argmaxr∈R BLEU(r, R)) from the set of reference compressions and used it as correct data for training. Note that r is a reference compression and R is the set of reference compressions. We employed both automatic evaluation and human subjective evaluation. For automatic evaluation, we employed BLEU (Papineni et al., 2002) by following (Unno et al., 2006). We utilized 5fold cross validation, i.e., we broke the whole data set into five blocks and used four of them for training and the remainder for testing and repeated the evaluation on the test data five times changing the test block each"
P09-1093,P05-1036,0,0.566966,"show that our method outperforms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1 Hereafter, we refer these compression processes as “tree trimming.” 826 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Chunk 1 推定 し suitei shi Chunk 2 推定 し た suitei shi ta Chunk 3 配点 について haiten Chunk 1 nitsuite 福武 Chunk 3 配点 について haiten ga Ch"
P09-1093,P06-2109,0,0.213731,"forms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1 Hereafter, we refer these compression processes as “tree trimming.” 826 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Chunk 1 推定 し suitei shi Chunk 2 推定 し た suitei shi ta Chunk 3 配点 について haiten Chunk 1 nitsuite 福武 Chunk 3 配点 について haiten ga Chunk 4 ta Chunk 2 が f"
P09-1093,P05-1012,0,\N,Missing
P09-2086,D07-1090,0,0.558593,"responds to its node position in the unigram table, we can remove the word ids for the first order. Our implementation merges across different orders of N -grams, then separates into multiple tables such as word ids, smoothed probabilities, back-off coefficients, and pointers. The starting positions of different orders are memorized to allow access to arbitrary orders. To store N -gram counts, we use three tables for word ids, counts and pointers. We share the same tables for word ids and pointers with additional probability and back-off coefficient tables. To support distributed computation (Brants et al., 2007), we further split the N -gram data into “shards” by hash values of the first bigram. Unigram data are shared across shards for efficiency. 3 an imaginary super root node emits 10 for its only child, i.e., the root node. After the root node, its first child or node 1 follows. Since (M + 1)0s and M 1s are emitted for a trie with M nodes, LOUDS occupies 2M + 1 bits. We define a basic operation on the bit string. sel1 (i) returns the position of the i-th 1. We can also define similar operations over zero bit strings, sel0 (i). Given selb , we define two operations for a node x. parent(x) gives x’"
P09-2086,D07-1021,0,0.0512316,"xperimented with English Web 1T 5-gram from LDC consisting of 25 GB of gzipped raw text N -gram counts. By using 8-bit floating point quantization 1 , N -gram language models are compressed into 10 GB, which is comparable to a lossy representation (Talbot and Brants, 2008). Introduction There has been an increase in available N -gram data and a large amount of web-scaled N -gram data has been successfully deployed in statistical machine translation. However, we need either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them. Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time. However, the lossy approaches may reduce accuracy, and tuning is necessary. A lossless approach is obviously better than a lossy one if other conditions are the same. In addtion, a lossless approach can easly combined with pruning. Therefore, lossless representation of N -gram is a key issue even for lossy approaches. Raj and Whittaker (2003) showed a general N gram language model structure and introduced a lossless algorithm that compressed a sorted integer vector by"
P09-2086,P08-1058,0,0.0116879,"f probability pointer Figure 1: Data structure for language model and LOUDS succinctly represents it by a 2M + 1 bit string. The space is further reduced by considering the N -gram structure. We also use variable length coding and block-wise compression to compress the values associated with each node, such as word ids, probabilities or counts. We experimented with English Web 1T 5-gram from LDC consisting of 25 GB of gzipped raw text N -gram counts. By using 8-bit floating point quantization 1 , N -gram language models are compressed into 10 GB, which is comparable to a lossy representation (Talbot and Brants, 2008). Introduction There has been an increase in available N -gram data and a large amount of web-scaled N -gram data has been successfully deployed in statistical machine translation. However, we need either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them. Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time. However, the lossy approaches may reduce accuracy, and tuning is necessary. A lossless approach is obviously better than a lo"
P09-2086,W07-0712,0,\N,Missing
P11-2112,P05-1001,0,0.758669,"orrelation with the best ˆ given by the base supervised model. This output y implies that fn is an informative and potent feature in the model. Then, the distribution of fn has very ˆ if |VD (fn )| small (or no) correlation to determine y is zero or near zero. In this case, fn can be evaluated as an uninformative feature in the model. From this perspective, we treat VD (fn ) as a measure of feature potency in terms of the base supervised model. The essence of this idea, evaluating features against each other on a certain model, is widely used in the context of semi-supervised learning, i.e., (Ando and Zhang, 2005; Suzuki and Isozaki, We define VD∗ (fn ) as VD∗ (fn ) = dδVD0 (fn )e if VD0 (fn ) > 0 and VD∗ (fn ) = bδVD0 (fn )c otherwise, where δ is a positive user-specified constant. Note that VD∗ (fn ) always becomes an integer, that is, VD∗ (fn ) ∈ N where N = {. . . , −2, −1, 0, 1, 2, . . .}. This calculation can be seen as mapping each feature into a discrete (integer) space with respect to VD0 (fn ). δ controls the range of VD0 (fn ) mapping into the same integer. with the same (similar) feature potency are given the same weight by supervised learning since they have ˆ. δ the same potency with reg"
P11-2112,D09-1060,0,0.211446,"e summation of a data-wise calculation (map phase), and VD∗ (fn ) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III 639 (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dependency parsing data derived from PTB-III (Koo et al., 2008)"
P11-2112,P10-1001,0,0.0505475,"Missing"
P11-2112,P08-1068,0,0.618373,"dels of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, with the iCWR approach, C is induced independently from F, and us"
P11-2112,P09-1116,0,0.546608,", supervised learning has become a standard way to train the models of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, w"
P11-2112,D10-1004,0,0.034847,"Missing"
P11-2112,W09-1119,0,0.107647,"Missing"
P11-2112,P08-1076,1,0.954742,"mputing framework. This is because Rn and An can be calculated by the summation of a data-wise calculation (map phase), and VD∗ (fn ) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III 639 (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dep"
P11-2112,D09-1058,1,0.917404,"Missing"
P11-2112,W03-0419,0,0.288854,"Missing"
P11-2112,P09-1054,0,0.1595,"arning. 3.2 Feature potency discounting To discount low potency values, we redefine feature potency as VD0 (fn ) instead of VD (fn ) as follows:   log [Rn +C]−log[An ] if Rn −An &lt; −C if − C ≤ Rn −An ≤ C VD0 (fn ) = 0  log [Rn −C]−log[An ] if C &lt; Rn −An where Rn and An are defined in Figure 2. Note that VD (fn ) = VD+ (fn ) − VD− (fn ) = Rn − An . The difference from VD (fn ) is that we cast it in the log-domain and introduce a non-negative constant C. The introduction of C is inspired by the L1 regularization technique used in supervised learning algorithms such as (Duchi and Singer, 2009; Tsuruoka et al., 2009). C controls how much we discount VD (fn ) toward zero, and is given by the user. 3.3 Feature potency quantization 638 and given input x. We write z ∈ y when the local sub-structure z is a part of output y, assuming that output y is constructed by a set of local substructures. Then formally, the ∑ n-th feature is written as fn (x, z), and fn (x, y) = z∈y fn (x, z) holds. Similarly, we introduce r(x, z), where r(x, z) = 1 ˆ , and r(x, z) = 0 otherwise, namely z ∈ ˆ. if z ∈ y /y We define Z(x) as the set of all local substructures possibly generated for all y in Y(x). Z(x) can be enumerated easi"
P11-2112,P10-1040,0,0.749404,"ing has become a standard way to train the models of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, with the iCWR approach,"
P11-2112,J93-2004,0,\N,Missing
W03-1024,P95-1017,0,0.0735895,"automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1 http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros accurately. Murata and Nagao (1997) proposed complicated heuristic r"
W03-1024,J95-2003,0,0.190148,"oun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1 http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros accurately. Murata and Nagao (1997) proposed complicated heuristic rules that take various features of antecedents and anaphors into account. We have to take even more factors into account, but it is difficult to maintain such heuristic rules. Th"
W03-1024,C02-1053,1,0.876791,"Missing"
W03-1024,W03-2604,0,0.217699,"independent, removing a heavily weighted feature does not necessarily degrade the system’s performance. Hence, feature elimination is more reliable for reducing the number of features. However, feature elimination takes a long time. On the other hand, feature weights can give rough guidance. According to the table, our new features (Parallel, Unfinished, and Intra) obtained relatively large weights. This implies their importance. When we eliminated these three features, vrads+svm2’s score for editorial dropped by 4 points. Therefore, combinations of these three features are useful. Recently, Iida et al. (2003a) proposed an SVMbased tournament model that compares two candidates and selects the better one. We would like to compare or combine their method with our method. For further improvement, we have to make the morphological analyzer and the dependency analyzer more reliable because they make many mistakes when they process complex sentences. SVM has often been criticized as being too slow. However, the above data were small enough for the state-of-the-art SVM programs. The number of examples in each set of training data was about 5,000– 6,100, and each training phase took only 5–18 seconds on a"
W03-1024,C02-1054,1,0.879479,"Missing"
W03-1024,P86-1031,0,0.512921,"ill increase. From this motivation, we are developing our system toward the ability to resolve anaphors in full-text newspaper articles. In Japanese, anaphors are often omitted and these omissions are called zero pronouns. Since they do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1 http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kamey"
W03-1024,N01-1025,0,0.0875759,"Missing"
W03-1024,C96-2147,0,0.635965,"t al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1 http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros accurately. Murata and Nagao (1997) proposed complicated heuristic rules that take various features of antecedents and anaphors into account. We have to take even more factors into account, but it is difficult to maintain such heuristic rules. Therefore, recent studies employ machine learning approaches. However, it is also difficult to prepare a sufficient number of annotated corpora. In this paper, we propose a method that combines these two approac"
W03-1024,C02-1078,0,0.617086,"a / Bobu ni / hon wo / okutta. Tom=subj Bob=object2 book=object sent (Tom sent a book to Bob.) Bunsetsu dependency is represented by a list of bunsetsu (modifier, modified). For instance,    pairs      ""!  # indicates that there are four bunsetsus in this sentence and that the first bunsetsu modifies the fourth bunsetsu and so on. The last bunsetsu modifies no bunsetsu, which is in!  dicated by . It takes a long time to construct high-quality annotated data, and we want to compare our results with conventional methods. Therefore, we obtained Seki’s data (Seki et al., 2002a; Seki et al., 2002b), which are based on the Kyoto University Corpus 2 2.0. These data are divided into two groups: general and editorial. General contains 30 general news articles, and editorial contains 30 editorial articles. According to his experiments, editorial is harder than general. Perhaps this is caused by the difference in rhetorical styles and the lengths of articles. The average number of sentences in an editorial article is 28.7, while that in a general article is 13.9. However, we found problems in his data. For instance, the data contained ambiguous antecedents like dou-shi ("
W03-1024,J94-2003,0,0.624289,"om this motivation, we are developing our system toward the ability to resolve anaphors in full-text newspaper articles. In Japanese, anaphors are often omitted and these omissions are called zero pronouns. Since they do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1 http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized"
W03-1024,C02-1051,0,0.011681,"in other cases of the verb. When a verb has two or more zeros, we resolve ga first, and its best candidate is excluded from the candidates of wo or ni. 2.2 Ranking rules Various heuristics have been reported in past literature. Here, we use the following heuristics. 1. Forward center ranking (Walker et al., 1994): (topic , empathy , subject , object2 , object , others). 2. Property-sharing (Kameyama, 1986): If a zero is the subject of a verb, its antecedent is perhaps a subject in the antecedent’s sentence. If a zero is an object, its antecedent is perhaps an object. 3. Semantic constraints (Yamura-Takei et al., 2002; Yoshino, 2001): If a zero is the subject of ‘eat,’ its antecedent is probably a person or an animal, and so on. We use Nihongo Goi Taikei (Ikehara et al., 1997), which has 14,730 English-to-Japanese translation patterns for 6,103 verbs, to check the acceptability of a candidate. Goi Taikei also has 300,000 words in about 3,000 semantic categories. (See Appendix A for details.) 4. Demotion of candidates in a relative clause (rentai shuushoku setsu): Usually, Japanese zeros do not refer to noun phrases in relative clauses (Ehara and Kim, 1996). (See Appendix B for details.) Since sentences in"
W03-1024,A92-1028,0,\N,Missing
W04-1014,J02-4006,0,0.0287356,"by automatic speech recognition instead of manually transcribed speech (Hori and Furui, 2000a). This summarization approach is word extraction (sentence compaction) that attempts to extract significant information, exclude acoustically and linguistically unreliable words, and maintain the meanings of the original speech. The summarization approaches that have been mainly researched so far are extracting sentences or words from original text or transcribed speech. There has also been research on generating an “abstract” like the much higher level summarization composed freely by human experts (Jing, 2002). This approach includes not only extracting sentences but also combining sentences to generate new sentences, replacing words, reconstructing syntactic structure, and so on. Evaluation Measures for Summarization Metrics that can be used to accurately evaluate the various appropriateness to summarization are needed.The simplest and probably the ideal way of evaluating automatic summarization is to have human subjects read the summaries and evaluate them in terms of the appropriateness of summarization. However, this type of evaluation is too expensive for comparing the efficiencies of many dif"
W04-1014,N03-1020,0,0.0264945,"n sentences or words have meanings, so some concatenations of sentences or words in the automatic summaries sometimes generate meanings different from the original. The evaluation metrics for summarization should thus consider each concatenation between components in the automatic results. To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al., 2002) for machine translation. Evaluation metrics based on word accuracy, summarization accuracy (SumACCY), using a word network made by merging manual summaries has been proposed (Hori and Furui, 2001). In addition, to solve the problems for the coverage of correct answers and the reliability of manual summaries as correct answers, weighted summarization accuracy (WSumACCY) in which SumACCY is weighted by the majority of the humans’ selections, has been proposed (Hori and Furui, 2003a). In contrast, summarization through sentence extraction has been"
W04-1014,P02-1040,0,0.0908383,"oncatenations of sentences or words in the automatic summaries sometimes generate meanings different from the original. The evaluation metrics for summarization should thus consider each concatenation between components in the automatic results. To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al., 2002) for machine translation. Evaluation metrics based on word accuracy, summarization accuracy (SumACCY), using a word network made by merging manual summaries has been proposed (Hori and Furui, 2001). In addition, to solve the problems for the coverage of correct answers and the reliability of manual summaries as correct answers, weighted summarization accuracy (WSumACCY) in which SumACCY is weighted by the majority of the humans’ selections, has been proposed (Hori and Furui, 2003a). In contrast, summarization through sentence extraction has been evaluated using only single sentence precision."
W06-3115,2004.iwslt-evaluation.13,0,0.016641,"3,979 23,186,379 es-en 17,221,890 16,601,306 16,540,767 12,677,192 21,709,212 fr-en 16,176,075 15,635,900 15,610,319 11,645,404 20,760,539 en-de 17,596,764 17,052,808 16,936,710 12,218,997 23,066,052 en-es 17,237,723 16,597,274 16,530,810 12,688,773 21,698,267 en-fr 16,220,520 15,658,940 15,613,755 11,653,242 20,789,570 Table 2: Number of phrases extracted from differently preprocessed corpora. lower stem prefix4 merged de-en 37,711,217 46,550,101 53,429,522 80,260,191 es-en 61,161,868 75,610,696 78,193,818 111,153,303 fr-en 56,025,918 68,210,968 70,514,377 103,523,206 lexicon model t( f |e) (Bender et al., 2004): hdel (e1I , f1J ) = J  X j=1 max t( f j |ei ) < τdel 0≤i≤I  (4) The deletion model simply counts the number of words whose lexicon model probability is lower than a threshold τdel . Likewise, we also added an insertion model hins (e1I , f1J ) that penalizes the spuriously inserted English words using a lexicon model t(e |f ). For the hierarchical phrase-based model, we employed the same feature set except for the distortion model and the lexicalized reordering model. 3 Phrase Extraction from Different Word Alignment en-de 38,142,663 46,749,195 53,647,033 80,666,414 en-es 60,619,435 75,473,"
W06-3115,P05-1033,0,0.568763,"language, i.e. English, e1I = e1 , e2 , ..., eI by seeking a maximum likelihood solution of eˆ 1I = argmax Pr(e1I |f1J ) (1) e1I  I , f J) λ h (e m m m=1 1 1 = argmax P (2) P M ′ I′ , f J ) I ′ exp λ h (e I e1 ′ m m m=1 e 1 1 exp P M 1 1 Introduction We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task. One is a phrase-based translation in which a phrasal unit is employed for translation (Koehn et al., 2003). The other is a hierarchical phrase-based translation in which translation is realized as a set of paired production rules (Chiang, 2005). Section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions. We also explored three types of corpus preprocessing in Section 3. As expected, different tokenization would lead to different word alignments which, in turn, resulted in the divergence of the extracted phrase/rule size. In our method, In this framework, the posterior probability Pr(e1I |f1J ) is directly maximized using a log-linear combination of feature functions hm (e1I , f1J ), such as a ngram language model or a translation model. When decoding, the denominator is dropped"
W06-3115,N03-1017,0,0.292419,"Models We used a log-linear approach (Och and Ney, 2002) in which a foreign language sentence f1J = f1 , f2 , ... fJ is translated into another language, i.e. English, e1I = e1 , e2 , ..., eI by seeking a maximum likelihood solution of eˆ 1I = argmax Pr(e1I |f1J ) (1) e1I  I , f J) λ h (e m m m=1 1 1 = argmax P (2) P M ′ I′ , f J ) I ′ exp λ h (e I e1 ′ m m m=1 e 1 1 exp P M 1 1 Introduction We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task. One is a phrase-based translation in which a phrasal unit is employed for translation (Koehn et al., 2003). The other is a hierarchical phrase-based translation in which translation is realized as a set of paired production rules (Chiang, 2005). Section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions. We also explored three types of corpus preprocessing in Section 3. As expected, different tokenization would lead to different word alignments which, in turn, resulted in the divergence of the extracted phrase/rule size. In our method, In this framework, the posterior probability Pr(e1I |f1J ) is directly maximized using a log-linear combinati"
W06-3115,P02-1038,0,0.483885,"resent two translation systems experimented for the shared-task of “Workshop on Statistical Machine Translation,” a phrase-based model and a hierarchical phrase-based model. The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronousCFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrasebased model performed very comparable to the phrase-based model. We also report a phrase/rule extraction technique differentiating tokenization of corpora. 2 Translation Models We used a log-linear approach (Och and Ney, 2002) in which a foreign language sentence f1J = f1 , f2 , ... fJ is translated into another language, i.e. English, e1I = e1 , e2 , ..., eI by seeking a maximum likelihood solution of eˆ 1I = argmax Pr(e1I |f1J ) (1) e1I  I , f J) λ h (e m m m=1 1 1 = argmax P (2) P M ′ I′ , f J ) I ′ exp λ h (e I e1 ′ m m m=1 e 1 1 exp P M 1 1 Introduction We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task. One is a phrase-based translation in which a phrasal unit is employed for translation (Koehn et al., 2003). The other is a hierarchical phrase-b"
W06-3115,J03-1002,0,0.0144663,"h sentence. In the hierarchical phrase-based model, decoding is realized as an Earley-style top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006). The major difference to the phrase-based model’s decoder is the handling of non-terminals, or holes, in each rule. 2.1 2.3 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004). Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003). The method exhaustively exj+m tracts phrase pairs ( f j , ei+n i ) from a sentence pair I J ( f1 , e1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases (Chiang, 2005): Our phrase-based model uses a standard pharaoh feature functions listed as follows (Koehn et al., 2003): • Re"
W06-3115,J04-4002,0,0.0486513,"e top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006). The major difference to the phrase-based model’s decoder is the handling of non-terminals, or holes, in each rule. 2.1 2.3 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004). Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003). The method exhaustively exj+m tracts phrase pairs ( f j , ei+n i ) from a sentence pair I J ( f1 , e1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases (Chiang, 2005): Our phrase-based model uses a standard pharaoh feature functions listed as follows (Koehn et al., 2003): • Relative-count based phrase translation probabilities in both directions. • Lexically weight"
W06-3115,P06-1098,1,0.825285,"ociated with ∼. existing hypothesis, new hypothesis is generated by consuming a phrase translation pair that covers untranslated foreign word positions. The score for the newly generated hypothesis is updated by combining the scores of feature functions described in Section 2.3. The English side of the phrase is simply concatenated to form a new prefix of English sentence. In the hierarchical phrase-based model, decoding is realized as an Earley-style top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006). The major difference to the phrase-based model’s decoder is the handling of non-terminals, or holes, in each rule. 2.1 2.3 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004). Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003). The method exhaustively exj+m tracts phrase pairs"
W06-3115,N04-1021,0,\N,Missing
W06-3115,P03-1021,0,\N,Missing
W09-3917,D08-1040,0,0.0839349,"stening-oriented dialogues and casual conversation have characteristically different dialogue flows and that it is important for listening agents to selfdisclose before asking questions and to utter more questions and acknowledgment than in casual conversation to be good listeners. 1 Introduction Although task-oriented dialogue systems have been actively researched over the years (Walker et al., 2001), systems that perform more flexible (less taskoriented) dialogues such as chats are beginning to be actively investigated from their social and entertainment aspects (Bickmore and Cassell, 2001; Higuchi et al., 2008). This paper deals with dialogues in which one conversational participant attentively listens to the other (hereafter, listening-oriented dialogue). Our aim is to build listening agents that can implement such a listening process so that a user can satisfy his/her desire to speak and have him/herself heard. Such agents would lead the user’s state of mind for the better as in a therapy session, although we want our listening agents to help users mentally in everyday conversation. It should also be noted that the purpose of the listening-oriented dialogue is to simply listen to users, not to eli"
W09-3917,P01-1066,0,0.0246529,"both listening-oriented dialogues and casual conversation, and analyzed them by comparing the frequency of dialogue acts, as well as the dialogue flows using Hidden Markov Models (HMMs). The analysis revealed that listening-oriented dialogues and casual conversation have characteristically different dialogue flows and that it is important for listening agents to selfdisclose before asking questions and to utter more questions and acknowledgment than in casual conversation to be good listeners. 1 Introduction Although task-oriented dialogue systems have been actively researched over the years (Walker et al., 2001), systems that perform more flexible (less taskoriented) dialogues such as chats are beginning to be actively investigated from their social and entertainment aspects (Bickmore and Cassell, 2001; Higuchi et al., 2008). This paper deals with dialogues in which one conversational participant attentively listens to the other (hereafter, listening-oriented dialogue). Our aim is to build listening agents that can implement such a listening process so that a user can satisfy his/her desire to speak and have him/herself heard. Such agents would lead the user’s state of mind for the better as in a the"
W10-1736,P07-1091,0,0.326335,"Missing"
W10-1736,2006.amta-papers.16,0,0.0396519,"Missing"
W10-1736,J03-1002,0,0.00303273,", 3, 2, 6] from this alignment. When the same word (or derivative words) appears twice or more in a single English sentence, two or more non-consecutive words in the English sentence are aligned to a single Japanese word: 3.1 Rough evaluation of reordering rate of change of speed NULL ({}) sokudo ({5}) henka ({3}) no ({2 4}) wariai ({1}) First, we examined rank correlation between Head Final English sentences produced by the Head Finalization rule and Japanese reference sentences. Since we do not have handcrafted word alignment data for an English-to-Japanese bilingual corpus, we used GIZA++ (Och and Ney, 2003) to get automatic word alignment. Based on this automatic word alignment, we measured Kendall’s τ for the word order between HFE sentences and Japanese sentences. Kendall’s τ is a kind of rank correlation measure defined as follows. Suppose a list of integers such as L = [2, 1, 3, 4]. The number of all integer pairs in this list is 4 C2 = 4 × 3/(2 × 1) = 6. The number of increasing pairs is five: (2, 3), (2, 4), (1, 3), (1, 4), and (3, 4). Kendall’s τ is defined by τ= We excluded the ambiguously aligned words (2 from the calculation of τ . We use only [5, 3, 1] and get τ = −1.0. The exclusion"
W10-1736,P05-1022,0,0.0389233,"Missing"
W10-1736,P02-1040,0,0.107213,"ecided to stop swapping them at coordination nodes, which are indicated cat and xcat attributes of the Enju output. We call this the coordination exception rule. In addition, we avoid Enju’s splitting of numerical expressions such as “12,345” and “(1)” because this splitting leads to inappropriate word orders. 246 3 Experiments John va1 a ball va2 hit . NULL ({3}) jon ({1}) wa ({2}) bohru ({4}) wo ({5}) utta ({6}) . ({7}) In order to show how closely our Head Finalization makes English follow Japanese word order, we measured Kendall’s τ , a rank correlation coefficient. We also measured BLEU (Papineni et al., 2002) and other automatic evaluation scores to show that Head Finalization can actually improve the translation quality. We used NTCIR7 PAT-MT’s Patent corpus (Fujii et al., 2008). Its training corpus has 1.8 million sentence pairs. We used MeCab (http:// mecab.sourceforge.net/) to segment Japanese sentences. Then, we get [1, 2, 4, 5, 6, 7] and τ = 1.0. We use τ or the average of τ over all training sentences to observe the tendency. Sometimes, one Japanese word corresponds to an English phrase: John went to Costa Rica . NULL ({}) jon ({1}) wa ({}) kosutarika ({4 5}) ni ({3}) itta ({2}) . ({6}) We"
W10-1736,P05-1066,0,0.769816,"Missing"
W10-1736,P05-1034,0,0.166265,"Missing"
W10-1736,de-marneffe-etal-2006-generating,0,0.00411069,"Missing"
W10-1736,2006.amta-papers.25,0,0.0277013,"it (http:// 5 Discussion Our method used an HPSG parser, which gives rich information, but it is not easy to build such a parser. It is much easier to build word dependency parsers and Penn Treebank-style parsers. In order use these parsers, we have to add some heuristic rules. www.mibel.cs.tsukuba.ac.jp/norimatsu/ bleu kit/) following the PATMT’s overview paper (Fujii et al., 2008). The table shows that dl=6 gives the best result, and even dl=0 (no reordering in Moses) gives better scores than the organizers’ Moses. Table 2 also shows Word Error Rates (WER) and Translation Error Rates (TER) (Snover et al., 2006). Since they are error rates, smaller is better. Although the improvement of BLEU is not very impressive, the score of WER is greatly reduced. This difference comes from the fact that BLEU measures only local word order, while WER mea5.1 Word Dependency Parsers At first, we thought that we could substitute a word dependency parser for Enju by simply rephrasing a head with a modified word. Xu et al. (2009) used a semantic head-based dependency parser for a similar purpose. Even when we use a syntactic head-based dependency parser instead, we encountered their ‘excessive movement’ problem. A str"
W10-1736,N07-1007,0,0.0105596,"ad c4 appears before c5, so c4 and c5 are swapped. The lower picture shows the swapped result. Then we get John a ball hit, which has the same word order as its Japanese translation jon wa bohru wo utta except for the functional words a, wa, and wo. We have to add Japanese particles wa (topic marker) or ga (nominative case marker) for John and wo (objective case marker) for ball to get an acceptable Japanese sentence. It is well known that SMT is not good at generating appropriate particles from English, whitch does not have particles. Particle generation was tackled by a few research groups (Toutanova and Suzuki, 2007; Hong et al., 2009). Here, we use Enju’s output to generate seeds Figure 1: Enju’s XML output (some attributes are removed for readability). c0 ? c1 ? c2 ? t0 John c3 ? c6 ? t2 a c4 ? t1 hit c0 c1 ? c2 ? t0 John jon (wa) Original English ? c5 ? c5 ? c7 ? t3 ball Head Final English c3 ? c7 c6 c4 ? ? ? t3 t2 t1 a ball hit – bohru (wo) utta Figure 2: Head Finalization of a simple sentence (? indicates a head). 245 0 Original English ? ? 4 ? 1? 2 John 5 went 6 ? 7 to 8 9 the 0 13 3 ? 10 police 11 ? 12 because 13 14 ? 15 Mary ? 16 ? 19 his 17 lost 18 ? 20 wallet Head Final English ? 3 11 ? ? ? 16"
W10-1736,C04-1073,0,0.835753,"Missing"
W10-1736,N09-1028,0,0.772582,"ral level. Why do we think this works? The reason is simple: Japanese is a typical head-final language. That is, a syntactic head word comes after nonhead (dependent) words. SOV is just one aspect of head-final languages. In order to implement this idea, we need a parser that outputs syntactic heads. Enju is such a parser from the University of Tokyo (http://www-tsujii.is.s. u-tokyo.ac.jp/enju). We discuss other parsers in section 5. There is another kind of head: semantic heads. Hong et al. (2009) used Stanford parser (de Marneffe et al., 2006), which outputs semantic headbased dependencies; Xu et al. (2009) also used the same representation. The use of syntactic heads and the number of dependents are essential for the simplicity of English is a typical SVO (Subject-VerbObject) language, while Japanese is a typical SOV language. Conventional Statistical Machine Translation (SMT) systems work well within each of these language families. However, SMT-based translation from an SVO language to an SOV language does not work well because their word orders are completely different. Recently, a few groups have proposed rulebased preprocessing methods to mitigate this problem (Xu et al., 2009; Hong et al."
W10-1736,P09-2059,0,0.540137,"ider part-of-speech tags or rule weights because the powerful Enju parser allows us to implement the rule at a general level. Why do we think this works? The reason is simple: Japanese is a typical head-final language. That is, a syntactic head word comes after nonhead (dependent) words. SOV is just one aspect of head-final languages. In order to implement this idea, we need a parser that outputs syntactic heads. Enju is such a parser from the University of Tokyo (http://www-tsujii.is.s. u-tokyo.ac.jp/enju). We discuss other parsers in section 5. There is another kind of head: semantic heads. Hong et al. (2009) used Stanford parser (de Marneffe et al., 2006), which outputs semantic headbased dependencies; Xu et al. (2009) also used the same representation. The use of syntactic heads and the number of dependents are essential for the simplicity of English is a typical SVO (Subject-VerbObject) language, while Japanese is a typical SOV language. Conventional Statistical Machine Translation (SMT) systems work well within each of these language families. However, SMT-based translation from an SVO language to an SOV language does not work well because their word orders are completely different. Recently,"
W10-1736,P01-1067,0,0.267073,"Missing"
W10-1757,W06-1615,0,0.0749079,"for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsity leads to overfitting. We addressed this by re-casting N-best lists as multitask learning data. Our MT experiments show consistent statistically significant improvements. From the Bayesian view, multitask formulation of N-best"
W10-1757,W09-2201,0,0.0321418,"r better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsi"
W10-1757,N09-1025,0,0.167697,"tanabe et al., 2007) defines feature templates based on bilingual word alignments, which lead to extraction of heavilylexicalized features of the form: 2. The input (f ) has high variability (e.g. large vocabulary size), so that features for different inputs are rarely shared. 3. The N-best list output also exhibits high variability (e.g. many different word reorderings). Larger N may improve reranking performance, but may also increase feature sparsity. When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail. Our goal here is to address this situation. 3 Proposed Reranking Framework h(e, f ) =  1       0 if foreign word “Monsieur” and English word “Mr.” co-occur in e,f otherwise In the following, we first give an intuitive comparison between single vs. multiple task learning (Section 3.1), before presenting the general metaalgorithm (Section 3.2) and particular instantiations (Section 3.3). (2) One can imagine that such features are sparse because it may only fire for input sentences that contain the word “Monsieur”. For all other input sentences, it is an useless, inactive featur"
W10-1757,J07-2003,0,0.0561179,"Missing"
W10-1757,J05-1003,0,0.264845,"f features. 1. We introduce the idea of viewing N-best reranking as a multitask learning problem. This view is particularly apt to any general reranking problem with sparse feature sets. 2. We propose a simple meta-algorithm that first discovers common feature representations across N-bests (via multitask learning) before training a conventional reranker. Thus it is easily applicable to existing systems. 1 Introduction Many natural language processing applications, such as machine translation (MT), parsing, and language modeling, benefit from the N-best reranking framework (Shen et al., 2004; Collins and Koo, 2005; Roark et al., 2007). The advantage of N-best reranking is that it abstracts away the complexities of first-pass decoding, allowing the researcher to try new features and learning algorithms with fast experimental turnover. In the N-best reranking scenario, the training data consists of sets of hypotheses (i.e. N-best lists) generated by a first-pass system, along with their labels. Given a new N-best list, the goal is to rerank it such that the best hypothesis appears near the top of the list. Existing research have focused on training a single reranker directly on the 3. We demonstrate that"
W10-1757,N09-1068,0,0.0391788,"gularizer to ensure that the learned functions of related tasks are close to each other. The popular ℓ1 /ℓ2 objective can be optimized by various methods, such as boosting (Obozinski et al., 2009) and convex programming (Argyriou et al., 2008). Yet another regularizer is the ℓ1 /ℓ∞ norm (Quattoni et al., 2009), which replaces the 2-norm with a max. One could also define a regularizer to ensure i that each task-specific to some average P wi is close parameter, e.g. i ||w − wavg ||2 . If we interpret wavg as a prior, we begin to see links to Hierarchical Bayesian methods for multitask learning (Finkel and Manning, 2009; Daume, 2009). 2. Shared Subspace: This approach assumes that there is an underlying feature subspace that is common to all tasks. Early works on multitask learning implement this by neural networks, where different tasks have different output layers but share the same hidden layer (Caruana, 1997). Another method is to write the weight vector as two parts w = [u; v] and let the task-specific function be uT · h(e, f ) + vT · Θ · h(e, f ) (Ando and Zhang, 2005). Θ is a D ′ × D matrix that maps the original features to a subspace common to all tasks. The new feature representation is computed by"
W10-1757,W08-0804,0,0.0274275,"res are shared: Wa : » – 4 0 0 4 0 3 3 0 4 4 3 3 → 14 Wb : » – 4 0 3 4 0 3 0 0 4 5 3 0 → 12 2 In MT, evaluation metrics like BLEU do not exactly decompose across sentences, so for some training algorithms this loss is an approximation. [optional] RandomHashing({Hi }) W = MultitaskLearn({(Hi , yi )}) hc = ExtractCommonFeature(W) {Hic } = RemapFeature({Hi }, hc ) wc = ConventionalReranker({(Hic , yi )}) The first step, random hashing, is optional. Random hashing is an effective trick for reducing the dimension of sparse feature sets without suffering losses in fidelity (Weinberger et al., 2009; Ganchev and Dredze, 2008). It works by collapsing random subsets of features. This step can be performed to speed-up multitask learning later. In some cases, the original feature dimension may be so large that hashed representations may be necessary. The next two steps are key. A multitask learning algorithm is run on the N-best lists, and a common feature space shared by all lists is extracted. For example, if one uses the multitask objective of Eq. 5, the result of step 2 is a set of weights W. ExtractCommonFeature(W) then returns the feature id’s (either from original or hashed representation) that receive nonzero"
W10-1757,P09-1114,0,0.0254945,"help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfor"
W10-1757,P05-1024,1,0.783566,"engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jo"
W10-1757,N04-1022,0,0.0383443,"Missing"
W10-1757,P06-1096,0,0.0716939,"., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on p"
W10-1757,P05-1012,0,0.149052,", 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing. Evaluation campaigns like WMT (Callison-Burch et al., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within m"
W10-1757,N04-1021,0,0.102794,"Missing"
W10-1757,P02-1040,0,0.0791554,"Missing"
W10-1757,2009.iwslt-evaluation.1,0,0.0128431,"feature sets, with corresponding feature size and train/test BLEU/PER. All multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency features also give significant improvements over the high frequency features alone. method. Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing. Evaluation campaigns like WMT (Callison-Burch et al., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Ch"
W10-1757,P08-1098,0,0.0306633,"me applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsity leads to overfitting. We addressed this by re-casting N-best lists as multitask learning data. Our MT experiments show con"
W10-1757,N04-1023,0,0.65709,"nvolving millions of features. 1. We introduce the idea of viewing N-best reranking as a multitask learning problem. This view is particularly apt to any general reranking problem with sparse feature sets. 2. We propose a simple meta-algorithm that first discovers common feature representations across N-bests (via multitask learning) before training a conventional reranker. Thus it is easily applicable to existing systems. 1 Introduction Many natural language processing applications, such as machine translation (MT), parsing, and language modeling, benefit from the N-best reranking framework (Shen et al., 2004; Collins and Koo, 2005; Roark et al., 2007). The advantage of N-best reranking is that it abstracts away the complexities of first-pass decoding, allowing the researcher to try new features and learning algorithms with fast experimental turnover. In the N-best reranking scenario, the training data consists of sets of hypotheses (i.e. N-best lists) generated by a first-pass system, along with their labels. Given a new N-best list, the goal is to rerank it such that the best hypothesis appears near the top of the list. Existing research have focused on training a single reranker directly on the"
W10-1757,P09-1054,0,0.143227,"Missing"
W10-1757,D07-1080,1,0.936069,") is a D-dimensional feature vector, w is the weight vector to be trained, and N (f ) is the set of likely translations of f , i.e. the N-best list. The feature h(e, f ) can be any quantity defined in terms of the sentence pair, such as translation model and language model probabilities. Here we are interested in situations where the feature definitions can be quite sparse. A common methodology in reranking is to first design feature templates based on linguistic intuition and domain knowledge. Then, numerous features are instantiated based on the training data seen. For example, the work of (Watanabe et al., 2007) defines feature templates based on bilingual word alignments, which lead to extraction of heavilylexicalized features of the form: 2. The input (f ) has high variability (e.g. large vocabulary size), so that features for different inputs are rarely shared. 3. The N-best list output also exhibits high variability (e.g. many different word reorderings). Larger N may improve reranking performance, but may also increase feature sparsity. When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009)"
W10-1757,zhang-etal-2004-interpreting,0,0.0226462,"“de”) or special characters (such as numeral symbol and punctuation). These are features that can be expected to be widely applicable, and it is promising that multitask learning is able to recover these from the millions of potential features. 10 3. All three multitask methods obtained features that outperformed the baseline. The BLEU scores are 28.8, 28.9, 29.1 for Unsupervised Feature Selection, Joint Regularization, and Shared Subspace, respectively, which all outperform the 28.6 baseline. All improvements are statistically significant by bootstrap sampling test (1000 samples, p &lt; 0.05) (Zhang et al., 2004). 300 4. Shared Subspace performed the best. We conjecture this is because its feature projection can create new feature combinations that is more expressive than the feature selection used by the two other methods. Bootstrap samples 250 50 0 −0.2 Wabbit 1.2 5 Related Work in NLP Previous reranking work in NLP can be classified into two different research focuses: 1. Engineering better features: In MT, (Och and others, 2004) investigates features extracted from a wide variety of syntactic representations, such as parse tree probability on the outputs. Although their results show that the propo"
W10-1757,W09-0401,0,\N,Missing
W10-1757,P05-1022,0,\N,Missing
W10-1757,W09-0438,0,\N,Missing
W14-3335,D13-1139,0,0.132672,"ears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. The term “scrambling” stands for these acceptable permutations. These case markers explicitly show grammatical cases and reordering of them does not hurt interpretation of these sentences. Almost all other permutations of words are not acceptable (∗). RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 o-sushi wo tabe-ta in order to penalize too short sentences. α sushi-ya de ∗ jon ga de sushi-ya o-sushi tabe-ta wo . ∗ jon de sushi-ya ga o-sushi wo tabe-ta . ∗ jon tabe-ta ga o-sushi wo sushi-ya de . ∗ sushi-ya ga jon tabe-ta de o-sushi wo . Most readers unfamiliar with Japanese will not understand which word order is acceptable. 2.1"
W14-3335,D10-1092,1,0.901921,"t al., 2013). Table 2 shows the result of metaevaluation at NICTR-9/10 PatentMT. The table shows that RIBES is more reliable than BLEU and NIST. Current RIBES has the following improvements. Introduction Statistical Machine Translation has grown with an automatic evaluation method BLEU (Papineni et al., 2002). BLEU measures local word order by ngrams and does not care about global word order. In JE/EJ translations, this insensitivity degrades BLEU’s correlation with human judgements. Therefore, alternative automatic evaluation methods are proposed. Echizen-ya and Araki (2007) proposed IMPACT. Isozaki et al. (2010) presented the idea of RIBES. Hirao et al. (2011) named this method “RIBES” (Rank-based Intuitive Bilingual Evaluation Score). This version of RIBES was defined as follows: RIBES = NKT × P α • BLEU’s Brevity Penalty (BP) was introduced 287 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 287–292, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics jon ga Table 2: Meta-evaluation at NTCIR-9/10 PatentMT (Spearman’s ρ, Goto et al. 2011, 2013) BLEU NIST RIBES NTCIR-9 JE −0.042 −0.114 0.632 NTCIR-9 EJ −0.029 −0.074 0.716 NTCIR-10 JE"
W14-3335,W05-0909,0,0.0892898,"between RIBES and human-judged adequacy. 1 BLEU METEOR ROUGE-L IMPACT RIBES 0.515 0.490 0.903 0.826 0.947 where NKT (Normalized Kendall’s τ ) is defined by (τ + 1)/2. This NKT is used for measuring word order similarity between a reference sentence and an MT output sentence. Thus, RIBES penalizes difference of global word order. P is precision of unigrams. RIBES is defined for each test sentence and averaged RIBES is used for evaluating the entire test corpus. Table 1 is a table in an IWSLT-2012 invited talk (http://hltc.cs.ust.hk/iwslt/slides/ Isozaki2012 slides.pdf). METEOR was proposed by Banerjee and Lavie (2005). ROUGE-L was proposed by Lin and Och (2004). According to this table, RIBES with α = 0.2 has a very strong correlation (Spearman’s ρ = 0.947) with human-judged adequacy. For each sentence, we use the average of adequacy scores of three judges. Here, we call this average “Adequacy”. We focus on Adequacy because current SMT systems tend to output inadequate sentences. Note that only single reference translations are available for this task although use of multiple references is common for BLEU. RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standar"
W14-3335,W02-2016,0,0.0829415,"We use only single reference translations provided by the NTCIR organizers. • postOrder: We generate all permutations of the given reference sentence generated by post-order traversals of its dependency tree. This can be achieved by the following two steps. First, we enumerate all permutations of child nodes at each node. Then, we combine these permutations. This is implemented by cartesian products of the permutation sets. 3 Results We applied the above four methods to the reference sentences of human-judged 100 sentences of NTCIR-7 Patent MT EJ task. (Fujii et al., 2008) We applied CaboCha (Kudo and Matsumoto, 2002) to the reference sentences, and manually corrected the dependency trees because Japanese dependency parsers are not satisfactory in terms of sentence accuracy (Tamura et al., 2007). To support this manual correction, CaboCha’s XML output was automatically converted to dependency tree pictures by using cabochatrees package for LATEX. http:// • caseMarkers: We reorder only “case marker (kaku joshi) phrases”. Here, a “case marker phrase” is post-order traversal of a subtree rooted at a case marker bunsetsu. For instance, the root of the following sentence S3 has a non-case marker child “kaburi ,"
W14-3335,W12-4207,0,0.0203146,"ne sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. The term “scrambling” stands for these acceptable permutations. These case markers explicitly show grammatical cases and reordering of them does not hurt interpretation of these sentences. Almost all other permutations of words are not acceptable (∗). RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 o-sushi wo tabe-ta in order to penalize too short sentences. α sushi-ya de ∗ jon ga de sushi-ya o-sushi tabe-ta wo . ∗ jon de sushi-ya ga o-sushi wo tabe-ta . ∗ jon tabe-ta ga o-sushi wo sushi-ya de . ∗ sushi-ya ga jon tabe-ta de o-sushi wo . Most readers unfamiliar with Japanese will not understand which word order is acceptable. 2.1 Scrambling as Post-"
W14-3335,P04-1077,0,0.056877,"TEOR ROUGE-L IMPACT RIBES 0.515 0.490 0.903 0.826 0.947 where NKT (Normalized Kendall’s τ ) is defined by (τ + 1)/2. This NKT is used for measuring word order similarity between a reference sentence and an MT output sentence. Thus, RIBES penalizes difference of global word order. P is precision of unigrams. RIBES is defined for each test sentence and averaged RIBES is used for evaluating the entire test corpus. Table 1 is a table in an IWSLT-2012 invited talk (http://hltc.cs.ust.hk/iwslt/slides/ Isozaki2012 slides.pdf). METEOR was proposed by Banerjee and Lavie (2005). ROUGE-L was proposed by Lin and Och (2004). According to this table, RIBES with α = 0.2 has a very strong correlation (Spearman’s ρ = 0.947) with human-judged adequacy. For each sentence, we use the average of adequacy scores of three judges. Here, we call this average “Adequacy”. We focus on Adequacy because current SMT systems tend to output inadequate sentences. Note that only single reference translations are available for this task although use of multiple references is common for BLEU. RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standard quality measure in recent NTCIR PatentMT t"
W14-3335,2007.mtsummit-papers.21,0,0.665733,"CIR PatentMT tasks (Goto et al., 2011; Goto et al., 2013). Table 2 shows the result of metaevaluation at NICTR-9/10 PatentMT. The table shows that RIBES is more reliable than BLEU and NIST. Current RIBES has the following improvements. Introduction Statistical Machine Translation has grown with an automatic evaluation method BLEU (Papineni et al., 2002). BLEU measures local word order by ngrams and does not care about global word order. In JE/EJ translations, this insensitivity degrades BLEU’s correlation with human judgements. Therefore, alternative automatic evaluation methods are proposed. Echizen-ya and Araki (2007) proposed IMPACT. Isozaki et al. (2010) presented the idea of RIBES. Hirao et al. (2011) named this method “RIBES” (Rank-based Intuitive Bilingual Evaluation Score). This version of RIBES was defined as follows: RIBES = NKT × P α • BLEU’s Brevity Penalty (BP) was introduced 287 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 287–292, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics jon ga Table 2: Meta-evaluation at NTCIR-9/10 PatentMT (Spearman’s ρ, Goto et al. 2011, 2013) BLEU NIST RIBES NTCIR-9 JE −0.042 −0.114 0.632 NTC"
W14-3335,D12-1077,0,0.0142759,"or disambiguation when the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. The term “scrambling” stands for these acceptable permutations. These case markers explicitly show grammatical cases and reordering of them does not hurt interpretation of these sentences. Almost all other permutations of words are not acceptable (∗). RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 o-sushi wo tabe-ta in order to penalize too short sentences. α sushi-ya de ∗ jon ga de sushi-ya o-sushi tabe-ta wo . ∗ jon de sushi-ya ga o-sushi wo tabe-ta . ∗ jon tabe-ta ga o-sushi wo sushi-ya de . ∗ sushi-ya ga jon tabe-ta de o-sushi wo . Most readers unfamiliar with Japanese will not under"
W14-3335,P02-1040,0,0.100355,"ences. Note that only single reference translations are available for this task although use of multiple references is common for BLEU. RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standard quality measure in recent NTCIR PatentMT tasks (Goto et al., 2011; Goto et al., 2013). Table 2 shows the result of metaevaluation at NICTR-9/10 PatentMT. The table shows that RIBES is more reliable than BLEU and NIST. Current RIBES has the following improvements. Introduction Statistical Machine Translation has grown with an automatic evaluation method BLEU (Papineni et al., 2002). BLEU measures local word order by ngrams and does not care about global word order. In JE/EJ translations, this insensitivity degrades BLEU’s correlation with human judgements. Therefore, alternative automatic evaluation methods are proposed. Echizen-ya and Araki (2007) proposed IMPACT. Isozaki et al. (2010) presented the idea of RIBES. Hirao et al. (2011) named this method “RIBES” (Rank-based Intuitive Bilingual Evaluation Score). This version of RIBES was defined as follows: RIBES = NKT × P α • BLEU’s Brevity Penalty (BP) was introduced 287 Proceedings of the Ninth Workshop on Statistical"
W14-3335,D07-1063,0,0.0249916,"f its dependency tree. This can be achieved by the following two steps. First, we enumerate all permutations of child nodes at each node. Then, we combine these permutations. This is implemented by cartesian products of the permutation sets. 3 Results We applied the above four methods to the reference sentences of human-judged 100 sentences of NTCIR-7 Patent MT EJ task. (Fujii et al., 2008) We applied CaboCha (Kudo and Matsumoto, 2002) to the reference sentences, and manually corrected the dependency trees because Japanese dependency parsers are not satisfactory in terms of sentence accuracy (Tamura et al., 2007). To support this manual correction, CaboCha’s XML output was automatically converted to dependency tree pictures by using cabochatrees package for LATEX. http:// • caseMarkers: We reorder only “case marker (kaku joshi) phrases”. Here, a “case marker phrase” is post-order traversal of a subtree rooted at a case marker bunsetsu. For instance, the root of the following sentence S3 has a non-case marker child “kaburi ,” (wear) between case marker children, “jon ga” and “zubon wo” (Trousers are the object). Figure 3 shows its dependency tree. jon ga shiroi boushi wo kaburi , kuroi zubon wo hai te"
W14-3335,P12-2061,0,0.0237623,"n the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. The term “scrambling” stands for these acceptable permutations. These case markers explicitly show grammatical cases and reordering of them does not hurt interpretation of these sentences. Almost all other permutations of words are not acceptable (∗). RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 o-sushi wo tabe-ta in order to penalize too short sentences. α sushi-ya de ∗ jon ga de sushi-ya o-sushi tabe-ta wo . ∗ jon de sushi-ya ga o-sushi wo tabe-ta . ∗ jon tabe-ta ga o-sushi wo sushi-ya de . ∗ sushi-ya ga jon tabe-ta de o-sushi wo . Most readers unfamiliar with Japanese will not understand which word or"
W14-3335,P12-3022,0,0.0153841,"ed only bigrams for disambiguation when the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. The term “scrambling” stands for these acceptable permutations. These case markers explicitly show grammatical cases and reordering of them does not hurt interpretation of these sentences. Almost all other permutations of words are not acceptable (∗). RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 o-sushi wo tabe-ta in order to penalize too short sentences. α sushi-ya de ∗ jon ga de sushi-ya o-sushi tabe-ta wo . ∗ jon de sushi-ya ga o-sushi wo tabe-ta . ∗ jon tabe-ta ga o-sushi wo sushi-ya de . ∗ sushi-ya ga jon tabe-ta de o-sushi wo . Most readers unfamiliar with Ja"
W15-3058,W08-0309,0,0.101047,"Missing"
W15-3058,W12-3102,0,0.0865422,"Missing"
W15-3058,P08-1007,0,0.0303177,"p w/o correction Figure 6: Effects of manual correction on compDep’s correlation with adequacy (NTCIR-9 EJ) new idea. Liu and Gildea (2005) compared parse trees of reference sentences and MT output sentences. They proposed four methods: STM, TKM, HWCM, DSTM, and DTKM. STM measures similarity by the number of matching subtrees. TKM uses Tree Kernel for the measurement. HWCM uses n-gram matches in dependency trees. DSTM and DTKM are dependency tree versions of STM and TKM respectively. Owczarzak et al. (2007) used LFG-based typed dependency trees. They also introduced processing of paraphrases. Chan and Ng (2008) proposed M AX S IM that is based on a bipartite graph matching algorithm and assigns different weights to matches. Dependency relation are used as a factor in this framework. Zhu et al. (2010) proposed an SVM-based MT metric that uses different features in different granularities. Dependency relations are used as a feature in this framework. We designed our method not to parse MT outputs because some MT outputs are broken and it is difficult to parse them. Our method does not parse MT outputs and we expect our method is more robust than these methods. Recently, Yu et al. (2014) proposed RED,"
W15-3058,W13-3707,0,0.0165679,"4. Tokyo de PC wo John ga katta . (0.71) 5. PC wo John ga Tokyo de katta . (0.71) 6. PC wo Tokyo de John ga katta . (0.57) katta All of the above sentences are acceptable and have the same meaning, and this is called “scrambling”. However, RIBES outputs different scores for these sentences. When we use the first one as the reference sentence, RIBES output scores in the parentheses. Human judges will give almost equal scores to all of them, and we should improve these RIBES scores for better evaluation. Scrambling is also observed in other languages such as German (Maier et al., 2014), Korean (Chun, 2013), Turkish (ldız et al., 2014), Hindi (Sharma and Paul, 2014), etc. Figure 2 (a) shows the dependency tree of “John ga Tokyo de PC wo katta”. Each box indicates a bunsetsu (chunk). Arrows indicate modification relations. The source node of an arrow modifies the target node of the arrow. The root “katta” has three modifiers (children), “John ga”, “Tokyo de”, and “PC wo”. We can generate 3! = 6 word orders by post-order traversal of this tree because the order of siblings does not matter. Figure 2 (b) shows a permutation and its dependency tree. In this case, all permutations are acceptable. Howe"
W15-3058,P10-1012,0,0.021047,"ndency tree in Figure 2 (d). (a) “John ga Tokyo de PC wo katta” Tokyo de PC wo John ga katta (b) “PC wo Tokyo de John ga katta” John ga PC wo katta ato ni Alice kara denwa ga atta (c) “John ga PC wo katta ato ni Alice kara denwa ga atta” Alice kara John ga PC wo katta denwa ga ato ni atta (d) “Alice kara John ga PC wo katta ato ni denwa ga atta” Figure 2: Dependency trees However, improvement of sentence-level correlation is more difficult than system-level correlation and current automatic evaluation metrics do not have strong correlation. (Leusch et al., 2003; Stanojevi´c and Sima’an, 2014; Echizen-ya and Araki, 2010; Callison-Burch et al., 2012) 1.1 Scrambling As for Japanese translation, however, we should consider “scrambling” or acceptable reordering of phrases. For example, “John ga Tokyo de PC wo katta” (John bought a PC in Tokyo) consists of the main verb “katta” (bought) and its modifiers. “Ga”, “de”, and “wo” are case markers. 1.2 Rule-based filtering of bad sentences Isozaki et al. (2014) tried to solve the above problem by automatic generation of reordered sentences and use of a heuristic rule (constraint) to filter out bad sentences. • “Ga” is a nominative case marker. • “De” is a locative cas"
W15-3058,D10-1092,1,0.776391,"an, Korean, NTCIR-7 JE RIBES JE BLEU NTCIR-9 JE RIBES JE BLEU EJ RIBES EJ BLEU NTCIR-10 JE RIBES JE BLEU EJ RIBES EJ BLEU 0.0 0.2 0.4 0.6 0.8 1.0 Figure 1: RIBES has better correlation with adequacy than BLEU (system-level correlation) Turkish, Hindi, etc. 1 Introduction For translation among European languages, BLEU (Papineni et al., 2002) has strong correlation with human judgments and almost all MT papers use BLEU for evaluation of translation quality. However, BLEU has very weak correlation with human judgments in English-toJapanese/Japanese-to-English translation, and a new metric RIBES (Isozaki et al., 2010; Hirao et al., 2014) has strong correlation with human judgments. RIBES measures similarity of the word order of a machine translated sentence and that of a human-translated reference sentence. Figure 1 compares RIBES and BLEU in terms of Spearman’s ρ with human judgments of adequacy based on NTCIR-7/9/10 data (Isozaki et al., 2010; Goto et al., 2011; Goto et al., 2013). Japanese and English have completely different word order, and phrase-based SMT systems tend to output bad word orders. RIBES correctly points out their word order problems. In this paper, we propose a method to improve “sent"
W15-3058,W14-3335,1,0.707083,"t of sentence-level correlation is more difficult than system-level correlation and current automatic evaluation metrics do not have strong correlation. (Leusch et al., 2003; Stanojevi´c and Sima’an, 2014; Echizen-ya and Araki, 2010; Callison-Burch et al., 2012) 1.1 Scrambling As for Japanese translation, however, we should consider “scrambling” or acceptable reordering of phrases. For example, “John ga Tokyo de PC wo katta” (John bought a PC in Tokyo) consists of the main verb “katta” (bought) and its modifiers. “Ga”, “de”, and “wo” are case markers. 1.2 Rule-based filtering of bad sentences Isozaki et al. (2014) tried to solve the above problem by automatic generation of reordered sentences and use of a heuristic rule (constraint) to filter out bad sentences. • “Ga” is a nominative case marker. • “De” is a locative case marker. • “Wo” is an accusative case marker. • Use a Japanese dependency parser to get dependency trees of reference sentences. • Check the dependency trees and manually correct wrong ones because sentence-level accuracy of dependency analyzers are still This sentence can be reordered as follows. 1. John ga Tokyo de PC wo katta . (1.00) 2. John ga PC wo Tokyo de katta . (0.86) 3. Toky"
W15-3058,P14-2019,0,0.0590965,"Missing"
W15-3058,2003.mtsummit-papers.32,0,0.0772783,"des “katta” (bought). This sentence will have a dependency tree in Figure 2 (d). (a) “John ga Tokyo de PC wo katta” Tokyo de PC wo John ga katta (b) “PC wo Tokyo de John ga katta” John ga PC wo katta ato ni Alice kara denwa ga atta (c) “John ga PC wo katta ato ni Alice kara denwa ga atta” Alice kara John ga PC wo katta denwa ga ato ni atta (d) “Alice kara John ga PC wo katta ato ni denwa ga atta” Figure 2: Dependency trees However, improvement of sentence-level correlation is more difficult than system-level correlation and current automatic evaluation metrics do not have strong correlation. (Leusch et al., 2003; Stanojevi´c and Sima’an, 2014; Echizen-ya and Araki, 2010; Callison-Burch et al., 2012) 1.1 Scrambling As for Japanese translation, however, we should consider “scrambling” or acceptable reordering of phrases. For example, “John ga Tokyo de PC wo katta” (John bought a PC in Tokyo) consists of the main verb “katta” (bought) and its modifiers. “Ga”, “de”, and “wo” are case markers. 1.2 Rule-based filtering of bad sentences Isozaki et al. (2014) tried to solve the above problem by automatic generation of reordered sentences and use of a heuristic rule (constraint) to filter out bad sentences. •"
W15-3058,W05-0904,0,0.0394216,"NTCIR-9 single ref compDep rule2014 postOrder 0.0 Spearman’s ρ with adequacy 0.2 0.4 0.6 0.8 1.0 NTCIR-7 single ref rule2014 compDep postOrder NTCIR-9 single ref rule2014 compDep postOrder Figure 5: System-level correlation with adequacy Adequacy tsbmt Moses NTT NICT-ATR kuro 3.527 2.897 2.740 2.587 2.420 Averaged RIBES single ref rule2014 compDep 0.722 0.726 0.750 0.707 0.720 0.745 0.670 0.682 0.722 0.658 0.667 0.706 0.633 0.643 0.711 0.4 0.6 0.8 1.0 single ref compDep compDep w/o correction Figure 6: Effects of manual correction on compDep’s correlation with adequacy (NTCIR-9 EJ) new idea. Liu and Gildea (2005) compared parse trees of reference sentences and MT output sentences. They proposed four methods: STM, TKM, HWCM, DSTM, and DTKM. STM measures similarity by the number of matching subtrees. TKM uses Tree Kernel for the measurement. HWCM uses n-gram matches in dependency trees. DSTM and DTKM are dependency tree versions of STM and TKM respectively. Owczarzak et al. (2007) used LFG-based typed dependency trees. They also introduced processing of paraphrases. Chan and Ng (2008) proposed M AX S IM that is based on a bipartite graph matching algorithm and assigns different weights to matches. Depen"
W15-3058,maier-etal-2014-discosuite,0,0.0277528,"tics. John ga Tokyo de PC wo 4. Tokyo de PC wo John ga katta . (0.71) 5. PC wo John ga Tokyo de katta . (0.71) 6. PC wo Tokyo de John ga katta . (0.57) katta All of the above sentences are acceptable and have the same meaning, and this is called “scrambling”. However, RIBES outputs different scores for these sentences. When we use the first one as the reference sentence, RIBES output scores in the parentheses. Human judges will give almost equal scores to all of them, and we should improve these RIBES scores for better evaluation. Scrambling is also observed in other languages such as German (Maier et al., 2014), Korean (Chun, 2013), Turkish (ldız et al., 2014), Hindi (Sharma and Paul, 2014), etc. Figure 2 (a) shows the dependency tree of “John ga Tokyo de PC wo katta”. Each box indicates a bunsetsu (chunk). Arrows indicate modification relations. The source node of an arrow modifies the target node of the arrow. The root “katta” has three modifiers (children), “John ga”, “Tokyo de”, and “PC wo”. We can generate 3! = 6 word orders by post-order traversal of this tree because the order of siblings does not matter. Figure 2 (b) shows a permutation and its dependency tree. In this case, all permutations"
W15-3058,W07-0411,0,0.0218909,"Missing"
W15-3058,P02-1040,0,0.10875,"sentences. The experimental results show that our method improves sentence-level correlation with human judgments. In addition, strong system-level correlation of single reference RIBES is not damaged very much. We expect this method can be applied to other languages such as German, Korean, NTCIR-7 JE RIBES JE BLEU NTCIR-9 JE RIBES JE BLEU EJ RIBES EJ BLEU NTCIR-10 JE RIBES JE BLEU EJ RIBES EJ BLEU 0.0 0.2 0.4 0.6 0.8 1.0 Figure 1: RIBES has better correlation with adequacy than BLEU (system-level correlation) Turkish, Hindi, etc. 1 Introduction For translation among European languages, BLEU (Papineni et al., 2002) has strong correlation with human judgments and almost all MT papers use BLEU for evaluation of translation quality. However, BLEU has very weak correlation with human judgments in English-toJapanese/Japanese-to-English translation, and a new metric RIBES (Isozaki et al., 2010; Hirao et al., 2014) has strong correlation with human judgments. RIBES measures similarity of the word order of a machine translated sentence and that of a human-translated reference sentence. Figure 1 compares RIBES and BLEU in terms of Spearman’s ρ with human judgments of adequacy based on NTCIR-7/9/10 data (Isozaki"
W15-3058,W14-5506,0,0.023885,"John ga Tokyo de katta . (0.71) 6. PC wo Tokyo de John ga katta . (0.57) katta All of the above sentences are acceptable and have the same meaning, and this is called “scrambling”. However, RIBES outputs different scores for these sentences. When we use the first one as the reference sentence, RIBES output scores in the parentheses. Human judges will give almost equal scores to all of them, and we should improve these RIBES scores for better evaluation. Scrambling is also observed in other languages such as German (Maier et al., 2014), Korean (Chun, 2013), Turkish (ldız et al., 2014), Hindi (Sharma and Paul, 2014), etc. Figure 2 (a) shows the dependency tree of “John ga Tokyo de PC wo katta”. Each box indicates a bunsetsu (chunk). Arrows indicate modification relations. The source node of an arrow modifies the target node of the arrow. The root “katta” has three modifiers (children), “John ga”, “Tokyo de”, and “PC wo”. We can generate 3! = 6 word orders by post-order traversal of this tree because the order of siblings does not matter. Figure 2 (b) shows a permutation and its dependency tree. In this case, all permutations are acceptable. However, more complex dependency trees tend to generate misleadi"
W15-3058,D14-1025,0,0.0260665,"Missing"
W15-3058,C14-1193,0,0.0200713,"paraphrases. Chan and Ng (2008) proposed M AX S IM that is based on a bipartite graph matching algorithm and assigns different weights to matches. Dependency relation are used as a factor in this framework. Zhu et al. (2010) proposed an SVM-based MT metric that uses different features in different granularities. Dependency relations are used as a feature in this framework. We designed our method not to parse MT outputs because some MT outputs are broken and it is difficult to parse them. Our method does not parse MT outputs and we expect our method is more robust than these methods. Recently, Yu et al. (2014) proposed RED, an evaluation metric based on reference dependency trees. They also avoided parsing of “results of Table 2: Details of system-level RIBES scores (NTCIR-7 EJ) gle ref, compDep, and compDep without correction are 0.388, 0.422, and 0.420, respectively. Thus, the difference between compDep (with correction) and compDep without correction is very small and we can skip the manual correction step. We used dependency analysis twice in the above method. First, we used it for generation of reordered reference sentences. Second, we used it for detecting misleading word orders. In the first"
W15-3058,C10-2175,0,0.0210931,"put sentences. They proposed four methods: STM, TKM, HWCM, DSTM, and DTKM. STM measures similarity by the number of matching subtrees. TKM uses Tree Kernel for the measurement. HWCM uses n-gram matches in dependency trees. DSTM and DTKM are dependency tree versions of STM and TKM respectively. Owczarzak et al. (2007) used LFG-based typed dependency trees. They also introduced processing of paraphrases. Chan and Ng (2008) proposed M AX S IM that is based on a bipartite graph matching algorithm and assigns different weights to matches. Dependency relation are used as a factor in this framework. Zhu et al. (2010) proposed an SVM-based MT metric that uses different features in different granularities. Dependency relations are used as a feature in this framework. We designed our method not to parse MT outputs because some MT outputs are broken and it is difficult to parse them. Our method does not parse MT outputs and we expect our method is more robust than these methods. Recently, Yu et al. (2014) proposed RED, an evaluation metric based on reference dependency trees. They also avoided parsing of “results of Table 2: Details of system-level RIBES scores (NTCIR-7 EJ) gle ref, compDep, and compDep witho"
